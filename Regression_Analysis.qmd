---
title: "Regression Analysis"
author: "Hyunsoo Kim"
date: "2022-03-14"
categories: [Basic, Code, R]
page-layout: full
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
editor_options: 
  chunk_output_type: console
mainfont: NanumGothic
---

# 0316 - 선형모형

> 예제를 통한 회귀분석

```{r}
getwd() #"C:/Users/Hyunsoo Kim/Documents/lecture/regression_analysis"
```

## 2장

### 표2.5

```{r}
data_2.5<-read.table("All_Data/p031.txt",header=TRUE,sep="\t")

dim(data_2.5) #14 2

head(data_2.5)

X<-data_2.5$Units

Y<-data_2.5$Minutes
```

### 표2.6

```{r}
df<-data.frame(

  #1:length(X),

  Y,

  X,

  Y-mean(Y),

  X-mean(X),

  (Y-mean(Y))^2,

  (X-mean(X))^2,

  (Y-mean(Y))*(X-mean(X))

)

df
```

### 공분산(Covariance) - 식2.2

```{r}
COV_XY<-sum((Y-mean(Y))*(X-mean(X))) / (length(X)-1) #136

### cov() 함수

cov(X,Y) #136

### 상관계수(correalationship)

### cor() 함수

cor(X,Y) #0.9936987 
```

## 0321 - 선형모형

```{r}
data_2.5<-read.table("All_Data/p031.txt",header=TRUE,sep="\t")

x<-data_2.5$Units

y<-data_2.5$Minutes
```

### 식 2.6

```{r}
cor_xy<- COV_XY / (sd(x)*sd(y))

cor_xy

### cor() 함수

cor(x,y)

cor(y,x)

data_2.5

cor(data_2.5)
```

### 그림 2.4

```{r}
class(X)

class(Y) #both numeric

plot(X,Y, pch=19,xlab="Units",ylab="Minutes") 
```

### 표 2.3

```{r}
data_2.3<-read.table("All_Data/p029a.txt",header=TRUE,sep="\t")

data_2.3

X<-data_2.3$X

Y<-data_2.3$Y
```

### 그림 2.2

```{r}
plot(X,Y)

cor(X,Y) # 0 완벽하게 2차함수의 형태도 0이 나옴(직선의 형태가 아닌것만)
```

### 표 2.4

```{r}
data_2.4<-read.table("All_Data/p029b.txt",header=TRUE,sep="\t")
```

### 그림 2.3

```{r}
plot(data_2.4$X1,data_2.4$Y1, pch=19); abline(3,0.5) #기울기 3 절편0.5인 선을 추가해라 

plot(data_2.4$X2,data_2.4$Y2, pch=19); abline(3,0.5)

plot(data_2.4$X3,data_2.4$Y3, pch=19); abline(3,0.5)

plot(data_2.4$X4,data_2.4$Y4, pch=19); abline(3,0.5)

m<-matrix(1:4,ncol=2,byrow=TRUE) #2행의 매트릭스 생성 

m

layout(m)

plot(Y1~X1, data=data_2.4,pch=19); abline(3,0.5)

#y~x  -> y=ax+b 이러한 형태를 가지는 모형식이라는 의미 

plot(Y2~X2, data=data_2.4,pch=19); abline(3,0.5)

plot(Y3~X3, data=data_2.4,pch=19); abline(3,0.5)

plot(Y4~X4, data=data_2.4,pch=19); abline(3,0.5)

layout(1) #변환을 다시 하지 않으면 설정한 매트릭스의 비율로 그래프가 그려짐 해제 필요 

# cor()

cor(data_2.4$X1,data_2.4$Y1) #0.8164205

cor(data_2.4$X2,data_2.4$Y2) #0.8162365

cor(data_2.4$X3,data_2.4$Y3) #0.8162867

cor(data_2.4$X4,data_2.4$Y4) #0.8165214

cor(data_2.4) #이렇게 한번에 할 수 있으나 가독성 떨어짐 
```

## 단순선형회귀모형 2.4

### 2.5 모수에 대한 추정

```{r}
data_2.5<-read.table("All_Data/p031.txt",header=TRUE,sep="\t")

x<-data_2.5$Units

y<-data_2.5$Minutes
```

### 식 2.14 & 2.15

```{r}
sum((y-mean(y))*(x-mean(x))) #1768

sum((x-mean(x))^2) #114

beta1_hat<-sum((y-mean(y))*(x-mean(x))) / sum((x-mean(x))^2)

beta1_hat #15.50877

beta0_hat <- mean(y) - (beta1_hat*mean(x))

beta0_hat #4.161654

### 최소제곱회귀 방정식

# Minutes = 4.161654 + 15.50877 * Units

plot(beta0_hat+beta1_hat*x,pch=19);

# 4개의 고장 난 부품을 수리하는데 걸리는 에측시간

4.161654 + 15.50877 * 4 #66.19673

units<-4

beta0_hat + beta1_hat*units

### 적합값(Fitted value)

y_hat<-beta0_hat + beta1_hat*(x)

### 최소 제곱 잔차(residual)

e<-y-y_hat

e #합이 0이라는 특징이 존재

sum(e) #1.278977e-13 0에 근사한 추지가 나옴
```

### 표2.7

```{r}
df_2.7<-data.frame(

  x=x,

  y=y,

  y_hat,

  e

)

df_2.7

### lm() 함수 (linear model)

# Minutes = beta0 + beta1 * Units + epsilon

# 모형식 : y~x

lm(y~x)

res_lm<-lm(Minutes~Units,data=data_2.5)

res_lm

# 리스트의 이름 

names(res_lm)

# 회귀계수

res_lm$coefficients

coef(res_lm)

# 적합값

res_lm$fitted.values

fitted(res_lm)

# 최소제곱잔차

res_lm$residuals

resid(res_lm)

residuals(res_lm)
```

### 그림 2.5 - 산점도 + 회귀선

```{r}
plot(beta0_hat+beta1_hat*x,pch=19);

#abline(beta0_hat,beta1_hat)

abline(res_lm)
```

## 0323

```{r}
data_2.5<-read.table("All_Data/p031.txt",header=TRUE,sep="\t")

res_lm <- lm(Minutes ~ Units, data = data_2.5)

res_lm

res_lm_summ<-summary(res_lm)

res_lm_summ #Pr(>|t|) - p-value

# unit은 시간에 영향을준다 약15.5분 만큼씩 

# coefficient에서 p-value에 대해서 알 수 있음 

# beta_0는 0이라고 보면되느냐? p-value가 0.05보다 크기에 
```

## 0328

### 2.7 신뢰구간

```{r}
confint(res_lm) # beta_0,1의 95% 신뢰구간을 뽑아줌 

?confint #level = 1-alpha

confint(res_lm, level=0.90) # 90%의 신뢰구간
```

### 2.8 예측

```{r}
# 4개의 고장난 부품을 수리하는 데에 걸리는 시간 예측

x<-4

4.161654 + 15.508772 *4

res_lm$coefficients[1]+(res_lm$coefficients[2]*x)

### predict()

df<-data.frame(Units=4) 

predict(res_lm,newdata=df) # res_lm을 만들때 사용한 데이터형식으로 만들어주어야함

res_lm_pred<-predict(res_lm,newdata=df,se.fit=TRUE)

### 예측값

res_lm_pred$fit

### 표준오차

res_lm_pred$se.fit # 평균반응에 대한 표준오차 

### 예측한계

df<-data.frame(Units=4) #예제서는 4대기준

df<-data.frame(Units=1:10) #다른것도 보고 싶은경우 

res_lm_pred_int_p<-predict(res_lm,newdata=df,interval="prediction")

### 신뢰한계

df<-data.frame(Units=1:10) #다른것도 보고 싶은경우 

res_lm_pred_int_c<-predict(res_lm,newdata=df,interval="confidence") #둘의 차이를 보면 예측한계의 범위가 더큼 

### 예측한계 & 신뢰한계

# 신뢰한계는 평균에서 멀어지만 오차의범위가 커지고 평균에 다가갈수록 오차가 줄어듬

plot(Minutes~Units,data=data_2.5,pch=19)

abline(res_lm,col="red",lwd=2)

lines(1:10,res_lm_pred_int_p[,"lwr"],col="darkgreen")

lines(1:10,res_lm_pred_int_p[,"upr"],col="darkgreen")

lines(1:10,res_lm_pred_int_c[,"lwr"],col="blue")

lines(1:10,res_lm_pred_int_c[,"upr"],col="blue")
```

### 2.9 적합성의 측정

```{r}
res_lm_summ<-summary(res_lm)

res_lm_summ #Multiple R-squared:0.9874 -> 반응변수의 전체변이중 98.94%가 예측변수에 의해 설명된다

# 만약 R-squared가 1이면 완벽한 선형의 관계 100%라는 것을 의미한다.

# R-squared는 변수가 들어갈수록 커지기에 adjust R-squared를 사용 추후 설명 
```

### 2.10 원점을 통과하는 회귀선

```{r}
# Minutes = beta1 + Units + epsilon

res_lm_no<-lm(Minutes~Units-1,data=data_2.5)

res_lm_no

summary(res_lm_no)

coef(summary(res_lm_no)) #rsquared=0.9975
```

### 2.11

```{r}
y<-rnorm(30)

t.test(y,mu=0)

summary(lm(y~1))
```

## 3장 다중선형회귀

### 3.3 사례 감독자 직무수행능력 데이터

```{r}
data_3.3<-read.table("All_Data/p060.txt",header=T,sep="\t")

dim(data_3.3)

class(data_3.3)

sapply(data_3.3,class) #all numeric

summary(data_3.3) #모든변수가 numeric이면 분위수도 보여준다 

### 산점도 행렬

plot(data_3.3)
```

### 3.4 모수 추정

```{r}
lm(Y~X1+X2+X3+X4+X5+X6,data=data_3.3)

lm(Y~.,data=data_3.3) # X1+X2+X3+X4+X5+X6쓰는 것이 아니라 .을 써서 모든 변수를 다써줌 
```

### 3.5 회귀계수에 대한 해석

```{r}
data_3.3<-read.table("All_Data/p060.txt",header=T,sep="\t")

lm(Y~X1+X2,data=data_3.3)

# (Intercept)         X1           X2  

#  15.32762      0.78034     -0.05016

# 1) Y에서 X1 효과 제거

m1<-lm(Y~X1,data=data_3.3) # y prime

m1$residuals # x1이 설명하지 못한값 / x1의 효과를 제거한 값

# 2) X2에서 X1 효과 제거

m2<-lm(X2~X1,data=data_3.3) # x2 prime 

m2$residuals 

# 3) X1의 효과가 제거된 Y와 X2의 적합 - 원점을 지나는 회귀선

lm(m1$residuals~m2$residuals-1) # 원점을 지나면 -1를 하고 진행 // -3.25e-17

# 다른 효과 없이(다른값이 고정) Y에 영향을 주는 순수한 X2의 값

# m2$residuals  : -0.05016  ==  X2 : -0.05016  

### 단위길이 척도화 - 잘사용하지않음

fn_scaling_len<-function(x){

  x0<-x-mean(x)

  x0/sqrt(sum(x0^2))

}

data_3.3_len<-sapply(data_3.3, fn_scaling_len)

data_3.3_len<-data.frame(data_3.3_len)

summary(data_3.3_len)

lm(Y~.,data=data_3.3_len)

### 표준화

# scale()

data_3.3_std<-scale(data_3.3)

#summary(data_3.3_std)

#sapply(data_3.3_std, sd, na.rm=T)

#class(data_3.3_std) #"matrix"

data_3.3_std<-data.frame(data_3.3_std)

#class(data_3.3_std) #"data.frame"

#sapply(data_3.3_std, sd, na.rm=T)

lm(Y~.,data=data_3.3_std) # beta게수 구하기 

```

### 3.7 최소제곱추정량의 성질

```{r}
res_lm<-lm(Y~.,data=data_3.3)

res_lm

summary(res_lm)

m1<-summary(lm(Y~X1+X2+X3+X4+X5+X6,data=data_3.3)) #Adjusted R-squared:  0.6628 

m2<-summary(lm(Y~X1+X2+X3+X4+X5,data=data_3.3)) #Adjusted R-squared:  0.6561 

# X6가 들어가는 것이 더 좋은 모델 

m1$adj.r.squared

m2$adj.r.squared #summary에서 보다 더 정확하게 수치가 나옴 
```

### 3.9 개별 회귀계수들에 대한 추론

```{r}
res_lm_summ<-summary(res_lm)

res_lm_summ #p-value의 존재는 무언가를 검정했다라는 반증

# p-value<0.05 H_1 귀무가설 채택 

# p-value>0.05 H_0 영가설 채택 // X1을 제외하고는 영가설 유의한 의미가 없음(Y에영향주는)

# 모두다 0이라는 가설을 가지고 분모 분자의 오차가 카이제곱을 따르고 거기서 나온 통계량

# F-분포 자유도는 분자 분모 두개를 가짐 //모아서 계산을 하기에 각각 계산하는것과 결과다름 

# 영가설-모든 회귀계수가 0이다.

# 대립가설-적어도 하나는 0이 아니다. p-value: 1.24e-05 <0.05 대립가설 채택 

# p-value가 0.05보다 작으면 대립가설 채택!!!!!! 기억해 

# 회귀계수에 대한 신뢰구간 - 95% 신뢰한계

confint(res_lm) #-13.18712881 ~ 34.7612816

#X1  0.28016866  0.9462066  사이에 0이 들어가있으면 영향을 준다라느걸 의미

#X2 -0.35381806  0.2077178  p-value없이도 알 수 있음 

#X5가 가장 영향이 적음 p-value가 가장 크기에(영향 효과의 크기를 비교할때)

#p-value가 작을 수록 영향을 많이 준다 beta값을 보는 것이 아닌 p-value를 보는 것 중요

#가장 의미있는 변수?->p-value가장 작은거 // 대립가설채택 Y에 영향을 가장

```

### 3.10 선형모형에서의 가설검정

### 3.10.1 모든 회귀예수들이 0인가에 대한 검정

```{r}
# H_0: beta_1:beta_6=0

model_reduce<-lm(Y~1,data=data_3.3)

model_full<-lm(Y~.,data=data_3.3)

anova(model_reduce,model_full)

#대립가설 = 완전모형이 적절하다 / 1.24e-05 *** < 0.05 

#의미 있는 예측 변수가 한개 이상 존재한다 

summary(model_full) #summary에서 beta_1~beta_6까지 모두가 0이라는 가설로 진행을 이미함

#F-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05

#예상// 가장의미있는변수? -> X1 이유?-> p-value 0.000903 로 가장 작기에 영향많이 줄것으로 예측 
```

### 3.10.2 회귀계수들의 부분집합이 0인가에 대한 검정

```{r}
model_reduce<-lm(Y~X1+X3,data=data_3.3)

model_full<-lm(Y~.,data=data_3.3)

anova(model_reduce,model_full) #0.7158 > 0.05

#영가설은 H_0: b_2=b_4=b_5=b_6=0 이라는 사실을 알 수 있다 

#b_1&b_3는 반응변수에 유의한 반응을 준다라는 것도 연계하여 알 수 있다 
```

### 3.10.3 회귀계수들의 통일성에 대한 검정

```{r}
#해당 조건이 주어지고 만족할 때 beta_1=beta_3은 맞는가?

model_reduce<-lm(Y~I(X1-X3),data=data_3.3) #I를 씌우면 새로운 변수를 만든것과 동일

# X1-X3를 한 그자체를 분석하라는 의미//본래는 X1-X3 해서 새로운 변수를 만들어서 해야함 

model_full<-lm(Y~X1+X3,data=data_3.3)

anova(model_reduce,model_full) 

#install.packages("car")

library(car)

model_full<-lm(Y~X1+X3,data=data_3.3)

car::linearHypothesis(model_full,c("X1=X3"))
```

### 3.10.4 제약조건하에서 회귀계수에 대한 추정과 검정

```{r}
# H_0: beta_1+beta_3=1 | beta_2=beta_3:beta_6=0

model_full<-lm(Y~X1+X3,data=data_3.3)

car::linearHypothesis(model_full,c("X1 + X3 = 1"))

# x1의 효과가 증가하면 x3의 효과는 감소한다 상대적인 관계 (반대로도 가능)
```

### 3.11 예측

```{r}
model_full<-lm(Y~.,data_3.3)

# 예측값 - 적합값

model_full$fitted.values

# 예측한계(Prediction Limits)

predict(model_full,newdata = data_3.3,interval = "prediction")

# 신뢰한계(Confidence limits)

predict(model_full,newdata = data_3.3,interval = "confidence")
```

## 부록 : 행렬을 이용한 회귀계수 추정

```{r}
data_3.3<-read.table("All_Data/p060.txt",header=T,sep="\t")

Y<-data_3.3$Y

X<-data_3.3[,-1]

X<-cbind(1,X)

X<-as.matrix(X)

#beta_hat<-solve(t(X) %*% X) %*% t(X) %*% Y # %*%행렬 계산 

P<-solve(t(X) %*% X) %*% t(X)

beta_hat<- P %*% Y

lm(Y~.,data_3.3)
```

## 4장 회귀진단: 모형위반의 검출

### 4.3 다양한 유형의 잔차들

```{r}
# 표준화 잔차

data_3.3<-read.table("All_Data/p060.txt",header=T,sep="\t")

res_lm<-lm(Y~.,data=data_3.3)

class(res_lm)

mode(res_lm)

names(res_lm)

res_lm$fitted.values

str(res_lm)

# 잔차 

res_lm$residuals

resid(res_lm) #실제값에서 예측된 값을 뺸값

### 내적 표준화잔차

rstandard(res_lm)

### 외적 표준화잔차

MASS::studres(res_lm)

redsid_df<-data.frame(

  Y=data_3.3$Y,

  Y_hat=res_lm$fitted.values,

  resid=resid(res_lm),

  rstandard=rstandard(res_lm),

  studres=MASS::studres(res_lm)

)

redsid_df
```

### 4.5 모형을 적합깅 이전의 그래프

### 4.5.1 일차원 그래프

```{r}
a<-rnorm(100,70,10) #연속형 데이터

# 히스토그램 

hist(a)

hist(a,breaks=5) #범위를 조절 막대의 5번 자름 

# 줄기 잎 그림 

stem(a)

stem(round(a)) #줄기잎을 그릴때는 반올림을 하고 항상 진행 

stem(round(a),scale=2) #scale을 2배로 늘려라 5기준으로 반으로 잘라서 

# 모든데이터를 볼 수있는 장점 데이터가 많으면 구림 

# 점플롯

idx<-rep(1,length(a)) #a의 갯수에 맞춰서 1를 반복 

plot(idx,a)

plot(jitter(idx),a,xlim=c(0.5,1.5))

# 상자그림

boxplot(a) #사분위수에 대해서 알 수 있음 

# 상자그림 + 점플롯

boxplot(a)

points(jitter(idx),a)
```

### 4.5.2 이차원 그래프

```{r}
data_4.1<-read.table("All_Data/p103.txt",header=T,sep="\t")

data_4.1

class(data_4.1) #data.frame

# 산점도 행렬

plot(data_4.1)

cor(data_4.1) #상관계수

pairs(data_4.1)

# Correlation panel

panel.cor<-function(x,y){

  par(usr=c(0,1,0,1))

  r<-round(cor(x,y),digits = 3)

  text(0.5,0.5,r,cex=1.5)

}

pairs(data_4.1,lower.panel = panel.cor)

# 회전도표, 동적 그래프(3차원)

#install.packages("rgl")

library(rgl)

plot3d(x=data_4.1$X1,y=data_4.1$X2,z=data_4.1$Y) 
```

### 4.7 선형성과 정규성 가정에 대한 검토

```{r}
data_3.3<-read.table("All_Data/p060.txt",header=T,sep="\t")

res_lm<-lm(Y~X1+X3,data=data_3.3)

layout(matrix(1:4,nrow=2,byrow=T))

plot(res_lm) #회귀진단 플랏이 나옴 

layout(1)

# 1. 표준화잔차의 정규확률분포

plot(res_lm,2)

# 2. 표준화잔차 대 각 예측변수들의 산점도

plot(res_lm,3) #이런경우에는 데이터를 추가한다 좌측하단이 없어서 

# 3. 표준화잔차 대 적합값의 플롯

# 4. 표준화잔차의 인덱스 플롯
```

## 0502 - 선형모형

```{r}
library(dplyr)
```

### 4.7 선형성과 정규성 가정에 대한 검토

```{r}
data_3.3<-read.table("All_Data/p060.txt",header=T,sep="\t")

res_lm<-lm(Y~X1+X3,data=data_3.3) #두개의 변수만 의미있다고 가정하고 진행 

# 회귀진단 그래프들

layout(matrix(1:4,nrow=2,byrow=T))

plot(res_lm) #회귀진단 플랏이 나옴 / 총6개임 

layout(1) #다시 한개의 플랏만 그리도록 

# 1. 표준화잔차의 정규확률분포

plot(res_lm,2) #QQ-plot y=x 기울기의 직선위에 점들이 있어야 한다 눈대중으로 

# 2. 표준화잔차 대 각 예측변수들의 산점도

plot(res_lm,3) #이런경우에는 데이터를 추가한다 좌측하단이 없어서 

#랜덤하게 데이터가 흩어져 있어야 한다

# 내적 표준화잔차

plot(data_3.3$X1,rstandard(res_lm))

plot(data_3.3$X3,rstandard(res_lm)) #각각의 잔차들이 랜덤하게 잘 퍼져야함 

# 3. 표준화잔차 대 적합값의 플롯

plot(res_lm,1) #잔차와 적합값은 상관성이 없어야하며 랜덤하게 퍼져야함 

# 4. 표준화잔차의 인덱스 플롯 

plot(res_lm,5)

data_2.4<-read.table("All_Data/p029b.txt",header=T,sep="\t")

m<-matrix(1:4,ncol=2,byrow=TRUE)  

layout(m)

plot(Y1~X1, data=data_2.4,pch=19); abline(3,0.5)

plot(Y2~X2, data=data_2.4,pch=19); abline(3,0.5)

plot(Y3~X3, data=data_2.4,pch=19); abline(3,0.5)

plot(Y4~X4, data=data_2.4,pch=19); abline(3,0.5)

layout(1)
```

### Figure4.4

```{r}
res_lm<-lm(Y1~X1,data=data_2.4) 

#1번플랏은 적당히 잘퍼짐,2번플랏은 어느정도 선형성 있음(데이터적어서그런거임)

res_lm<-lm(Y2~X2,data=data_2.4)

res_lm<-lm(Y3~X3,data=data_2.4)

res_lm<-lm(Y4~X4,data=data_2.4)

res_lm

layout(matrix(1:4,nrow=2,byrow=T))

plot(res_lm) 

layout(1)
```

### 4.8 지레점, 영향력, 특이값

```{r}
# 사례: 뉴욕 강 데이터

# agr-농업, forest-숲, rsdntial-주거, comlndl-산업, nitrogen-질소

data_1.9<-read.table("All_Data/p010.txt",header=T,sep="\t")

head(data_1.9)

plot(data_1.9[-1],pch=19) #river라는 첫번째 컬럼을 제외하고 진행 

res_1<-lm(Nitrogen~.,data=data_1.9[-1]) #모든데이터 사용

res_2<-lm(Nitrogen~.,data=data_1.9[-4,-1]) #4번쨰 데이터 제외

res_3<-lm(Nitrogen~.,data=data_1.9[-5,-1]) #5번쨰 데이터 제외 

#회귀계수

data.frame(all=coef(res_1),

           rm4=coef(res_2),

           rm5=coef(res_3))

#p-value

data.frame(all=coef(summary(res_1))[,4],

           rm4=coef(summary(res_2))[,4],

           rm5=coef(summary(res_3))[,4])

#4,5번째 데이터를 각각 뺴고 진행을 해보니 영향을 끼치는 값임을 알수있고

#5번째는 주거 관련해서는 부호를 바꿀 정도로 강력하다 

# 단순선형회구모형

res<-lm(Nitrogen~ComIndl,data=data_1.9)
```

### 그림 \[4.5\]

```{r}
plot(Nitrogen~ComIndl,data=data_1.9,pch=19)

abline(res)

#leverage values 지레값

p_ii<-hatvalues(res)

hiegh_leverage<-ifelse(p_ii>2*2/30,data_1.9$River,"")

hiegh_leverage #높은 지레값을 가지고 있는 강의 이름을 표시(이는 보기에 편하기 위해서함)

text(data_1.9$ComIndl,data_1.9$Nitrogen-0.1,hiegh_leverage)
```

### 그림 \[4.6\] - 잔차의 인덱스플롯

```{r}
plot(rstandard(res),pch=19) #2또는 3시그마를 넘으면 특이값이라 함 
```

### 그림 \[4.6\] - 지레값의 인덱스플롯

```{r}
plot(p_ii,pch=19) #평균의 2배를 기준으로 비교함 

abline(h=2*2/30,col="red") #이보다 높은것이 지레값이 높은것 높은 영향력을 가진것 

# 단순선형회귀모형

res<-lm(Nitrogen~ComIndl,data=data_1.9)

plot(Nitrogen~ComIndl,data=data_1.9,pch=19)

abline(res)

res<-lm(Nitrogen~ComIndl,data=data_1.9[-4,-1])

plot(Nitrogen~ComIndl,data=data_1.9[-4,-1],pch=19)

abline(res) #4번째 제외

res<-lm(Nitrogen~ComIndl,data=data_1.9[-5,-1])

plot(Nitrogen~ComIndl,data=data_1.9[-5,-1],pch=19)

abline(res) #5번쨰 제외

#이처럼 4,5번을 빼고 진행을 하면 조금더 잘 나타냄

res<-lm(Nitrogen~ComIndl,data=data_1.9[c(-4,-5),-1])

plot(Nitrogen~ComIndl,data=data_1.9[-5,-1],pch=19)

abline(res) #4,5번째 제외 
```

### 4.9 영향력의 측도

### 4.9.1 Cook의 거리

```{r}
res<-lm(Nitrogen~ComIndl,data=data_1.9)

plot(res,4)

#install.packages("olsrr")

library(olsrr)

ols_plot_cooksd_chart(res)
```

### 4.9.2 Welsch & Kuh의 측도

```{r}
olsrr::ols_plot_dffits(res)
```

### 4.9.3 Hadi의 영향력 측도

```{r}
olsrr::ols_plot_hadi(res)

# Residual & Leverage & Cook's distance

plot(res,5) #영향력 관측치를 보기 위한 플랏 

```

### 4.10 잠재성 - 잔차플롯

```{r}
olsrr::ols_plot_resid_pot(res) #2.0에 있는 값외에도 x축 0.2이상의 것들도 특이값으로 

#지레값이 커도 영향력이 없는 애들은 신경 안써도 되나 영향력이 큰애들을 탐색해 보아야 함
```

### 4.11 특이값에 대한 처리

```{r}
#특이값이 큰경우 그것이 TRUE데이터면 오류가 있는 경우 수정을 하거나 가중치를 변화를

#하거나 데이터를 수정을 시켜주거나 다시 실험을 하는 등 여러가지 방법을 사용하여.. 

## 첨가변수플롯(added-variable plot), 편회귀플롯(partial regression plot)

res<-lm(Nitrogen~ComIndl,data=data_1.9)

car::avPlots(res,pch=19)

res<-lm(Nitrogen~.,data=data_1.9[-1])

car::avPlots(res,pch=19)

#beta별로 각각의 어떤 변수가 영향력을 많이 주는지 알게하는 함수 
```

### 사례:스코틀랜드 언덕 경주 데이터

```{r}
data_4.5<-read.table("All_Data/p120.txt",header=T,sep="\t")

dim(data_4.5)

names(data_4.5)

head(data_4.5)

# 회전도표 

library(rgl)

with(data_4.5,plot3d(x=Distance,y=Climb,z=Time))

res<-lm(Time~Distance+Climb,data=data_4.5)

summary(res) #beta_0가 마이너스여도 신경안씀 관심있는 것은 회귀계수임 

#Time=-539.4829+373.0727*Distance+0.6629*Climb 
```

```{r}
### 첨가변수플롯(added-variable plot), 편회귀플롯(partial regression plot)

car::avPlots(res,pch=19)

### 성분잔차플롯(component plus residual plots), 편자차플롯(partial residual plot)

car::crPlots(res,id=T,pch=19) #첨가변수보다 성분잔차를 더 많이 사용 / 비선형여부를 확인

# 점선에 비해서 분홍선이 크게 떨어져 있지 않아 선형적인 추세를 가지고 있다고 추정이 가능 

### 잠재성-잔차플롯

olsrr::ols_plot_resid_pot(res)

### Hadi의 영향력 측도

olsrr::ols_plot_hadi(res)

### Cook의 거리

olsrr::ols_plot_cooksd_chart(res) #전체적은 플롯을 보면서 이상치에 대한 확인을 함 

# 이러한 값들의 제외 여부는 연구자가 선택해서 진행을 함 

# outlier에 대한 처리를 어떻게 했다고 말을 해야함 

# 보고서는 옆에 사람이 보고 쉽게 따라할 수 있을 정도로 

# 어떤 속성으로 어떻게 진행을 했다라는 것을 중시 코드 보단 결과를 보여줘라 
```

### 4.14 로버스트 회귀 - outlier에 크게 영향을 받지 않음

```{r}
library(MASS)

res<-lm(Time~Distance+Climb,data=data_4.5)

res

res_rlm<-MASS::rlm(Time~Distance+Climb,data=data_4.5)

res_rlm

summary(res)

summary(res_rlm)

#install.packages("robustbase")

library(robustbase)

res_lmrob<-lmrob(Time~Distance+Climb,data=data_4.5)

res_lmrob

summary(res_lmrob)

#standard error가 res > res_rlm > res_lmrob 순으로 되어 있음 

# 4장 연습문제 해보기
```

## 0509 - regression analysis

```{r}
library(dplyr)
```

## 5장 질적 예측 변수

```{r}
#install.packages("fastDummies")

library(fastDummies)
```

## 5.2 급료조사 데이터

```{r}
data_5.1<-read.table("All_Data/p130.txt",header=T,sep="\t")

# S:급료 X:경력 E:교육수준 M:관리(형태) / E,M은 범주형 변수

names(data_5.1)

# 범주형 질적 변수를 수치형으로 변형시켜서 예측하는데 사용한것이 질적 예측변수이다 

# E_1,E_2,E_3이런식으로 나누어서 0,1로 분류를 한다 (이것이 가변수)

# 더미변수를 만들 경우에는 역행렬의 조건에 의해서 -1개의 변수만 만들면 된다 

# 이는 공산성의 문제또한 있기에 이를 위해서 -1를 한것임 

### 자료형 변경 : 정수 -> 범주

data_5.1$E<-as.factor(data_5.1$E)

data_5.1$M<-as.factor(data_5.1$M)

head(data_5.1)

data_5.1$E #Levels: 1 2 3이라는 것이 생김 문자로 처리한다는 의미 

### 가변수 생성

data_5.1$E<-factor(as.character(data_5.1$E),levels = c("3","1","2")) 

#3번을 베이스 카테고리로 쓰기위한 설정 / 설정을 안하면 베이스는 e_1이 된다

data_5.1$M<-factor(as.character(data_5.1$M),levels = c("0","1")) 

data_dummy<-dummy_cols(data_5.1,

                       select_columns = c("E","M"),

                       remove_first_dummy = T,

                       remove_selected_columns = T) #첫번째 생성되는 더미 변수를 제거

data_dummy #가변수의 더미는 n-1개를 하는 것이 역행렬을 위한 것이기에 지워준다 

# 더미를 만든 이후 더미의 모체 변수인 E M을 지워주어야 한다 

### 회귀분석(1) - 가변수

res<-lm(S~.,data = data_dummy)

res 

### 회귀분석(2) - lm()

res_1<-lm(S~.,data=data_5.1)

res_1 

#더미변수를 많이 쓰기에 factor로 바꾸어주고 분석하면 알아서 더미변수를 만들어서 진행함

summary(res_1)

#E_3와 M_0는 Intercept에 포함이 되어있음 그렇기에 베이스 카테고리라고 한다 

#E가 3이고 M이 0이면 대학원이상 일반관리 직급 -> Intercept(Beta_0) + X*Beta_1 = Y

### 표준화잔차 대 경력연수

plot(data_5.1$X, rstandard(res_1),pch=19,xlab="범주",ylab="잔차")

# 0을 중심으로 잘 퍼져있는가를 확인해야함 

#### 표준화잔차 대 교육수준 - 관리 조합

EM<-paste0(data_5.1$E,data_5.1$M)

plot(EM,rstandard(res_1),pch=19,xlabl="범주",ylab="잔차")

### 상호작용 효과(Interaction Effect)

res<-lm(S~X+E+M+E*M,data=data_5.1)

res

summary(res)

### 표준화잔차 대 경력연수

plot(data_5.1$X,rstandard(res),pch=19,xlab="범주",ylab="잔차")

### Cook의 거리

plot(res,4) #33번째 데이터만 제외하고 다시 회귀모형을 생성예정

### 상호작용 효과 - 관측개체 33 제외 

data_use<-data_5.1[-33,]

res<-lm(S~X+E+M+E*M,data=data_use)

res

summary(res)

### 표준화잔차 대 경력연수

plot(data_use$X,rstandard(res),pch=19,xlab="범주",ylab="잔차")

#### 표준화잔차 대 교육수준 - 관리 조합

EM<-paste0(data_use$E,data_use$M)

plot(EM,rstandard(res),pch=19,xlabl="범주",ylab="잔차")

#상호 호과가 들어간 이 모형이 더 괜찮은 모양이라고 판단이 된다 

### 기본 급료 추정 - 표 5.6

res<-lm(S~X+E+M+E*M,data=data_use)

df_new<-data.frame(X=rep(0,6),

                   E=rep(1:3,c(2,2,2)),

                   M=rep(c(0,1),3))

### 가변수 생성 - 분석용

df_new$E<-factor(as.character(df_new$E),levels = c("3","1","2")) 

df_new$M<-factor(as.character(df_new$M),levels = c("0","1")) 

cbind(df_new,predict = predict(res,df_new,interval = "confidence"))

# 이를 보고 각 레벨에 따른 차이를 보고 얼마나 나는 지 분석이 가능해야 한다 

# ex) 고졸과 대학원졸의 관리자 직급의 급여의 차이는?(평균적으로)
```

### 5.4 회귀방정식의 체계: 두 집단의 비교

### 표 5.7 - 고용전 검사 프로그램 데이터

```{r}
data_5.7<-read.table("All_Data/p140.txt",header=T,sep="\t")

head(data_5.7)

# 모형 1 - 통합모형 인종간 차이가 없을때

model_1<-lm(JPERF~TEST,data=data_5.7)

summary(model_1) 

# 모형 3 

model_3<-lm(JPERF~TEST+RACE+TEST*RACE,data=data_5.7)

summary(model_3)

# 인종적인 차이가 있는지 없는지를 확인해야 한다 어떤 모형을 사용할지 

### H_0:gamma=delta=0

anova(model_1,model_3) #model_3가 FM(완전모형)

#P-value(0.05424)<0.05이기에 H_0 이기에 모형1을 선택하는 것이 옳다고 판단(그러나 확신x)

#서로의 R-squared롤 보면 model_3가 더 좋음 / ANOVA는 참고용 절대적이지는 않다 
```

### 그림 5.7 - 표준화잔차 대 검사점수 : 모형 1

```{r}
plot(data_5.7$TEST,rstandard(model_1),

     pch=19,xlab="검사점수",ylab="잔차",

     main="그림 5.7 - 표준화잔차 대 검사점수 : 모형 1")
```

### 그림 5.8 - 표준화잔차 대 검사점수 : 모형 3

```{r}
plot(data_5.7$TEST,rstandard(model_3),

     pch=19,xlab="검사점수",ylab="잔차",

     main="그림 5.8 - 표준화잔차 대 검사점수 : 모형 3")

# 통계에서 나오는 결과는 결정의 보조수단이지 절대적이지 않아 

# 3번 모형을 선택한다고 결정한다고 진행 

summary(model_3) # Multiple R-squared:  0.6643
```

### 그림 5.9 - 표준화잔차 대 검사점수 : 모형 1

```{r}
plot(data_5.7$RACE,rstandard(model_1),

     pch=19,xlab="검사점수",ylab="잔차",

     main="그림 5.9 - 표준화잔차 대 검사점수 : 모형 1")

# 분리된 회귀분석 결과

data_5.7_R1<-subset(data_5.7,RACE==1)

model_R1<-lm(JPERF~TEST,data=data_5.7_R1)

summary(model_R1) #소수민족

data_5.7_R0<-subset(data_5.7,RACE==0)

model_R0<-lm(JPERF~TEST,data=data_5.7_R0)

summary(model_R0) #백인

# 통합모형에서 나온 각각의 회귀식이 통합모형에서 나온것과 동일함 따라서 인종별로 나누어서

# 진행할 필요없이 통일모형을 사용해서 진행을 하면 된다(이는 데이터셋을 나눈경우와 동일함)
```

### 그림 5.10 - 표준화잔차 대 검사점수 : 모형 1. 소수민족만

```{r}
plot(data_5.7_R1$TEST,rstandard(model_R1),

     pch=19,xlab="검사점수",ylab="잔차",

     main="그림 5.10 - 표준화잔차 대 검사점수 : 모형 1. 소수민족만만")
```

### 그림 5.11 - 표준화잔차 대 검사점수 : 모형 1 . 백인만

```{r}
plot(data_5.7_R0$TEST,rstandard(model_R0),

     pch=19,xlab="검사점수",ylab="잔차",

     main="그림 5.11 - 표준화잔차 대 검사점수 : 모형 1. 백인만")

# 적절한 합격점수의 결정 - 소수민족

# 고용전 검사점수의 합격점에 대한 95% 신뢰구간

ym<-4

xm<-(ym-0.09712)/3.31095

s<-1.292

n<-10

t<-qt(1-0.05/2,8)

c(xm-(t*s/n)/3.31095, xm+(t*s/n)/3.31095) #신뢰구간
```

### 5.4.2 동일한 기울기와 다른 절편항을 가지는 모형

```{r}
model_3<-lm(JPERF~TEST+RACE,data=data_5.7)

summary(model_3)

#Intercept = BETA_0 / TEST = BETA_1 / RACE = gamma

## H_0: gamma=0

anova(model_1,model_3) #p-value(0.1552)>0.05 이기에 H_0은 참이다//gamma=0

#기울기가 같고 절편이 다른 모형은 아니라고 데이터가 이야기하고 있다 

# 소수민족(RACE=1): (0.6120+1.0276)+2.2988*TEST = 1.6396+2.2988*TEST

# 백인(RACE=0): (0.6120)+2.2988*TEST
```

### 5.4.3 동일한 절편항과 다른 기울기를 가지는 모형

```{r}
model_3<-lm(JPERF~TEST+I(TEST*RACE),data=data_5.7)

summary(model_3) #I()를 하면 교호작용하는 것만 보이게 하려고 없으면 RACE항이 자동추가됨

## H_0: delta=0

anova(model_1,model_3) #p-value(0.03395)<0.05보다 작기에 delta항은 필요한 변수라는 사실 

#Intercept = BETA_0 / TEST = BETA_1 / I(TEST*RACE) = delta
```

## 6장 변수변환 : Transformation of Varibables

```{r}
library(dplyr)
```

### 6.3 x-선 방사에 의한 박테리아 사망률

```{r}
data_6.2<-read.table("All_Data/p168.txt",header=T,sep="\t")

head(data_6.2)
```

### 그림 6.5

```{r}
plot(N_t~t,data=data_6.2,pch=19)
```

### 6.3.1 선형모형의 부적절성

```{r}
res<-lm(N_t~t,data=data_6.2)

summary(res)
```

### 그림 6.6

```{r}
plot(data_6.2$t,rstandard(res),pch=19) # 표준화 잔차의 플랏

# 잔차가 랜덤하게 잘 퍼져있어야 하는데 적절한 회귀모형이 아니라는 사실이 나옴
```

### 6.3.2 선형성을 위한 로그변환

### 그림 6.7

```{r}
plot(log(N_t)~t,data=data_6.2,pch=19)

# 박테리아의 수에 로그를 취하니 선형성이 보인다 

res<-lm(log(N_t)~t,data=data_6.2)

summary(res)

plot(data_6.2$t,rstandard(res),pch=19)

# 로그를 취해주니 잔차가 랜덤하게 이루어져 있음 

# n_0에 대한 추론

exp(5.973160)

exp(coef(res)[1]) #로그를 취해주고 하는 부분이 잘 이해가 안됨 

exp(coef(res)[1]-0.0588/2) #불편추정량 구하기 (참고용)
```

### 6.4 분산안정화 변환

```{r}
data_6.6<-read.table("All_Data/p174.txt",header=T,sep="\t")

data_6.6 #운항률과 사고 발생 건수에 대한 자료

plot(Y~N,data=data_6.6,pch=19) #잔차의 분산이 계속 커지는 효과를 보임 

res_1<-lm(Y~N,data=data_6.6)

summary(res_1)

# 표준화잔차 대 N의 플롯 / 그림 6.11

plot(data_6.6$N,rstandard(res_1),pch=19) #등분산성에 만족하지 못하는 모형을 보인다 

# N에 대한 sqrt(Y)의 회귀

# N에 대한 Y의 회귀

res_2<-lm(sqrt(Y)~N,data=data_6.6)

summary(res_2)
```

### 그림 6.12

```{r}
plot(data_6.6$N,rstandard(res_2),pch=19) #0을 중심으로 잔차가 보다 잘 퍼져있음이 보임 
```

### 6.5 이분산성의 검출

```{r}
data_6.9<-read.table("All_Data/p176.txt",header=T,sep="\t")

data_6.9

# Y 대 X의 플로

plot(Y~X,data=data_6.9,pch=19,main="그림 6.13")

# X에 대한 Y의 회귀

res_1<-lm(Y~X,data=data_6.9)

summary(res_1)

# 표준화잔차 대 X의 플롯

plot(data_6.9$X, rstandard(res_1),pch=19,main = "그림 6.14")
```

### 6.6 이분산성의 제거

```{r}
# 변환된 Y/X와 1/X를 적합한 회귀

data_6.9_1<-data.frame(Y=data_6.9$Y/data_6.9$X,

                       X=1/data_6.9$X)

res_2<-lm(Y~X,data=data_6.9_1)

summary(res_2) #지금은 변환하고 프라임 값들의 추정치가 나온것이기에 원래 회귀식으로 돌아가야함 

# 본래 변환시 X를 나눴기에 X를 곱해준다 -> B_0,B_1의 추정값이 바뀐다 서로 

plot(data_6.9_1$X, rstandard(res_2),pch=19,

     xlab="1/X",ylab="잔차",main = "[그림 6.15]")

```

### 6.7 가중최소제곱법

```{r}
wt<-1/data_6.9$X^2 #가중값

res_3<-lm(Y~X,data=data_6.9,weights = wt)

summary(res_3) #결과는 위의 6.6과 동일한 값이 나옴 

```

### 6.8 데이터에 대한 로그변환

```{r}
data_6.9<-read.table("All_Data/p176.txt",header=T,sep="\t")

head(data_6.9)

# log(Y) 대 X의 산점도

plot((Y)~X,data=data_6.9,pch=19,main="식 6.9")

plot(log(Y)~X,data=data_6.9,pch=19,main="그림 6.16")

res_4<-lm(log(Y)~X,data=data_6.9)

summary(res_4)

# X에 대한 log(Y)의 회귀로 부터 얻은 표준화잔차플롯

plot(data_6.9$X,rstandard(res_4),pch=19,

     xlab="X",ylab="잔차",main="그림 6.17")

# X와 X^2에 대한 log(Y)의 회귀

df<-data.frame(log_Y=log(data_6.9$Y),

               X=data_6.9$X,

               X2=data_6.9$X^2)

res_5<-lm(log_Y~X+X2,data=df) #그러나 이러한 과정은 필요업고 아래방식으로..

res_5<-lm(log(Y)~X+I(X^2),data=data_6.9)

summary(res_5)

# 표준화잔차 대 적합값 플롯 : X와 X^2에 대한 log(Y)의 회귀

plot(fitted(res_5),rstandard(res_5),pch=19,

     xlab="X",ylab="잔차",main="그림 6.18")

# 표준화잔차 대 X의 플롯 : X와 X^2에 대한 log(Y)의 회귀

plot(data_6.9$X,rstandard(res_5),pch=19,

     xlab="X",ylab="잔차",main="그림 6.19")

# 표준화잔차 대 X^2의 플롯 : X와 X^2에 대한 log(Y)의 회귀

plot(data_6.9$X^2,rstandard(res_5),pch=19,

     xlab="X",ylab="잔차",main="그림 6.20") #2차항을 추가함으로서 더 잘 피팅됨 

# 7장은 스킵 / 8장 스킵
```

### 9.2 통계적 추론에 미치는 효과

```{r}
data_9.1<-read.table("All_Data/p236.txt",header=T,sep="\t")

head(data_9.1)

res<-lm(ACHV~.,data_9.1)

summary(res)

#F-statistic: 5.717 on 3 and 66 DF, p-value: 0.001535

#이는 의미있는 beta가 존재한다라는 의미

#그러나 각 계수들의 회귀계수를 보니 모두 0이라는 결과가 나옴 이는 다중공선성이다

# Correlation panel

panel.cor<-function(x,y){

par(usr=c(0,1,0,1))

r<-round(cor(x,y),digits = 3)

text(0.5,0.5,r,cex=1.5)

}

pairs(data_9.1[-1],lower.panel = panel.cor)

```

### 그림 9.1

```{r}
plot(fitted(res),rstandard(res),pch=19,

xlab="예측값",ylab="잔차",main="[그림 9.1]")

#이는 회귀모형에 동시에 들어가면 안된다라는 의미 (제거가 필요)
```

### 9.3 예측에 미치는 효과

```{r}
data_9.5<-read.table("All_Data/p241.txt",header=T,sep="\t")

head(data_9.5)

### 산점도

pairs(data_9.5[-1],lower.panel = panel.cor)

# 회귀분석(1) : 데이터 1949~1966

res<-lm(IMPORT~.,data_9.5[-1])

summary(res) #다중공선성이 존재한다고 예측을 일단함
```

### 그림 9.3 : 표준화잔차의 인덱스플롯

```{r}
plot(1:nrow(data_9.5),rstandard(res),

pch=19,type="b",

xla="번호",ylab="잔차",main="[그림 9.3]")

# 회귀분석(2) : 데이터 1949~1959

data_use<-subset(data_9.5,YEAR<=59)

res<-lm(IMPORT~.,data_use[-1])

summary(res)

```

### 그림 9.4 : 잔차플롯

```{r}
plot(1:nrow(data_use),rstandard(res),

pch=19,type="b",

xla="번호",ylab="잔차",main="[그림 9.4]")
```

### 9.4 다중공선성의 탐색

```{r}
# 분산확대인자

library(olsrr)

# 교육기회 균등(EEO)

res_9.1<-lm(ACHV~.,data_9.1)

summary(res_9.1)

#car::vif(res_9.1)

olsrr::ols_vif_tol(res_9.1) # 10보다 크면 유의한 영향을 준다 제거필요?

```

### 표 9.5 : 프랑스 경제 데이터

```{r}
data_use<-subset(data_9.5,YEAR<=59)

res_9.5<-lm(IMPORT~.,data_use[-1])

#car::vif(res_9.1)

olsrr::ols_vif_tol(res_9.5) #작으면 새로운 변수를 만들거나 제거를 통해서 진행

# 상태 지수

cnd.idx<-olsrr::ols_eigen_cindex(res_9.1)

round(cnd.idx,4)

cnd.idx<-olsrr::ols_eigen_cindex(res_9.5)

round(cnd.idx,4)
```

## 10장 - 공선형 데이터의 처리

```{r}
data_9.5<-read.table("All_Data/p241.txt",header=T,sep="\t")

head(data_9.5)

 

# 회귀분석(2) : 데이터 1949~1959

data_use<-subset(data_9.5,YEAR<=59)

res<-lm(IMPORT~.,data_use[-1]) #1열 제거 

summary(res)

head(data_use[-c(1:2)])

### 주성분(principle component)

pc<-prcomp(data_use[-c(1:2)],scale.=T)

pc$rotation

pc$x #변화된 새로운 변수가 저장된곳

### 주성분회귀 - 원래데이터를 주성분분석을 통해 새로운 데이터를 생성 이를 가지고 회귀 

df<-data.frame(IMPORT = scale(data_use$IMPORT),

               pc$x)

df

res<-lm(IMPORT~.,data=df)

summary(res) #유의한 1,2번째 계수들만 사용하고 3번째 것은 없이 해도 되겠다..

#중간고사 이후의것이 주가나오겠지만 앞에것도 알아야함 연습문제에서 나올것 예상

```

## 11장

```{r}
# 11.10 - 

data_3.3<-read.table("All_Data/p060.txt",header=T,sep="\t")

head(data_3.3)

# 분산확대 인자(VIF) : 10초과 > 심각한 공산성의 문제가 있음

res<-lm(Y~.,data=data_3.3)

summary(res)

olsrr::ols_vif_tol(res) #10을 넘는 것이 없기에 공산성의 문제는 없다 

# 수정결정계수

res_summ<-summary(res)

res_summ$adj.r.squared #0.662846

# Mallow's Cp - 작을수록 좋음

# 축소모형

res_subset<-lm(Y~X1+X3,data=data_3.3)

olsrr::ols_mallows_cp(res_subset,res) 

# AIC - 참고만

olsrr::ols_aic(res_subset,method = "SAS")

# BIC

olsrr::ols_sbic(res_subset,res)

### 전진적 선택법 - AIC

res_step<-step(res,direction = "forward")

### 후진적 제거법 -AIC

res_step<-step(res,direction = "backward")

### 단계적 방법

res_step<-step(res) #최종적으로는 X1+X3이 선택됨 이거 사용하는 것이 좋을듯 

summary(res_step) #X3가 의미 없다고 나와도 아님 검증된것임
```

## 0613 - regression analysis

### 연습문제 11.5

```{r}
data_use<-read.table("All_Data/p329.txt",header=T,sep="\t")

head(data_use)

### 데이터 탐색 - 자료형

sapply(data_use,class)

### 데이터 탐색 - 기초 통계량

summary(data_use) #결측값의 여부를 확인

### 데이터 탐색 - 히스토그램 & 상자그림

hist(data_use$Y,main="histogram of data_use$Y")

boxplot(data_use$Y)

### 데이터 탐색 - 산점도 행렬 & 상관계수

plot(data_use)

cor(data_use) 

pairs(data_use)

panel.cor<-function(x,y){

  par(usr=c(0,1,0,1))

  r<-round(cor(x,y),digits = 3)

  text(0.5,0.5,r,cex=1.5)

}

pairs(data_use,lower.panel = panel.cor)

### (a) 모든 변수들이 모형에 포함시킬 것인가?

cor(data_use)

model_1<-lm(data_use$Y~.,data_use)

summary(model_1) #전형적인 다중공선성이 보인다 

#coef가 X6 0.820이므로 확인 필요

olsrr::ols_vif_tol(model_1) #X6,X7이 10에 가깝기에 제거해야하는 대상중 최우선 

#다중공선성이 보이기에 모든 변수를 모형에 포함 시킬수는 없다 

#10보다 크면 심각한 공선성이 존재 

### (b) 지방세(X1) 방의 수(X6), 건물의 나이(X8)가 판매가격(Y)을 

#       설명하는데 적절하다는 의견에 동의 하는가?

model_2<-lm(Y~X1+X6+X8,data=data_use)

summary(model_2)

# VIF

olsrr::ols_vif_tol(model_2)

# 회귀진단 그래프들

layout(matrix(1:4,nrow=2,byrow=T))

plot(model_2)

layout(1)

# Cook의 거리 - 1을 넘으면 확인을 해야한다 

olsrr::ols_plot_cooksd_chart(model_2) 

#동의는 할 수 있으나(적절하지만) 최고의 모형이라는데는 동의 못함

### (c) 지방세 X1가 단독으로 판매가격 Y을 설명하는데 적절하다는 의견에 동의?

model_3<-lm(Y~X1,data=data_use)

summary(model_3) #adj-rsquared는 변수를 선택할때 사용 

# 회귀진단 그래프들

layout(matrix(1:4,nrow=2,byrow=T))

plot(model_3)

layout(1)

# Cook의 거리 - 1을 넘으면 확인을 해야한다 

olsrr::ols_plot_cooksd_chart(model_3) 

### 적절한 모형 제시

data_use_2<-data_use[-6]

model_4<-lm(Y~.,data_use_2)

summary(model_4)

# VIF

olsrr::ols_vif_tol(model_4)

# 회귀진단 그래프들

layout(matrix(1:4,nrow=2,byrow=T))

plot(model_4)

layout(1)

# Cook의 거리 - 1을 넘으면 확인을 해야한다 

olsrr::ols_plot_cooksd_chart(model_4) 

### 단계적 선택법 - AIC

model_5<-step(model_4)

summary(model_5)

# 회귀진단 그래프들

layout(matrix(1:4,nrow=2,byrow=T))

plot(model_5)

layout(1)

# Cook의 거리 - 0.5보다 작으면 괜찮음 / 1보다 큰 값을 고려 / 전체적으로 봤을때 튀는 값이 있는 경우 

olsrr::ols_plot_cooksd_chart(model_5) 

### 17번 제거

data_use_3<-model_5$model[-17,]

model_6<-lm(Y~.,data_use_3)

summary(model_6)

# VIF

olsrr::ols_vif_tol(model_6)

# 회귀진단 그래프들

layout(matrix(1:4,nrow=2,byrow=T))

plot(model_6)

layout(1)

# Cook의 거리 - 1을 넘으면 확인을 해야한다 

olsrr::ols_plot_cooksd_chart(model_6)
```
