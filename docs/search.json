[
  {
    "objectID": "Spatial_Information_Analysis.html",
    "href": "Spatial_Information_Analysis.html",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "Geocomputation with R\n\n\n\n\n\n\n\n\n공간정보란 ? 사람들이 생활하고 있는 공간 상에서 사건이나 사물에 대한 위치를 나타내는 정보\n\n위치를 나타내는 정보는 (1) 위치를 표현하는 정보 (2) 해당 위치에 나타나는 특성에 대한 정보\n\n위치를 표현하는 정보 : 공간 상에서 사건이나 사물의 위치가 어디에 있는지를 나타내는 정보\n\nex) 주소, 위경도, x,y 좌표 등\n\n해당 위치에 나타나는 특성에 대한 정보 : 특정 위치에 있는 사건이나 사물을 설명하는 정보\n\nex) 학교, 회사, 학생 수 , 교사 수, 사고 건 수, 사고 유형 등\n\n\n\n지리정보 시스템(Geographic Information System) : 공간정보데이터를 처리, 가공하여 새로운 정보를 도출하는 일련의 과정 또는 기법\n\nex) 교통사고 데이터 분석 (TIMS)\n\n공간정보를 이용하여 GIS 분석을 수행하기 위한 소프트웨어\n\n전용 소프트웨어\n\nArcGIS : 전문적인 공간정보의 처리와 분석 가능, 고가(유료)\nQGIS : 오픈소스 GIS 소프트웨어, 최근 많은 분야에서 GIS 소프트웨어로 활용\n\n오픈소스 소프트웨어\n\nR 소프트웨어 : 오픈소스 기반의 통계 프로그램, 공간정보의 처리와 븐석에도 강력한 기능\nPython 소프트웨어 : 배우기 쉽고, 강력한 프로그래밍 언어, 공간정보를 다루는데 유용한 라이브러리가 개발\n\n\n\n\n\n\n\n위치정보와 속성정보로 구분\n\n위치정보\n\n좌표체계를 이용한 위치정보\n\n지리좌표계에서 이용하는 경도와 위도로 표현 ex) 경위도좌표\n수학적으로 X좌표와 Y좌표로 위치 정보를 표현 ex) 평면직각좌표(지도좌표)\n\n공간정보 데이터의 위치정보 표현 방식\n\n벡터 (점, 선, 면)\n래스터 (일정한 격자 또는 화소)\n\n\n속성정보\n\n주어진 위치에 있는 사건이나 사물에 대한 자료\n\n\n\n\n\n\n\n지리좌표체계 : 경도와 위도로 위치를 표현하는 지리좌표체계\n투영좌표체계 : 지도투영법을 적용하여 둥근 지구를 평면으로 변환한 후, 직각좌표체계를 이용하여 x좌표와 y좌표의 직각좌표체계로 위치를 표현\n\n원통도법, 원추도법, 평면도법이 있음.\nUTM 좌표체계, TM 좌표계, UTM-K 좌표계\n우리나라는 ITRF2000 지구중심좌표계를 따르고 타원체로는 GRS80 타원체를 적용\n\n\n\n\n\n\nshapefile\n\n.shp : 공간정보(점, 선, 다각형)\n.shx : geometry와 속성 정보 연결\n.dbf : 속성정보\n.drj : 좌표계 정보 저장\n.sbn : 위치 정보 저장\n\ngeojson : json 또는 xml 파일 포맷 필요요\n\n\n\n\n\n\n\n패키지\n\nsf : 지리 공간 벡터 데이터(vector data) 분석을 위한 패키지\nraster : 지리 공간 레스터 데이터(raster data)를 처리 및 분석하는데 사용\nspData : 37개의 지리 공간 데이터셋이 내장\nspDataLarge : 지리공간 데이터 샘플을 내장\n\nvignetee(package = \" \") : 설치된 모든 패키지에 대한 이용가능한 모든 목록을 출력\nst_as_sf() : st 데이터를 sf로 변환하는 함수\nst_centroid : 폴리곤의 중심점을 계산하는 함수\nplot 함수 위에 다른 지도 층을 추가 : plot() 함수 안에 add = TRUE 사용\n\n\n\n\nst_point() : A point\nst_linestring() : A linestring\nst_polygon() : A polygon\nst_multipoint() : A multipoint\nst_multilinestring() : A multilinestring\nst_multipolygon() : A multipolygon\nst_geometrycollection() : A geometry collection\n\n\n\n\n\nst_sfc() : 두 개의 지리특성(feature)을 하나의 칼럼 객체로 합치는 함수\nst_geometry_type() : 기하유형을 확인\nst_crs() : 특정 CRS를 지정\n\n특정 CRS를 지정하기 위해 epsg(SRID) 또는 proj4string 속성을 사용\n\nepsg 코드\n\n장점 : 짧아서 기억하기 쉬움\nsfc 객체 내의 모든 geometries는 동일한 CRS를 가져야 함.\nEPSG : 4326 : GPS가 사용하는 좌표계\n\nproj4string 정의\n\n장점 : 투사 유형이나 datum, 타원체 등의 다른 모수들을 구체화할 수 있는 유연성이 있음\n단점 : 사용자가 구체화를 해야하므로 길고 복잡하며 기억하기 어려움\n\nst_sf() : sfc와 class sf의 객체들을 하나로 통합\n\n\nlibrary(raster)\nlibrary(rgdal)\n\nraster_filepath &lt;- system.file(\"raster/srtm.tif\", package = \"spDataLarge\")\nnew_raster &lt;- raster(raster_filepath)\nnew_raster\n# class      : RasterLayer\n# dimensions : 457, 465, 212505  (nrow, ncol, ncell)\n# resolution : 0.0008333333, 0.0008333333  (x, y)\n# extent     : -113.2396, -112.8521, 37.13208, 37.51292  (xmin, xmax, ymin, ymax)\n# crs        : +proj=longlat +datum=WGS84 +no_defs\n# source     : srtm.tif\n# names      : srtm\n# values     : 1024, 2892  (min, max)\n\n\ndim() : 행, 열, 층의 수\nncell() : 셀의 수\nres() : 해상도\nextent() : 경계값\ncrs() : 좌표계\ninMemory() : 래스터 데이터가 메모리에 저장되어 있는지(논리값 출력)\n\n\n\n\n\nRasterLayer class\nRasterBrick Class\nRasterStack class\n\n\nRasterLayer : 한 개의 층으로 구성되어 있는 래스터\nRasterBrick : 여러개의 층으로 구성되어 있는 래스터\n\n단일 다중 스펙트럼 위성 파일, 메모리의 단일 다층 객체의 형태\nbrick() 함수를 사용하여 다층 래스터 파일을 로드\n\nRasterStack : 여러개의 층으로 구성되어 있는 래스터\nnlayers() : 래스터 데이터의 층의 수\n\n\n\n\nRasterBrick : 동일한 복수 개의 RasterLayer 층으로 구성\nRasterStack : 여러 개의 RasterLayer과 RasterBrick 객체가 혼합\n\n\n\n\n\nRasterBrick : 하나의 다층 래스터 파일이나 객체를 처리\nRasterStack : 여러 개의 래스터 파일들이나 여러 종류의 래스터 클래스를 한꺼번에 연걸해서 연산하고 처리\n\n\n\n\n\n\n지리 좌표계\n\n위도와 경도를 이용해 지구 표면의 위치를 정의\n미터가 아니라, 각도로 거리 측정\n타원 표면, 구면 표면\nWGS84\n\n투영(투사) 좌표계\n\n암묵적으로 “평평한 표면” 위의 데카르트 좌표 기반 -&gt; 왜곡 발생\n원점, x축, y축\n미터와 같은 선형 측정 단위\n평면, 원뿔, 원통의 3가지 투영 유형\n\nst_set_crs() : 좌표계가 비어있거나 잘못 입력되어 있는 경우에 좌표계를 설정\nst_transform() : 투영 데이터 변환\nst_area() : 벡터 데이터의 면적 계산 -&gt; [m^2] 단위가 같이 반환\n좌표계 설정할 때,\n\n벡터 데이터 : epsg코드나 proj4string정의 모두 사용 가능\n래스터 데이터 : proj4string 정의만 사용\n\n\n\n\n\n\n\n\n\n\nsf 객체에서 속성 정보만 가져오기 : st_drop_geometry()\n\n\nBase R 구문으로 벡터 데이터 속성 정보의 행과 열 가져오기\n\n\ndplyr로 벡터 데이터 속성 정보의 행과 열 가져오기\n\n\n한 개 컬럼만 가져온 결과를 벡터로 반환하기\n\n\n\n\n\n지리공간 sf 객체는 항상 점, 선, 면 등의 지리기하 데이터를 리스트로 가지고 있는 geometry 칼럼이 항상 따라다님\nsf 객체로부터 이 geometry 칼럼을 제거하고 나머지 속성 정보만으로 Dataframe을 만들고 싶다면 sf패키지의 st_drop_geometry()를 사용\ngeometry 칼럼의 경우 지리기하 점, 선, 면 등의 리스트 정보를 가지고 있어 메모리 점유가 크기때문에, 사용할 필요가 없다면 geometry 칼럼을 제거하고 속성 정보만으로 Dataframe으로 만들어서 분석을 진행하는게 좋음\n\n\n\n\n\nR Dataframe에서 i행과 j열을 가져올 때 : df[i, j], subset(), $을 사용\n\n\ni행과 j열 위치를 지정 ex) world[1:6, ]\n\n\nj행의 이름을 이용 ex) world[, c(\"name_long\", \"lifeExp\")]\n\n\n논리 벡터를 사용해서 i행의 부분집합 ex) sel_area &lt;- world$area_km2 &lt; 10000\n\n\n\n\n\n\n\ndplyr 패키지에서는 체인(%&gt;%)으로 파이프 연산자를 사용하여 가독성이 좋고, 속도가 빠름\n\n\nselect() 함수를 사용하여 특정 열 선택\n\n\nselect(sf, name)\nselect(sf, name1:name2)\nselect(sf, position) ex) select(world, 2, 7)\nselect(sf, -name)\nselect(sf, name_new = name_old) : 열 선택하여 이름 변경\nselect(sf, contain(string)) : 특정 문자열을 포함한 칼럼을 선택\n\ncontain(), starts_with(), ends_with(), matches(), num_range()\n\n\n\nfilter() 함수를 사용하여 조건을 만족하는 특정 행 추출\n\n\nsubset() 함수와 동일한 기능\n\n\naggregate() 함수를 사용하여 지리 벡터 데이터의 속성 정보를 그룹별로 집계\n\n\naggregate(x ~ group, FUN, data, ...)\ndata.frame을 반환하며, 집계된 결과에 지리 기하(geometry) 정보는 없음\nworld[‘pop’]은 “sf” 객체이기 때문에 집계 결과가 “sf” 객체로 반환\nworld$pop은 숫자형 벡터이므로 aggregate() 함수를 적용하면 집계 결과가 “data.frame”으로 반환\n\n\nsummarize(), group_by() 함수를 이용한 지리벡터 데이터의 속성 정보를 그룹별로 집계\n\n\ngroup_by() : 기준이 되는 그룹을 지정\nsummarize() : 다양한 집계 함수를 사용\n\nsum(), n() : 합계와 개수 집계\ntop_n() : 상위 n개 추출\narrange() : 오름차순 정렬, desc()를 사용하면 내림차순 정렬\nst_drop_geometry() : geometry 열 제거\n\n\n\n\n\n\n\n\nR의 sf클래스 객체인 지리공간 벡터 데이터를 dplyr의 함수를 사용해서 두 테이블을 join하면 속성과 함께 지리공간 geometry 칼럼과 정보도 join된 후의 테이블에 자동으로 그대로 따라감\n\n\n## 두 데이터 셋에 같은 이름을 가지는 변수가 없는 경우\n\n```         \na)  하나의 key variable의 이름을 바꿔서 통일시켜줌\n```\n\n-   \n\n    b)  `by`를 사용하여 결합변수를 지정\n\n\n\n# coffee_data의 name_long변수 이름을 nm으로 변경\ncoffee_renamed &lt;- rename(coffee_data, nm = name_long)\n# by 사용하여 결합 변수를 지정하여 다른이름변수를 기준으로 조인하기\nworld_coffee1 &lt;- left_join(world, coffee_renamed, by = c(name_long = \"nm\"))\n\n\ninner_join() 함수를 사용하면 겹치는 행만 추출\n\nsetdiff() : 일치하지 않는 행 추출\ngrepl() : 텍스트 찾는 함수 (논리값으로 출력)\ngrep() : 텍스트 찾는 함수 (행 번호 출력)\n\n\n\n\n\n\ndplyr로 지리 벡터 데이터에 새로운 속성 만들기\n\nmutate() : 기존 데이터 셋에 새로 만든 변수(열) 추가\ntransmute() : 기존의 열은 모두 제거하고 새로 만든 열과 지리기하 geometry열만을 반환\n\ntidyr로 지리 벡터 데이터의 기존 속성을 합치거나 분리하기\n\nunite(data, 병합 열, sep = \"_\", remove = TRUE) : 기존 속성 열을 합쳐서 새로운 속성 열을 만듦\n\nremove = TRUE를 설정해주면 기존의 합치려는 두 개의 열은 제거되고, 새로 만들어진 열만 남음\n\nseparate() : 기존에 존재하는 열을 구분자를 기준으로 두 개의 열로 분리\n\n\n\nworld_unite &lt;- world %&gt;%\n  unite(\"con_reg\", continent:region_un, sep = \":\", remove = TRUE)\nnames(world_unite)\n# \"iso_a2\"    \"name_long\" \"con_reg\"   \"subregion\" \"type\"\n# \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\" \"geom\"\n\nworld_separate &lt;- world_unite %&gt;%\n  separate(con_reg, c(\"continent\", \"region_un\"), sep = \":\")\nnames(world_separate)\n# \"iso_a2\"    \"name_long\" \"continent\" \"region_un\" \"subregion\" \"type\"\n# \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\" \"geom\" \n\ndplyr로 지리 벡터 데이터의 속성 이름 바꾸기\n\nrename(data, new_name = old_name) : 특정 속성 변수 이름 변경\nsetNames(object = nm, nm) : 여러개의 속성 칼럼을 한꺼번에 변경 또는 부여\n\n\nworld %&gt;% rename(name = name_long)\n\nnew_names &lt;- c(\"i\", \"n\", \"c\", \"r\", \"s\", \"t\", \"a\", \"p\", \"l\", \"gP\", \"geom\")\nworld %&gt;% setNames(new_names)\n\n\n\n\n\n\n\n래스터 객체의 데이터 속성은 숫자형(numeric), 정수형(integer), 논리형(logical), 요인형(factor) 데이터를 지원하며, 문자형(character)은 지원하지 않음\n\n1.  **문자형을 요인형으로 변환**(또는 논리형으로 변환) -\\&gt; `factor()` 함수 사용\n\n\n요인형 값을 속성 값으로 하여 래스터 객체를 만듦\n\n\n래스터 객체의 모든 값을 추출하거나 전체 행을 추출 : values(), getValues()\n\n\n\n\n\n\n\n\n\n\nst_intersects() : 공간 부분집합 추출(교집합)\n\n\n\n\n\n\n\n\n\n\nst_intersects() : 공간적으로 관련이 있는 객체를 출력\nst_disjoint() : 공간적으로 관련되지 않은 객체만 반환\nst_within() : 공간적으로 완전히 객체 내부에 있는 객체들만 출력\nst_touches() : 공간적으로 테두리에 있는 객체들만 출력\nst_is_within_distance() : 공간적으로 주어진 거리보다 가까운 객체들을 반환\nsparse = FALSE 매개변수를 설정하면 논리값으로 출력\n\n\nst_intersects(p, a)\n#&gt; Sparse geometry binary predicate list of length 4, where the predicate\n#&gt; was `intersects'\n#&gt;  1: 1\n#&gt;  2: 1\n#&gt;  3: (empty)\n#&gt;  4: (empty)\n\nst_intersects(p, a, sparse = FALSE)\n#&gt;       [,1]\n#&gt; [1,]  TRUE\n#&gt; [2,]  TRUE\n#&gt; [3,] FALSE\n#&gt; [4,] FALSE\n\nst_disjoint(p, a, sparse = FALSE)[, 1]\n#&gt; [1] FALSE FALSE  TRUE  TRUE\n\nst_within(p, a, sparse = FALSE )[, 1]\n#&gt; [1]  TRUE FALSE FALSE FALSE\n\nst_touches(p, a, sparse = FALSE)[, 1]\n#&gt; [1] FALSE  TRUE FALSE FALSE\n\nsel &lt;- st_is_within_distance(p, a, dist = 0.9) # can only return a sparse matrix\nlengths(sel) &gt; 0\n#&gt; [1]  TRUE  TRUE FALSE  TRUE\n\n\n\n\n\nst_join() : 공간 결합 함수\n\n\nrandom_joined = st_join(random_points, world[\"name_long\"]) ; random_joined\n#&gt; Simple feature collection with 10 features and 1 field\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -158.1893 ymin: -42.91501 xmax: 165.1157 ymax: 80.5408\n#&gt; Geodetic CRS:  WGS 84\n#&gt; # A tibble: 10 × 2\n#&gt;                 geometry name_long\n#&gt;  *           &lt;POINT [°]&gt; &lt;chr&gt;    \n#&gt;  1 (-58.98475 -21.24278) Paraguay \n#&gt;  2  (-13.05963 25.42744) Morocco  \n#&gt;  3   (-158.1893 80.5408) &lt;NA&gt;     \n#&gt;  4  (-108.9239 27.80098) Mexico   \n#&gt;  5   (-9.246895 49.9822) &lt;NA&gt;     \n#&gt;  6  (-71.62251 20.15883) &lt;NA&gt;     \n#&gt;  7  (38.43318 -42.91501) &lt;NA&gt;     \n#&gt;  8  (-133.1956 6.053818) &lt;NA&gt;     \n#&gt;  9   (165.1157 38.16862) &lt;NA&gt;     \n#&gt; 10   (16.86581 53.86485) Poland\n\n\n\n\n\n\n기호(plotting symbols, characters) : pch\n기호의 크기 : cex\n선 두께 : lwd\n선 유형 : lty\n\n\n\n\nany() : 특정 값이 포함되어 있는지 확인할 때 유용, 여기서 TRUE가 있는지 확인 가능\n\n\nany(st_touches(cycle_hire, cycle_hire_osm, sparse = FALSE))\n#&gt; [1] FALSE\n\n\nlibrary(mapview)\nlibrary(tmap)\ntmap_mode(\"view\")\ntm_basemap(\"Stamen.Terrain\") +\n  tm_shape(cycle_hire) +\n  tm_symbols(col = \"red\", shape = 16, size = 0.5, alpha = .5) +\n  tm_shape(cycle_hire_osm) +\n  tm_symbols(col = \"blue\", shape = 16, size = 0.5, alpha = .5) +\n  tm_tiles(\"Stamen.TonerLabels\")\n\n\n\n\n\n\n\nst_transform() : 투영데이터로 변환을 위한 함수\nst_is_within_distance() : 임계 거리보다 가까운 객체들을 반환\n\n\ncycle_hire_P &lt;- st_transform(cycle_hire, 27700)\ncycle_hire_osm_P &lt;- st_transform(cycle_hire_osm, 27700)\nsel &lt;- st_is_within_distance(cycle_hire_P, cycle_hire_osm_P, dist = 20)\nsummary(lengths(sel) &gt; 0)\n#&gt;    Mode   FALSE    TRUE \n#&gt; logical     304     438\n\n\nst_join()을 사용하여 dist 인수를 추가하여 구할 수도 있음\n\nst_join()을 사용하면 조인된 결과의 행 수가 더 크다.\n이는 cycle_hire_P의 일부 자전거 대여소가 cycle_hire_osm_P와 여러개가 겹치기 때문임\n겹치는 점에 대한 값을 집계하고 평균을 반환하여 문제를 해결 가능\n\n\nz = st_join(cycle_hire_P, cycle_hire_osm_P,\n            join = st_is_within_distance, dist = 20)\nnrow(cycle_hire) ; nrow(z)\n#&gt; [1] 742\n#&gt; [1] 762\n\nz = z %&gt;%\n  group_by(id) %&gt;%\n  summarize(capacity = mean(capacity))\nnrow(z) == nrow(cycle_hire)\n#&gt; [1] TRUE\n\n\n\n\n\n\naggregate()와 group_by() %&gt;% summarize()를 활용하여 그룹별 통계값 계산(평균, 합 등)\n\n\n# aggregate() 사용\nnz_avheight &lt;- aggregate(x = nz_height, by = nz, FUN = mean)\nplot(nz_avheight[2])\n\n\n\n# group_by() %&gt;% summarize() 사용\nnz_avheight2 &lt;- nz %&gt;%\n  st_join(nz_height) %&gt;%\n  group_by(Name) %&gt;%\n  summarize(elevation = mean(elevation, na.rm = TRUE))\nplot(nz_avheight2[2])\n\n\n\n\n\nst_interpolate_aw() : 면적의 크기에 비례하게 계산(면적 가중 공간 보간)\n\n\nsum(incongruent$value)\n#&gt; [1] 45.41184\n\nagg_aw = st_interpolate_aw(incongruent[, \"value\"],\n                           aggregating_zones,\n                           extensive = TRUE)\n#&gt; Warning in st_interpolate_aw.sf(incongruent[, \"value\"], aggregating_zones, :\n#&gt; st_interpolate_aw assumes attributes are constant or uniform over areas of x\nagg_aw$value\n#&gt; [1] 19.61613 25.66872\n\n\n\n\n\n위상 관계는 binary인 반면 거리 관계는 연속적임\nst_distance() : 두 객체 사이의 거리 계산\n\n\nnz_heighest &lt;- nz_height %&gt;% top_n(n = 1, wt = elevation)\ncanterbury_centroid &lt;- st_centroid(canterbury)\n#&gt; Warning: st_centroid assumes attributes are constant over geometries\n\nst_distance(nz_heighest, canterbury_centroid)\n#&gt; Units: [m]\n#&gt;        [,1]\n#&gt; [1,] 115540\n\nco &lt;- filter(nz, grepl(\"Canter|Otag\", Name))\nst_distance(nz_height[1:3, ], co)\n#&gt; Units: [m]\n#&gt;           [,1]     [,2]\n#&gt; [1,] 123537.16 15497.72\n#&gt; [2,]  94282.77     0.00\n#&gt; [3,]  93018.56     0.00\n\nplot(st_geometry(co)[2])\nplot(st_geometry(nz_height)[2:3], add = TRUE)\n\n\n\n\n\n\n\n\n\n\n\ncellFromXY() or raster::extract() : 좌표값을 Cell ID로 변환\n\n\n\n\n\n\n\nid = cellFromXY(elev, xy = matrix(c(0.1, 0.1), ncol = 2))\nelev[id]\n#&gt;   elev\n#&gt; 1   16\nterra::extract(elev, matrix(c(0.1, 0.1), ncol = 2))\n#&gt;   elev\n#&gt; 1   16\n\nclip = rast(xmin = 0.9, xmax = 1.8, ymin = -0.45, ymax = 0.45,\n            res = 0.3, vals = rep(1, 9))\nelev[clip]\n#&gt;   elev\n#&gt; 1   18\n#&gt; 2   24\nterra::extract(elev, ext(clip))\n\n\noperator는 raster의 다양한 inputs을 받고, drop=FALSE로 설정했을 때, raster 객체를 반환\n\n\nelev[1:2]\n#&gt;   elev\n#&gt; 1    1\n#&gt; 2    2\nelev[2, 1:2]\n#&gt;   elev\n#&gt; 1    7\n#&gt; 2    8\nelev[1:2, drop = FALSE] # spatial subsetting with cell IDs\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 1, 2, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 0.5, 0.5  (x, y)\n#&gt; extent      : -1.5, -0.5, 1, 1.5  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : lon/lat WGS 84 (EPSG:4326) \n#&gt; source(s)   : memory\n#&gt; name        : elev \n#&gt; min value   :    1 \n#&gt; max value   :    2\nelev[2, 1:2, drop = FALSE] # spatial subsetting by row,column indices\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 1, 2, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 0.5, 0.5  (x, y)\n#&gt; extent      : -1.5, -0.5, 0.5, 1  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : lon/lat WGS 84 (EPSG:4326) \n#&gt; source(s)   : memory\n#&gt; name        : elev \n#&gt; min value   :    7 \n#&gt; max value   :    8\n\n\n\n\n\nelev + elev # 더하기\nelev^2      # 제곱\nlog(elev)   # 로그\nelev &gt; 5    # 논리\n\n\n\n\n\n\n\ntmap을 plot하기 위해서는 우선 tm_shape()로 지정해야하며, + 연산자로 레이어를 추가해야함\n\nex) tm_polygons(), tm_raster(), tm_borders(), tm_symbols() 등\n\nInteractive maps : tmap_mode()를 사용하여 \"plot\",과 \"view\"모드 사용 가능\nFacet : 하나의 창에 여러 맵을 동시에 그리기\n\nFacet 하는 3가지 방법\n\n여러변수 이름 추가\nby argument of tm_facets로 공간 데이터를 나누기\ntmap_arrange() 사용\n\n\n\ntm_basemap() : 지도를 표현할 수 있는 바탕이 되는 지도\n\n\n\n# 1. 여러 변수 이름 추가\ntmap_mode(\"plot\")\ndata(World)\ntm_shape(World) +\n  tm_polygons(c(\"HPI\", \"economy\")) +\n  tm_facets(sync = TRUE, ncol = 2)\n\n\n\n\n\n# 2. by argument of `tm_facets`로 공간 데이터 나누기\ntmap_mode(\"plot\")\ndata(NLD_muni)\nNLD_muni$perc_men &lt;- NLD_muni$pop_men / NLD_muni$population * 100\ntm_shape(NLD_muni) +\n  tm_polygons(\"perc_men\", palette = \"RdYlBu\") +\n  tm_facets(by = \"province\")\n\n\n\n\n\n# 3. `tmap_arrange` 함수 사용 : 각각 그린다음에 배치\ntmap_mode(\"plot\")\ndata(NLD_muni)\ntm1 &lt;- tm_shape(NLD_muni) + tm_polygons(\"population\", convert2density = TRUE)\ntm2 &lt;- tm_shape(NLD_muni) + tm_bubbles(size = \"population\")\ntmap_arrange(tm1, tm2)\n\n\n\n\n\ntmap_mode(\"view\")\ndata(World, metro, rivers, land)\ntm_basemap(\"Stamen.Watercolor\") +\n  tm_shape(metro) + tm_bubbles(size = \"pop2020\", col = \"red\") +\n  tm_tiles(\"Stamen.TonerLabels\")\n\n\n\n\n\n\n\nOption and styles\n\ntm_layout() : map layout 지정\ntm_options() 내에서 설정\n\ntmap_options_diff() : default tmap options과 차이점 출력\ntmap_options_reset() : default tmap options으로 설정\n\nreset을 해주지 않으면 option이 계속 설정되어있음\n\n\ntmap_style() : 지도 스타일 설정\n\n\ntmap_mode(\"plot\")\ntm_shape(World) +\n  tm_polygons(\"HPI\") +\n  tm_layout(bg.color = \"skyblue\", inner.margins = c(0, .02, .02, .02))\n\n\n\n\n\ntmap_options(bg.color = \"black\", legend.text.color = \"white\")\ntm_shape(World) + tm_polygons(\"HPI\", legend.title = \"Happy Planet Index\")\n\n\n\n\n\ntmap_style(\"classic\")\n## tmap style set to \"classic\"\n## other available styles are: \"white\", \"gray\", \"natural\", \"cobalt\",\n## \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"watercolor\"\n\ntm_shape(World) +\n  tm_polygons(\"HPI\", legend.title = \"Happy Planet Index\")\n\n\n\n\nExporting maps\n\n\ntm &lt;- tm_shape(World) +\n  tm_polygons(\"HPI\", legend.title = \"Happy Planet Index\")\n\n## save an image (\"plot\" mode)\ntmap_save(tm, filename = \"./Spatial_Information_Analysis/world_map.png\")\n\n## save as stand-alone HTML file (\"view\" mode)\ntmap_save(tm, filename = \"./Spatial_Information_Analysis/world_map.html\")\n\n\nQuick thematic map\n\n\nqtm(World, fill = \"HPI\", fill.pallete = \"RdYlGn\")\n\n\n\n\n\n\n\n\n\n\n\n\n단순화는 일반적으로 더 작은 축척 지도에서 사용하기 위한 벡터 객체(선, 다각형)의 일반화를 위한 프로세스\nst_simplify() : 정점을 제거하여 선을 단순화시킴\n\ndTolerance : 단위가 m이며 커질수록 더 단순화\n\n\nseine_simp &lt;- st_simplify(seine, dTolerance = 2000) # 2000m\nplot(seine)\nplot(seine_simp)\nobject.size(seine) ; object.size(seine_simp)\n#&gt; 18096 bytes  9112 bytes\n\n\n\n\n\n\n단순화는 다각형에도 적용 가능\nst_simplify()를 사용하였을 때, 영역이 겹치는 경우도 발생\nrmapshaper 패키지의 ms_simplify() 함수를 사용\nkeep_shapes = TRUE : 개체 수는 그대로 유지\n\n\nus_states\nus_states2163 &lt;- st_transform(us_states, 2163)\nus_states2163\n\nus_states_simp1 &lt;- st_simplify(us_states2163, dTolerance = 100000)\nplot(us_states[1])\nplot(us_states_simp1[1])\n\nus_states2163$AREA &lt;- as.numeric(us_states2163$AREA)\n\nlibrary(rmapshaper)\nus_states_simp2 &lt;- rmapshaper::ms_simplify(us_states2163, keep = 0.01,\n                                           keep_shapes = FALSE)\nplot(us_states_simp2[1])\n\n\n\n\n\n\n\n\n\n\n가장 일반적으로 사용되는 중심 연산은 지리적 중심 : 공간객체의 질량 중심\nst_centroid() : 지리적 중심을 생성하지만, 때때로 지리적 중심이 상위 개체의 경계를 벗어나는 경우가 발생\nst_point_on_surface() : 상위 개체 위에 중심이 생성\n\n\nnz_centroid &lt;- st_centroid(nz)\nseine_centroid &lt;- st_centroid(seine)\n\nnz_pos &lt;- st_point_on_surface(nz)\nseine_pos &lt;- st_point_on_surface(seine)\n\nplot(st_geometry(nz), main = \"nz\")\nplot(nz_centroid ,add=T, col=\"black\")\nplot(nz_pos ,add=T, col=\"red\")\n\nplot(st_geometry(seine), main = \"seine\")\nplot(seine_centroid ,add=T, col=\"black\")\nplot(seine_pos ,add=T, col=\"red\")\n\n\n\n\n\n\n\n\n\n\n버퍼 : 기하학적 특징의 주어진 거리 내 영역을 나타내는 다각형\n지리데이터 분석에 자주 활용됨\nst_buffer() : 버퍼 생성 함수, 최소 두 개의 인수가 필요함\n\n\nseine_buff_5km &lt;- st_buffer(seine, joinStyle = \"ROUND\", dist = 5000)\nseine_buff_20km &lt;- st_buffer(seine, dist = 20000)\n\nplot(seine,col=\"black\", reset = FALSE)\nplot(seine_buff_5km, col=adjustcolor(1:3, alpha = 0.2), add=T)\n\nplot(seine,col=\"black\", reset = FALSE)\ncol1 &lt;- adjustcolor(\"red\", alpha=0.2)\ncol2 &lt;- adjustcolor(\"blue\", alpha=0.2)\ncol3 &lt;- adjustcolor(\"green\", alpha=0.2)\nplot(seine_buff_20km, col=c(col1,col2,col3), add=T)\n\n\n\n\n\n\n\n\n\n\n왜곡되거나 잘못 투영된 지도를 기반으로 생성된 geometry를 재투영하거나 개선할 때 많은 Affine 변환이 적용\n이동 : 맵 단위로 모든 포인트가 동일한 거리만큼 이동\n\n\nnz_sfc &lt;- st_geometry(nz)\nnz_shift &lt;- nz_sfc + c(0, 100000)\nplot(nz_sfc)\nplot(nz_shift,add=T, col=\"Red\")\n\n\n\n\n\n배율 조정 : 개체를 요소만큼 확대하거나 축소\n\n모든 기하 도형의 토폴로지 관계를 그대로 유지하면서 원점 좌표와 관련된 모든 좌표값을 늘리거나 줄일 수 있음\n중심점을 기준으로 기하 도형의 차이 만큼을 늘리고 0.5배 줄인 다음 다시 중심점을 더해줌\n\n\nnz_centroid_sfc &lt;- st_centroid(nz_sfc)\nnz_scale &lt;- (nz_sfc - nz_centroid_sfc) * 0.5 + nz_centroid_sfc\n\nplot(nz_sfc)\nplot(nz_scale, add=T, col=\"Red\")\n\n\n\n\n회전 : 2차원 좌표의 회전하기 위한 회전변환행렬\n\n\nmatrix(c(cos(30), sin(30), -sin(30), cos(30)), nrow = 2, ncol = 2)\n#&gt;            [,1]      [,2]\n#&gt; [1,]  0.1542514 0.9880316\n#&gt; [2,] -0.9880316 0.1542514\n\nrotation &lt;- function(a){\n  r = a * pi / 180 #degrees to radians\n  matrix(c(cos(r), sin(r), -sin(r), cos(r)), nrow = 2, ncol = 2)\n}\nnz_rotate &lt;- (nz_sfc - nz_centroid_sfc) * rotation(30) + nz_centroid_sfc\n\nplot(nz_sfc)\nplot(nz_rotate, add=T, col=\"red\")\n\n\n\n\n\n\n\n\n공간 클리핑은 영향을 받는 일부 형상의 지오메트리 열의 변경을 수반하는 공간 부분 집합의 한 형태\n\n\nb &lt;- st_sfc(st_point(c(0, 1)), st_point(c(1, 1))) # create 2 points\nb &lt;- st_buffer(b, dist = 1) # convert points to circles\nplot(b, border = \"grey\")\ntext(x = c(-0.5, 1.5), y = 1, labels = c(\"x\", \"y\"), cex = 3) # add text\n\n\n\n\n\nst_intersection() : X∩Y (x와 y의 교집합)\nst_difference() : X-Y (x와 y의 차집합)\nst_union() : X∪Y (x와 y의 합집합)\nst_sym_difference() : (X∩Y)^c (드모르간의 법칙)\n\n\npar(mfrow = c(2,2))\n\nx &lt;- b[1] ; y &lt;- b[2]\n\n# X ∩ Y\nx_and_y &lt;- st_intersection(x, y)\nplot(b, border = \"grey\", main = \"X ∩ Y\")\nplot(x_and_y, col = \"lightgrey\", border = \"grey\", add = TRUE)\ntext(x = c(-0.5, 1.5), y = 1, labels = c(\"x\", \"y\"), cex = 3)\n\n# X - Y\nx_dif_y &lt;- st_difference(x,y)\nplot(b, border = \"grey\", main = \"X - Y\")\nplot(x_dif_y, col = \"lightgrey\", border = \"grey\", add = TRUE)\ntext(x = c(-0.5, 1.5), y = 1, labels = c(\"x\", \"y\"), cex = 3)\n\n# X U Y\nx_union_y &lt;- st_union(x,y)\nplot(b, border = \"grey\", main = \"X U Y\")\nplot(x_union_y, col = \"lightgrey\", border = \"grey\", add = TRUE)\ntext(x = c(-0.5, 1.5), y = 1, labels = c(\"x\", \"y\"), cex = 3)\n\n# (X ∩ Y)^c\nx_sdif_y &lt;- st_sym_difference(x,y)\nplot(b, border = \"grey\", main = \"(X ∩ Y)^c\")\nplot(x_sdif_y, col = \"lightgrey\", border = \"grey\", add = TRUE)\ntext(x = c(-0.5, 1.5), y = 1, labels = c(\"x\", \"y\"), cex = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n클리핑 오브젝트는 지오메트리를 변경할 수 있지만 오브젝트의 부분 집합을 지정할 수도 있으며 클리핑/하위 설정 오브젝트와 교차하는 피쳐만 반환할 수도 있음\nst_sample() : x와 y의 범위 내에서 점들의 간단한 무작위 분포를 생성\n\n\nbb = st_bbox(st_union(x, y))\nbox = st_as_sfc(bb)\nset.seed(2017)\n\np = st_sample(x = box, size = 10)\nx_and_y = st_intersection(x, y)\n\nplot(b, border = \"grey\")\nplot(p, add=T)\n\n\n\n\n\nX와 Y 둘 다와 교차하는 점만을 반환하는 방법\n\n\n\n## 1번째방법\np_xy1 &lt;- p[x_and_y]\nplot(p_xy1, add=T, col=\"red\")\n\n## 2번째방법\np_xy2 &lt;- st_intersection(p, x_and_y)\nplot(p_xy2, add=T, col=\"blue\")\n\n## 3번째방법\nsel_p_xy &lt;- st_intersects(p, x, sparse = FALSE)[, 1] &\n  st_intersects(p, y, sparse = FALSE)[, 1]\np_xy3 &lt;- p[sel_p_xy]\nplot(p_xy3, add=T, col=\"green\")\n\n\n\n\n\n\n\n\n\n\n미국의 49개 주의 정보를 4개 지역으로 재구분\n\n\nplot(us_states[6])\n\n\n\n## 1. aggregate함수\nregions &lt;- aggregate(x = us_states[, \"total_pop_15\"], by = list(us_states$REGION),\n                     FUN = sum, na.rm = TRUE)\nplot(regions[2])\n\n\n\n## 2. group_by, summarize함수\nregions2 &lt;- us_states %&gt;% group_by(REGION) %&gt;%\n  summarize(pop = sum(total_pop_15, na.rm = TRUE))\n\nplot(regions2[2])\n\n\n\n\n\n위에서 aggregate()와 summarize()가 모두 지오메트리를 결합하고 st_union()을 사용하면 지오메트리만을 분해\n\n\nus_west &lt;- us_states[us_states$REGION == \"West\", ]\nplot(us_west[6])\n\n\n\nus_west_union &lt;- st_union(us_west)\nplot(us_west_union)\n\n\n\ntexas &lt;- us_states[us_states$NAME == \"Texas\", ]\ntexas_union &lt;- st_union(us_west_union, texas)\nplot(texas_union)\n\n\n\n\n\n\n\n\nst_cast() : 지오메트리 유형을 변환\n\n\nmultipoint &lt;- st_multipoint(matrix(c(1, 3, 5, 1, 3, 1), ncol = 2))\nlinestring &lt;- st_cast(multipoint, \"LINESTRING\")\npolyg &lt;- st_cast(multipoint, \"POLYGON\")\n\nplot(multipoint)\nplot(linestring)\nplot(polyg)\n\nst_length(linestring) # 길이 계산\n# [1] 5.656854\nst_area(polyg) # 면적 계산\n# [1] 4\n\n\n\n\n\n\n\nmultilinestring : 여러 개의 linestring을 하나의 묶음으로 처리\n\n\n\n\n\n\n\nmultilinestring은 각 선 세그먼트에 이름을 추가하거나 단일 선 길이를 계산할 수 없는 등 수행할 수 있는 작업 수가 제한됨\nst_cast() 함수를 사용하여 하나의 multilinestring을 세 개의 linestring로 분리\n\n\nlinestring_sf2 = st_cast(multilinestring_sf, \"LINESTRING\")\nlinestring_sf2\n#&gt; Simple feature collection with 3 features and 0 fields\n#&gt; Geometry type: LINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 1 ymin: 1 xmax: 4 ymax: 5\n#&gt; CRS:           NA\n#&gt;                    geom\n#&gt; 1 LINESTRING (1 5, 4 3)\n#&gt; 2 LINESTRING (4 4, 4 1)\n#&gt; 3 LINESTRING (2 2, 4 2)\n\n\nname과 length 추가\n\n\nlinestring_sf2$name &lt;- c(\"Riddle Rd\", \"Marshall Ave\", \"Foulke St\")\nlinestring_sf2$length &lt;- st_length(linestring_sf2)\nlinestring_sf2\n#&gt; Simple feature collection with 3 features and 2 fields\n#&gt; Geometry type: LINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 1 ymin: 1 xmax: 4 ymax: 5\n#&gt; CRS:           NA\n#&gt;                    geom         name   length\n#&gt; 1 LINESTRING (1 5, 4 3)    Riddle Rd 3.605551\n#&gt; 2 LINESTRING (4 4, 4 1) Marshall Ave 3.000000\n#&gt; 3 LINESTRING (2 2, 4 2)    Foulke St 2.000000\nplot(linestring_sf2[2])\n\n\n\n\n\n\n\n\n\n\n\n다른 공간 객체에 의해 중첩된 래스터에서 값을 추출하는 방법\n공간 출력을 검색하기 위해 거의 동일한 부분 집합 구문(많이 겹치는 부분)을 사용\ndrop = FALSE를 설정하여 행렬 구조를 유지\ncell 중간점이 clip과 겹치는 셀을 포함하는 래스터 개체를 반환\n\n\nelev &lt;- rast(system.file(\"raster/elev.tif\", package = \"spData\"))\nclip &lt;- rast(xmin = 0.9, xmax = 1.8, ymin = -0.45, ymax = 0.45,\n             resolution = 0.3, vals = rep(1, 9))\nplot(elev)\nplot(clip, add=T)\n\n\n\nelve_clip &lt;- elev[clip, drop = FALSE]\nplot(elve_clip)\n\n\n\nelev_raster &lt;- rast(system.file(\"raster/elev.tif\", package = \"spData\"))\nrcc &lt;- vect(xyFromCell(elev_raster, cell = 1:ncell(elev_raster))) # 셀의 중앙점 표시\nxyFromCell(elev_raster,1) # 1번 셀의 중앙점 좌표\n#&gt;          x    y\n#&gt; [1,] -1.25 1.25\nplot(elev)\nplot(rcc,add=T)\nplot(clip, add=T)\n\n\n\n\n\n\n\n\n다른 투사 및 해상도를 가진 두 이미지를 병합하려할 때 사용\nextend() : 래스터 범위 확장\n\n새로 추가된 행과 열은 값 매개변수의 기본값(예 : NA)를 가짐\n\norigin() : 래스터의 원점 좌표를 반환\n\n래스터의 원점은 좌표(0,0)에 가장 가까운 셀 모서리\n\n\n\nelev &lt;- rast(system.file(\"raster/elev.tif\", package = \"spData\"))\nelev_2 &lt;- extend(elev, c(1,2), snap=\"near\") # 아래/위 1행, 좌/우 2열 확장\nplot(elev)\n\n\n\nplot(elev_2, colNA=\"gray\")\n\nelev_3 &lt;- elev + elev_2\n#&gt; Error: [+] extents do not match\n\nelev_4 &lt;- extend(elev, elev_2)\nplot(elev_4, colNA=\"gray\")\n\norigin(elev_4)\n#&gt; [1] 0 0\n\norigin(elev_4) &lt;- c(0.25, 0.25)\nplot(elev_4, colNA=\"black\", add=T)\n\n\n\n\n\n\n\n\n\n래스터 데이터 셋은 해상도가 서로 다를 수 있음\n해상도를 match 시키기 위해 하나의 래스터 해상도를 감소(aggregate())시키거나 증가(disagg()) 시켜야 함\n\n\n# devtools::install_github(\"geocompr/geocompkg\")\ndem &lt;- rast(system.file(\"raster/dem.tif\", package = \"spDataLarge\"))\ndem_agg &lt;- aggregate(dem, fact = 5, fun = mean)\ndem_disagg &lt;- disagg(dem_agg, fact = 5, method = \"bilinear\")\nplot(dem)\n\n\n\nplot(dem_agg)\n\n\n\nplot(dem_disagg)\n\n\n\nidentical(dem, dem_disagg)\n#&gt; [1] FALSE\n\n\n새롭게 만들어지는 cell의 값을 만드는 두가지 방법\n\nDefault method(method = “near”) : 입력 셀의 값을 모든 출력 셀에 제공\nbilinear method : 입력 이미지의 가장 가까운 4개의 픽셀 중심을 사용하여 거리에 의해 가중된 평균을 계산\n\n\n\n\n\n\nResampling : 원래 그리드에서 다른 그리드로 래스터 값을 전송하는 프로세스\n이 프로세스는 원래 래스터의 값을 가지고, 사용자 지정 해상도와 원점을 가지고 대상 래스터의 새 값을 다시 계산함\n해상도/원점이 다른 래스터의 값을 재계산(추정)하는 방법\n\nNearest neighbor : 원래 래스터의 가장 가까운 셀 값을 대상 래스터의 셀에 할당. 속도가 빠르고 일반적으로 범주형 래스터에 적합\nBilinear interpolation(이중선형보간) : 원래 래스터에서 가장 가까운 4개의 셀의 가중 평균을 대상 1개의 셀에 할당. 연속 래스터를 위한 가장 빠른 방법\nCubic interpolation(큐빅 보간) : 본 래스터의 가장 가까운 16개 셀의 값을 사용하여 출력 셀 값을 결정하고 3차 다항식 함수를 적용. 연속 래스터에 사용. 2선형 보간보다 더 매끄러운 표면을 만들지만, 계산적으로 까다로움\nCubic spline interpolation(큐빅 스플라인 보간) : 원래 래스터의 가장 가까운 16개의 셀의 값을 사용하여 출력 셀 값을 결정하지만 큐빅 스플라인(3차 다항식 함수)을 적용\nLanczos windowed sinc resampling(Lanczos 윈도우 재샘플링) : 원래 래스터의 가장 가까운 셀 36개의 값을 사용하여 출력 셀 값을 결정\nsum\nmin, q1, med, q3, max, average, mode, rms\n\nNearest neighbor은 범주형 래스터에 적합한 반면, 모든 방법은 연속형 래스터에 사용\nresample(x, y, method = \"bilinear\", filename = \"\", ...) : 리샘플링 함수\n\n\nlibrary(terra)\n\ntarget_rast &lt;- rast(xmin = 794600, xmax = 798200,\n                    ymin = 8931800, ymax = 8935400,\n                    resolution = 150, crs = \"EPSG:32717\")\ntarget_rast\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 24, 24, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 150, 150  (x, y)\n#&gt; extent      : 794600, 798200, 8931800, 8935400  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : WGS 84 / UTM zone 17S (EPSG:32717)\n\nplot(dem)\n\n\n\nplot(target_rast)\n\n\n\n\n\n\"near\" : 셀에 가장 가까운 픽셀에서 값을 가져옴\n\n\ndem_resampl_1 &lt;- resample(dem, y = target_rast, method = \"near\")\nplot(dem_resampl_1)\n\n\n\n\n\n\"bilinear\" : 네 개의 가장 가까운 셀의 가중 평균\n\n\ndem_resampl_2 &lt;- resample(dem, y = target_rast, method = \"bilinear\")\nplot(dem_resampl_2)\n\n\n\n\n\n\"average\" : 각각의 새로운 셀이 중복되는 모든 입력 셀의 가중 평균\n\n\ndem_resampl_3 &lt;- resample(dem, y = target_rast, method = \"average\")\nplot(dem_resampl_3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n입력 래스터 데이터 세트의 범위가 관심 영역보다 클 경우 래스터 자르기(Cropping) 및 마스킹(Masking)은 입력 데이터의 공간 범위를 통합하는 데 유용함\n두 작업 모두 후속 분석 단계에 대한 객체 메모리 사용 및 관련 계산 리소스를 줄이고 래스터 데이터를 포함하는 매력적인 맵을 만들기 전에 필요한 전처리 단계임\n대상 개체와 자르기 개체는 모두 동일한 투영을 가져야 함\ncrop() : 두 번째 인수에 대한 래스터를 잘라냄\nmask() : 두 번째 인수에 전달된 개체의 경계를 벗어나는 값을 NA로 설정\n\n대부분의 경우 crop()과 mask()를 함께 사용\n\n\nsrtm &lt;- rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\nzion &lt;- read_sf(system.file(\"vector/zion.gpkg\", package = \"spDataLarge\"))\nzion &lt;- st_transform(zion, crs(srtm)) # zion을 srtm 좌표계랑 동일하게\nplot(srtm)\nplot(vect(zion),add=T)\n\n\n\nsrtm_cropped &lt;- crop(srtm, vect(zion))\nplot(srtm_cropped)\n\n\n\nsrtm_masked &lt;- mask(srtm, vect(zion))\nplot(srtm_masked)\n\n\n\nsrtm_cropped &lt;- crop(srtm, vect(zion))\nsrtm_final &lt;- mask(srtm_cropped, vect(zion))\nplot(srtm_final)\n\n\n\n\nupdatevalue = 0 : 외부의 모든 픽셀이 0으로 설정\ninverse = TRUE : 경계 내에 있는 것들이 마스킹\n\n\nsrtm_update0 &lt;- mask(srtm, vect(zion), updatevalue = 0)\nplot(srtm_update0)\n\n\n\nsrtm_inv_masked &lt;- mask(srtm, vect(zion), inverse = TRUE)\nplot(srtm_inv_masked)\n\n\n\n\n\n\n\n## Original / Crop / Mask / Inverse Map\nlibrary(tmap)\nlibrary(rcartocolor)\n\nterrain_colors = carto_pal(7, \"Geyser\")\n\npz1 = tm_shape(srtm) +\n  tm_raster(palette = terrain_colors, legend.show = FALSE, style = \"cont\") +\n  tm_shape(zion) +\n  tm_borders(lwd = 2) +\n  tm_layout(main.title = \"A. Original\", inner.margins = 0)\n\npz2 = tm_shape(srtm_cropped) +\n  tm_raster(palette = terrain_colors, legend.show = FALSE, style = \"cont\") +\n  tm_shape(zion) +\n  tm_borders(lwd = 2) +\n  tm_layout(main.title = \"B. Crop\", inner.margins = 0)\n\npz3 = tm_shape(srtm_masked) +\n  tm_raster(palette = terrain_colors, legend.show = FALSE, style = \"cont\") +\n  tm_shape(zion) +\n  tm_borders(lwd = 2) +\n  tm_layout(main.title = \"C. Mask\", inner.margins = 0)\n\npz4 = tm_shape(srtm_inv_masked) +\n  tm_raster(palette = terrain_colors, legend.show = FALSE, style = \"cont\") +\n  tm_shape(zion) +\n  tm_borders(lwd = 2) +\n  tm_layout(main.title = \"D. Inverse mask\", inner.margins = 0)\n\ntmap_arrange(pz1, pz2, pz3, pz4, ncol = 4, asp = NA)\n\n\n\n\n\n\n\n\n\n특정 위치에 있는 대상 래스터와 관련된 값을 식별하여 반환\n\n\ndata(\"zion_points\", package = \"spDataLarge\")\nelevation &lt;-terra::extract(srtm, vect(zion_points))\nzion_points &lt;- cbind(zion_points, elevation)\nplot(srtm)\nplot(vect(zion),add=T)\nplot(zion_points,col=\"black\", pch = 19, cex = 0.5, add=T)\n#&gt; Warning in plot.sf(zion_points, col = \"black\", pch = 19, cex = 0.5, add = T):\n#&gt; ignoring all but the first attribute\n\n\n\n\n\nst_segmentize() : 제공된 density로 line을 따라 point를 추가\n\ndfMaxLength : 최대 점의 개수\n\nst_cast() : 추가된 point를 “POINT” 형식으로 변환\n\n\nzion_transect &lt;- cbind(c(-113.2, -112.9), c(37.45, 37.2)) %&gt;%\n  st_linestring() %&gt;%\n  st_sfc(crs = crs(srtm)) %&gt;%\n  st_sf()\nzion_transect$id &lt;- 1:nrow(zion_transect)\nzion_transect &lt;- st_segmentize(zion_transect, dfMaxLength = 250)\nzion_transect &lt;- st_cast(zion_transect, \"POINT\")\n#&gt; Warning in st_cast.sf(zion_transect, \"POINT\"): repeating attributes for all\n#&gt; sub-geometries for which they may not be constant\n\n\nzion_transect &lt;- zion_transect %&gt;%\n  group_by(id) %&gt;%\n  mutate(dist = st_distance(geometry)[, 1])\n\nzion_elev &lt;- terra::extract(srtm, vect(zion_transect))\nzion_transect &lt;- cbind(zion_transect, zion_elev)\n\n\n많은 Point들 간의 거리를 산출 : 첫번째 점들과 이후의 각각의 점들 사이의 거리 계산하기\n횡단면의 각 점에 대한 고도값을 추출하고 이 정보를 주요 객체와 결합\n\n\n\n\nlibrary(tmap)\nlibrary(grid)\nlibrary(ggplot2)\nzion_transect_line &lt;- cbind(c(-113.2, -112.9), c(37.45, 37.2)) %&gt;%\n  st_linestring() %&gt;%\n  st_sfc(crs = crs(srtm)) %&gt;%\n  st_sf()\nzion_transect_points &lt;- st_cast(zion_transect, \"POINT\")[c(1, nrow(zion_transect)), ]\nzion_transect_points$name &lt;- c(\"start\", \"end\")\nrast_poly_line &lt;- tm_shape(srtm) +\n  tm_raster(palette = terrain_colors, title = \"Elevation (m)\",\n            legend.show = TRUE, style = \"cont\") +\n  tm_shape(zion) +\n  tm_borders(lwd = 2) +\n  tm_shape(zion_transect_line) +\n  tm_lines(col = \"black\", lwd = 4) +\n  tm_shape(zion_transect_points) +\n  tm_text(\"name\", bg.color = \"white\", bg.alpha = 0.75, auto.placement = TRUE) +\n  tm_layout(legend.frame = TRUE, legend.position = c(\"right\", \"top\"))\nrast_poly_line\n\n\n\nplot_transect &lt;- ggplot(zion_transect, aes(as.numeric(dist), srtm)) +\n  geom_line() +\n  labs(x = \"Distance (m)\", y = \"Elevation (m a.s.l.)\") +\n  theme_bw() +\n  # facet_wrap(~id) +\n  theme(plot.margin = unit(c(5.5, 15.5, 5.5, 5.5), \"pt\"))\nplot_transect\n\n\n\n\n## grid 그리기\ngrid.newpage() #This function erases the current device or moves to a new page.\npushViewport(viewport(layout = grid.layout(2, 2, heights = unit(c(0.25, 5), \"null\"))))\ngrid.text(\"A. Line extraction\", vp = viewport(layout.pos.row = 1, layout.pos.col = 1))\ngrid.text(\"B. Elevation along the line\", vp = viewport(layout.pos.row = 1, layout.pos.col = 2))\nprint(rast_poly_line, vp = viewport(layout.pos.row = 2, layout.pos.col = 1))\nprint(plot_transect, vp = viewport(layout.pos.row = 2, layout.pos.col = 2))\n\n\n\n\n\nzion_srtm_values &lt;- terra::extract(x = srtm, y = vect(zion))\ngroup_by(zion_srtm_values, ID) %&gt;%\n  summarize(across(srtm, list(min = min, mean = mean, max = max)))\n#&gt; # A tibble: 1 × 4\n#&gt;      ID srtm_min srtm_mean srtm_max\n#&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n#&gt; 1     1     1122     1818.     2661\n\n\n단일 영역을 특성화하거나 여러 영역을 비교하기 위해 폴리곤 당 래스터 값에 대한 요약 통계 생성\n\n\n\n\n\n\n벡터 객체를 래스터 객체의 표현으로 변환\n\n\ncycle_hire_osm &lt;- spData::cycle_hire_osm\ncycle_hire_osm_projected &lt;- st_transform(cycle_hire_osm, \"EPSG:27700\")\nraster_template &lt;- rast(ext(cycle_hire_osm_projected), resolution = 1000,\n                        crs = st_crs(cycle_hire_osm_projected)$wkt) # ext : 경계값\nch_raster1 &lt;- rasterize(vect(cycle_hire_osm_projected), raster_template,\n                        field = 1)\nch_raster2 &lt;- rasterize(vect(cycle_hire_osm_projected), raster_template,\n                        fun = \"length\")\nch_raster3 &lt;- rasterize(vect(cycle_hire_osm_projected), raster_template,\n                        field = \"capacity\", fun = sum)\n\n\n\n\n\n\n\n폴리곤 객체를 여러 줄 문자열로 casting한 후 0.5도의 해상도로 탬플릿 래스터 생성\n\ntouches = TRUE : 경계에 해당되는 래스터만 색칠(FALSE이면 경계 내부까지)\n\n\n\ncalifornia &lt;- dplyr::filter(us_states, NAME == \"California\")\ncalifornia_borders &lt;- st_cast(california, \"MULTILINESTRING\")\nraster_template2 &lt;- rast(ext(california),\n                         resolution = 0.5,\n                         crs = st_crs(california)$wkt)\ncalifornia_raster1 &lt;-\n  rasterize(vect(california_borders), raster_template2,\n            touches = TRUE) # touches = TRUE : 경계값만\ncalifornia_raster2 &lt;-\n  rasterize(vect(california), raster_template2)\n# with `touches = FALSE` by default, which selects only cell\n\n\n\n\n\n\n\n\n\n\n공간적으로 연속적인 래스터 데이터를 점, 선 또는 다각형과 같은 공간적으로 분리된 벡터 데이터로 변환\n벡터화의 가장 간단한 형태는 래스터 셀의 중심부를 점으로 변환하는 것\nas.points() : 모든 raster grid 셀에 대해 중심점으로 반환\n\n\nelev &lt;- rast(system.file(\"raster/elev.tif\", package = \"spData\"))\nelev_point &lt;- as.points(elev) %&gt;%\n  st_as_sf()\nplot(elev)\n\n\n\nplot(elev_point)\n\n\n\n\n\ncontour() : 선에 해당하는 수치 표현\n등고선의 생성 : 공간 벡터화의 또 다른 일반적인 유형은 연속적인 높이 또는 온도(등온선)의 선을 나타내는 등고선 생성\n\n\ndem = rast(system.file(\"raster/dem.tif\", package = \"spDataLarge\"))\ncl = as.contour(dem)\nplot(dem, axes = FALSE)\nplot(cl, add = TRUE)\n\n\n\nplot(dem, axes = FALSE)\ncontour(dem, add = T) # 수치까지 표현\n\n\n\n\n\nas.polygons() : 래스터를 다각형으로 변환하는 것\n\n\ngrain &lt;- rast(system.file(\"raster/grain.tif\", package = \"spData\"))\ngrain_poly &lt;- as.polygons(grain) %&gt;%\n  st_as_sf()\nplot(grain)\n\n\n\nplot(grain_poly)\n\n\n\n\n\n\n\n\n\n\n\nCRS를 설명할 수 있는 여러가지 방법\n\n단순하지만 “lon/lat 좌표”와 같이 모호할 수 있는 문장\n\n\n공식화되었지만 지금은 구식인 proj4 strings\n\n\nproj=lonlat +ellps=WGS84 +datum=WGS84 +no_defs\n\n\nEPSG:4326과 같이 식별되는 authority:code 텍스트 문자열\n\n-&gt; 3번째 방법이 가장 정확(짧고 기억하기 쉬우며 온라인에서 찾기 쉬움)\n\n\nst_crs(\"EPSG:4326\")\n\n\n\n\n\n벡터 지리 데이터 객체에서 CRS를 가져오고 설정\n\n\nvector_filepath &lt;- system.file(\"shapes/world.gpkg\", package = \"spData\")\nnew_vector &lt;- read_sf(vector_filepath)\n\nst_crs(new_vector)\n#&gt; Coordinate Reference System:\n#&gt;   User input: WGS 84 \n#&gt;   wkt:\n#&gt; GEOGCRS[\"WGS 84\",\n#&gt;     ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n#&gt;         MEMBER[\"World Geodetic System 1984 (Transit)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G730)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G873)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G1150)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G1674)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G1762)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G2139)\"],\n#&gt;         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n#&gt;             LENGTHUNIT[\"metre\",1]],\n#&gt;         ENSEMBLEACCURACY[2.0]],\n#&gt;     PRIMEM[\"Greenwich\",0,\n#&gt;         ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;     CS[ellipsoidal,2],\n#&gt;         AXIS[\"geodetic latitude (Lat)\",north,\n#&gt;             ORDER[1],\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;         AXIS[\"geodetic longitude (Lon)\",east,\n#&gt;             ORDER[2],\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;     USAGE[\n#&gt;         SCOPE[\"Horizontal component of 3D system.\"],\n#&gt;         AREA[\"World.\"],\n#&gt;         BBOX[-90,-180,90,180]],\n#&gt;     ID[\"EPSG\",4326]]\n\n\nUser input : CRS식별자 (WGS 84, 입력 파일에서 가져온 EPSG:4326의 동의어)\nwkt : CRS에 대한 모든 관련 정보와 함께 전체 WKT 문자열을 포함\ninput 요소는 유연함(AUTHORITY:CODE (ex. EPSG:4326), CRS 이름(ex. WGS84), proj4string 정의)\nwkt 요소는 객체를 파일에 저장하거나 좌표 연산을 수행할 때 사용되는 WKT 표현을 저장\nnew_vector 객체가 WGS84 타원체를 가지며, 그리니치 프라임 자오선을 사용하고, 위도와 경도의 축 순서를 사용하는 것을 볼 수 있음\n이 경우 이 CRS 사용에 적합한 영역을 설명하는 USAGE와 CRS 식별자 EPSG:4326을 가리키는 ID와 같은 추가 요소도 있음\n\n\nst_crs(new_vector)$IsGeographic\n#&gt; [1] TRUE\nst_crs(new_vector)$units_gdal\n#&gt; [1] \"degree\"\nst_crs(new_vector)$srid\n#&gt; [1] \"EPSG:4326\"\nst_crs(new_vector)$proj4string\n#&gt; [1] \"+proj=longlat +datum=WGS84 +no_defs\"\n\n\nst_crs 함수에는 유용한 기능이 하나 있는데, 사용된 CRS에 대한 추가 정보를 검색할 수 있음.\n\nst_crs(new_vector)$IsGeographic : CRS가 지리적 상태인지 확인\nst_crs(new_vector)$units_gdal : CRS 단위\nst_crs(new_vector)$srid : 해당 ‘SRID’ 식별자를 추출(사용 가능한 경우)\nst_crs(new_vector)$proj4string : proj4string 표현을 추출\n\nst_set_crs() : CRS가 없거나 잘못 설정되어 있는 경우 CRS 설정\n\n\nnew_vector &lt;- st_set_crs(new_vector, \"EPSG:4326\") # set CRS\n\n\nterra::crs() : 래스터 객체에 대한 CRS를 설정\n하지만, crs() 함수를 사용하면 좌표계는 바뀌지만 값이 바뀌지는 않음.\n\n\nraster_filepath &lt;- system.file(\"raster/srtm.tif\", package = \"spDataLarge\")\nmy_rast &lt;- rast(raster_filepath)\ncrs(my_rast)\n#&gt; [1] \"GEOGCRS[\\\"WGS 84\\\",\\n    ENSEMBLE[\\\"World Geodetic System 1984 ensemble\\\",\\n        MEMBER[\\\"World Geodetic System 1984 (Transit)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G730)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G873)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1150)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1674)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1762)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G2139)\\\"],\\n        ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        ENSEMBLEACCURACY[2.0]],\\n    PRIMEM[\\\"Greenwich\\\",0,\\n        ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    CS[ellipsoidal,2],\\n        AXIS[\\\"geodetic latitude (Lat)\\\",north,\\n            ORDER[1],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        AXIS[\\\"geodetic longitude (Lon)\\\",east,\\n            ORDER[2],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    USAGE[\\n        SCOPE[\\\"Horizontal component of 3D system.\\\"],\\n        AREA[\\\"World.\\\"],\\n        BBOX[-90,-180,90,180]],\\n    ID[\\\"EPSG\\\",4326]]\"\ncat(crs(my_rast)) # get CRS\n#&gt; GEOGCRS[\"WGS 84\",\n#&gt;     ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n#&gt;         MEMBER[\"World Geodetic System 1984 (Transit)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G730)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G873)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G1150)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G1674)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G1762)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G2139)\"],\n#&gt;         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n#&gt;             LENGTHUNIT[\"metre\",1]],\n#&gt;         ENSEMBLEACCURACY[2.0]],\n#&gt;     PRIMEM[\"Greenwich\",0,\n#&gt;         ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;     CS[ellipsoidal,2],\n#&gt;         AXIS[\"geodetic latitude (Lat)\",north,\n#&gt;             ORDER[1],\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;         AXIS[\"geodetic longitude (Lon)\",east,\n#&gt;             ORDER[2],\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;     USAGE[\n#&gt;         SCOPE[\"Horizontal component of 3D system.\"],\n#&gt;         AREA[\"World.\"],\n#&gt;         BBOX[-90,-180,90,180]],\n#&gt;     ID[\"EPSG\",4326]]\ncrs(my_rast) &lt;- \"EPSG:26912\" # set CRS\n\nlondon &lt;- data.frame(lon = -0.1, lat = 51.5) %&gt;%\n  st_as_sf(coords = c(\"lon\", \"lat\"))\nst_is_longlat(london)\n#&gt; [1] NA\n\nlondon_geo &lt;- st_set_crs(london, \"EPSG:4326\")\nst_is_longlat(london_geo)\n#&gt; [1] TRUE\n\n\n\n\n\n\nsf는 지리 벡터 데이터에 대한 클래스와 지리 계산을 위한 중요한 하위 수준 라이브러리에 대한 일관된 명령줄 인터페이스 제공\n\n구면 geometry 연산을 sf:sf_use_sf(FALSE) 명령으로 끄면 버퍼는 미터와 같은 적절한 거리 단위를 대체하지 못하는 위도와 경도의 단위를 사용하기 때문에 쓸모없는 출력이 됨.\n공간 및 기하학적 연산을 수행하는 것은 경우에 따라 거의 또는 전혀 차이가 없음. (ex: 공간 부분 집합) 그러나 버퍼링과 같은 거리가 포함된 연산의 경우 (구면 지오메트리 엔진을 사용하지 않고) 좋은 결과를 보장하는 유일한 방법은 데이터의 투영된 복사본을 만들고 그에 대한 연산을 실행하는 것임.\n그 결과 런던과 동일하지만 미터 단위의 EPSG 코드를 가진 적절한 CRS(영국 국가 그리드)에 재투사된 새로운 물체가 되었음.\nCRS의 단위가 (도가 아닌) 미터라는 사실은 이것이 투영된 CRS임을 알려줌\n\n\n\nlondon_buff_no_crs &lt;-\n  st_buffer(london, dist = 1) # incorrect: no CRS\nlondon_buff_no_crs\n#&gt; Simple feature collection with 1 feature and 0 fields\n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -1.1 ymin: 50.5 xmax: 0.9 ymax: 52.5\n#&gt; CRS:           NA\n#&gt;                         geometry\n#&gt; 1 POLYGON ((0.9 51.5, 0.89862...\nlondon_buff_s2 &lt;-\n  st_buffer(london_geo, dist = 1e5) # silent use of s2 (1e5 : 10^5m = 100,000m)\nlondon_buff_s2\n#&gt; Simple feature collection with 1 feature and 0 fields\n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -1.552818 ymin: 50.59609 xmax: 1.356603 ymax: 52.40393\n#&gt; Geodetic CRS:  WGS 84\n#&gt;                         geometry\n#&gt; 1 POLYGON ((-0.3523255 52.392...\nlondon_buff_s2_100_cells &lt;-\n  st_buffer(london_geo, dist = 1e5, max_cells = 100)\nlondon_buff_s2_100_cells\n#&gt; Simple feature collection with 1 feature and 0 fields\n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -1.718303 ymin: 50.51128 xmax: 1.524546 ymax: 52.53186\n#&gt; Geodetic CRS:  WGS 84\n#&gt;                         geometry\n#&gt; 1 POLYGON ((-0.3908656 52.531...\n\nsf::sf_use_s2(FALSE)\n#&gt; Spherical geometry (s2) switched off\n\nlondon_buff_lonlat &lt;-\n  st_buffer(london_geo, dist = 1) # incorrect result\n#&gt; Warning in st_buffer.sfc(st_geometry(x), dist, nQuadSegs, endCapStyle =\n#&gt; endCapStyle, : st_buffer does not correctly buffer longitude/latitude data\n#&gt; dist is assumed to be in decimal degrees (arc_degrees).\n\nsf::sf_use_s2(TRUE)\n#&gt; Spherical geometry (s2) switched on\n\nlondon_proj &lt;- data.frame(x = 530000, y = 180000) %&gt;%\n  st_as_sf(coords = 1:2, crs = \"EPSG:27700\")\n\nst_crs(london_proj)\n#&gt; Coordinate Reference System:\n#&gt;   User input: EPSG:27700 \n#&gt;   wkt:\n#&gt; PROJCRS[\"OSGB36 / British National Grid\",\n#&gt;     BASEGEOGCRS[\"OSGB36\",\n#&gt;         DATUM[\"Ordnance Survey of Great Britain 1936\",\n#&gt;             ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,\n#&gt;                 LENGTHUNIT[\"metre\",1]]],\n#&gt;         PRIMEM[\"Greenwich\",0,\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;         ID[\"EPSG\",4277]],\n#&gt;     CONVERSION[\"British National Grid\",\n#&gt;         METHOD[\"Transverse Mercator\",\n#&gt;             ID[\"EPSG\",9807]],\n#&gt;         PARAMETER[\"Latitude of natural origin\",49,\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433],\n#&gt;             ID[\"EPSG\",8801]],\n#&gt;         PARAMETER[\"Longitude of natural origin\",-2,\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433],\n#&gt;             ID[\"EPSG\",8802]],\n#&gt;         PARAMETER[\"Scale factor at natural origin\",0.9996012717,\n#&gt;             SCALEUNIT[\"unity\",1],\n#&gt;             ID[\"EPSG\",8805]],\n#&gt;         PARAMETER[\"False easting\",400000,\n#&gt;             LENGTHUNIT[\"metre\",1],\n#&gt;             ID[\"EPSG\",8806]],\n#&gt;         PARAMETER[\"False northing\",-100000,\n#&gt;             LENGTHUNIT[\"metre\",1],\n#&gt;             ID[\"EPSG\",8807]]],\n#&gt;     CS[Cartesian,2],\n#&gt;         AXIS[\"(E)\",east,\n#&gt;             ORDER[1],\n#&gt;             LENGTHUNIT[\"metre\",1]],\n#&gt;         AXIS[\"(N)\",north,\n#&gt;             ORDER[2],\n#&gt;             LENGTHUNIT[\"metre\",1]],\n#&gt;     USAGE[\n#&gt;         SCOPE[\"Engineering survey, topographic mapping.\"],\n#&gt;         AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],\n#&gt;         BBOX[49.75,-9.01,61.01,2.01]],\n#&gt;     ID[\"EPSG\",27700]]\n\n\n\n\n\n\n\ndownload.file(url = \"https://irma.nps.gov/DataStore/DownloadFile/666527\",\n              destfile = \"nps_boundary.zip\")\nunzip(zipfile = \"nps_boundary.zip\")\nusa_parks = read_sf(dsn = \"nps_boundary.shp\")\n\n\n해외여서 접속이 막혀있음\n공공데이터포털에서 shape 파일 다운받아 불러오기\n\n공공데이터포털에서 데이터를 작업 공간에 다운 받기\n\n\n# unzip(zipfile=\"C:/202201/GIS/data/부산광역시_교통정보서비스센터 보유 ITS CCTV 현황(SHP)_20210601.zip\")\n#busan &lt;- read_sf(dsn = \"./Spatial_Information_Analysis/tl_tracffic_cctv_info.shp\", options = \"ENCODING:CP949\")\n#busan\n#plot(busan)\n\n# unzip(zipfile = \"C:/202201/GIS/data/CTPRVN_20220324.zip\")\n#sido &lt;- read_sf(dsn = \"./Spatial_Information_Analysis/ctp_rvn.shp\", options = \"ENCODING:CP949\")\n#sido\n#plot(sido)\n\n\n\n\n\n\nrnaturalearth 패키지의 ne_countries() 기능을 사용하면 국가 경계 기능을 사용할 수 있음\nosmdata 패키지는 속도가 제한되어 있다는 단점이 있음\n\n이러한 한계를 극복하기 위해 osmextract 패키지가 개발\n\n\nlibrary(rnaturalearth)\n#&gt; Support for Spatial objects (`sp`) will be deprecated in {rnaturalearth} and will be removed in a future release of the package. Please use `sf` objects with {rnaturalearth}. For example: `ne_download(returnclass = 'sf')`\nusa &lt;- ne_countries(country = \"United States of America\") # United States borders\n#&gt; Warning: The `returnclass` argument of `ne_download()` sp as of rnaturalearth 1.0.0.\n#&gt; ℹ Please use `sf` objects with {rnaturalearth}, support for Spatial objects\n#&gt;   (sp) will be removed in a future release of the package.\nclass(usa)\n#&gt; [1] \"SpatialPolygonsDataFrame\"\n#&gt; attr(,\"package\")\n#&gt; [1] \"sp\"\n\nusa_sf &lt;- st_as_sf(usa)\nplot(usa_sf[1])\n\n\n\nkorea &lt;- ne_countries(country = \"South Korea\") # United States borders\nclass(korea)\n#&gt; [1] \"SpatialPolygonsDataFrame\"\n#&gt; attr(,\"package\")\n#&gt; [1] \"sp\"\nkorea_sf &lt;- st_as_sf(korea)\nplot(korea_sf[1])\n\n\n\n\n\n\n\n\n\nhttps://r.geocompx.org/read-write.html#file-formats\n\n\n\n\n\ngpkg 형식 불러오기\n\n\nf &lt;- system.file(\"shapes/world.gpkg\", package = \"spData\")\nworld = read_sf(f, quiet = TRUE)\ntanzania = read_sf(f, query = 'SELECT * FROM world WHERE name_long = \"Tanzania\"')\ntanzania_buf = st_buffer(tanzania, 50000)\ntanzania_buf_geom = st_geometry(tanzania_buf)\ntanzania_buf_wkt = st_as_text(tanzania_buf_geom)\ntanzania_neigh = read_sf(f, wkt_filter = tanzania_buf_wkt)\n\n\ncsv 형식 불러오기\n\n\ncycle_hire_txt = system.file(\"misc/cycle_hire_xy.csv\", package = \"spData\")\ncycle_hire_xy = read_sf(cycle_hire_txt,\n                        options = c(\"X_POSSIBLE_NAMES=X\", \"Y_POSSIBLE_NAMES=Y\"))\n\n\nWell-known text(WKT), Well-known binary(WKB), and the GeoJSON formats\n\n\nworld_txt = system.file(\"misc/world_wkt.csv\", package = \"spData\")\nworld_wkt = read_sf(world_txt, options = \"GEOM_POSSIBLE_NAMES=WKT\")\n# the same as\nworld_wkt2 = st_read(world_txt, options = \"GEOM_POSSIBLE_NAMES=WKT\",\n                     quiet = TRUE, stringsAsFactors = FALSE, as_tibble = TRUE)\n\n\nKML file stores geographic information in XML format\n\n\nu = \"https://developers.google.com/kml/documentation/KML_Samples.kml\"\ndownload.file(u, \"./Spatial_Information_Analysis/KML_Samples.kml\")\nst_layers(\"./Spatial_Information_Analysis/KML_Samples.kml\")\n#&gt; Driver: KML \n#&gt; Available layers:\n#&gt;              layer_name  geometry_type features fields crs_name\n#&gt; 1            Placemarks       3D Point        3      2   WGS 84\n#&gt; 2      Highlighted Icon       3D Point        1      2   WGS 84\n#&gt; 3                 Paths 3D Line String        6      2   WGS 84\n#&gt; 4         Google Campus     3D Polygon        4      2   WGS 84\n#&gt; 5      Extruded Polygon     3D Polygon        1      2   WGS 84\n#&gt; 6 Absolute and Relative     3D Polygon        4      2   WGS 84\nkml = read_sf(\"./Spatial_Information_Analysis/KML_Samples.kml\", layer = \"Placemarks\")\n\n\n\n\n\n\n\n\n정적인 지도는 지리 계산의 가장 일반적인 시각적 출력 유형\nplot() 또는 tmap_mode(plot)\n\n\n\n\n# Add fill layer to nz shape\ntm_shape(nz) +\n  tm_fill()\n# Add border layer to nz shape\ntm_shape(nz) +\n  tm_borders()\n# Add fill and border layers to nz shape\ntm_shape(nz) +\n  tm_fill() +\n  tm_borders()\n\n\n\n\n\n\n\n\n\n\nmap_nz &lt;- tm_shape(nz) + tm_polygons()\nclass(map_nz)\n#&gt; [1] \"tmap\"\nmap_nz\n\n\n\n\nnz_elev = rast(system.file(\"raster/nz_elev.tif\", package = \"spDataLarge\"))\n\nmap_nz1 &lt;- map_nz + tm_shape(nz_elev) + tm_raster(alpha = 0.7)\n\nnz_water &lt;- st_union(nz) %&gt;% st_buffer(22200) %&gt;%\n  st_cast(to = \"LINESTRING\")\n\nmap_nz2 &lt;- map_nz1 +\n  tm_shape(nz_water) + tm_lines()\n\nmap_nz3 &lt;- map_nz2 +\n  tm_shape(nz_height) + tm_dots()\n\ntmap_arrange(map_nz1, map_nz2, map_nz3)\n\n\n\n\n\nalpha : 레이어를 반투명하게 만들기 위해 설정\n\n\n\n\n\nma1 &lt;- tm_shape(nz) + tm_fill(col = \"red\")\nma2 &lt;- tm_shape(nz) + tm_fill(col = \"red\", alpha = 0.3)\nma3 &lt;- tm_shape(nz) + tm_borders(col = \"blue\")\nma4 &lt;- tm_shape(nz) + tm_borders(lwd = 3)\nma5 &lt;- tm_shape(nz) + tm_borders(lty = 2)\nma6 &lt;- tm_shape(nz) + tm_fill(col = \"red\", alpha = 0.3) +\n  tm_borders(col = \"blue\", lwd = 3, lty = 2)\n\ntmap_arrange(ma1, ma2, ma3, ma4, ma5, ma6)\n\n\n\n\n\ntm_fill()과 tm_bubbles()에서 레이어는 기본적으로 회색으로 채워지고 tm_lines()은 검은선으로 그려짐\ntmap의 인수는 숫자 벡터를 허용하지 않음\n\n\nplot(st_geometry(nz), col = nz$Land_area) # works\ntm_shape(nz) + tm_fill(col = nz$Land_area) # fails\n&gt; Error: Fill argument neither colors nor valid variable name(s)\ntm_shape(nz) + tm_fill(col = \"Land_area\")\n\n\n\n\n\n\n\n범례의 제목 설정\n\n\nlegend_title &lt;- expression(\"Area (km\"^2*\")\")\nmap_nza &lt;- tm_shape(nz) +\n  tm_fill(col = \"Land_area\", title = legend_title) + tm_borders()\nmap_nza\n\n\n\n\n\n\n\n\nbreaks : 색상의 표현 값 범위를 수동으로 설정\nn : 숫자 변수가 범주화되는 Bin의 수 설정\npalette : 색 구성표를 정의 (ex. BuGn)\n\n\ntm1 &lt;- tm_shape(nz) + tm_polygons(col = \"Median_income\")\nbreaks = c(0, 3, 4, 5) * 10000\ntm2 &lt;- tm_shape(nz) + tm_polygons(col = \"Median_income\", breaks = breaks)\ntm3 &lt;- tm_shape(nz) + tm_polygons(col = \"Median_income\", n = 10)\ntm4 &lt;- tm_shape(nz) + tm_polygons(col = \"Median_income\", palette = \"BuGn\")\n\ntmap_arrange(tm1, tm2, tm3, tm4)\n\n\n\n\n\ntm_shape(nz) + tm_polygons(col = \"Median_income\", style = \"pretty\")\ntm_shape(nz) + tm_polygons(col = \"Median_income\", style = \"equal\")\ntm_shape(nz) + tm_polygons(col = \"Median_income\", style = \"quantile\")\ntm_shape(nz) + tm_polygons(col = \"Median_income\", style = \"jenks\")\ntm_shape(nz) + tm_polygons(col = \"Median_income\", style = \"cont\")\ntm_shape(nz) + tm_polygons(col = \"Median_income\", style = \"cat\")\n\n\n\n\n\n\n\nstyle = \"pretty\" : 기본 설정은 가능한 경우 정수로 반올림하고 간격을 균등하게 유지\nstyle = \"equal\" : 입력 값을 동일한 범위의 빈으로 나누고 균일한 분포의 변수에 적합(결과 맵이 색상 다양성이 거의 없을 수 있으므로 분포가 치우친 변수에는 권장하지 않음)\nstyle = \"quantile\" : 동일한 수의 관찰이 각 범주에 포함되도록 함(빈 범위가 크게 다를 수 있다는 잠재적인 단점이 있음).\nstyle = \"jenks\" : 데이터에서 유사한 값의 그룹을 식별하고 범주 간의 차이를 최대화\nstyle = \"cont\" : 연속 색상 필드에 많은 색상을 표시하고 연속 래스터에 특히 적합\nstyle = \"cat\" : 범주 값을 나타내도록 설계되었으며 각 범주가 고유한 색상을 받도록 함\n\n\ntm_p1 &lt;- tm_shape(nz) + tm_polygons(\"Population\", palette = \"Blues\")\ntm_p2 &lt;- tm_shape(nz) + tm_polygons(\"Population\", palette = \"YlOrBr\")\n\ntmap_arrange(tm_p1, tm_p2)\n\n\n\n\n\n순차 팔레트는 단일(ex. Blues : 밝은 파란색에서 진한 파란색으로 이동) 또는 다중 색상/색조(ex. YlOrBr : 주황색을 통해 밝은 노란색에서 갈색으로 그라데이션)\n\n\n\n\n\nmap_nz +\n  tm_compass(type = \"8star\", position = c(\"left\", \"top\")) +\n  tm_scale_bar(breaks = c(0, 100, 200), text.size = 1)\n\n\n\n\ntm_l1 &lt;- map_nz + tm_layout(title = \"New Zealand\")\ntm_l2 &lt;- map_nz + tm_layout(scale = 5)\ntm_l3 &lt;- map_nz + tm_layout(bg.color = \"lightblue\")\ntm_l4 &lt;- map_nz + tm_layout(frame = FALSE)\n\ntmap_arrange(tm_l1, tm_l2, tm_l3, tm_l4)\n\n\n\n\n\ntm_layout()의 다양한 옵션\n\nframe.lwd : 프레임 너비\nframe.double.line : 이중선 허용 옵션\nouter.margin, inner.margin : 여백 설정\nfontface, fontfamily : 글꼴 설정\nlegend.show : 범례 표시 여부\nlegend.position : 범례 위치 변경\n\n\n\n\n\n\n\n\ntm_s1 &lt;- map_nza + tm_style(\"bw\")\ntm_s2 &lt;- map_nza + tm_style(\"classic\")\ntm_s3 &lt;- map_nza + tm_style(\"cobalt\")\ntm_s4 &lt;- map_nza + tm_style(\"col_blind\")\n\ntmap_arrange(tm_s1, tm_s2, tm_s3, tm_s4)\n\n\n\n\n\n\n\n\nurb_1970_2030 &lt;- urban_agglomerations %&gt;%\n  filter(year %in% c(1970, 1990, 2010, 2030))\ntm_shape(world) +\n  tm_polygons() +\n  tm_shape(urb_1970_2030) +\n  tm_symbols(col = \"black\", border.col = \"white\", size = \"population_millions\") +\n  tm_facets(by = \"year\", nrow = 2, free.coords = TRUE)\n\n\n\n#free.coords : 지도에 자체 경계 상자가 있는지 여부를 지정\n\n\n\n\n\nnz_region &lt;- st_bbox(c(xmin = 1340000, xmax = 1450000,\n                       ymin = 5130000, ymax = 5210000),\n                     crs = st_crs(nz_height)) %&gt;% st_as_sfc()\n\nnz_height_map &lt;- tm_shape(nz_elev, bbox = nz_region) +\n  tm_raster(style = \"cont\", palette = \"YlGn\", legend.show = TRUE) +\n  tm_shape(nz_height) + tm_symbols(shape = 2, col = \"red\", size = 1) +\n  tm_scale_bar(position = c(\"left\", \"bottom\"))\n\nnz_map &lt;- tm_shape(nz) + tm_polygons() +\n  tm_shape(nz_height) + tm_symbols(shape = 2, col = \"red\", size = 0.1) +\n  tm_shape(nz_region) + tm_borders(lwd = 3)\n\nlibrary(grid)\nnz_height_map\nprint(nz_map, vp = viewport(0.8, 0.27, width = 0.5, height = 0.5))\n\n\n\n\n\nviewport() : 두개의 맵을 결합\n\n\n\n\n\n\nurb_anim &lt;- tm_shape(world) + tm_polygons() +\n  tm_shape(urban_agglomerations) + tm_dots(size = \"population_millions\") +\n  tm_facets(along = \"year\", free.coords = FALSE)\n\ntmap_animation(urb_anim, filename = \"./Spatial_Information_Analysis/urb_anim.gif\", delay = 25)\n#&gt; Creating frames\n#&gt; =========\n#&gt; ====\n#&gt; =====\n#&gt; ====\n#&gt; =====\n#&gt; ====\n#&gt; =====\n#&gt; ====\n#&gt; ====\n#&gt; =====\n#&gt; ====\n#&gt; =====\n#&gt; ====\n#&gt; =====\n#&gt; ====\n#&gt; =====\n#&gt; ====\n#&gt; \n#&gt; Creating animation\n#&gt; Animation saved to C:\\Users\\Hyunsoo Kim\\Desktop\\senior_grade\\blog\\my-quarto-website\\Spatial_Information_Analysis\\urb_anim.gif\n\n\nby = year대신 along = year을 사용\nfree.coords = FALSE : 각 맵 반복에 대한 맵 범위 유지\ntmap_animation()을 사용하여 .gif로 저장\n\n\n\n\n\n대화형 지도는 데이터 세트를 새로운 차원으로 끌어올릴 수 있음\n지도를 기울이고 회전하는 기능과 사용자가 이동 및 확대/축소 할 때 자동으로 업데이트\ntmap, mapview, mapdeck, leaflet으로 표현 가능\n\n\n\n\ntmap_mode(\"view\") #interactive mode\n#&gt; tmap mode set to interactive viewing\nmap_nz\n\n\n\n\n\n\nmap_nz + tm_basemap(server = \"OpenTopoMap\")\n\n\n\n\n\n\nworld_coffee = left_join(world, coffee_data, by = \"name_long\")\nfacets = c(\"coffee_production_2016\", \"coffee_production_2017\")\ntm_shape(world_coffee) + tm_polygons(facets) +\n  tm_facets(nrow = 1, sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntm_basemap() 또는 tm_options()로 basemap 지정 가능\ntm_facets()에서 sync옵션을 TRUE로 선택하면 여러개의 맵을 동시에 확대/축소할 수 있음\n\n\n\n\n\nmapview::mapview(nz)\n\ntrails %&gt;%\n  st_transform(st_crs(franconia)) %&gt;%\n  st_intersection(franconia[franconia$district == \"Oberfranken\", ][1]) %&gt;%\n  st_collection_extract(\"LINE\") %&gt;%\n  mapview(color = \"red\", lwd = 3, layer.name = \"trails\") +\n  mapview(franconiWa, zcol = \"district\", burst = TRUE) +\n  breweries\n\n\n\n\n\nset_token(Sys.getenv(\"pk.eyJ1IjoiancwMTEyIiwiYSI6ImNsM2ppbzYzNzBrbjQzZHBjMmlocnY2dDUifQ.58-gXpPtvcCGmMt2xEW-ig\"))\ncrash_data = read.csv(\"https://git.io/geocompr-mapdeck\")\ncrash_data = na.omit(crash_data)\nms = mapdeck_style(\"dark\")\nmapdeck(style = ms, pitch = 45, location = c(0, 52), zoom = 4) %&gt;%\n  add_grid(data = crash_data, lat = \"lat\", lon = \"lng\", cell_size = 1000,\n           elevation_scale = 50, layer_id = \"grid_layer\",\n           colour_range = viridisLite::plasma(6))\n#&gt; Registered S3 method overwritten by 'jsonify':\n#&gt;   method     from    \n#&gt;   print.json jsonlite\n\n\n\n\n\n\n\n\n\nadd_arc() 함수\n\n\nurl &lt;- 'https://raw.githubusercontent.com/plotly/datasets/master/2011_february_aa_flight_paths.csv'\nflights &lt;- read.csv(url)\nflights$id &lt;- seq_len(nrow(flights))\nflights$stroke &lt;- sample(1:3, size = nrow(flights), replace = T)\nkey = \"pk.eyJ1IjoiancwMTEyIiwiYSI6ImNsM2ppbzYzNzBrbjQzZHBjMmlocnY2dDUifQ.58-gXpPtvcCGmMt2xEW-ig\"\n\nmapdeck(token = key, style = mapdeck_style(\"dark\"), pitch = 45 ) %&gt;%\n  add_arc(\n    data = flights\n    , layer_id = \"arc_layer\"\n    , origin = c(\"start_lon\", \"start_lat\")\n    , destination = c(\"end_lon\", \"end_lat\")\n    , stroke_from = \"airport1\"\n    , stroke_to = \"airport2\"\n    , stroke_width = \"stroke\"\n  )\n\n\nadd_animated_arc() 함수\n\n\nmapdeck(token = key, style = 'mapbox://styles/mapbox/dark-v9', pitch = 45 ) %&gt;%\n  add_animated_arc(\n    data = flights\n    , layer_id = \"arc_layer\"\n    , origin = c(\"start_lon\", \"start_lat\")\n    , destination = c(\"end_lon\", \"end_lat\")\n    , stroke_from = \"airport1\"\n    , stroke_to = \"airport2\"\n    , stroke_width = \"stroke\"\n  )\n\n\nadd_heatmap() 함수\n\n\nmapdeck(token = key, style = mapdeck_style('dark'), pitch = 45 ) %&gt;%\n  add_heatmap(\n    data = df[1:30000, ]\n    , lat = \"lat\"\n    , lon = \"lng\"\n    , weight = \"weight\"\n    , colour_range = colourvalues::colour_values(1:6, palette = \"inferno\")\n  )\n\n\nadd_path() 함수\n\n\nmapdeck(\n  token = key\n  , style = mapdeck_style(\"dark\")\n  , zoom = 10) %&gt;%\n  add_path(\n    data = roads\n    , stroke_colour = \"RIGHT_LOC\"\n    , layer_id = \"path_layer\"\n  )\n\n\nadd_geojson(), add_scatterplot(), add_text() 등이 있음\n\n\n\n\n\npal = colorNumeric(\"RdYlBu\", domain = cycle_hire$nbikes)\nleaflet(data = cycle_hire) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%       # Background Map\n  addCircles(col = ~pal(nbikes), opacity = 0.9) %&gt;%      # nbikes의 값으로 색이 다르게 circle 생성\n  addPolygons(data = lnd, fill = FALSE) %&gt;%              # land에 따라 Polygon 생성\n  addLegend(pal = pal, values = ~nbikes) %&gt;%             # 범례 생성\n  setView(lng = -0.1, 51.5, zoom = 12) %&gt;%               # zoom\n  addMiniMap()                                           # minimap 생성\n\n\n\n\n\n\n# create a basic map\n\nleaflet() %&gt;%\n  addTiles() %&gt;% # add default OpenStreetMap map tiles\n  setView(lng=127.063, lat=37.513, zoom = 6) # korea, zoom 6\n\n\n\n\n\n# map style: NASA\n\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng=127.063, lat=37.513, zoom = 6) %&gt;%\n  addProviderTiles(\"NASAGIBS.ViirsEarthAtNight2012\")\n\n\n\n\n\n# map style: Esri.WorldImagery\n\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng=127.063, lat=37.513, zoom = 16) %&gt;%\n  addProviderTiles(\"Esri.WorldImagery\")\n\n\n\n\n\n# adding Popup\n\npopup = c(\"한남대학교 빅데이터응용학과\")\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  addMarkers(lng = c(127.4219), # longitude\n             lat = c(36.3548), # latitude\n             popup = popup)\n\n\n\n\n\n\nzoom : 확대/축소 비율 설정\naddProviderTiles() : 외부 지도 타일 추가\naddMarkers() : 커서를 클릭했을 때 팝업으로 나타나는 설명을 추가\n\n\n\n\n\n\n\n\nR을 사용하여 한걸음 더 나아가 웹 어플리케이션을 제작할 수 있게 해주는 패키지\nui 라고 말하는 화면은 실제로 사용자가 보는 화면\nshiny에서는 크게 titlePanel과 sidebarPanel, mainPanal의 세 가지로 구성\n\n\nui = fluidPage(\n  sliderInput(inputId = \"life\", \"Life expectancy\", 49, 84, value = 80),\n  leafletOutput(outputId = \"map\")\n)\nserver = function(input, output) {\n  output$map = renderLeaflet({\n    leaflet() %&gt;%\n      # addProviderTiles(\"OpenStreetMap.BlackAndWhite\") %&gt;%\n      addPolygons(data = world[world$lifeExp &lt; input$life,])\n  })\n}\nshinyApp(ui, server)\n#&gt; PhantomJS not found. You can install it with webshot::install_phantomjs(). If it is installed, please make sure the phantomjs executable can be found via the PATH variable.\n\nShiny applications not supported in static R Markdown documents\n\n\n\nui &lt;- fluidPage(#Application title\n  titlePanel(\"Hello Shiny!\"),\n  #Sidebar with a slider input for the number of bins\n  sidebarLayout(sidebarPanel(\n    sliderInput(\n      \"bins\",\n      \"Number of bins:\",\n      min = 1,\n      max = 50,\n      value = 30\n    )\n  ),\n  #Show a plot of the generated distribution\n  mainPanel(plotOutput(\"distPlot\"))))\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    x &lt;- faithful[, 2]\n    bins &lt;- seq(min(x), max(x), length.out = input$bins + 1)\n    hist(x,\n         breaks = bins,\n         col = 'darkgray',\n         border = 'white')\n  })\n}\nshinyApp(ui, server)\n\nShiny applications not supported in static R Markdown documents\n\n\n\n\n\n\n\n\n지도 공간 기법으로 시각화하는 ggmap 패키지는 Google Maps, Stamen Maps, 네이버 맵, 등의 다양한 온라인 소스로부터 가져온 정적인 지도 위에 특별한 데이터나 모형을 시각화하는 함수들을 제공함\nggmap()의 주요 함수\n\ngeocode() : 거리주소 또는 장소 이름을 이용하여 이용 지도 정보(위도, 경도) 획득\nget_googlemap() : 구글 지도 서비스 API에 접근하여 정적 지도 다운로드 지원과 지도에 marker 등을 삽입하고 자신이 원하는 줌 레벨과 center를 지정하여 지도 정보 생성\nget_map() : 지도 서비스 관련 서버에 관련 질의어를 지능형으로 인식하여 지도 정보 생성\nget_navermap() : 네이버 지도 서비스 API에 접근하여 정적 지도 다운로드 지원\nggimage() : ggplot2 패키지의 이미지와 동등한 수준으로 지도 이미지 생성\nggmap(), ggmapplot() : get_map() 함수에 의해서 생성된 픽셀 객체를 지도 이미지로 시각화\nqmap() : ggmap()함수와 get_map() 함수의 통합기능\nqmplot() : ggplot2 패키지의 qplot()와 동등한 수준으로 빠르게 지도 이미지 시각화\n\n\n\n\n\nget_googlemap() 함수를 통해 불러오고 싶은 곳의 장소를 문자열 값으로 첫 번째 인자에 넣어 실행해 이를 객체화 함\nggmap() 함수 안에 방금 만든 객체를 입력시킨 후 실행하면 원하는 장소를 중심으로 구글 지도가 plotting 됨\n\n\n# install.packages(\"ggmap\")\nlibrary(ggmap)\nregister_google(key = 'AIzaSyB4jjrVVAzb9fl8FQrQqUONAsaRBppWuSA')\n\n# 우리나라 지도 호출\ngetmap &lt;- get_googlemap(\"seoul\")\nggmap(getmap)\n\n\n\n\n\nggmap() 으로 반환되는 결과물은 ggplot2 패키지의 함수와 조합해 지도 위에 새로운 정보들을 추가할 수 있음\n\n\n\n\ndaejeon_map &lt;- get_googlemap(\"daejeon\") %&gt;% ggmap\nlocation &lt;- data.frame(\n  Name = c(\"한남대학교\", \"대전신세계\"),\n  lon = c(127.4219, 127.3821), #경도\n  lat = c(36.3548, 36.3752)    #위도\n)\n\ndaejeon_map + geom_point(data = location, aes(x = lon, y = lat))\n\ndaejeon_map &lt;- get_googlemap(\"daejeon\", zoom = 13) %&gt;% ggmap\nlocation &lt;- data.frame(\n  Name = c(\"한남대학교\", \"대전신세계\"),\n  lon = c(127.4219, 127.3821),\n  lat = c(36.3548, 36.3752)\n)\n\ndaejeon_map + geom_point(data = location, aes(x = lon, y = lat)) +\n  geom_text(data = location,\n            aes(label = Name),\n            size = 5,   # text 크기\n            vjust = -1) # text 위치\n\n\ngeom_point() 내의 옵션을 선택하여 점의 크기, 색깔, 모양 등 변경 가능\n\n\ndaejeon_map + geom_point(data = location, aes(x = lon, y = lat),\n                         size = 5, color = \"red\", alpha = 0.4) +\ngeom_text(data = location, aes(label = Name), size = 5, vjust = -1)\n\n\n한남대학교를 중심으로 그리기(center)\n\nenc2utf8 : UTF-8로 인코딩\nmaptype : “terrain”, “satellite”, “roadmap”, “hybrid”\ncenter : 맵의 중심\n\n\n# 한남대학교를 중심으로 그리기\ngc &lt;- geocode(enc2utf8(\"한남대학교\"))\n\nmap &lt;- get_googlemap(\n  center = as.numeric(gc),\n  maptype = \"roadmap\",\n  zoom = 13,\n  size = c(640, 640),\n  markers = gc) %&gt;% ggmap\n\nmap + geom_point(data = location, aes(x = lon, y = lat)) +\ngeom_text(data = location, aes(label = Name), size = 5, vjust = -1)\n\nPath(경로)\n\n\nmap + geom_path(data = location, aes(x = lon, y = lat), color = \"blue\", alpha = .5, lwd = 1)\n\n\n두 지역 사이의 경로 좌표 추출\n\nggmap::route : find a route from Google using different possible modes (\"driving\", \"walking\", \"bicycling\", \"transit\")\n\n\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(tmap)\nlibrary(stplanr)\n\ngc_st &lt;- geocode(enc2utf8(\"한남대학교\"))\ngc_ed &lt;- geocode(enc2utf8(\"신세계백화점 대전신세계아트앤사이언스\"))\ngc_od &lt;- st_linestring(rbind(as.numeric(gc_st), as.numeric(gc_ed)))\n\nst_sfc(gc_od) # Linestring, CRS 없음\nst_crs(gc_od)\ngc_od &lt;- st_sfc(gc_od, crs = 4326)\n# st_sfc() : 좌표계가 비어있는 경우에 좌표계 지정\nst_crs(gc_od)\n\nqtm(gc_od)\ngc_od &lt;- st_sf(gc_od)\n# st_sf() : sfc와 sf class의 객체들을 하나로 통합\ngc_od$distance &lt;- as.numeric(st_length(gc_od))\n\nroute_od = route(l = gc_od,             # l : linestring\n                 route_fun = route_osrm,\n                 osrm.profile = \"car\")  # foot, bike, car\nqtm(route_od)\n\nmap &lt;- get_googlemap(\n  center = c(127.41, 36.37),\n  maptype = \"roadmap\",\n  zoom = 14,\n  size = c(640, 640),\n  markers = gc\n) %&gt;% ggmap(extent = \"device\")\n\nmap\n\nmap + geom_sf(data = route_od, inherit.aes = F)\n# inherit.aes = F : sf형식의 데이터를 그릴 때 필수 옵션\n\n지도를 꽉 채워서 출력(x, y축 삭제하고 그림만 출력)\n\nextent = \"device\"\n+ theme_void()\n\n\nmap &lt;- get_googlemap(\n  center = as.numeric(gc),\n  maptype = \"roadmap\",\n  zoom = 13,\n  size = c(640, 640),\n  markers = gc\n) %&gt;% ggmap(extent = \"device\")\nmap\n\nmap &lt;- get_googlemap(\n  center = as.numeric(gc),\n  maptype = \"roadmap\",\n  zoom = 13,\n  size = c(640, 640),\n  markers = gc\n) %&gt;% ggmap() + theme_void()\nmap\n\n\n\n\n\n\n# Houston 범죄 데이터\nstr(crime)\nHoustonmap &lt;- get_map(\"Houston\")\nggmap(Houstonmap)\nggmap(Houstonmap) + geom_point(data = crime, aes(x = lon, y = lat))\nggmap(Houstonmap) + geom_point(data = crime, aes(x = lon, y = lat), size = 0.1, alpha = 0.1) # 점의 크기, 점의 투명도 조절\n\n#지도 확대 & 특정 지역 데이터만 추출하기\nHoustonmap &lt;- get_map(\"Houston\", zoom = 14)\ncrime1 &lt;- crime[(crime$lon &lt; -95.344 & crime$lon &gt; -95.395) & (crime$lat &lt; 29.783 & crime$lat &gt; 29.738), ]\ncrime11 &lt;- crime %&gt;% filter((lon &lt; -95.344 & lon &gt; -95.395) & (lat &lt; 29.783 & lat &gt; 29.738))\nnrow(crime1) ; nrow(crime11)\ncrime1 %&gt;% arrange(desc(lon)) %&gt;% nrow()\ncrime11 %&gt;% arrange(desc(lon)) %&gt;% nrow()\n\nggmap(Houstonmap) + geom_point(data = crime1, aes(x = lon, y = lat), alpha = 0.3)\nggmap(Houstonmap) + geom_point(data = crime1, aes(x = lon, y = lat, colour = offense))\n\ncrime2 &lt;- crime1[!duplicated(crime1[, c(\"lon\", \"lat\")]), ] # 위, 경도에 대해 중복되지 않게 하나의 관측치만 선택\n\ncrime2$offense &lt;- as.character(crime2$offense) # 범죄 종류 문자형으로 변경\n\ncrime2$offense[crime2$offense == \"murder\" | crime2$offense == \"rape\"] &lt;- \"4\"\ncrime2$offense[crime2$offense == \"robbery\" | crime2$offense == \"aggravated assault\"] &lt;- \"3\"\ncrime2$offense[crime2$offense == \"burglary\" | crime2$offense == \"auto theft\"] &lt;- \"2\"\ncrime2$offense[crime2$offense == \"theft\"] &lt;- \"1\"\n\ncrime2$offense &lt;- as.numeric(crime2$offense) # 범죄 종류 문자형을 숫자형으로 변경\n\nggmap(Houstonmap) + geom_point(data = crime2, aes(x = lon, y = lat, size = offense), alpha = 0.2)\n\n# 범죄 위험도에 따라 점의 크기 및 색깔로 구별\nggmap(Houstonmap) + geom_point(data = crime2, aes(x = lon, y = lat, size = offense, colour = offense), alpha = 0.5) +\n  scale_colour_gradient(low = \"white\", high = \"red\")\n\ncrime3 &lt;- crime2[crime2$date == \"1/1/2010\", ]\n\ncrime4 &lt;- crime3[!duplicated(crime3[, c(\"hour\")]), ]\n\nnrow(crime3) ; nrow(crime4)\n\nggmap(Houstonmap) + geom_point(data = crime3, aes(x = lon, y = lat)) +\n  geom_text(data = crime4, aes(label = street), vjust = 1.2) +\n  geom_path(data = crime4, aes(x = lon, y = lat), color = \"red\")\n\n\n\n\n\n\n\nnames(bristol_zones) ; names(bristol_od)\n#&gt; [1] \"geo_code\" \"name\"     \"geometry\"\n#&gt; [1] \"o\"          \"d\"          \"all\"        \"bicycle\"    \"foot\"      \n#&gt; [6] \"car_driver\" \"train\"\nnrow(bristol_zones)  ; nrow(bristol_od)\n#&gt; [1] 102\n#&gt; [1] 2910\n\n# O : Zone of the Origin / D : Zone of the Dest\n\nzones_attr = bristol_od %&gt;%\n  group_by(o) %&gt;%\n  summarize_if(is.numeric, sum) %&gt;%\n  dplyr::rename(geo_code = o)\n\nsummary(zones_attr$geo_code %in% bristol_zones$geo_code) # 일치하는지 확인\n#&gt;    Mode    TRUE \n#&gt; logical     102\n\n\nzones_joined = left_join(bristol_zones, zones_attr, by = \"geo_code\")\nnrow(zones_joined)\n#&gt; [1] 102\nsum(zones_joined$all)\n#&gt; [1] 238805\n\nnames(zones_joined)\n#&gt; [1] \"geo_code\"   \"name\"       \"all\"        \"bicycle\"    \"foot\"      \n#&gt; [6] \"car_driver\" \"train\"      \"geometry\"\n#&gt; [1] \"geo_code\" \"name\" \"all\" \"bicycle\" \"foot\" \"car_driver\" \"train\" \"geometry\"\nnames(zones_joined)[3] &lt;- c(\"all_orig\")\nnames(zones_joined)\n#&gt; [1] \"geo_code\"   \"name\"       \"all_orig\"   \"bicycle\"    \"foot\"      \n#&gt; [6] \"car_driver\" \"train\"      \"geometry\"\n\nzones_od = bristol_od %&gt;%\n  group_by(d) %&gt;%\n  summarize_if(is.numeric, sum) %&gt;%\n  dplyr::select(geo_code = d, all_dest = all) %&gt;%\n  inner_join(zones_joined, ., by = \"geo_code\")\n\nzones_od\n#&gt; Simple feature collection with 102 features and 8 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -2.845847 ymin: 51.28248 xmax: -2.252388 ymax: 51.73982\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;     geo_code                             name all_orig bicycle foot car_driver\n#&gt; 1  E02002985 Bath and North East Somerset 001      868      30  173        414\n#&gt; 2  E02002987 Bath and North East Somerset 003      898      34  117        523\n#&gt; 3  E02003005 Bath and North East Somerset 021      786      19   91        593\n#&gt; 4  E02003012                      Bristol 001     3312     161  330       2058\n#&gt; 5  E02003013                      Bristol 002     3715     188  615       2021\n#&gt; 6  E02003014                      Bristol 003     2220     126  270       1239\n#&gt; 7  E02003015                      Bristol 004     1633     166  307        786\n#&gt; 8  E02003016                      Bristol 005     2411     218  440       1105\n#&gt; 9  E02003017                      Bristol 006     1590     187  208        898\n#&gt; 10 E02003018                      Bristol 007     1690      96  143       1048\n#&gt;    train all_dest                       geometry\n#&gt; 1     43      744 MULTIPOLYGON (((-2.510462 5...\n#&gt; 2     58      561 MULTIPOLYGON (((-2.476122 5...\n#&gt; 3      8      427 MULTIPOLYGON (((-2.55073 51...\n#&gt; 4     12      701 MULTIPOLYGON (((-2.595763 5...\n#&gt; 5      6      940 MULTIPOLYGON (((-2.593783 5...\n#&gt; 6      5     3469 MULTIPOLYGON (((-2.639581 5...\n#&gt; 7      7     4980 MULTIPOLYGON (((-2.584973 5...\n#&gt; 8     23      297 MULTIPOLYGON (((-2.565948 5...\n#&gt; 9      9     1459 MULTIPOLYGON (((-2.616485 5...\n#&gt; 10    20      128 MULTIPOLYGON (((-2.637681 5...\n\nqtm(zones_od, c(\"all_orig\", \"all_dest\")) +\ntm_layout(panel.labels = c(\"Origin\", \"Destination\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nod_top5 = bristol_od %&gt;%\n  arrange(desc(all)) %&gt;%\n  top_n(5, wt = all)\n\nbristol_od$Active = (bristol_od$bicycle + bristol_od$foot) / bristol_od$all * 100\n\nod_intra = filter(bristol_od, o == d) # 지역 내 이동\nod_inter = filter(bristol_od, o != d) # 지역 외 이동\nod_intra ; od_inter # 102행 / 2808행\n#&gt; # A tibble: 102 × 8\n#&gt;    o         d           all bicycle  foot car_driver train Active\n#&gt;    &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1 E02002985 E02002985   209       5   127         59     0   63.2\n#&gt;  2 E02002987 E02002987   166       8    61         89     2   41.6\n#&gt;  3 E02003005 E02003005   383       8    87        256     1   24.8\n#&gt;  4 E02003012 E02003012   315       5   181        102     0   59.0\n#&gt;  5 E02003013 E02003013   318       7   165        112     0   54.1\n#&gt;  6 E02003014 E02003014   414      35   139        185     0   42.0\n#&gt;  7 E02003015 E02003015   240      18   142         61     0   66.7\n#&gt;  8 E02003016 E02003016   119       7    65         30     2   60.5\n#&gt;  9 E02003017 E02003017   147       8    70         60     1   53.1\n#&gt; 10 E02003018 E02003018    67       0    39         24     1   58.2\n#&gt; # ℹ 92 more rows\n#&gt; # A tibble: 2,808 × 8\n#&gt;    o         d           all bicycle  foot car_driver train Active\n#&gt;    &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1 E02002985 E02002987   121       7    35         62     0  34.7 \n#&gt;  2 E02002985 E02003036    32       2     1         10     1   9.38\n#&gt;  3 E02002985 E02003043   141       1     2         56    17   2.13\n#&gt;  4 E02002985 E02003049    56       2     4         36     0  10.7 \n#&gt;  5 E02002985 E02003054    42       4     0         21     0   9.52\n#&gt;  6 E02002985 E02003100    22       0     0         19     3   0   \n#&gt;  7 E02002985 E02003106    48       3     1         33     8   8.33\n#&gt;  8 E02002985 E02003108    31       0     0         29     1   0   \n#&gt;  9 E02002985 E02003121    42       1     2         34     0   7.14\n#&gt; 10 E02002985 E02006887   103       5     1         36    13   5.83\n#&gt; # ℹ 2,798 more rows\n\ndesire_lines = od2line(od_inter, zones_od)\n#&gt; Creating centroids representing desire line start and end points.\n# od2line : polygon으로 되어있는 두 지역의 중심점을 계산해서 linestring으로 변환\n#&gt; Creating centroids representing desire line start and end points.\nqtm(desire_lines, lines.lwd = \"all\")\n#&gt; Legend for line widths not available in view mode.\n\n\n\n\n\n\ndesire_lines\\(distance = as.numeric(st_length(desire_lines)) desire_carshort = dplyr::filter(desire_lines, car_driver &gt; 300 & distance &lt; 5000) route_carshort = route(l = desire_carshort, route_fun = route_osrm, osrm.profile = \"car\") # foot, bike, car desire_carshort\\)geom_car = st_geometry(route_carshort)\nplot(st_geometry(desire_carshort)) plot(desire_carshort$geom_car, col = “red”, add = TRUE) plot(st_geometry(st_centroid(zones_od)), add = TRUE)\n\ngetmap &lt;- get_googlemap(\"bristol\", zoom = 11)\nbristol_map &lt;- ggmap(getmap)\n\n# 센터 조정\ngetmap &lt;- get_googlemap(center = c(-2.56, 51.53), zoom = 12)\nbristol_map &lt;- ggmap(getmap)\nbristol_map + geom_sf(data = desire_carshort, inherit.aes = F) +\n  geom_sf(data = desire_carshort$geom_car,\n          inherit.aes = F,\n          col = \"red\") +\n  geom_sf(data = st_geometry(st_centroid(zones_od)), inherit.aes = F)\n\n\n\n\n\n도로교통공단 TAAS에서는 사망교통사고 정보를 공개하고 있음\n\n교통사고 일시 부터 30일이내 사망한 경우를 사망교통사고라 정의하고 사고정보를 선택한 조건에 따라 json/xml형식으로 제공\n사망 교통 사고 정보\n\n사망사고 년, 월, 일, 시, 주야\n사망사고 건수\n사망사고 사망자수, 부상자수, 중상자수, 경상자수, 부상신고자수\n사망사고 위치 좌표 및 지역명\n사망사고 유형, 위반사항, 차량 종류, 도로 형태\n\n\n데이터 불러오기(https://taas.koroad.or.kr/api/selectDeathDataSet.do)\n\n다운받은 데이터를 R로 불러온 뒤 데이터 속성 확인하세요. 어떤 정보가 있는지, 활용할 위치 정보가 있는지 확인하세요\n\n\nSys.setlocale(\"LC_ALL\",\"Korean\")\n#&gt; Warning in Sys.setlocale(\"LC_ALL\", \"Korean\"): using locale code page other than\n#&gt; 65001 (\"UTF-8\") may cause problems\n#&gt; [1] \"LC_COLLATE=Korean_Korea.949;LC_CTYPE=Korean_Korea.949;LC_MONETARY=Korean_Korea.949;LC_NUMERIC=C;LC_TIME=Korean_Korea.949\"\ngetwd()\n#&gt; [1] \"C:/Users/Hyunsoo Kim/Desktop/senior_grade/blog/my-quarto-website\"\nraw.data &lt;- read.csv(\"./Spatial_Information_Analysis/12_20_death.csv\", header = TRUE, fileEncoding = \"EUC-KR\")\n## 구조 확인\nstr(raw.data)\n#&gt; 'data.frame':    37128 obs. of  23 variables:\n#&gt;  $ 발생년               : int  2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 ...\n#&gt;  $ 발생년월일시         : int  2012010101 2012010101 2012010108 2012010110 2012010103 2012010116 2012010210 2012010104 2012010104 2012010102 ...\n#&gt;  $ 주야                 : chr  \"야간\" \"야간\" \"주간\" \"주간\" ...\n#&gt;  $ 요일                 : chr  \"일\" \"일\" \"일\" \"일\" ...\n#&gt;  $ 사망자수             : int  1 1 1 2 1 1 2 1 1 1 ...\n#&gt;  $ 사상자수             : int  1 6 1 2 1 1 2 1 2 4 ...\n#&gt;  $ 중상자수             : int  0 5 0 0 0 0 0 0 1 0 ...\n#&gt;  $ 경상자수             : int  0 0 0 0 0 0 0 0 0 3 ...\n#&gt;  $ 부상신고자수         : int  0 0 0 0 0 0 0 0 0 0 ...\n#&gt;  $ 발생지시도           : chr  \"서울\" \"전북\" \"충남\" \"경남\" ...\n#&gt;  $ 발생지시군구         : chr  \"은평구\" \"정읍시\" \"청양군\" \"합천군\" ...\n#&gt;  $ 사고유형_대분류      : chr  \"차대사람\" \"차대차\" \"차량단독\" \"차대차\" ...\n#&gt;  $ 사고유형_중분류      : chr  \"차도통행중\" \"정면충돌\" \"공작물충돌\" \"측면충돌\" ...\n#&gt;  $ 사고유형             : chr  \"차도통행중\" \"정면충돌\" \"공작물충돌\" \"측면충돌\" ...\n#&gt;  $ 법규위반             : chr  \"안전운전 의무 불이행\" \"중앙선 침범\" \"안전운전 의무 불이행\" \"과속\" ...\n#&gt;  $ 도로형태_대분류      : chr  \"단일로\" \"단일로\" \"단일로\" \"교차로\" ...\n#&gt;  $ 도로형태             : chr  \"기타단일로\" \"기타단일로\" \"기타단일로\" \"교차로내\" ...\n#&gt;  $ 당사자종별_1당_대분류: chr  \"승용차\" \"승용차\" \"승용차\" \"승합차\" ...\n#&gt;  $ 당사자종별_2당_대분류: chr  \"보행자\" \"승용차\" \"없음\" \"승용차\" ...\n#&gt;  $ 발생위치X_UTMK       : int  949860 946537 940016 1059321 1070222 1036880 1079124 1114053 911131 955269 ...\n#&gt;  $ 발생위치Y_UTMK       : int  1957179 1737695 1832833 1748774 1834630 1827821 1708218 1761943 1861851 1952221 ...\n#&gt;  $ 경도                 : num  127 127 127 128 128 ...\n#&gt;  $ 위도                 : num  37.6 35.6 36.5 35.7 36.5 ...\n## 테이블 확인\nView(raw.data)\n\n데이터 추출하기\n\n다운받은 데이터는 전국에 대한 사망교통사고 정보이다. 대전지역에 2016년부터 2020년까지의 정보만을 추출하세요.\n\n추출한 데이터의 경도, 위도에 결측값 및 0인 데이터가 있는지 확인하세요.\n\n\n## 1. 대전 지역 2016 ~ 2020년 데이터 추출\ndaejeon &lt;- filter(raw.data,  발생지시도 == \"대전\" &  발생년 &gt; 2015)\n\n## 2. 사고 발생 시작점 경도/위도 데이터의 범위 살펴보기\nrange(daejeon$경도) ; range(daejeon$위도)\n#&gt; [1] 127.2653 127.5278\n#&gt; [1] 36.22335 36.45634\n\n## 3. 경도/위도 데이터가 NA인 데이터 확인하기\nsum(is.na(daejeon$경도)) ; sum(is.na(daejeon$위도))\n#&gt; [1] 0\n#&gt; [1] 0\n\n## 4. 경도/위도 데이터가 0인 데이터 확인하기\ndaejeon[daejeon$경도 == 0,]\n#&gt;  [1] 발생년                발생년월일시          주야                 \n#&gt;  [4] 요일                  사망자수              사상자수             \n#&gt;  [7] 중상자수              경상자수              부상신고자수         \n#&gt; [10] 발생지시도            발생지시군구          사고유형_대분류      \n#&gt; [13] 사고유형_중분류       사고유형              법규위반             \n#&gt; [16] 도로형태_대분류       도로형태              당사자종별_1당_대분류\n#&gt; [19] 당사자종별_2당_대분류 발생위치X_UTMK        발생위치Y_UTMK       \n#&gt; [22] 경도                  위도                 \n#&gt; &lt;0 rows&gt; (or 0-length row.names)\ndaejeon[daejeon$위도 == 0,]\n#&gt;  [1] 발생년                발생년월일시          주야                 \n#&gt;  [4] 요일                  사망자수              사상자수             \n#&gt;  [7] 중상자수              경상자수              부상신고자수         \n#&gt; [10] 발생지시도            발생지시군구          사고유형_대분류      \n#&gt; [13] 사고유형_중분류       사고유형              법규위반             \n#&gt; [16] 도로형태_대분류       도로형태              당사자종별_1당_대분류\n#&gt; [19] 당사자종별_2당_대분류 발생위치X_UTMK        발생위치Y_UTMK       \n#&gt; [22] 경도                  위도                 \n#&gt; &lt;0 rows&gt; (or 0-length row.names)\n\n\n\n\n\n\n\n\n\n## 5. 년도별 사고 위치 정보 지도 상에 표출하기\nlibrary(ggmap)\nregister_google(key = 'AIzaSyB4jjrVVAzb9fl8FQrQqUONAsaRBppWuSA')\nmap &lt;- qmap(location = enc2utf8(\"대전\"),\n            zoom = 12,\n            maptype = \"roadmap\")\np &lt;-\n  map + geom_point(\n    data = daejeon,\n    aes(x = 경도, y = 위도, colour = factor(발생년)),\n    size = 2,\n    alpha = 0.7\n  )\np + ggtitle(\"대전시 사망사고 위치(2016-2020)\")\n\n\nstat_bin2d() 함수 활용하여 Grid 내 사고횟수 출력\n\n\n## stat_bin2d() 함수 활용하여 특정 영역 내 사고 횟수 출력\np &lt;- map + stat_bin2d(data = daejeon,\n                      aes(x = 경도, y = 위도),\n                      bins = 30,   # bins : grid의 개수\n                      alpha = 0.5) # binwidth 로도 가능\np\n## stat_bin2d() 함수 활용하여 특정 영역 내 사고 횟수 출력/위성지도/컬러 변경\nmap &lt;- qmap(location = \"Daejeon\",\n            zoom = 12,\n            maptype = \"satellite\")\np &lt;- map + stat_bin2d(data = daejeon,\n                      aes(x = 경도, y = 위도),\n                      bins = 30,\n                      alpha = 0.5) # binwidth 로도 가능\np + scale_fill_gradient(low = \"yellow\", high = \"red\")\n\n\n\n\nGrid 내에 Count된 값 및 위치 확인하기\n\n\np_count &lt;- ggplot_build(p)$data[[4]] # cell의 값 출력\np_count &lt;- arrange(p_count, desc(value))\nhead(p_count)\n\n\nGrid 내(중심)에 Count값 표출\n\n\np + scale_fill_gradient(low = \"yellow\", high = \"red\") +\n  geom_text(data = p_count, aes((xmin + xmax) / 2, (ymin + ymax) / 2,\n                                label = count), col = \"white\")\n\n\n사고 유형 별로 표시하기\n\n\np &lt;-\n  map + stat_bin2d(\n    data = daejeon,\n    aes(\n      x = 경도,\n      y = 위도,\n      colour = factor(사고유형),\n      fill = factor(사고유형)\n    ),\n    bins = 30,\n    alpha = 0.5\n  )\np\n\n\nstat_density2d() 함수 활용하여 등고선으로 지도 위에 출력하기\n\n\nmap &lt;-\n  qmap(\n    location = \"Daejeon\",\n    zoom = 12,\n    maptype = 'roadmap',\n    color = 'bw'\n  )\np &lt;-\n  map + stat_density2d(\n    data = daejeon,\n    aes(x = 경도, y = 위도, fill = ..level..),\n    bins = 5,\n    alpha = 0.45,\n    size = 2,\n    geom = \"polygon\"\n  )\n# level : 레벨이 높을수록 더 진한색, size : 선 굵기, bins: 선 간격\np\n\n\ngeom_hex() 함수 활용하여 벌집 블롯으로 출력하기\n\n\n## 벌집 블롯으로 출력(geom_hex(), scale_fill_gradientn())\nlibrary(hexbin)\nmap &lt;- qmap(location = \"Daejeon\",\n            zoom = 11,\n            maptype = \"roadmap\")\np &lt;-\n  map + coord_cartesian() + # coord_cartesian() : 데카르트 좌표계\n  geom_hex(\n    data = daejeon,\n    aes(x = 경도, y = 위도),\n    bins = 12,\n    alpha = 0.6,\n    color = \"white\"\n  ) # geom_hex() only works with Cartesian coordinates\np\np &lt;- p + scale_fill_gradientn(colours = terrain.colors(15))\np\n\n# binwidth로 출력\np &lt;-\n  map + coord_cartesian() + geom_hex(\n    data = daejeon,\n    binwidth = c(0.05, 0.05), # binwidth : bin의 크기 설정\n    aes(x = 경도, y = 위도),\n    alpha = 0.6,\n    color = \"white\"\n  ) # geom_hex() only works with Cartesian coordinates\np\np &lt;- p + scale_fill_gradientn(colours = terrain.colors(15))\np\n\np_count &lt;- ggplot_build(p)$data[[4]] # cell의 값 출력\np_count &lt;- arrange(p_count, desc(count))\nhead(p_count)\n\n\n\n\n\n\n행정구역시군구 경계를 얻기 위해 데이터로 대전시 구 경계 shape 파일 획득\n\n\nlibrary(raster)\nlibrary(rgdal)\nlibrary(sf)\n\n## 동별 사망사고 추출하기\nmap &lt;-\n  qmap(\n    location = \"Daejeon\",\n    zoom = 11,\n    maptype = 'roadmap',\n    color = 'bw'\n  )\n\ndaejeon_area &lt;- shapefile('./Spatial_Information_Analysis/LARD_ADM_SECT_SGG_30/LARD_ADM_SECT_SGG_30.shp')\ndaejeon_area # 좌표체계 확인\n# str(daejeon_area)\nplot(daejeon_area, axes = T)\n\n\n위 plot의 좌표단위를 보면 평면직각좌표계(Projected Coordinate)를 기준으로 측정할 때 나올 수 있는 단위\n앞에서 사고 데이터의 좌표는 위경도 좌표이므로, 두 자료의 위치 좌표체계를 통일 시켜줄 필요가 있음\nspTransform() 를 통해 좌표변형 가능\n\nto_crs = CRS(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\")\n\n\nto_crs = CRS(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\")\ndaejeon_area2 &lt;- spTransform(daejeon_area, to_crs)\ndaejeon_area2\ndaejeon_area2@data # SP 데이터 내에서 출력을 하려면 @로 호출해야함\n\nplot(daejeon_area2, axes = T)\nmap + geom_polygon(\n  data = daejeon_area2,\n  aes(x = long, y = lat, group = group),\n  fill = 'white',\n  color = 'black'\n)\n\n구를 기준으로 사고 발생 횟수 계산\n\n\ngu_accident &lt;- daejeon %&gt;% group_by(발생지시군구) %&gt;% summarise(n = n())\ngu_accident\n#&gt; # A tibble: 5 x 2\n#&gt;   발생지시군구     n\n#&gt;   &lt;chr&gt;        &lt;int&gt;\n#&gt; 1 대덕구          81\n#&gt; 2 동구            98\n#&gt; 3 서구           100\n#&gt; 4 유성구          73\n#&gt; 5 중구            58\n\n\ndaejeon_area2 객체의 클래스는 SpatitalPloygonsDataFrame임\n이것을 데이터 프레임 형태로 변환해줄 때 사용하는 함수로는 ggplot2 패키지의 fortify() 함수가 있음\n구를 나타내는 SGG_NM 열로 기준\n\n\nclass(daejeon_area2)\ndaejeon_area2 &lt;- fortify(daejeon_area2, region = 'SGG_NM')\nclass(daejeon_area2)\nhead(daejeon_area2)\n\n\ndaejeon_area2의 “id”열과 gu_accident의 “발생지시군구”열을 기준으로 합치기 위해서 열Name을 “id”로 통일\nid열을 기준으로 두 데이터셋을 합쳐줌\n\n\nnames(gu_accident)[1] &lt;- \"id\"\ndaejeon_area3 &lt;- merge(daejeon_area2, gu_accident, by = 'id')\nhead(daejeon_area3)\n\ndaejeon_area3 %&gt;% group_by(id) %&gt;% summarise(n = mean(n))\n\n\ngeom_polygon()을 이용한 시각화\n\n\np &lt;-\n  map + geom_polygon(data = daejeon_area3,\n                     aes(\n                       x = long,\n                       y = lat,\n                       group = group,\n                       fill = n\n                     ),\n                     alpha = .5)\np\np + scale_fill_gradient(low = 'yellow', high = 'red')\nlibrary(viridis)\np + scale_fill_viridis()\n\n\n\n\n\n\n\n\n구경계 데이터(daejeon_area2)를 sf클래스로 변환\nst_as_sf() : sp클래스를 sf클래스로 변환\n\n\ndaejeon_area2 &lt;- spTransform(daejeon_area, to_crs)\ndaejeon_area2\ndaejeon_area_sf &lt;- st_as_sf(daejeon_area2) # sp 클래스를 sf 클래스로 전환하기\ndaejeon_area_sf\nplot(st_geometry(daejeon_area_sf))\n\n\nst_point_on_surface() : 각 구별 지도상 중심점 구한 뒤 지도상에 표출\n\n\n# 각 구별 Center\ndaejeon_area_center &lt;- st_point_on_surface(daejeon_area_sf)\nplot(st_geometry(daejeon_area_sf))\nplot(daejeon_area_center , add = T, col = \"black\")\n\n\n사망사고데이터(point)를 sf클래스로 변환\n\n\ndaejeon_acc_sf &lt;-\n  daejeon %&gt;% st_as_sf(coords = c(\"경도\", \"위도\"),\n                       crs = 4326,\n                       remove = FALSE)\ndaejeon_acc_sf ## CRS : # WGS84\n\n# daejeon_acc &lt;- daejeon %&gt;% st_as_sf(coords = c(\"발생위치X_UTMK\", \"발생위치Y_UTMK\"),\n#                                     crs = 4326,\n#                                     remove = FALSE)\n# daejeon_acc\n\n\nst_intersection을 통해서 폴리곤(구경계)와 포인트(사망사고지점)데이터 합치기\n\n\n## Intersection between polygon and points\nintersection &lt;- st_intersection(daejeon_area_sf, daejeon_acc_sf)\nhead(intersection)\n\n## Plot intersection\nplot(st_geometry(daejeon_area_sf))\nplot(intersection, add = T, pch = 1)\n\n\n구별 사망사고 건수 Count하기\n\n\n## View result\ntable(intersection$SGG_NM)\n\n## Using dplyr\nint_result &lt;- intersection %&gt;%\n  group_by(SGG_NM) %&gt;%\n  count()\nint_result\n\n\nst_join() : 경계 데이터(daejeon_area_sf)에 결과(int_result) 합치기\n\n\nint_result0 &lt;- st_join(daejeon_area_sf, int_result)\nint_result0\n\n\nmap 위에 시각화\n\n\nmap &lt;-\n  qmap(\n    location = \"Daejeon\",\n    zoom = 11,\n    maptype = 'roadmap',\n    color = 'bw'\n  )\np &lt;-\n  map + geom_sf(data = int_result0,\n                inherit.aes = F, # sf형태 data 그릴 때 반드시 필요\n                aes(fill = n),\n                alpha = .5)\np\np + scale_fill_gradient(low = 'yellow', high = 'red')"
  },
  {
    "objectID": "Spatial_Information_Analysis.html#chapter-1-r을-이용한-공간정보-분석",
    "href": "Spatial_Information_Analysis.html#chapter-1-r을-이용한-공간정보-분석",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "공간정보란 ? 사람들이 생활하고 있는 공간 상에서 사건이나 사물에 대한 위치를 나타내는 정보\n\n위치를 나타내는 정보는 (1) 위치를 표현하는 정보 (2) 해당 위치에 나타나는 특성에 대한 정보\n\n위치를 표현하는 정보 : 공간 상에서 사건이나 사물의 위치가 어디에 있는지를 나타내는 정보\n\nex) 주소, 위경도, x,y 좌표 등\n\n해당 위치에 나타나는 특성에 대한 정보 : 특정 위치에 있는 사건이나 사물을 설명하는 정보\n\nex) 학교, 회사, 학생 수 , 교사 수, 사고 건 수, 사고 유형 등\n\n\n\n지리정보 시스템(Geographic Information System) : 공간정보데이터를 처리, 가공하여 새로운 정보를 도출하는 일련의 과정 또는 기법\n\nex) 교통사고 데이터 분석 (TIMS)\n\n공간정보를 이용하여 GIS 분석을 수행하기 위한 소프트웨어\n\n전용 소프트웨어\n\nArcGIS : 전문적인 공간정보의 처리와 분석 가능, 고가(유료)\nQGIS : 오픈소스 GIS 소프트웨어, 최근 많은 분야에서 GIS 소프트웨어로 활용\n\n오픈소스 소프트웨어\n\nR 소프트웨어 : 오픈소스 기반의 통계 프로그램, 공간정보의 처리와 븐석에도 강력한 기능\nPython 소프트웨어 : 배우기 쉽고, 강력한 프로그래밍 언어, 공간정보를 다루는데 유용한 라이브러리가 개발\n\n\n\n\n\n\n\n위치정보와 속성정보로 구분\n\n위치정보\n\n좌표체계를 이용한 위치정보\n\n지리좌표계에서 이용하는 경도와 위도로 표현 ex) 경위도좌표\n수학적으로 X좌표와 Y좌표로 위치 정보를 표현 ex) 평면직각좌표(지도좌표)\n\n공간정보 데이터의 위치정보 표현 방식\n\n벡터 (점, 선, 면)\n래스터 (일정한 격자 또는 화소)\n\n\n속성정보\n\n주어진 위치에 있는 사건이나 사물에 대한 자료\n\n\n\n\n\n\n\n지리좌표체계 : 경도와 위도로 위치를 표현하는 지리좌표체계\n투영좌표체계 : 지도투영법을 적용하여 둥근 지구를 평면으로 변환한 후, 직각좌표체계를 이용하여 x좌표와 y좌표의 직각좌표체계로 위치를 표현\n\n원통도법, 원추도법, 평면도법이 있음.\nUTM 좌표체계, TM 좌표계, UTM-K 좌표계\n우리나라는 ITRF2000 지구중심좌표계를 따르고 타원체로는 GRS80 타원체를 적용\n\n\n\n\n\n\nshapefile\n\n.shp : 공간정보(점, 선, 다각형)\n.shx : geometry와 속성 정보 연결\n.dbf : 속성정보\n.drj : 좌표계 정보 저장\n.sbn : 위치 정보 저장\n\ngeojson : json 또는 xml 파일 포맷 필요요"
  },
  {
    "objectID": "Spatial_Information_Analysis.html#chapter2-geographic-data-in-r",
    "href": "Spatial_Information_Analysis.html#chapter2-geographic-data-in-r",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "패키지\n\nsf : 지리 공간 벡터 데이터(vector data) 분석을 위한 패키지\nraster : 지리 공간 레스터 데이터(raster data)를 처리 및 분석하는데 사용\nspData : 37개의 지리 공간 데이터셋이 내장\nspDataLarge : 지리공간 데이터 샘플을 내장\n\nvignetee(package = \" \") : 설치된 모든 패키지에 대한 이용가능한 모든 목록을 출력\nst_as_sf() : st 데이터를 sf로 변환하는 함수\nst_centroid : 폴리곤의 중심점을 계산하는 함수\nplot 함수 위에 다른 지도 층을 추가 : plot() 함수 안에 add = TRUE 사용\n\n\n\n\nst_point() : A point\nst_linestring() : A linestring\nst_polygon() : A polygon\nst_multipoint() : A multipoint\nst_multilinestring() : A multilinestring\nst_multipolygon() : A multipolygon\nst_geometrycollection() : A geometry collection\n\n\n\n\n\nst_sfc() : 두 개의 지리특성(feature)을 하나의 칼럼 객체로 합치는 함수\nst_geometry_type() : 기하유형을 확인\nst_crs() : 특정 CRS를 지정\n\n특정 CRS를 지정하기 위해 epsg(SRID) 또는 proj4string 속성을 사용\n\nepsg 코드\n\n장점 : 짧아서 기억하기 쉬움\nsfc 객체 내의 모든 geometries는 동일한 CRS를 가져야 함.\nEPSG : 4326 : GPS가 사용하는 좌표계\n\nproj4string 정의\n\n장점 : 투사 유형이나 datum, 타원체 등의 다른 모수들을 구체화할 수 있는 유연성이 있음\n단점 : 사용자가 구체화를 해야하므로 길고 복잡하며 기억하기 어려움\n\nst_sf() : sfc와 class sf의 객체들을 하나로 통합\n\n\nlibrary(raster)\nlibrary(rgdal)\n\nraster_filepath &lt;- system.file(\"raster/srtm.tif\", package = \"spDataLarge\")\nnew_raster &lt;- raster(raster_filepath)\nnew_raster\n# class      : RasterLayer\n# dimensions : 457, 465, 212505  (nrow, ncol, ncell)\n# resolution : 0.0008333333, 0.0008333333  (x, y)\n# extent     : -113.2396, -112.8521, 37.13208, 37.51292  (xmin, xmax, ymin, ymax)\n# crs        : +proj=longlat +datum=WGS84 +no_defs\n# source     : srtm.tif\n# names      : srtm\n# values     : 1024, 2892  (min, max)\n\n\ndim() : 행, 열, 층의 수\nncell() : 셀의 수\nres() : 해상도\nextent() : 경계값\ncrs() : 좌표계\ninMemory() : 래스터 데이터가 메모리에 저장되어 있는지(논리값 출력)\n\n\n\n\n\nRasterLayer class\nRasterBrick Class\nRasterStack class\n\n\nRasterLayer : 한 개의 층으로 구성되어 있는 래스터\nRasterBrick : 여러개의 층으로 구성되어 있는 래스터\n\n단일 다중 스펙트럼 위성 파일, 메모리의 단일 다층 객체의 형태\nbrick() 함수를 사용하여 다층 래스터 파일을 로드\n\nRasterStack : 여러개의 층으로 구성되어 있는 래스터\nnlayers() : 래스터 데이터의 층의 수\n\n\n\n\nRasterBrick : 동일한 복수 개의 RasterLayer 층으로 구성\nRasterStack : 여러 개의 RasterLayer과 RasterBrick 객체가 혼합\n\n\n\n\n\nRasterBrick : 하나의 다층 래스터 파일이나 객체를 처리\nRasterStack : 여러 개의 래스터 파일들이나 여러 종류의 래스터 클래스를 한꺼번에 연걸해서 연산하고 처리\n\n\n\n\n\n\n지리 좌표계\n\n위도와 경도를 이용해 지구 표면의 위치를 정의\n미터가 아니라, 각도로 거리 측정\n타원 표면, 구면 표면\nWGS84\n\n투영(투사) 좌표계\n\n암묵적으로 “평평한 표면” 위의 데카르트 좌표 기반 -&gt; 왜곡 발생\n원점, x축, y축\n미터와 같은 선형 측정 단위\n평면, 원뿔, 원통의 3가지 투영 유형\n\nst_set_crs() : 좌표계가 비어있거나 잘못 입력되어 있는 경우에 좌표계를 설정\nst_transform() : 투영 데이터 변환\nst_area() : 벡터 데이터의 면적 계산 -&gt; [m^2] 단위가 같이 반환\n좌표계 설정할 때,\n\n벡터 데이터 : epsg코드나 proj4string정의 모두 사용 가능\n래스터 데이터 : proj4string 정의만 사용"
  },
  {
    "objectID": "Spatial_Information_Analysis.html#chapter3-attribute-data-operations",
    "href": "Spatial_Information_Analysis.html#chapter3-attribute-data-operations",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "sf 객체에서 속성 정보만 가져오기 : st_drop_geometry()\n\n\nBase R 구문으로 벡터 데이터 속성 정보의 행과 열 가져오기\n\n\ndplyr로 벡터 데이터 속성 정보의 행과 열 가져오기\n\n\n한 개 컬럼만 가져온 결과를 벡터로 반환하기\n\n\n\n\n\n지리공간 sf 객체는 항상 점, 선, 면 등의 지리기하 데이터를 리스트로 가지고 있는 geometry 칼럼이 항상 따라다님\nsf 객체로부터 이 geometry 칼럼을 제거하고 나머지 속성 정보만으로 Dataframe을 만들고 싶다면 sf패키지의 st_drop_geometry()를 사용\ngeometry 칼럼의 경우 지리기하 점, 선, 면 등의 리스트 정보를 가지고 있어 메모리 점유가 크기때문에, 사용할 필요가 없다면 geometry 칼럼을 제거하고 속성 정보만으로 Dataframe으로 만들어서 분석을 진행하는게 좋음\n\n\n\n\n\nR Dataframe에서 i행과 j열을 가져올 때 : df[i, j], subset(), $을 사용\n\n\ni행과 j열 위치를 지정 ex) world[1:6, ]\n\n\nj행의 이름을 이용 ex) world[, c(\"name_long\", \"lifeExp\")]\n\n\n논리 벡터를 사용해서 i행의 부분집합 ex) sel_area &lt;- world$area_km2 &lt; 10000\n\n\n\n\n\n\n\ndplyr 패키지에서는 체인(%&gt;%)으로 파이프 연산자를 사용하여 가독성이 좋고, 속도가 빠름\n\n\nselect() 함수를 사용하여 특정 열 선택\n\n\nselect(sf, name)\nselect(sf, name1:name2)\nselect(sf, position) ex) select(world, 2, 7)\nselect(sf, -name)\nselect(sf, name_new = name_old) : 열 선택하여 이름 변경\nselect(sf, contain(string)) : 특정 문자열을 포함한 칼럼을 선택\n\ncontain(), starts_with(), ends_with(), matches(), num_range()\n\n\n\nfilter() 함수를 사용하여 조건을 만족하는 특정 행 추출\n\n\nsubset() 함수와 동일한 기능\n\n\naggregate() 함수를 사용하여 지리 벡터 데이터의 속성 정보를 그룹별로 집계\n\n\naggregate(x ~ group, FUN, data, ...)\ndata.frame을 반환하며, 집계된 결과에 지리 기하(geometry) 정보는 없음\nworld[‘pop’]은 “sf” 객체이기 때문에 집계 결과가 “sf” 객체로 반환\nworld$pop은 숫자형 벡터이므로 aggregate() 함수를 적용하면 집계 결과가 “data.frame”으로 반환\n\n\nsummarize(), group_by() 함수를 이용한 지리벡터 데이터의 속성 정보를 그룹별로 집계\n\n\ngroup_by() : 기준이 되는 그룹을 지정\nsummarize() : 다양한 집계 함수를 사용\n\nsum(), n() : 합계와 개수 집계\ntop_n() : 상위 n개 추출\narrange() : 오름차순 정렬, desc()를 사용하면 내림차순 정렬\nst_drop_geometry() : geometry 열 제거\n\n\n\n\n\n\n\n\nR의 sf클래스 객체인 지리공간 벡터 데이터를 dplyr의 함수를 사용해서 두 테이블을 join하면 속성과 함께 지리공간 geometry 칼럼과 정보도 join된 후의 테이블에 자동으로 그대로 따라감\n\n\n## 두 데이터 셋에 같은 이름을 가지는 변수가 없는 경우\n\n```         \na)  하나의 key variable의 이름을 바꿔서 통일시켜줌\n```\n\n-   \n\n    b)  `by`를 사용하여 결합변수를 지정\n\n\n\n# coffee_data의 name_long변수 이름을 nm으로 변경\ncoffee_renamed &lt;- rename(coffee_data, nm = name_long)\n# by 사용하여 결합 변수를 지정하여 다른이름변수를 기준으로 조인하기\nworld_coffee1 &lt;- left_join(world, coffee_renamed, by = c(name_long = \"nm\"))\n\n\ninner_join() 함수를 사용하면 겹치는 행만 추출\n\nsetdiff() : 일치하지 않는 행 추출\ngrepl() : 텍스트 찾는 함수 (논리값으로 출력)\ngrep() : 텍스트 찾는 함수 (행 번호 출력)\n\n\n\n\n\n\ndplyr로 지리 벡터 데이터에 새로운 속성 만들기\n\nmutate() : 기존 데이터 셋에 새로 만든 변수(열) 추가\ntransmute() : 기존의 열은 모두 제거하고 새로 만든 열과 지리기하 geometry열만을 반환\n\ntidyr로 지리 벡터 데이터의 기존 속성을 합치거나 분리하기\n\nunite(data, 병합 열, sep = \"_\", remove = TRUE) : 기존 속성 열을 합쳐서 새로운 속성 열을 만듦\n\nremove = TRUE를 설정해주면 기존의 합치려는 두 개의 열은 제거되고, 새로 만들어진 열만 남음\n\nseparate() : 기존에 존재하는 열을 구분자를 기준으로 두 개의 열로 분리\n\n\n\nworld_unite &lt;- world %&gt;%\n  unite(\"con_reg\", continent:region_un, sep = \":\", remove = TRUE)\nnames(world_unite)\n# \"iso_a2\"    \"name_long\" \"con_reg\"   \"subregion\" \"type\"\n# \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\" \"geom\"\n\nworld_separate &lt;- world_unite %&gt;%\n  separate(con_reg, c(\"continent\", \"region_un\"), sep = \":\")\nnames(world_separate)\n# \"iso_a2\"    \"name_long\" \"continent\" \"region_un\" \"subregion\" \"type\"\n# \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\" \"geom\" \n\ndplyr로 지리 벡터 데이터의 속성 이름 바꾸기\n\nrename(data, new_name = old_name) : 특정 속성 변수 이름 변경\nsetNames(object = nm, nm) : 여러개의 속성 칼럼을 한꺼번에 변경 또는 부여\n\n\nworld %&gt;% rename(name = name_long)\n\nnew_names &lt;- c(\"i\", \"n\", \"c\", \"r\", \"s\", \"t\", \"a\", \"p\", \"l\", \"gP\", \"geom\")\nworld %&gt;% setNames(new_names)\n\n\n\n\n\n\n\n래스터 객체의 데이터 속성은 숫자형(numeric), 정수형(integer), 논리형(logical), 요인형(factor) 데이터를 지원하며, 문자형(character)은 지원하지 않음\n\n1.  **문자형을 요인형으로 변환**(또는 논리형으로 변환) -\\&gt; `factor()` 함수 사용\n\n\n요인형 값을 속성 값으로 하여 래스터 객체를 만듦\n\n\n래스터 객체의 모든 값을 추출하거나 전체 행을 추출 : values(), getValues()"
  },
  {
    "objectID": "Spatial_Information_Analysis.html#chapter4-spatial-data-operations",
    "href": "Spatial_Information_Analysis.html#chapter4-spatial-data-operations",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "st_intersects() : 공간 부분집합 추출(교집합)\n\n\n\n\n\n\n\n\n\n\nst_intersects() : 공간적으로 관련이 있는 객체를 출력\nst_disjoint() : 공간적으로 관련되지 않은 객체만 반환\nst_within() : 공간적으로 완전히 객체 내부에 있는 객체들만 출력\nst_touches() : 공간적으로 테두리에 있는 객체들만 출력\nst_is_within_distance() : 공간적으로 주어진 거리보다 가까운 객체들을 반환\nsparse = FALSE 매개변수를 설정하면 논리값으로 출력\n\n\nst_intersects(p, a)\n#&gt; Sparse geometry binary predicate list of length 4, where the predicate\n#&gt; was `intersects'\n#&gt;  1: 1\n#&gt;  2: 1\n#&gt;  3: (empty)\n#&gt;  4: (empty)\n\nst_intersects(p, a, sparse = FALSE)\n#&gt;       [,1]\n#&gt; [1,]  TRUE\n#&gt; [2,]  TRUE\n#&gt; [3,] FALSE\n#&gt; [4,] FALSE\n\nst_disjoint(p, a, sparse = FALSE)[, 1]\n#&gt; [1] FALSE FALSE  TRUE  TRUE\n\nst_within(p, a, sparse = FALSE )[, 1]\n#&gt; [1]  TRUE FALSE FALSE FALSE\n\nst_touches(p, a, sparse = FALSE)[, 1]\n#&gt; [1] FALSE  TRUE FALSE FALSE\n\nsel &lt;- st_is_within_distance(p, a, dist = 0.9) # can only return a sparse matrix\nlengths(sel) &gt; 0\n#&gt; [1]  TRUE  TRUE FALSE  TRUE\n\n\n\n\n\nst_join() : 공간 결합 함수\n\n\nrandom_joined = st_join(random_points, world[\"name_long\"]) ; random_joined\n#&gt; Simple feature collection with 10 features and 1 field\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -158.1893 ymin: -42.91501 xmax: 165.1157 ymax: 80.5408\n#&gt; Geodetic CRS:  WGS 84\n#&gt; # A tibble: 10 × 2\n#&gt;                 geometry name_long\n#&gt;  *           &lt;POINT [°]&gt; &lt;chr&gt;    \n#&gt;  1 (-58.98475 -21.24278) Paraguay \n#&gt;  2  (-13.05963 25.42744) Morocco  \n#&gt;  3   (-158.1893 80.5408) &lt;NA&gt;     \n#&gt;  4  (-108.9239 27.80098) Mexico   \n#&gt;  5   (-9.246895 49.9822) &lt;NA&gt;     \n#&gt;  6  (-71.62251 20.15883) &lt;NA&gt;     \n#&gt;  7  (38.43318 -42.91501) &lt;NA&gt;     \n#&gt;  8  (-133.1956 6.053818) &lt;NA&gt;     \n#&gt;  9   (165.1157 38.16862) &lt;NA&gt;     \n#&gt; 10   (16.86581 53.86485) Poland\n\n\n\n\n\n\n기호(plotting symbols, characters) : pch\n기호의 크기 : cex\n선 두께 : lwd\n선 유형 : lty\n\n\n\n\nany() : 특정 값이 포함되어 있는지 확인할 때 유용, 여기서 TRUE가 있는지 확인 가능\n\n\nany(st_touches(cycle_hire, cycle_hire_osm, sparse = FALSE))\n#&gt; [1] FALSE\n\n\nlibrary(mapview)\nlibrary(tmap)\ntmap_mode(\"view\")\ntm_basemap(\"Stamen.Terrain\") +\n  tm_shape(cycle_hire) +\n  tm_symbols(col = \"red\", shape = 16, size = 0.5, alpha = .5) +\n  tm_shape(cycle_hire_osm) +\n  tm_symbols(col = \"blue\", shape = 16, size = 0.5, alpha = .5) +\n  tm_tiles(\"Stamen.TonerLabels\")\n\n\n\n\n\n\n\nst_transform() : 투영데이터로 변환을 위한 함수\nst_is_within_distance() : 임계 거리보다 가까운 객체들을 반환\n\n\ncycle_hire_P &lt;- st_transform(cycle_hire, 27700)\ncycle_hire_osm_P &lt;- st_transform(cycle_hire_osm, 27700)\nsel &lt;- st_is_within_distance(cycle_hire_P, cycle_hire_osm_P, dist = 20)\nsummary(lengths(sel) &gt; 0)\n#&gt;    Mode   FALSE    TRUE \n#&gt; logical     304     438\n\n\nst_join()을 사용하여 dist 인수를 추가하여 구할 수도 있음\n\nst_join()을 사용하면 조인된 결과의 행 수가 더 크다.\n이는 cycle_hire_P의 일부 자전거 대여소가 cycle_hire_osm_P와 여러개가 겹치기 때문임\n겹치는 점에 대한 값을 집계하고 평균을 반환하여 문제를 해결 가능\n\n\nz = st_join(cycle_hire_P, cycle_hire_osm_P,\n            join = st_is_within_distance, dist = 20)\nnrow(cycle_hire) ; nrow(z)\n#&gt; [1] 742\n#&gt; [1] 762\n\nz = z %&gt;%\n  group_by(id) %&gt;%\n  summarize(capacity = mean(capacity))\nnrow(z) == nrow(cycle_hire)\n#&gt; [1] TRUE\n\n\n\n\n\n\naggregate()와 group_by() %&gt;% summarize()를 활용하여 그룹별 통계값 계산(평균, 합 등)\n\n\n# aggregate() 사용\nnz_avheight &lt;- aggregate(x = nz_height, by = nz, FUN = mean)\nplot(nz_avheight[2])\n\n\n\n# group_by() %&gt;% summarize() 사용\nnz_avheight2 &lt;- nz %&gt;%\n  st_join(nz_height) %&gt;%\n  group_by(Name) %&gt;%\n  summarize(elevation = mean(elevation, na.rm = TRUE))\nplot(nz_avheight2[2])\n\n\n\n\n\nst_interpolate_aw() : 면적의 크기에 비례하게 계산(면적 가중 공간 보간)\n\n\nsum(incongruent$value)\n#&gt; [1] 45.41184\n\nagg_aw = st_interpolate_aw(incongruent[, \"value\"],\n                           aggregating_zones,\n                           extensive = TRUE)\n#&gt; Warning in st_interpolate_aw.sf(incongruent[, \"value\"], aggregating_zones, :\n#&gt; st_interpolate_aw assumes attributes are constant or uniform over areas of x\nagg_aw$value\n#&gt; [1] 19.61613 25.66872\n\n\n\n\n\n위상 관계는 binary인 반면 거리 관계는 연속적임\nst_distance() : 두 객체 사이의 거리 계산\n\n\nnz_heighest &lt;- nz_height %&gt;% top_n(n = 1, wt = elevation)\ncanterbury_centroid &lt;- st_centroid(canterbury)\n#&gt; Warning: st_centroid assumes attributes are constant over geometries\n\nst_distance(nz_heighest, canterbury_centroid)\n#&gt; Units: [m]\n#&gt;        [,1]\n#&gt; [1,] 115540\n\nco &lt;- filter(nz, grepl(\"Canter|Otag\", Name))\nst_distance(nz_height[1:3, ], co)\n#&gt; Units: [m]\n#&gt;           [,1]     [,2]\n#&gt; [1,] 123537.16 15497.72\n#&gt; [2,]  94282.77     0.00\n#&gt; [3,]  93018.56     0.00\n\nplot(st_geometry(co)[2])\nplot(st_geometry(nz_height)[2:3], add = TRUE)\n\n\n\n\n\n\n\n\n\n\n\ncellFromXY() or raster::extract() : 좌표값을 Cell ID로 변환\n\n\n\n\n\n\n\nid = cellFromXY(elev, xy = matrix(c(0.1, 0.1), ncol = 2))\nelev[id]\n#&gt;   elev\n#&gt; 1   16\nterra::extract(elev, matrix(c(0.1, 0.1), ncol = 2))\n#&gt;   elev\n#&gt; 1   16\n\nclip = rast(xmin = 0.9, xmax = 1.8, ymin = -0.45, ymax = 0.45,\n            res = 0.3, vals = rep(1, 9))\nelev[clip]\n#&gt;   elev\n#&gt; 1   18\n#&gt; 2   24\nterra::extract(elev, ext(clip))\n\n\noperator는 raster의 다양한 inputs을 받고, drop=FALSE로 설정했을 때, raster 객체를 반환\n\n\nelev[1:2]\n#&gt;   elev\n#&gt; 1    1\n#&gt; 2    2\nelev[2, 1:2]\n#&gt;   elev\n#&gt; 1    7\n#&gt; 2    8\nelev[1:2, drop = FALSE] # spatial subsetting with cell IDs\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 1, 2, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 0.5, 0.5  (x, y)\n#&gt; extent      : -1.5, -0.5, 1, 1.5  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : lon/lat WGS 84 (EPSG:4326) \n#&gt; source(s)   : memory\n#&gt; name        : elev \n#&gt; min value   :    1 \n#&gt; max value   :    2\nelev[2, 1:2, drop = FALSE] # spatial subsetting by row,column indices\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 1, 2, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 0.5, 0.5  (x, y)\n#&gt; extent      : -1.5, -0.5, 0.5, 1  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : lon/lat WGS 84 (EPSG:4326) \n#&gt; source(s)   : memory\n#&gt; name        : elev \n#&gt; min value   :    7 \n#&gt; max value   :    8\n\n\n\n\n\nelev + elev # 더하기\nelev^2      # 제곱\nlog(elev)   # 로그\nelev &gt; 5    # 논리"
  },
  {
    "objectID": "Spatial_Information_Analysis.html#tmap을-활용한-시각화",
    "href": "Spatial_Information_Analysis.html#tmap을-활용한-시각화",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "tmap을 plot하기 위해서는 우선 tm_shape()로 지정해야하며, + 연산자로 레이어를 추가해야함\n\nex) tm_polygons(), tm_raster(), tm_borders(), tm_symbols() 등\n\nInteractive maps : tmap_mode()를 사용하여 \"plot\",과 \"view\"모드 사용 가능\nFacet : 하나의 창에 여러 맵을 동시에 그리기\n\nFacet 하는 3가지 방법\n\n여러변수 이름 추가\nby argument of tm_facets로 공간 데이터를 나누기\ntmap_arrange() 사용\n\n\n\ntm_basemap() : 지도를 표현할 수 있는 바탕이 되는 지도\n\n\n\n# 1. 여러 변수 이름 추가\ntmap_mode(\"plot\")\ndata(World)\ntm_shape(World) +\n  tm_polygons(c(\"HPI\", \"economy\")) +\n  tm_facets(sync = TRUE, ncol = 2)\n\n\n\n\n\n# 2. by argument of `tm_facets`로 공간 데이터 나누기\ntmap_mode(\"plot\")\ndata(NLD_muni)\nNLD_muni$perc_men &lt;- NLD_muni$pop_men / NLD_muni$population * 100\ntm_shape(NLD_muni) +\n  tm_polygons(\"perc_men\", palette = \"RdYlBu\") +\n  tm_facets(by = \"province\")\n\n\n\n\n\n# 3. `tmap_arrange` 함수 사용 : 각각 그린다음에 배치\ntmap_mode(\"plot\")\ndata(NLD_muni)\ntm1 &lt;- tm_shape(NLD_muni) + tm_polygons(\"population\", convert2density = TRUE)\ntm2 &lt;- tm_shape(NLD_muni) + tm_bubbles(size = \"population\")\ntmap_arrange(tm1, tm2)\n\n\n\n\n\ntmap_mode(\"view\")\ndata(World, metro, rivers, land)\ntm_basemap(\"Stamen.Watercolor\") +\n  tm_shape(metro) + tm_bubbles(size = \"pop2020\", col = \"red\") +\n  tm_tiles(\"Stamen.TonerLabels\")\n\n\n\n\n\n\n\nOption and styles\n\ntm_layout() : map layout 지정\ntm_options() 내에서 설정\n\ntmap_options_diff() : default tmap options과 차이점 출력\ntmap_options_reset() : default tmap options으로 설정\n\nreset을 해주지 않으면 option이 계속 설정되어있음\n\n\ntmap_style() : 지도 스타일 설정\n\n\ntmap_mode(\"plot\")\ntm_shape(World) +\n  tm_polygons(\"HPI\") +\n  tm_layout(bg.color = \"skyblue\", inner.margins = c(0, .02, .02, .02))\n\n\n\n\n\ntmap_options(bg.color = \"black\", legend.text.color = \"white\")\ntm_shape(World) + tm_polygons(\"HPI\", legend.title = \"Happy Planet Index\")\n\n\n\n\n\ntmap_style(\"classic\")\n## tmap style set to \"classic\"\n## other available styles are: \"white\", \"gray\", \"natural\", \"cobalt\",\n## \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"watercolor\"\n\ntm_shape(World) +\n  tm_polygons(\"HPI\", legend.title = \"Happy Planet Index\")\n\n\n\n\nExporting maps\n\n\ntm &lt;- tm_shape(World) +\n  tm_polygons(\"HPI\", legend.title = \"Happy Planet Index\")\n\n## save an image (\"plot\" mode)\ntmap_save(tm, filename = \"./Spatial_Information_Analysis/world_map.png\")\n\n## save as stand-alone HTML file (\"view\" mode)\ntmap_save(tm, filename = \"./Spatial_Information_Analysis/world_map.html\")\n\n\nQuick thematic map\n\n\nqtm(World, fill = \"HPI\", fill.pallete = \"RdYlGn\")"
  },
  {
    "objectID": "Spatial_Information_Analysis.html#chapter5-geometry-operations",
    "href": "Spatial_Information_Analysis.html#chapter5-geometry-operations",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "단순화는 일반적으로 더 작은 축척 지도에서 사용하기 위한 벡터 객체(선, 다각형)의 일반화를 위한 프로세스\nst_simplify() : 정점을 제거하여 선을 단순화시킴\n\ndTolerance : 단위가 m이며 커질수록 더 단순화\n\n\nseine_simp &lt;- st_simplify(seine, dTolerance = 2000) # 2000m\nplot(seine)\nplot(seine_simp)\nobject.size(seine) ; object.size(seine_simp)\n#&gt; 18096 bytes  9112 bytes\n\n\n\n\n\n\n단순화는 다각형에도 적용 가능\nst_simplify()를 사용하였을 때, 영역이 겹치는 경우도 발생\nrmapshaper 패키지의 ms_simplify() 함수를 사용\nkeep_shapes = TRUE : 개체 수는 그대로 유지\n\n\nus_states\nus_states2163 &lt;- st_transform(us_states, 2163)\nus_states2163\n\nus_states_simp1 &lt;- st_simplify(us_states2163, dTolerance = 100000)\nplot(us_states[1])\nplot(us_states_simp1[1])\n\nus_states2163$AREA &lt;- as.numeric(us_states2163$AREA)\n\nlibrary(rmapshaper)\nus_states_simp2 &lt;- rmapshaper::ms_simplify(us_states2163, keep = 0.01,\n                                           keep_shapes = FALSE)\nplot(us_states_simp2[1])\n\n\n\n\n\n\n\n\n\n\n가장 일반적으로 사용되는 중심 연산은 지리적 중심 : 공간객체의 질량 중심\nst_centroid() : 지리적 중심을 생성하지만, 때때로 지리적 중심이 상위 개체의 경계를 벗어나는 경우가 발생\nst_point_on_surface() : 상위 개체 위에 중심이 생성\n\n\nnz_centroid &lt;- st_centroid(nz)\nseine_centroid &lt;- st_centroid(seine)\n\nnz_pos &lt;- st_point_on_surface(nz)\nseine_pos &lt;- st_point_on_surface(seine)\n\nplot(st_geometry(nz), main = \"nz\")\nplot(nz_centroid ,add=T, col=\"black\")\nplot(nz_pos ,add=T, col=\"red\")\n\nplot(st_geometry(seine), main = \"seine\")\nplot(seine_centroid ,add=T, col=\"black\")\nplot(seine_pos ,add=T, col=\"red\")\n\n\n\n\n\n\n\n\n\n\n버퍼 : 기하학적 특징의 주어진 거리 내 영역을 나타내는 다각형\n지리데이터 분석에 자주 활용됨\nst_buffer() : 버퍼 생성 함수, 최소 두 개의 인수가 필요함\n\n\nseine_buff_5km &lt;- st_buffer(seine, joinStyle = \"ROUND\", dist = 5000)\nseine_buff_20km &lt;- st_buffer(seine, dist = 20000)\n\nplot(seine,col=\"black\", reset = FALSE)\nplot(seine_buff_5km, col=adjustcolor(1:3, alpha = 0.2), add=T)\n\nplot(seine,col=\"black\", reset = FALSE)\ncol1 &lt;- adjustcolor(\"red\", alpha=0.2)\ncol2 &lt;- adjustcolor(\"blue\", alpha=0.2)\ncol3 &lt;- adjustcolor(\"green\", alpha=0.2)\nplot(seine_buff_20km, col=c(col1,col2,col3), add=T)\n\n\n\n\n\n\n\n\n\n\n왜곡되거나 잘못 투영된 지도를 기반으로 생성된 geometry를 재투영하거나 개선할 때 많은 Affine 변환이 적용\n이동 : 맵 단위로 모든 포인트가 동일한 거리만큼 이동\n\n\nnz_sfc &lt;- st_geometry(nz)\nnz_shift &lt;- nz_sfc + c(0, 100000)\nplot(nz_sfc)\nplot(nz_shift,add=T, col=\"Red\")\n\n\n\n\n\n배율 조정 : 개체를 요소만큼 확대하거나 축소\n\n모든 기하 도형의 토폴로지 관계를 그대로 유지하면서 원점 좌표와 관련된 모든 좌표값을 늘리거나 줄일 수 있음\n중심점을 기준으로 기하 도형의 차이 만큼을 늘리고 0.5배 줄인 다음 다시 중심점을 더해줌\n\n\nnz_centroid_sfc &lt;- st_centroid(nz_sfc)\nnz_scale &lt;- (nz_sfc - nz_centroid_sfc) * 0.5 + nz_centroid_sfc\n\nplot(nz_sfc)\nplot(nz_scale, add=T, col=\"Red\")\n\n\n\n\n회전 : 2차원 좌표의 회전하기 위한 회전변환행렬\n\n\nmatrix(c(cos(30), sin(30), -sin(30), cos(30)), nrow = 2, ncol = 2)\n#&gt;            [,1]      [,2]\n#&gt; [1,]  0.1542514 0.9880316\n#&gt; [2,] -0.9880316 0.1542514\n\nrotation &lt;- function(a){\n  r = a * pi / 180 #degrees to radians\n  matrix(c(cos(r), sin(r), -sin(r), cos(r)), nrow = 2, ncol = 2)\n}\nnz_rotate &lt;- (nz_sfc - nz_centroid_sfc) * rotation(30) + nz_centroid_sfc\n\nplot(nz_sfc)\nplot(nz_rotate, add=T, col=\"red\")\n\n\n\n\n\n\n\n\n공간 클리핑은 영향을 받는 일부 형상의 지오메트리 열의 변경을 수반하는 공간 부분 집합의 한 형태\n\n\nb &lt;- st_sfc(st_point(c(0, 1)), st_point(c(1, 1))) # create 2 points\nb &lt;- st_buffer(b, dist = 1) # convert points to circles\nplot(b, border = \"grey\")\ntext(x = c(-0.5, 1.5), y = 1, labels = c(\"x\", \"y\"), cex = 3) # add text\n\n\n\n\n\nst_intersection() : X∩Y (x와 y의 교집합)\nst_difference() : X-Y (x와 y의 차집합)\nst_union() : X∪Y (x와 y의 합집합)\nst_sym_difference() : (X∩Y)^c (드모르간의 법칙)\n\n\npar(mfrow = c(2,2))\n\nx &lt;- b[1] ; y &lt;- b[2]\n\n# X ∩ Y\nx_and_y &lt;- st_intersection(x, y)\nplot(b, border = \"grey\", main = \"X ∩ Y\")\nplot(x_and_y, col = \"lightgrey\", border = \"grey\", add = TRUE)\ntext(x = c(-0.5, 1.5), y = 1, labels = c(\"x\", \"y\"), cex = 3)\n\n# X - Y\nx_dif_y &lt;- st_difference(x,y)\nplot(b, border = \"grey\", main = \"X - Y\")\nplot(x_dif_y, col = \"lightgrey\", border = \"grey\", add = TRUE)\ntext(x = c(-0.5, 1.5), y = 1, labels = c(\"x\", \"y\"), cex = 3)\n\n# X U Y\nx_union_y &lt;- st_union(x,y)\nplot(b, border = \"grey\", main = \"X U Y\")\nplot(x_union_y, col = \"lightgrey\", border = \"grey\", add = TRUE)\ntext(x = c(-0.5, 1.5), y = 1, labels = c(\"x\", \"y\"), cex = 3)\n\n# (X ∩ Y)^c\nx_sdif_y &lt;- st_sym_difference(x,y)\nplot(b, border = \"grey\", main = \"(X ∩ Y)^c\")\nplot(x_sdif_y, col = \"lightgrey\", border = \"grey\", add = TRUE)\ntext(x = c(-0.5, 1.5), y = 1, labels = c(\"x\", \"y\"), cex = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n클리핑 오브젝트는 지오메트리를 변경할 수 있지만 오브젝트의 부분 집합을 지정할 수도 있으며 클리핑/하위 설정 오브젝트와 교차하는 피쳐만 반환할 수도 있음\nst_sample() : x와 y의 범위 내에서 점들의 간단한 무작위 분포를 생성\n\n\nbb = st_bbox(st_union(x, y))\nbox = st_as_sfc(bb)\nset.seed(2017)\n\np = st_sample(x = box, size = 10)\nx_and_y = st_intersection(x, y)\n\nplot(b, border = \"grey\")\nplot(p, add=T)\n\n\n\n\n\nX와 Y 둘 다와 교차하는 점만을 반환하는 방법\n\n\n\n## 1번째방법\np_xy1 &lt;- p[x_and_y]\nplot(p_xy1, add=T, col=\"red\")\n\n## 2번째방법\np_xy2 &lt;- st_intersection(p, x_and_y)\nplot(p_xy2, add=T, col=\"blue\")\n\n## 3번째방법\nsel_p_xy &lt;- st_intersects(p, x, sparse = FALSE)[, 1] &\n  st_intersects(p, y, sparse = FALSE)[, 1]\np_xy3 &lt;- p[sel_p_xy]\nplot(p_xy3, add=T, col=\"green\")\n\n\n\n\n\n\n\n\n\n\n미국의 49개 주의 정보를 4개 지역으로 재구분\n\n\nplot(us_states[6])\n\n\n\n## 1. aggregate함수\nregions &lt;- aggregate(x = us_states[, \"total_pop_15\"], by = list(us_states$REGION),\n                     FUN = sum, na.rm = TRUE)\nplot(regions[2])\n\n\n\n## 2. group_by, summarize함수\nregions2 &lt;- us_states %&gt;% group_by(REGION) %&gt;%\n  summarize(pop = sum(total_pop_15, na.rm = TRUE))\n\nplot(regions2[2])\n\n\n\n\n\n위에서 aggregate()와 summarize()가 모두 지오메트리를 결합하고 st_union()을 사용하면 지오메트리만을 분해\n\n\nus_west &lt;- us_states[us_states$REGION == \"West\", ]\nplot(us_west[6])\n\n\n\nus_west_union &lt;- st_union(us_west)\nplot(us_west_union)\n\n\n\ntexas &lt;- us_states[us_states$NAME == \"Texas\", ]\ntexas_union &lt;- st_union(us_west_union, texas)\nplot(texas_union)\n\n\n\n\n\n\n\n\nst_cast() : 지오메트리 유형을 변환\n\n\nmultipoint &lt;- st_multipoint(matrix(c(1, 3, 5, 1, 3, 1), ncol = 2))\nlinestring &lt;- st_cast(multipoint, \"LINESTRING\")\npolyg &lt;- st_cast(multipoint, \"POLYGON\")\n\nplot(multipoint)\nplot(linestring)\nplot(polyg)\n\nst_length(linestring) # 길이 계산\n# [1] 5.656854\nst_area(polyg) # 면적 계산\n# [1] 4\n\n\n\n\n\n\n\nmultilinestring : 여러 개의 linestring을 하나의 묶음으로 처리\n\n\n\n\n\n\n\nmultilinestring은 각 선 세그먼트에 이름을 추가하거나 단일 선 길이를 계산할 수 없는 등 수행할 수 있는 작업 수가 제한됨\nst_cast() 함수를 사용하여 하나의 multilinestring을 세 개의 linestring로 분리\n\n\nlinestring_sf2 = st_cast(multilinestring_sf, \"LINESTRING\")\nlinestring_sf2\n#&gt; Simple feature collection with 3 features and 0 fields\n#&gt; Geometry type: LINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 1 ymin: 1 xmax: 4 ymax: 5\n#&gt; CRS:           NA\n#&gt;                    geom\n#&gt; 1 LINESTRING (1 5, 4 3)\n#&gt; 2 LINESTRING (4 4, 4 1)\n#&gt; 3 LINESTRING (2 2, 4 2)\n\n\nname과 length 추가\n\n\nlinestring_sf2$name &lt;- c(\"Riddle Rd\", \"Marshall Ave\", \"Foulke St\")\nlinestring_sf2$length &lt;- st_length(linestring_sf2)\nlinestring_sf2\n#&gt; Simple feature collection with 3 features and 2 fields\n#&gt; Geometry type: LINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 1 ymin: 1 xmax: 4 ymax: 5\n#&gt; CRS:           NA\n#&gt;                    geom         name   length\n#&gt; 1 LINESTRING (1 5, 4 3)    Riddle Rd 3.605551\n#&gt; 2 LINESTRING (4 4, 4 1) Marshall Ave 3.000000\n#&gt; 3 LINESTRING (2 2, 4 2)    Foulke St 2.000000\nplot(linestring_sf2[2])\n\n\n\n\n\n\n\n\n\n\n\n다른 공간 객체에 의해 중첩된 래스터에서 값을 추출하는 방법\n공간 출력을 검색하기 위해 거의 동일한 부분 집합 구문(많이 겹치는 부분)을 사용\ndrop = FALSE를 설정하여 행렬 구조를 유지\ncell 중간점이 clip과 겹치는 셀을 포함하는 래스터 개체를 반환\n\n\nelev &lt;- rast(system.file(\"raster/elev.tif\", package = \"spData\"))\nclip &lt;- rast(xmin = 0.9, xmax = 1.8, ymin = -0.45, ymax = 0.45,\n             resolution = 0.3, vals = rep(1, 9))\nplot(elev)\nplot(clip, add=T)\n\n\n\nelve_clip &lt;- elev[clip, drop = FALSE]\nplot(elve_clip)\n\n\n\nelev_raster &lt;- rast(system.file(\"raster/elev.tif\", package = \"spData\"))\nrcc &lt;- vect(xyFromCell(elev_raster, cell = 1:ncell(elev_raster))) # 셀의 중앙점 표시\nxyFromCell(elev_raster,1) # 1번 셀의 중앙점 좌표\n#&gt;          x    y\n#&gt; [1,] -1.25 1.25\nplot(elev)\nplot(rcc,add=T)\nplot(clip, add=T)\n\n\n\n\n\n\n\n\n다른 투사 및 해상도를 가진 두 이미지를 병합하려할 때 사용\nextend() : 래스터 범위 확장\n\n새로 추가된 행과 열은 값 매개변수의 기본값(예 : NA)를 가짐\n\norigin() : 래스터의 원점 좌표를 반환\n\n래스터의 원점은 좌표(0,0)에 가장 가까운 셀 모서리\n\n\n\nelev &lt;- rast(system.file(\"raster/elev.tif\", package = \"spData\"))\nelev_2 &lt;- extend(elev, c(1,2), snap=\"near\") # 아래/위 1행, 좌/우 2열 확장\nplot(elev)\n\n\n\nplot(elev_2, colNA=\"gray\")\n\nelev_3 &lt;- elev + elev_2\n#&gt; Error: [+] extents do not match\n\nelev_4 &lt;- extend(elev, elev_2)\nplot(elev_4, colNA=\"gray\")\n\norigin(elev_4)\n#&gt; [1] 0 0\n\norigin(elev_4) &lt;- c(0.25, 0.25)\nplot(elev_4, colNA=\"black\", add=T)\n\n\n\n\n\n\n\n\n\n래스터 데이터 셋은 해상도가 서로 다를 수 있음\n해상도를 match 시키기 위해 하나의 래스터 해상도를 감소(aggregate())시키거나 증가(disagg()) 시켜야 함\n\n\n# devtools::install_github(\"geocompr/geocompkg\")\ndem &lt;- rast(system.file(\"raster/dem.tif\", package = \"spDataLarge\"))\ndem_agg &lt;- aggregate(dem, fact = 5, fun = mean)\ndem_disagg &lt;- disagg(dem_agg, fact = 5, method = \"bilinear\")\nplot(dem)\n\n\n\nplot(dem_agg)\n\n\n\nplot(dem_disagg)\n\n\n\nidentical(dem, dem_disagg)\n#&gt; [1] FALSE\n\n\n새롭게 만들어지는 cell의 값을 만드는 두가지 방법\n\nDefault method(method = “near”) : 입력 셀의 값을 모든 출력 셀에 제공\nbilinear method : 입력 이미지의 가장 가까운 4개의 픽셀 중심을 사용하여 거리에 의해 가중된 평균을 계산\n\n\n\n\n\n\nResampling : 원래 그리드에서 다른 그리드로 래스터 값을 전송하는 프로세스\n이 프로세스는 원래 래스터의 값을 가지고, 사용자 지정 해상도와 원점을 가지고 대상 래스터의 새 값을 다시 계산함\n해상도/원점이 다른 래스터의 값을 재계산(추정)하는 방법\n\nNearest neighbor : 원래 래스터의 가장 가까운 셀 값을 대상 래스터의 셀에 할당. 속도가 빠르고 일반적으로 범주형 래스터에 적합\nBilinear interpolation(이중선형보간) : 원래 래스터에서 가장 가까운 4개의 셀의 가중 평균을 대상 1개의 셀에 할당. 연속 래스터를 위한 가장 빠른 방법\nCubic interpolation(큐빅 보간) : 본 래스터의 가장 가까운 16개 셀의 값을 사용하여 출력 셀 값을 결정하고 3차 다항식 함수를 적용. 연속 래스터에 사용. 2선형 보간보다 더 매끄러운 표면을 만들지만, 계산적으로 까다로움\nCubic spline interpolation(큐빅 스플라인 보간) : 원래 래스터의 가장 가까운 16개의 셀의 값을 사용하여 출력 셀 값을 결정하지만 큐빅 스플라인(3차 다항식 함수)을 적용\nLanczos windowed sinc resampling(Lanczos 윈도우 재샘플링) : 원래 래스터의 가장 가까운 셀 36개의 값을 사용하여 출력 셀 값을 결정\nsum\nmin, q1, med, q3, max, average, mode, rms\n\nNearest neighbor은 범주형 래스터에 적합한 반면, 모든 방법은 연속형 래스터에 사용\nresample(x, y, method = \"bilinear\", filename = \"\", ...) : 리샘플링 함수\n\n\nlibrary(terra)\n\ntarget_rast &lt;- rast(xmin = 794600, xmax = 798200,\n                    ymin = 8931800, ymax = 8935400,\n                    resolution = 150, crs = \"EPSG:32717\")\ntarget_rast\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 24, 24, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 150, 150  (x, y)\n#&gt; extent      : 794600, 798200, 8931800, 8935400  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : WGS 84 / UTM zone 17S (EPSG:32717)\n\nplot(dem)\n\n\n\nplot(target_rast)\n\n\n\n\n\n\"near\" : 셀에 가장 가까운 픽셀에서 값을 가져옴\n\n\ndem_resampl_1 &lt;- resample(dem, y = target_rast, method = \"near\")\nplot(dem_resampl_1)\n\n\n\n\n\n\"bilinear\" : 네 개의 가장 가까운 셀의 가중 평균\n\n\ndem_resampl_2 &lt;- resample(dem, y = target_rast, method = \"bilinear\")\nplot(dem_resampl_2)\n\n\n\n\n\n\"average\" : 각각의 새로운 셀이 중복되는 모든 입력 셀의 가중 평균\n\n\ndem_resampl_3 &lt;- resample(dem, y = target_rast, method = \"average\")\nplot(dem_resampl_3)"
  },
  {
    "objectID": "Spatial_Information_Analysis.html#chapter-6-raster-vector-interactions",
    "href": "Spatial_Information_Analysis.html#chapter-6-raster-vector-interactions",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "입력 래스터 데이터 세트의 범위가 관심 영역보다 클 경우 래스터 자르기(Cropping) 및 마스킹(Masking)은 입력 데이터의 공간 범위를 통합하는 데 유용함\n두 작업 모두 후속 분석 단계에 대한 객체 메모리 사용 및 관련 계산 리소스를 줄이고 래스터 데이터를 포함하는 매력적인 맵을 만들기 전에 필요한 전처리 단계임\n대상 개체와 자르기 개체는 모두 동일한 투영을 가져야 함\ncrop() : 두 번째 인수에 대한 래스터를 잘라냄\nmask() : 두 번째 인수에 전달된 개체의 경계를 벗어나는 값을 NA로 설정\n\n대부분의 경우 crop()과 mask()를 함께 사용\n\n\nsrtm &lt;- rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\nzion &lt;- read_sf(system.file(\"vector/zion.gpkg\", package = \"spDataLarge\"))\nzion &lt;- st_transform(zion, crs(srtm)) # zion을 srtm 좌표계랑 동일하게\nplot(srtm)\nplot(vect(zion),add=T)\n\n\n\nsrtm_cropped &lt;- crop(srtm, vect(zion))\nplot(srtm_cropped)\n\n\n\nsrtm_masked &lt;- mask(srtm, vect(zion))\nplot(srtm_masked)\n\n\n\nsrtm_cropped &lt;- crop(srtm, vect(zion))\nsrtm_final &lt;- mask(srtm_cropped, vect(zion))\nplot(srtm_final)\n\n\n\n\nupdatevalue = 0 : 외부의 모든 픽셀이 0으로 설정\ninverse = TRUE : 경계 내에 있는 것들이 마스킹\n\n\nsrtm_update0 &lt;- mask(srtm, vect(zion), updatevalue = 0)\nplot(srtm_update0)\n\n\n\nsrtm_inv_masked &lt;- mask(srtm, vect(zion), inverse = TRUE)\nplot(srtm_inv_masked)\n\n\n\n\n\n\n\n## Original / Crop / Mask / Inverse Map\nlibrary(tmap)\nlibrary(rcartocolor)\n\nterrain_colors = carto_pal(7, \"Geyser\")\n\npz1 = tm_shape(srtm) +\n  tm_raster(palette = terrain_colors, legend.show = FALSE, style = \"cont\") +\n  tm_shape(zion) +\n  tm_borders(lwd = 2) +\n  tm_layout(main.title = \"A. Original\", inner.margins = 0)\n\npz2 = tm_shape(srtm_cropped) +\n  tm_raster(palette = terrain_colors, legend.show = FALSE, style = \"cont\") +\n  tm_shape(zion) +\n  tm_borders(lwd = 2) +\n  tm_layout(main.title = \"B. Crop\", inner.margins = 0)\n\npz3 = tm_shape(srtm_masked) +\n  tm_raster(palette = terrain_colors, legend.show = FALSE, style = \"cont\") +\n  tm_shape(zion) +\n  tm_borders(lwd = 2) +\n  tm_layout(main.title = \"C. Mask\", inner.margins = 0)\n\npz4 = tm_shape(srtm_inv_masked) +\n  tm_raster(palette = terrain_colors, legend.show = FALSE, style = \"cont\") +\n  tm_shape(zion) +\n  tm_borders(lwd = 2) +\n  tm_layout(main.title = \"D. Inverse mask\", inner.margins = 0)\n\ntmap_arrange(pz1, pz2, pz3, pz4, ncol = 4, asp = NA)\n\n\n\n\n\n\n\n\n\n특정 위치에 있는 대상 래스터와 관련된 값을 식별하여 반환\n\n\ndata(\"zion_points\", package = \"spDataLarge\")\nelevation &lt;-terra::extract(srtm, vect(zion_points))\nzion_points &lt;- cbind(zion_points, elevation)\nplot(srtm)\nplot(vect(zion),add=T)\nplot(zion_points,col=\"black\", pch = 19, cex = 0.5, add=T)\n#&gt; Warning in plot.sf(zion_points, col = \"black\", pch = 19, cex = 0.5, add = T):\n#&gt; ignoring all but the first attribute\n\n\n\n\n\nst_segmentize() : 제공된 density로 line을 따라 point를 추가\n\ndfMaxLength : 최대 점의 개수\n\nst_cast() : 추가된 point를 “POINT” 형식으로 변환\n\n\nzion_transect &lt;- cbind(c(-113.2, -112.9), c(37.45, 37.2)) %&gt;%\n  st_linestring() %&gt;%\n  st_sfc(crs = crs(srtm)) %&gt;%\n  st_sf()\nzion_transect$id &lt;- 1:nrow(zion_transect)\nzion_transect &lt;- st_segmentize(zion_transect, dfMaxLength = 250)\nzion_transect &lt;- st_cast(zion_transect, \"POINT\")\n#&gt; Warning in st_cast.sf(zion_transect, \"POINT\"): repeating attributes for all\n#&gt; sub-geometries for which they may not be constant\n\n\nzion_transect &lt;- zion_transect %&gt;%\n  group_by(id) %&gt;%\n  mutate(dist = st_distance(geometry)[, 1])\n\nzion_elev &lt;- terra::extract(srtm, vect(zion_transect))\nzion_transect &lt;- cbind(zion_transect, zion_elev)\n\n\n많은 Point들 간의 거리를 산출 : 첫번째 점들과 이후의 각각의 점들 사이의 거리 계산하기\n횡단면의 각 점에 대한 고도값을 추출하고 이 정보를 주요 객체와 결합\n\n\n\n\nlibrary(tmap)\nlibrary(grid)\nlibrary(ggplot2)\nzion_transect_line &lt;- cbind(c(-113.2, -112.9), c(37.45, 37.2)) %&gt;%\n  st_linestring() %&gt;%\n  st_sfc(crs = crs(srtm)) %&gt;%\n  st_sf()\nzion_transect_points &lt;- st_cast(zion_transect, \"POINT\")[c(1, nrow(zion_transect)), ]\nzion_transect_points$name &lt;- c(\"start\", \"end\")\nrast_poly_line &lt;- tm_shape(srtm) +\n  tm_raster(palette = terrain_colors, title = \"Elevation (m)\",\n            legend.show = TRUE, style = \"cont\") +\n  tm_shape(zion) +\n  tm_borders(lwd = 2) +\n  tm_shape(zion_transect_line) +\n  tm_lines(col = \"black\", lwd = 4) +\n  tm_shape(zion_transect_points) +\n  tm_text(\"name\", bg.color = \"white\", bg.alpha = 0.75, auto.placement = TRUE) +\n  tm_layout(legend.frame = TRUE, legend.position = c(\"right\", \"top\"))\nrast_poly_line\n\n\n\nplot_transect &lt;- ggplot(zion_transect, aes(as.numeric(dist), srtm)) +\n  geom_line() +\n  labs(x = \"Distance (m)\", y = \"Elevation (m a.s.l.)\") +\n  theme_bw() +\n  # facet_wrap(~id) +\n  theme(plot.margin = unit(c(5.5, 15.5, 5.5, 5.5), \"pt\"))\nplot_transect\n\n\n\n\n## grid 그리기\ngrid.newpage() #This function erases the current device or moves to a new page.\npushViewport(viewport(layout = grid.layout(2, 2, heights = unit(c(0.25, 5), \"null\"))))\ngrid.text(\"A. Line extraction\", vp = viewport(layout.pos.row = 1, layout.pos.col = 1))\ngrid.text(\"B. Elevation along the line\", vp = viewport(layout.pos.row = 1, layout.pos.col = 2))\nprint(rast_poly_line, vp = viewport(layout.pos.row = 2, layout.pos.col = 1))\nprint(plot_transect, vp = viewport(layout.pos.row = 2, layout.pos.col = 2))\n\n\n\n\n\nzion_srtm_values &lt;- terra::extract(x = srtm, y = vect(zion))\ngroup_by(zion_srtm_values, ID) %&gt;%\n  summarize(across(srtm, list(min = min, mean = mean, max = max)))\n#&gt; # A tibble: 1 × 4\n#&gt;      ID srtm_min srtm_mean srtm_max\n#&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n#&gt; 1     1     1122     1818.     2661\n\n\n단일 영역을 특성화하거나 여러 영역을 비교하기 위해 폴리곤 당 래스터 값에 대한 요약 통계 생성\n\n\n\n\n\n\n벡터 객체를 래스터 객체의 표현으로 변환\n\n\ncycle_hire_osm &lt;- spData::cycle_hire_osm\ncycle_hire_osm_projected &lt;- st_transform(cycle_hire_osm, \"EPSG:27700\")\nraster_template &lt;- rast(ext(cycle_hire_osm_projected), resolution = 1000,\n                        crs = st_crs(cycle_hire_osm_projected)$wkt) # ext : 경계값\nch_raster1 &lt;- rasterize(vect(cycle_hire_osm_projected), raster_template,\n                        field = 1)\nch_raster2 &lt;- rasterize(vect(cycle_hire_osm_projected), raster_template,\n                        fun = \"length\")\nch_raster3 &lt;- rasterize(vect(cycle_hire_osm_projected), raster_template,\n                        field = \"capacity\", fun = sum)\n\n\n\n\n\n\n\n폴리곤 객체를 여러 줄 문자열로 casting한 후 0.5도의 해상도로 탬플릿 래스터 생성\n\ntouches = TRUE : 경계에 해당되는 래스터만 색칠(FALSE이면 경계 내부까지)\n\n\n\ncalifornia &lt;- dplyr::filter(us_states, NAME == \"California\")\ncalifornia_borders &lt;- st_cast(california, \"MULTILINESTRING\")\nraster_template2 &lt;- rast(ext(california),\n                         resolution = 0.5,\n                         crs = st_crs(california)$wkt)\ncalifornia_raster1 &lt;-\n  rasterize(vect(california_borders), raster_template2,\n            touches = TRUE) # touches = TRUE : 경계값만\ncalifornia_raster2 &lt;-\n  rasterize(vect(california), raster_template2)\n# with `touches = FALSE` by default, which selects only cell\n\n\n\n\n\n\n\n\n\n\n공간적으로 연속적인 래스터 데이터를 점, 선 또는 다각형과 같은 공간적으로 분리된 벡터 데이터로 변환\n벡터화의 가장 간단한 형태는 래스터 셀의 중심부를 점으로 변환하는 것\nas.points() : 모든 raster grid 셀에 대해 중심점으로 반환\n\n\nelev &lt;- rast(system.file(\"raster/elev.tif\", package = \"spData\"))\nelev_point &lt;- as.points(elev) %&gt;%\n  st_as_sf()\nplot(elev)\n\n\n\nplot(elev_point)\n\n\n\n\n\ncontour() : 선에 해당하는 수치 표현\n등고선의 생성 : 공간 벡터화의 또 다른 일반적인 유형은 연속적인 높이 또는 온도(등온선)의 선을 나타내는 등고선 생성\n\n\ndem = rast(system.file(\"raster/dem.tif\", package = \"spDataLarge\"))\ncl = as.contour(dem)\nplot(dem, axes = FALSE)\nplot(cl, add = TRUE)\n\n\n\nplot(dem, axes = FALSE)\ncontour(dem, add = T) # 수치까지 표현\n\n\n\n\n\nas.polygons() : 래스터를 다각형으로 변환하는 것\n\n\ngrain &lt;- rast(system.file(\"raster/grain.tif\", package = \"spData\"))\ngrain_poly &lt;- as.polygons(grain) %&gt;%\n  st_as_sf()\nplot(grain)\n\n\n\nplot(grain_poly)"
  },
  {
    "objectID": "Spatial_Information_Analysis.html#chapter-7-reprojecting-geographic-data",
    "href": "Spatial_Information_Analysis.html#chapter-7-reprojecting-geographic-data",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "CRS를 설명할 수 있는 여러가지 방법\n\n단순하지만 “lon/lat 좌표”와 같이 모호할 수 있는 문장\n\n\n공식화되었지만 지금은 구식인 proj4 strings\n\n\nproj=lonlat +ellps=WGS84 +datum=WGS84 +no_defs\n\n\nEPSG:4326과 같이 식별되는 authority:code 텍스트 문자열\n\n-&gt; 3번째 방법이 가장 정확(짧고 기억하기 쉬우며 온라인에서 찾기 쉬움)\n\n\nst_crs(\"EPSG:4326\")\n\n\n\n\n\n벡터 지리 데이터 객체에서 CRS를 가져오고 설정\n\n\nvector_filepath &lt;- system.file(\"shapes/world.gpkg\", package = \"spData\")\nnew_vector &lt;- read_sf(vector_filepath)\n\nst_crs(new_vector)\n#&gt; Coordinate Reference System:\n#&gt;   User input: WGS 84 \n#&gt;   wkt:\n#&gt; GEOGCRS[\"WGS 84\",\n#&gt;     ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n#&gt;         MEMBER[\"World Geodetic System 1984 (Transit)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G730)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G873)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G1150)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G1674)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G1762)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G2139)\"],\n#&gt;         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n#&gt;             LENGTHUNIT[\"metre\",1]],\n#&gt;         ENSEMBLEACCURACY[2.0]],\n#&gt;     PRIMEM[\"Greenwich\",0,\n#&gt;         ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;     CS[ellipsoidal,2],\n#&gt;         AXIS[\"geodetic latitude (Lat)\",north,\n#&gt;             ORDER[1],\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;         AXIS[\"geodetic longitude (Lon)\",east,\n#&gt;             ORDER[2],\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;     USAGE[\n#&gt;         SCOPE[\"Horizontal component of 3D system.\"],\n#&gt;         AREA[\"World.\"],\n#&gt;         BBOX[-90,-180,90,180]],\n#&gt;     ID[\"EPSG\",4326]]\n\n\nUser input : CRS식별자 (WGS 84, 입력 파일에서 가져온 EPSG:4326의 동의어)\nwkt : CRS에 대한 모든 관련 정보와 함께 전체 WKT 문자열을 포함\ninput 요소는 유연함(AUTHORITY:CODE (ex. EPSG:4326), CRS 이름(ex. WGS84), proj4string 정의)\nwkt 요소는 객체를 파일에 저장하거나 좌표 연산을 수행할 때 사용되는 WKT 표현을 저장\nnew_vector 객체가 WGS84 타원체를 가지며, 그리니치 프라임 자오선을 사용하고, 위도와 경도의 축 순서를 사용하는 것을 볼 수 있음\n이 경우 이 CRS 사용에 적합한 영역을 설명하는 USAGE와 CRS 식별자 EPSG:4326을 가리키는 ID와 같은 추가 요소도 있음\n\n\nst_crs(new_vector)$IsGeographic\n#&gt; [1] TRUE\nst_crs(new_vector)$units_gdal\n#&gt; [1] \"degree\"\nst_crs(new_vector)$srid\n#&gt; [1] \"EPSG:4326\"\nst_crs(new_vector)$proj4string\n#&gt; [1] \"+proj=longlat +datum=WGS84 +no_defs\"\n\n\nst_crs 함수에는 유용한 기능이 하나 있는데, 사용된 CRS에 대한 추가 정보를 검색할 수 있음.\n\nst_crs(new_vector)$IsGeographic : CRS가 지리적 상태인지 확인\nst_crs(new_vector)$units_gdal : CRS 단위\nst_crs(new_vector)$srid : 해당 ‘SRID’ 식별자를 추출(사용 가능한 경우)\nst_crs(new_vector)$proj4string : proj4string 표현을 추출\n\nst_set_crs() : CRS가 없거나 잘못 설정되어 있는 경우 CRS 설정\n\n\nnew_vector &lt;- st_set_crs(new_vector, \"EPSG:4326\") # set CRS\n\n\nterra::crs() : 래스터 객체에 대한 CRS를 설정\n하지만, crs() 함수를 사용하면 좌표계는 바뀌지만 값이 바뀌지는 않음.\n\n\nraster_filepath &lt;- system.file(\"raster/srtm.tif\", package = \"spDataLarge\")\nmy_rast &lt;- rast(raster_filepath)\ncrs(my_rast)\n#&gt; [1] \"GEOGCRS[\\\"WGS 84\\\",\\n    ENSEMBLE[\\\"World Geodetic System 1984 ensemble\\\",\\n        MEMBER[\\\"World Geodetic System 1984 (Transit)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G730)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G873)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1150)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1674)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1762)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G2139)\\\"],\\n        ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        ENSEMBLEACCURACY[2.0]],\\n    PRIMEM[\\\"Greenwich\\\",0,\\n        ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    CS[ellipsoidal,2],\\n        AXIS[\\\"geodetic latitude (Lat)\\\",north,\\n            ORDER[1],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        AXIS[\\\"geodetic longitude (Lon)\\\",east,\\n            ORDER[2],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    USAGE[\\n        SCOPE[\\\"Horizontal component of 3D system.\\\"],\\n        AREA[\\\"World.\\\"],\\n        BBOX[-90,-180,90,180]],\\n    ID[\\\"EPSG\\\",4326]]\"\ncat(crs(my_rast)) # get CRS\n#&gt; GEOGCRS[\"WGS 84\",\n#&gt;     ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n#&gt;         MEMBER[\"World Geodetic System 1984 (Transit)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G730)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G873)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G1150)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G1674)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G1762)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G2139)\"],\n#&gt;         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n#&gt;             LENGTHUNIT[\"metre\",1]],\n#&gt;         ENSEMBLEACCURACY[2.0]],\n#&gt;     PRIMEM[\"Greenwich\",0,\n#&gt;         ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;     CS[ellipsoidal,2],\n#&gt;         AXIS[\"geodetic latitude (Lat)\",north,\n#&gt;             ORDER[1],\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;         AXIS[\"geodetic longitude (Lon)\",east,\n#&gt;             ORDER[2],\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;     USAGE[\n#&gt;         SCOPE[\"Horizontal component of 3D system.\"],\n#&gt;         AREA[\"World.\"],\n#&gt;         BBOX[-90,-180,90,180]],\n#&gt;     ID[\"EPSG\",4326]]\ncrs(my_rast) &lt;- \"EPSG:26912\" # set CRS\n\nlondon &lt;- data.frame(lon = -0.1, lat = 51.5) %&gt;%\n  st_as_sf(coords = c(\"lon\", \"lat\"))\nst_is_longlat(london)\n#&gt; [1] NA\n\nlondon_geo &lt;- st_set_crs(london, \"EPSG:4326\")\nst_is_longlat(london_geo)\n#&gt; [1] TRUE"
  },
  {
    "objectID": "Spatial_Information_Analysis.html#geometry-operations-on-projected-and-unprojected-data",
    "href": "Spatial_Information_Analysis.html#geometry-operations-on-projected-and-unprojected-data",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "sf는 지리 벡터 데이터에 대한 클래스와 지리 계산을 위한 중요한 하위 수준 라이브러리에 대한 일관된 명령줄 인터페이스 제공\n\n구면 geometry 연산을 sf:sf_use_sf(FALSE) 명령으로 끄면 버퍼는 미터와 같은 적절한 거리 단위를 대체하지 못하는 위도와 경도의 단위를 사용하기 때문에 쓸모없는 출력이 됨.\n공간 및 기하학적 연산을 수행하는 것은 경우에 따라 거의 또는 전혀 차이가 없음. (ex: 공간 부분 집합) 그러나 버퍼링과 같은 거리가 포함된 연산의 경우 (구면 지오메트리 엔진을 사용하지 않고) 좋은 결과를 보장하는 유일한 방법은 데이터의 투영된 복사본을 만들고 그에 대한 연산을 실행하는 것임.\n그 결과 런던과 동일하지만 미터 단위의 EPSG 코드를 가진 적절한 CRS(영국 국가 그리드)에 재투사된 새로운 물체가 되었음.\nCRS의 단위가 (도가 아닌) 미터라는 사실은 이것이 투영된 CRS임을 알려줌\n\n\n\nlondon_buff_no_crs &lt;-\n  st_buffer(london, dist = 1) # incorrect: no CRS\nlondon_buff_no_crs\n#&gt; Simple feature collection with 1 feature and 0 fields\n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -1.1 ymin: 50.5 xmax: 0.9 ymax: 52.5\n#&gt; CRS:           NA\n#&gt;                         geometry\n#&gt; 1 POLYGON ((0.9 51.5, 0.89862...\nlondon_buff_s2 &lt;-\n  st_buffer(london_geo, dist = 1e5) # silent use of s2 (1e5 : 10^5m = 100,000m)\nlondon_buff_s2\n#&gt; Simple feature collection with 1 feature and 0 fields\n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -1.552818 ymin: 50.59609 xmax: 1.356603 ymax: 52.40393\n#&gt; Geodetic CRS:  WGS 84\n#&gt;                         geometry\n#&gt; 1 POLYGON ((-0.3523255 52.392...\nlondon_buff_s2_100_cells &lt;-\n  st_buffer(london_geo, dist = 1e5, max_cells = 100)\nlondon_buff_s2_100_cells\n#&gt; Simple feature collection with 1 feature and 0 fields\n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -1.718303 ymin: 50.51128 xmax: 1.524546 ymax: 52.53186\n#&gt; Geodetic CRS:  WGS 84\n#&gt;                         geometry\n#&gt; 1 POLYGON ((-0.3908656 52.531...\n\nsf::sf_use_s2(FALSE)\n#&gt; Spherical geometry (s2) switched off\n\nlondon_buff_lonlat &lt;-\n  st_buffer(london_geo, dist = 1) # incorrect result\n#&gt; Warning in st_buffer.sfc(st_geometry(x), dist, nQuadSegs, endCapStyle =\n#&gt; endCapStyle, : st_buffer does not correctly buffer longitude/latitude data\n#&gt; dist is assumed to be in decimal degrees (arc_degrees).\n\nsf::sf_use_s2(TRUE)\n#&gt; Spherical geometry (s2) switched on\n\nlondon_proj &lt;- data.frame(x = 530000, y = 180000) %&gt;%\n  st_as_sf(coords = 1:2, crs = \"EPSG:27700\")\n\nst_crs(london_proj)\n#&gt; Coordinate Reference System:\n#&gt;   User input: EPSG:27700 \n#&gt;   wkt:\n#&gt; PROJCRS[\"OSGB36 / British National Grid\",\n#&gt;     BASEGEOGCRS[\"OSGB36\",\n#&gt;         DATUM[\"Ordnance Survey of Great Britain 1936\",\n#&gt;             ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,\n#&gt;                 LENGTHUNIT[\"metre\",1]]],\n#&gt;         PRIMEM[\"Greenwich\",0,\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;         ID[\"EPSG\",4277]],\n#&gt;     CONVERSION[\"British National Grid\",\n#&gt;         METHOD[\"Transverse Mercator\",\n#&gt;             ID[\"EPSG\",9807]],\n#&gt;         PARAMETER[\"Latitude of natural origin\",49,\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433],\n#&gt;             ID[\"EPSG\",8801]],\n#&gt;         PARAMETER[\"Longitude of natural origin\",-2,\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433],\n#&gt;             ID[\"EPSG\",8802]],\n#&gt;         PARAMETER[\"Scale factor at natural origin\",0.9996012717,\n#&gt;             SCALEUNIT[\"unity\",1],\n#&gt;             ID[\"EPSG\",8805]],\n#&gt;         PARAMETER[\"False easting\",400000,\n#&gt;             LENGTHUNIT[\"metre\",1],\n#&gt;             ID[\"EPSG\",8806]],\n#&gt;         PARAMETER[\"False northing\",-100000,\n#&gt;             LENGTHUNIT[\"metre\",1],\n#&gt;             ID[\"EPSG\",8807]]],\n#&gt;     CS[Cartesian,2],\n#&gt;         AXIS[\"(E)\",east,\n#&gt;             ORDER[1],\n#&gt;             LENGTHUNIT[\"metre\",1]],\n#&gt;         AXIS[\"(N)\",north,\n#&gt;             ORDER[2],\n#&gt;             LENGTHUNIT[\"metre\",1]],\n#&gt;     USAGE[\n#&gt;         SCOPE[\"Engineering survey, topographic mapping.\"],\n#&gt;         AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],\n#&gt;         BBOX[49.75,-9.01,61.01,2.01]],\n#&gt;     ID[\"EPSG\",27700]]"
  },
  {
    "objectID": "Spatial_Information_Analysis.html#chapter-8-geographic-data-i-and-oinput-and-output",
    "href": "Spatial_Information_Analysis.html#chapter-8-geographic-data-i-and-oinput-and-output",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "download.file(url = \"https://irma.nps.gov/DataStore/DownloadFile/666527\",\n              destfile = \"nps_boundary.zip\")\nunzip(zipfile = \"nps_boundary.zip\")\nusa_parks = read_sf(dsn = \"nps_boundary.shp\")\n\n\n해외여서 접속이 막혀있음\n공공데이터포털에서 shape 파일 다운받아 불러오기\n\n공공데이터포털에서 데이터를 작업 공간에 다운 받기\n\n\n# unzip(zipfile=\"C:/202201/GIS/data/부산광역시_교통정보서비스센터 보유 ITS CCTV 현황(SHP)_20210601.zip\")\n#busan &lt;- read_sf(dsn = \"./Spatial_Information_Analysis/tl_tracffic_cctv_info.shp\", options = \"ENCODING:CP949\")\n#busan\n#plot(busan)\n\n# unzip(zipfile = \"C:/202201/GIS/data/CTPRVN_20220324.zip\")\n#sido &lt;- read_sf(dsn = \"./Spatial_Information_Analysis/ctp_rvn.shp\", options = \"ENCODING:CP949\")\n#sido\n#plot(sido)\n\n\n\n\n\n\nrnaturalearth 패키지의 ne_countries() 기능을 사용하면 국가 경계 기능을 사용할 수 있음\nosmdata 패키지는 속도가 제한되어 있다는 단점이 있음\n\n이러한 한계를 극복하기 위해 osmextract 패키지가 개발\n\n\nlibrary(rnaturalearth)\n#&gt; Support for Spatial objects (`sp`) will be deprecated in {rnaturalearth} and will be removed in a future release of the package. Please use `sf` objects with {rnaturalearth}. For example: `ne_download(returnclass = 'sf')`\nusa &lt;- ne_countries(country = \"United States of America\") # United States borders\n#&gt; Warning: The `returnclass` argument of `ne_download()` sp as of rnaturalearth 1.0.0.\n#&gt; ℹ Please use `sf` objects with {rnaturalearth}, support for Spatial objects\n#&gt;   (sp) will be removed in a future release of the package.\nclass(usa)\n#&gt; [1] \"SpatialPolygonsDataFrame\"\n#&gt; attr(,\"package\")\n#&gt; [1] \"sp\"\n\nusa_sf &lt;- st_as_sf(usa)\nplot(usa_sf[1])\n\n\n\nkorea &lt;- ne_countries(country = \"South Korea\") # United States borders\nclass(korea)\n#&gt; [1] \"SpatialPolygonsDataFrame\"\n#&gt; attr(,\"package\")\n#&gt; [1] \"sp\"\nkorea_sf &lt;- st_as_sf(korea)\nplot(korea_sf[1])\n\n\n\n\n\n\n\n\n\nhttps://r.geocompx.org/read-write.html#file-formats\n\n\n\n\n\ngpkg 형식 불러오기\n\n\nf &lt;- system.file(\"shapes/world.gpkg\", package = \"spData\")\nworld = read_sf(f, quiet = TRUE)\ntanzania = read_sf(f, query = 'SELECT * FROM world WHERE name_long = \"Tanzania\"')\ntanzania_buf = st_buffer(tanzania, 50000)\ntanzania_buf_geom = st_geometry(tanzania_buf)\ntanzania_buf_wkt = st_as_text(tanzania_buf_geom)\ntanzania_neigh = read_sf(f, wkt_filter = tanzania_buf_wkt)\n\n\ncsv 형식 불러오기\n\n\ncycle_hire_txt = system.file(\"misc/cycle_hire_xy.csv\", package = \"spData\")\ncycle_hire_xy = read_sf(cycle_hire_txt,\n                        options = c(\"X_POSSIBLE_NAMES=X\", \"Y_POSSIBLE_NAMES=Y\"))\n\n\nWell-known text(WKT), Well-known binary(WKB), and the GeoJSON formats\n\n\nworld_txt = system.file(\"misc/world_wkt.csv\", package = \"spData\")\nworld_wkt = read_sf(world_txt, options = \"GEOM_POSSIBLE_NAMES=WKT\")\n# the same as\nworld_wkt2 = st_read(world_txt, options = \"GEOM_POSSIBLE_NAMES=WKT\",\n                     quiet = TRUE, stringsAsFactors = FALSE, as_tibble = TRUE)\n\n\nKML file stores geographic information in XML format\n\n\nu = \"https://developers.google.com/kml/documentation/KML_Samples.kml\"\ndownload.file(u, \"./Spatial_Information_Analysis/KML_Samples.kml\")\nst_layers(\"./Spatial_Information_Analysis/KML_Samples.kml\")\n#&gt; Driver: KML \n#&gt; Available layers:\n#&gt;              layer_name  geometry_type features fields crs_name\n#&gt; 1            Placemarks       3D Point        3      2   WGS 84\n#&gt; 2      Highlighted Icon       3D Point        1      2   WGS 84\n#&gt; 3                 Paths 3D Line String        6      2   WGS 84\n#&gt; 4         Google Campus     3D Polygon        4      2   WGS 84\n#&gt; 5      Extruded Polygon     3D Polygon        1      2   WGS 84\n#&gt; 6 Absolute and Relative     3D Polygon        4      2   WGS 84\nkml = read_sf(\"./Spatial_Information_Analysis/KML_Samples.kml\", layer = \"Placemarks\")"
  },
  {
    "objectID": "Spatial_Information_Analysis.html#chapter-9-making-maps-with-r",
    "href": "Spatial_Information_Analysis.html#chapter-9-making-maps-with-r",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "정적인 지도는 지리 계산의 가장 일반적인 시각적 출력 유형\nplot() 또는 tmap_mode(plot)\n\n\n\n\n# Add fill layer to nz shape\ntm_shape(nz) +\n  tm_fill()\n# Add border layer to nz shape\ntm_shape(nz) +\n  tm_borders()\n# Add fill and border layers to nz shape\ntm_shape(nz) +\n  tm_fill() +\n  tm_borders()\n\n\n\n\n\n\n\n\n\n\nmap_nz &lt;- tm_shape(nz) + tm_polygons()\nclass(map_nz)\n#&gt; [1] \"tmap\"\nmap_nz\n\n\n\n\nnz_elev = rast(system.file(\"raster/nz_elev.tif\", package = \"spDataLarge\"))\n\nmap_nz1 &lt;- map_nz + tm_shape(nz_elev) + tm_raster(alpha = 0.7)\n\nnz_water &lt;- st_union(nz) %&gt;% st_buffer(22200) %&gt;%\n  st_cast(to = \"LINESTRING\")\n\nmap_nz2 &lt;- map_nz1 +\n  tm_shape(nz_water) + tm_lines()\n\nmap_nz3 &lt;- map_nz2 +\n  tm_shape(nz_height) + tm_dots()\n\ntmap_arrange(map_nz1, map_nz2, map_nz3)\n\n\n\n\n\nalpha : 레이어를 반투명하게 만들기 위해 설정\n\n\n\n\n\nma1 &lt;- tm_shape(nz) + tm_fill(col = \"red\")\nma2 &lt;- tm_shape(nz) + tm_fill(col = \"red\", alpha = 0.3)\nma3 &lt;- tm_shape(nz) + tm_borders(col = \"blue\")\nma4 &lt;- tm_shape(nz) + tm_borders(lwd = 3)\nma5 &lt;- tm_shape(nz) + tm_borders(lty = 2)\nma6 &lt;- tm_shape(nz) + tm_fill(col = \"red\", alpha = 0.3) +\n  tm_borders(col = \"blue\", lwd = 3, lty = 2)\n\ntmap_arrange(ma1, ma2, ma3, ma4, ma5, ma6)\n\n\n\n\n\ntm_fill()과 tm_bubbles()에서 레이어는 기본적으로 회색으로 채워지고 tm_lines()은 검은선으로 그려짐\ntmap의 인수는 숫자 벡터를 허용하지 않음\n\n\nplot(st_geometry(nz), col = nz$Land_area) # works\ntm_shape(nz) + tm_fill(col = nz$Land_area) # fails\n&gt; Error: Fill argument neither colors nor valid variable name(s)\ntm_shape(nz) + tm_fill(col = \"Land_area\")\n\n\n\n\n\n\n\n범례의 제목 설정\n\n\nlegend_title &lt;- expression(\"Area (km\"^2*\")\")\nmap_nza &lt;- tm_shape(nz) +\n  tm_fill(col = \"Land_area\", title = legend_title) + tm_borders()\nmap_nza\n\n\n\n\n\n\n\n\nbreaks : 색상의 표현 값 범위를 수동으로 설정\nn : 숫자 변수가 범주화되는 Bin의 수 설정\npalette : 색 구성표를 정의 (ex. BuGn)\n\n\ntm1 &lt;- tm_shape(nz) + tm_polygons(col = \"Median_income\")\nbreaks = c(0, 3, 4, 5) * 10000\ntm2 &lt;- tm_shape(nz) + tm_polygons(col = \"Median_income\", breaks = breaks)\ntm3 &lt;- tm_shape(nz) + tm_polygons(col = \"Median_income\", n = 10)\ntm4 &lt;- tm_shape(nz) + tm_polygons(col = \"Median_income\", palette = \"BuGn\")\n\ntmap_arrange(tm1, tm2, tm3, tm4)\n\n\n\n\n\ntm_shape(nz) + tm_polygons(col = \"Median_income\", style = \"pretty\")\ntm_shape(nz) + tm_polygons(col = \"Median_income\", style = \"equal\")\ntm_shape(nz) + tm_polygons(col = \"Median_income\", style = \"quantile\")\ntm_shape(nz) + tm_polygons(col = \"Median_income\", style = \"jenks\")\ntm_shape(nz) + tm_polygons(col = \"Median_income\", style = \"cont\")\ntm_shape(nz) + tm_polygons(col = \"Median_income\", style = \"cat\")\n\n\n\n\n\n\n\nstyle = \"pretty\" : 기본 설정은 가능한 경우 정수로 반올림하고 간격을 균등하게 유지\nstyle = \"equal\" : 입력 값을 동일한 범위의 빈으로 나누고 균일한 분포의 변수에 적합(결과 맵이 색상 다양성이 거의 없을 수 있으므로 분포가 치우친 변수에는 권장하지 않음)\nstyle = \"quantile\" : 동일한 수의 관찰이 각 범주에 포함되도록 함(빈 범위가 크게 다를 수 있다는 잠재적인 단점이 있음).\nstyle = \"jenks\" : 데이터에서 유사한 값의 그룹을 식별하고 범주 간의 차이를 최대화\nstyle = \"cont\" : 연속 색상 필드에 많은 색상을 표시하고 연속 래스터에 특히 적합\nstyle = \"cat\" : 범주 값을 나타내도록 설계되었으며 각 범주가 고유한 색상을 받도록 함\n\n\ntm_p1 &lt;- tm_shape(nz) + tm_polygons(\"Population\", palette = \"Blues\")\ntm_p2 &lt;- tm_shape(nz) + tm_polygons(\"Population\", palette = \"YlOrBr\")\n\ntmap_arrange(tm_p1, tm_p2)\n\n\n\n\n\n순차 팔레트는 단일(ex. Blues : 밝은 파란색에서 진한 파란색으로 이동) 또는 다중 색상/색조(ex. YlOrBr : 주황색을 통해 밝은 노란색에서 갈색으로 그라데이션)\n\n\n\n\n\nmap_nz +\n  tm_compass(type = \"8star\", position = c(\"left\", \"top\")) +\n  tm_scale_bar(breaks = c(0, 100, 200), text.size = 1)\n\n\n\n\ntm_l1 &lt;- map_nz + tm_layout(title = \"New Zealand\")\ntm_l2 &lt;- map_nz + tm_layout(scale = 5)\ntm_l3 &lt;- map_nz + tm_layout(bg.color = \"lightblue\")\ntm_l4 &lt;- map_nz + tm_layout(frame = FALSE)\n\ntmap_arrange(tm_l1, tm_l2, tm_l3, tm_l4)\n\n\n\n\n\ntm_layout()의 다양한 옵션\n\nframe.lwd : 프레임 너비\nframe.double.line : 이중선 허용 옵션\nouter.margin, inner.margin : 여백 설정\nfontface, fontfamily : 글꼴 설정\nlegend.show : 범례 표시 여부\nlegend.position : 범례 위치 변경\n\n\n\n\n\n\n\n\ntm_s1 &lt;- map_nza + tm_style(\"bw\")\ntm_s2 &lt;- map_nza + tm_style(\"classic\")\ntm_s3 &lt;- map_nza + tm_style(\"cobalt\")\ntm_s4 &lt;- map_nza + tm_style(\"col_blind\")\n\ntmap_arrange(tm_s1, tm_s2, tm_s3, tm_s4)\n\n\n\n\n\n\n\n\nurb_1970_2030 &lt;- urban_agglomerations %&gt;%\n  filter(year %in% c(1970, 1990, 2010, 2030))\ntm_shape(world) +\n  tm_polygons() +\n  tm_shape(urb_1970_2030) +\n  tm_symbols(col = \"black\", border.col = \"white\", size = \"population_millions\") +\n  tm_facets(by = \"year\", nrow = 2, free.coords = TRUE)\n\n\n\n#free.coords : 지도에 자체 경계 상자가 있는지 여부를 지정\n\n\n\n\n\nnz_region &lt;- st_bbox(c(xmin = 1340000, xmax = 1450000,\n                       ymin = 5130000, ymax = 5210000),\n                     crs = st_crs(nz_height)) %&gt;% st_as_sfc()\n\nnz_height_map &lt;- tm_shape(nz_elev, bbox = nz_region) +\n  tm_raster(style = \"cont\", palette = \"YlGn\", legend.show = TRUE) +\n  tm_shape(nz_height) + tm_symbols(shape = 2, col = \"red\", size = 1) +\n  tm_scale_bar(position = c(\"left\", \"bottom\"))\n\nnz_map &lt;- tm_shape(nz) + tm_polygons() +\n  tm_shape(nz_height) + tm_symbols(shape = 2, col = \"red\", size = 0.1) +\n  tm_shape(nz_region) + tm_borders(lwd = 3)\n\nlibrary(grid)\nnz_height_map\nprint(nz_map, vp = viewport(0.8, 0.27, width = 0.5, height = 0.5))\n\n\n\n\n\nviewport() : 두개의 맵을 결합\n\n\n\n\n\n\nurb_anim &lt;- tm_shape(world) + tm_polygons() +\n  tm_shape(urban_agglomerations) + tm_dots(size = \"population_millions\") +\n  tm_facets(along = \"year\", free.coords = FALSE)\n\ntmap_animation(urb_anim, filename = \"./Spatial_Information_Analysis/urb_anim.gif\", delay = 25)\n#&gt; Creating frames\n#&gt; =========\n#&gt; ====\n#&gt; =====\n#&gt; ====\n#&gt; =====\n#&gt; ====\n#&gt; =====\n#&gt; ====\n#&gt; ====\n#&gt; =====\n#&gt; ====\n#&gt; =====\n#&gt; ====\n#&gt; =====\n#&gt; ====\n#&gt; =====\n#&gt; ====\n#&gt; \n#&gt; Creating animation\n#&gt; Animation saved to C:\\Users\\Hyunsoo Kim\\Desktop\\senior_grade\\blog\\my-quarto-website\\Spatial_Information_Analysis\\urb_anim.gif\n\n\nby = year대신 along = year을 사용\nfree.coords = FALSE : 각 맵 반복에 대한 맵 범위 유지\ntmap_animation()을 사용하여 .gif로 저장\n\n\n\n\n\n대화형 지도는 데이터 세트를 새로운 차원으로 끌어올릴 수 있음\n지도를 기울이고 회전하는 기능과 사용자가 이동 및 확대/축소 할 때 자동으로 업데이트\ntmap, mapview, mapdeck, leaflet으로 표현 가능\n\n\n\n\ntmap_mode(\"view\") #interactive mode\n#&gt; tmap mode set to interactive viewing\nmap_nz\n\n\n\n\n\n\nmap_nz + tm_basemap(server = \"OpenTopoMap\")\n\n\n\n\n\n\nworld_coffee = left_join(world, coffee_data, by = \"name_long\")\nfacets = c(\"coffee_production_2016\", \"coffee_production_2017\")\ntm_shape(world_coffee) + tm_polygons(facets) +\n  tm_facets(nrow = 1, sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntm_basemap() 또는 tm_options()로 basemap 지정 가능\ntm_facets()에서 sync옵션을 TRUE로 선택하면 여러개의 맵을 동시에 확대/축소할 수 있음\n\n\n\n\n\nmapview::mapview(nz)\n\ntrails %&gt;%\n  st_transform(st_crs(franconia)) %&gt;%\n  st_intersection(franconia[franconia$district == \"Oberfranken\", ][1]) %&gt;%\n  st_collection_extract(\"LINE\") %&gt;%\n  mapview(color = \"red\", lwd = 3, layer.name = \"trails\") +\n  mapview(franconiWa, zcol = \"district\", burst = TRUE) +\n  breweries\n\n\n\n\n\nset_token(Sys.getenv(\"pk.eyJ1IjoiancwMTEyIiwiYSI6ImNsM2ppbzYzNzBrbjQzZHBjMmlocnY2dDUifQ.58-gXpPtvcCGmMt2xEW-ig\"))\ncrash_data = read.csv(\"https://git.io/geocompr-mapdeck\")\ncrash_data = na.omit(crash_data)\nms = mapdeck_style(\"dark\")\nmapdeck(style = ms, pitch = 45, location = c(0, 52), zoom = 4) %&gt;%\n  add_grid(data = crash_data, lat = \"lat\", lon = \"lng\", cell_size = 1000,\n           elevation_scale = 50, layer_id = \"grid_layer\",\n           colour_range = viridisLite::plasma(6))\n#&gt; Registered S3 method overwritten by 'jsonify':\n#&gt;   method     from    \n#&gt;   print.json jsonlite\n\n\n\n\n\n\n\n\n\nadd_arc() 함수\n\n\nurl &lt;- 'https://raw.githubusercontent.com/plotly/datasets/master/2011_february_aa_flight_paths.csv'\nflights &lt;- read.csv(url)\nflights$id &lt;- seq_len(nrow(flights))\nflights$stroke &lt;- sample(1:3, size = nrow(flights), replace = T)\nkey = \"pk.eyJ1IjoiancwMTEyIiwiYSI6ImNsM2ppbzYzNzBrbjQzZHBjMmlocnY2dDUifQ.58-gXpPtvcCGmMt2xEW-ig\"\n\nmapdeck(token = key, style = mapdeck_style(\"dark\"), pitch = 45 ) %&gt;%\n  add_arc(\n    data = flights\n    , layer_id = \"arc_layer\"\n    , origin = c(\"start_lon\", \"start_lat\")\n    , destination = c(\"end_lon\", \"end_lat\")\n    , stroke_from = \"airport1\"\n    , stroke_to = \"airport2\"\n    , stroke_width = \"stroke\"\n  )\n\n\nadd_animated_arc() 함수\n\n\nmapdeck(token = key, style = 'mapbox://styles/mapbox/dark-v9', pitch = 45 ) %&gt;%\n  add_animated_arc(\n    data = flights\n    , layer_id = \"arc_layer\"\n    , origin = c(\"start_lon\", \"start_lat\")\n    , destination = c(\"end_lon\", \"end_lat\")\n    , stroke_from = \"airport1\"\n    , stroke_to = \"airport2\"\n    , stroke_width = \"stroke\"\n  )\n\n\nadd_heatmap() 함수\n\n\nmapdeck(token = key, style = mapdeck_style('dark'), pitch = 45 ) %&gt;%\n  add_heatmap(\n    data = df[1:30000, ]\n    , lat = \"lat\"\n    , lon = \"lng\"\n    , weight = \"weight\"\n    , colour_range = colourvalues::colour_values(1:6, palette = \"inferno\")\n  )\n\n\nadd_path() 함수\n\n\nmapdeck(\n  token = key\n  , style = mapdeck_style(\"dark\")\n  , zoom = 10) %&gt;%\n  add_path(\n    data = roads\n    , stroke_colour = \"RIGHT_LOC\"\n    , layer_id = \"path_layer\"\n  )\n\n\nadd_geojson(), add_scatterplot(), add_text() 등이 있음\n\n\n\n\n\npal = colorNumeric(\"RdYlBu\", domain = cycle_hire$nbikes)\nleaflet(data = cycle_hire) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%       # Background Map\n  addCircles(col = ~pal(nbikes), opacity = 0.9) %&gt;%      # nbikes의 값으로 색이 다르게 circle 생성\n  addPolygons(data = lnd, fill = FALSE) %&gt;%              # land에 따라 Polygon 생성\n  addLegend(pal = pal, values = ~nbikes) %&gt;%             # 범례 생성\n  setView(lng = -0.1, 51.5, zoom = 12) %&gt;%               # zoom\n  addMiniMap()                                           # minimap 생성\n\n\n\n\n\n\n# create a basic map\n\nleaflet() %&gt;%\n  addTiles() %&gt;% # add default OpenStreetMap map tiles\n  setView(lng=127.063, lat=37.513, zoom = 6) # korea, zoom 6\n\n\n\n\n\n# map style: NASA\n\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng=127.063, lat=37.513, zoom = 6) %&gt;%\n  addProviderTiles(\"NASAGIBS.ViirsEarthAtNight2012\")\n\n\n\n\n\n# map style: Esri.WorldImagery\n\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng=127.063, lat=37.513, zoom = 16) %&gt;%\n  addProviderTiles(\"Esri.WorldImagery\")\n\n\n\n\n\n# adding Popup\n\npopup = c(\"한남대학교 빅데이터응용학과\")\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  addMarkers(lng = c(127.4219), # longitude\n             lat = c(36.3548), # latitude\n             popup = popup)\n\n\n\n\n\n\nzoom : 확대/축소 비율 설정\naddProviderTiles() : 외부 지도 타일 추가\naddMarkers() : 커서를 클릭했을 때 팝업으로 나타나는 설명을 추가\n\n\n\n\n\n\n\n\nR을 사용하여 한걸음 더 나아가 웹 어플리케이션을 제작할 수 있게 해주는 패키지\nui 라고 말하는 화면은 실제로 사용자가 보는 화면\nshiny에서는 크게 titlePanel과 sidebarPanel, mainPanal의 세 가지로 구성\n\n\nui = fluidPage(\n  sliderInput(inputId = \"life\", \"Life expectancy\", 49, 84, value = 80),\n  leafletOutput(outputId = \"map\")\n)\nserver = function(input, output) {\n  output$map = renderLeaflet({\n    leaflet() %&gt;%\n      # addProviderTiles(\"OpenStreetMap.BlackAndWhite\") %&gt;%\n      addPolygons(data = world[world$lifeExp &lt; input$life,])\n  })\n}\nshinyApp(ui, server)\n#&gt; PhantomJS not found. You can install it with webshot::install_phantomjs(). If it is installed, please make sure the phantomjs executable can be found via the PATH variable.\n\nShiny applications not supported in static R Markdown documents\n\n\n\nui &lt;- fluidPage(#Application title\n  titlePanel(\"Hello Shiny!\"),\n  #Sidebar with a slider input for the number of bins\n  sidebarLayout(sidebarPanel(\n    sliderInput(\n      \"bins\",\n      \"Number of bins:\",\n      min = 1,\n      max = 50,\n      value = 30\n    )\n  ),\n  #Show a plot of the generated distribution\n  mainPanel(plotOutput(\"distPlot\"))))\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    x &lt;- faithful[, 2]\n    bins &lt;- seq(min(x), max(x), length.out = input$bins + 1)\n    hist(x,\n         breaks = bins,\n         col = 'darkgray',\n         border = 'white')\n  })\n}\nshinyApp(ui, server)\n\nShiny applications not supported in static R Markdown documents"
  },
  {
    "objectID": "Spatial_Information_Analysis.html#ggmap",
    "href": "Spatial_Information_Analysis.html#ggmap",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "지도 공간 기법으로 시각화하는 ggmap 패키지는 Google Maps, Stamen Maps, 네이버 맵, 등의 다양한 온라인 소스로부터 가져온 정적인 지도 위에 특별한 데이터나 모형을 시각화하는 함수들을 제공함\nggmap()의 주요 함수\n\ngeocode() : 거리주소 또는 장소 이름을 이용하여 이용 지도 정보(위도, 경도) 획득\nget_googlemap() : 구글 지도 서비스 API에 접근하여 정적 지도 다운로드 지원과 지도에 marker 등을 삽입하고 자신이 원하는 줌 레벨과 center를 지정하여 지도 정보 생성\nget_map() : 지도 서비스 관련 서버에 관련 질의어를 지능형으로 인식하여 지도 정보 생성\nget_navermap() : 네이버 지도 서비스 API에 접근하여 정적 지도 다운로드 지원\nggimage() : ggplot2 패키지의 이미지와 동등한 수준으로 지도 이미지 생성\nggmap(), ggmapplot() : get_map() 함수에 의해서 생성된 픽셀 객체를 지도 이미지로 시각화\nqmap() : ggmap()함수와 get_map() 함수의 통합기능\nqmplot() : ggplot2 패키지의 qplot()와 동등한 수준으로 빠르게 지도 이미지 시각화\n\n\n\n\n\nget_googlemap() 함수를 통해 불러오고 싶은 곳의 장소를 문자열 값으로 첫 번째 인자에 넣어 실행해 이를 객체화 함\nggmap() 함수 안에 방금 만든 객체를 입력시킨 후 실행하면 원하는 장소를 중심으로 구글 지도가 plotting 됨\n\n\n# install.packages(\"ggmap\")\nlibrary(ggmap)\nregister_google(key = 'AIzaSyB4jjrVVAzb9fl8FQrQqUONAsaRBppWuSA')\n\n# 우리나라 지도 호출\ngetmap &lt;- get_googlemap(\"seoul\")\nggmap(getmap)\n\n\n\n\n\nggmap() 으로 반환되는 결과물은 ggplot2 패키지의 함수와 조합해 지도 위에 새로운 정보들을 추가할 수 있음\n\n\n\n\ndaejeon_map &lt;- get_googlemap(\"daejeon\") %&gt;% ggmap\nlocation &lt;- data.frame(\n  Name = c(\"한남대학교\", \"대전신세계\"),\n  lon = c(127.4219, 127.3821), #경도\n  lat = c(36.3548, 36.3752)    #위도\n)\n\ndaejeon_map + geom_point(data = location, aes(x = lon, y = lat))\n\ndaejeon_map &lt;- get_googlemap(\"daejeon\", zoom = 13) %&gt;% ggmap\nlocation &lt;- data.frame(\n  Name = c(\"한남대학교\", \"대전신세계\"),\n  lon = c(127.4219, 127.3821),\n  lat = c(36.3548, 36.3752)\n)\n\ndaejeon_map + geom_point(data = location, aes(x = lon, y = lat)) +\n  geom_text(data = location,\n            aes(label = Name),\n            size = 5,   # text 크기\n            vjust = -1) # text 위치\n\n\ngeom_point() 내의 옵션을 선택하여 점의 크기, 색깔, 모양 등 변경 가능\n\n\ndaejeon_map + geom_point(data = location, aes(x = lon, y = lat),\n                         size = 5, color = \"red\", alpha = 0.4) +\ngeom_text(data = location, aes(label = Name), size = 5, vjust = -1)\n\n\n한남대학교를 중심으로 그리기(center)\n\nenc2utf8 : UTF-8로 인코딩\nmaptype : “terrain”, “satellite”, “roadmap”, “hybrid”\ncenter : 맵의 중심\n\n\n# 한남대학교를 중심으로 그리기\ngc &lt;- geocode(enc2utf8(\"한남대학교\"))\n\nmap &lt;- get_googlemap(\n  center = as.numeric(gc),\n  maptype = \"roadmap\",\n  zoom = 13,\n  size = c(640, 640),\n  markers = gc) %&gt;% ggmap\n\nmap + geom_point(data = location, aes(x = lon, y = lat)) +\ngeom_text(data = location, aes(label = Name), size = 5, vjust = -1)\n\nPath(경로)\n\n\nmap + geom_path(data = location, aes(x = lon, y = lat), color = \"blue\", alpha = .5, lwd = 1)\n\n\n두 지역 사이의 경로 좌표 추출\n\nggmap::route : find a route from Google using different possible modes (\"driving\", \"walking\", \"bicycling\", \"transit\")\n\n\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(tmap)\nlibrary(stplanr)\n\ngc_st &lt;- geocode(enc2utf8(\"한남대학교\"))\ngc_ed &lt;- geocode(enc2utf8(\"신세계백화점 대전신세계아트앤사이언스\"))\ngc_od &lt;- st_linestring(rbind(as.numeric(gc_st), as.numeric(gc_ed)))\n\nst_sfc(gc_od) # Linestring, CRS 없음\nst_crs(gc_od)\ngc_od &lt;- st_sfc(gc_od, crs = 4326)\n# st_sfc() : 좌표계가 비어있는 경우에 좌표계 지정\nst_crs(gc_od)\n\nqtm(gc_od)\ngc_od &lt;- st_sf(gc_od)\n# st_sf() : sfc와 sf class의 객체들을 하나로 통합\ngc_od$distance &lt;- as.numeric(st_length(gc_od))\n\nroute_od = route(l = gc_od,             # l : linestring\n                 route_fun = route_osrm,\n                 osrm.profile = \"car\")  # foot, bike, car\nqtm(route_od)\n\nmap &lt;- get_googlemap(\n  center = c(127.41, 36.37),\n  maptype = \"roadmap\",\n  zoom = 14,\n  size = c(640, 640),\n  markers = gc\n) %&gt;% ggmap(extent = \"device\")\n\nmap\n\nmap + geom_sf(data = route_od, inherit.aes = F)\n# inherit.aes = F : sf형식의 데이터를 그릴 때 필수 옵션\n\n지도를 꽉 채워서 출력(x, y축 삭제하고 그림만 출력)\n\nextent = \"device\"\n+ theme_void()\n\n\nmap &lt;- get_googlemap(\n  center = as.numeric(gc),\n  maptype = \"roadmap\",\n  zoom = 13,\n  size = c(640, 640),\n  markers = gc\n) %&gt;% ggmap(extent = \"device\")\nmap\n\nmap &lt;- get_googlemap(\n  center = as.numeric(gc),\n  maptype = \"roadmap\",\n  zoom = 13,\n  size = c(640, 640),\n  markers = gc\n) %&gt;% ggmap() + theme_void()\nmap\n\n\n\n\n\n\n# Houston 범죄 데이터\nstr(crime)\nHoustonmap &lt;- get_map(\"Houston\")\nggmap(Houstonmap)\nggmap(Houstonmap) + geom_point(data = crime, aes(x = lon, y = lat))\nggmap(Houstonmap) + geom_point(data = crime, aes(x = lon, y = lat), size = 0.1, alpha = 0.1) # 점의 크기, 점의 투명도 조절\n\n#지도 확대 & 특정 지역 데이터만 추출하기\nHoustonmap &lt;- get_map(\"Houston\", zoom = 14)\ncrime1 &lt;- crime[(crime$lon &lt; -95.344 & crime$lon &gt; -95.395) & (crime$lat &lt; 29.783 & crime$lat &gt; 29.738), ]\ncrime11 &lt;- crime %&gt;% filter((lon &lt; -95.344 & lon &gt; -95.395) & (lat &lt; 29.783 & lat &gt; 29.738))\nnrow(crime1) ; nrow(crime11)\ncrime1 %&gt;% arrange(desc(lon)) %&gt;% nrow()\ncrime11 %&gt;% arrange(desc(lon)) %&gt;% nrow()\n\nggmap(Houstonmap) + geom_point(data = crime1, aes(x = lon, y = lat), alpha = 0.3)\nggmap(Houstonmap) + geom_point(data = crime1, aes(x = lon, y = lat, colour = offense))\n\ncrime2 &lt;- crime1[!duplicated(crime1[, c(\"lon\", \"lat\")]), ] # 위, 경도에 대해 중복되지 않게 하나의 관측치만 선택\n\ncrime2$offense &lt;- as.character(crime2$offense) # 범죄 종류 문자형으로 변경\n\ncrime2$offense[crime2$offense == \"murder\" | crime2$offense == \"rape\"] &lt;- \"4\"\ncrime2$offense[crime2$offense == \"robbery\" | crime2$offense == \"aggravated assault\"] &lt;- \"3\"\ncrime2$offense[crime2$offense == \"burglary\" | crime2$offense == \"auto theft\"] &lt;- \"2\"\ncrime2$offense[crime2$offense == \"theft\"] &lt;- \"1\"\n\ncrime2$offense &lt;- as.numeric(crime2$offense) # 범죄 종류 문자형을 숫자형으로 변경\n\nggmap(Houstonmap) + geom_point(data = crime2, aes(x = lon, y = lat, size = offense), alpha = 0.2)\n\n# 범죄 위험도에 따라 점의 크기 및 색깔로 구별\nggmap(Houstonmap) + geom_point(data = crime2, aes(x = lon, y = lat, size = offense, colour = offense), alpha = 0.5) +\n  scale_colour_gradient(low = \"white\", high = \"red\")\n\ncrime3 &lt;- crime2[crime2$date == \"1/1/2010\", ]\n\ncrime4 &lt;- crime3[!duplicated(crime3[, c(\"hour\")]), ]\n\nnrow(crime3) ; nrow(crime4)\n\nggmap(Houstonmap) + geom_point(data = crime3, aes(x = lon, y = lat)) +\n  geom_text(data = crime4, aes(label = street), vjust = 1.2) +\n  geom_path(data = crime4, aes(x = lon, y = lat), color = \"red\")"
  },
  {
    "objectID": "Spatial_Information_Analysis.html#chapter13-transportation",
    "href": "Spatial_Information_Analysis.html#chapter13-transportation",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "names(bristol_zones) ; names(bristol_od)\n#&gt; [1] \"geo_code\" \"name\"     \"geometry\"\n#&gt; [1] \"o\"          \"d\"          \"all\"        \"bicycle\"    \"foot\"      \n#&gt; [6] \"car_driver\" \"train\"\nnrow(bristol_zones)  ; nrow(bristol_od)\n#&gt; [1] 102\n#&gt; [1] 2910\n\n# O : Zone of the Origin / D : Zone of the Dest\n\nzones_attr = bristol_od %&gt;%\n  group_by(o) %&gt;%\n  summarize_if(is.numeric, sum) %&gt;%\n  dplyr::rename(geo_code = o)\n\nsummary(zones_attr$geo_code %in% bristol_zones$geo_code) # 일치하는지 확인\n#&gt;    Mode    TRUE \n#&gt; logical     102\n\n\nzones_joined = left_join(bristol_zones, zones_attr, by = \"geo_code\")\nnrow(zones_joined)\n#&gt; [1] 102\nsum(zones_joined$all)\n#&gt; [1] 238805\n\nnames(zones_joined)\n#&gt; [1] \"geo_code\"   \"name\"       \"all\"        \"bicycle\"    \"foot\"      \n#&gt; [6] \"car_driver\" \"train\"      \"geometry\"\n#&gt; [1] \"geo_code\" \"name\" \"all\" \"bicycle\" \"foot\" \"car_driver\" \"train\" \"geometry\"\nnames(zones_joined)[3] &lt;- c(\"all_orig\")\nnames(zones_joined)\n#&gt; [1] \"geo_code\"   \"name\"       \"all_orig\"   \"bicycle\"    \"foot\"      \n#&gt; [6] \"car_driver\" \"train\"      \"geometry\"\n\nzones_od = bristol_od %&gt;%\n  group_by(d) %&gt;%\n  summarize_if(is.numeric, sum) %&gt;%\n  dplyr::select(geo_code = d, all_dest = all) %&gt;%\n  inner_join(zones_joined, ., by = \"geo_code\")\n\nzones_od\n#&gt; Simple feature collection with 102 features and 8 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -2.845847 ymin: 51.28248 xmax: -2.252388 ymax: 51.73982\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;     geo_code                             name all_orig bicycle foot car_driver\n#&gt; 1  E02002985 Bath and North East Somerset 001      868      30  173        414\n#&gt; 2  E02002987 Bath and North East Somerset 003      898      34  117        523\n#&gt; 3  E02003005 Bath and North East Somerset 021      786      19   91        593\n#&gt; 4  E02003012                      Bristol 001     3312     161  330       2058\n#&gt; 5  E02003013                      Bristol 002     3715     188  615       2021\n#&gt; 6  E02003014                      Bristol 003     2220     126  270       1239\n#&gt; 7  E02003015                      Bristol 004     1633     166  307        786\n#&gt; 8  E02003016                      Bristol 005     2411     218  440       1105\n#&gt; 9  E02003017                      Bristol 006     1590     187  208        898\n#&gt; 10 E02003018                      Bristol 007     1690      96  143       1048\n#&gt;    train all_dest                       geometry\n#&gt; 1     43      744 MULTIPOLYGON (((-2.510462 5...\n#&gt; 2     58      561 MULTIPOLYGON (((-2.476122 5...\n#&gt; 3      8      427 MULTIPOLYGON (((-2.55073 51...\n#&gt; 4     12      701 MULTIPOLYGON (((-2.595763 5...\n#&gt; 5      6      940 MULTIPOLYGON (((-2.593783 5...\n#&gt; 6      5     3469 MULTIPOLYGON (((-2.639581 5...\n#&gt; 7      7     4980 MULTIPOLYGON (((-2.584973 5...\n#&gt; 8     23      297 MULTIPOLYGON (((-2.565948 5...\n#&gt; 9      9     1459 MULTIPOLYGON (((-2.616485 5...\n#&gt; 10    20      128 MULTIPOLYGON (((-2.637681 5...\n\nqtm(zones_od, c(\"all_orig\", \"all_dest\")) +\ntm_layout(panel.labels = c(\"Origin\", \"Destination\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nod_top5 = bristol_od %&gt;%\n  arrange(desc(all)) %&gt;%\n  top_n(5, wt = all)\n\nbristol_od$Active = (bristol_od$bicycle + bristol_od$foot) / bristol_od$all * 100\n\nod_intra = filter(bristol_od, o == d) # 지역 내 이동\nod_inter = filter(bristol_od, o != d) # 지역 외 이동\nod_intra ; od_inter # 102행 / 2808행\n#&gt; # A tibble: 102 × 8\n#&gt;    o         d           all bicycle  foot car_driver train Active\n#&gt;    &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1 E02002985 E02002985   209       5   127         59     0   63.2\n#&gt;  2 E02002987 E02002987   166       8    61         89     2   41.6\n#&gt;  3 E02003005 E02003005   383       8    87        256     1   24.8\n#&gt;  4 E02003012 E02003012   315       5   181        102     0   59.0\n#&gt;  5 E02003013 E02003013   318       7   165        112     0   54.1\n#&gt;  6 E02003014 E02003014   414      35   139        185     0   42.0\n#&gt;  7 E02003015 E02003015   240      18   142         61     0   66.7\n#&gt;  8 E02003016 E02003016   119       7    65         30     2   60.5\n#&gt;  9 E02003017 E02003017   147       8    70         60     1   53.1\n#&gt; 10 E02003018 E02003018    67       0    39         24     1   58.2\n#&gt; # ℹ 92 more rows\n#&gt; # A tibble: 2,808 × 8\n#&gt;    o         d           all bicycle  foot car_driver train Active\n#&gt;    &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1 E02002985 E02002987   121       7    35         62     0  34.7 \n#&gt;  2 E02002985 E02003036    32       2     1         10     1   9.38\n#&gt;  3 E02002985 E02003043   141       1     2         56    17   2.13\n#&gt;  4 E02002985 E02003049    56       2     4         36     0  10.7 \n#&gt;  5 E02002985 E02003054    42       4     0         21     0   9.52\n#&gt;  6 E02002985 E02003100    22       0     0         19     3   0   \n#&gt;  7 E02002985 E02003106    48       3     1         33     8   8.33\n#&gt;  8 E02002985 E02003108    31       0     0         29     1   0   \n#&gt;  9 E02002985 E02003121    42       1     2         34     0   7.14\n#&gt; 10 E02002985 E02006887   103       5     1         36    13   5.83\n#&gt; # ℹ 2,798 more rows\n\ndesire_lines = od2line(od_inter, zones_od)\n#&gt; Creating centroids representing desire line start and end points.\n# od2line : polygon으로 되어있는 두 지역의 중심점을 계산해서 linestring으로 변환\n#&gt; Creating centroids representing desire line start and end points.\nqtm(desire_lines, lines.lwd = \"all\")\n#&gt; Legend for line widths not available in view mode.\n\n\n\n\n\n\ndesire_lines\\(distance = as.numeric(st_length(desire_lines)) desire_carshort = dplyr::filter(desire_lines, car_driver &gt; 300 & distance &lt; 5000) route_carshort = route(l = desire_carshort, route_fun = route_osrm, osrm.profile = \"car\") # foot, bike, car desire_carshort\\)geom_car = st_geometry(route_carshort)\nplot(st_geometry(desire_carshort)) plot(desire_carshort$geom_car, col = “red”, add = TRUE) plot(st_geometry(st_centroid(zones_od)), add = TRUE)\n\ngetmap &lt;- get_googlemap(\"bristol\", zoom = 11)\nbristol_map &lt;- ggmap(getmap)\n\n# 센터 조정\ngetmap &lt;- get_googlemap(center = c(-2.56, 51.53), zoom = 12)\nbristol_map &lt;- ggmap(getmap)\nbristol_map + geom_sf(data = desire_carshort, inherit.aes = F) +\n  geom_sf(data = desire_carshort$geom_car,\n          inherit.aes = F,\n          col = \"red\") +\n  geom_sf(data = st_geometry(st_centroid(zones_od)), inherit.aes = F)"
  },
  {
    "objectID": "Spatial_Information_Analysis.html#사망교통사고-정보-분석",
    "href": "Spatial_Information_Analysis.html#사망교통사고-정보-분석",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "도로교통공단 TAAS에서는 사망교통사고 정보를 공개하고 있음\n\n교통사고 일시 부터 30일이내 사망한 경우를 사망교통사고라 정의하고 사고정보를 선택한 조건에 따라 json/xml형식으로 제공\n사망 교통 사고 정보\n\n사망사고 년, 월, 일, 시, 주야\n사망사고 건수\n사망사고 사망자수, 부상자수, 중상자수, 경상자수, 부상신고자수\n사망사고 위치 좌표 및 지역명\n사망사고 유형, 위반사항, 차량 종류, 도로 형태\n\n\n데이터 불러오기(https://taas.koroad.or.kr/api/selectDeathDataSet.do)\n\n다운받은 데이터를 R로 불러온 뒤 데이터 속성 확인하세요. 어떤 정보가 있는지, 활용할 위치 정보가 있는지 확인하세요\n\n\nSys.setlocale(\"LC_ALL\",\"Korean\")\n#&gt; Warning in Sys.setlocale(\"LC_ALL\", \"Korean\"): using locale code page other than\n#&gt; 65001 (\"UTF-8\") may cause problems\n#&gt; [1] \"LC_COLLATE=Korean_Korea.949;LC_CTYPE=Korean_Korea.949;LC_MONETARY=Korean_Korea.949;LC_NUMERIC=C;LC_TIME=Korean_Korea.949\"\ngetwd()\n#&gt; [1] \"C:/Users/Hyunsoo Kim/Desktop/senior_grade/blog/my-quarto-website\"\nraw.data &lt;- read.csv(\"./Spatial_Information_Analysis/12_20_death.csv\", header = TRUE, fileEncoding = \"EUC-KR\")\n## 구조 확인\nstr(raw.data)\n#&gt; 'data.frame':    37128 obs. of  23 variables:\n#&gt;  $ 발생년               : int  2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 ...\n#&gt;  $ 발생년월일시         : int  2012010101 2012010101 2012010108 2012010110 2012010103 2012010116 2012010210 2012010104 2012010104 2012010102 ...\n#&gt;  $ 주야                 : chr  \"야간\" \"야간\" \"주간\" \"주간\" ...\n#&gt;  $ 요일                 : chr  \"일\" \"일\" \"일\" \"일\" ...\n#&gt;  $ 사망자수             : int  1 1 1 2 1 1 2 1 1 1 ...\n#&gt;  $ 사상자수             : int  1 6 1 2 1 1 2 1 2 4 ...\n#&gt;  $ 중상자수             : int  0 5 0 0 0 0 0 0 1 0 ...\n#&gt;  $ 경상자수             : int  0 0 0 0 0 0 0 0 0 3 ...\n#&gt;  $ 부상신고자수         : int  0 0 0 0 0 0 0 0 0 0 ...\n#&gt;  $ 발생지시도           : chr  \"서울\" \"전북\" \"충남\" \"경남\" ...\n#&gt;  $ 발생지시군구         : chr  \"은평구\" \"정읍시\" \"청양군\" \"합천군\" ...\n#&gt;  $ 사고유형_대분류      : chr  \"차대사람\" \"차대차\" \"차량단독\" \"차대차\" ...\n#&gt;  $ 사고유형_중분류      : chr  \"차도통행중\" \"정면충돌\" \"공작물충돌\" \"측면충돌\" ...\n#&gt;  $ 사고유형             : chr  \"차도통행중\" \"정면충돌\" \"공작물충돌\" \"측면충돌\" ...\n#&gt;  $ 법규위반             : chr  \"안전운전 의무 불이행\" \"중앙선 침범\" \"안전운전 의무 불이행\" \"과속\" ...\n#&gt;  $ 도로형태_대분류      : chr  \"단일로\" \"단일로\" \"단일로\" \"교차로\" ...\n#&gt;  $ 도로형태             : chr  \"기타단일로\" \"기타단일로\" \"기타단일로\" \"교차로내\" ...\n#&gt;  $ 당사자종별_1당_대분류: chr  \"승용차\" \"승용차\" \"승용차\" \"승합차\" ...\n#&gt;  $ 당사자종별_2당_대분류: chr  \"보행자\" \"승용차\" \"없음\" \"승용차\" ...\n#&gt;  $ 발생위치X_UTMK       : int  949860 946537 940016 1059321 1070222 1036880 1079124 1114053 911131 955269 ...\n#&gt;  $ 발생위치Y_UTMK       : int  1957179 1737695 1832833 1748774 1834630 1827821 1708218 1761943 1861851 1952221 ...\n#&gt;  $ 경도                 : num  127 127 127 128 128 ...\n#&gt;  $ 위도                 : num  37.6 35.6 36.5 35.7 36.5 ...\n## 테이블 확인\nView(raw.data)\n\n데이터 추출하기\n\n다운받은 데이터는 전국에 대한 사망교통사고 정보이다. 대전지역에 2016년부터 2020년까지의 정보만을 추출하세요.\n\n추출한 데이터의 경도, 위도에 결측값 및 0인 데이터가 있는지 확인하세요.\n\n\n## 1. 대전 지역 2016 ~ 2020년 데이터 추출\ndaejeon &lt;- filter(raw.data,  발생지시도 == \"대전\" &  발생년 &gt; 2015)\n\n## 2. 사고 발생 시작점 경도/위도 데이터의 범위 살펴보기\nrange(daejeon$경도) ; range(daejeon$위도)\n#&gt; [1] 127.2653 127.5278\n#&gt; [1] 36.22335 36.45634\n\n## 3. 경도/위도 데이터가 NA인 데이터 확인하기\nsum(is.na(daejeon$경도)) ; sum(is.na(daejeon$위도))\n#&gt; [1] 0\n#&gt; [1] 0\n\n## 4. 경도/위도 데이터가 0인 데이터 확인하기\ndaejeon[daejeon$경도 == 0,]\n#&gt;  [1] 발생년                발생년월일시          주야                 \n#&gt;  [4] 요일                  사망자수              사상자수             \n#&gt;  [7] 중상자수              경상자수              부상신고자수         \n#&gt; [10] 발생지시도            발생지시군구          사고유형_대분류      \n#&gt; [13] 사고유형_중분류       사고유형              법규위반             \n#&gt; [16] 도로형태_대분류       도로형태              당사자종별_1당_대분류\n#&gt; [19] 당사자종별_2당_대분류 발생위치X_UTMK        발생위치Y_UTMK       \n#&gt; [22] 경도                  위도                 \n#&gt; &lt;0 rows&gt; (or 0-length row.names)\ndaejeon[daejeon$위도 == 0,]\n#&gt;  [1] 발생년                발생년월일시          주야                 \n#&gt;  [4] 요일                  사망자수              사상자수             \n#&gt;  [7] 중상자수              경상자수              부상신고자수         \n#&gt; [10] 발생지시도            발생지시군구          사고유형_대분류      \n#&gt; [13] 사고유형_중분류       사고유형              법규위반             \n#&gt; [16] 도로형태_대분류       도로형태              당사자종별_1당_대분류\n#&gt; [19] 당사자종별_2당_대분류 발생위치X_UTMK        발생위치Y_UTMK       \n#&gt; [22] 경도                  위도                 \n#&gt; &lt;0 rows&gt; (or 0-length row.names)"
  },
  {
    "objectID": "Spatial_Information_Analysis.html#spspatial-objects-객체클래스로-문제-풀기",
    "href": "Spatial_Information_Analysis.html#spspatial-objects-객체클래스로-문제-풀기",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "## 5. 년도별 사고 위치 정보 지도 상에 표출하기\nlibrary(ggmap)\nregister_google(key = 'AIzaSyB4jjrVVAzb9fl8FQrQqUONAsaRBppWuSA')\nmap &lt;- qmap(location = enc2utf8(\"대전\"),\n            zoom = 12,\n            maptype = \"roadmap\")\np &lt;-\n  map + geom_point(\n    data = daejeon,\n    aes(x = 경도, y = 위도, colour = factor(발생년)),\n    size = 2,\n    alpha = 0.7\n  )\np + ggtitle(\"대전시 사망사고 위치(2016-2020)\")\n\n\nstat_bin2d() 함수 활용하여 Grid 내 사고횟수 출력\n\n\n## stat_bin2d() 함수 활용하여 특정 영역 내 사고 횟수 출력\np &lt;- map + stat_bin2d(data = daejeon,\n                      aes(x = 경도, y = 위도),\n                      bins = 30,   # bins : grid의 개수\n                      alpha = 0.5) # binwidth 로도 가능\np\n## stat_bin2d() 함수 활용하여 특정 영역 내 사고 횟수 출력/위성지도/컬러 변경\nmap &lt;- qmap(location = \"Daejeon\",\n            zoom = 12,\n            maptype = \"satellite\")\np &lt;- map + stat_bin2d(data = daejeon,\n                      aes(x = 경도, y = 위도),\n                      bins = 30,\n                      alpha = 0.5) # binwidth 로도 가능\np + scale_fill_gradient(low = \"yellow\", high = \"red\")\n\n\n\n\nGrid 내에 Count된 값 및 위치 확인하기\n\n\np_count &lt;- ggplot_build(p)$data[[4]] # cell의 값 출력\np_count &lt;- arrange(p_count, desc(value))\nhead(p_count)\n\n\nGrid 내(중심)에 Count값 표출\n\n\np + scale_fill_gradient(low = \"yellow\", high = \"red\") +\n  geom_text(data = p_count, aes((xmin + xmax) / 2, (ymin + ymax) / 2,\n                                label = count), col = \"white\")\n\n\n사고 유형 별로 표시하기\n\n\np &lt;-\n  map + stat_bin2d(\n    data = daejeon,\n    aes(\n      x = 경도,\n      y = 위도,\n      colour = factor(사고유형),\n      fill = factor(사고유형)\n    ),\n    bins = 30,\n    alpha = 0.5\n  )\np\n\n\nstat_density2d() 함수 활용하여 등고선으로 지도 위에 출력하기\n\n\nmap &lt;-\n  qmap(\n    location = \"Daejeon\",\n    zoom = 12,\n    maptype = 'roadmap',\n    color = 'bw'\n  )\np &lt;-\n  map + stat_density2d(\n    data = daejeon,\n    aes(x = 경도, y = 위도, fill = ..level..),\n    bins = 5,\n    alpha = 0.45,\n    size = 2,\n    geom = \"polygon\"\n  )\n# level : 레벨이 높을수록 더 진한색, size : 선 굵기, bins: 선 간격\np\n\n\ngeom_hex() 함수 활용하여 벌집 블롯으로 출력하기\n\n\n## 벌집 블롯으로 출력(geom_hex(), scale_fill_gradientn())\nlibrary(hexbin)\nmap &lt;- qmap(location = \"Daejeon\",\n            zoom = 11,\n            maptype = \"roadmap\")\np &lt;-\n  map + coord_cartesian() + # coord_cartesian() : 데카르트 좌표계\n  geom_hex(\n    data = daejeon,\n    aes(x = 경도, y = 위도),\n    bins = 12,\n    alpha = 0.6,\n    color = \"white\"\n  ) # geom_hex() only works with Cartesian coordinates\np\np &lt;- p + scale_fill_gradientn(colours = terrain.colors(15))\np\n\n# binwidth로 출력\np &lt;-\n  map + coord_cartesian() + geom_hex(\n    data = daejeon,\n    binwidth = c(0.05, 0.05), # binwidth : bin의 크기 설정\n    aes(x = 경도, y = 위도),\n    alpha = 0.6,\n    color = \"white\"\n  ) # geom_hex() only works with Cartesian coordinates\np\np &lt;- p + scale_fill_gradientn(colours = terrain.colors(15))\np\n\np_count &lt;- ggplot_build(p)$data[[4]] # cell의 값 출력\np_count &lt;- arrange(p_count, desc(count))\nhead(p_count)\n\n\n\n\n\n\n행정구역시군구 경계를 얻기 위해 데이터로 대전시 구 경계 shape 파일 획득\n\n\nlibrary(raster)\nlibrary(rgdal)\nlibrary(sf)\n\n## 동별 사망사고 추출하기\nmap &lt;-\n  qmap(\n    location = \"Daejeon\",\n    zoom = 11,\n    maptype = 'roadmap',\n    color = 'bw'\n  )\n\ndaejeon_area &lt;- shapefile('./Spatial_Information_Analysis/LARD_ADM_SECT_SGG_30/LARD_ADM_SECT_SGG_30.shp')\ndaejeon_area # 좌표체계 확인\n# str(daejeon_area)\nplot(daejeon_area, axes = T)\n\n\n위 plot의 좌표단위를 보면 평면직각좌표계(Projected Coordinate)를 기준으로 측정할 때 나올 수 있는 단위\n앞에서 사고 데이터의 좌표는 위경도 좌표이므로, 두 자료의 위치 좌표체계를 통일 시켜줄 필요가 있음\nspTransform() 를 통해 좌표변형 가능\n\nto_crs = CRS(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\")\n\n\nto_crs = CRS(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\")\ndaejeon_area2 &lt;- spTransform(daejeon_area, to_crs)\ndaejeon_area2\ndaejeon_area2@data # SP 데이터 내에서 출력을 하려면 @로 호출해야함\n\nplot(daejeon_area2, axes = T)\nmap + geom_polygon(\n  data = daejeon_area2,\n  aes(x = long, y = lat, group = group),\n  fill = 'white',\n  color = 'black'\n)\n\n구를 기준으로 사고 발생 횟수 계산\n\n\ngu_accident &lt;- daejeon %&gt;% group_by(발생지시군구) %&gt;% summarise(n = n())\ngu_accident\n#&gt; # A tibble: 5 x 2\n#&gt;   발생지시군구     n\n#&gt;   &lt;chr&gt;        &lt;int&gt;\n#&gt; 1 대덕구          81\n#&gt; 2 동구            98\n#&gt; 3 서구           100\n#&gt; 4 유성구          73\n#&gt; 5 중구            58\n\n\ndaejeon_area2 객체의 클래스는 SpatitalPloygonsDataFrame임\n이것을 데이터 프레임 형태로 변환해줄 때 사용하는 함수로는 ggplot2 패키지의 fortify() 함수가 있음\n구를 나타내는 SGG_NM 열로 기준\n\n\nclass(daejeon_area2)\ndaejeon_area2 &lt;- fortify(daejeon_area2, region = 'SGG_NM')\nclass(daejeon_area2)\nhead(daejeon_area2)\n\n\ndaejeon_area2의 “id”열과 gu_accident의 “발생지시군구”열을 기준으로 합치기 위해서 열Name을 “id”로 통일\nid열을 기준으로 두 데이터셋을 합쳐줌\n\n\nnames(gu_accident)[1] &lt;- \"id\"\ndaejeon_area3 &lt;- merge(daejeon_area2, gu_accident, by = 'id')\nhead(daejeon_area3)\n\ndaejeon_area3 %&gt;% group_by(id) %&gt;% summarise(n = mean(n))\n\n\ngeom_polygon()을 이용한 시각화\n\n\np &lt;-\n  map + geom_polygon(data = daejeon_area3,\n                     aes(\n                       x = long,\n                       y = lat,\n                       group = group,\n                       fill = n\n                     ),\n                     alpha = .5)\np\np + scale_fill_gradient(low = 'yellow', high = 'red')\nlibrary(viridis)\np + scale_fill_viridis()"
  },
  {
    "objectID": "Spatial_Information_Analysis.html#sfsimple-features-객체클래스로-문제-풀어보기",
    "href": "Spatial_Information_Analysis.html#sfsimple-features-객체클래스로-문제-풀어보기",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "구경계 데이터(daejeon_area2)를 sf클래스로 변환\nst_as_sf() : sp클래스를 sf클래스로 변환\n\n\ndaejeon_area2 &lt;- spTransform(daejeon_area, to_crs)\ndaejeon_area2\ndaejeon_area_sf &lt;- st_as_sf(daejeon_area2) # sp 클래스를 sf 클래스로 전환하기\ndaejeon_area_sf\nplot(st_geometry(daejeon_area_sf))\n\n\nst_point_on_surface() : 각 구별 지도상 중심점 구한 뒤 지도상에 표출\n\n\n# 각 구별 Center\ndaejeon_area_center &lt;- st_point_on_surface(daejeon_area_sf)\nplot(st_geometry(daejeon_area_sf))\nplot(daejeon_area_center , add = T, col = \"black\")\n\n\n사망사고데이터(point)를 sf클래스로 변환\n\n\ndaejeon_acc_sf &lt;-\n  daejeon %&gt;% st_as_sf(coords = c(\"경도\", \"위도\"),\n                       crs = 4326,\n                       remove = FALSE)\ndaejeon_acc_sf ## CRS : # WGS84\n\n# daejeon_acc &lt;- daejeon %&gt;% st_as_sf(coords = c(\"발생위치X_UTMK\", \"발생위치Y_UTMK\"),\n#                                     crs = 4326,\n#                                     remove = FALSE)\n# daejeon_acc\n\n\nst_intersection을 통해서 폴리곤(구경계)와 포인트(사망사고지점)데이터 합치기\n\n\n## Intersection between polygon and points\nintersection &lt;- st_intersection(daejeon_area_sf, daejeon_acc_sf)\nhead(intersection)\n\n## Plot intersection\nplot(st_geometry(daejeon_area_sf))\nplot(intersection, add = T, pch = 1)\n\n\n구별 사망사고 건수 Count하기\n\n\n## View result\ntable(intersection$SGG_NM)\n\n## Using dplyr\nint_result &lt;- intersection %&gt;%\n  group_by(SGG_NM) %&gt;%\n  count()\nint_result\n\n\nst_join() : 경계 데이터(daejeon_area_sf)에 결과(int_result) 합치기\n\n\nint_result0 &lt;- st_join(daejeon_area_sf, int_result)\nint_result0\n\n\nmap 위에 시각화\n\n\nmap &lt;-\n  qmap(\n    location = \"Daejeon\",\n    zoom = 11,\n    maptype = 'roadmap',\n    color = 'bw'\n  )\np &lt;-\n  map + geom_sf(data = int_result0,\n                inherit.aes = F, # sf형태 data 그릴 때 반드시 필요\n                aes(fill = n),\n                alpha = .5)\np\np + scale_fill_gradient(low = 'yellow', high = 'red')"
  },
  {
    "objectID": "Regression_Analysis.html",
    "href": "Regression_Analysis.html",
    "title": "Regression Analysis",
    "section": "",
    "text": "예제를 통한 회귀분석\n\n\ngetwd() #\"C:/Users/Hyunsoo Kim/Documents/lecture/regression_analysis\"\n\n[1] \"C:/Users/Hyunsoo Kim/Desktop/senior_grade/blog/my-quarto-website\"\n\n\n\n\n\n\n\ndata_2.5&lt;-read.table(\"All_Data/p031.txt\",header=TRUE,sep=\"\\t\")\n\ndim(data_2.5) #14 2\n\n[1] 14  2\n\nhead(data_2.5)\n\n  Minutes Units\n1      23     1\n2      29     2\n3      49     3\n4      64     4\n5      74     4\n6      87     5\n\nX&lt;-data_2.5$Units\n\nY&lt;-data_2.5$Minutes\n\n\n\n\n\ndf&lt;-data.frame(\n\n  #1:length(X),\n\n  Y,\n\n  X,\n\n  Y-mean(Y),\n\n  X-mean(X),\n\n  (Y-mean(Y))^2,\n\n  (X-mean(X))^2,\n\n  (Y-mean(Y))*(X-mean(X))\n\n)\n\ndf\n\n     Y  X Y...mean.Y. X...mean.X. X.Y...mean.Y...2 X.X...mean.X...2\n1   23  1 -74.2142857          -5     5.507760e+03               25\n2   29  2 -68.2142857          -4     4.653189e+03               16\n3   49  3 -48.2142857          -3     2.324617e+03                9\n4   64  4 -33.2142857          -2     1.103189e+03                4\n5   74  4 -23.2142857          -2     5.389031e+02                4\n6   87  5 -10.2142857          -1     1.043316e+02                1\n7   96  6  -1.2142857           0     1.474490e+00                0\n8   97  6  -0.2142857           0     4.591837e-02                0\n9  109  7  11.7857143           1     1.389031e+02                1\n10 119  8  21.7857143           2     4.746173e+02                4\n11 149  9  51.7857143           3     2.681760e+03                9\n12 145  9  47.7857143           3     2.283474e+03                9\n13 154 10  56.7857143           4     3.224617e+03               16\n14 166 10  68.7857143           4     4.731474e+03               16\n   X.Y...mean.Y......X...mean.X..\n1                       371.07143\n2                       272.85714\n3                       144.64286\n4                        66.42857\n5                        46.42857\n6                        10.21429\n7                         0.00000\n8                         0.00000\n9                        11.78571\n10                       43.57143\n11                      155.35714\n12                      143.35714\n13                      227.14286\n14                      275.14286\n\n\n\n\n\n\nCOV_XY&lt;-sum((Y-mean(Y))*(X-mean(X))) / (length(X)-1) #136\n\n### cov() 함수\n\ncov(X,Y) #136\n\n[1] 136\n\n### 상관계수(correalationship)\n\n### cor() 함수\n\ncor(X,Y) #0.9936987 \n\n[1] 0.9936987\n\n\n\n\n\n\n\ndata_2.5&lt;-read.table(\"All_Data/p031.txt\",header=TRUE,sep=\"\\t\")\n\nx&lt;-data_2.5$Units\n\ny&lt;-data_2.5$Minutes\n\n\n\n\ncor_xy&lt;- COV_XY / (sd(x)*sd(y))\n\ncor_xy\n\n[1] 0.9936987\n\n### cor() 함수\n\ncor(x,y)\n\n[1] 0.9936987\n\ncor(y,x)\n\n[1] 0.9936987\n\ndata_2.5\n\n   Minutes Units\n1       23     1\n2       29     2\n3       49     3\n4       64     4\n5       74     4\n6       87     5\n7       96     6\n8       97     6\n9      109     7\n10     119     8\n11     149     9\n12     145     9\n13     154    10\n14     166    10\n\ncor(data_2.5)\n\n          Minutes     Units\nMinutes 1.0000000 0.9936987\nUnits   0.9936987 1.0000000\n\n\n\n\n\n\nclass(X)\n\n[1] \"numeric\"\n\nclass(Y) #both numeric\n\n[1] \"numeric\"\n\nplot(X,Y, pch=19,xlab=\"Units\",ylab=\"Minutes\") \n\n\n\n\n\n\n\n\ndata_2.3&lt;-read.table(\"All_Data/p029a.txt\",header=TRUE,sep=\"\\t\")\n\ndata_2.3\n\n    Y  X\n1   1 -7\n2  14 -6\n3  25 -5\n4  34 -4\n5  41 -3\n6  46 -2\n7  49 -1\n8  50  0\n9  49  1\n10 46  2\n11 41  3\n12 34  4\n13 25  5\n14 14  6\n15  1  7\n\nX&lt;-data_2.3$X\n\nY&lt;-data_2.3$Y\n\n\n\n\n\nplot(X,Y)\n\n\n\ncor(X,Y) # 0 완벽하게 2차함수의 형태도 0이 나옴(직선의 형태가 아닌것만)\n\n[1] 0\n\n\n\n\n\n\ndata_2.4&lt;-read.table(\"All_Data/p029b.txt\",header=TRUE,sep=\"\\t\")\n\n\n\n\n\nplot(data_2.4$X1,data_2.4$Y1, pch=19); abline(3,0.5) #기울기 3 절편0.5인 선을 추가해라 \n\n\n\nplot(data_2.4$X2,data_2.4$Y2, pch=19); abline(3,0.5)\n\n\n\nplot(data_2.4$X3,data_2.4$Y3, pch=19); abline(3,0.5)\n\n\n\nplot(data_2.4$X4,data_2.4$Y4, pch=19); abline(3,0.5)\n\n\n\nm&lt;-matrix(1:4,ncol=2,byrow=TRUE) #2행의 매트릭스 생성 \n\nm\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nlayout(m)\n\nplot(Y1~X1, data=data_2.4,pch=19); abline(3,0.5)\n\n#y~x  -&gt; y=ax+b 이러한 형태를 가지는 모형식이라는 의미 \n\nplot(Y2~X2, data=data_2.4,pch=19); abline(3,0.5)\n\nplot(Y3~X3, data=data_2.4,pch=19); abline(3,0.5)\n\nplot(Y4~X4, data=data_2.4,pch=19); abline(3,0.5)\n\n\n\nlayout(1) #변환을 다시 하지 않으면 설정한 매트릭스의 비율로 그래프가 그려짐 해제 필요 \n\n# cor()\n\ncor(data_2.4$X1,data_2.4$Y1) #0.8164205\n\n[1] 0.8164205\n\ncor(data_2.4$X2,data_2.4$Y2) #0.8162365\n\n[1] 0.8162365\n\ncor(data_2.4$X3,data_2.4$Y3) #0.8162867\n\n[1] 0.8162867\n\ncor(data_2.4$X4,data_2.4$Y4) #0.8165214\n\n[1] 0.8165214\n\ncor(data_2.4) #이렇게 한번에 할 수 있으나 가독성 떨어짐 \n\n           Y1         X1         Y2         X2         Y3         X3         Y4\nY1  1.0000000  0.8164205  0.7500054  0.8164205  0.4687167  0.8164205 -0.4891162\nX1  0.8164205  1.0000000  0.8162365  1.0000000  0.8162867  1.0000000 -0.3140467\nY2  0.7500054  0.8162365  1.0000000  0.8162365  0.5879193  0.8162365 -0.4780949\nX2  0.8164205  1.0000000  0.8162365  1.0000000  0.8162867  1.0000000 -0.3140467\nY3  0.4687167  0.8162867  0.5879193  0.8162867  1.0000000  0.8162867 -0.1554718\nX3  0.8164205  1.0000000  0.8162365  1.0000000  0.8162867  1.0000000 -0.3140467\nY4 -0.4891162 -0.3140467 -0.4780949 -0.3140467 -0.1554718 -0.3140467  1.0000000\nX4 -0.5290927 -0.5000000 -0.7184365 -0.5000000 -0.3446610 -0.5000000  0.8165214\n           X4\nY1 -0.5290927\nX1 -0.5000000\nY2 -0.7184365\nX2 -0.5000000\nY3 -0.3446610\nX3 -0.5000000\nY4  0.8165214\nX4  1.0000000\n\n\n\n\n\n\n\n\n\ndata_2.5&lt;-read.table(\"All_Data/p031.txt\",header=TRUE,sep=\"\\t\")\n\nx&lt;-data_2.5$Units\n\ny&lt;-data_2.5$Minutes\n\n\n\n\n\nsum((y-mean(y))*(x-mean(x))) #1768\n\n[1] 1768\n\nsum((x-mean(x))^2) #114\n\n[1] 114\n\nbeta1_hat&lt;-sum((y-mean(y))*(x-mean(x))) / sum((x-mean(x))^2)\n\nbeta1_hat #15.50877\n\n[1] 15.50877\n\nbeta0_hat &lt;- mean(y) - (beta1_hat*mean(x))\n\nbeta0_hat #4.161654\n\n[1] 4.161654\n\n### 최소제곱회귀 방정식\n\n# Minutes = 4.161654 + 15.50877 * Units\n\nplot(beta0_hat+beta1_hat*x,pch=19);\n\n\n\n# 4개의 고장 난 부품을 수리하는데 걸리는 에측시간\n\n4.161654 + 15.50877 * 4 #66.19673\n\n[1] 66.19673\n\nunits&lt;-4\n\nbeta0_hat + beta1_hat*units\n\n[1] 66.19674\n\n### 적합값(Fitted value)\n\ny_hat&lt;-beta0_hat + beta1_hat*(x)\n\n### 최소 제곱 잔차(residual)\n\ne&lt;-y-y_hat\n\ne #합이 0이라는 특징이 존재\n\n [1]  3.3295739 -6.1791980 -1.6879699 -2.1967419  7.8032581  5.2944862\n [7] -1.2142857 -0.2142857 -3.7230576 -9.2318296  5.2593985  1.2593985\n[13] -5.2493734  6.7506266\n\nsum(e) #1.278977e-13 0에 근사한 추지가 나옴\n\n[1] 1.278977e-13\n\n\n\n\n\n\ndf_2.7&lt;-data.frame(\n\n  x=x,\n\n  y=y,\n\n  y_hat,\n\n  e\n\n)\n\ndf_2.7\n\n    x   y     y_hat          e\n1   1  23  19.67043  3.3295739\n2   2  29  35.17920 -6.1791980\n3   3  49  50.68797 -1.6879699\n4   4  64  66.19674 -2.1967419\n5   4  74  66.19674  7.8032581\n6   5  87  81.70551  5.2944862\n7   6  96  97.21429 -1.2142857\n8   6  97  97.21429 -0.2142857\n9   7 109 112.72306 -3.7230576\n10  8 119 128.23183 -9.2318296\n11  9 149 143.74060  5.2593985\n12  9 145 143.74060  1.2593985\n13 10 154 159.24937 -5.2493734\n14 10 166 159.24937  6.7506266\n\n### lm() 함수 (linear model)\n\n# Minutes = beta0 + beta1 * Units + epsilon\n\n# 모형식 : y~x\n\nlm(y~x)\n\n\nCall:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)            x  \n      4.162       15.509  \n\nres_lm&lt;-lm(Minutes~Units,data=data_2.5)\n\nres_lm\n\n\nCall:\nlm(formula = Minutes ~ Units, data = data_2.5)\n\nCoefficients:\n(Intercept)        Units  \n      4.162       15.509  \n\n# 리스트의 이름 \n\nnames(res_lm)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n# 회귀계수\n\nres_lm$coefficients\n\n(Intercept)       Units \n   4.161654   15.508772 \n\ncoef(res_lm)\n\n(Intercept)       Units \n   4.161654   15.508772 \n\n# 적합값\n\nres_lm$fitted.values\n\n        1         2         3         4         5         6         7         8 \n 19.67043  35.17920  50.68797  66.19674  66.19674  81.70551  97.21429  97.21429 \n        9        10        11        12        13        14 \n112.72306 128.23183 143.74060 143.74060 159.24937 159.24937 \n\nfitted(res_lm)\n\n        1         2         3         4         5         6         7         8 \n 19.67043  35.17920  50.68797  66.19674  66.19674  81.70551  97.21429  97.21429 \n        9        10        11        12        13        14 \n112.72306 128.23183 143.74060 143.74060 159.24937 159.24937 \n\n# 최소제곱잔차\n\nres_lm$residuals\n\n         1          2          3          4          5          6          7 \n 3.3295739 -6.1791980 -1.6879699 -2.1967419  7.8032581  5.2944862 -1.2142857 \n         8          9         10         11         12         13         14 \n-0.2142857 -3.7230576 -9.2318296  5.2593985  1.2593985 -5.2493734  6.7506266 \n\nresid(res_lm)\n\n         1          2          3          4          5          6          7 \n 3.3295739 -6.1791980 -1.6879699 -2.1967419  7.8032581  5.2944862 -1.2142857 \n         8          9         10         11         12         13         14 \n-0.2142857 -3.7230576 -9.2318296  5.2593985  1.2593985 -5.2493734  6.7506266 \n\nresiduals(res_lm)\n\n         1          2          3          4          5          6          7 \n 3.3295739 -6.1791980 -1.6879699 -2.1967419  7.8032581  5.2944862 -1.2142857 \n         8          9         10         11         12         13         14 \n-0.2142857 -3.7230576 -9.2318296  5.2593985  1.2593985 -5.2493734  6.7506266 \n\n\n\n\n\n\nplot(beta0_hat+beta1_hat*x,pch=19);\n\n#abline(beta0_hat,beta1_hat)\n\nabline(res_lm)\n\n\n\n\n\n\n\n\n\ndata_2.5&lt;-read.table(\"All_Data/p031.txt\",header=TRUE,sep=\"\\t\")\n\nres_lm &lt;- lm(Minutes ~ Units, data = data_2.5)\n\nres_lm\n\n\nCall:\nlm(formula = Minutes ~ Units, data = data_2.5)\n\nCoefficients:\n(Intercept)        Units  \n      4.162       15.509  \n\nres_lm_summ&lt;-summary(res_lm)\n\nres_lm_summ #Pr(&gt;|t|) - p-value\n\n\nCall:\nlm(formula = Minutes ~ Units, data = data_2.5)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.2318 -3.3415 -0.7143  4.7769  7.8033 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    4.162      3.355    1.24    0.239    \nUnits         15.509      0.505   30.71 8.92e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.392 on 12 degrees of freedom\nMultiple R-squared:  0.9874,    Adjusted R-squared:  0.9864 \nF-statistic: 943.2 on 1 and 12 DF,  p-value: 8.916e-13\n\n# unit은 시간에 영향을준다 약15.5분 만큼씩 \n\n# coefficient에서 p-value에 대해서 알 수 있음 \n\n# beta_0는 0이라고 보면되느냐? p-value가 0.05보다 크기에 \n\n\n\n\n\n\n\nconfint(res_lm) # beta_0,1의 95% 신뢰구간을 뽑아줌 \n\n                2.5 %   97.5 %\n(Intercept) -3.148482 11.47179\nUnits       14.408512 16.60903\n\n?confint #level = 1-alpha\n\nstarting httpd help server ... done\n\nconfint(res_lm, level=0.90) # 90%의 신뢰구간\n\n                 5 %     95 %\n(Intercept) -1.81810 10.14141\nUnits       14.60875 16.40879\n\n\n\n\n\n\n# 4개의 고장난 부품을 수리하는 데에 걸리는 시간 예측\n\nx&lt;-4\n\n4.161654 + 15.508772 *4\n\n[1] 66.19674\n\nres_lm$coefficients[1]+(res_lm$coefficients[2]*x)\n\n(Intercept) \n   66.19674 \n\n### predict()\n\ndf&lt;-data.frame(Units=4) \n\npredict(res_lm,newdata=df) # res_lm을 만들때 사용한 데이터형식으로 만들어주어야함\n\n       1 \n66.19674 \n\nres_lm_pred&lt;-predict(res_lm,newdata=df,se.fit=TRUE)\n\n### 예측값\n\nres_lm_pred$fit\n\n       1 \n66.19674 \n\n### 표준오차\n\nres_lm_pred$se.fit # 평균반응에 대한 표준오차 \n\n[1] 1.759688\n\n### 예측한계\n\ndf&lt;-data.frame(Units=4) #예제서는 4대기준\n\ndf&lt;-data.frame(Units=1:10) #다른것도 보고 싶은경우 \n\nres_lm_pred_int_p&lt;-predict(res_lm,newdata=df,interval=\"prediction\")\n\n### 신뢰한계\n\ndf&lt;-data.frame(Units=1:10) #다른것도 보고 싶은경우 \n\nres_lm_pred_int_c&lt;-predict(res_lm,newdata=df,interval=\"confidence\") #둘의 차이를 보면 예측한계의 범위가 더큼 \n\n### 예측한계 & 신뢰한계\n\n# 신뢰한계는 평균에서 멀어지만 오차의범위가 커지고 평균에 다가갈수록 오차가 줄어듬\n\nplot(Minutes~Units,data=data_2.5,pch=19)\n\nabline(res_lm,col=\"red\",lwd=2)\n\nlines(1:10,res_lm_pred_int_p[,\"lwr\"],col=\"darkgreen\")\n\nlines(1:10,res_lm_pred_int_p[,\"upr\"],col=\"darkgreen\")\n\nlines(1:10,res_lm_pred_int_c[,\"lwr\"],col=\"blue\")\n\nlines(1:10,res_lm_pred_int_c[,\"upr\"],col=\"blue\")\n\n\n\n\n\n\n\n\nres_lm_summ&lt;-summary(res_lm)\n\nres_lm_summ #Multiple R-squared:0.9874 -&gt; 반응변수의 전체변이중 98.94%가 예측변수에 의해 설명된다\n\n\nCall:\nlm(formula = Minutes ~ Units, data = data_2.5)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.2318 -3.3415 -0.7143  4.7769  7.8033 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    4.162      3.355    1.24    0.239    \nUnits         15.509      0.505   30.71 8.92e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.392 on 12 degrees of freedom\nMultiple R-squared:  0.9874,    Adjusted R-squared:  0.9864 \nF-statistic: 943.2 on 1 and 12 DF,  p-value: 8.916e-13\n\n# 만약 R-squared가 1이면 완벽한 선형의 관계 100%라는 것을 의미한다.\n\n# R-squared는 변수가 들어갈수록 커지기에 adjust R-squared를 사용 추후 설명 \n\n\n\n\n\n# Minutes = beta1 + Units + epsilon\n\nres_lm_no&lt;-lm(Minutes~Units-1,data=data_2.5)\n\nres_lm_no\n\n\nCall:\nlm(formula = Minutes ~ Units - 1, data = data_2.5)\n\nCoefficients:\nUnits  \n16.07  \n\nsummary(res_lm_no)\n\n\nCall:\nlm(formula = Minutes ~ Units - 1, data = data_2.5)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5955 -2.4733  0.4417  5.0243  9.7023 \n\nCoefficients:\n      Estimate Std. Error t value Pr(&gt;|t|)    \nUnits  16.0744     0.2213   72.62   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.502 on 13 degrees of freedom\nMultiple R-squared:  0.9975,    Adjusted R-squared:  0.9974 \nF-statistic:  5274 on 1 and 13 DF,  p-value: &lt; 2.2e-16\n\ncoef(summary(res_lm_no)) #rsquared=0.9975\n\n      Estimate Std. Error  t value     Pr(&gt;|t|)\nUnits 16.07443  0.2213341 72.62519 2.380325e-18\n\n\n\n\n\n\ny&lt;-rnorm(30)\n\nt.test(y,mu=0)\n\n\n    One Sample t-test\n\ndata:  y\nt = 0.8445, df = 29, p-value = 0.4053\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.2121536  0.5105780\nsample estimates:\nmean of x \n0.1492122 \n\nsummary(lm(y~1))\n\n\nCall:\nlm(formula = y ~ 1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6420 -0.8088  0.2048  0.7985  1.9749 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   0.1492     0.1767   0.844    0.405\n\nResidual standard error: 0.9678 on 29 degrees of freedom\n\n\n\n\n\n\n\n\n\ndata_3.3&lt;-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\ndim(data_3.3)\n\n[1] 30  7\n\nclass(data_3.3)\n\n[1] \"data.frame\"\n\nsapply(data_3.3,class) #all numeric\n\n        Y        X1        X2        X3        X4        X5        X6 \n\"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \n\nsummary(data_3.3) #모든변수가 numeric이면 분위수도 보여준다 \n\n       Y               X1             X2              X3              X4       \n Min.   :40.00   Min.   :37.0   Min.   :30.00   Min.   :34.00   Min.   :43.00  \n 1st Qu.:58.75   1st Qu.:58.5   1st Qu.:45.00   1st Qu.:47.00   1st Qu.:58.25  \n Median :65.50   Median :65.0   Median :51.50   Median :56.50   Median :63.50  \n Mean   :64.63   Mean   :66.6   Mean   :53.13   Mean   :56.37   Mean   :64.63  \n 3rd Qu.:71.75   3rd Qu.:77.0   3rd Qu.:62.50   3rd Qu.:66.75   3rd Qu.:71.00  \n Max.   :85.00   Max.   :90.0   Max.   :83.00   Max.   :75.00   Max.   :88.00  \n       X5              X6       \n Min.   :49.00   Min.   :25.00  \n 1st Qu.:69.25   1st Qu.:35.00  \n Median :77.50   Median :41.00  \n Mean   :74.77   Mean   :42.93  \n 3rd Qu.:80.00   3rd Qu.:47.75  \n Max.   :92.00   Max.   :72.00  \n\n### 산점도 행렬\n\nplot(data_3.3)\n\n\n\n\n\n\n\n\nlm(Y~X1+X2+X3+X4+X5+X6,data=data_3.3)\n\n\nCall:\nlm(formula = Y ~ X1 + X2 + X3 + X4 + X5 + X6, data = data_3.3)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n   10.78708      0.61319     -0.07305      0.32033      0.08173      0.03838  \n         X6  \n   -0.21706  \n\nlm(Y~.,data=data_3.3) # X1+X2+X3+X4+X5+X6쓰는 것이 아니라 .을 써서 모든 변수를 다써줌 \n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n   10.78708      0.61319     -0.07305      0.32033      0.08173      0.03838  \n         X6  \n   -0.21706  \n\n\n\n\n\n\ndata_3.3&lt;-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\nlm(Y~X1+X2,data=data_3.3)\n\n\nCall:\nlm(formula = Y ~ X1 + X2, data = data_3.3)\n\nCoefficients:\n(Intercept)           X1           X2  \n   15.32762      0.78034     -0.05016  \n\n# (Intercept)         X1           X2  \n\n#  15.32762      0.78034     -0.05016\n\n# 1) Y에서 X1 효과 제거\n\nm1&lt;-lm(Y~X1,data=data_3.3) # y prime\n\nm1$residuals # x1이 설명하지 못한값 / x1의 효과를 제거한 값\n\n           1            2            3            4            5            6 \n -9.86142016   0.32865220   3.80099328  -0.91673799   7.76411473 -12.87985944 \n           7            8            9           10           11           12 \n -6.93517726   0.02794419  -4.25432454   6.59248165   9.62936020   7.34709147 \n          13           14           15           16           17           18 \n  7.83787183  -9.00893436   4.51872455  -1.29120309  -4.51815400   5.34709147 \n          19           20           21           22           23           24 \n -2.19900672  -8.14368889   5.43928784   3.59248165 -11.18056744  -2.29688270 \n          25           26           27           28           29           30 \n  7.87475038  -6.48127545   7.02794419  -9.38907907   6.48184600   5.74567546 \n\n# 2) X2에서 X1 효과 제거\n\nm2&lt;-lm(X2~X1,data=data_3.3) # x2 prime \n\nm2$residuals \n\n          1           2           3           4           5           6 \n-15.1300345  -0.7994502  13.1223579  -6.2864182  -2.9818979   1.8178376 \n          7           8           9          10          11          12 \n-11.3385461  -7.4428019  10.9659742  -5.2603543   6.8439016  -2.7473223 \n         13          14          15          16          17          18 \n  6.2266138  21.4529422  -4.4688659 -15.1382816   1.4268783  15.2526777 \n         19          20          21          22          23          24 \n -8.8776421  19.2787417  -6.4866827   1.7396457  -0.8255141   4.0524132 \n         25          26          27          28          29          30 \n -4.6691304   7.5311341   0.5571981  -4.2082264   8.4268783 -22.0340258 \n\n# 3) X1의 효과가 제거된 Y와 X2의 적합 - 원점을 지나는 회귀선\n\nlm(m1$residuals~m2$residuals-1) # 원점을 지나면 -1를 하고 진행 // -3.25e-17\n\n\nCall:\nlm(formula = m1$residuals ~ m2$residuals - 1)\n\nCoefficients:\nm2$residuals  \n    -0.05016  \n\n# 다른 효과 없이(다른값이 고정) Y에 영향을 주는 순수한 X2의 값\n\n# m2$residuals  : -0.05016  ==  X2 : -0.05016  \n\n### 단위길이 척도화 - 잘사용하지않음\n\nfn_scaling_len&lt;-function(x){\n\n  x0&lt;-x-mean(x)\n\n  x0/sqrt(sum(x0^2))\n\n}\n\ndata_3.3_len&lt;-sapply(data_3.3, fn_scaling_len)\n\ndata_3.3_len&lt;-data.frame(data_3.3_len)\n\nsummary(data_3.3_len)\n\n       Y                  X1                 X2                 X3           \n Min.   :-0.37579   Min.   :-0.41282   Min.   :-0.35109   Min.   :-0.353871  \n 1st Qu.:-0.08975   1st Qu.:-0.11297   1st Qu.:-0.12344   1st Qu.:-0.148193  \n Median : 0.01322   Median :-0.02231   Median :-0.02479   Median : 0.002109  \n Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.000000  \n 3rd Qu.: 0.10857   3rd Qu.: 0.14504   3rd Qu.: 0.14216   3rd Qu.: 0.164278  \n Max.   : 0.31070   Max.   : 0.32635   Max.   : 0.45328   Max.   : 0.294804  \n       X4                 X5                 X6          \n Min.   :-0.38637   Min.   :-0.48356   Min.   :-0.32367  \n 1st Qu.:-0.11401   1st Qu.:-0.10353   1st Qu.:-0.14318  \n Median :-0.02024   Median : 0.05130   Median :-0.03489  \n Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.00000  \n 3rd Qu.: 0.11371   3rd Qu.: 0.09821   3rd Qu.: 0.08693  \n Max.   : 0.41733   Max.   : 0.32341   Max.   : 0.52461  \n\nlm(Y~.,data=data_3.3_len)\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3_len)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n -1.259e-16    6.707e-01   -7.343e-02    3.089e-01    6.981e-02    3.120e-02  \n         X6  \n -1.835e-01  \n\n### 표준화\n\n# scale()\n\ndata_3.3_std&lt;-scale(data_3.3)\n\n#summary(data_3.3_std)\n\n#sapply(data_3.3_std, sd, na.rm=T)\n\n#class(data_3.3_std) #\"matrix\"\n\ndata_3.3_std&lt;-data.frame(data_3.3_std)\n\n#class(data_3.3_std) #\"data.frame\"\n\n#sapply(data_3.3_std, sd, na.rm=T)\n\nlm(Y~.,data=data_3.3_std) # beta게수 구하기 \n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3_std)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n -7.717e-16    6.707e-01   -7.343e-02    3.089e-01    6.981e-02    3.120e-02  \n         X6  \n -1.835e-01  \n\n\n\n\n\n\nres_lm&lt;-lm(Y~.,data=data_3.3)\n\nres_lm\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n   10.78708      0.61319     -0.07305      0.32033      0.08173      0.03838  \n         X6  \n   -0.21706  \n\nsummary(res_lm)\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.9418  -4.3555   0.3158   5.5425  11.5990 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10.78708   11.58926   0.931 0.361634    \nX1           0.61319    0.16098   3.809 0.000903 ***\nX2          -0.07305    0.13572  -0.538 0.595594    \nX3           0.32033    0.16852   1.901 0.069925 .  \nX4           0.08173    0.22148   0.369 0.715480    \nX5           0.03838    0.14700   0.261 0.796334    \nX6          -0.21706    0.17821  -1.218 0.235577    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.068 on 23 degrees of freedom\nMultiple R-squared:  0.7326,    Adjusted R-squared:  0.6628 \nF-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05\n\nm1&lt;-summary(lm(Y~X1+X2+X3+X4+X5+X6,data=data_3.3)) #Adjusted R-squared:  0.6628 \n\nm2&lt;-summary(lm(Y~X1+X2+X3+X4+X5,data=data_3.3)) #Adjusted R-squared:  0.6561 \n\n# X6가 들어가는 것이 더 좋은 모델 \n\nm1$adj.r.squared\n\n[1] 0.662846\n\nm2$adj.r.squared #summary에서 보다 더 정확하게 수치가 나옴 \n\n[1] 0.6560539\n\n\n\n\n\n\nres_lm_summ&lt;-summary(res_lm)\n\nres_lm_summ #p-value의 존재는 무언가를 검정했다라는 반증\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.9418  -4.3555   0.3158   5.5425  11.5990 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10.78708   11.58926   0.931 0.361634    \nX1           0.61319    0.16098   3.809 0.000903 ***\nX2          -0.07305    0.13572  -0.538 0.595594    \nX3           0.32033    0.16852   1.901 0.069925 .  \nX4           0.08173    0.22148   0.369 0.715480    \nX5           0.03838    0.14700   0.261 0.796334    \nX6          -0.21706    0.17821  -1.218 0.235577    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.068 on 23 degrees of freedom\nMultiple R-squared:  0.7326,    Adjusted R-squared:  0.6628 \nF-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05\n\n# p-value&lt;0.05 H_1 귀무가설 채택 \n\n# p-value&gt;0.05 H_0 영가설 채택 // X1을 제외하고는 영가설 유의한 의미가 없음(Y에영향주는)\n\n# 모두다 0이라는 가설을 가지고 분모 분자의 오차가 카이제곱을 따르고 거기서 나온 통계량\n\n# F-분포 자유도는 분자 분모 두개를 가짐 //모아서 계산을 하기에 각각 계산하는것과 결과다름 \n\n# 영가설-모든 회귀계수가 0이다.\n\n# 대립가설-적어도 하나는 0이 아니다. p-value: 1.24e-05 &lt;0.05 대립가설 채택 \n\n# p-value가 0.05보다 작으면 대립가설 채택!!!!!! 기억해 \n\n# 회귀계수에 대한 신뢰구간 - 95% 신뢰한계\n\nconfint(res_lm) #-13.18712881 ~ 34.7612816\n\n                   2.5 %     97.5 %\n(Intercept) -13.18712881 34.7612816\nX1            0.28016866  0.9462066\nX2           -0.35381806  0.2077178\nX3           -0.02827872  0.6689430\nX4           -0.37642935  0.5398936\nX5           -0.26570179  0.3424647\nX6           -0.58571106  0.1515977\n\n#X1  0.28016866  0.9462066  사이에 0이 들어가있으면 영향을 준다라느걸 의미\n\n#X2 -0.35381806  0.2077178  p-value없이도 알 수 있음 \n\n#X5가 가장 영향이 적음 p-value가 가장 크기에(영향 효과의 크기를 비교할때)\n\n#p-value가 작을 수록 영향을 많이 준다 beta값을 보는 것이 아닌 p-value를 보는 것 중요\n\n#가장 의미있는 변수?-&gt;p-value가장 작은거 // 대립가설채택 Y에 영향을 가장\n\n\n\n\n\n\n\n\n# H_0: beta_1:beta_6=0\n\nmodel_reduce&lt;-lm(Y~1,data=data_3.3)\n\nmodel_full&lt;-lm(Y~.,data=data_3.3)\n\nanova(model_reduce,model_full)\n\nAnalysis of Variance Table\n\nModel 1: Y ~ 1\nModel 2: Y ~ X1 + X2 + X3 + X4 + X5 + X6\n  Res.Df  RSS Df Sum of Sq      F   Pr(&gt;F)    \n1     29 4297                                 \n2     23 1149  6      3148 10.502 1.24e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#대립가설 = 완전모형이 적절하다 / 1.24e-05 *** &lt; 0.05 \n\n#의미 있는 예측 변수가 한개 이상 존재한다 \n\nsummary(model_full) #summary에서 beta_1~beta_6까지 모두가 0이라는 가설로 진행을 이미함\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.9418  -4.3555   0.3158   5.5425  11.5990 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10.78708   11.58926   0.931 0.361634    \nX1           0.61319    0.16098   3.809 0.000903 ***\nX2          -0.07305    0.13572  -0.538 0.595594    \nX3           0.32033    0.16852   1.901 0.069925 .  \nX4           0.08173    0.22148   0.369 0.715480    \nX5           0.03838    0.14700   0.261 0.796334    \nX6          -0.21706    0.17821  -1.218 0.235577    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.068 on 23 degrees of freedom\nMultiple R-squared:  0.7326,    Adjusted R-squared:  0.6628 \nF-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05\n\n#F-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05\n\n#예상// 가장의미있는변수? -&gt; X1 이유?-&gt; p-value 0.000903 로 가장 작기에 영향많이 줄것으로 예측 \n\n\n\n\n\nmodel_reduce&lt;-lm(Y~X1+X3,data=data_3.3)\n\nmodel_full&lt;-lm(Y~.,data=data_3.3)\n\nanova(model_reduce,model_full) #0.7158 &gt; 0.05\n\nAnalysis of Variance Table\n\nModel 1: Y ~ X1 + X3\nModel 2: Y ~ X1 + X2 + X3 + X4 + X5 + X6\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     27 1254.7                           \n2     23 1149.0  4    105.65 0.5287 0.7158\n\n#영가설은 H_0: b_2=b_4=b_5=b_6=0 이라는 사실을 알 수 있다 \n\n#b_1&b_3는 반응변수에 유의한 반응을 준다라는 것도 연계하여 알 수 있다 \n\n\n\n\n\n#해당 조건이 주어지고 만족할 때 beta_1=beta_3은 맞는가?\n\nmodel_reduce&lt;-lm(Y~I(X1-X3),data=data_3.3) #I를 씌우면 새로운 변수를 만든것과 동일\n\n# X1-X3를 한 그자체를 분석하라는 의미//본래는 X1-X3 해서 새로운 변수를 만들어서 해야함 \n\nmodel_full&lt;-lm(Y~X1+X3,data=data_3.3)\n\nanova(model_reduce,model_full) \n\nAnalysis of Variance Table\n\nModel 1: Y ~ I(X1 - X3)\nModel 2: Y ~ X1 + X3\n  Res.Df    RSS Df Sum of Sq     F    Pr(&gt;F)    \n1     28 3846.7                                 \n2     27 1254.6  1      2592 55.78 4.925e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#install.packages(\"car\")\n\nlibrary(car)\n\nLoading required package: carData\n\nmodel_full&lt;-lm(Y~X1+X3,data=data_3.3)\n\ncar::linearHypothesis(model_full,c(\"X1=X3\"))\n\nLinear hypothesis test\n\nHypothesis:\nX1 - X3 = 0\n\nModel 1: restricted model\nModel 2: Y ~ X1 + X3\n\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     28 1424.6                              \n2     27 1254.7  1    169.95 3.6572 0.06649 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n# H_0: beta_1+beta_3=1 | beta_2=beta_3:beta_6=0\n\nmodel_full&lt;-lm(Y~X1+X3,data=data_3.3)\n\ncar::linearHypothesis(model_full,c(\"X1 + X3 = 1\"))\n\nLinear hypothesis test\n\nHypothesis:\nX1  + X3 = 1\n\nModel 1: restricted model\nModel 2: Y ~ X1 + X3\n\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     28 1329.5                           \n2     27 1254.7  1    74.898 1.6118 0.2151\n\n# x1의 효과가 증가하면 x3의 효과는 감소한다 상대적인 관계 (반대로도 가능)\n\n\n\n\n\nmodel_full&lt;-lm(Y~.,data_3.3)\n\n# 예측값 - 적합값\n\nmodel_full$fitted.values\n\n       1        2        3        4        5        6        7        8 \n51.11030 61.35277 69.93944 61.22684 74.45380 53.94185 67.14841 70.09701 \n       9       10       11       12       13       14       15       16 \n79.53099 59.19846 57.92572 55.40103 59.58168 70.21401 76.54933 84.54785 \n      17       18       19       20       21       22       23       24 \n76.15013 61.39736 68.01656 55.62014 42.60324 63.81902 63.66400 44.62475 \n      25       26       27       28       29       30 \n57.31710 67.84347 75.14036 56.04535 77.66053 76.87850 \n\n# 예측한계(Prediction Limits)\n\npredict(model_full,newdata = data_3.3,interval = \"prediction\")\n\n        fit      lwr       upr\n1  51.11030 34.16999  68.05060\n2  61.35277 46.34536  76.36018\n3  69.93944 53.94267  85.93622\n4  61.22684 45.44586  77.00783\n5  74.45380 59.17630  89.73129\n6  53.94185 37.21813  70.66557\n7  67.14841 51.64493  82.65189\n8  70.09701 54.54384  85.65017\n9  79.53099 62.71383  96.34814\n10 59.19846 44.03506  74.36185\n11 57.92572 42.00674  73.84470\n12 55.40103 39.79333  71.00873\n13 59.58168 43.39853  75.76483\n14 70.21401 52.06636  88.36167\n15 76.54933 60.79444  92.30422\n16 84.54785 66.41374 102.68197\n17 76.15013 59.99991  92.30036\n18 61.39736 43.23384  79.56088\n19 68.01656 52.44673  83.58639\n20 55.62014 39.63744  71.60284\n21 42.60324 26.35046  58.85603\n22 63.81902 48.40145  79.23659\n23 63.66400 48.56222  78.76578\n24 44.62475 27.25435  61.99514\n25 57.31710 41.29380  73.34041\n26 67.84347 49.98605  85.70089\n27 75.14036 59.31975  90.96097\n28 56.04535 40.18723  71.90348\n29 77.66053 61.97564  93.34541\n30 76.87850 60.27441  93.48258\n\n# 신뢰한계(Confidence limits)\n\npredict(model_full,newdata = data_3.3,interval = \"confidence\")\n\n        fit      lwr      upr\n1  51.11030 42.55502 59.66557\n2  61.35277 57.97029 64.73524\n3  69.93944 63.44979 76.42909\n4  61.22684 55.28897 67.16471\n5  74.45380 70.02428 78.88332\n6  53.94185 45.82386 62.05984\n7  67.14841 61.99316 72.30367\n8  70.09701 64.79421 75.39980\n9  79.53099 71.22222 87.83975\n10 59.19846 55.18008 63.21683\n11 57.92572 51.63028 64.22116\n12 55.40103 49.94035 60.86171\n13 59.58168 52.64531 66.51805\n14 70.21401 59.46431 80.96372\n15 76.54933 70.68118 82.41748\n16 84.54785 73.82102 95.27468\n17 76.15013 69.29093 83.00933\n18 61.39736 50.62090 72.17383\n19 68.01656 62.66507 73.36805\n20 55.62014 49.16527 62.07502\n21 42.60324 35.50593 49.70055\n22 63.81902 58.92819 68.70985\n23 63.66400 59.88479 67.44321\n24 44.62475 35.24662 54.00288\n25 57.31710 50.76233 63.87187\n26 67.84347 57.59134 78.09561\n27 75.14036 69.09798 81.18275\n28 56.04535 49.90540 62.18531\n29 77.66053 71.98300 83.33806\n30 76.87850 69.00992 84.74707\n\n\n\n\n\n\n\ndata_3.3&lt;-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\nY&lt;-data_3.3$Y\n\nX&lt;-data_3.3[,-1]\n\nX&lt;-cbind(1,X)\n\nX&lt;-as.matrix(X)\n\n#beta_hat&lt;-solve(t(X) %*% X) %*% t(X) %*% Y # %*%행렬 계산 \n\nP&lt;-solve(t(X) %*% X) %*% t(X)\n\nbeta_hat&lt;- P %*% Y\n\nlm(Y~.,data_3.3)\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n   10.78708      0.61319     -0.07305      0.32033      0.08173      0.03838  \n         X6  \n   -0.21706  \n\n\n\n\n\n\n\n\n# 표준화 잔차\n\ndata_3.3&lt;-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\nres_lm&lt;-lm(Y~.,data=data_3.3)\n\nclass(res_lm)\n\n[1] \"lm\"\n\nmode(res_lm)\n\n[1] \"list\"\n\nnames(res_lm)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\nres_lm$fitted.values\n\n       1        2        3        4        5        6        7        8 \n51.11030 61.35277 69.93944 61.22684 74.45380 53.94185 67.14841 70.09701 \n       9       10       11       12       13       14       15       16 \n79.53099 59.19846 57.92572 55.40103 59.58168 70.21401 76.54933 84.54785 \n      17       18       19       20       21       22       23       24 \n76.15013 61.39736 68.01656 55.62014 42.60324 63.81902 63.66400 44.62475 \n      25       26       27       28       29       30 \n57.31710 67.84347 75.14036 56.04535 77.66053 76.87850 \n\nstr(res_lm)\n\nList of 12\n $ coefficients : Named num [1:7] 10.7871 0.6132 -0.0731 0.3203 0.0817 ...\n  ..- attr(*, \"names\")= chr [1:7] \"(Intercept)\" \"X1\" \"X2\" \"X3\" ...\n $ residuals    : Named num [1:30] -8.11 1.647 1.061 -0.227 6.546 ...\n  ..- attr(*, \"names\")= chr [1:30] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:30] -354.011 54.107 2.742 11.715 -0.971 ...\n  ..- attr(*, \"names\")= chr [1:30] \"(Intercept)\" \"X1\" \"X2\" \"X3\" ...\n $ rank         : int 7\n $ fitted.values: Named num [1:30] 51.1 61.4 69.9 61.2 74.5 ...\n  ..- attr(*, \"names\")= chr [1:30] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:7] 0 1 2 3 4 5 6\n $ qr           :List of 5\n  ..$ qr   : num [1:30, 1:7] -5.477 0.183 0.183 0.183 0.183 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:30] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ : chr [1:7] \"(Intercept)\" \"X1\" \"X2\" \"X3\" ...\n  .. ..- attr(*, \"assign\")= int [1:7] 0 1 2 3 4 5 6\n  ..$ qraux: num [1:7] 1.18 1 1.29 1.1 1.07 ...\n  ..$ pivot: int [1:7] 1 2 3 4 5 6 7\n  ..$ tol  : num 1e-07\n  ..$ rank : int 7\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 23\n $ xlevels      : Named list()\n $ call         : language lm(formula = Y ~ ., data = data_3.3)\n $ terms        :Classes 'terms', 'formula'  language Y ~ X1 + X2 + X3 + X4 + X5 + X6\n  .. ..- attr(*, \"variables\")= language list(Y, X1, X2, X3, X4, X5, X6)\n  .. ..- attr(*, \"factors\")= int [1:7, 1:6] 0 1 0 0 0 0 0 0 0 1 ...\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:7] \"Y\" \"X1\" \"X2\" \"X3\" ...\n  .. .. .. ..$ : chr [1:6] \"X1\" \"X2\" \"X3\" \"X4\" ...\n  .. ..- attr(*, \"term.labels\")= chr [1:6] \"X1\" \"X2\" \"X3\" \"X4\" ...\n  .. ..- attr(*, \"order\")= int [1:6] 1 1 1 1 1 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. ..- attr(*, \"predvars\")= language list(Y, X1, X2, X3, X4, X5, X6)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:7] \"numeric\" \"numeric\" \"numeric\" \"numeric\" ...\n  .. .. ..- attr(*, \"names\")= chr [1:7] \"Y\" \"X1\" \"X2\" \"X3\" ...\n $ model        :'data.frame':  30 obs. of  7 variables:\n  ..$ Y : num [1:30] 43 63 71 61 81 43 58 71 72 67 ...\n  ..$ X1: num [1:30] 51 64 70 63 78 55 67 75 82 61 ...\n  ..$ X2: num [1:30] 30 51 68 45 56 49 42 50 72 45 ...\n  ..$ X3: num [1:30] 39 54 69 47 66 44 56 55 67 47 ...\n  ..$ X4: num [1:30] 61 63 76 54 71 54 66 70 71 62 ...\n  ..$ X5: num [1:30] 92 73 86 84 83 49 68 66 83 80 ...\n  ..$ X6: num [1:30] 45 47 48 35 47 34 35 41 31 41 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language Y ~ X1 + X2 + X3 + X4 + X5 + X6\n  .. .. ..- attr(*, \"variables\")= language list(Y, X1, X2, X3, X4, X5, X6)\n  .. .. ..- attr(*, \"factors\")= int [1:7, 1:6] 0 1 0 0 0 0 0 0 0 1 ...\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:7] \"Y\" \"X1\" \"X2\" \"X3\" ...\n  .. .. .. .. ..$ : chr [1:6] \"X1\" \"X2\" \"X3\" \"X4\" ...\n  .. .. ..- attr(*, \"term.labels\")= chr [1:6] \"X1\" \"X2\" \"X3\" \"X4\" ...\n  .. .. ..- attr(*, \"order\")= int [1:6] 1 1 1 1 1 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(Y, X1, X2, X3, X4, X5, X6)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:7] \"numeric\" \"numeric\" \"numeric\" \"numeric\" ...\n  .. .. .. ..- attr(*, \"names\")= chr [1:7] \"Y\" \"X1\" \"X2\" \"X3\" ...\n - attr(*, \"class\")= chr \"lm\"\n\n# 잔차 \n\nres_lm$residuals\n\n          1           2           3           4           5           6 \n -8.1102953   1.6472337   1.0605589  -0.2268416   6.5462010 -10.9418499 \n          7           8           9          10          11          12 \n -9.1484140   0.9029929  -7.5309862   7.8015424   6.0742817  11.5989723 \n         13          14          15          16          17          18 \n  9.4183197  -2.2140147   0.4506705  -3.5478519  -2.1501319   3.6026355 \n         19          20          21          22          23          24 \n -3.0165587  -5.6201442   7.3967582   0.1809831 -10.6639999  -4.6247464 \n         25          26          27          28          29          30 \n  5.6828983  -1.8434727   2.8596385  -8.0453540   7.3394730   5.1215016 \n\nresid(res_lm) #실제값에서 예측된 값을 뺸값\n\n          1           2           3           4           5           6 \n -8.1102953   1.6472337   1.0605589  -0.2268416   6.5462010 -10.9418499 \n          7           8           9          10          11          12 \n -9.1484140   0.9029929  -7.5309862   7.8015424   6.0742817  11.5989723 \n         13          14          15          16          17          18 \n  9.4183197  -2.2140147   0.4506705  -3.5478519  -2.1501319   3.6026355 \n         19          20          21          22          23          24 \n -3.0165587  -5.6201442   7.3967582   0.1809831 -10.6639999  -4.6247464 \n         25          26          27          28          29          30 \n  5.6828983  -1.8434727   2.8596385  -8.0453540   7.3394730   5.1215016 \n\n### 내적 표준화잔차\n\nrstandard(res_lm)\n\n          1           2           3           4           5           6 \n-1.41498026  0.23955370  0.16744867 -0.03512080  0.97184596 -1.86133876 \n          7           8           9          10          11          12 \n-1.38317210  0.13709194 -1.29490454  1.14799070  0.95218982  1.76906521 \n         13          14          15          16          17          18 \n 1.51371017 -0.46212316  0.06961486 -0.73868563 -0.34446368  0.75418016 \n         19          20          21          22          23          24 \n-0.45861365 -0.88618779  1.19699287  0.02717120 -1.56184734 -0.85286680 \n         25          26          27          28          29          30 \n 0.89948517 -0.36581416  0.44430497 -1.25422677  1.12683185  0.85971512 \n\n### 외적 표준화잔차\n\nMASS::studres(res_lm)\n\n          1           2           3           4           5           6 \n-1.44835328  0.23458097  0.16386794 -0.03434974  0.97062209 -1.97526518 \n          7           8           9          10          11          12 \n-1.41280382  0.13413337 -1.31529351  1.15637546  0.95017640  1.86145176 \n         13          14          15          16          17          18 \n 1.56019127 -0.45407837  0.06809185 -0.73117411 -0.33776450  0.74689589 \n         19          20          21          22          23          24 \n-0.45059801 -0.88189556  1.20894332  0.02657438 -1.61559196 -0.84763116 \n         25          26          27          28          29          30 \n 0.89560731 -0.35881868  0.43641573 -1.27088888  1.13380428  0.85466249 \n\nredsid_df&lt;-data.frame(\n\n  Y=data_3.3$Y,\n\n  Y_hat=res_lm$fitted.values,\n\n  resid=resid(res_lm),\n\n  rstandard=rstandard(res_lm),\n\n  studres=MASS::studres(res_lm)\n\n)\n\nredsid_df\n\n    Y    Y_hat       resid   rstandard     studres\n1  43 51.11030  -8.1102953 -1.41498026 -1.44835328\n2  63 61.35277   1.6472337  0.23955370  0.23458097\n3  71 69.93944   1.0605589  0.16744867  0.16386794\n4  61 61.22684  -0.2268416 -0.03512080 -0.03434974\n5  81 74.45380   6.5462010  0.97184596  0.97062209\n6  43 53.94185 -10.9418499 -1.86133876 -1.97526518\n7  58 67.14841  -9.1484140 -1.38317210 -1.41280382\n8  71 70.09701   0.9029929  0.13709194  0.13413337\n9  72 79.53099  -7.5309862 -1.29490454 -1.31529351\n10 67 59.19846   7.8015424  1.14799070  1.15637546\n11 64 57.92572   6.0742817  0.95218982  0.95017640\n12 67 55.40103  11.5989723  1.76906521  1.86145176\n13 69 59.58168   9.4183197  1.51371017  1.56019127\n14 68 70.21401  -2.2140147 -0.46212316 -0.45407837\n15 77 76.54933   0.4506705  0.06961486  0.06809185\n16 81 84.54785  -3.5478519 -0.73868563 -0.73117411\n17 74 76.15013  -2.1501319 -0.34446368 -0.33776450\n18 65 61.39736   3.6026355  0.75418016  0.74689589\n19 65 68.01656  -3.0165587 -0.45861365 -0.45059801\n20 50 55.62014  -5.6201442 -0.88618779 -0.88189556\n21 50 42.60324   7.3967582  1.19699287  1.20894332\n22 64 63.81902   0.1809831  0.02717120  0.02657438\n23 53 63.66400 -10.6639999 -1.56184734 -1.61559196\n24 40 44.62475  -4.6247464 -0.85286680 -0.84763116\n25 63 57.31710   5.6828983  0.89948517  0.89560731\n26 66 67.84347  -1.8434727 -0.36581416 -0.35881868\n27 78 75.14036   2.8596385  0.44430497  0.43641573\n28 48 56.04535  -8.0453540 -1.25422677 -1.27088888\n29 85 77.66053   7.3394730  1.12683185  1.13380428\n30 82 76.87850   5.1215016  0.85971512  0.85466249\n\n\n\n\n\n\n\n\n\na&lt;-rnorm(100,70,10) #연속형 데이터\n\n# 히스토그램 \n\nhist(a)\n\n\n\nhist(a,breaks=5) #범위를 조절 막대의 5번 자름 \n\n\n\n# 줄기 잎 그림 \n\nstem(a)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  4 | \n  4 | 5\n  5 | 03444\n  5 | 566789\n  6 | 0011112223334444444444\n  6 | 56666667778999999\n  7 | 0000011111233333444444\n  7 | 556677788999\n  8 | 00022234\n  8 | 55679\n  9 | 33\n\nstem(round(a)) #줄기잎을 그릴때는 반올림을 하고 항상 진행 \n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  4 | 5\n  5 | 03444\n  5 | 566789\n  6 | 0011112223334444444444\n  6 | 56666667778999999\n  7 | 0000011111233333444444\n  7 | 556677788999\n  8 | 00022234\n  8 | 55679\n  9 | 33\n\nstem(round(a),scale=2) #scale을 2배로 늘려라 5기준으로 반으로 잘라서 \n\n\n  The decimal point is at the |\n\n  44 | 0\n  46 | \n  48 | \n  50 | 0\n  52 | 0\n  54 | 0000\n  56 | 000\n  58 | 00\n  60 | 000000\n  62 | 000000\n  64 | 00000000000\n  66 | 000000000\n  68 | 0000000\n  70 | 0000000000\n  72 | 000000\n  74 | 00000000\n  76 | 00000\n  78 | 00000\n  80 | 000\n  82 | 0000\n  84 | 000\n  86 | 00\n  88 | 0\n  90 | \n  92 | 00\n\n# 모든데이터를 볼 수있는 장점 데이터가 많으면 구림 \n\n# 점플롯\n\nidx&lt;-rep(1,length(a)) #a의 갯수에 맞춰서 1를 반복 \n\nplot(idx,a)\n\n\n\nplot(jitter(idx),a,xlim=c(0.5,1.5))\n\n\n\n# 상자그림\n\nboxplot(a) #사분위수에 대해서 알 수 있음 \n\n# 상자그림 + 점플롯\n\nboxplot(a)\n\npoints(jitter(idx),a)\n\n\n\n\n\n\n\n\ndata_4.1&lt;-read.table(\"All_Data/p103.txt\",header=T,sep=\"\\t\")\n\ndata_4.1\n\n       Y   X1   X2\n1  12.37 2.23 9.66\n2  12.66 2.57 8.94\n3  12.00 3.87 4.40\n4  11.93 3.10 6.64\n5  11.06 3.39 4.91\n6  13.03 2.83 8.52\n7  13.13 3.02 8.04\n8  11.44 2.14 9.05\n9  12.86 3.04 7.71\n10 10.84 3.26 5.11\n11 11.20 3.39 5.05\n12 11.56 2.35 8.51\n13 10.83 2.76 6.59\n14 12.63 3.90 4.90\n15 12.46 3.16 6.96\n\nclass(data_4.1) #data.frame\n\n[1] \"data.frame\"\n\n# 산점도 행렬\n\nplot(data_4.1)\n\ncor(data_4.1) #상관계수\n\n             Y           X1         X2\nY  1.000000000  0.002497966  0.4340688\nX1 0.002497966  1.000000000 -0.8997765\nX2 0.434068758 -0.899776481  1.0000000\n\npairs(data_4.1)\n\n\n\n# Correlation panel\n\npanel.cor&lt;-function(x,y){\n\n  par(usr=c(0,1,0,1))\n\n  r&lt;-round(cor(x,y),digits = 3)\n\n  text(0.5,0.5,r,cex=1.5)\n\n}\n\npairs(data_4.1,lower.panel = panel.cor)\n\n\n\n# 회전도표, 동적 그래프(3차원)\n\n#install.packages(\"rgl\")\n\nlibrary(rgl)\n\nplot3d(x=data_4.1$X1,y=data_4.1$X2,z=data_4.1$Y) \n\n\n\n\n\ndata_3.3&lt;-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\nres_lm&lt;-lm(Y~X1+X3,data=data_3.3)\n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(res_lm) #회귀진단 플랏이 나옴 \n\n\n\nlayout(1)\n\n# 1. 표준화잔차의 정규확률분포\n\nplot(res_lm,2)\n\n\n\n# 2. 표준화잔차 대 각 예측변수들의 산점도\n\nplot(res_lm,3) #이런경우에는 데이터를 추가한다 좌측하단이 없어서 \n\n\n\n# 3. 표준화잔차 대 적합값의 플롯\n\n# 4. 표준화잔차의 인덱스 플롯\n\n\n\n\n\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:car':\n\n    recode\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n\n\ndata_3.3&lt;-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\nres_lm&lt;-lm(Y~X1+X3,data=data_3.3) #두개의 변수만 의미있다고 가정하고 진행 \n\n# 회귀진단 그래프들\n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(res_lm) #회귀진단 플랏이 나옴 / 총6개임 \n\n\n\nlayout(1) #다시 한개의 플랏만 그리도록 \n\n# 1. 표준화잔차의 정규확률분포\n\nplot(res_lm,2) #QQ-plot y=x 기울기의 직선위에 점들이 있어야 한다 눈대중으로 \n\n\n\n# 2. 표준화잔차 대 각 예측변수들의 산점도\n\nplot(res_lm,3) #이런경우에는 데이터를 추가한다 좌측하단이 없어서 \n\n\n\n#랜덤하게 데이터가 흩어져 있어야 한다\n\n# 내적 표준화잔차\n\nplot(data_3.3$X1,rstandard(res_lm))\n\n\n\nplot(data_3.3$X3,rstandard(res_lm)) #각각의 잔차들이 랜덤하게 잘 퍼져야함 \n\n\n\n# 3. 표준화잔차 대 적합값의 플롯\n\nplot(res_lm,1) #잔차와 적합값은 상관성이 없어야하며 랜덤하게 퍼져야함 \n\n\n\n# 4. 표준화잔차의 인덱스 플롯 \n\nplot(res_lm,5)\n\n\n\ndata_2.4&lt;-read.table(\"All_Data/p029b.txt\",header=T,sep=\"\\t\")\n\nm&lt;-matrix(1:4,ncol=2,byrow=TRUE)  \n\nlayout(m)\n\nplot(Y1~X1, data=data_2.4,pch=19); abline(3,0.5)\n\nplot(Y2~X2, data=data_2.4,pch=19); abline(3,0.5)\n\nplot(Y3~X3, data=data_2.4,pch=19); abline(3,0.5)\n\nplot(Y4~X4, data=data_2.4,pch=19); abline(3,0.5)\n\n\n\nlayout(1)\n\n\n\n\n\nres_lm&lt;-lm(Y1~X1,data=data_2.4) \n\n#1번플랏은 적당히 잘퍼짐,2번플랏은 어느정도 선형성 있음(데이터적어서그런거임)\n\nres_lm&lt;-lm(Y2~X2,data=data_2.4)\n\nres_lm&lt;-lm(Y3~X3,data=data_2.4)\n\nres_lm&lt;-lm(Y4~X4,data=data_2.4)\n\nres_lm\n\n\nCall:\nlm(formula = Y4 ~ X4, data = data_2.4)\n\nCoefficients:\n(Intercept)           X4  \n     3.0017       0.4999  \n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(res_lm) \n\nWarning: not plotting observations with leverage one:\n  8\n\n\n\n\nlayout(1)\n\n\n\n\n\n# 사례: 뉴욕 강 데이터\n\n# agr-농업, forest-숲, rsdntial-주거, comlndl-산업, nitrogen-질소\n\ndata_1.9&lt;-read.table(\"All_Data/p010.txt\",header=T,sep=\"\\t\")\n\nhead(data_1.9)\n\n       River Agr Forest Rsdntial ComIndl Nitrogen\n1      Olean  26     63      1.2    0.29     1.10\n2  Cassadaga  29     57      0.7    0.09     1.01\n3      Oatka  54     26      1.8    0.58     1.90\n4  Neversink   2     84      1.9    1.98     1.00\n5 Hackensack   3     27     29.4    3.11     1.99\n6  Wappinger  19     61      3.4    0.56     1.42\n\nplot(data_1.9[-1],pch=19) #river라는 첫번째 컬럼을 제외하고 진행 \n\n\n\nres_1&lt;-lm(Nitrogen~.,data=data_1.9[-1]) #모든데이터 사용\n\nres_2&lt;-lm(Nitrogen~.,data=data_1.9[-4,-1]) #4번쨰 데이터 제외\n\nres_3&lt;-lm(Nitrogen~.,data=data_1.9[-5,-1]) #5번쨰 데이터 제외 \n\n#회귀계수\n\ndata.frame(all=coef(res_1),\n\n           rm4=coef(res_2),\n\n           rm5=coef(res_3))\n\n                     all          rm4          rm5\n(Intercept)  1.722213529  1.099471134  1.626014115\nAgr          0.005809126  0.010136685  0.002352222\nForest      -0.012967887 -0.007589231 -0.012760349\nRsdntial    -0.007226768 -0.123792917  0.181160986\nComIndl      0.305027765  1.528956204  0.075617570\n\n#p-value\n\ndata.frame(all=coef(summary(res_1))[,4],\n\n           rm4=coef(summary(res_2))[,4],\n\n           rm5=coef(summary(res_3))[,4])\n\n                   all          rm4        rm5\n(Intercept) 0.18316946 0.2477883387 0.05619948\nAgr         0.70462624 0.3717054741 0.80880737\nForest      0.36667966 0.4700975391 0.16975563\nRsdntial    0.83372002 0.0071342930 0.00112280\nComIndl     0.08230952 0.0005512227 0.51774981\n\n#4,5번째 데이터를 각각 뺴고 진행을 해보니 영향을 끼치는 값임을 알수있고\n\n#5번째는 주거 관련해서는 부호를 바꿀 정도로 강력하다 \n\n# 단순선형회구모형\n\nres&lt;-lm(Nitrogen~ComIndl,data=data_1.9)\n\n\n\n\n\nplot(Nitrogen~ComIndl,data=data_1.9,pch=19)\n\nabline(res)\n\n#leverage values 지레값\n\np_ii&lt;-hatvalues(res)\n\nhiegh_leverage&lt;-ifelse(p_ii&gt;2*2/30,data_1.9$River,\"\")\n\nhiegh_leverage #높은 지레값을 가지고 있는 강의 이름을 표시(이는 보기에 편하기 위해서함)\n\n           1            2            3            4            5            6 \n          \"\"           \"\"           \"\"  \"Neversink\" \"Hackensack\"           \"\" \n           7            8            9           10           11           12 \n          \"\"           \"\"           \"\"           \"\"           \"\"           \"\" \n          13           14           15           16           17           18 \n          \"\"           \"\"           \"\"           \"\"           \"\"           \"\" \n          19           20 \n          \"\"           \"\" \n\ntext(data_1.9$ComIndl,data_1.9$Nitrogen-0.1,hiegh_leverage)\n\n\n\n\n\n\n\n\nplot(rstandard(res),pch=19) #2또는 3시그마를 넘으면 특이값이라 함 \n\n\n\n\n\n\n\n\nplot(p_ii,pch=19) #평균의 2배를 기준으로 비교함 \n\nabline(h=2*2/30,col=\"red\") #이보다 높은것이 지레값이 높은것 높은 영향력을 가진것 \n\n\n\n# 단순선형회귀모형\n\nres&lt;-lm(Nitrogen~ComIndl,data=data_1.9)\n\nplot(Nitrogen~ComIndl,data=data_1.9,pch=19)\n\nabline(res)\n\n\n\nres&lt;-lm(Nitrogen~ComIndl,data=data_1.9[-4,-1])\n\nplot(Nitrogen~ComIndl,data=data_1.9[-4,-1],pch=19)\n\nabline(res) #4번째 제외\n\n\n\nres&lt;-lm(Nitrogen~ComIndl,data=data_1.9[-5,-1])\n\nplot(Nitrogen~ComIndl,data=data_1.9[-5,-1],pch=19)\n\nabline(res) #5번쨰 제외\n\n\n\n#이처럼 4,5번을 빼고 진행을 하면 조금더 잘 나타냄\n\nres&lt;-lm(Nitrogen~ComIndl,data=data_1.9[c(-4,-5),-1])\n\nplot(Nitrogen~ComIndl,data=data_1.9[-5,-1],pch=19)\n\nabline(res) #4,5번째 제외 \n\n\n\n\n\n\n\n\n\n\n\nres&lt;-lm(Nitrogen~ComIndl,data=data_1.9)\n\nplot(res,4)\n\n#install.packages(\"olsrr\")\n\nlibrary(olsrr)\n\n\nAttaching package: 'olsrr'\n\n\nThe following object is masked from 'package:datasets':\n\n    rivers\n\n\n\n\nols_plot_cooksd_chart(res)\n\n\n\n\n\n\n\n\nolsrr::ols_plot_dffits(res)\n\n\n\n\n\n\n\n\nolsrr::ols_plot_hadi(res)\n\n\n\n# Residual & Leverage & Cook's distance\n\nplot(res,5) #영향력 관측치를 보기 위한 플랏 \n\n\n\n\n\n\n\n\nolsrr::ols_plot_resid_pot(res) #2.0에 있는 값외에도 x축 0.2이상의 것들도 특이값으로 \n\n\n\n#지레값이 커도 영향력이 없는 애들은 신경 안써도 되나 영향력이 큰애들을 탐색해 보아야 함\n\n\n\n\n\n#특이값이 큰경우 그것이 TRUE데이터면 오류가 있는 경우 수정을 하거나 가중치를 변화를\n\n#하거나 데이터를 수정을 시켜주거나 다시 실험을 하는 등 여러가지 방법을 사용하여.. \n\n## 첨가변수플롯(added-variable plot), 편회귀플롯(partial regression plot)\n\nres&lt;-lm(Nitrogen~ComIndl,data=data_1.9)\n\ncar::avPlots(res,pch=19)\n\n\n\nres&lt;-lm(Nitrogen~.,data=data_1.9[-1])\n\ncar::avPlots(res,pch=19)\n\n\n\n#beta별로 각각의 어떤 변수가 영향력을 많이 주는지 알게하는 함수 \n\n\n\n\n\ndata_4.5&lt;-read.table(\"All_Data/p120.txt\",header=T,sep=\"\\t\")\n\ndim(data_4.5)\n\n[1] 35  4\n\nnames(data_4.5)\n\n[1] \"Hill.Race\" \"Time\"      \"Distance\"  \"Climb\"    \n\nhead(data_4.5)\n\n                    Hill.Race Time Distance Climb\n1 Greenmantle New Year Dash    965      2.5   650\n2                  Carnethy   2901      6.0  2500\n3              Craig Dunain   2019      6.0   900\n4                   Ben Rha   2736      7.5   800\n5                Ben Lomond   3736      8.0  3070\n6                  Goatfell   4393      8.0  2866\n\n# 회전도표 \n\nlibrary(rgl)\n\nwith(data_4.5,plot3d(x=Distance,y=Climb,z=Time))\n\nres&lt;-lm(Time~Distance+Climb,data=data_4.5)\n\nsummary(res) #beta_0가 마이너스여도 신경안씀 관심있는 것은 회귀계수임 \n\n\nCall:\nlm(formula = Time ~ Distance + Climb, data = data_4.5)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-973.0 -427.7  -71.2  142.2 3907.3 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -539.4829   258.1607  -2.090   0.0447 *  \nDistance     373.0727    36.0684  10.343 9.86e-12 ***\nClimb          0.6629     0.1231   5.387 6.44e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 880.5 on 32 degrees of freedom\nMultiple R-squared:  0.9191,    Adjusted R-squared:  0.914 \nF-statistic: 181.7 on 2 and 32 DF,  p-value: &lt; 2.2e-16\n\n#Time=-539.4829+373.0727*Distance+0.6629*Climb \n\n\n### 첨가변수플롯(added-variable plot), 편회귀플롯(partial regression plot)\n\ncar::avPlots(res,pch=19)\n\n\n\n### 성분잔차플롯(component plus residual plots), 편자차플롯(partial residual plot)\n\ncar::crPlots(res,id=T,pch=19) #첨가변수보다 성분잔차를 더 많이 사용 / 비선형여부를 확인\n\n\n\n# 점선에 비해서 분홍선이 크게 떨어져 있지 않아 선형적인 추세를 가지고 있다고 추정이 가능 \n\n### 잠재성-잔차플롯\n\nolsrr::ols_plot_resid_pot(res)\n\n\n\n### Hadi의 영향력 측도\n\nolsrr::ols_plot_hadi(res)\n\n\n\n### Cook의 거리\n\nolsrr::ols_plot_cooksd_chart(res) #전체적은 플롯을 보면서 이상치에 대한 확인을 함 \n\n\n\n# 이러한 값들의 제외 여부는 연구자가 선택해서 진행을 함 \n\n# outlier에 대한 처리를 어떻게 했다고 말을 해야함 \n\n# 보고서는 옆에 사람이 보고 쉽게 따라할 수 있을 정도로 \n\n# 어떤 속성으로 어떻게 진행을 했다라는 것을 중시 코드 보단 결과를 보여줘라 \n\n\n\n\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:olsrr':\n\n    cement\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nres&lt;-lm(Time~Distance+Climb,data=data_4.5)\n\nres\n\n\nCall:\nlm(formula = Time ~ Distance + Climb, data = data_4.5)\n\nCoefficients:\n(Intercept)     Distance        Climb  \n  -539.4829     373.0727       0.6629  \n\nres_rlm&lt;-MASS::rlm(Time~Distance+Climb,data=data_4.5)\n\nres_rlm\n\nCall:\nrlm(formula = Time ~ Distance + Climb, data = data_4.5)\nConverged in 10 iterations\n\nCoefficients:\n (Intercept)     Distance        Climb \n-576.3836570  393.0374614    0.4977894 \n\nDegrees of freedom: 35 total; 32 residual\nScale estimate: 313 \n\nsummary(res)\n\n\nCall:\nlm(formula = Time ~ Distance + Climb, data = data_4.5)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-973.0 -427.7  -71.2  142.2 3907.3 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -539.4829   258.1607  -2.090   0.0447 *  \nDistance     373.0727    36.0684  10.343 9.86e-12 ***\nClimb          0.6629     0.1231   5.387 6.44e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 880.5 on 32 degrees of freedom\nMultiple R-squared:  0.9191,    Adjusted R-squared:  0.914 \nF-statistic: 181.7 on 2 and 32 DF,  p-value: &lt; 2.2e-16\n\nsummary(res_rlm)\n\n\nCall: rlm(formula = Time ~ Distance + Climb, data = data_4.5)\nResiduals:\n     Min       1Q   Median       3Q      Max \n-645.074 -197.082   -2.035  212.266 3942.045 \n\nCoefficients:\n            Value     Std. Error t value  \n(Intercept) -576.3837  105.2774    -5.4749\nDistance     393.0375   14.7086    26.7216\nClimb          0.4978    0.0502     9.9200\n\nResidual standard error: 312.6 on 32 degrees of freedom\n\n#install.packages(\"robustbase\")\n\nlibrary(robustbase)\n\nres_lmrob&lt;-lmrob(Time~Distance+Climb,data=data_4.5)\n\nres_lmrob\n\n\nCall:\nlmrob(formula = Time ~ Distance + Climb, data = data_4.5)\n \\--&gt; method = \"MM\"\nCoefficients:\n(Intercept)     Distance        Climb  \n  -487.3793     398.2784       0.3901  \n\nsummary(res_lmrob)\n\n\nCall:\nlmrob(formula = Time ~ Distance + Climb, data = data_4.5)\n \\--&gt; method = \"MM\"\nResiduals:\n    Min      1Q  Median      3Q     Max \n-650.01 -160.60   23.37  216.11 3875.00 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -487.37929   86.33384  -5.645 3.04e-06 ***\nDistance     398.27843    5.98522  66.544  &lt; 2e-16 ***\nClimb          0.39013    0.04165   9.368 1.09e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nRobust residual standard error: 290.8 \nMultiple R-squared:  0.9868,    Adjusted R-squared:  0.986 \nConvergence in 9 IRWLS iterations\n\nRobustness weights: \n 3 observations c(7,18,33) are outliers with |weight| = 0 ( &lt; 0.0029); \n 2 weights are ~= 1. The remaining 30 ones are summarized as\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.5965  0.9098  0.9623  0.9214  0.9875  0.9990 \nAlgorithmic parameters: \n       tuning.chi                bb        tuning.psi        refine.tol \n        1.548e+00         5.000e-01         4.685e+00         1.000e-07 \n          rel.tol         scale.tol         solve.tol          zero.tol \n        1.000e-07         1.000e-10         1.000e-07         1.000e-10 \n      eps.outlier             eps.x warn.limit.reject warn.limit.meanrw \n        2.857e-03         1.364e-08         5.000e-01         5.000e-01 \n     nResample         max.it       best.r.s       k.fast.s          k.max \n           500             50              2              1            200 \n   maxit.scale      trace.lev            mts     compute.rd fast.s.large.n \n           200              0           1000              0           2000 \n                  psi           subsampling                   cov \n           \"bisquare\"         \"nonsingular\"         \".vcov.avar1\" \ncompute.outlier.stats \n                 \"SM\" \nseed : int(0) \n\n#standard error가 res &gt; res_rlm &gt; res_lmrob 순으로 되어 있음 \n\n# 4장 연습문제 해보기\n\n\n\n\n\n\nlibrary(dplyr)\n\n\n\n\n\n#install.packages(\"fastDummies\")\n\nlibrary(fastDummies)\n\nThank you for using fastDummies!\n\n\nTo acknowledge our work, please cite the package:\n\n\nKaplan, J. & Schlegel, B. (2023). fastDummies: Fast Creation of Dummy (Binary) Columns and Rows from Categorical Variables. Version 1.7.1. URL: https://github.com/jacobkap/fastDummies, https://jacobkap.github.io/fastDummies/.\n\n\n\n\n\n\ndata_5.1&lt;-read.table(\"All_Data/p130.txt\",header=T,sep=\"\\t\")\n\n# S:급료 X:경력 E:교육수준 M:관리(형태) / E,M은 범주형 변수\n\nnames(data_5.1)\n\n[1] \"S\" \"X\" \"E\" \"M\"\n\n# 범주형 질적 변수를 수치형으로 변형시켜서 예측하는데 사용한것이 질적 예측변수이다 \n\n# E_1,E_2,E_3이런식으로 나누어서 0,1로 분류를 한다 (이것이 가변수)\n\n# 더미변수를 만들 경우에는 역행렬의 조건에 의해서 -1개의 변수만 만들면 된다 \n\n# 이는 공산성의 문제또한 있기에 이를 위해서 -1를 한것임 \n\n### 자료형 변경 : 정수 -&gt; 범주\n\ndata_5.1$E&lt;-as.factor(data_5.1$E)\n\ndata_5.1$M&lt;-as.factor(data_5.1$M)\n\nhead(data_5.1)\n\n      S X E M\n1 13876 1 1 1\n2 11608 1 3 0\n3 18701 1 3 1\n4 11283 1 2 0\n5 11767 1 3 0\n6 20872 2 2 1\n\ndata_5.1$E #Levels: 1 2 3이라는 것이 생김 문자로 처리한다는 의미 \n\n [1] 1 3 3 2 3 2 2 1 3 2 1 2 3 1 3 3 2 2 3 1 1 3 2 2 1 2 1 3 1 1 2 3 2 2 1 2 3 1\n[39] 2 2 3 2 2 1 2 1\nLevels: 1 2 3\n\n### 가변수 생성\n\ndata_5.1$E&lt;-factor(as.character(data_5.1$E),levels = c(\"3\",\"1\",\"2\")) \n\n#3번을 베이스 카테고리로 쓰기위한 설정 / 설정을 안하면 베이스는 e_1이 된다\n\ndata_5.1$M&lt;-factor(as.character(data_5.1$M),levels = c(\"0\",\"1\")) \n\ndata_dummy&lt;-dummy_cols(data_5.1,\n\n                       select_columns = c(\"E\",\"M\"),\n\n                       remove_first_dummy = T,\n\n                       remove_selected_columns = T) #첫번째 생성되는 더미 변수를 제거\n\ndata_dummy #가변수의 더미는 n-1개를 하는 것이 역행렬을 위한 것이기에 지워준다 \n\n       S  X E_1 E_2 M_1\n1  13876  1   1   0   1\n2  11608  1   0   0   0\n3  18701  1   0   0   1\n4  11283  1   0   1   0\n5  11767  1   0   0   0\n6  20872  2   0   1   1\n7  11772  2   0   1   0\n8  10535  2   1   0   0\n9  12195  2   0   0   0\n10 12313  3   0   1   0\n11 14975  3   1   0   1\n12 21371  3   0   1   1\n13 19800  3   0   0   1\n14 11417  4   1   0   0\n15 20263  4   0   0   1\n16 13231  4   0   0   0\n17 12884  4   0   1   0\n18 13245  5   0   1   0\n19 13677  5   0   0   0\n20 15965  5   1   0   1\n21 12336  6   1   0   0\n22 21352  6   0   0   1\n23 13839  6   0   1   0\n24 22884  6   0   1   1\n25 16978  7   1   0   1\n26 14803  8   0   1   0\n27 17404  8   1   0   1\n28 22184  8   0   0   1\n29 13548  8   1   0   0\n30 14467 10   1   0   0\n31 15942 10   0   1   0\n32 23174 10   0   0   1\n33 23780 10   0   1   1\n34 25410 11   0   1   1\n35 14861 11   1   0   0\n36 16882 12   0   1   0\n37 24170 12   0   0   1\n38 15990 13   1   0   0\n39 26330 13   0   1   1\n40 17949 14   0   1   0\n41 25685 15   0   0   1\n42 27837 16   0   1   1\n43 18838 16   0   1   0\n44 17483 16   1   0   0\n45 19207 17   0   1   0\n46 19346 20   1   0   0\n\n# 더미를 만든 이후 더미의 모체 변수인 E M을 지워주어야 한다 \n\n### 회귀분석(1) - 가변수\n\nres&lt;-lm(S~.,data = data_dummy)\n\nres \n\n\nCall:\nlm(formula = S ~ ., data = data_dummy)\n\nCoefficients:\n(Intercept)            X          E_1          E_2          M_1  \n    11031.8        546.2      -2996.2        147.8       6883.5  \n\n### 회귀분석(2) - lm()\n\nres_1&lt;-lm(S~.,data=data_5.1)\n\nres_1 \n\n\nCall:\nlm(formula = S ~ ., data = data_5.1)\n\nCoefficients:\n(Intercept)            X           E1           E2           M1  \n    11031.8        546.2      -2996.2        147.8       6883.5  \n\n#더미변수를 많이 쓰기에 factor로 바꾸어주고 분석하면 알아서 더미변수를 만들어서 진행함\n\nsummary(res_1)\n\n\nCall:\nlm(formula = S ~ ., data = data_5.1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1884.60  -653.60    22.23   844.85  1716.47 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11031.81     383.22  28.787  &lt; 2e-16 ***\nX             546.18      30.52  17.896  &lt; 2e-16 ***\nE1          -2996.21     411.75  -7.277 6.72e-09 ***\nE2            147.82     387.66   0.381    0.705    \nM1           6883.53     313.92  21.928  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1027 on 41 degrees of freedom\nMultiple R-squared:  0.9568,    Adjusted R-squared:  0.9525 \nF-statistic: 226.8 on 4 and 41 DF,  p-value: &lt; 2.2e-16\n\n#E_3와 M_0는 Intercept에 포함이 되어있음 그렇기에 베이스 카테고리라고 한다 \n\n#E가 3이고 M이 0이면 대학원이상 일반관리 직급 -&gt; Intercept(Beta_0) + X*Beta_1 = Y\n\n### 표준화잔차 대 경력연수\n\nplot(data_5.1$X, rstandard(res_1),pch=19,xlab=\"범주\",ylab=\"잔차\")\n\n\n\n# 0을 중심으로 잘 퍼져있는가를 확인해야함 \n\n#### 표준화잔차 대 교육수준 - 관리 조합\n\nEM&lt;-paste0(data_5.1$E,data_5.1$M)\n\nplot(EM,rstandard(res_1),pch=19,xlabl=\"범주\",ylab=\"잔차\")\n\nWarning in plot.window(...): \"xlabl\" is not a graphical parameter\n\n\nWarning in plot.xy(xy, type, ...): \"xlabl\" is not a graphical parameter\n\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"xlabl\" is not a\ngraphical parameter\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"xlabl\" is not a\ngraphical parameter\n\n\nWarning in box(...): \"xlabl\" is not a graphical parameter\n\n\nWarning in title(...): \"xlabl\" is not a graphical parameter\n\n\n\n\n### 상호작용 효과(Interaction Effect)\n\nres&lt;-lm(S~X+E+M+E*M,data=data_5.1)\n\nres\n\n\nCall:\nlm(formula = S ~ X + E + M + E * M, data = data_5.1)\n\nCoefficients:\n(Intercept)            X           E1           E2           M1        E1:M1  \n    11203.4        497.0      -1730.7       -349.1       7047.4      -3066.0  \n      E2:M1  \n     1836.5  \n\nsummary(res)\n\n\nCall:\nlm(formula = S ~ X + E + M + E * M, data = data_5.1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-928.13  -46.21   24.33   65.88  204.89 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11203.434     79.065 141.698  &lt; 2e-16 ***\nX             496.987      5.566  89.283  &lt; 2e-16 ***\nE1          -1730.748    105.334 -16.431  &lt; 2e-16 ***\nE2           -349.078     97.568  -3.578 0.000945 ***\nM1           7047.412    102.589  68.695  &lt; 2e-16 ***\nE1:M1       -3066.035    149.330 -20.532  &lt; 2e-16 ***\nE2:M1        1836.488    131.167  14.001  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 173.8 on 39 degrees of freedom\nMultiple R-squared:  0.9988,    Adjusted R-squared:  0.9986 \nF-statistic:  5517 on 6 and 39 DF,  p-value: &lt; 2.2e-16\n\n### 표준화잔차 대 경력연수\n\nplot(data_5.1$X,rstandard(res),pch=19,xlab=\"범주\",ylab=\"잔차\")\n\n\n\n### Cook의 거리\n\nplot(res,4) #33번째 데이터만 제외하고 다시 회귀모형을 생성예정\n\n\n\n### 상호작용 효과 - 관측개체 33 제외 \n\ndata_use&lt;-data_5.1[-33,]\n\nres&lt;-lm(S~X+E+M+E*M,data=data_use)\n\nres\n\n\nCall:\nlm(formula = S ~ X + E + M + E * M, data = data_use)\n\nCoefficients:\n(Intercept)            X           E1           E2           M1        E1:M1  \n    11199.7        498.4      -1741.3       -357.0       7040.6      -3051.8  \n      E2:M1  \n     1997.5  \n\nsummary(res)\n\n\nCall:\nlm(formula = S ~ X + E + M + E * M, data = data_use)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-112.884  -43.636   -5.036   46.622  128.480 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11199.714     30.533 366.802  &lt; 2e-16 ***\nX             498.418      2.152 231.640  &lt; 2e-16 ***\nE1          -1741.336     40.683 -42.803  &lt; 2e-16 ***\nE2           -357.042     37.681  -9.475 1.49e-11 ***\nM1           7040.580     39.619 177.707  &lt; 2e-16 ***\nE1:M1       -3051.763     57.674 -52.914  &lt; 2e-16 ***\nE2:M1        1997.531     51.785  38.574  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 67.12 on 38 degrees of freedom\nMultiple R-squared:  0.9998,    Adjusted R-squared:  0.9998 \nF-statistic: 3.543e+04 on 6 and 38 DF,  p-value: &lt; 2.2e-16\n\n### 표준화잔차 대 경력연수\n\nplot(data_use$X,rstandard(res),pch=19,xlab=\"범주\",ylab=\"잔차\")\n\n\n\n#### 표준화잔차 대 교육수준 - 관리 조합\n\nEM&lt;-paste0(data_use$E,data_use$M)\n\nplot(EM,rstandard(res),pch=19,xlabl=\"범주\",ylab=\"잔차\")\n\nWarning in plot.window(...): \"xlabl\" is not a graphical parameter\n\n\nWarning in plot.xy(xy, type, ...): \"xlabl\" is not a graphical parameter\n\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"xlabl\" is not a\ngraphical parameter\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"xlabl\" is not a\ngraphical parameter\n\n\nWarning in box(...): \"xlabl\" is not a graphical parameter\n\n\nWarning in title(...): \"xlabl\" is not a graphical parameter\n\n\n\n\n#상호 호과가 들어간 이 모형이 더 괜찮은 모양이라고 판단이 된다 \n\n### 기본 급료 추정 - 표 5.6\n\nres&lt;-lm(S~X+E+M+E*M,data=data_use)\n\ndf_new&lt;-data.frame(X=rep(0,6),\n\n                   E=rep(1:3,c(2,2,2)),\n\n                   M=rep(c(0,1),3))\n\n### 가변수 생성 - 분석용\n\ndf_new$E&lt;-factor(as.character(df_new$E),levels = c(\"3\",\"1\",\"2\")) \n\ndf_new$M&lt;-factor(as.character(df_new$M),levels = c(\"0\",\"1\")) \n\ncbind(df_new,predict = predict(res,df_new,interval = \"confidence\"))\n\n  X E M predict.fit predict.lwr predict.upr\n1 0 1 0    9458.378    9395.539    9521.216\n2 0 1 1   13447.195   13382.933   13511.456\n3 0 2 0   10842.672   10789.719   10895.624\n4 0 2 1   19880.782   19814.090   19947.474\n5 0 3 0   11199.714   11137.902   11261.525\n6 0 3 1   18240.294   18182.503   18298.084\n\n# 이를 보고 각 레벨에 따른 차이를 보고 얼마나 나는 지 분석이 가능해야 한다 \n\n# ex) 고졸과 대학원졸의 관리자 직급의 급여의 차이는?(평균적으로)\n\n\n\n\n\n\n\ndata_5.7&lt;-read.table(\"All_Data/p140.txt\",header=T,sep=\"\\t\")\n\nhead(data_5.7)\n\n  TEST RACE JPERF\n1 0.28    1  1.83\n2 0.97    1  4.59\n3 1.25    1  2.97\n4 2.46    1  8.14\n5 2.51    1  8.00\n6 1.17    1  3.30\n\n# 모형 1 - 통합모형 인종간 차이가 없을때\n\nmodel_1&lt;-lm(JPERF~TEST,data=data_5.7)\n\nsummary(model_1) \n\n\nCall:\nlm(formula = JPERF ~ TEST, data = data_5.7)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3558 -0.8798 -0.1897  1.2735  2.3312 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.0350     0.8680   1.192 0.248617    \nTEST          2.3605     0.5381   4.387 0.000356 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.591 on 18 degrees of freedom\nMultiple R-squared:  0.5167,    Adjusted R-squared:  0.4899 \nF-statistic: 19.25 on 1 and 18 DF,  p-value: 0.0003555\n\n# 모형 3 \n\nmodel_3&lt;-lm(JPERF~TEST+RACE+TEST*RACE,data=data_5.7)\n\nsummary(model_3)\n\n\nCall:\nlm(formula = JPERF ~ TEST + RACE + TEST * RACE, data = data_5.7)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0734 -1.0594 -0.2548  1.2830  2.1980 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   2.0103     1.0501   1.914   0.0736 .\nTEST          1.3134     0.6704   1.959   0.0677 .\nRACE         -1.9132     1.5403  -1.242   0.2321  \nTEST:RACE     1.9975     0.9544   2.093   0.0527 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.407 on 16 degrees of freedom\nMultiple R-squared:  0.6643,    Adjusted R-squared:  0.6013 \nF-statistic: 10.55 on 3 and 16 DF,  p-value: 0.0004511\n\n# 인종적인 차이가 있는지 없는지를 확인해야 한다 어떤 모형을 사용할지 \n\n### H_0:gamma=delta=0\n\nanova(model_1,model_3) #model_3가 FM(완전모형)\n\nAnalysis of Variance Table\n\nModel 1: JPERF ~ TEST\nModel 2: JPERF ~ TEST + RACE + TEST * RACE\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     18 45.568                              \n2     16 31.655  2    13.913 3.5161 0.05424 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#P-value(0.05424)&lt;0.05이기에 H_0 이기에 모형1을 선택하는 것이 옳다고 판단(그러나 확신x)\n\n#서로의 R-squared롤 보면 model_3가 더 좋음 / ANOVA는 참고용 절대적이지는 않다 \n\n\n\n\n\nplot(data_5.7$TEST,rstandard(model_1),\n\n     pch=19,xlab=\"검사점수\",ylab=\"잔차\",\n\n     main=\"그림 5.7 - 표준화잔차 대 검사점수 : 모형 1\")\n\n\n\n\n\n\n\n\nplot(data_5.7$TEST,rstandard(model_3),\n\n     pch=19,xlab=\"검사점수\",ylab=\"잔차\",\n\n     main=\"그림 5.8 - 표준화잔차 대 검사점수 : 모형 3\")\n\n\n\n# 통계에서 나오는 결과는 결정의 보조수단이지 절대적이지 않아 \n\n# 3번 모형을 선택한다고 결정한다고 진행 \n\nsummary(model_3) # Multiple R-squared:  0.6643\n\n\nCall:\nlm(formula = JPERF ~ TEST + RACE + TEST * RACE, data = data_5.7)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0734 -1.0594 -0.2548  1.2830  2.1980 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   2.0103     1.0501   1.914   0.0736 .\nTEST          1.3134     0.6704   1.959   0.0677 .\nRACE         -1.9132     1.5403  -1.242   0.2321  \nTEST:RACE     1.9975     0.9544   2.093   0.0527 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.407 on 16 degrees of freedom\nMultiple R-squared:  0.6643,    Adjusted R-squared:  0.6013 \nF-statistic: 10.55 on 3 and 16 DF,  p-value: 0.0004511\n\n\n\n\n\n\nplot(data_5.7$RACE,rstandard(model_1),\n\n     pch=19,xlab=\"검사점수\",ylab=\"잔차\",\n\n     main=\"그림 5.9 - 표준화잔차 대 검사점수 : 모형 1\")\n\n\n\n# 분리된 회귀분석 결과\n\ndata_5.7_R1&lt;-subset(data_5.7,RACE==1)\n\nmodel_R1&lt;-lm(JPERF~TEST,data=data_5.7_R1)\n\nsummary(model_R1) #소수민족\n\n\nCall:\nlm(formula = JPERF ~ TEST, data = data_5.7_R1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0734 -0.6267 -0.2548  1.1624  1.5394 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.09712    1.03519   0.094 0.927564    \nTEST         3.31095    0.62411   5.305 0.000724 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.292 on 8 degrees of freedom\nMultiple R-squared:  0.7787,    Adjusted R-squared:  0.751 \nF-statistic: 28.14 on 1 and 8 DF,  p-value: 0.0007239\n\ndata_5.7_R0&lt;-subset(data_5.7,RACE==0)\n\nmodel_R0&lt;-lm(JPERF~TEST,data=data_5.7_R0)\n\nsummary(model_R0) #백인\n\n\nCall:\nlm(formula = JPERF ~ TEST, data = data_5.7_R0)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8599 -1.0663 -0.3061  1.0957  2.1980 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   2.0103     1.1291   1.780    0.113\nTEST          1.3134     0.7208   1.822    0.106\n\nResidual standard error: 1.512 on 8 degrees of freedom\nMultiple R-squared:  0.2933,    Adjusted R-squared:  0.205 \nF-statistic:  3.32 on 1 and 8 DF,  p-value: 0.1059\n\n# 통합모형에서 나온 각각의 회귀식이 통합모형에서 나온것과 동일함 따라서 인종별로 나누어서\n\n# 진행할 필요없이 통일모형을 사용해서 진행을 하면 된다(이는 데이터셋을 나눈경우와 동일함)\n\n\n\n\n\nplot(data_5.7_R1$TEST,rstandard(model_R1),\n\n     pch=19,xlab=\"검사점수\",ylab=\"잔차\",\n\n     main=\"그림 5.10 - 표준화잔차 대 검사점수 : 모형 1. 소수민족만만\")\n\n\n\n\n\n\n\n\nplot(data_5.7_R0$TEST,rstandard(model_R0),\n\n     pch=19,xlab=\"검사점수\",ylab=\"잔차\",\n\n     main=\"그림 5.11 - 표준화잔차 대 검사점수 : 모형 1. 백인만\")\n\n\n\n# 적절한 합격점수의 결정 - 소수민족\n\n# 고용전 검사점수의 합격점에 대한 95% 신뢰구간\n\nym&lt;-4\n\nxm&lt;-(ym-0.09712)/3.31095\n\ns&lt;-1.292\n\nn&lt;-10\n\nt&lt;-qt(1-0.05/2,8)\n\nc(xm-(t*s/n)/3.31095, xm+(t*s/n)/3.31095) #신뢰구간\n\n[1] 1.088795 1.268764\n\n\n\n\n\n\nmodel_3&lt;-lm(JPERF~TEST+RACE,data=data_5.7)\n\nsummary(model_3)\n\n\nCall:\nlm(formula = JPERF ~ TEST + RACE, data = data_5.7)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7872 -1.0370 -0.2095  0.9198  2.3645 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.6120     0.8870   0.690 0.499578    \nTEST          2.2988     0.5225   4.400 0.000391 ***\nRACE          1.0276     0.6909   1.487 0.155246    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.54 on 17 degrees of freedom\nMultiple R-squared:  0.5724,    Adjusted R-squared:  0.5221 \nF-statistic: 11.38 on 2 and 17 DF,  p-value: 0.0007312\n\n#Intercept = BETA_0 / TEST = BETA_1 / RACE = gamma\n\n## H_0: gamma=0\n\nanova(model_1,model_3) #p-value(0.1552)&gt;0.05 이기에 H_0은 참이다//gamma=0\n\nAnalysis of Variance Table\n\nModel 1: JPERF ~ TEST\nModel 2: JPERF ~ TEST + RACE\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     18 45.568                           \n2     17 40.322  1    5.2468 2.2121 0.1552\n\n#기울기가 같고 절편이 다른 모형은 아니라고 데이터가 이야기하고 있다 \n\n# 소수민족(RACE=1): (0.6120+1.0276)+2.2988*TEST = 1.6396+2.2988*TEST\n\n# 백인(RACE=0): (0.6120)+2.2988*TEST\n\n\n\n\n\nmodel_3&lt;-lm(JPERF~TEST+I(TEST*RACE),data=data_5.7)\n\nsummary(model_3) #I()를 하면 교호작용하는 것만 보이게 하려고 없으면 RACE항이 자동추가됨\n\n\nCall:\nlm(formula = JPERF ~ TEST + I(TEST * RACE), data = data_5.7)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.41100 -0.88871 -0.03359  0.97720  2.44440 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)      1.1211     0.7804   1.437  0.16900   \nTEST             1.8276     0.5356   3.412  0.00332 **\nI(TEST * RACE)   0.9161     0.3972   2.306  0.03395 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.429 on 17 degrees of freedom\nMultiple R-squared:  0.6319,    Adjusted R-squared:  0.5886 \nF-statistic: 14.59 on 2 and 17 DF,  p-value: 0.0002045\n\n## H_0: delta=0\n\nanova(model_1,model_3) #p-value(0.03395)&lt;0.05보다 작기에 delta항은 필요한 변수라는 사실 \n\nAnalysis of Variance Table\n\nModel 1: JPERF ~ TEST\nModel 2: JPERF ~ TEST + I(TEST * RACE)\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     18 45.568                              \n2     17 34.708  1    10.861 5.3196 0.03395 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#Intercept = BETA_0 / TEST = BETA_1 / I(TEST*RACE) = delta\n\n\n\n\n\n\nlibrary(dplyr)\n\n\n\n\ndata_6.2&lt;-read.table(\"All_Data/p168.txt\",header=T,sep=\"\\t\")\n\nhead(data_6.2)\n\n  t N_t\n1 1 355\n2 2 211\n3 3 197\n4 4 166\n5 5 142\n6 6 106\n\n\n\n\n\n\nplot(N_t~t,data=data_6.2,pch=19)\n\n\n\n\n\n\n\n\nres&lt;-lm(N_t~t,data=data_6.2)\n\nsummary(res)\n\n\nCall:\nlm(formula = N_t ~ t, data = data_6.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-43.867 -23.599  -9.652  10.223 114.883 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   259.58      22.73  11.420 3.78e-08 ***\nt             -19.46       2.50  -7.786 3.01e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 41.83 on 13 degrees of freedom\nMultiple R-squared:  0.8234,    Adjusted R-squared:  0.8098 \nF-statistic: 60.62 on 1 and 13 DF,  p-value: 3.006e-06\n\n\n\n\n\n\nplot(data_6.2$t,rstandard(res),pch=19) # 표준화 잔차의 플랏\n\n\n\n# 잔차가 랜덤하게 잘 퍼져있어야 하는데 적절한 회귀모형이 아니라는 사실이 나옴\n\n\n\n\n\n\n\n\nplot(log(N_t)~t,data=data_6.2,pch=19)\n\n\n\n# 박테리아의 수에 로그를 취하니 선형성이 보인다 \n\nres&lt;-lm(log(N_t)~t,data=data_6.2)\n\nsummary(res)\n\n\nCall:\nlm(formula = log(N_t) ~ t, data = data_6.2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.18445 -0.06189  0.01253  0.05201  0.20021 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.973160   0.059778   99.92  &lt; 2e-16 ***\nt           -0.218425   0.006575  -33.22 5.86e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.11 on 13 degrees of freedom\nMultiple R-squared:  0.9884,    Adjusted R-squared:  0.9875 \nF-statistic:  1104 on 1 and 13 DF,  p-value: 5.86e-14\n\nplot(data_6.2$t,rstandard(res),pch=19)\n\n\n\n# 로그를 취해주니 잔차가 랜덤하게 이루어져 있음 \n\n# n_0에 대한 추론\n\nexp(5.973160)\n\n[1] 392.7448\n\nexp(coef(res)[1]) #로그를 취해주고 하는 부분이 잘 이해가 안됨 \n\n(Intercept) \n   392.7449 \n\nexp(coef(res)[1]-0.0588/2) #불편추정량 구하기 (참고용)\n\n(Intercept) \n   381.3663 \n\n\n\n\n\n\ndata_6.6&lt;-read.table(\"All_Data/p174.txt\",header=T,sep=\"\\t\")\n\ndata_6.6 #운항률과 사고 발생 건수에 대한 자료\n\n   Y      N\n1 11 0.0950\n2  7 0.1920\n3  7 0.0750\n4 19 0.2078\n5  9 0.1382\n6  4 0.0540\n7  3 0.1292\n8  1 0.0503\n9  3 0.0629\n\nplot(Y~N,data=data_6.6,pch=19) #잔차의 분산이 계속 커지는 효과를 보임 \n\n\n\nres_1&lt;-lm(Y~N,data=data_6.6)\n\nsummary(res_1)\n\n\nCall:\nlm(formula = Y ~ N, data = data_6.6)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.3351 -2.1281  0.1605  2.2670  5.6382 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  -0.1402     3.1412  -0.045   0.9657  \nN            64.9755    25.1959   2.579   0.0365 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.201 on 7 degrees of freedom\nMultiple R-squared:  0.4872,    Adjusted R-squared:  0.4139 \nF-statistic:  6.65 on 1 and 7 DF,  p-value: 0.03654\n\n# 표준화잔차 대 N의 플롯 / 그림 6.11\n\nplot(data_6.6$N,rstandard(res_1),pch=19) #등분산성에 만족하지 못하는 모형을 보인다 \n\n\n\n# N에 대한 sqrt(Y)의 회귀\n\n# N에 대한 Y의 회귀\n\nres_2&lt;-lm(sqrt(Y)~N,data=data_6.6)\n\nsummary(res_2)\n\n\nCall:\nlm(formula = sqrt(Y) ~ N, data = data_6.6)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.9690 -0.7655  0.1906  0.5874  1.0211 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   1.1692     0.5783   2.022   0.0829 .\nN            11.8564     4.6382   2.556   0.0378 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7733 on 7 degrees of freedom\nMultiple R-squared:  0.4828,    Adjusted R-squared:  0.4089 \nF-statistic: 6.535 on 1 and 7 DF,  p-value: 0.03776\n\n\n\n\n\n\nplot(data_6.6$N,rstandard(res_2),pch=19) #0을 중심으로 잔차가 보다 잘 퍼져있음이 보임 \n\n\n\n\n\n\n\n\ndata_6.9&lt;-read.table(\"All_Data/p176.txt\",header=T,sep=\"\\t\")\n\ndata_6.9\n\n      X   Y\n1   294  30\n2   247  32\n3   267  37\n4   358  44\n5   423  47\n6   311  49\n7   450  56\n8   534  62\n9   438  68\n10  697  78\n11  688  80\n12  630  84\n13  709  88\n14  627  97\n15  615 100\n16  999 109\n17 1022 114\n18 1015 117\n19  700 106\n20  850 128\n21  980 130\n22 1025 160\n23 1021  97\n24 1200 180\n25 1250 112\n26 1500 210\n27 1650 135\n\n# Y 대 X의 플로\n\nplot(Y~X,data=data_6.9,pch=19,main=\"그림 6.13\")\n\n\n\n# X에 대한 Y의 회귀\n\nres_1&lt;-lm(Y~X,data=data_6.9)\n\nsummary(res_1)\n\n\nCall:\nlm(formula = Y ~ X, data = data_6.9)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-53.294  -9.298  -5.579  14.394  39.119 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 14.44806    9.56201   1.511    0.143    \nX            0.10536    0.01133   9.303 1.35e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.73 on 25 degrees of freedom\nMultiple R-squared:  0.7759,    Adjusted R-squared:  0.7669 \nF-statistic: 86.54 on 1 and 25 DF,  p-value: 1.35e-09\n\n# 표준화잔차 대 X의 플롯\n\nplot(data_6.9$X, rstandard(res_1),pch=19,main = \"그림 6.14\")\n\n\n\n\n\n\n\n\n# 변환된 Y/X와 1/X를 적합한 회귀\n\ndata_6.9_1&lt;-data.frame(Y=data_6.9$Y/data_6.9$X,\n\n                       X=1/data_6.9$X)\n\nres_2&lt;-lm(Y~X,data=data_6.9_1)\n\nsummary(res_2) #지금은 변환하고 프라임 값들의 추정치가 나온것이기에 원래 회귀식으로 돌아가야함 \n\n\nCall:\nlm(formula = Y ~ X, data = data_6.9_1)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.041477 -0.013852 -0.004998  0.024671  0.035427 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.120990   0.008999  13.445 6.04e-13 ***\nX           3.803296   4.569745   0.832    0.413    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02266 on 25 degrees of freedom\nMultiple R-squared:  0.02696,   Adjusted R-squared:  -0.01196 \nF-statistic: 0.6927 on 1 and 25 DF,  p-value: 0.4131\n\n# 본래 변환시 X를 나눴기에 X를 곱해준다 -&gt; B_0,B_1의 추정값이 바뀐다 서로 \n\nplot(data_6.9_1$X, rstandard(res_2),pch=19,\n\n     xlab=\"1/X\",ylab=\"잔차\",main = \"[그림 6.15]\")\n\n\n\n\n\n\n\n\nwt&lt;-1/data_6.9$X^2 #가중값\n\nres_3&lt;-lm(Y~X,data=data_6.9,weights = wt)\n\nsummary(res_3) #결과는 위의 6.6과 동일한 값이 나옴 \n\n\nCall:\nlm(formula = Y ~ X, data = data_6.9, weights = wt)\n\nWeighted Residuals:\n      Min        1Q    Median        3Q       Max \n-0.041477 -0.013852 -0.004998  0.024671  0.035427 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 3.803296   4.569745   0.832    0.413    \nX           0.120990   0.008999  13.445 6.04e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02266 on 25 degrees of freedom\nMultiple R-squared:  0.8785,    Adjusted R-squared:  0.8737 \nF-statistic: 180.8 on 1 and 25 DF,  p-value: 6.044e-13\n\n\n\n\n\n\ndata_6.9&lt;-read.table(\"All_Data/p176.txt\",header=T,sep=\"\\t\")\n\nhead(data_6.9)\n\n    X  Y\n1 294 30\n2 247 32\n3 267 37\n4 358 44\n5 423 47\n6 311 49\n\n# log(Y) 대 X의 산점도\n\nplot((Y)~X,data=data_6.9,pch=19,main=\"식 6.9\")\n\n\n\nplot(log(Y)~X,data=data_6.9,pch=19,main=\"그림 6.16\")\n\n\n\nres_4&lt;-lm(log(Y)~X,data=data_6.9)\n\nsummary(res_4)\n\n\nCall:\nlm(formula = log(Y) ~ X, data = data_6.9)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.59648 -0.16578  0.00244  0.17481  0.34964 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 3.5150232  0.1110670  31.648  &lt; 2e-16 ***\nX           0.0012041  0.0001316   9.153 1.85e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2524 on 25 degrees of freedom\nMultiple R-squared:  0.7702,    Adjusted R-squared:  0.761 \nF-statistic: 83.77 on 1 and 25 DF,  p-value: 1.855e-09\n\n# X에 대한 log(Y)의 회귀로 부터 얻은 표준화잔차플롯\n\nplot(data_6.9$X,rstandard(res_4),pch=19,\n\n     xlab=\"X\",ylab=\"잔차\",main=\"그림 6.17\")\n\n\n\n# X와 X^2에 대한 log(Y)의 회귀\n\ndf&lt;-data.frame(log_Y=log(data_6.9$Y),\n\n               X=data_6.9$X,\n\n               X2=data_6.9$X^2)\n\nres_5&lt;-lm(log_Y~X+X2,data=df) #그러나 이러한 과정은 필요업고 아래방식으로..\n\nres_5&lt;-lm(log(Y)~X+I(X^2),data=data_6.9)\n\nsummary(res_5)\n\n\nCall:\nlm(formula = log(Y) ~ X + I(X^2), data = data_6.9)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.30589 -0.11705 -0.02707  0.17593  0.30657 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.852e+00  1.566e-01  18.205 1.50e-15 ***\nX            3.113e-03  3.989e-04   7.803 4.90e-08 ***\nI(X^2)      -1.102e-06  2.238e-07  -4.925 5.03e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1817 on 24 degrees of freedom\nMultiple R-squared:  0.8857,    Adjusted R-squared:  0.8762 \nF-statistic: 92.98 on 2 and 24 DF,  p-value: 4.976e-12\n\n# 표준화잔차 대 적합값 플롯 : X와 X^2에 대한 log(Y)의 회귀\n\nplot(fitted(res_5),rstandard(res_5),pch=19,\n\n     xlab=\"X\",ylab=\"잔차\",main=\"그림 6.18\")\n\n\n\n# 표준화잔차 대 X의 플롯 : X와 X^2에 대한 log(Y)의 회귀\n\nplot(data_6.9$X,rstandard(res_5),pch=19,\n\n     xlab=\"X\",ylab=\"잔차\",main=\"그림 6.19\")\n\n\n\n# 표준화잔차 대 X^2의 플롯 : X와 X^2에 대한 log(Y)의 회귀\n\nplot(data_6.9$X^2,rstandard(res_5),pch=19,\n\n     xlab=\"X\",ylab=\"잔차\",main=\"그림 6.20\") #2차항을 추가함으로서 더 잘 피팅됨 \n\n\n\n# 7장은 스킵 / 8장 스킵\n\n\n\n\n\ndata_9.1&lt;-read.table(\"All_Data/p236.txt\",header=T,sep=\"\\t\")\n\nhead(data_9.1)\n\n      ACHV      FAM     PEER   SCHOOL\n1 -0.43148  0.60814  0.03509  0.16607\n2  0.79969  0.79369  0.47924  0.53356\n3 -0.92467 -0.82630 -0.61951 -0.78635\n4 -2.19081 -1.25310 -1.21675 -1.04076\n5 -2.84818  0.17399 -0.18517  0.14229\n6 -0.66233  0.20246  0.12764  0.27311\n\nres&lt;-lm(ACHV~.,data_9.1)\n\nsummary(res)\n\n\nCall:\nlm(formula = ACHV ~ ., data = data_9.1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.2096 -1.3934 -0.2947  1.1415  4.5881 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -0.06996    0.25064  -0.279    0.781\nFAM          1.10126    1.41056   0.781    0.438\nPEER         2.32206    1.48129   1.568    0.122\nSCHOOL      -2.28100    2.22045  -1.027    0.308\n\nResidual standard error: 2.07 on 66 degrees of freedom\nMultiple R-squared:  0.2063,    Adjusted R-squared:  0.1702 \nF-statistic: 5.717 on 3 and 66 DF,  p-value: 0.001535\n\n#F-statistic: 5.717 on 3 and 66 DF, p-value: 0.001535\n\n#이는 의미있는 beta가 존재한다라는 의미\n\n#그러나 각 계수들의 회귀계수를 보니 모두 0이라는 결과가 나옴 이는 다중공선성이다\n\n# Correlation panel\n\npanel.cor&lt;-function(x,y){\n\npar(usr=c(0,1,0,1))\n\nr&lt;-round(cor(x,y),digits = 3)\n\ntext(0.5,0.5,r,cex=1.5)\n\n}\n\npairs(data_9.1[-1],lower.panel = panel.cor)\n\n\n\n\n\n\n\n\nplot(fitted(res),rstandard(res),pch=19,\n\nxlab=\"예측값\",ylab=\"잔차\",main=\"[그림 9.1]\")\n\n\n\n#이는 회귀모형에 동시에 들어가면 안된다라는 의미 (제거가 필요)\n\n\n\n\n\ndata_9.5&lt;-read.table(\"All_Data/p241.txt\",header=T,sep=\"\\t\")\n\nhead(data_9.5)\n\n  YEAR IMPORT DOPROD STOCK CONSUM\n1   49   15.9  149.3   4.2  108.1\n2   50   16.4  161.2   4.1  114.8\n3   51   19.0  171.5   3.1  123.2\n4   52   19.1  175.5   3.1  126.9\n5   53   18.8  180.8   1.1  132.1\n6   54   20.4  190.7   2.2  137.7\n\n### 산점도\n\npairs(data_9.5[-1],lower.panel = panel.cor)\n\n\n\n# 회귀분석(1) : 데이터 1949~1966\n\nres&lt;-lm(IMPORT~.,data_9.5[-1])\n\nsummary(res) #다중공선성이 존재한다고 예측을 일단함\n\n\nCall:\nlm(formula = IMPORT ~ ., data = data_9.5[-1])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7208 -1.8354 -0.3479  1.2973  4.1008 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -19.7251     4.1253  -4.782 0.000293 ***\nDOPROD        0.0322     0.1869   0.172 0.865650    \nSTOCK         0.4142     0.3223   1.285 0.219545    \nCONSUM        0.2427     0.2854   0.851 0.409268    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.258 on 14 degrees of freedom\nMultiple R-squared:  0.973, Adjusted R-squared:  0.9673 \nF-statistic: 168.4 on 3 and 14 DF,  p-value: 3.212e-11\n\n\n\n\n\n\nplot(1:nrow(data_9.5),rstandard(res),\n\npch=19,type=\"b\",\n\nxla=\"번호\",ylab=\"잔차\",main=\"[그림 9.3]\")\n\n\n\n# 회귀분석(2) : 데이터 1949~1959\n\ndata_use&lt;-subset(data_9.5,YEAR&lt;=59)\n\nres&lt;-lm(IMPORT~.,data_use[-1])\n\nsummary(res)\n\n\nCall:\nlm(formula = IMPORT ~ ., data = data_use[-1])\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.52367 -0.38953  0.05424  0.22644  0.78313 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -10.12799    1.21216  -8.355  6.9e-05 ***\nDOPROD       -0.05140    0.07028  -0.731 0.488344    \nSTOCK         0.58695    0.09462   6.203 0.000444 ***\nCONSUM        0.28685    0.10221   2.807 0.026277 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4889 on 7 degrees of freedom\nMultiple R-squared:  0.9919,    Adjusted R-squared:  0.9884 \nF-statistic: 285.6 on 3 and 7 DF,  p-value: 1.112e-07\n\n\n\n\n\n\nplot(1:nrow(data_use),rstandard(res),\n\npch=19,type=\"b\",\n\nxla=\"번호\",ylab=\"잔차\",main=\"[그림 9.4]\")\n\n\n\n\n\n\n\n\n# 분산확대인자\n\nlibrary(olsrr)\n\n# 교육기회 균등(EEO)\n\nres_9.1&lt;-lm(ACHV~.,data_9.1)\n\nsummary(res_9.1)\n\n\nCall:\nlm(formula = ACHV ~ ., data = data_9.1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.2096 -1.3934 -0.2947  1.1415  4.5881 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -0.06996    0.25064  -0.279    0.781\nFAM          1.10126    1.41056   0.781    0.438\nPEER         2.32206    1.48129   1.568    0.122\nSCHOOL      -2.28100    2.22045  -1.027    0.308\n\nResidual standard error: 2.07 on 66 degrees of freedom\nMultiple R-squared:  0.2063,    Adjusted R-squared:  0.1702 \nF-statistic: 5.717 on 3 and 66 DF,  p-value: 0.001535\n\n#car::vif(res_9.1)\n\nolsrr::ols_vif_tol(res_9.1) # 10보다 크면 유의한 영향을 준다 제거필요?\n\n  Variables  Tolerance      VIF\n1       FAM 0.02660945 37.58064\n2      PEER 0.03309981 30.21166\n3    SCHOOL 0.01202567 83.15544\n\n\n\n\n\n\ndata_use&lt;-subset(data_9.5,YEAR&lt;=59)\n\nres_9.5&lt;-lm(IMPORT~.,data_use[-1])\n\n#car::vif(res_9.1)\n\nolsrr::ols_vif_tol(res_9.5) #작으면 새로운 변수를 만들거나 제거를 통해서 진행\n\n  Variables   Tolerance        VIF\n1    DOPROD 0.005376417 185.997470\n2     STOCK 0.981441657   1.018909\n3    CONSUM 0.005373166 186.110015\n\n# 상태 지수\n\ncnd.idx&lt;-olsrr::ols_eigen_cindex(res_9.1)\n\nround(cnd.idx,4)\n\n  Eigenvalue Condition Index intercept    FAM   PEER SCHOOL\n1     2.9547          1.0000    0.0005 0.0030 0.0037 0.0014\n2     0.9974          1.7211    0.9756 0.0000 0.0000 0.0000\n3     0.0400          8.5996    0.0004 0.3068 0.4428 0.0008\n4     0.0079         19.2826    0.0235 0.6903 0.5535 0.9978\n\ncnd.idx&lt;-olsrr::ols_eigen_cindex(res_9.5)\n\nround(cnd.idx,4)\n\n  Eigenvalue Condition Index intercept DOPROD  STOCK CONSUM\n1     3.8384          1.0000    0.0010 0.0000 0.0109 0.0000\n2     0.1484          5.0863    0.0053 0.0001 0.9385 0.0001\n3     0.0132         17.0732    0.7743 0.0015 0.0330 0.0011\n4     0.0001        265.4613    0.2193 0.9984 0.0175 0.9989\n\n\n\n\n\n\n\ndata_9.5&lt;-read.table(\"All_Data/p241.txt\",header=T,sep=\"\\t\")\n\nhead(data_9.5)\n\n  YEAR IMPORT DOPROD STOCK CONSUM\n1   49   15.9  149.3   4.2  108.1\n2   50   16.4  161.2   4.1  114.8\n3   51   19.0  171.5   3.1  123.2\n4   52   19.1  175.5   3.1  126.9\n5   53   18.8  180.8   1.1  132.1\n6   54   20.4  190.7   2.2  137.7\n\n# 회귀분석(2) : 데이터 1949~1959\n\ndata_use&lt;-subset(data_9.5,YEAR&lt;=59)\n\nres&lt;-lm(IMPORT~.,data_use[-1]) #1열 제거 \n\nsummary(res)\n\n\nCall:\nlm(formula = IMPORT ~ ., data = data_use[-1])\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.52367 -0.38953  0.05424  0.22644  0.78313 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -10.12799    1.21216  -8.355  6.9e-05 ***\nDOPROD       -0.05140    0.07028  -0.731 0.488344    \nSTOCK         0.58695    0.09462   6.203 0.000444 ***\nCONSUM        0.28685    0.10221   2.807 0.026277 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4889 on 7 degrees of freedom\nMultiple R-squared:  0.9919,    Adjusted R-squared:  0.9884 \nF-statistic: 285.6 on 3 and 7 DF,  p-value: 1.112e-07\n\nhead(data_use[-c(1:2)])\n\n  DOPROD STOCK CONSUM\n1  149.3   4.2  108.1\n2  161.2   4.1  114.8\n3  171.5   3.1  123.2\n4  175.5   3.1  126.9\n5  180.8   1.1  132.1\n6  190.7   2.2  137.7\n\n### 주성분(principle component)\n\npc&lt;-prcomp(data_use[-c(1:2)],scale.=T)\n\npc$rotation\n\n              PC1         PC2          PC3\nDOPROD 0.70633041 -0.03568867 -0.706982083\nSTOCK  0.04350059  0.99902908 -0.006970795\nCONSUM 0.70654444 -0.02583046  0.707197102\n\npc$x #변화된 새로운 변수가 저장된곳\n\n          PC1         PC2         PC3\n1  -2.1258872  0.63865815 -0.02072230\n2  -1.6189273  0.55553922 -0.07111317\n3  -1.1151675 -0.07297970 -0.02173008\n4  -0.8942966 -0.08236998  0.01081318\n5  -0.6442081 -1.30668523  0.07258248\n6  -0.1903514 -0.65914745  0.02655252\n7   0.3596219 -0.74367447  0.04278124\n8   0.9718018  1.35405877  0.06286252\n9   1.5593159  0.96404558  0.02357446\n10  1.7669951  1.01521706 -0.04498818\n11  1.9311034 -1.66266195 -0.08061267\n\n### 주성분회귀 - 원래데이터를 주성분분석을 통해 새로운 데이터를 생성 이를 가지고 회귀 \n\ndf&lt;-data.frame(IMPORT = scale(data_use$IMPORT),\n\n               pc$x)\n\ndf\n\n       IMPORT        PC1         PC2         PC3\n1  -1.3185185 -2.1258872  0.63865815 -0.02072230\n2  -1.2084753 -1.6189273  0.55553922 -0.07111317\n3  -0.6362502 -1.1151675 -0.07297970 -0.02173008\n4  -0.6142416 -0.8942966 -0.08236998  0.01081318\n5  -0.6802675 -0.6442081 -1.30668523  0.07258248\n6  -0.3281290 -0.1903514 -0.65914745  0.02655252\n7   0.1780700  0.3596219 -0.74367447  0.04278124\n8   1.0143989  0.9718018  1.35405877  0.06286252\n9   1.3665374  1.5593159  0.96404558  0.02357446\n10  1.2564942  1.7669951  1.01521706 -0.04498818\n11  0.9703816  1.9311034 -1.66266195 -0.08061267\n\nres&lt;-lm(IMPORT~.,data=df)\n\nsummary(res) #유의한 1,2번째 계수들만 사용하고 3번째 것은 없이 해도 되겠다..\n\n\nCall:\nlm(formula = IMPORT ~ ., data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.11525 -0.08573  0.01194  0.04984  0.17236 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 8.900e-16  3.244e-02   0.000 1.000000    \nPC1         6.900e-01  2.406e-02  28.673 1.61e-08 ***\nPC2         1.913e-01  3.406e-02   5.617 0.000801 ***\nPC3         1.160e+00  6.559e-01   1.768 0.120376    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1076 on 7 degrees of freedom\nMultiple R-squared:  0.9919,    Adjusted R-squared:  0.9884 \nF-statistic: 285.6 on 3 and 7 DF,  p-value: 1.112e-07\n\n#중간고사 이후의것이 주가나오겠지만 앞에것도 알아야함 연습문제에서 나올것 예상\n\n\n\n\n\n# 11.10 - \n\ndata_3.3&lt;-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\nhead(data_3.3)\n\n   Y X1 X2 X3 X4 X5 X6\n1 43 51 30 39 61 92 45\n2 63 64 51 54 63 73 47\n3 71 70 68 69 76 86 48\n4 61 63 45 47 54 84 35\n5 81 78 56 66 71 83 47\n6 43 55 49 44 54 49 34\n\n# 분산확대 인자(VIF) : 10초과 &gt; 심각한 공산성의 문제가 있음\n\nres&lt;-lm(Y~.,data=data_3.3)\n\nsummary(res)\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.9418  -4.3555   0.3158   5.5425  11.5990 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10.78708   11.58926   0.931 0.361634    \nX1           0.61319    0.16098   3.809 0.000903 ***\nX2          -0.07305    0.13572  -0.538 0.595594    \nX3           0.32033    0.16852   1.901 0.069925 .  \nX4           0.08173    0.22148   0.369 0.715480    \nX5           0.03838    0.14700   0.261 0.796334    \nX6          -0.21706    0.17821  -1.218 0.235577    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.068 on 23 degrees of freedom\nMultiple R-squared:  0.7326,    Adjusted R-squared:  0.6628 \nF-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05\n\nolsrr::ols_vif_tol(res) #10을 넘는 것이 없기에 공산성의 문제는 없다 \n\n  Variables Tolerance      VIF\n1        X1 0.3749447 2.667060\n2        X2 0.6246520 1.600891\n3        X3 0.4403263 2.271043\n4        X4 0.3248624 3.078226\n5        X5 0.8142600 1.228109\n6        X6 0.5124025 1.951591\n\n# 수정결정계수\n\nres_summ&lt;-summary(res)\n\nres_summ$adj.r.squared #0.662846\n\n[1] 0.662846\n\n# Mallow's Cp - 작을수록 좋음\n\n# 축소모형\n\nres_subset&lt;-lm(Y~X1+X3,data=data_3.3)\n\nolsrr::ols_mallows_cp(res_subset,res) \n\n[1] 1.114811\n\n# AIC - 참고만\n\nolsrr::ols_aic(res_subset,method = \"SAS\")\n\n[1] 118.0024\n\n# BIC\n\nolsrr::ols_sbic(res_subset,res)\n\n[1] 121.0938\n\n### 전진적 선택법 - AIC\n\nres_step&lt;-step(res,direction = \"forward\")\n\nStart:  AIC=123.36\nY ~ X1 + X2 + X3 + X4 + X5 + X6\n\n### 후진적 제거법 -AIC\n\nres_step&lt;-step(res,direction = \"backward\")\n\nStart:  AIC=123.36\nY ~ X1 + X2 + X3 + X4 + X5 + X6\n\n       Df Sum of Sq    RSS    AIC\n- X5    1      3.41 1152.4 121.45\n- X4    1      6.80 1155.8 121.54\n- X2    1     14.47 1163.5 121.74\n- X6    1     74.11 1223.1 123.24\n&lt;none&gt;              1149.0 123.36\n- X3    1    180.50 1329.5 125.74\n- X1    1    724.80 1873.8 136.04\n\nStep:  AIC=121.45\nY ~ X1 + X2 + X3 + X4 + X6\n\n       Df Sum of Sq    RSS    AIC\n- X4    1     10.61 1163.0 119.73\n- X2    1     14.16 1166.6 119.82\n- X6    1     71.27 1223.7 121.25\n&lt;none&gt;              1152.4 121.45\n- X3    1    177.74 1330.1 123.75\n- X1    1    724.70 1877.1 134.09\n\nStep:  AIC=119.73\nY ~ X1 + X2 + X3 + X6\n\n       Df Sum of Sq    RSS    AIC\n- X2    1     16.10 1179.1 118.14\n- X6    1     61.60 1224.6 119.28\n&lt;none&gt;              1163.0 119.73\n- X3    1    197.03 1360.0 122.42\n- X1    1   1165.94 2328.9 138.56\n\nStep:  AIC=118.14\nY ~ X1 + X3 + X6\n\n       Df Sum of Sq    RSS    AIC\n- X6    1     75.54 1254.7 118.00\n&lt;none&gt;              1179.1 118.14\n- X3    1    186.12 1365.2 120.54\n- X1    1   1259.91 2439.0 137.94\n\nStep:  AIC=118\nY ~ X1 + X3\n\n       Df Sum of Sq    RSS    AIC\n&lt;none&gt;              1254.7 118.00\n- X3    1    114.73 1369.4 118.63\n- X1    1   1370.91 2625.6 138.16\n\n### 단계적 방법\n\nres_step&lt;-step(res) #최종적으로는 X1+X3이 선택됨 이거 사용하는 것이 좋을듯 \n\nStart:  AIC=123.36\nY ~ X1 + X2 + X3 + X4 + X5 + X6\n\n       Df Sum of Sq    RSS    AIC\n- X5    1      3.41 1152.4 121.45\n- X4    1      6.80 1155.8 121.54\n- X2    1     14.47 1163.5 121.74\n- X6    1     74.11 1223.1 123.24\n&lt;none&gt;              1149.0 123.36\n- X3    1    180.50 1329.5 125.74\n- X1    1    724.80 1873.8 136.04\n\nStep:  AIC=121.45\nY ~ X1 + X2 + X3 + X4 + X6\n\n       Df Sum of Sq    RSS    AIC\n- X4    1     10.61 1163.0 119.73\n- X2    1     14.16 1166.6 119.82\n- X6    1     71.27 1223.7 121.25\n&lt;none&gt;              1152.4 121.45\n- X3    1    177.74 1330.1 123.75\n- X1    1    724.70 1877.1 134.09\n\nStep:  AIC=119.73\nY ~ X1 + X2 + X3 + X6\n\n       Df Sum of Sq    RSS    AIC\n- X2    1     16.10 1179.1 118.14\n- X6    1     61.60 1224.6 119.28\n&lt;none&gt;              1163.0 119.73\n- X3    1    197.03 1360.0 122.42\n- X1    1   1165.94 2328.9 138.56\n\nStep:  AIC=118.14\nY ~ X1 + X3 + X6\n\n       Df Sum of Sq    RSS    AIC\n- X6    1     75.54 1254.7 118.00\n&lt;none&gt;              1179.1 118.14\n- X3    1    186.12 1365.2 120.54\n- X1    1   1259.91 2439.0 137.94\n\nStep:  AIC=118\nY ~ X1 + X3\n\n       Df Sum of Sq    RSS    AIC\n&lt;none&gt;              1254.7 118.00\n- X3    1    114.73 1369.4 118.63\n- X1    1   1370.91 2625.6 138.16\n\nsummary(res_step) #X3가 의미 없다고 나와도 아님 검증된것임\n\n\nCall:\nlm(formula = Y ~ X1 + X3, data = data_3.3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.5568  -5.7331   0.6701   6.5341  10.3610 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   9.8709     7.0612   1.398    0.174    \nX1            0.6435     0.1185   5.432 9.57e-06 ***\nX3            0.2112     0.1344   1.571    0.128    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.817 on 27 degrees of freedom\nMultiple R-squared:  0.708, Adjusted R-squared:  0.6864 \nF-statistic: 32.74 on 2 and 27 DF,  p-value: 6.058e-08\n\n\n\n\n\n\n\n\ndata_use&lt;-read.table(\"All_Data/p329.txt\",header=T,sep=\"\\t\")\n\nhead(data_use)\n\n     X1 X2    X3    X4 X5 X6 X7 X8 X9    Y\n1 4.918  1 3.472 0.998  1  7  4 42  0 25.9\n2 5.021  1 3.531 1.500  2  7  4 62  0 29.5\n3 4.543  1 2.275 1.175  1  6  3 40  0 27.9\n4 4.557  1 4.050 1.232  1  6  3 54  0 25.9\n5 5.060  1 4.455 1.121  1  6  3 42  0 29.9\n6 3.891  1 4.455 0.988  1  6  3 56  0 29.9\n\n### 데이터 탐색 - 자료형\n\nsapply(data_use,class)\n\n       X1        X2        X3        X4        X5        X6        X7        X8 \n\"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \n       X9         Y \n\"numeric\" \"numeric\" \n\n### 데이터 탐색 - 기초 통계량\n\nsummary(data_use) #결측값의 여부를 확인\n\n       X1              X2              X3              X4       \n Min.   :3.891   Min.   :1.000   Min.   :2.275   Min.   :0.975  \n 1st Qu.:5.058   1st Qu.:1.000   1st Qu.:4.855   1st Qu.:1.161  \n Median :5.974   Median :1.000   Median :5.685   Median :1.432  \n Mean   :6.405   Mean   :1.167   Mean   :6.033   Mean   :1.384  \n 3rd Qu.:7.873   3rd Qu.:1.500   3rd Qu.:7.158   3rd Qu.:1.577  \n Max.   :9.142   Max.   :1.500   Max.   :9.890   Max.   :1.831  \n       X5              X6            X7              X8              X9      \n Min.   :0.000   Min.   :5.0   Min.   :2.000   Min.   : 3.00   Min.   :0.00  \n 1st Qu.:1.000   1st Qu.:6.0   1st Qu.:3.000   1st Qu.:30.00   1st Qu.:0.00  \n Median :1.000   Median :6.0   Median :3.000   Median :40.00   Median :0.00  \n Mean   :1.312   Mean   :6.5   Mean   :3.167   Mean   :37.46   Mean   :0.25  \n 3rd Qu.:2.000   3rd Qu.:7.0   3rd Qu.:3.250   3rd Qu.:48.50   3rd Qu.:0.25  \n Max.   :2.000   Max.   :8.0   Max.   :4.000   Max.   :62.00   Max.   :1.00  \n       Y        \n Min.   :25.90  \n 1st Qu.:29.90  \n Median :33.70  \n Mean   :34.63  \n 3rd Qu.:38.15  \n Max.   :45.80  \n\n### 데이터 탐색 - 히스토그램 & 상자그림\n\nhist(data_use$Y,main=\"histogram of data_use$Y\")\n\n\n\nboxplot(data_use$Y)\n\n\n\n### 데이터 탐색 - 산점도 행렬 & 상관계수\n\nplot(data_use)\n\ncor(data_use) \n\n           X1         X2         X3         X4          X5        X6        X7\nX1  1.0000000  0.6512669  0.6892117  0.7342737  0.45855650 0.6406157 0.3671126\nX2  0.6512669  1.0000000  0.4129558  0.7285916  0.22402204 0.5103104 0.4264014\nX3  0.6892117  0.4129558  1.0000000  0.5715520  0.20466375 0.3921244 0.1516093\nX4  0.7342737  0.7285916  0.5715520  1.0000000  0.35888351 0.6788606 0.5743353\nX5  0.4585565  0.2240220  0.2046638  0.3588835  1.00000000 0.5893871 0.5412988\nX6  0.6406157  0.5103104  0.3921244  0.6788606  0.58938707 1.0000000 0.8703883\nX7  0.3671126  0.4264014  0.1516093  0.5743353  0.54129880 0.8703883 1.0000000\nX8 -0.4371012 -0.1007485 -0.3527514 -0.1390869 -0.02016883 0.1242663 0.3135114\nX9  0.1466825  0.2041241  0.3059946  0.1065612  0.10161846 0.2222222 0.0000000\nY   0.8739117  0.7097771  0.6476364  0.7077656  0.46146792 0.5284436 0.2815200\n            X8        X9          Y\nX1 -0.43710116 0.1466825  0.8739117\nX2 -0.10074847 0.2041241  0.7097771\nX3 -0.35275139 0.3059946  0.6476364\nX4 -0.13908686 0.1065612  0.7077656\nX5 -0.02016883 0.1016185  0.4614679\nX6  0.12426629 0.2222222  0.5284436\nX7  0.31351144 0.0000000  0.2815200\nX8  1.00000000 0.2257796 -0.3974034\nX9  0.22577960 1.0000000  0.2668783\nY  -0.39740338 0.2668783  1.0000000\n\npairs(data_use)\n\n\n\npanel.cor&lt;-function(x,y){\n\n  par(usr=c(0,1,0,1))\n\n  r&lt;-round(cor(x,y),digits = 3)\n\n  text(0.5,0.5,r,cex=1.5)\n\n}\n\npairs(data_use,lower.panel = panel.cor)\n\n\n\n### (a) 모든 변수들이 모형에 포함시킬 것인가?\n\ncor(data_use)\n\n           X1         X2         X3         X4          X5        X6        X7\nX1  1.0000000  0.6512669  0.6892117  0.7342737  0.45855650 0.6406157 0.3671126\nX2  0.6512669  1.0000000  0.4129558  0.7285916  0.22402204 0.5103104 0.4264014\nX3  0.6892117  0.4129558  1.0000000  0.5715520  0.20466375 0.3921244 0.1516093\nX4  0.7342737  0.7285916  0.5715520  1.0000000  0.35888351 0.6788606 0.5743353\nX5  0.4585565  0.2240220  0.2046638  0.3588835  1.00000000 0.5893871 0.5412988\nX6  0.6406157  0.5103104  0.3921244  0.6788606  0.58938707 1.0000000 0.8703883\nX7  0.3671126  0.4264014  0.1516093  0.5743353  0.54129880 0.8703883 1.0000000\nX8 -0.4371012 -0.1007485 -0.3527514 -0.1390869 -0.02016883 0.1242663 0.3135114\nX9  0.1466825  0.2041241  0.3059946  0.1065612  0.10161846 0.2222222 0.0000000\nY   0.8739117  0.7097771  0.6476364  0.7077656  0.46146792 0.5284436 0.2815200\n            X8        X9          Y\nX1 -0.43710116 0.1466825  0.8739117\nX2 -0.10074847 0.2041241  0.7097771\nX3 -0.35275139 0.3059946  0.6476364\nX4 -0.13908686 0.1065612  0.7077656\nX5 -0.02016883 0.1016185  0.4614679\nX6  0.12426629 0.2222222  0.5284436\nX7  0.31351144 0.0000000  0.2815200\nX8  1.00000000 0.2257796 -0.3974034\nX9  0.22577960 1.0000000  0.2668783\nY  -0.39740338 0.2668783  1.0000000\n\nmodel_1&lt;-lm(data_use$Y~.,data_use)\n\nsummary(model_1) #전형적인 다중공선성이 보인다 \n\n\nCall:\nlm(formula = data_use$Y ~ ., data = data_use)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7729 -1.9801 -0.0868  1.6615  4.2618 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) 15.31044    5.96093   2.568   0.0223 *\nX1           1.95413    1.03833   1.882   0.0808 .\nX2           6.84552    4.33529   1.579   0.1367  \nX3           0.13761    0.49436   0.278   0.7848  \nX4           2.78143    4.39482   0.633   0.5370  \nX5           2.05076    1.38457   1.481   0.1607  \nX6          -0.55590    2.39791  -0.232   0.8200  \nX7          -1.24516    3.42293  -0.364   0.7215  \nX8          -0.03800    0.06726  -0.565   0.5810  \nX9           1.70446    1.95317   0.873   0.3976  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.973 on 14 degrees of freedom\nMultiple R-squared:  0.8512,    Adjusted R-squared:  0.7555 \nF-statistic: 8.898 on 9 and 14 DF,  p-value: 0.0002015\n\n#coef가 X6 0.820이므로 확인 필요\n\nolsrr::ols_vif_tol(model_1) #X6,X7이 10에 가깝기에 제거해야하는 대상중 최우선 \n\n  Variables  Tolerance       VIF\n1        X1 0.14241176  7.021892\n2        X2 0.35267392  2.835480\n3        X3 0.40735454  2.454864\n4        X4 0.26066568  3.836332\n5        X5 0.54842184  1.823414\n6        X6 0.08539078 11.710866\n7        X7 0.10286111  9.721847\n8        X8 0.43083904  2.321052\n9        X9 0.51482060  1.942424\n\n#다중공선성이 보이기에 모든 변수를 모형에 포함 시킬수는 없다 \n\n#10보다 크면 심각한 공선성이 존재 \n\n### (b) 지방세(X1) 방의 수(X6), 건물의 나이(X8)가 판매가격(Y)을 \n\n#       설명하는데 적절하다는 의견에 동의 하는가?\n\nmodel_2&lt;-lm(Y~X1+X6+X8,data=data_use)\n\nsummary(model_2)\n\n\nCall:\nlm(formula = Y ~ X1 + X6 + X8, data = data_use)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7486 -2.4082 -0.3594  2.1378  6.5353 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 14.796013   4.971105   2.976 0.007462 ** \nX1           3.489464   0.729368   4.784 0.000113 ***\nX6          -0.415515   1.182262  -0.351 0.728921    \nX8           0.004923   0.063597   0.077 0.939062    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.123 on 20 degrees of freedom\nMultiple R-squared:  0.7655,    Adjusted R-squared:  0.7303 \nF-statistic: 21.76 on 3 and 20 DF,  p-value: 1.653e-06\n\n# VIF\n\nolsrr::ols_vif_tol(model_2)\n\n  Variables Tolerance      VIF\n1        X1 0.3184367 3.140342\n2        X6 0.3875669 2.580200\n3        X8 0.5317389 1.880622\n\n# 회귀진단 그래프들\n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(model_2)\n\n\n\nlayout(1)\n\n# Cook의 거리 - 1을 넘으면 확인을 해야한다 \n\nolsrr::ols_plot_cooksd_chart(model_2) \n\n\n\n#동의는 할 수 있으나(적절하지만) 최고의 모형이라는데는 동의 못함\n\n### (c) 지방세 X1가 단독으로 판매가격 Y을 설명하는데 적절하다는 의견에 동의?\n\nmodel_3&lt;-lm(Y~X1,data=data_use)\n\nsummary(model_3) #adj-rsquared는 변수를 선택할때 사용 \n\n\nCall:\nlm(formula = Y ~ X1, data = data_use)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8445 -2.3340 -0.3841  1.9689  6.3005 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  13.3553     2.5955   5.146 3.71e-05 ***\nX1            3.3215     0.3939   8.433 2.44e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.988 on 22 degrees of freedom\nMultiple R-squared:  0.7637,    Adjusted R-squared:  0.753 \nF-statistic: 71.11 on 1 and 22 DF,  p-value: 2.435e-08\n\n# 회귀진단 그래프들\n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(model_3)\n\n\n\nlayout(1)\n\n# Cook의 거리 - 1을 넘으면 확인을 해야한다 \n\nolsrr::ols_plot_cooksd_chart(model_3) \n\n\n\n### 적절한 모형 제시\n\ndata_use_2&lt;-data_use[-6]\n\nmodel_4&lt;-lm(Y~.,data_use_2)\n\nsummary(model_4)\n\n\nCall:\nlm(formula = Y ~ ., data = data_use_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7340 -1.8513 -0.0154  1.5472  4.2113 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) 14.40540    4.36026   3.304  0.00482 **\nX1           1.81642    0.82431   2.204  0.04360 * \nX2           7.13892    4.01353   1.779  0.09556 . \nX3           0.14721    0.47683   0.309  0.76177   \nX4           2.73339    4.24921   0.643  0.52976   \nX5           2.06520    1.33883   1.543  0.14377   \nX7          -1.91236    1.79355  -1.066  0.30318   \nX8          -0.03832    0.06509  -0.589  0.56481   \nX9           1.48746    1.65931   0.896  0.38418   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.878 on 15 degrees of freedom\nMultiple R-squared:  0.8506,    Adjusted R-squared:  0.771 \nF-statistic: 10.68 on 8 and 15 DF,  p-value: 5.936e-05\n\n# VIF\n\nolsrr::ols_vif_tol(model_4)\n\n  Variables Tolerance      VIF\n1        X1 0.2117072 4.723504\n2        X2 0.3855308 2.593827\n3        X3 0.4102334 2.437637\n4        X4 0.2612467 3.827800\n5        X5 0.5495336 1.819725\n6        X7 0.3510102 2.848920\n7        X8 0.4310175 2.320091\n8        X9 0.6683173 1.496295\n\n# 회귀진단 그래프들\n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(model_4)\n\n\n\nlayout(1)\n\n# Cook의 거리 - 1을 넘으면 확인을 해야한다 \n\nolsrr::ols_plot_cooksd_chart(model_4) \n\n\n\n### 단계적 선택법 - AIC\n\nmodel_5&lt;-step(model_4)\n\nStart:  AIC=57.45\nY ~ X1 + X2 + X3 + X4 + X5 + X7 + X8 + X9\n\n       Df Sum of Sq    RSS    AIC\n- X3    1     0.789 125.00 55.605\n- X8    1     2.870 127.08 56.001\n- X4    1     3.426 127.63 56.106\n- X9    1     6.654 130.86 56.706\n- X7    1     9.414 133.62 57.207\n&lt;none&gt;              124.21 57.453\n- X5    1    19.702 143.91 58.987\n- X2    1    26.198 150.40 60.046\n- X1    1    40.207 164.41 62.184\n\nStep:  AIC=55.61\nY ~ X1 + X2 + X4 + X5 + X7 + X8 + X9\n\n       Df Sum of Sq    RSS    AIC\n- X8    1     3.369 128.36 54.243\n- X4    1     4.816 129.81 54.513\n- X9    1     9.581 134.58 55.378\n- X7    1     9.655 134.65 55.391\n&lt;none&gt;              125.00 55.605\n- X5    1    18.957 143.95 56.994\n- X2    1    25.474 150.47 58.057\n- X1    1    53.245 178.24 62.122\n\nStep:  AIC=54.24\nY ~ X1 + X2 + X4 + X5 + X7 + X9\n\n       Df Sum of Sq    RSS    AIC\n- X4    1     5.011 133.38 53.163\n- X9    1     6.543 134.91 53.437\n&lt;none&gt;              128.36 54.243\n- X5    1    20.033 148.40 55.724\n- X7    1    22.819 151.18 56.170\n- X2    1    24.299 152.66 56.404\n- X1    1    95.134 223.50 65.552\n\nStep:  AIC=53.16\nY ~ X1 + X2 + X5 + X7 + X9\n\n       Df Sum of Sq    RSS    AIC\n- X9    1     6.223 139.60 52.257\n&lt;none&gt;              133.38 53.163\n- X5    1    17.801 151.18 54.169\n- X7    1    17.873 151.25 54.181\n- X2    1    39.335 172.71 57.365\n- X1    1   157.972 291.35 69.915\n\nStep:  AIC=52.26\nY ~ X1 + X2 + X5 + X7\n\n       Df Sum of Sq    RSS    AIC\n&lt;none&gt;              139.60 52.257\n- X5    1    20.836 160.43 53.596\n- X7    1    21.669 161.27 53.720\n- X2    1    47.409 187.01 57.274\n- X1    1   156.606 296.20 68.312\n\nsummary(model_5)\n\n\nCall:\nlm(formula = Y ~ X1 + X2 + X5 + X7, data = data_use_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5605 -2.0856  0.0238  1.8580  3.8981 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  13.6212     3.6725   3.709 0.001489 ** \nX1            2.4123     0.5225   4.617 0.000188 ***\nX2            8.4589     3.3300   2.540 0.019970 *  \nX5            2.0604     1.2235   1.684 0.108541    \nX7           -2.2154     1.2901  -1.717 0.102176    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.711 on 19 degrees of freedom\nMultiple R-squared:  0.8321,    Adjusted R-squared:  0.7968 \nF-statistic: 23.54 on 4 and 19 DF,  p-value: 3.866e-07\n\n# 회귀진단 그래프들\n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(model_5)\n\n\n\nlayout(1)\n\n# Cook의 거리 - 0.5보다 작으면 괜찮음 / 1보다 큰 값을 고려 / 전체적으로 봤을때 튀는 값이 있는 경우 \n\nolsrr::ols_plot_cooksd_chart(model_5) \n\n\n\n### 17번 제거\n\ndata_use_3&lt;-model_5$model[-17,]\n\nmodel_6&lt;-lm(Y~.,data_use_3)\n\nsummary(model_6)\n\n\nCall:\nlm(formula = Y ~ ., data = data_use_3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.4577 -1.6655  0.1575  1.7978  4.1865 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  13.2263     3.4659   3.816  0.00127 **\nX1            1.7014     0.6247   2.723  0.01394 * \nX2           12.0705     3.6958   3.266  0.00429 **\nX5            2.4602     1.1726   2.098  0.05028 . \nX7           -2.2310     1.2152  -1.836  0.08294 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.553 on 18 degrees of freedom\nMultiple R-squared:  0.8418,    Adjusted R-squared:  0.8067 \nF-statistic: 23.95 on 4 and 18 DF,  p-value: 5.316e-07\n\n# VIF\n\nolsrr::ols_vif_tol(model_6)\n\n  Variables Tolerance      VIF\n1        X1 0.3319061 3.012900\n2        X2 0.3658998 2.732989\n3        X5 0.5664655 1.765333\n4        X7 0.6043771 1.654596\n\n# 회귀진단 그래프들\n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(model_6)\n\n\n\nlayout(1)\n\n# Cook의 거리 - 1을 넘으면 확인을 해야한다 \n\nolsrr::ols_plot_cooksd_chart(model_6)"
  },
  {
    "objectID": "Regression_Analysis.html#장",
    "href": "Regression_Analysis.html#장",
    "title": "Regression Analysis",
    "section": "",
    "text": "data_2.5&lt;-read.table(\"All_Data/p031.txt\",header=TRUE,sep=\"\\t\")\n\ndim(data_2.5) #14 2\n\n[1] 14  2\n\nhead(data_2.5)\n\n  Minutes Units\n1      23     1\n2      29     2\n3      49     3\n4      64     4\n5      74     4\n6      87     5\n\nX&lt;-data_2.5$Units\n\nY&lt;-data_2.5$Minutes\n\n\n\n\n\ndf&lt;-data.frame(\n\n  #1:length(X),\n\n  Y,\n\n  X,\n\n  Y-mean(Y),\n\n  X-mean(X),\n\n  (Y-mean(Y))^2,\n\n  (X-mean(X))^2,\n\n  (Y-mean(Y))*(X-mean(X))\n\n)\n\ndf\n\n     Y  X Y...mean.Y. X...mean.X. X.Y...mean.Y...2 X.X...mean.X...2\n1   23  1 -74.2142857          -5     5.507760e+03               25\n2   29  2 -68.2142857          -4     4.653189e+03               16\n3   49  3 -48.2142857          -3     2.324617e+03                9\n4   64  4 -33.2142857          -2     1.103189e+03                4\n5   74  4 -23.2142857          -2     5.389031e+02                4\n6   87  5 -10.2142857          -1     1.043316e+02                1\n7   96  6  -1.2142857           0     1.474490e+00                0\n8   97  6  -0.2142857           0     4.591837e-02                0\n9  109  7  11.7857143           1     1.389031e+02                1\n10 119  8  21.7857143           2     4.746173e+02                4\n11 149  9  51.7857143           3     2.681760e+03                9\n12 145  9  47.7857143           3     2.283474e+03                9\n13 154 10  56.7857143           4     3.224617e+03               16\n14 166 10  68.7857143           4     4.731474e+03               16\n   X.Y...mean.Y......X...mean.X..\n1                       371.07143\n2                       272.85714\n3                       144.64286\n4                        66.42857\n5                        46.42857\n6                        10.21429\n7                         0.00000\n8                         0.00000\n9                        11.78571\n10                       43.57143\n11                      155.35714\n12                      143.35714\n13                      227.14286\n14                      275.14286\n\n\n\n\n\n\nCOV_XY&lt;-sum((Y-mean(Y))*(X-mean(X))) / (length(X)-1) #136\n\n### cov() 함수\n\ncov(X,Y) #136\n\n[1] 136\n\n### 상관계수(correalationship)\n\n### cor() 함수\n\ncor(X,Y) #0.9936987 \n\n[1] 0.9936987"
  },
  {
    "objectID": "Regression_Analysis.html#선형모형-1",
    "href": "Regression_Analysis.html#선형모형-1",
    "title": "Regression Analysis",
    "section": "",
    "text": "data_2.5&lt;-read.table(\"All_Data/p031.txt\",header=TRUE,sep=\"\\t\")\n\nx&lt;-data_2.5$Units\n\ny&lt;-data_2.5$Minutes\n\n\n\n\ncor_xy&lt;- COV_XY / (sd(x)*sd(y))\n\ncor_xy\n\n[1] 0.9936987\n\n### cor() 함수\n\ncor(x,y)\n\n[1] 0.9936987\n\ncor(y,x)\n\n[1] 0.9936987\n\ndata_2.5\n\n   Minutes Units\n1       23     1\n2       29     2\n3       49     3\n4       64     4\n5       74     4\n6       87     5\n7       96     6\n8       97     6\n9      109     7\n10     119     8\n11     149     9\n12     145     9\n13     154    10\n14     166    10\n\ncor(data_2.5)\n\n          Minutes     Units\nMinutes 1.0000000 0.9936987\nUnits   0.9936987 1.0000000\n\n\n\n\n\n\nclass(X)\n\n[1] \"numeric\"\n\nclass(Y) #both numeric\n\n[1] \"numeric\"\n\nplot(X,Y, pch=19,xlab=\"Units\",ylab=\"Minutes\") \n\n\n\n\n\n\n\n\ndata_2.3&lt;-read.table(\"All_Data/p029a.txt\",header=TRUE,sep=\"\\t\")\n\ndata_2.3\n\n    Y  X\n1   1 -7\n2  14 -6\n3  25 -5\n4  34 -4\n5  41 -3\n6  46 -2\n7  49 -1\n8  50  0\n9  49  1\n10 46  2\n11 41  3\n12 34  4\n13 25  5\n14 14  6\n15  1  7\n\nX&lt;-data_2.3$X\n\nY&lt;-data_2.3$Y\n\n\n\n\n\nplot(X,Y)\n\n\n\ncor(X,Y) # 0 완벽하게 2차함수의 형태도 0이 나옴(직선의 형태가 아닌것만)\n\n[1] 0\n\n\n\n\n\n\ndata_2.4&lt;-read.table(\"All_Data/p029b.txt\",header=TRUE,sep=\"\\t\")\n\n\n\n\n\nplot(data_2.4$X1,data_2.4$Y1, pch=19); abline(3,0.5) #기울기 3 절편0.5인 선을 추가해라 \n\n\n\nplot(data_2.4$X2,data_2.4$Y2, pch=19); abline(3,0.5)\n\n\n\nplot(data_2.4$X3,data_2.4$Y3, pch=19); abline(3,0.5)\n\n\n\nplot(data_2.4$X4,data_2.4$Y4, pch=19); abline(3,0.5)\n\n\n\nm&lt;-matrix(1:4,ncol=2,byrow=TRUE) #2행의 매트릭스 생성 \n\nm\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nlayout(m)\n\nplot(Y1~X1, data=data_2.4,pch=19); abline(3,0.5)\n\n#y~x  -&gt; y=ax+b 이러한 형태를 가지는 모형식이라는 의미 \n\nplot(Y2~X2, data=data_2.4,pch=19); abline(3,0.5)\n\nplot(Y3~X3, data=data_2.4,pch=19); abline(3,0.5)\n\nplot(Y4~X4, data=data_2.4,pch=19); abline(3,0.5)\n\n\n\nlayout(1) #변환을 다시 하지 않으면 설정한 매트릭스의 비율로 그래프가 그려짐 해제 필요 \n\n# cor()\n\ncor(data_2.4$X1,data_2.4$Y1) #0.8164205\n\n[1] 0.8164205\n\ncor(data_2.4$X2,data_2.4$Y2) #0.8162365\n\n[1] 0.8162365\n\ncor(data_2.4$X3,data_2.4$Y3) #0.8162867\n\n[1] 0.8162867\n\ncor(data_2.4$X4,data_2.4$Y4) #0.8165214\n\n[1] 0.8165214\n\ncor(data_2.4) #이렇게 한번에 할 수 있으나 가독성 떨어짐 \n\n           Y1         X1         Y2         X2         Y3         X3         Y4\nY1  1.0000000  0.8164205  0.7500054  0.8164205  0.4687167  0.8164205 -0.4891162\nX1  0.8164205  1.0000000  0.8162365  1.0000000  0.8162867  1.0000000 -0.3140467\nY2  0.7500054  0.8162365  1.0000000  0.8162365  0.5879193  0.8162365 -0.4780949\nX2  0.8164205  1.0000000  0.8162365  1.0000000  0.8162867  1.0000000 -0.3140467\nY3  0.4687167  0.8162867  0.5879193  0.8162867  1.0000000  0.8162867 -0.1554718\nX3  0.8164205  1.0000000  0.8162365  1.0000000  0.8162867  1.0000000 -0.3140467\nY4 -0.4891162 -0.3140467 -0.4780949 -0.3140467 -0.1554718 -0.3140467  1.0000000\nX4 -0.5290927 -0.5000000 -0.7184365 -0.5000000 -0.3446610 -0.5000000  0.8165214\n           X4\nY1 -0.5290927\nX1 -0.5000000\nY2 -0.7184365\nX2 -0.5000000\nY3 -0.3446610\nX3 -0.5000000\nY4  0.8165214\nX4  1.0000000"
  },
  {
    "objectID": "Regression_Analysis.html#단순선형회귀모형-2.4",
    "href": "Regression_Analysis.html#단순선형회귀모형-2.4",
    "title": "Regression Analysis",
    "section": "",
    "text": "data_2.5&lt;-read.table(\"All_Data/p031.txt\",header=TRUE,sep=\"\\t\")\n\nx&lt;-data_2.5$Units\n\ny&lt;-data_2.5$Minutes\n\n\n\n\n\nsum((y-mean(y))*(x-mean(x))) #1768\n\n[1] 1768\n\nsum((x-mean(x))^2) #114\n\n[1] 114\n\nbeta1_hat&lt;-sum((y-mean(y))*(x-mean(x))) / sum((x-mean(x))^2)\n\nbeta1_hat #15.50877\n\n[1] 15.50877\n\nbeta0_hat &lt;- mean(y) - (beta1_hat*mean(x))\n\nbeta0_hat #4.161654\n\n[1] 4.161654\n\n### 최소제곱회귀 방정식\n\n# Minutes = 4.161654 + 15.50877 * Units\n\nplot(beta0_hat+beta1_hat*x,pch=19);\n\n\n\n# 4개의 고장 난 부품을 수리하는데 걸리는 에측시간\n\n4.161654 + 15.50877 * 4 #66.19673\n\n[1] 66.19673\n\nunits&lt;-4\n\nbeta0_hat + beta1_hat*units\n\n[1] 66.19674\n\n### 적합값(Fitted value)\n\ny_hat&lt;-beta0_hat + beta1_hat*(x)\n\n### 최소 제곱 잔차(residual)\n\ne&lt;-y-y_hat\n\ne #합이 0이라는 특징이 존재\n\n [1]  3.3295739 -6.1791980 -1.6879699 -2.1967419  7.8032581  5.2944862\n [7] -1.2142857 -0.2142857 -3.7230576 -9.2318296  5.2593985  1.2593985\n[13] -5.2493734  6.7506266\n\nsum(e) #1.278977e-13 0에 근사한 추지가 나옴\n\n[1] 1.278977e-13\n\n\n\n\n\n\ndf_2.7&lt;-data.frame(\n\n  x=x,\n\n  y=y,\n\n  y_hat,\n\n  e\n\n)\n\ndf_2.7\n\n    x   y     y_hat          e\n1   1  23  19.67043  3.3295739\n2   2  29  35.17920 -6.1791980\n3   3  49  50.68797 -1.6879699\n4   4  64  66.19674 -2.1967419\n5   4  74  66.19674  7.8032581\n6   5  87  81.70551  5.2944862\n7   6  96  97.21429 -1.2142857\n8   6  97  97.21429 -0.2142857\n9   7 109 112.72306 -3.7230576\n10  8 119 128.23183 -9.2318296\n11  9 149 143.74060  5.2593985\n12  9 145 143.74060  1.2593985\n13 10 154 159.24937 -5.2493734\n14 10 166 159.24937  6.7506266\n\n### lm() 함수 (linear model)\n\n# Minutes = beta0 + beta1 * Units + epsilon\n\n# 모형식 : y~x\n\nlm(y~x)\n\n\nCall:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)            x  \n      4.162       15.509  \n\nres_lm&lt;-lm(Minutes~Units,data=data_2.5)\n\nres_lm\n\n\nCall:\nlm(formula = Minutes ~ Units, data = data_2.5)\n\nCoefficients:\n(Intercept)        Units  \n      4.162       15.509  \n\n# 리스트의 이름 \n\nnames(res_lm)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n# 회귀계수\n\nres_lm$coefficients\n\n(Intercept)       Units \n   4.161654   15.508772 \n\ncoef(res_lm)\n\n(Intercept)       Units \n   4.161654   15.508772 \n\n# 적합값\n\nres_lm$fitted.values\n\n        1         2         3         4         5         6         7         8 \n 19.67043  35.17920  50.68797  66.19674  66.19674  81.70551  97.21429  97.21429 \n        9        10        11        12        13        14 \n112.72306 128.23183 143.74060 143.74060 159.24937 159.24937 \n\nfitted(res_lm)\n\n        1         2         3         4         5         6         7         8 \n 19.67043  35.17920  50.68797  66.19674  66.19674  81.70551  97.21429  97.21429 \n        9        10        11        12        13        14 \n112.72306 128.23183 143.74060 143.74060 159.24937 159.24937 \n\n# 최소제곱잔차\n\nres_lm$residuals\n\n         1          2          3          4          5          6          7 \n 3.3295739 -6.1791980 -1.6879699 -2.1967419  7.8032581  5.2944862 -1.2142857 \n         8          9         10         11         12         13         14 \n-0.2142857 -3.7230576 -9.2318296  5.2593985  1.2593985 -5.2493734  6.7506266 \n\nresid(res_lm)\n\n         1          2          3          4          5          6          7 \n 3.3295739 -6.1791980 -1.6879699 -2.1967419  7.8032581  5.2944862 -1.2142857 \n         8          9         10         11         12         13         14 \n-0.2142857 -3.7230576 -9.2318296  5.2593985  1.2593985 -5.2493734  6.7506266 \n\nresiduals(res_lm)\n\n         1          2          3          4          5          6          7 \n 3.3295739 -6.1791980 -1.6879699 -2.1967419  7.8032581  5.2944862 -1.2142857 \n         8          9         10         11         12         13         14 \n-0.2142857 -3.7230576 -9.2318296  5.2593985  1.2593985 -5.2493734  6.7506266 \n\n\n\n\n\n\nplot(beta0_hat+beta1_hat*x,pch=19);\n\n#abline(beta0_hat,beta1_hat)\n\nabline(res_lm)"
  },
  {
    "objectID": "Regression_Analysis.html#section",
    "href": "Regression_Analysis.html#section",
    "title": "Regression Analysis",
    "section": "",
    "text": "data_2.5&lt;-read.table(\"All_Data/p031.txt\",header=TRUE,sep=\"\\t\")\n\nres_lm &lt;- lm(Minutes ~ Units, data = data_2.5)\n\nres_lm\n\n\nCall:\nlm(formula = Minutes ~ Units, data = data_2.5)\n\nCoefficients:\n(Intercept)        Units  \n      4.162       15.509  \n\nres_lm_summ&lt;-summary(res_lm)\n\nres_lm_summ #Pr(&gt;|t|) - p-value\n\n\nCall:\nlm(formula = Minutes ~ Units, data = data_2.5)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.2318 -3.3415 -0.7143  4.7769  7.8033 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    4.162      3.355    1.24    0.239    \nUnits         15.509      0.505   30.71 8.92e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.392 on 12 degrees of freedom\nMultiple R-squared:  0.9874,    Adjusted R-squared:  0.9864 \nF-statistic: 943.2 on 1 and 12 DF,  p-value: 8.916e-13\n\n# unit은 시간에 영향을준다 약15.5분 만큼씩 \n\n# coefficient에서 p-value에 대해서 알 수 있음 \n\n# beta_0는 0이라고 보면되느냐? p-value가 0.05보다 크기에"
  },
  {
    "objectID": "Regression_Analysis.html#section-1",
    "href": "Regression_Analysis.html#section-1",
    "title": "Regression Analysis",
    "section": "",
    "text": "confint(res_lm) # beta_0,1의 95% 신뢰구간을 뽑아줌 \n\n                2.5 %   97.5 %\n(Intercept) -3.148482 11.47179\nUnits       14.408512 16.60903\n\n?confint #level = 1-alpha\n\nstarting httpd help server ... done\n\nconfint(res_lm, level=0.90) # 90%의 신뢰구간\n\n                 5 %     95 %\n(Intercept) -1.81810 10.14141\nUnits       14.60875 16.40879\n\n\n\n\n\n\n# 4개의 고장난 부품을 수리하는 데에 걸리는 시간 예측\n\nx&lt;-4\n\n4.161654 + 15.508772 *4\n\n[1] 66.19674\n\nres_lm$coefficients[1]+(res_lm$coefficients[2]*x)\n\n(Intercept) \n   66.19674 \n\n### predict()\n\ndf&lt;-data.frame(Units=4) \n\npredict(res_lm,newdata=df) # res_lm을 만들때 사용한 데이터형식으로 만들어주어야함\n\n       1 \n66.19674 \n\nres_lm_pred&lt;-predict(res_lm,newdata=df,se.fit=TRUE)\n\n### 예측값\n\nres_lm_pred$fit\n\n       1 \n66.19674 \n\n### 표준오차\n\nres_lm_pred$se.fit # 평균반응에 대한 표준오차 \n\n[1] 1.759688\n\n### 예측한계\n\ndf&lt;-data.frame(Units=4) #예제서는 4대기준\n\ndf&lt;-data.frame(Units=1:10) #다른것도 보고 싶은경우 \n\nres_lm_pred_int_p&lt;-predict(res_lm,newdata=df,interval=\"prediction\")\n\n### 신뢰한계\n\ndf&lt;-data.frame(Units=1:10) #다른것도 보고 싶은경우 \n\nres_lm_pred_int_c&lt;-predict(res_lm,newdata=df,interval=\"confidence\") #둘의 차이를 보면 예측한계의 범위가 더큼 \n\n### 예측한계 & 신뢰한계\n\n# 신뢰한계는 평균에서 멀어지만 오차의범위가 커지고 평균에 다가갈수록 오차가 줄어듬\n\nplot(Minutes~Units,data=data_2.5,pch=19)\n\nabline(res_lm,col=\"red\",lwd=2)\n\nlines(1:10,res_lm_pred_int_p[,\"lwr\"],col=\"darkgreen\")\n\nlines(1:10,res_lm_pred_int_p[,\"upr\"],col=\"darkgreen\")\n\nlines(1:10,res_lm_pred_int_c[,\"lwr\"],col=\"blue\")\n\nlines(1:10,res_lm_pred_int_c[,\"upr\"],col=\"blue\")\n\n\n\n\n\n\n\n\nres_lm_summ&lt;-summary(res_lm)\n\nres_lm_summ #Multiple R-squared:0.9874 -&gt; 반응변수의 전체변이중 98.94%가 예측변수에 의해 설명된다\n\n\nCall:\nlm(formula = Minutes ~ Units, data = data_2.5)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.2318 -3.3415 -0.7143  4.7769  7.8033 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    4.162      3.355    1.24    0.239    \nUnits         15.509      0.505   30.71 8.92e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.392 on 12 degrees of freedom\nMultiple R-squared:  0.9874,    Adjusted R-squared:  0.9864 \nF-statistic: 943.2 on 1 and 12 DF,  p-value: 8.916e-13\n\n# 만약 R-squared가 1이면 완벽한 선형의 관계 100%라는 것을 의미한다.\n\n# R-squared는 변수가 들어갈수록 커지기에 adjust R-squared를 사용 추후 설명 \n\n\n\n\n\n# Minutes = beta1 + Units + epsilon\n\nres_lm_no&lt;-lm(Minutes~Units-1,data=data_2.5)\n\nres_lm_no\n\n\nCall:\nlm(formula = Minutes ~ Units - 1, data = data_2.5)\n\nCoefficients:\nUnits  \n16.07  \n\nsummary(res_lm_no)\n\n\nCall:\nlm(formula = Minutes ~ Units - 1, data = data_2.5)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5955 -2.4733  0.4417  5.0243  9.7023 \n\nCoefficients:\n      Estimate Std. Error t value Pr(&gt;|t|)    \nUnits  16.0744     0.2213   72.62   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.502 on 13 degrees of freedom\nMultiple R-squared:  0.9975,    Adjusted R-squared:  0.9974 \nF-statistic:  5274 on 1 and 13 DF,  p-value: &lt; 2.2e-16\n\ncoef(summary(res_lm_no)) #rsquared=0.9975\n\n      Estimate Std. Error  t value     Pr(&gt;|t|)\nUnits 16.07443  0.2213341 72.62519 2.380325e-18\n\n\n\n\n\n\ny&lt;-rnorm(30)\n\nt.test(y,mu=0)\n\n\n    One Sample t-test\n\ndata:  y\nt = 0.8445, df = 29, p-value = 0.4053\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.2121536  0.5105780\nsample estimates:\nmean of x \n0.1492122 \n\nsummary(lm(y~1))\n\n\nCall:\nlm(formula = y ~ 1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6420 -0.8088  0.2048  0.7985  1.9749 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   0.1492     0.1767   0.844    0.405\n\nResidual standard error: 0.9678 on 29 degrees of freedom"
  },
  {
    "objectID": "Regression_Analysis.html#장-다중선형회귀",
    "href": "Regression_Analysis.html#장-다중선형회귀",
    "title": "Regression Analysis",
    "section": "",
    "text": "data_3.3&lt;-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\ndim(data_3.3)\n\n[1] 30  7\n\nclass(data_3.3)\n\n[1] \"data.frame\"\n\nsapply(data_3.3,class) #all numeric\n\n        Y        X1        X2        X3        X4        X5        X6 \n\"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \n\nsummary(data_3.3) #모든변수가 numeric이면 분위수도 보여준다 \n\n       Y               X1             X2              X3              X4       \n Min.   :40.00   Min.   :37.0   Min.   :30.00   Min.   :34.00   Min.   :43.00  \n 1st Qu.:58.75   1st Qu.:58.5   1st Qu.:45.00   1st Qu.:47.00   1st Qu.:58.25  \n Median :65.50   Median :65.0   Median :51.50   Median :56.50   Median :63.50  \n Mean   :64.63   Mean   :66.6   Mean   :53.13   Mean   :56.37   Mean   :64.63  \n 3rd Qu.:71.75   3rd Qu.:77.0   3rd Qu.:62.50   3rd Qu.:66.75   3rd Qu.:71.00  \n Max.   :85.00   Max.   :90.0   Max.   :83.00   Max.   :75.00   Max.   :88.00  \n       X5              X6       \n Min.   :49.00   Min.   :25.00  \n 1st Qu.:69.25   1st Qu.:35.00  \n Median :77.50   Median :41.00  \n Mean   :74.77   Mean   :42.93  \n 3rd Qu.:80.00   3rd Qu.:47.75  \n Max.   :92.00   Max.   :72.00  \n\n### 산점도 행렬\n\nplot(data_3.3)\n\n\n\n\n\n\n\n\nlm(Y~X1+X2+X3+X4+X5+X6,data=data_3.3)\n\n\nCall:\nlm(formula = Y ~ X1 + X2 + X3 + X4 + X5 + X6, data = data_3.3)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n   10.78708      0.61319     -0.07305      0.32033      0.08173      0.03838  \n         X6  \n   -0.21706  \n\nlm(Y~.,data=data_3.3) # X1+X2+X3+X4+X5+X6쓰는 것이 아니라 .을 써서 모든 변수를 다써줌 \n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n   10.78708      0.61319     -0.07305      0.32033      0.08173      0.03838  \n         X6  \n   -0.21706  \n\n\n\n\n\n\ndata_3.3&lt;-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\nlm(Y~X1+X2,data=data_3.3)\n\n\nCall:\nlm(formula = Y ~ X1 + X2, data = data_3.3)\n\nCoefficients:\n(Intercept)           X1           X2  \n   15.32762      0.78034     -0.05016  \n\n# (Intercept)         X1           X2  \n\n#  15.32762      0.78034     -0.05016\n\n# 1) Y에서 X1 효과 제거\n\nm1&lt;-lm(Y~X1,data=data_3.3) # y prime\n\nm1$residuals # x1이 설명하지 못한값 / x1의 효과를 제거한 값\n\n           1            2            3            4            5            6 \n -9.86142016   0.32865220   3.80099328  -0.91673799   7.76411473 -12.87985944 \n           7            8            9           10           11           12 \n -6.93517726   0.02794419  -4.25432454   6.59248165   9.62936020   7.34709147 \n          13           14           15           16           17           18 \n  7.83787183  -9.00893436   4.51872455  -1.29120309  -4.51815400   5.34709147 \n          19           20           21           22           23           24 \n -2.19900672  -8.14368889   5.43928784   3.59248165 -11.18056744  -2.29688270 \n          25           26           27           28           29           30 \n  7.87475038  -6.48127545   7.02794419  -9.38907907   6.48184600   5.74567546 \n\n# 2) X2에서 X1 효과 제거\n\nm2&lt;-lm(X2~X1,data=data_3.3) # x2 prime \n\nm2$residuals \n\n          1           2           3           4           5           6 \n-15.1300345  -0.7994502  13.1223579  -6.2864182  -2.9818979   1.8178376 \n          7           8           9          10          11          12 \n-11.3385461  -7.4428019  10.9659742  -5.2603543   6.8439016  -2.7473223 \n         13          14          15          16          17          18 \n  6.2266138  21.4529422  -4.4688659 -15.1382816   1.4268783  15.2526777 \n         19          20          21          22          23          24 \n -8.8776421  19.2787417  -6.4866827   1.7396457  -0.8255141   4.0524132 \n         25          26          27          28          29          30 \n -4.6691304   7.5311341   0.5571981  -4.2082264   8.4268783 -22.0340258 \n\n# 3) X1의 효과가 제거된 Y와 X2의 적합 - 원점을 지나는 회귀선\n\nlm(m1$residuals~m2$residuals-1) # 원점을 지나면 -1를 하고 진행 // -3.25e-17\n\n\nCall:\nlm(formula = m1$residuals ~ m2$residuals - 1)\n\nCoefficients:\nm2$residuals  \n    -0.05016  \n\n# 다른 효과 없이(다른값이 고정) Y에 영향을 주는 순수한 X2의 값\n\n# m2$residuals  : -0.05016  ==  X2 : -0.05016  \n\n### 단위길이 척도화 - 잘사용하지않음\n\nfn_scaling_len&lt;-function(x){\n\n  x0&lt;-x-mean(x)\n\n  x0/sqrt(sum(x0^2))\n\n}\n\ndata_3.3_len&lt;-sapply(data_3.3, fn_scaling_len)\n\ndata_3.3_len&lt;-data.frame(data_3.3_len)\n\nsummary(data_3.3_len)\n\n       Y                  X1                 X2                 X3           \n Min.   :-0.37579   Min.   :-0.41282   Min.   :-0.35109   Min.   :-0.353871  \n 1st Qu.:-0.08975   1st Qu.:-0.11297   1st Qu.:-0.12344   1st Qu.:-0.148193  \n Median : 0.01322   Median :-0.02231   Median :-0.02479   Median : 0.002109  \n Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.000000  \n 3rd Qu.: 0.10857   3rd Qu.: 0.14504   3rd Qu.: 0.14216   3rd Qu.: 0.164278  \n Max.   : 0.31070   Max.   : 0.32635   Max.   : 0.45328   Max.   : 0.294804  \n       X4                 X5                 X6          \n Min.   :-0.38637   Min.   :-0.48356   Min.   :-0.32367  \n 1st Qu.:-0.11401   1st Qu.:-0.10353   1st Qu.:-0.14318  \n Median :-0.02024   Median : 0.05130   Median :-0.03489  \n Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.00000  \n 3rd Qu.: 0.11371   3rd Qu.: 0.09821   3rd Qu.: 0.08693  \n Max.   : 0.41733   Max.   : 0.32341   Max.   : 0.52461  \n\nlm(Y~.,data=data_3.3_len)\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3_len)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n -1.259e-16    6.707e-01   -7.343e-02    3.089e-01    6.981e-02    3.120e-02  \n         X6  \n -1.835e-01  \n\n### 표준화\n\n# scale()\n\ndata_3.3_std&lt;-scale(data_3.3)\n\n#summary(data_3.3_std)\n\n#sapply(data_3.3_std, sd, na.rm=T)\n\n#class(data_3.3_std) #\"matrix\"\n\ndata_3.3_std&lt;-data.frame(data_3.3_std)\n\n#class(data_3.3_std) #\"data.frame\"\n\n#sapply(data_3.3_std, sd, na.rm=T)\n\nlm(Y~.,data=data_3.3_std) # beta게수 구하기 \n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3_std)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n -7.717e-16    6.707e-01   -7.343e-02    3.089e-01    6.981e-02    3.120e-02  \n         X6  \n -1.835e-01  \n\n\n\n\n\n\nres_lm&lt;-lm(Y~.,data=data_3.3)\n\nres_lm\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n   10.78708      0.61319     -0.07305      0.32033      0.08173      0.03838  \n         X6  \n   -0.21706  \n\nsummary(res_lm)\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.9418  -4.3555   0.3158   5.5425  11.5990 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10.78708   11.58926   0.931 0.361634    \nX1           0.61319    0.16098   3.809 0.000903 ***\nX2          -0.07305    0.13572  -0.538 0.595594    \nX3           0.32033    0.16852   1.901 0.069925 .  \nX4           0.08173    0.22148   0.369 0.715480    \nX5           0.03838    0.14700   0.261 0.796334    \nX6          -0.21706    0.17821  -1.218 0.235577    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.068 on 23 degrees of freedom\nMultiple R-squared:  0.7326,    Adjusted R-squared:  0.6628 \nF-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05\n\nm1&lt;-summary(lm(Y~X1+X2+X3+X4+X5+X6,data=data_3.3)) #Adjusted R-squared:  0.6628 \n\nm2&lt;-summary(lm(Y~X1+X2+X3+X4+X5,data=data_3.3)) #Adjusted R-squared:  0.6561 \n\n# X6가 들어가는 것이 더 좋은 모델 \n\nm1$adj.r.squared\n\n[1] 0.662846\n\nm2$adj.r.squared #summary에서 보다 더 정확하게 수치가 나옴 \n\n[1] 0.6560539\n\n\n\n\n\n\nres_lm_summ&lt;-summary(res_lm)\n\nres_lm_summ #p-value의 존재는 무언가를 검정했다라는 반증\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.9418  -4.3555   0.3158   5.5425  11.5990 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10.78708   11.58926   0.931 0.361634    \nX1           0.61319    0.16098   3.809 0.000903 ***\nX2          -0.07305    0.13572  -0.538 0.595594    \nX3           0.32033    0.16852   1.901 0.069925 .  \nX4           0.08173    0.22148   0.369 0.715480    \nX5           0.03838    0.14700   0.261 0.796334    \nX6          -0.21706    0.17821  -1.218 0.235577    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.068 on 23 degrees of freedom\nMultiple R-squared:  0.7326,    Adjusted R-squared:  0.6628 \nF-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05\n\n# p-value&lt;0.05 H_1 귀무가설 채택 \n\n# p-value&gt;0.05 H_0 영가설 채택 // X1을 제외하고는 영가설 유의한 의미가 없음(Y에영향주는)\n\n# 모두다 0이라는 가설을 가지고 분모 분자의 오차가 카이제곱을 따르고 거기서 나온 통계량\n\n# F-분포 자유도는 분자 분모 두개를 가짐 //모아서 계산을 하기에 각각 계산하는것과 결과다름 \n\n# 영가설-모든 회귀계수가 0이다.\n\n# 대립가설-적어도 하나는 0이 아니다. p-value: 1.24e-05 &lt;0.05 대립가설 채택 \n\n# p-value가 0.05보다 작으면 대립가설 채택!!!!!! 기억해 \n\n# 회귀계수에 대한 신뢰구간 - 95% 신뢰한계\n\nconfint(res_lm) #-13.18712881 ~ 34.7612816\n\n                   2.5 %     97.5 %\n(Intercept) -13.18712881 34.7612816\nX1            0.28016866  0.9462066\nX2           -0.35381806  0.2077178\nX3           -0.02827872  0.6689430\nX4           -0.37642935  0.5398936\nX5           -0.26570179  0.3424647\nX6           -0.58571106  0.1515977\n\n#X1  0.28016866  0.9462066  사이에 0이 들어가있으면 영향을 준다라느걸 의미\n\n#X2 -0.35381806  0.2077178  p-value없이도 알 수 있음 \n\n#X5가 가장 영향이 적음 p-value가 가장 크기에(영향 효과의 크기를 비교할때)\n\n#p-value가 작을 수록 영향을 많이 준다 beta값을 보는 것이 아닌 p-value를 보는 것 중요\n\n#가장 의미있는 변수?-&gt;p-value가장 작은거 // 대립가설채택 Y에 영향을 가장\n\n\n\n\n\n\n\n\n# H_0: beta_1:beta_6=0\n\nmodel_reduce&lt;-lm(Y~1,data=data_3.3)\n\nmodel_full&lt;-lm(Y~.,data=data_3.3)\n\nanova(model_reduce,model_full)\n\nAnalysis of Variance Table\n\nModel 1: Y ~ 1\nModel 2: Y ~ X1 + X2 + X3 + X4 + X5 + X6\n  Res.Df  RSS Df Sum of Sq      F   Pr(&gt;F)    \n1     29 4297                                 \n2     23 1149  6      3148 10.502 1.24e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#대립가설 = 완전모형이 적절하다 / 1.24e-05 *** &lt; 0.05 \n\n#의미 있는 예측 변수가 한개 이상 존재한다 \n\nsummary(model_full) #summary에서 beta_1~beta_6까지 모두가 0이라는 가설로 진행을 이미함\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.9418  -4.3555   0.3158   5.5425  11.5990 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10.78708   11.58926   0.931 0.361634    \nX1           0.61319    0.16098   3.809 0.000903 ***\nX2          -0.07305    0.13572  -0.538 0.595594    \nX3           0.32033    0.16852   1.901 0.069925 .  \nX4           0.08173    0.22148   0.369 0.715480    \nX5           0.03838    0.14700   0.261 0.796334    \nX6          -0.21706    0.17821  -1.218 0.235577    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.068 on 23 degrees of freedom\nMultiple R-squared:  0.7326,    Adjusted R-squared:  0.6628 \nF-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05\n\n#F-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05\n\n#예상// 가장의미있는변수? -&gt; X1 이유?-&gt; p-value 0.000903 로 가장 작기에 영향많이 줄것으로 예측 \n\n\n\n\n\nmodel_reduce&lt;-lm(Y~X1+X3,data=data_3.3)\n\nmodel_full&lt;-lm(Y~.,data=data_3.3)\n\nanova(model_reduce,model_full) #0.7158 &gt; 0.05\n\nAnalysis of Variance Table\n\nModel 1: Y ~ X1 + X3\nModel 2: Y ~ X1 + X2 + X3 + X4 + X5 + X6\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     27 1254.7                           \n2     23 1149.0  4    105.65 0.5287 0.7158\n\n#영가설은 H_0: b_2=b_4=b_5=b_6=0 이라는 사실을 알 수 있다 \n\n#b_1&b_3는 반응변수에 유의한 반응을 준다라는 것도 연계하여 알 수 있다 \n\n\n\n\n\n#해당 조건이 주어지고 만족할 때 beta_1=beta_3은 맞는가?\n\nmodel_reduce&lt;-lm(Y~I(X1-X3),data=data_3.3) #I를 씌우면 새로운 변수를 만든것과 동일\n\n# X1-X3를 한 그자체를 분석하라는 의미//본래는 X1-X3 해서 새로운 변수를 만들어서 해야함 \n\nmodel_full&lt;-lm(Y~X1+X3,data=data_3.3)\n\nanova(model_reduce,model_full) \n\nAnalysis of Variance Table\n\nModel 1: Y ~ I(X1 - X3)\nModel 2: Y ~ X1 + X3\n  Res.Df    RSS Df Sum of Sq     F    Pr(&gt;F)    \n1     28 3846.7                                 \n2     27 1254.6  1      2592 55.78 4.925e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#install.packages(\"car\")\n\nlibrary(car)\n\nLoading required package: carData\n\nmodel_full&lt;-lm(Y~X1+X3,data=data_3.3)\n\ncar::linearHypothesis(model_full,c(\"X1=X3\"))\n\nLinear hypothesis test\n\nHypothesis:\nX1 - X3 = 0\n\nModel 1: restricted model\nModel 2: Y ~ X1 + X3\n\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     28 1424.6                              \n2     27 1254.7  1    169.95 3.6572 0.06649 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n# H_0: beta_1+beta_3=1 | beta_2=beta_3:beta_6=0\n\nmodel_full&lt;-lm(Y~X1+X3,data=data_3.3)\n\ncar::linearHypothesis(model_full,c(\"X1 + X3 = 1\"))\n\nLinear hypothesis test\n\nHypothesis:\nX1  + X3 = 1\n\nModel 1: restricted model\nModel 2: Y ~ X1 + X3\n\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     28 1329.5                           \n2     27 1254.7  1    74.898 1.6118 0.2151\n\n# x1의 효과가 증가하면 x3의 효과는 감소한다 상대적인 관계 (반대로도 가능)\n\n\n\n\n\nmodel_full&lt;-lm(Y~.,data_3.3)\n\n# 예측값 - 적합값\n\nmodel_full$fitted.values\n\n       1        2        3        4        5        6        7        8 \n51.11030 61.35277 69.93944 61.22684 74.45380 53.94185 67.14841 70.09701 \n       9       10       11       12       13       14       15       16 \n79.53099 59.19846 57.92572 55.40103 59.58168 70.21401 76.54933 84.54785 \n      17       18       19       20       21       22       23       24 \n76.15013 61.39736 68.01656 55.62014 42.60324 63.81902 63.66400 44.62475 \n      25       26       27       28       29       30 \n57.31710 67.84347 75.14036 56.04535 77.66053 76.87850 \n\n# 예측한계(Prediction Limits)\n\npredict(model_full,newdata = data_3.3,interval = \"prediction\")\n\n        fit      lwr       upr\n1  51.11030 34.16999  68.05060\n2  61.35277 46.34536  76.36018\n3  69.93944 53.94267  85.93622\n4  61.22684 45.44586  77.00783\n5  74.45380 59.17630  89.73129\n6  53.94185 37.21813  70.66557\n7  67.14841 51.64493  82.65189\n8  70.09701 54.54384  85.65017\n9  79.53099 62.71383  96.34814\n10 59.19846 44.03506  74.36185\n11 57.92572 42.00674  73.84470\n12 55.40103 39.79333  71.00873\n13 59.58168 43.39853  75.76483\n14 70.21401 52.06636  88.36167\n15 76.54933 60.79444  92.30422\n16 84.54785 66.41374 102.68197\n17 76.15013 59.99991  92.30036\n18 61.39736 43.23384  79.56088\n19 68.01656 52.44673  83.58639\n20 55.62014 39.63744  71.60284\n21 42.60324 26.35046  58.85603\n22 63.81902 48.40145  79.23659\n23 63.66400 48.56222  78.76578\n24 44.62475 27.25435  61.99514\n25 57.31710 41.29380  73.34041\n26 67.84347 49.98605  85.70089\n27 75.14036 59.31975  90.96097\n28 56.04535 40.18723  71.90348\n29 77.66053 61.97564  93.34541\n30 76.87850 60.27441  93.48258\n\n# 신뢰한계(Confidence limits)\n\npredict(model_full,newdata = data_3.3,interval = \"confidence\")\n\n        fit      lwr      upr\n1  51.11030 42.55502 59.66557\n2  61.35277 57.97029 64.73524\n3  69.93944 63.44979 76.42909\n4  61.22684 55.28897 67.16471\n5  74.45380 70.02428 78.88332\n6  53.94185 45.82386 62.05984\n7  67.14841 61.99316 72.30367\n8  70.09701 64.79421 75.39980\n9  79.53099 71.22222 87.83975\n10 59.19846 55.18008 63.21683\n11 57.92572 51.63028 64.22116\n12 55.40103 49.94035 60.86171\n13 59.58168 52.64531 66.51805\n14 70.21401 59.46431 80.96372\n15 76.54933 70.68118 82.41748\n16 84.54785 73.82102 95.27468\n17 76.15013 69.29093 83.00933\n18 61.39736 50.62090 72.17383\n19 68.01656 62.66507 73.36805\n20 55.62014 49.16527 62.07502\n21 42.60324 35.50593 49.70055\n22 63.81902 58.92819 68.70985\n23 63.66400 59.88479 67.44321\n24 44.62475 35.24662 54.00288\n25 57.31710 50.76233 63.87187\n26 67.84347 57.59134 78.09561\n27 75.14036 69.09798 81.18275\n28 56.04535 49.90540 62.18531\n29 77.66053 71.98300 83.33806\n30 76.87850 69.00992 84.74707"
  },
  {
    "objectID": "Regression_Analysis.html#부록-행렬을-이용한-회귀계수-추정",
    "href": "Regression_Analysis.html#부록-행렬을-이용한-회귀계수-추정",
    "title": "Regression Analysis",
    "section": "",
    "text": "data_3.3&lt;-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\nY&lt;-data_3.3$Y\n\nX&lt;-data_3.3[,-1]\n\nX&lt;-cbind(1,X)\n\nX&lt;-as.matrix(X)\n\n#beta_hat&lt;-solve(t(X) %*% X) %*% t(X) %*% Y # %*%행렬 계산 \n\nP&lt;-solve(t(X) %*% X) %*% t(X)\n\nbeta_hat&lt;- P %*% Y\n\nlm(Y~.,data_3.3)\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n   10.78708      0.61319     -0.07305      0.32033      0.08173      0.03838  \n         X6  \n   -0.21706"
  },
  {
    "objectID": "Regression_Analysis.html#장-회귀진단-모형위반의-검출",
    "href": "Regression_Analysis.html#장-회귀진단-모형위반의-검출",
    "title": "Regression Analysis",
    "section": "",
    "text": "# 표준화 잔차\n\ndata_3.3&lt;-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\nres_lm&lt;-lm(Y~.,data=data_3.3)\n\nclass(res_lm)\n\n[1] \"lm\"\n\nmode(res_lm)\n\n[1] \"list\"\n\nnames(res_lm)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\nres_lm$fitted.values\n\n       1        2        3        4        5        6        7        8 \n51.11030 61.35277 69.93944 61.22684 74.45380 53.94185 67.14841 70.09701 \n       9       10       11       12       13       14       15       16 \n79.53099 59.19846 57.92572 55.40103 59.58168 70.21401 76.54933 84.54785 \n      17       18       19       20       21       22       23       24 \n76.15013 61.39736 68.01656 55.62014 42.60324 63.81902 63.66400 44.62475 \n      25       26       27       28       29       30 \n57.31710 67.84347 75.14036 56.04535 77.66053 76.87850 \n\nstr(res_lm)\n\nList of 12\n $ coefficients : Named num [1:7] 10.7871 0.6132 -0.0731 0.3203 0.0817 ...\n  ..- attr(*, \"names\")= chr [1:7] \"(Intercept)\" \"X1\" \"X2\" \"X3\" ...\n $ residuals    : Named num [1:30] -8.11 1.647 1.061 -0.227 6.546 ...\n  ..- attr(*, \"names\")= chr [1:30] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:30] -354.011 54.107 2.742 11.715 -0.971 ...\n  ..- attr(*, \"names\")= chr [1:30] \"(Intercept)\" \"X1\" \"X2\" \"X3\" ...\n $ rank         : int 7\n $ fitted.values: Named num [1:30] 51.1 61.4 69.9 61.2 74.5 ...\n  ..- attr(*, \"names\")= chr [1:30] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:7] 0 1 2 3 4 5 6\n $ qr           :List of 5\n  ..$ qr   : num [1:30, 1:7] -5.477 0.183 0.183 0.183 0.183 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:30] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ : chr [1:7] \"(Intercept)\" \"X1\" \"X2\" \"X3\" ...\n  .. ..- attr(*, \"assign\")= int [1:7] 0 1 2 3 4 5 6\n  ..$ qraux: num [1:7] 1.18 1 1.29 1.1 1.07 ...\n  ..$ pivot: int [1:7] 1 2 3 4 5 6 7\n  ..$ tol  : num 1e-07\n  ..$ rank : int 7\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 23\n $ xlevels      : Named list()\n $ call         : language lm(formula = Y ~ ., data = data_3.3)\n $ terms        :Classes 'terms', 'formula'  language Y ~ X1 + X2 + X3 + X4 + X5 + X6\n  .. ..- attr(*, \"variables\")= language list(Y, X1, X2, X3, X4, X5, X6)\n  .. ..- attr(*, \"factors\")= int [1:7, 1:6] 0 1 0 0 0 0 0 0 0 1 ...\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:7] \"Y\" \"X1\" \"X2\" \"X3\" ...\n  .. .. .. ..$ : chr [1:6] \"X1\" \"X2\" \"X3\" \"X4\" ...\n  .. ..- attr(*, \"term.labels\")= chr [1:6] \"X1\" \"X2\" \"X3\" \"X4\" ...\n  .. ..- attr(*, \"order\")= int [1:6] 1 1 1 1 1 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. ..- attr(*, \"predvars\")= language list(Y, X1, X2, X3, X4, X5, X6)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:7] \"numeric\" \"numeric\" \"numeric\" \"numeric\" ...\n  .. .. ..- attr(*, \"names\")= chr [1:7] \"Y\" \"X1\" \"X2\" \"X3\" ...\n $ model        :'data.frame':  30 obs. of  7 variables:\n  ..$ Y : num [1:30] 43 63 71 61 81 43 58 71 72 67 ...\n  ..$ X1: num [1:30] 51 64 70 63 78 55 67 75 82 61 ...\n  ..$ X2: num [1:30] 30 51 68 45 56 49 42 50 72 45 ...\n  ..$ X3: num [1:30] 39 54 69 47 66 44 56 55 67 47 ...\n  ..$ X4: num [1:30] 61 63 76 54 71 54 66 70 71 62 ...\n  ..$ X5: num [1:30] 92 73 86 84 83 49 68 66 83 80 ...\n  ..$ X6: num [1:30] 45 47 48 35 47 34 35 41 31 41 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language Y ~ X1 + X2 + X3 + X4 + X5 + X6\n  .. .. ..- attr(*, \"variables\")= language list(Y, X1, X2, X3, X4, X5, X6)\n  .. .. ..- attr(*, \"factors\")= int [1:7, 1:6] 0 1 0 0 0 0 0 0 0 1 ...\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:7] \"Y\" \"X1\" \"X2\" \"X3\" ...\n  .. .. .. .. ..$ : chr [1:6] \"X1\" \"X2\" \"X3\" \"X4\" ...\n  .. .. ..- attr(*, \"term.labels\")= chr [1:6] \"X1\" \"X2\" \"X3\" \"X4\" ...\n  .. .. ..- attr(*, \"order\")= int [1:6] 1 1 1 1 1 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(Y, X1, X2, X3, X4, X5, X6)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:7] \"numeric\" \"numeric\" \"numeric\" \"numeric\" ...\n  .. .. .. ..- attr(*, \"names\")= chr [1:7] \"Y\" \"X1\" \"X2\" \"X3\" ...\n - attr(*, \"class\")= chr \"lm\"\n\n# 잔차 \n\nres_lm$residuals\n\n          1           2           3           4           5           6 \n -8.1102953   1.6472337   1.0605589  -0.2268416   6.5462010 -10.9418499 \n          7           8           9          10          11          12 \n -9.1484140   0.9029929  -7.5309862   7.8015424   6.0742817  11.5989723 \n         13          14          15          16          17          18 \n  9.4183197  -2.2140147   0.4506705  -3.5478519  -2.1501319   3.6026355 \n         19          20          21          22          23          24 \n -3.0165587  -5.6201442   7.3967582   0.1809831 -10.6639999  -4.6247464 \n         25          26          27          28          29          30 \n  5.6828983  -1.8434727   2.8596385  -8.0453540   7.3394730   5.1215016 \n\nresid(res_lm) #실제값에서 예측된 값을 뺸값\n\n          1           2           3           4           5           6 \n -8.1102953   1.6472337   1.0605589  -0.2268416   6.5462010 -10.9418499 \n          7           8           9          10          11          12 \n -9.1484140   0.9029929  -7.5309862   7.8015424   6.0742817  11.5989723 \n         13          14          15          16          17          18 \n  9.4183197  -2.2140147   0.4506705  -3.5478519  -2.1501319   3.6026355 \n         19          20          21          22          23          24 \n -3.0165587  -5.6201442   7.3967582   0.1809831 -10.6639999  -4.6247464 \n         25          26          27          28          29          30 \n  5.6828983  -1.8434727   2.8596385  -8.0453540   7.3394730   5.1215016 \n\n### 내적 표준화잔차\n\nrstandard(res_lm)\n\n          1           2           3           4           5           6 \n-1.41498026  0.23955370  0.16744867 -0.03512080  0.97184596 -1.86133876 \n          7           8           9          10          11          12 \n-1.38317210  0.13709194 -1.29490454  1.14799070  0.95218982  1.76906521 \n         13          14          15          16          17          18 \n 1.51371017 -0.46212316  0.06961486 -0.73868563 -0.34446368  0.75418016 \n         19          20          21          22          23          24 \n-0.45861365 -0.88618779  1.19699287  0.02717120 -1.56184734 -0.85286680 \n         25          26          27          28          29          30 \n 0.89948517 -0.36581416  0.44430497 -1.25422677  1.12683185  0.85971512 \n\n### 외적 표준화잔차\n\nMASS::studres(res_lm)\n\n          1           2           3           4           5           6 \n-1.44835328  0.23458097  0.16386794 -0.03434974  0.97062209 -1.97526518 \n          7           8           9          10          11          12 \n-1.41280382  0.13413337 -1.31529351  1.15637546  0.95017640  1.86145176 \n         13          14          15          16          17          18 \n 1.56019127 -0.45407837  0.06809185 -0.73117411 -0.33776450  0.74689589 \n         19          20          21          22          23          24 \n-0.45059801 -0.88189556  1.20894332  0.02657438 -1.61559196 -0.84763116 \n         25          26          27          28          29          30 \n 0.89560731 -0.35881868  0.43641573 -1.27088888  1.13380428  0.85466249 \n\nredsid_df&lt;-data.frame(\n\n  Y=data_3.3$Y,\n\n  Y_hat=res_lm$fitted.values,\n\n  resid=resid(res_lm),\n\n  rstandard=rstandard(res_lm),\n\n  studres=MASS::studres(res_lm)\n\n)\n\nredsid_df\n\n    Y    Y_hat       resid   rstandard     studres\n1  43 51.11030  -8.1102953 -1.41498026 -1.44835328\n2  63 61.35277   1.6472337  0.23955370  0.23458097\n3  71 69.93944   1.0605589  0.16744867  0.16386794\n4  61 61.22684  -0.2268416 -0.03512080 -0.03434974\n5  81 74.45380   6.5462010  0.97184596  0.97062209\n6  43 53.94185 -10.9418499 -1.86133876 -1.97526518\n7  58 67.14841  -9.1484140 -1.38317210 -1.41280382\n8  71 70.09701   0.9029929  0.13709194  0.13413337\n9  72 79.53099  -7.5309862 -1.29490454 -1.31529351\n10 67 59.19846   7.8015424  1.14799070  1.15637546\n11 64 57.92572   6.0742817  0.95218982  0.95017640\n12 67 55.40103  11.5989723  1.76906521  1.86145176\n13 69 59.58168   9.4183197  1.51371017  1.56019127\n14 68 70.21401  -2.2140147 -0.46212316 -0.45407837\n15 77 76.54933   0.4506705  0.06961486  0.06809185\n16 81 84.54785  -3.5478519 -0.73868563 -0.73117411\n17 74 76.15013  -2.1501319 -0.34446368 -0.33776450\n18 65 61.39736   3.6026355  0.75418016  0.74689589\n19 65 68.01656  -3.0165587 -0.45861365 -0.45059801\n20 50 55.62014  -5.6201442 -0.88618779 -0.88189556\n21 50 42.60324   7.3967582  1.19699287  1.20894332\n22 64 63.81902   0.1809831  0.02717120  0.02657438\n23 53 63.66400 -10.6639999 -1.56184734 -1.61559196\n24 40 44.62475  -4.6247464 -0.85286680 -0.84763116\n25 63 57.31710   5.6828983  0.89948517  0.89560731\n26 66 67.84347  -1.8434727 -0.36581416 -0.35881868\n27 78 75.14036   2.8596385  0.44430497  0.43641573\n28 48 56.04535  -8.0453540 -1.25422677 -1.27088888\n29 85 77.66053   7.3394730  1.12683185  1.13380428\n30 82 76.87850   5.1215016  0.85971512  0.85466249\n\n\n\n\n\n\n\n\n\na&lt;-rnorm(100,70,10) #연속형 데이터\n\n# 히스토그램 \n\nhist(a)\n\n\n\nhist(a,breaks=5) #범위를 조절 막대의 5번 자름 \n\n\n\n# 줄기 잎 그림 \n\nstem(a)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  4 | \n  4 | 5\n  5 | 03444\n  5 | 566789\n  6 | 0011112223334444444444\n  6 | 56666667778999999\n  7 | 0000011111233333444444\n  7 | 556677788999\n  8 | 00022234\n  8 | 55679\n  9 | 33\n\nstem(round(a)) #줄기잎을 그릴때는 반올림을 하고 항상 진행 \n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  4 | 5\n  5 | 03444\n  5 | 566789\n  6 | 0011112223334444444444\n  6 | 56666667778999999\n  7 | 0000011111233333444444\n  7 | 556677788999\n  8 | 00022234\n  8 | 55679\n  9 | 33\n\nstem(round(a),scale=2) #scale을 2배로 늘려라 5기준으로 반으로 잘라서 \n\n\n  The decimal point is at the |\n\n  44 | 0\n  46 | \n  48 | \n  50 | 0\n  52 | 0\n  54 | 0000\n  56 | 000\n  58 | 00\n  60 | 000000\n  62 | 000000\n  64 | 00000000000\n  66 | 000000000\n  68 | 0000000\n  70 | 0000000000\n  72 | 000000\n  74 | 00000000\n  76 | 00000\n  78 | 00000\n  80 | 000\n  82 | 0000\n  84 | 000\n  86 | 00\n  88 | 0\n  90 | \n  92 | 00\n\n# 모든데이터를 볼 수있는 장점 데이터가 많으면 구림 \n\n# 점플롯\n\nidx&lt;-rep(1,length(a)) #a의 갯수에 맞춰서 1를 반복 \n\nplot(idx,a)\n\n\n\nplot(jitter(idx),a,xlim=c(0.5,1.5))\n\n\n\n# 상자그림\n\nboxplot(a) #사분위수에 대해서 알 수 있음 \n\n# 상자그림 + 점플롯\n\nboxplot(a)\n\npoints(jitter(idx),a)\n\n\n\n\n\n\n\n\ndata_4.1&lt;-read.table(\"All_Data/p103.txt\",header=T,sep=\"\\t\")\n\ndata_4.1\n\n       Y   X1   X2\n1  12.37 2.23 9.66\n2  12.66 2.57 8.94\n3  12.00 3.87 4.40\n4  11.93 3.10 6.64\n5  11.06 3.39 4.91\n6  13.03 2.83 8.52\n7  13.13 3.02 8.04\n8  11.44 2.14 9.05\n9  12.86 3.04 7.71\n10 10.84 3.26 5.11\n11 11.20 3.39 5.05\n12 11.56 2.35 8.51\n13 10.83 2.76 6.59\n14 12.63 3.90 4.90\n15 12.46 3.16 6.96\n\nclass(data_4.1) #data.frame\n\n[1] \"data.frame\"\n\n# 산점도 행렬\n\nplot(data_4.1)\n\ncor(data_4.1) #상관계수\n\n             Y           X1         X2\nY  1.000000000  0.002497966  0.4340688\nX1 0.002497966  1.000000000 -0.8997765\nX2 0.434068758 -0.899776481  1.0000000\n\npairs(data_4.1)\n\n\n\n# Correlation panel\n\npanel.cor&lt;-function(x,y){\n\n  par(usr=c(0,1,0,1))\n\n  r&lt;-round(cor(x,y),digits = 3)\n\n  text(0.5,0.5,r,cex=1.5)\n\n}\n\npairs(data_4.1,lower.panel = panel.cor)\n\n\n\n# 회전도표, 동적 그래프(3차원)\n\n#install.packages(\"rgl\")\n\nlibrary(rgl)\n\nplot3d(x=data_4.1$X1,y=data_4.1$X2,z=data_4.1$Y) \n\n\n\n\n\ndata_3.3&lt;-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\nres_lm&lt;-lm(Y~X1+X3,data=data_3.3)\n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(res_lm) #회귀진단 플랏이 나옴 \n\n\n\nlayout(1)\n\n# 1. 표준화잔차의 정규확률분포\n\nplot(res_lm,2)\n\n\n\n# 2. 표준화잔차 대 각 예측변수들의 산점도\n\nplot(res_lm,3) #이런경우에는 데이터를 추가한다 좌측하단이 없어서 \n\n\n\n# 3. 표준화잔차 대 적합값의 플롯\n\n# 4. 표준화잔차의 인덱스 플롯"
  },
  {
    "objectID": "Regression_Analysis.html#선형모형-2",
    "href": "Regression_Analysis.html#선형모형-2",
    "title": "Regression Analysis",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:car':\n\n    recode\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n\n\ndata_3.3&lt;-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\nres_lm&lt;-lm(Y~X1+X3,data=data_3.3) #두개의 변수만 의미있다고 가정하고 진행 \n\n# 회귀진단 그래프들\n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(res_lm) #회귀진단 플랏이 나옴 / 총6개임 \n\n\n\nlayout(1) #다시 한개의 플랏만 그리도록 \n\n# 1. 표준화잔차의 정규확률분포\n\nplot(res_lm,2) #QQ-plot y=x 기울기의 직선위에 점들이 있어야 한다 눈대중으로 \n\n\n\n# 2. 표준화잔차 대 각 예측변수들의 산점도\n\nplot(res_lm,3) #이런경우에는 데이터를 추가한다 좌측하단이 없어서 \n\n\n\n#랜덤하게 데이터가 흩어져 있어야 한다\n\n# 내적 표준화잔차\n\nplot(data_3.3$X1,rstandard(res_lm))\n\n\n\nplot(data_3.3$X3,rstandard(res_lm)) #각각의 잔차들이 랜덤하게 잘 퍼져야함 \n\n\n\n# 3. 표준화잔차 대 적합값의 플롯\n\nplot(res_lm,1) #잔차와 적합값은 상관성이 없어야하며 랜덤하게 퍼져야함 \n\n\n\n# 4. 표준화잔차의 인덱스 플롯 \n\nplot(res_lm,5)\n\n\n\ndata_2.4&lt;-read.table(\"All_Data/p029b.txt\",header=T,sep=\"\\t\")\n\nm&lt;-matrix(1:4,ncol=2,byrow=TRUE)  \n\nlayout(m)\n\nplot(Y1~X1, data=data_2.4,pch=19); abline(3,0.5)\n\nplot(Y2~X2, data=data_2.4,pch=19); abline(3,0.5)\n\nplot(Y3~X3, data=data_2.4,pch=19); abline(3,0.5)\n\nplot(Y4~X4, data=data_2.4,pch=19); abline(3,0.5)\n\n\n\nlayout(1)\n\n\n\n\n\nres_lm&lt;-lm(Y1~X1,data=data_2.4) \n\n#1번플랏은 적당히 잘퍼짐,2번플랏은 어느정도 선형성 있음(데이터적어서그런거임)\n\nres_lm&lt;-lm(Y2~X2,data=data_2.4)\n\nres_lm&lt;-lm(Y3~X3,data=data_2.4)\n\nres_lm&lt;-lm(Y4~X4,data=data_2.4)\n\nres_lm\n\n\nCall:\nlm(formula = Y4 ~ X4, data = data_2.4)\n\nCoefficients:\n(Intercept)           X4  \n     3.0017       0.4999  \n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(res_lm) \n\nWarning: not plotting observations with leverage one:\n  8\n\n\n\n\nlayout(1)\n\n\n\n\n\n# 사례: 뉴욕 강 데이터\n\n# agr-농업, forest-숲, rsdntial-주거, comlndl-산업, nitrogen-질소\n\ndata_1.9&lt;-read.table(\"All_Data/p010.txt\",header=T,sep=\"\\t\")\n\nhead(data_1.9)\n\n       River Agr Forest Rsdntial ComIndl Nitrogen\n1      Olean  26     63      1.2    0.29     1.10\n2  Cassadaga  29     57      0.7    0.09     1.01\n3      Oatka  54     26      1.8    0.58     1.90\n4  Neversink   2     84      1.9    1.98     1.00\n5 Hackensack   3     27     29.4    3.11     1.99\n6  Wappinger  19     61      3.4    0.56     1.42\n\nplot(data_1.9[-1],pch=19) #river라는 첫번째 컬럼을 제외하고 진행 \n\n\n\nres_1&lt;-lm(Nitrogen~.,data=data_1.9[-1]) #모든데이터 사용\n\nres_2&lt;-lm(Nitrogen~.,data=data_1.9[-4,-1]) #4번쨰 데이터 제외\n\nres_3&lt;-lm(Nitrogen~.,data=data_1.9[-5,-1]) #5번쨰 데이터 제외 \n\n#회귀계수\n\ndata.frame(all=coef(res_1),\n\n           rm4=coef(res_2),\n\n           rm5=coef(res_3))\n\n                     all          rm4          rm5\n(Intercept)  1.722213529  1.099471134  1.626014115\nAgr          0.005809126  0.010136685  0.002352222\nForest      -0.012967887 -0.007589231 -0.012760349\nRsdntial    -0.007226768 -0.123792917  0.181160986\nComIndl      0.305027765  1.528956204  0.075617570\n\n#p-value\n\ndata.frame(all=coef(summary(res_1))[,4],\n\n           rm4=coef(summary(res_2))[,4],\n\n           rm5=coef(summary(res_3))[,4])\n\n                   all          rm4        rm5\n(Intercept) 0.18316946 0.2477883387 0.05619948\nAgr         0.70462624 0.3717054741 0.80880737\nForest      0.36667966 0.4700975391 0.16975563\nRsdntial    0.83372002 0.0071342930 0.00112280\nComIndl     0.08230952 0.0005512227 0.51774981\n\n#4,5번째 데이터를 각각 뺴고 진행을 해보니 영향을 끼치는 값임을 알수있고\n\n#5번째는 주거 관련해서는 부호를 바꿀 정도로 강력하다 \n\n# 단순선형회구모형\n\nres&lt;-lm(Nitrogen~ComIndl,data=data_1.9)\n\n\n\n\n\nplot(Nitrogen~ComIndl,data=data_1.9,pch=19)\n\nabline(res)\n\n#leverage values 지레값\n\np_ii&lt;-hatvalues(res)\n\nhiegh_leverage&lt;-ifelse(p_ii&gt;2*2/30,data_1.9$River,\"\")\n\nhiegh_leverage #높은 지레값을 가지고 있는 강의 이름을 표시(이는 보기에 편하기 위해서함)\n\n           1            2            3            4            5            6 \n          \"\"           \"\"           \"\"  \"Neversink\" \"Hackensack\"           \"\" \n           7            8            9           10           11           12 \n          \"\"           \"\"           \"\"           \"\"           \"\"           \"\" \n          13           14           15           16           17           18 \n          \"\"           \"\"           \"\"           \"\"           \"\"           \"\" \n          19           20 \n          \"\"           \"\" \n\ntext(data_1.9$ComIndl,data_1.9$Nitrogen-0.1,hiegh_leverage)\n\n\n\n\n\n\n\n\nplot(rstandard(res),pch=19) #2또는 3시그마를 넘으면 특이값이라 함 \n\n\n\n\n\n\n\n\nplot(p_ii,pch=19) #평균의 2배를 기준으로 비교함 \n\nabline(h=2*2/30,col=\"red\") #이보다 높은것이 지레값이 높은것 높은 영향력을 가진것 \n\n\n\n# 단순선형회귀모형\n\nres&lt;-lm(Nitrogen~ComIndl,data=data_1.9)\n\nplot(Nitrogen~ComIndl,data=data_1.9,pch=19)\n\nabline(res)\n\n\n\nres&lt;-lm(Nitrogen~ComIndl,data=data_1.9[-4,-1])\n\nplot(Nitrogen~ComIndl,data=data_1.9[-4,-1],pch=19)\n\nabline(res) #4번째 제외\n\n\n\nres&lt;-lm(Nitrogen~ComIndl,data=data_1.9[-5,-1])\n\nplot(Nitrogen~ComIndl,data=data_1.9[-5,-1],pch=19)\n\nabline(res) #5번쨰 제외\n\n\n\n#이처럼 4,5번을 빼고 진행을 하면 조금더 잘 나타냄\n\nres&lt;-lm(Nitrogen~ComIndl,data=data_1.9[c(-4,-5),-1])\n\nplot(Nitrogen~ComIndl,data=data_1.9[-5,-1],pch=19)\n\nabline(res) #4,5번째 제외 \n\n\n\n\n\n\n\n\n\n\n\nres&lt;-lm(Nitrogen~ComIndl,data=data_1.9)\n\nplot(res,4)\n\n#install.packages(\"olsrr\")\n\nlibrary(olsrr)\n\n\nAttaching package: 'olsrr'\n\n\nThe following object is masked from 'package:datasets':\n\n    rivers\n\n\n\n\nols_plot_cooksd_chart(res)\n\n\n\n\n\n\n\n\nolsrr::ols_plot_dffits(res)\n\n\n\n\n\n\n\n\nolsrr::ols_plot_hadi(res)\n\n\n\n# Residual & Leverage & Cook's distance\n\nplot(res,5) #영향력 관측치를 보기 위한 플랏 \n\n\n\n\n\n\n\n\nolsrr::ols_plot_resid_pot(res) #2.0에 있는 값외에도 x축 0.2이상의 것들도 특이값으로 \n\n\n\n#지레값이 커도 영향력이 없는 애들은 신경 안써도 되나 영향력이 큰애들을 탐색해 보아야 함\n\n\n\n\n\n#특이값이 큰경우 그것이 TRUE데이터면 오류가 있는 경우 수정을 하거나 가중치를 변화를\n\n#하거나 데이터를 수정을 시켜주거나 다시 실험을 하는 등 여러가지 방법을 사용하여.. \n\n## 첨가변수플롯(added-variable plot), 편회귀플롯(partial regression plot)\n\nres&lt;-lm(Nitrogen~ComIndl,data=data_1.9)\n\ncar::avPlots(res,pch=19)\n\n\n\nres&lt;-lm(Nitrogen~.,data=data_1.9[-1])\n\ncar::avPlots(res,pch=19)\n\n\n\n#beta별로 각각의 어떤 변수가 영향력을 많이 주는지 알게하는 함수 \n\n\n\n\n\ndata_4.5&lt;-read.table(\"All_Data/p120.txt\",header=T,sep=\"\\t\")\n\ndim(data_4.5)\n\n[1] 35  4\n\nnames(data_4.5)\n\n[1] \"Hill.Race\" \"Time\"      \"Distance\"  \"Climb\"    \n\nhead(data_4.5)\n\n                    Hill.Race Time Distance Climb\n1 Greenmantle New Year Dash    965      2.5   650\n2                  Carnethy   2901      6.0  2500\n3              Craig Dunain   2019      6.0   900\n4                   Ben Rha   2736      7.5   800\n5                Ben Lomond   3736      8.0  3070\n6                  Goatfell   4393      8.0  2866\n\n# 회전도표 \n\nlibrary(rgl)\n\nwith(data_4.5,plot3d(x=Distance,y=Climb,z=Time))\n\nres&lt;-lm(Time~Distance+Climb,data=data_4.5)\n\nsummary(res) #beta_0가 마이너스여도 신경안씀 관심있는 것은 회귀계수임 \n\n\nCall:\nlm(formula = Time ~ Distance + Climb, data = data_4.5)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-973.0 -427.7  -71.2  142.2 3907.3 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -539.4829   258.1607  -2.090   0.0447 *  \nDistance     373.0727    36.0684  10.343 9.86e-12 ***\nClimb          0.6629     0.1231   5.387 6.44e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 880.5 on 32 degrees of freedom\nMultiple R-squared:  0.9191,    Adjusted R-squared:  0.914 \nF-statistic: 181.7 on 2 and 32 DF,  p-value: &lt; 2.2e-16\n\n#Time=-539.4829+373.0727*Distance+0.6629*Climb \n\n\n### 첨가변수플롯(added-variable plot), 편회귀플롯(partial regression plot)\n\ncar::avPlots(res,pch=19)\n\n\n\n### 성분잔차플롯(component plus residual plots), 편자차플롯(partial residual plot)\n\ncar::crPlots(res,id=T,pch=19) #첨가변수보다 성분잔차를 더 많이 사용 / 비선형여부를 확인\n\n\n\n# 점선에 비해서 분홍선이 크게 떨어져 있지 않아 선형적인 추세를 가지고 있다고 추정이 가능 \n\n### 잠재성-잔차플롯\n\nolsrr::ols_plot_resid_pot(res)\n\n\n\n### Hadi의 영향력 측도\n\nolsrr::ols_plot_hadi(res)\n\n\n\n### Cook의 거리\n\nolsrr::ols_plot_cooksd_chart(res) #전체적은 플롯을 보면서 이상치에 대한 확인을 함 \n\n\n\n# 이러한 값들의 제외 여부는 연구자가 선택해서 진행을 함 \n\n# outlier에 대한 처리를 어떻게 했다고 말을 해야함 \n\n# 보고서는 옆에 사람이 보고 쉽게 따라할 수 있을 정도로 \n\n# 어떤 속성으로 어떻게 진행을 했다라는 것을 중시 코드 보단 결과를 보여줘라 \n\n\n\n\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:olsrr':\n\n    cement\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nres&lt;-lm(Time~Distance+Climb,data=data_4.5)\n\nres\n\n\nCall:\nlm(formula = Time ~ Distance + Climb, data = data_4.5)\n\nCoefficients:\n(Intercept)     Distance        Climb  \n  -539.4829     373.0727       0.6629  \n\nres_rlm&lt;-MASS::rlm(Time~Distance+Climb,data=data_4.5)\n\nres_rlm\n\nCall:\nrlm(formula = Time ~ Distance + Climb, data = data_4.5)\nConverged in 10 iterations\n\nCoefficients:\n (Intercept)     Distance        Climb \n-576.3836570  393.0374614    0.4977894 \n\nDegrees of freedom: 35 total; 32 residual\nScale estimate: 313 \n\nsummary(res)\n\n\nCall:\nlm(formula = Time ~ Distance + Climb, data = data_4.5)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-973.0 -427.7  -71.2  142.2 3907.3 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -539.4829   258.1607  -2.090   0.0447 *  \nDistance     373.0727    36.0684  10.343 9.86e-12 ***\nClimb          0.6629     0.1231   5.387 6.44e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 880.5 on 32 degrees of freedom\nMultiple R-squared:  0.9191,    Adjusted R-squared:  0.914 \nF-statistic: 181.7 on 2 and 32 DF,  p-value: &lt; 2.2e-16\n\nsummary(res_rlm)\n\n\nCall: rlm(formula = Time ~ Distance + Climb, data = data_4.5)\nResiduals:\n     Min       1Q   Median       3Q      Max \n-645.074 -197.082   -2.035  212.266 3942.045 \n\nCoefficients:\n            Value     Std. Error t value  \n(Intercept) -576.3837  105.2774    -5.4749\nDistance     393.0375   14.7086    26.7216\nClimb          0.4978    0.0502     9.9200\n\nResidual standard error: 312.6 on 32 degrees of freedom\n\n#install.packages(\"robustbase\")\n\nlibrary(robustbase)\n\nres_lmrob&lt;-lmrob(Time~Distance+Climb,data=data_4.5)\n\nres_lmrob\n\n\nCall:\nlmrob(formula = Time ~ Distance + Climb, data = data_4.5)\n \\--&gt; method = \"MM\"\nCoefficients:\n(Intercept)     Distance        Climb  \n  -487.3793     398.2784       0.3901  \n\nsummary(res_lmrob)\n\n\nCall:\nlmrob(formula = Time ~ Distance + Climb, data = data_4.5)\n \\--&gt; method = \"MM\"\nResiduals:\n    Min      1Q  Median      3Q     Max \n-650.01 -160.60   23.37  216.11 3875.00 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -487.37929   86.33384  -5.645 3.04e-06 ***\nDistance     398.27843    5.98522  66.544  &lt; 2e-16 ***\nClimb          0.39013    0.04165   9.368 1.09e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nRobust residual standard error: 290.8 \nMultiple R-squared:  0.9868,    Adjusted R-squared:  0.986 \nConvergence in 9 IRWLS iterations\n\nRobustness weights: \n 3 observations c(7,18,33) are outliers with |weight| = 0 ( &lt; 0.0029); \n 2 weights are ~= 1. The remaining 30 ones are summarized as\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.5965  0.9098  0.9623  0.9214  0.9875  0.9990 \nAlgorithmic parameters: \n       tuning.chi                bb        tuning.psi        refine.tol \n        1.548e+00         5.000e-01         4.685e+00         1.000e-07 \n          rel.tol         scale.tol         solve.tol          zero.tol \n        1.000e-07         1.000e-10         1.000e-07         1.000e-10 \n      eps.outlier             eps.x warn.limit.reject warn.limit.meanrw \n        2.857e-03         1.364e-08         5.000e-01         5.000e-01 \n     nResample         max.it       best.r.s       k.fast.s          k.max \n           500             50              2              1            200 \n   maxit.scale      trace.lev            mts     compute.rd fast.s.large.n \n           200              0           1000              0           2000 \n                  psi           subsampling                   cov \n           \"bisquare\"         \"nonsingular\"         \".vcov.avar1\" \ncompute.outlier.stats \n                 \"SM\" \nseed : int(0) \n\n#standard error가 res &gt; res_rlm &gt; res_lmrob 순으로 되어 있음 \n\n# 4장 연습문제 해보기"
  },
  {
    "objectID": "Regression_Analysis.html#regression-analysis",
    "href": "Regression_Analysis.html#regression-analysis",
    "title": "Regression Analysis",
    "section": "",
    "text": "library(dplyr)"
  },
  {
    "objectID": "Regression_Analysis.html#장-질적-예측-변수",
    "href": "Regression_Analysis.html#장-질적-예측-변수",
    "title": "Regression Analysis",
    "section": "",
    "text": "#install.packages(\"fastDummies\")\n\nlibrary(fastDummies)\n\nThank you for using fastDummies!\n\n\nTo acknowledge our work, please cite the package:\n\n\nKaplan, J. & Schlegel, B. (2023). fastDummies: Fast Creation of Dummy (Binary) Columns and Rows from Categorical Variables. Version 1.7.1. URL: https://github.com/jacobkap/fastDummies, https://jacobkap.github.io/fastDummies/."
  },
  {
    "objectID": "Regression_Analysis.html#급료조사-데이터",
    "href": "Regression_Analysis.html#급료조사-데이터",
    "title": "Regression Analysis",
    "section": "",
    "text": "data_5.1&lt;-read.table(\"All_Data/p130.txt\",header=T,sep=\"\\t\")\n\n# S:급료 X:경력 E:교육수준 M:관리(형태) / E,M은 범주형 변수\n\nnames(data_5.1)\n\n[1] \"S\" \"X\" \"E\" \"M\"\n\n# 범주형 질적 변수를 수치형으로 변형시켜서 예측하는데 사용한것이 질적 예측변수이다 \n\n# E_1,E_2,E_3이런식으로 나누어서 0,1로 분류를 한다 (이것이 가변수)\n\n# 더미변수를 만들 경우에는 역행렬의 조건에 의해서 -1개의 변수만 만들면 된다 \n\n# 이는 공산성의 문제또한 있기에 이를 위해서 -1를 한것임 \n\n### 자료형 변경 : 정수 -&gt; 범주\n\ndata_5.1$E&lt;-as.factor(data_5.1$E)\n\ndata_5.1$M&lt;-as.factor(data_5.1$M)\n\nhead(data_5.1)\n\n      S X E M\n1 13876 1 1 1\n2 11608 1 3 0\n3 18701 1 3 1\n4 11283 1 2 0\n5 11767 1 3 0\n6 20872 2 2 1\n\ndata_5.1$E #Levels: 1 2 3이라는 것이 생김 문자로 처리한다는 의미 \n\n [1] 1 3 3 2 3 2 2 1 3 2 1 2 3 1 3 3 2 2 3 1 1 3 2 2 1 2 1 3 1 1 2 3 2 2 1 2 3 1\n[39] 2 2 3 2 2 1 2 1\nLevels: 1 2 3\n\n### 가변수 생성\n\ndata_5.1$E&lt;-factor(as.character(data_5.1$E),levels = c(\"3\",\"1\",\"2\")) \n\n#3번을 베이스 카테고리로 쓰기위한 설정 / 설정을 안하면 베이스는 e_1이 된다\n\ndata_5.1$M&lt;-factor(as.character(data_5.1$M),levels = c(\"0\",\"1\")) \n\ndata_dummy&lt;-dummy_cols(data_5.1,\n\n                       select_columns = c(\"E\",\"M\"),\n\n                       remove_first_dummy = T,\n\n                       remove_selected_columns = T) #첫번째 생성되는 더미 변수를 제거\n\ndata_dummy #가변수의 더미는 n-1개를 하는 것이 역행렬을 위한 것이기에 지워준다 \n\n       S  X E_1 E_2 M_1\n1  13876  1   1   0   1\n2  11608  1   0   0   0\n3  18701  1   0   0   1\n4  11283  1   0   1   0\n5  11767  1   0   0   0\n6  20872  2   0   1   1\n7  11772  2   0   1   0\n8  10535  2   1   0   0\n9  12195  2   0   0   0\n10 12313  3   0   1   0\n11 14975  3   1   0   1\n12 21371  3   0   1   1\n13 19800  3   0   0   1\n14 11417  4   1   0   0\n15 20263  4   0   0   1\n16 13231  4   0   0   0\n17 12884  4   0   1   0\n18 13245  5   0   1   0\n19 13677  5   0   0   0\n20 15965  5   1   0   1\n21 12336  6   1   0   0\n22 21352  6   0   0   1\n23 13839  6   0   1   0\n24 22884  6   0   1   1\n25 16978  7   1   0   1\n26 14803  8   0   1   0\n27 17404  8   1   0   1\n28 22184  8   0   0   1\n29 13548  8   1   0   0\n30 14467 10   1   0   0\n31 15942 10   0   1   0\n32 23174 10   0   0   1\n33 23780 10   0   1   1\n34 25410 11   0   1   1\n35 14861 11   1   0   0\n36 16882 12   0   1   0\n37 24170 12   0   0   1\n38 15990 13   1   0   0\n39 26330 13   0   1   1\n40 17949 14   0   1   0\n41 25685 15   0   0   1\n42 27837 16   0   1   1\n43 18838 16   0   1   0\n44 17483 16   1   0   0\n45 19207 17   0   1   0\n46 19346 20   1   0   0\n\n# 더미를 만든 이후 더미의 모체 변수인 E M을 지워주어야 한다 \n\n### 회귀분석(1) - 가변수\n\nres&lt;-lm(S~.,data = data_dummy)\n\nres \n\n\nCall:\nlm(formula = S ~ ., data = data_dummy)\n\nCoefficients:\n(Intercept)            X          E_1          E_2          M_1  \n    11031.8        546.2      -2996.2        147.8       6883.5  \n\n### 회귀분석(2) - lm()\n\nres_1&lt;-lm(S~.,data=data_5.1)\n\nres_1 \n\n\nCall:\nlm(formula = S ~ ., data = data_5.1)\n\nCoefficients:\n(Intercept)            X           E1           E2           M1  \n    11031.8        546.2      -2996.2        147.8       6883.5  \n\n#더미변수를 많이 쓰기에 factor로 바꾸어주고 분석하면 알아서 더미변수를 만들어서 진행함\n\nsummary(res_1)\n\n\nCall:\nlm(formula = S ~ ., data = data_5.1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1884.60  -653.60    22.23   844.85  1716.47 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11031.81     383.22  28.787  &lt; 2e-16 ***\nX             546.18      30.52  17.896  &lt; 2e-16 ***\nE1          -2996.21     411.75  -7.277 6.72e-09 ***\nE2            147.82     387.66   0.381    0.705    \nM1           6883.53     313.92  21.928  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1027 on 41 degrees of freedom\nMultiple R-squared:  0.9568,    Adjusted R-squared:  0.9525 \nF-statistic: 226.8 on 4 and 41 DF,  p-value: &lt; 2.2e-16\n\n#E_3와 M_0는 Intercept에 포함이 되어있음 그렇기에 베이스 카테고리라고 한다 \n\n#E가 3이고 M이 0이면 대학원이상 일반관리 직급 -&gt; Intercept(Beta_0) + X*Beta_1 = Y\n\n### 표준화잔차 대 경력연수\n\nplot(data_5.1$X, rstandard(res_1),pch=19,xlab=\"범주\",ylab=\"잔차\")\n\n\n\n# 0을 중심으로 잘 퍼져있는가를 확인해야함 \n\n#### 표준화잔차 대 교육수준 - 관리 조합\n\nEM&lt;-paste0(data_5.1$E,data_5.1$M)\n\nplot(EM,rstandard(res_1),pch=19,xlabl=\"범주\",ylab=\"잔차\")\n\nWarning in plot.window(...): \"xlabl\" is not a graphical parameter\n\n\nWarning in plot.xy(xy, type, ...): \"xlabl\" is not a graphical parameter\n\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"xlabl\" is not a\ngraphical parameter\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"xlabl\" is not a\ngraphical parameter\n\n\nWarning in box(...): \"xlabl\" is not a graphical parameter\n\n\nWarning in title(...): \"xlabl\" is not a graphical parameter\n\n\n\n\n### 상호작용 효과(Interaction Effect)\n\nres&lt;-lm(S~X+E+M+E*M,data=data_5.1)\n\nres\n\n\nCall:\nlm(formula = S ~ X + E + M + E * M, data = data_5.1)\n\nCoefficients:\n(Intercept)            X           E1           E2           M1        E1:M1  \n    11203.4        497.0      -1730.7       -349.1       7047.4      -3066.0  \n      E2:M1  \n     1836.5  \n\nsummary(res)\n\n\nCall:\nlm(formula = S ~ X + E + M + E * M, data = data_5.1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-928.13  -46.21   24.33   65.88  204.89 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11203.434     79.065 141.698  &lt; 2e-16 ***\nX             496.987      5.566  89.283  &lt; 2e-16 ***\nE1          -1730.748    105.334 -16.431  &lt; 2e-16 ***\nE2           -349.078     97.568  -3.578 0.000945 ***\nM1           7047.412    102.589  68.695  &lt; 2e-16 ***\nE1:M1       -3066.035    149.330 -20.532  &lt; 2e-16 ***\nE2:M1        1836.488    131.167  14.001  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 173.8 on 39 degrees of freedom\nMultiple R-squared:  0.9988,    Adjusted R-squared:  0.9986 \nF-statistic:  5517 on 6 and 39 DF,  p-value: &lt; 2.2e-16\n\n### 표준화잔차 대 경력연수\n\nplot(data_5.1$X,rstandard(res),pch=19,xlab=\"범주\",ylab=\"잔차\")\n\n\n\n### Cook의 거리\n\nplot(res,4) #33번째 데이터만 제외하고 다시 회귀모형을 생성예정\n\n\n\n### 상호작용 효과 - 관측개체 33 제외 \n\ndata_use&lt;-data_5.1[-33,]\n\nres&lt;-lm(S~X+E+M+E*M,data=data_use)\n\nres\n\n\nCall:\nlm(formula = S ~ X + E + M + E * M, data = data_use)\n\nCoefficients:\n(Intercept)            X           E1           E2           M1        E1:M1  \n    11199.7        498.4      -1741.3       -357.0       7040.6      -3051.8  \n      E2:M1  \n     1997.5  \n\nsummary(res)\n\n\nCall:\nlm(formula = S ~ X + E + M + E * M, data = data_use)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-112.884  -43.636   -5.036   46.622  128.480 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11199.714     30.533 366.802  &lt; 2e-16 ***\nX             498.418      2.152 231.640  &lt; 2e-16 ***\nE1          -1741.336     40.683 -42.803  &lt; 2e-16 ***\nE2           -357.042     37.681  -9.475 1.49e-11 ***\nM1           7040.580     39.619 177.707  &lt; 2e-16 ***\nE1:M1       -3051.763     57.674 -52.914  &lt; 2e-16 ***\nE2:M1        1997.531     51.785  38.574  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 67.12 on 38 degrees of freedom\nMultiple R-squared:  0.9998,    Adjusted R-squared:  0.9998 \nF-statistic: 3.543e+04 on 6 and 38 DF,  p-value: &lt; 2.2e-16\n\n### 표준화잔차 대 경력연수\n\nplot(data_use$X,rstandard(res),pch=19,xlab=\"범주\",ylab=\"잔차\")\n\n\n\n#### 표준화잔차 대 교육수준 - 관리 조합\n\nEM&lt;-paste0(data_use$E,data_use$M)\n\nplot(EM,rstandard(res),pch=19,xlabl=\"범주\",ylab=\"잔차\")\n\nWarning in plot.window(...): \"xlabl\" is not a graphical parameter\n\n\nWarning in plot.xy(xy, type, ...): \"xlabl\" is not a graphical parameter\n\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"xlabl\" is not a\ngraphical parameter\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"xlabl\" is not a\ngraphical parameter\n\n\nWarning in box(...): \"xlabl\" is not a graphical parameter\n\n\nWarning in title(...): \"xlabl\" is not a graphical parameter\n\n\n\n\n#상호 호과가 들어간 이 모형이 더 괜찮은 모양이라고 판단이 된다 \n\n### 기본 급료 추정 - 표 5.6\n\nres&lt;-lm(S~X+E+M+E*M,data=data_use)\n\ndf_new&lt;-data.frame(X=rep(0,6),\n\n                   E=rep(1:3,c(2,2,2)),\n\n                   M=rep(c(0,1),3))\n\n### 가변수 생성 - 분석용\n\ndf_new$E&lt;-factor(as.character(df_new$E),levels = c(\"3\",\"1\",\"2\")) \n\ndf_new$M&lt;-factor(as.character(df_new$M),levels = c(\"0\",\"1\")) \n\ncbind(df_new,predict = predict(res,df_new,interval = \"confidence\"))\n\n  X E M predict.fit predict.lwr predict.upr\n1 0 1 0    9458.378    9395.539    9521.216\n2 0 1 1   13447.195   13382.933   13511.456\n3 0 2 0   10842.672   10789.719   10895.624\n4 0 2 1   19880.782   19814.090   19947.474\n5 0 3 0   11199.714   11137.902   11261.525\n6 0 3 1   18240.294   18182.503   18298.084\n\n# 이를 보고 각 레벨에 따른 차이를 보고 얼마나 나는 지 분석이 가능해야 한다 \n\n# ex) 고졸과 대학원졸의 관리자 직급의 급여의 차이는?(평균적으로)\n\n\n\n\n\n\n\ndata_5.7&lt;-read.table(\"All_Data/p140.txt\",header=T,sep=\"\\t\")\n\nhead(data_5.7)\n\n  TEST RACE JPERF\n1 0.28    1  1.83\n2 0.97    1  4.59\n3 1.25    1  2.97\n4 2.46    1  8.14\n5 2.51    1  8.00\n6 1.17    1  3.30\n\n# 모형 1 - 통합모형 인종간 차이가 없을때\n\nmodel_1&lt;-lm(JPERF~TEST,data=data_5.7)\n\nsummary(model_1) \n\n\nCall:\nlm(formula = JPERF ~ TEST, data = data_5.7)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3558 -0.8798 -0.1897  1.2735  2.3312 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.0350     0.8680   1.192 0.248617    \nTEST          2.3605     0.5381   4.387 0.000356 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.591 on 18 degrees of freedom\nMultiple R-squared:  0.5167,    Adjusted R-squared:  0.4899 \nF-statistic: 19.25 on 1 and 18 DF,  p-value: 0.0003555\n\n# 모형 3 \n\nmodel_3&lt;-lm(JPERF~TEST+RACE+TEST*RACE,data=data_5.7)\n\nsummary(model_3)\n\n\nCall:\nlm(formula = JPERF ~ TEST + RACE + TEST * RACE, data = data_5.7)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0734 -1.0594 -0.2548  1.2830  2.1980 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   2.0103     1.0501   1.914   0.0736 .\nTEST          1.3134     0.6704   1.959   0.0677 .\nRACE         -1.9132     1.5403  -1.242   0.2321  \nTEST:RACE     1.9975     0.9544   2.093   0.0527 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.407 on 16 degrees of freedom\nMultiple R-squared:  0.6643,    Adjusted R-squared:  0.6013 \nF-statistic: 10.55 on 3 and 16 DF,  p-value: 0.0004511\n\n# 인종적인 차이가 있는지 없는지를 확인해야 한다 어떤 모형을 사용할지 \n\n### H_0:gamma=delta=0\n\nanova(model_1,model_3) #model_3가 FM(완전모형)\n\nAnalysis of Variance Table\n\nModel 1: JPERF ~ TEST\nModel 2: JPERF ~ TEST + RACE + TEST * RACE\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     18 45.568                              \n2     16 31.655  2    13.913 3.5161 0.05424 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#P-value(0.05424)&lt;0.05이기에 H_0 이기에 모형1을 선택하는 것이 옳다고 판단(그러나 확신x)\n\n#서로의 R-squared롤 보면 model_3가 더 좋음 / ANOVA는 참고용 절대적이지는 않다 \n\n\n\n\n\nplot(data_5.7$TEST,rstandard(model_1),\n\n     pch=19,xlab=\"검사점수\",ylab=\"잔차\",\n\n     main=\"그림 5.7 - 표준화잔차 대 검사점수 : 모형 1\")\n\n\n\n\n\n\n\n\nplot(data_5.7$TEST,rstandard(model_3),\n\n     pch=19,xlab=\"검사점수\",ylab=\"잔차\",\n\n     main=\"그림 5.8 - 표준화잔차 대 검사점수 : 모형 3\")\n\n\n\n# 통계에서 나오는 결과는 결정의 보조수단이지 절대적이지 않아 \n\n# 3번 모형을 선택한다고 결정한다고 진행 \n\nsummary(model_3) # Multiple R-squared:  0.6643\n\n\nCall:\nlm(formula = JPERF ~ TEST + RACE + TEST * RACE, data = data_5.7)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0734 -1.0594 -0.2548  1.2830  2.1980 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   2.0103     1.0501   1.914   0.0736 .\nTEST          1.3134     0.6704   1.959   0.0677 .\nRACE         -1.9132     1.5403  -1.242   0.2321  \nTEST:RACE     1.9975     0.9544   2.093   0.0527 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.407 on 16 degrees of freedom\nMultiple R-squared:  0.6643,    Adjusted R-squared:  0.6013 \nF-statistic: 10.55 on 3 and 16 DF,  p-value: 0.0004511\n\n\n\n\n\n\nplot(data_5.7$RACE,rstandard(model_1),\n\n     pch=19,xlab=\"검사점수\",ylab=\"잔차\",\n\n     main=\"그림 5.9 - 표준화잔차 대 검사점수 : 모형 1\")\n\n\n\n# 분리된 회귀분석 결과\n\ndata_5.7_R1&lt;-subset(data_5.7,RACE==1)\n\nmodel_R1&lt;-lm(JPERF~TEST,data=data_5.7_R1)\n\nsummary(model_R1) #소수민족\n\n\nCall:\nlm(formula = JPERF ~ TEST, data = data_5.7_R1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0734 -0.6267 -0.2548  1.1624  1.5394 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.09712    1.03519   0.094 0.927564    \nTEST         3.31095    0.62411   5.305 0.000724 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.292 on 8 degrees of freedom\nMultiple R-squared:  0.7787,    Adjusted R-squared:  0.751 \nF-statistic: 28.14 on 1 and 8 DF,  p-value: 0.0007239\n\ndata_5.7_R0&lt;-subset(data_5.7,RACE==0)\n\nmodel_R0&lt;-lm(JPERF~TEST,data=data_5.7_R0)\n\nsummary(model_R0) #백인\n\n\nCall:\nlm(formula = JPERF ~ TEST, data = data_5.7_R0)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8599 -1.0663 -0.3061  1.0957  2.1980 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   2.0103     1.1291   1.780    0.113\nTEST          1.3134     0.7208   1.822    0.106\n\nResidual standard error: 1.512 on 8 degrees of freedom\nMultiple R-squared:  0.2933,    Adjusted R-squared:  0.205 \nF-statistic:  3.32 on 1 and 8 DF,  p-value: 0.1059\n\n# 통합모형에서 나온 각각의 회귀식이 통합모형에서 나온것과 동일함 따라서 인종별로 나누어서\n\n# 진행할 필요없이 통일모형을 사용해서 진행을 하면 된다(이는 데이터셋을 나눈경우와 동일함)\n\n\n\n\n\nplot(data_5.7_R1$TEST,rstandard(model_R1),\n\n     pch=19,xlab=\"검사점수\",ylab=\"잔차\",\n\n     main=\"그림 5.10 - 표준화잔차 대 검사점수 : 모형 1. 소수민족만만\")\n\n\n\n\n\n\n\n\nplot(data_5.7_R0$TEST,rstandard(model_R0),\n\n     pch=19,xlab=\"검사점수\",ylab=\"잔차\",\n\n     main=\"그림 5.11 - 표준화잔차 대 검사점수 : 모형 1. 백인만\")\n\n\n\n# 적절한 합격점수의 결정 - 소수민족\n\n# 고용전 검사점수의 합격점에 대한 95% 신뢰구간\n\nym&lt;-4\n\nxm&lt;-(ym-0.09712)/3.31095\n\ns&lt;-1.292\n\nn&lt;-10\n\nt&lt;-qt(1-0.05/2,8)\n\nc(xm-(t*s/n)/3.31095, xm+(t*s/n)/3.31095) #신뢰구간\n\n[1] 1.088795 1.268764\n\n\n\n\n\n\nmodel_3&lt;-lm(JPERF~TEST+RACE,data=data_5.7)\n\nsummary(model_3)\n\n\nCall:\nlm(formula = JPERF ~ TEST + RACE, data = data_5.7)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7872 -1.0370 -0.2095  0.9198  2.3645 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.6120     0.8870   0.690 0.499578    \nTEST          2.2988     0.5225   4.400 0.000391 ***\nRACE          1.0276     0.6909   1.487 0.155246    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.54 on 17 degrees of freedom\nMultiple R-squared:  0.5724,    Adjusted R-squared:  0.5221 \nF-statistic: 11.38 on 2 and 17 DF,  p-value: 0.0007312\n\n#Intercept = BETA_0 / TEST = BETA_1 / RACE = gamma\n\n## H_0: gamma=0\n\nanova(model_1,model_3) #p-value(0.1552)&gt;0.05 이기에 H_0은 참이다//gamma=0\n\nAnalysis of Variance Table\n\nModel 1: JPERF ~ TEST\nModel 2: JPERF ~ TEST + RACE\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     18 45.568                           \n2     17 40.322  1    5.2468 2.2121 0.1552\n\n#기울기가 같고 절편이 다른 모형은 아니라고 데이터가 이야기하고 있다 \n\n# 소수민족(RACE=1): (0.6120+1.0276)+2.2988*TEST = 1.6396+2.2988*TEST\n\n# 백인(RACE=0): (0.6120)+2.2988*TEST\n\n\n\n\n\nmodel_3&lt;-lm(JPERF~TEST+I(TEST*RACE),data=data_5.7)\n\nsummary(model_3) #I()를 하면 교호작용하는 것만 보이게 하려고 없으면 RACE항이 자동추가됨\n\n\nCall:\nlm(formula = JPERF ~ TEST + I(TEST * RACE), data = data_5.7)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.41100 -0.88871 -0.03359  0.97720  2.44440 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)      1.1211     0.7804   1.437  0.16900   \nTEST             1.8276     0.5356   3.412  0.00332 **\nI(TEST * RACE)   0.9161     0.3972   2.306  0.03395 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.429 on 17 degrees of freedom\nMultiple R-squared:  0.6319,    Adjusted R-squared:  0.5886 \nF-statistic: 14.59 on 2 and 17 DF,  p-value: 0.0002045\n\n## H_0: delta=0\n\nanova(model_1,model_3) #p-value(0.03395)&lt;0.05보다 작기에 delta항은 필요한 변수라는 사실 \n\nAnalysis of Variance Table\n\nModel 1: JPERF ~ TEST\nModel 2: JPERF ~ TEST + I(TEST * RACE)\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     18 45.568                              \n2     17 34.708  1    10.861 5.3196 0.03395 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#Intercept = BETA_0 / TEST = BETA_1 / I(TEST*RACE) = delta"
  },
  {
    "objectID": "Regression_Analysis.html#장-변수변환-transformation-of-varibables",
    "href": "Regression_Analysis.html#장-변수변환-transformation-of-varibables",
    "title": "Regression Analysis",
    "section": "",
    "text": "library(dplyr)\n\n\n\n\ndata_6.2&lt;-read.table(\"All_Data/p168.txt\",header=T,sep=\"\\t\")\n\nhead(data_6.2)\n\n  t N_t\n1 1 355\n2 2 211\n3 3 197\n4 4 166\n5 5 142\n6 6 106\n\n\n\n\n\n\nplot(N_t~t,data=data_6.2,pch=19)\n\n\n\n\n\n\n\n\nres&lt;-lm(N_t~t,data=data_6.2)\n\nsummary(res)\n\n\nCall:\nlm(formula = N_t ~ t, data = data_6.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-43.867 -23.599  -9.652  10.223 114.883 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   259.58      22.73  11.420 3.78e-08 ***\nt             -19.46       2.50  -7.786 3.01e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 41.83 on 13 degrees of freedom\nMultiple R-squared:  0.8234,    Adjusted R-squared:  0.8098 \nF-statistic: 60.62 on 1 and 13 DF,  p-value: 3.006e-06\n\n\n\n\n\n\nplot(data_6.2$t,rstandard(res),pch=19) # 표준화 잔차의 플랏\n\n\n\n# 잔차가 랜덤하게 잘 퍼져있어야 하는데 적절한 회귀모형이 아니라는 사실이 나옴\n\n\n\n\n\n\n\n\nplot(log(N_t)~t,data=data_6.2,pch=19)\n\n\n\n# 박테리아의 수에 로그를 취하니 선형성이 보인다 \n\nres&lt;-lm(log(N_t)~t,data=data_6.2)\n\nsummary(res)\n\n\nCall:\nlm(formula = log(N_t) ~ t, data = data_6.2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.18445 -0.06189  0.01253  0.05201  0.20021 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.973160   0.059778   99.92  &lt; 2e-16 ***\nt           -0.218425   0.006575  -33.22 5.86e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.11 on 13 degrees of freedom\nMultiple R-squared:  0.9884,    Adjusted R-squared:  0.9875 \nF-statistic:  1104 on 1 and 13 DF,  p-value: 5.86e-14\n\nplot(data_6.2$t,rstandard(res),pch=19)\n\n\n\n# 로그를 취해주니 잔차가 랜덤하게 이루어져 있음 \n\n# n_0에 대한 추론\n\nexp(5.973160)\n\n[1] 392.7448\n\nexp(coef(res)[1]) #로그를 취해주고 하는 부분이 잘 이해가 안됨 \n\n(Intercept) \n   392.7449 \n\nexp(coef(res)[1]-0.0588/2) #불편추정량 구하기 (참고용)\n\n(Intercept) \n   381.3663 \n\n\n\n\n\n\ndata_6.6&lt;-read.table(\"All_Data/p174.txt\",header=T,sep=\"\\t\")\n\ndata_6.6 #운항률과 사고 발생 건수에 대한 자료\n\n   Y      N\n1 11 0.0950\n2  7 0.1920\n3  7 0.0750\n4 19 0.2078\n5  9 0.1382\n6  4 0.0540\n7  3 0.1292\n8  1 0.0503\n9  3 0.0629\n\nplot(Y~N,data=data_6.6,pch=19) #잔차의 분산이 계속 커지는 효과를 보임 \n\n\n\nres_1&lt;-lm(Y~N,data=data_6.6)\n\nsummary(res_1)\n\n\nCall:\nlm(formula = Y ~ N, data = data_6.6)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.3351 -2.1281  0.1605  2.2670  5.6382 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  -0.1402     3.1412  -0.045   0.9657  \nN            64.9755    25.1959   2.579   0.0365 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.201 on 7 degrees of freedom\nMultiple R-squared:  0.4872,    Adjusted R-squared:  0.4139 \nF-statistic:  6.65 on 1 and 7 DF,  p-value: 0.03654\n\n# 표준화잔차 대 N의 플롯 / 그림 6.11\n\nplot(data_6.6$N,rstandard(res_1),pch=19) #등분산성에 만족하지 못하는 모형을 보인다 \n\n\n\n# N에 대한 sqrt(Y)의 회귀\n\n# N에 대한 Y의 회귀\n\nres_2&lt;-lm(sqrt(Y)~N,data=data_6.6)\n\nsummary(res_2)\n\n\nCall:\nlm(formula = sqrt(Y) ~ N, data = data_6.6)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.9690 -0.7655  0.1906  0.5874  1.0211 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   1.1692     0.5783   2.022   0.0829 .\nN            11.8564     4.6382   2.556   0.0378 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7733 on 7 degrees of freedom\nMultiple R-squared:  0.4828,    Adjusted R-squared:  0.4089 \nF-statistic: 6.535 on 1 and 7 DF,  p-value: 0.03776\n\n\n\n\n\n\nplot(data_6.6$N,rstandard(res_2),pch=19) #0을 중심으로 잔차가 보다 잘 퍼져있음이 보임 \n\n\n\n\n\n\n\n\ndata_6.9&lt;-read.table(\"All_Data/p176.txt\",header=T,sep=\"\\t\")\n\ndata_6.9\n\n      X   Y\n1   294  30\n2   247  32\n3   267  37\n4   358  44\n5   423  47\n6   311  49\n7   450  56\n8   534  62\n9   438  68\n10  697  78\n11  688  80\n12  630  84\n13  709  88\n14  627  97\n15  615 100\n16  999 109\n17 1022 114\n18 1015 117\n19  700 106\n20  850 128\n21  980 130\n22 1025 160\n23 1021  97\n24 1200 180\n25 1250 112\n26 1500 210\n27 1650 135\n\n# Y 대 X의 플로\n\nplot(Y~X,data=data_6.9,pch=19,main=\"그림 6.13\")\n\n\n\n# X에 대한 Y의 회귀\n\nres_1&lt;-lm(Y~X,data=data_6.9)\n\nsummary(res_1)\n\n\nCall:\nlm(formula = Y ~ X, data = data_6.9)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-53.294  -9.298  -5.579  14.394  39.119 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 14.44806    9.56201   1.511    0.143    \nX            0.10536    0.01133   9.303 1.35e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.73 on 25 degrees of freedom\nMultiple R-squared:  0.7759,    Adjusted R-squared:  0.7669 \nF-statistic: 86.54 on 1 and 25 DF,  p-value: 1.35e-09\n\n# 표준화잔차 대 X의 플롯\n\nplot(data_6.9$X, rstandard(res_1),pch=19,main = \"그림 6.14\")\n\n\n\n\n\n\n\n\n# 변환된 Y/X와 1/X를 적합한 회귀\n\ndata_6.9_1&lt;-data.frame(Y=data_6.9$Y/data_6.9$X,\n\n                       X=1/data_6.9$X)\n\nres_2&lt;-lm(Y~X,data=data_6.9_1)\n\nsummary(res_2) #지금은 변환하고 프라임 값들의 추정치가 나온것이기에 원래 회귀식으로 돌아가야함 \n\n\nCall:\nlm(formula = Y ~ X, data = data_6.9_1)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.041477 -0.013852 -0.004998  0.024671  0.035427 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.120990   0.008999  13.445 6.04e-13 ***\nX           3.803296   4.569745   0.832    0.413    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02266 on 25 degrees of freedom\nMultiple R-squared:  0.02696,   Adjusted R-squared:  -0.01196 \nF-statistic: 0.6927 on 1 and 25 DF,  p-value: 0.4131\n\n# 본래 변환시 X를 나눴기에 X를 곱해준다 -&gt; B_0,B_1의 추정값이 바뀐다 서로 \n\nplot(data_6.9_1$X, rstandard(res_2),pch=19,\n\n     xlab=\"1/X\",ylab=\"잔차\",main = \"[그림 6.15]\")\n\n\n\n\n\n\n\n\nwt&lt;-1/data_6.9$X^2 #가중값\n\nres_3&lt;-lm(Y~X,data=data_6.9,weights = wt)\n\nsummary(res_3) #결과는 위의 6.6과 동일한 값이 나옴 \n\n\nCall:\nlm(formula = Y ~ X, data = data_6.9, weights = wt)\n\nWeighted Residuals:\n      Min        1Q    Median        3Q       Max \n-0.041477 -0.013852 -0.004998  0.024671  0.035427 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 3.803296   4.569745   0.832    0.413    \nX           0.120990   0.008999  13.445 6.04e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02266 on 25 degrees of freedom\nMultiple R-squared:  0.8785,    Adjusted R-squared:  0.8737 \nF-statistic: 180.8 on 1 and 25 DF,  p-value: 6.044e-13\n\n\n\n\n\n\ndata_6.9&lt;-read.table(\"All_Data/p176.txt\",header=T,sep=\"\\t\")\n\nhead(data_6.9)\n\n    X  Y\n1 294 30\n2 247 32\n3 267 37\n4 358 44\n5 423 47\n6 311 49\n\n# log(Y) 대 X의 산점도\n\nplot((Y)~X,data=data_6.9,pch=19,main=\"식 6.9\")\n\n\n\nplot(log(Y)~X,data=data_6.9,pch=19,main=\"그림 6.16\")\n\n\n\nres_4&lt;-lm(log(Y)~X,data=data_6.9)\n\nsummary(res_4)\n\n\nCall:\nlm(formula = log(Y) ~ X, data = data_6.9)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.59648 -0.16578  0.00244  0.17481  0.34964 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 3.5150232  0.1110670  31.648  &lt; 2e-16 ***\nX           0.0012041  0.0001316   9.153 1.85e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2524 on 25 degrees of freedom\nMultiple R-squared:  0.7702,    Adjusted R-squared:  0.761 \nF-statistic: 83.77 on 1 and 25 DF,  p-value: 1.855e-09\n\n# X에 대한 log(Y)의 회귀로 부터 얻은 표준화잔차플롯\n\nplot(data_6.9$X,rstandard(res_4),pch=19,\n\n     xlab=\"X\",ylab=\"잔차\",main=\"그림 6.17\")\n\n\n\n# X와 X^2에 대한 log(Y)의 회귀\n\ndf&lt;-data.frame(log_Y=log(data_6.9$Y),\n\n               X=data_6.9$X,\n\n               X2=data_6.9$X^2)\n\nres_5&lt;-lm(log_Y~X+X2,data=df) #그러나 이러한 과정은 필요업고 아래방식으로..\n\nres_5&lt;-lm(log(Y)~X+I(X^2),data=data_6.9)\n\nsummary(res_5)\n\n\nCall:\nlm(formula = log(Y) ~ X + I(X^2), data = data_6.9)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.30589 -0.11705 -0.02707  0.17593  0.30657 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.852e+00  1.566e-01  18.205 1.50e-15 ***\nX            3.113e-03  3.989e-04   7.803 4.90e-08 ***\nI(X^2)      -1.102e-06  2.238e-07  -4.925 5.03e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1817 on 24 degrees of freedom\nMultiple R-squared:  0.8857,    Adjusted R-squared:  0.8762 \nF-statistic: 92.98 on 2 and 24 DF,  p-value: 4.976e-12\n\n# 표준화잔차 대 적합값 플롯 : X와 X^2에 대한 log(Y)의 회귀\n\nplot(fitted(res_5),rstandard(res_5),pch=19,\n\n     xlab=\"X\",ylab=\"잔차\",main=\"그림 6.18\")\n\n\n\n# 표준화잔차 대 X의 플롯 : X와 X^2에 대한 log(Y)의 회귀\n\nplot(data_6.9$X,rstandard(res_5),pch=19,\n\n     xlab=\"X\",ylab=\"잔차\",main=\"그림 6.19\")\n\n\n\n# 표준화잔차 대 X^2의 플롯 : X와 X^2에 대한 log(Y)의 회귀\n\nplot(data_6.9$X^2,rstandard(res_5),pch=19,\n\n     xlab=\"X\",ylab=\"잔차\",main=\"그림 6.20\") #2차항을 추가함으로서 더 잘 피팅됨 \n\n\n\n# 7장은 스킵 / 8장 스킵\n\n\n\n\n\ndata_9.1&lt;-read.table(\"All_Data/p236.txt\",header=T,sep=\"\\t\")\n\nhead(data_9.1)\n\n      ACHV      FAM     PEER   SCHOOL\n1 -0.43148  0.60814  0.03509  0.16607\n2  0.79969  0.79369  0.47924  0.53356\n3 -0.92467 -0.82630 -0.61951 -0.78635\n4 -2.19081 -1.25310 -1.21675 -1.04076\n5 -2.84818  0.17399 -0.18517  0.14229\n6 -0.66233  0.20246  0.12764  0.27311\n\nres&lt;-lm(ACHV~.,data_9.1)\n\nsummary(res)\n\n\nCall:\nlm(formula = ACHV ~ ., data = data_9.1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.2096 -1.3934 -0.2947  1.1415  4.5881 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -0.06996    0.25064  -0.279    0.781\nFAM          1.10126    1.41056   0.781    0.438\nPEER         2.32206    1.48129   1.568    0.122\nSCHOOL      -2.28100    2.22045  -1.027    0.308\n\nResidual standard error: 2.07 on 66 degrees of freedom\nMultiple R-squared:  0.2063,    Adjusted R-squared:  0.1702 \nF-statistic: 5.717 on 3 and 66 DF,  p-value: 0.001535\n\n#F-statistic: 5.717 on 3 and 66 DF, p-value: 0.001535\n\n#이는 의미있는 beta가 존재한다라는 의미\n\n#그러나 각 계수들의 회귀계수를 보니 모두 0이라는 결과가 나옴 이는 다중공선성이다\n\n# Correlation panel\n\npanel.cor&lt;-function(x,y){\n\npar(usr=c(0,1,0,1))\n\nr&lt;-round(cor(x,y),digits = 3)\n\ntext(0.5,0.5,r,cex=1.5)\n\n}\n\npairs(data_9.1[-1],lower.panel = panel.cor)\n\n\n\n\n\n\n\n\nplot(fitted(res),rstandard(res),pch=19,\n\nxlab=\"예측값\",ylab=\"잔차\",main=\"[그림 9.1]\")\n\n\n\n#이는 회귀모형에 동시에 들어가면 안된다라는 의미 (제거가 필요)\n\n\n\n\n\ndata_9.5&lt;-read.table(\"All_Data/p241.txt\",header=T,sep=\"\\t\")\n\nhead(data_9.5)\n\n  YEAR IMPORT DOPROD STOCK CONSUM\n1   49   15.9  149.3   4.2  108.1\n2   50   16.4  161.2   4.1  114.8\n3   51   19.0  171.5   3.1  123.2\n4   52   19.1  175.5   3.1  126.9\n5   53   18.8  180.8   1.1  132.1\n6   54   20.4  190.7   2.2  137.7\n\n### 산점도\n\npairs(data_9.5[-1],lower.panel = panel.cor)\n\n\n\n# 회귀분석(1) : 데이터 1949~1966\n\nres&lt;-lm(IMPORT~.,data_9.5[-1])\n\nsummary(res) #다중공선성이 존재한다고 예측을 일단함\n\n\nCall:\nlm(formula = IMPORT ~ ., data = data_9.5[-1])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7208 -1.8354 -0.3479  1.2973  4.1008 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -19.7251     4.1253  -4.782 0.000293 ***\nDOPROD        0.0322     0.1869   0.172 0.865650    \nSTOCK         0.4142     0.3223   1.285 0.219545    \nCONSUM        0.2427     0.2854   0.851 0.409268    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.258 on 14 degrees of freedom\nMultiple R-squared:  0.973, Adjusted R-squared:  0.9673 \nF-statistic: 168.4 on 3 and 14 DF,  p-value: 3.212e-11\n\n\n\n\n\n\nplot(1:nrow(data_9.5),rstandard(res),\n\npch=19,type=\"b\",\n\nxla=\"번호\",ylab=\"잔차\",main=\"[그림 9.3]\")\n\n\n\n# 회귀분석(2) : 데이터 1949~1959\n\ndata_use&lt;-subset(data_9.5,YEAR&lt;=59)\n\nres&lt;-lm(IMPORT~.,data_use[-1])\n\nsummary(res)\n\n\nCall:\nlm(formula = IMPORT ~ ., data = data_use[-1])\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.52367 -0.38953  0.05424  0.22644  0.78313 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -10.12799    1.21216  -8.355  6.9e-05 ***\nDOPROD       -0.05140    0.07028  -0.731 0.488344    \nSTOCK         0.58695    0.09462   6.203 0.000444 ***\nCONSUM        0.28685    0.10221   2.807 0.026277 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4889 on 7 degrees of freedom\nMultiple R-squared:  0.9919,    Adjusted R-squared:  0.9884 \nF-statistic: 285.6 on 3 and 7 DF,  p-value: 1.112e-07\n\n\n\n\n\n\nplot(1:nrow(data_use),rstandard(res),\n\npch=19,type=\"b\",\n\nxla=\"번호\",ylab=\"잔차\",main=\"[그림 9.4]\")\n\n\n\n\n\n\n\n\n# 분산확대인자\n\nlibrary(olsrr)\n\n# 교육기회 균등(EEO)\n\nres_9.1&lt;-lm(ACHV~.,data_9.1)\n\nsummary(res_9.1)\n\n\nCall:\nlm(formula = ACHV ~ ., data = data_9.1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.2096 -1.3934 -0.2947  1.1415  4.5881 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -0.06996    0.25064  -0.279    0.781\nFAM          1.10126    1.41056   0.781    0.438\nPEER         2.32206    1.48129   1.568    0.122\nSCHOOL      -2.28100    2.22045  -1.027    0.308\n\nResidual standard error: 2.07 on 66 degrees of freedom\nMultiple R-squared:  0.2063,    Adjusted R-squared:  0.1702 \nF-statistic: 5.717 on 3 and 66 DF,  p-value: 0.001535\n\n#car::vif(res_9.1)\n\nolsrr::ols_vif_tol(res_9.1) # 10보다 크면 유의한 영향을 준다 제거필요?\n\n  Variables  Tolerance      VIF\n1       FAM 0.02660945 37.58064\n2      PEER 0.03309981 30.21166\n3    SCHOOL 0.01202567 83.15544\n\n\n\n\n\n\ndata_use&lt;-subset(data_9.5,YEAR&lt;=59)\n\nres_9.5&lt;-lm(IMPORT~.,data_use[-1])\n\n#car::vif(res_9.1)\n\nolsrr::ols_vif_tol(res_9.5) #작으면 새로운 변수를 만들거나 제거를 통해서 진행\n\n  Variables   Tolerance        VIF\n1    DOPROD 0.005376417 185.997470\n2     STOCK 0.981441657   1.018909\n3    CONSUM 0.005373166 186.110015\n\n# 상태 지수\n\ncnd.idx&lt;-olsrr::ols_eigen_cindex(res_9.1)\n\nround(cnd.idx,4)\n\n  Eigenvalue Condition Index intercept    FAM   PEER SCHOOL\n1     2.9547          1.0000    0.0005 0.0030 0.0037 0.0014\n2     0.9974          1.7211    0.9756 0.0000 0.0000 0.0000\n3     0.0400          8.5996    0.0004 0.3068 0.4428 0.0008\n4     0.0079         19.2826    0.0235 0.6903 0.5535 0.9978\n\ncnd.idx&lt;-olsrr::ols_eigen_cindex(res_9.5)\n\nround(cnd.idx,4)\n\n  Eigenvalue Condition Index intercept DOPROD  STOCK CONSUM\n1     3.8384          1.0000    0.0010 0.0000 0.0109 0.0000\n2     0.1484          5.0863    0.0053 0.0001 0.9385 0.0001\n3     0.0132         17.0732    0.7743 0.0015 0.0330 0.0011\n4     0.0001        265.4613    0.2193 0.9984 0.0175 0.9989"
  },
  {
    "objectID": "Regression_Analysis.html#장---공선형-데이터의-처리",
    "href": "Regression_Analysis.html#장---공선형-데이터의-처리",
    "title": "Regression Analysis",
    "section": "",
    "text": "data_9.5&lt;-read.table(\"All_Data/p241.txt\",header=T,sep=\"\\t\")\n\nhead(data_9.5)\n\n  YEAR IMPORT DOPROD STOCK CONSUM\n1   49   15.9  149.3   4.2  108.1\n2   50   16.4  161.2   4.1  114.8\n3   51   19.0  171.5   3.1  123.2\n4   52   19.1  175.5   3.1  126.9\n5   53   18.8  180.8   1.1  132.1\n6   54   20.4  190.7   2.2  137.7\n\n# 회귀분석(2) : 데이터 1949~1959\n\ndata_use&lt;-subset(data_9.5,YEAR&lt;=59)\n\nres&lt;-lm(IMPORT~.,data_use[-1]) #1열 제거 \n\nsummary(res)\n\n\nCall:\nlm(formula = IMPORT ~ ., data = data_use[-1])\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.52367 -0.38953  0.05424  0.22644  0.78313 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -10.12799    1.21216  -8.355  6.9e-05 ***\nDOPROD       -0.05140    0.07028  -0.731 0.488344    \nSTOCK         0.58695    0.09462   6.203 0.000444 ***\nCONSUM        0.28685    0.10221   2.807 0.026277 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4889 on 7 degrees of freedom\nMultiple R-squared:  0.9919,    Adjusted R-squared:  0.9884 \nF-statistic: 285.6 on 3 and 7 DF,  p-value: 1.112e-07\n\nhead(data_use[-c(1:2)])\n\n  DOPROD STOCK CONSUM\n1  149.3   4.2  108.1\n2  161.2   4.1  114.8\n3  171.5   3.1  123.2\n4  175.5   3.1  126.9\n5  180.8   1.1  132.1\n6  190.7   2.2  137.7\n\n### 주성분(principle component)\n\npc&lt;-prcomp(data_use[-c(1:2)],scale.=T)\n\npc$rotation\n\n              PC1         PC2          PC3\nDOPROD 0.70633041 -0.03568867 -0.706982083\nSTOCK  0.04350059  0.99902908 -0.006970795\nCONSUM 0.70654444 -0.02583046  0.707197102\n\npc$x #변화된 새로운 변수가 저장된곳\n\n          PC1         PC2         PC3\n1  -2.1258872  0.63865815 -0.02072230\n2  -1.6189273  0.55553922 -0.07111317\n3  -1.1151675 -0.07297970 -0.02173008\n4  -0.8942966 -0.08236998  0.01081318\n5  -0.6442081 -1.30668523  0.07258248\n6  -0.1903514 -0.65914745  0.02655252\n7   0.3596219 -0.74367447  0.04278124\n8   0.9718018  1.35405877  0.06286252\n9   1.5593159  0.96404558  0.02357446\n10  1.7669951  1.01521706 -0.04498818\n11  1.9311034 -1.66266195 -0.08061267\n\n### 주성분회귀 - 원래데이터를 주성분분석을 통해 새로운 데이터를 생성 이를 가지고 회귀 \n\ndf&lt;-data.frame(IMPORT = scale(data_use$IMPORT),\n\n               pc$x)\n\ndf\n\n       IMPORT        PC1         PC2         PC3\n1  -1.3185185 -2.1258872  0.63865815 -0.02072230\n2  -1.2084753 -1.6189273  0.55553922 -0.07111317\n3  -0.6362502 -1.1151675 -0.07297970 -0.02173008\n4  -0.6142416 -0.8942966 -0.08236998  0.01081318\n5  -0.6802675 -0.6442081 -1.30668523  0.07258248\n6  -0.3281290 -0.1903514 -0.65914745  0.02655252\n7   0.1780700  0.3596219 -0.74367447  0.04278124\n8   1.0143989  0.9718018  1.35405877  0.06286252\n9   1.3665374  1.5593159  0.96404558  0.02357446\n10  1.2564942  1.7669951  1.01521706 -0.04498818\n11  0.9703816  1.9311034 -1.66266195 -0.08061267\n\nres&lt;-lm(IMPORT~.,data=df)\n\nsummary(res) #유의한 1,2번째 계수들만 사용하고 3번째 것은 없이 해도 되겠다..\n\n\nCall:\nlm(formula = IMPORT ~ ., data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.11525 -0.08573  0.01194  0.04984  0.17236 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 8.900e-16  3.244e-02   0.000 1.000000    \nPC1         6.900e-01  2.406e-02  28.673 1.61e-08 ***\nPC2         1.913e-01  3.406e-02   5.617 0.000801 ***\nPC3         1.160e+00  6.559e-01   1.768 0.120376    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1076 on 7 degrees of freedom\nMultiple R-squared:  0.9919,    Adjusted R-squared:  0.9884 \nF-statistic: 285.6 on 3 and 7 DF,  p-value: 1.112e-07\n\n#중간고사 이후의것이 주가나오겠지만 앞에것도 알아야함 연습문제에서 나올것 예상"
  },
  {
    "objectID": "Regression_Analysis.html#장-1",
    "href": "Regression_Analysis.html#장-1",
    "title": "Regression Analysis",
    "section": "",
    "text": "# 11.10 - \n\ndata_3.3&lt;-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\nhead(data_3.3)\n\n   Y X1 X2 X3 X4 X5 X6\n1 43 51 30 39 61 92 45\n2 63 64 51 54 63 73 47\n3 71 70 68 69 76 86 48\n4 61 63 45 47 54 84 35\n5 81 78 56 66 71 83 47\n6 43 55 49 44 54 49 34\n\n# 분산확대 인자(VIF) : 10초과 &gt; 심각한 공산성의 문제가 있음\n\nres&lt;-lm(Y~.,data=data_3.3)\n\nsummary(res)\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.9418  -4.3555   0.3158   5.5425  11.5990 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10.78708   11.58926   0.931 0.361634    \nX1           0.61319    0.16098   3.809 0.000903 ***\nX2          -0.07305    0.13572  -0.538 0.595594    \nX3           0.32033    0.16852   1.901 0.069925 .  \nX4           0.08173    0.22148   0.369 0.715480    \nX5           0.03838    0.14700   0.261 0.796334    \nX6          -0.21706    0.17821  -1.218 0.235577    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.068 on 23 degrees of freedom\nMultiple R-squared:  0.7326,    Adjusted R-squared:  0.6628 \nF-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05\n\nolsrr::ols_vif_tol(res) #10을 넘는 것이 없기에 공산성의 문제는 없다 \n\n  Variables Tolerance      VIF\n1        X1 0.3749447 2.667060\n2        X2 0.6246520 1.600891\n3        X3 0.4403263 2.271043\n4        X4 0.3248624 3.078226\n5        X5 0.8142600 1.228109\n6        X6 0.5124025 1.951591\n\n# 수정결정계수\n\nres_summ&lt;-summary(res)\n\nres_summ$adj.r.squared #0.662846\n\n[1] 0.662846\n\n# Mallow's Cp - 작을수록 좋음\n\n# 축소모형\n\nres_subset&lt;-lm(Y~X1+X3,data=data_3.3)\n\nolsrr::ols_mallows_cp(res_subset,res) \n\n[1] 1.114811\n\n# AIC - 참고만\n\nolsrr::ols_aic(res_subset,method = \"SAS\")\n\n[1] 118.0024\n\n# BIC\n\nolsrr::ols_sbic(res_subset,res)\n\n[1] 121.0938\n\n### 전진적 선택법 - AIC\n\nres_step&lt;-step(res,direction = \"forward\")\n\nStart:  AIC=123.36\nY ~ X1 + X2 + X3 + X4 + X5 + X6\n\n### 후진적 제거법 -AIC\n\nres_step&lt;-step(res,direction = \"backward\")\n\nStart:  AIC=123.36\nY ~ X1 + X2 + X3 + X4 + X5 + X6\n\n       Df Sum of Sq    RSS    AIC\n- X5    1      3.41 1152.4 121.45\n- X4    1      6.80 1155.8 121.54\n- X2    1     14.47 1163.5 121.74\n- X6    1     74.11 1223.1 123.24\n&lt;none&gt;              1149.0 123.36\n- X3    1    180.50 1329.5 125.74\n- X1    1    724.80 1873.8 136.04\n\nStep:  AIC=121.45\nY ~ X1 + X2 + X3 + X4 + X6\n\n       Df Sum of Sq    RSS    AIC\n- X4    1     10.61 1163.0 119.73\n- X2    1     14.16 1166.6 119.82\n- X6    1     71.27 1223.7 121.25\n&lt;none&gt;              1152.4 121.45\n- X3    1    177.74 1330.1 123.75\n- X1    1    724.70 1877.1 134.09\n\nStep:  AIC=119.73\nY ~ X1 + X2 + X3 + X6\n\n       Df Sum of Sq    RSS    AIC\n- X2    1     16.10 1179.1 118.14\n- X6    1     61.60 1224.6 119.28\n&lt;none&gt;              1163.0 119.73\n- X3    1    197.03 1360.0 122.42\n- X1    1   1165.94 2328.9 138.56\n\nStep:  AIC=118.14\nY ~ X1 + X3 + X6\n\n       Df Sum of Sq    RSS    AIC\n- X6    1     75.54 1254.7 118.00\n&lt;none&gt;              1179.1 118.14\n- X3    1    186.12 1365.2 120.54\n- X1    1   1259.91 2439.0 137.94\n\nStep:  AIC=118\nY ~ X1 + X3\n\n       Df Sum of Sq    RSS    AIC\n&lt;none&gt;              1254.7 118.00\n- X3    1    114.73 1369.4 118.63\n- X1    1   1370.91 2625.6 138.16\n\n### 단계적 방법\n\nres_step&lt;-step(res) #최종적으로는 X1+X3이 선택됨 이거 사용하는 것이 좋을듯 \n\nStart:  AIC=123.36\nY ~ X1 + X2 + X3 + X4 + X5 + X6\n\n       Df Sum of Sq    RSS    AIC\n- X5    1      3.41 1152.4 121.45\n- X4    1      6.80 1155.8 121.54\n- X2    1     14.47 1163.5 121.74\n- X6    1     74.11 1223.1 123.24\n&lt;none&gt;              1149.0 123.36\n- X3    1    180.50 1329.5 125.74\n- X1    1    724.80 1873.8 136.04\n\nStep:  AIC=121.45\nY ~ X1 + X2 + X3 + X4 + X6\n\n       Df Sum of Sq    RSS    AIC\n- X4    1     10.61 1163.0 119.73\n- X2    1     14.16 1166.6 119.82\n- X6    1     71.27 1223.7 121.25\n&lt;none&gt;              1152.4 121.45\n- X3    1    177.74 1330.1 123.75\n- X1    1    724.70 1877.1 134.09\n\nStep:  AIC=119.73\nY ~ X1 + X2 + X3 + X6\n\n       Df Sum of Sq    RSS    AIC\n- X2    1     16.10 1179.1 118.14\n- X6    1     61.60 1224.6 119.28\n&lt;none&gt;              1163.0 119.73\n- X3    1    197.03 1360.0 122.42\n- X1    1   1165.94 2328.9 138.56\n\nStep:  AIC=118.14\nY ~ X1 + X3 + X6\n\n       Df Sum of Sq    RSS    AIC\n- X6    1     75.54 1254.7 118.00\n&lt;none&gt;              1179.1 118.14\n- X3    1    186.12 1365.2 120.54\n- X1    1   1259.91 2439.0 137.94\n\nStep:  AIC=118\nY ~ X1 + X3\n\n       Df Sum of Sq    RSS    AIC\n&lt;none&gt;              1254.7 118.00\n- X3    1    114.73 1369.4 118.63\n- X1    1   1370.91 2625.6 138.16\n\nsummary(res_step) #X3가 의미 없다고 나와도 아님 검증된것임\n\n\nCall:\nlm(formula = Y ~ X1 + X3, data = data_3.3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.5568  -5.7331   0.6701   6.5341  10.3610 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   9.8709     7.0612   1.398    0.174    \nX1            0.6435     0.1185   5.432 9.57e-06 ***\nX3            0.2112     0.1344   1.571    0.128    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.817 on 27 degrees of freedom\nMultiple R-squared:  0.708, Adjusted R-squared:  0.6864 \nF-statistic: 32.74 on 2 and 27 DF,  p-value: 6.058e-08"
  },
  {
    "objectID": "Regression_Analysis.html#regression-analysis-1",
    "href": "Regression_Analysis.html#regression-analysis-1",
    "title": "Regression Analysis",
    "section": "",
    "text": "data_use&lt;-read.table(\"All_Data/p329.txt\",header=T,sep=\"\\t\")\n\nhead(data_use)\n\n     X1 X2    X3    X4 X5 X6 X7 X8 X9    Y\n1 4.918  1 3.472 0.998  1  7  4 42  0 25.9\n2 5.021  1 3.531 1.500  2  7  4 62  0 29.5\n3 4.543  1 2.275 1.175  1  6  3 40  0 27.9\n4 4.557  1 4.050 1.232  1  6  3 54  0 25.9\n5 5.060  1 4.455 1.121  1  6  3 42  0 29.9\n6 3.891  1 4.455 0.988  1  6  3 56  0 29.9\n\n### 데이터 탐색 - 자료형\n\nsapply(data_use,class)\n\n       X1        X2        X3        X4        X5        X6        X7        X8 \n\"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \n       X9         Y \n\"numeric\" \"numeric\" \n\n### 데이터 탐색 - 기초 통계량\n\nsummary(data_use) #결측값의 여부를 확인\n\n       X1              X2              X3              X4       \n Min.   :3.891   Min.   :1.000   Min.   :2.275   Min.   :0.975  \n 1st Qu.:5.058   1st Qu.:1.000   1st Qu.:4.855   1st Qu.:1.161  \n Median :5.974   Median :1.000   Median :5.685   Median :1.432  \n Mean   :6.405   Mean   :1.167   Mean   :6.033   Mean   :1.384  \n 3rd Qu.:7.873   3rd Qu.:1.500   3rd Qu.:7.158   3rd Qu.:1.577  \n Max.   :9.142   Max.   :1.500   Max.   :9.890   Max.   :1.831  \n       X5              X6            X7              X8              X9      \n Min.   :0.000   Min.   :5.0   Min.   :2.000   Min.   : 3.00   Min.   :0.00  \n 1st Qu.:1.000   1st Qu.:6.0   1st Qu.:3.000   1st Qu.:30.00   1st Qu.:0.00  \n Median :1.000   Median :6.0   Median :3.000   Median :40.00   Median :0.00  \n Mean   :1.312   Mean   :6.5   Mean   :3.167   Mean   :37.46   Mean   :0.25  \n 3rd Qu.:2.000   3rd Qu.:7.0   3rd Qu.:3.250   3rd Qu.:48.50   3rd Qu.:0.25  \n Max.   :2.000   Max.   :8.0   Max.   :4.000   Max.   :62.00   Max.   :1.00  \n       Y        \n Min.   :25.90  \n 1st Qu.:29.90  \n Median :33.70  \n Mean   :34.63  \n 3rd Qu.:38.15  \n Max.   :45.80  \n\n### 데이터 탐색 - 히스토그램 & 상자그림\n\nhist(data_use$Y,main=\"histogram of data_use$Y\")\n\n\n\nboxplot(data_use$Y)\n\n\n\n### 데이터 탐색 - 산점도 행렬 & 상관계수\n\nplot(data_use)\n\ncor(data_use) \n\n           X1         X2         X3         X4          X5        X6        X7\nX1  1.0000000  0.6512669  0.6892117  0.7342737  0.45855650 0.6406157 0.3671126\nX2  0.6512669  1.0000000  0.4129558  0.7285916  0.22402204 0.5103104 0.4264014\nX3  0.6892117  0.4129558  1.0000000  0.5715520  0.20466375 0.3921244 0.1516093\nX4  0.7342737  0.7285916  0.5715520  1.0000000  0.35888351 0.6788606 0.5743353\nX5  0.4585565  0.2240220  0.2046638  0.3588835  1.00000000 0.5893871 0.5412988\nX6  0.6406157  0.5103104  0.3921244  0.6788606  0.58938707 1.0000000 0.8703883\nX7  0.3671126  0.4264014  0.1516093  0.5743353  0.54129880 0.8703883 1.0000000\nX8 -0.4371012 -0.1007485 -0.3527514 -0.1390869 -0.02016883 0.1242663 0.3135114\nX9  0.1466825  0.2041241  0.3059946  0.1065612  0.10161846 0.2222222 0.0000000\nY   0.8739117  0.7097771  0.6476364  0.7077656  0.46146792 0.5284436 0.2815200\n            X8        X9          Y\nX1 -0.43710116 0.1466825  0.8739117\nX2 -0.10074847 0.2041241  0.7097771\nX3 -0.35275139 0.3059946  0.6476364\nX4 -0.13908686 0.1065612  0.7077656\nX5 -0.02016883 0.1016185  0.4614679\nX6  0.12426629 0.2222222  0.5284436\nX7  0.31351144 0.0000000  0.2815200\nX8  1.00000000 0.2257796 -0.3974034\nX9  0.22577960 1.0000000  0.2668783\nY  -0.39740338 0.2668783  1.0000000\n\npairs(data_use)\n\n\n\npanel.cor&lt;-function(x,y){\n\n  par(usr=c(0,1,0,1))\n\n  r&lt;-round(cor(x,y),digits = 3)\n\n  text(0.5,0.5,r,cex=1.5)\n\n}\n\npairs(data_use,lower.panel = panel.cor)\n\n\n\n### (a) 모든 변수들이 모형에 포함시킬 것인가?\n\ncor(data_use)\n\n           X1         X2         X3         X4          X5        X6        X7\nX1  1.0000000  0.6512669  0.6892117  0.7342737  0.45855650 0.6406157 0.3671126\nX2  0.6512669  1.0000000  0.4129558  0.7285916  0.22402204 0.5103104 0.4264014\nX3  0.6892117  0.4129558  1.0000000  0.5715520  0.20466375 0.3921244 0.1516093\nX4  0.7342737  0.7285916  0.5715520  1.0000000  0.35888351 0.6788606 0.5743353\nX5  0.4585565  0.2240220  0.2046638  0.3588835  1.00000000 0.5893871 0.5412988\nX6  0.6406157  0.5103104  0.3921244  0.6788606  0.58938707 1.0000000 0.8703883\nX7  0.3671126  0.4264014  0.1516093  0.5743353  0.54129880 0.8703883 1.0000000\nX8 -0.4371012 -0.1007485 -0.3527514 -0.1390869 -0.02016883 0.1242663 0.3135114\nX9  0.1466825  0.2041241  0.3059946  0.1065612  0.10161846 0.2222222 0.0000000\nY   0.8739117  0.7097771  0.6476364  0.7077656  0.46146792 0.5284436 0.2815200\n            X8        X9          Y\nX1 -0.43710116 0.1466825  0.8739117\nX2 -0.10074847 0.2041241  0.7097771\nX3 -0.35275139 0.3059946  0.6476364\nX4 -0.13908686 0.1065612  0.7077656\nX5 -0.02016883 0.1016185  0.4614679\nX6  0.12426629 0.2222222  0.5284436\nX7  0.31351144 0.0000000  0.2815200\nX8  1.00000000 0.2257796 -0.3974034\nX9  0.22577960 1.0000000  0.2668783\nY  -0.39740338 0.2668783  1.0000000\n\nmodel_1&lt;-lm(data_use$Y~.,data_use)\n\nsummary(model_1) #전형적인 다중공선성이 보인다 \n\n\nCall:\nlm(formula = data_use$Y ~ ., data = data_use)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7729 -1.9801 -0.0868  1.6615  4.2618 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) 15.31044    5.96093   2.568   0.0223 *\nX1           1.95413    1.03833   1.882   0.0808 .\nX2           6.84552    4.33529   1.579   0.1367  \nX3           0.13761    0.49436   0.278   0.7848  \nX4           2.78143    4.39482   0.633   0.5370  \nX5           2.05076    1.38457   1.481   0.1607  \nX6          -0.55590    2.39791  -0.232   0.8200  \nX7          -1.24516    3.42293  -0.364   0.7215  \nX8          -0.03800    0.06726  -0.565   0.5810  \nX9           1.70446    1.95317   0.873   0.3976  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.973 on 14 degrees of freedom\nMultiple R-squared:  0.8512,    Adjusted R-squared:  0.7555 \nF-statistic: 8.898 on 9 and 14 DF,  p-value: 0.0002015\n\n#coef가 X6 0.820이므로 확인 필요\n\nolsrr::ols_vif_tol(model_1) #X6,X7이 10에 가깝기에 제거해야하는 대상중 최우선 \n\n  Variables  Tolerance       VIF\n1        X1 0.14241176  7.021892\n2        X2 0.35267392  2.835480\n3        X3 0.40735454  2.454864\n4        X4 0.26066568  3.836332\n5        X5 0.54842184  1.823414\n6        X6 0.08539078 11.710866\n7        X7 0.10286111  9.721847\n8        X8 0.43083904  2.321052\n9        X9 0.51482060  1.942424\n\n#다중공선성이 보이기에 모든 변수를 모형에 포함 시킬수는 없다 \n\n#10보다 크면 심각한 공선성이 존재 \n\n### (b) 지방세(X1) 방의 수(X6), 건물의 나이(X8)가 판매가격(Y)을 \n\n#       설명하는데 적절하다는 의견에 동의 하는가?\n\nmodel_2&lt;-lm(Y~X1+X6+X8,data=data_use)\n\nsummary(model_2)\n\n\nCall:\nlm(formula = Y ~ X1 + X6 + X8, data = data_use)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7486 -2.4082 -0.3594  2.1378  6.5353 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 14.796013   4.971105   2.976 0.007462 ** \nX1           3.489464   0.729368   4.784 0.000113 ***\nX6          -0.415515   1.182262  -0.351 0.728921    \nX8           0.004923   0.063597   0.077 0.939062    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.123 on 20 degrees of freedom\nMultiple R-squared:  0.7655,    Adjusted R-squared:  0.7303 \nF-statistic: 21.76 on 3 and 20 DF,  p-value: 1.653e-06\n\n# VIF\n\nolsrr::ols_vif_tol(model_2)\n\n  Variables Tolerance      VIF\n1        X1 0.3184367 3.140342\n2        X6 0.3875669 2.580200\n3        X8 0.5317389 1.880622\n\n# 회귀진단 그래프들\n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(model_2)\n\n\n\nlayout(1)\n\n# Cook의 거리 - 1을 넘으면 확인을 해야한다 \n\nolsrr::ols_plot_cooksd_chart(model_2) \n\n\n\n#동의는 할 수 있으나(적절하지만) 최고의 모형이라는데는 동의 못함\n\n### (c) 지방세 X1가 단독으로 판매가격 Y을 설명하는데 적절하다는 의견에 동의?\n\nmodel_3&lt;-lm(Y~X1,data=data_use)\n\nsummary(model_3) #adj-rsquared는 변수를 선택할때 사용 \n\n\nCall:\nlm(formula = Y ~ X1, data = data_use)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8445 -2.3340 -0.3841  1.9689  6.3005 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  13.3553     2.5955   5.146 3.71e-05 ***\nX1            3.3215     0.3939   8.433 2.44e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.988 on 22 degrees of freedom\nMultiple R-squared:  0.7637,    Adjusted R-squared:  0.753 \nF-statistic: 71.11 on 1 and 22 DF,  p-value: 2.435e-08\n\n# 회귀진단 그래프들\n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(model_3)\n\n\n\nlayout(1)\n\n# Cook의 거리 - 1을 넘으면 확인을 해야한다 \n\nolsrr::ols_plot_cooksd_chart(model_3) \n\n\n\n### 적절한 모형 제시\n\ndata_use_2&lt;-data_use[-6]\n\nmodel_4&lt;-lm(Y~.,data_use_2)\n\nsummary(model_4)\n\n\nCall:\nlm(formula = Y ~ ., data = data_use_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7340 -1.8513 -0.0154  1.5472  4.2113 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) 14.40540    4.36026   3.304  0.00482 **\nX1           1.81642    0.82431   2.204  0.04360 * \nX2           7.13892    4.01353   1.779  0.09556 . \nX3           0.14721    0.47683   0.309  0.76177   \nX4           2.73339    4.24921   0.643  0.52976   \nX5           2.06520    1.33883   1.543  0.14377   \nX7          -1.91236    1.79355  -1.066  0.30318   \nX8          -0.03832    0.06509  -0.589  0.56481   \nX9           1.48746    1.65931   0.896  0.38418   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.878 on 15 degrees of freedom\nMultiple R-squared:  0.8506,    Adjusted R-squared:  0.771 \nF-statistic: 10.68 on 8 and 15 DF,  p-value: 5.936e-05\n\n# VIF\n\nolsrr::ols_vif_tol(model_4)\n\n  Variables Tolerance      VIF\n1        X1 0.2117072 4.723504\n2        X2 0.3855308 2.593827\n3        X3 0.4102334 2.437637\n4        X4 0.2612467 3.827800\n5        X5 0.5495336 1.819725\n6        X7 0.3510102 2.848920\n7        X8 0.4310175 2.320091\n8        X9 0.6683173 1.496295\n\n# 회귀진단 그래프들\n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(model_4)\n\n\n\nlayout(1)\n\n# Cook의 거리 - 1을 넘으면 확인을 해야한다 \n\nolsrr::ols_plot_cooksd_chart(model_4) \n\n\n\n### 단계적 선택법 - AIC\n\nmodel_5&lt;-step(model_4)\n\nStart:  AIC=57.45\nY ~ X1 + X2 + X3 + X4 + X5 + X7 + X8 + X9\n\n       Df Sum of Sq    RSS    AIC\n- X3    1     0.789 125.00 55.605\n- X8    1     2.870 127.08 56.001\n- X4    1     3.426 127.63 56.106\n- X9    1     6.654 130.86 56.706\n- X7    1     9.414 133.62 57.207\n&lt;none&gt;              124.21 57.453\n- X5    1    19.702 143.91 58.987\n- X2    1    26.198 150.40 60.046\n- X1    1    40.207 164.41 62.184\n\nStep:  AIC=55.61\nY ~ X1 + X2 + X4 + X5 + X7 + X8 + X9\n\n       Df Sum of Sq    RSS    AIC\n- X8    1     3.369 128.36 54.243\n- X4    1     4.816 129.81 54.513\n- X9    1     9.581 134.58 55.378\n- X7    1     9.655 134.65 55.391\n&lt;none&gt;              125.00 55.605\n- X5    1    18.957 143.95 56.994\n- X2    1    25.474 150.47 58.057\n- X1    1    53.245 178.24 62.122\n\nStep:  AIC=54.24\nY ~ X1 + X2 + X4 + X5 + X7 + X9\n\n       Df Sum of Sq    RSS    AIC\n- X4    1     5.011 133.38 53.163\n- X9    1     6.543 134.91 53.437\n&lt;none&gt;              128.36 54.243\n- X5    1    20.033 148.40 55.724\n- X7    1    22.819 151.18 56.170\n- X2    1    24.299 152.66 56.404\n- X1    1    95.134 223.50 65.552\n\nStep:  AIC=53.16\nY ~ X1 + X2 + X5 + X7 + X9\n\n       Df Sum of Sq    RSS    AIC\n- X9    1     6.223 139.60 52.257\n&lt;none&gt;              133.38 53.163\n- X5    1    17.801 151.18 54.169\n- X7    1    17.873 151.25 54.181\n- X2    1    39.335 172.71 57.365\n- X1    1   157.972 291.35 69.915\n\nStep:  AIC=52.26\nY ~ X1 + X2 + X5 + X7\n\n       Df Sum of Sq    RSS    AIC\n&lt;none&gt;              139.60 52.257\n- X5    1    20.836 160.43 53.596\n- X7    1    21.669 161.27 53.720\n- X2    1    47.409 187.01 57.274\n- X1    1   156.606 296.20 68.312\n\nsummary(model_5)\n\n\nCall:\nlm(formula = Y ~ X1 + X2 + X5 + X7, data = data_use_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5605 -2.0856  0.0238  1.8580  3.8981 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  13.6212     3.6725   3.709 0.001489 ** \nX1            2.4123     0.5225   4.617 0.000188 ***\nX2            8.4589     3.3300   2.540 0.019970 *  \nX5            2.0604     1.2235   1.684 0.108541    \nX7           -2.2154     1.2901  -1.717 0.102176    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.711 on 19 degrees of freedom\nMultiple R-squared:  0.8321,    Adjusted R-squared:  0.7968 \nF-statistic: 23.54 on 4 and 19 DF,  p-value: 3.866e-07\n\n# 회귀진단 그래프들\n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(model_5)\n\n\n\nlayout(1)\n\n# Cook의 거리 - 0.5보다 작으면 괜찮음 / 1보다 큰 값을 고려 / 전체적으로 봤을때 튀는 값이 있는 경우 \n\nolsrr::ols_plot_cooksd_chart(model_5) \n\n\n\n### 17번 제거\n\ndata_use_3&lt;-model_5$model[-17,]\n\nmodel_6&lt;-lm(Y~.,data_use_3)\n\nsummary(model_6)\n\n\nCall:\nlm(formula = Y ~ ., data = data_use_3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.4577 -1.6655  0.1575  1.7978  4.1865 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  13.2263     3.4659   3.816  0.00127 **\nX1            1.7014     0.6247   2.723  0.01394 * \nX2           12.0705     3.6958   3.266  0.00429 **\nX5            2.4602     1.1726   2.098  0.05028 . \nX7           -2.2310     1.2152  -1.836  0.08294 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.553 on 18 degrees of freedom\nMultiple R-squared:  0.8418,    Adjusted R-squared:  0.8067 \nF-statistic: 23.95 on 4 and 18 DF,  p-value: 5.316e-07\n\n# VIF\n\nolsrr::ols_vif_tol(model_6)\n\n  Variables Tolerance      VIF\n1        X1 0.3319061 3.012900\n2        X2 0.3658998 2.732989\n3        X5 0.5664655 1.765333\n4        X7 0.6043771 1.654596\n\n# 회귀진단 그래프들\n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(model_6)\n\n\n\nlayout(1)\n\n# Cook의 거리 - 1을 넘으면 확인을 해야한다 \n\nolsrr::ols_plot_cooksd_chart(model_6)"
  },
  {
    "objectID": "OpenData_Analysis/실기_1유형.html",
    "href": "OpenData_Analysis/실기_1유형.html",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "",
    "text": "실기 1 유형"
  },
  {
    "objectID": "OpenData_Analysis/실기_1유형.html#데이터-다루기-유형",
    "href": "OpenData_Analysis/실기_1유형.html#데이터-다루기-유형",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "✅ 데이터 다루기 유형",
    "text": "✅ 데이터 다루기 유형\n\n\n데이터 타입(object, int, float, bool 등)\n기초통계량(평균, 중앙값, 사분위수, IQR, 표준편차 등)\n데이터 인덱싱, 필터링, 정렬, 변경 등\n중복값, 결측치, 이상치 처리 (제거 or 대체)\n데이터 Scaling (데이터 표준화(z), 데이터 정규화(min-max))\n데이터 합치기\n날짜/시간 데이터, index 다루기"
  },
  {
    "objectID": "OpenData_Analysis/실기_1유형.html#핵심문제-27개-110",
    "href": "OpenData_Analysis/실기_1유형.html#핵심문제-27개-110",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "✅ 핵심문제 27개 (1~10)",
    "text": "✅ 핵심문제 27개 (1~10)\n\n# 데이터 불러오기\nimport pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"mtcars.txt\")\ndf.head()\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\nMazda RX4\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\nHornet 4 Drive\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n4\nHornet Sportabout\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n\n\n\n\n# 문제 1\n# mpg 변수의 제 1사분위수를 구하고 정수값으로 출력하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\nQ1 = df['mpg'].quantile(.25)\nprint(round(Q1))\n\n15\n\n\n\n# 문제 2\n# mpg 값이 19이상 21이하인 데이터의 수를 구하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이 1)\ncond1 = (df[(df['mpg']&gt;=19) & (df['mpg']&lt;=21)])\nprint(len(cond1))\n\n# (풀이 2)\n# cond1 = (df['mpg']&gt;=19)\n# cond2 = (df['mpg']&lt;=21)\n# print(len(df[cond1&cond2]))\n\n5\n\n\n\n# 문제 3\n# hp 변수의 IQR 값을 구하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\nQ1 = df['hp'].quantile(.25)\nQ3 = df['hp'].quantile(.75)\nIQR = Q3 - Q1\nprint(IQR)\n\n83.5\n\n\n\n# 문제 4 \n# wt 변수의 상위 10개 값의 총합을 구하여 소수점을 버리고 정수로 출력하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\ntop10 = df.sort_values('wt', ascending=False)\nsum_top10 = sum(top10['wt'].head(10))\nprint(int(sum_top10))\n\n# top_10 = df.sort_values('wt',ascending=False).head(10)\n# print(int(sum(top_10['wt']))) #주의: 소수점 반올림이 아니라 버리는 문제\n\n42\n\n\n\n# 문제 5\n# 전체 자동차에서 cyl가 6인 비율이 얼마인지 소수점 첫째짜리까지 출력하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\ncond1 = len(df[df['cyl']==6])\ncond2 = len(df['cyl'])\nprint(round(cond1/cond2, 1))\n\n0.2\n\n\n\n# 문제 6\n# 첫번째 행부터 순서대로 10개 뽑은 후 mpg 열의 평균값을 반올림하여 정수로 출력하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\ncond1 = df.head(10) # df[:10], df.loc[0:9]\nmean_mpg = cond1['mpg'].mean()\nprint(round(mean_mpg))\n\n20\n\n\n\n# 문제 7\n# 첫번째 행부터 순서대로 50% 까지 데이터를 뽑아 wt 변수의 중앙값을 구하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\n# 50% 데이터에 해당하는 행의 수 (정수로 구하기)\np50 = int(len(df)*0.5)\ndf50 = df[:p50]\nprint(df50['wt'].median())\n\n# len(df)/2\n# cond1 = df[:17]\n# cond2 = cond1['wt'].median()\n# print(cond2)\n\n3.44\n\n\n\n# 문제 8\n# 결측값이 있는 데이터의 수를 출력하시오\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    '판매수' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n\n\n\n날짜\n제품\n판매수\n개당수익\n\n\n\n\n0\n20220103\nA\n3.0\n300\n\n\n1\n20220105\nB\nNaN\n400\n\n\n2\nNone\nNone\n5.0\n500\n\n\n3\n20230127\nB\n10.0\n600\n\n\n4\n20220203\nA\n10.0\n400\n\n\n\n\n\n\n\n\n# (풀이)\nm_value = df.isnull().sum()\nprint(m_value.sum())\n\n5\n\n\n\n# 문제 9 \n# '판매수' 컬럼의 결측값을 판매수의 중앙값으로 대체하고 판매수의 \n#  평균값을 정수로 출력하시오\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    '판매수' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\n\n\n# (풀이)\ndf[df['판매수'].isnull()]\nmedian = df['판매수'].median()\ndf['판매수'] = df['판매수'].fillna(median)\nmean = df['판매수'].mean()\nprint(int(mean))\n\n15\n\n\n\n# 문제 10\n# 판매수 컬럼에 결측치가 있는 행을 제거하고\n# 첫번째 행부터 순서대로 50%까지 데이터를 추출하여\n# 판매수 변수의 Q1(제1사분위수) 값을 정수로 출력하시오\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    '판매수' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\n\n\n# (풀이)\n# 결측치 삭제(행 기준)\ndf = df['판매수'].dropna()\n# 첫번째 행부터 순서대로 50%까지의 데이터 추출\nper50 = int(len(df)*0.5)\ndf = df[:per50]\n# 값구하기\nprint(round(df.quantile(.25)))\n\n# df[df['판매수'].isnull()]\n# df['판매수'] = df['판매수'].dropna()\n# int(len(df['판매수'])/2)\n# cond1 = df['판매수'].head(6)\n# Q1 = cond1.quantile(.25)\n# print(int(Q1))\n\n5"
  },
  {
    "objectID": "OpenData_Analysis/실기_1유형.html#핵심문제-27개-1120",
    "href": "OpenData_Analysis/실기_1유형.html#핵심문제-27개-1120",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "✅ 핵심문제 27개 (11~20)",
    "text": "✅ 핵심문제 27개 (11~20)\n\ndf = pd.read_csv('mtcars.txt')\ndf.head()\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\nMazda RX4\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\nHornet 4 Drive\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n4\nHornet Sportabout\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n\n\n\n\n# 문제 11\n# cyl가 4인 자동차와 6인 자동차 그룹의 mpg 평균값이 차이를 절대값으로 정수로 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\ncyl_4 = df[df['cyl']==4]\ncyl_4_mean = cyl_4['mpg'].mean()\n\ncyl_6 = df[df['cyl']==6]\ncyl_6_mean = cyl_6['mpg'].mean()\n\nprint(round(abs(cyl_4_mean - cyl_6_mean)))\n\n7\n\n\n\n# 문제 12\n# hp 변수에 대해 데이터표준화(Z-score)를 진행하고 이상치의 수를 구하시오\n# (단, 이상치는 Z값이 1.5를 초과하거나 -1.5미만인 값이다)\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\n# Z = (X-평균) / 표준편차\nstd = df['hp'].std()\nmean_hp = df['hp'].mean()\ndf['zscore'] = (df['hp']-mean_hp)/std\n\ncond1 = (df['zscore']&gt;1.5)\ncond2 = (df['zscore']&lt;-1.5)\nprint(len(df[cond1] + df[cond2]))\n\n2\n\n\n\n# 문제 13\n# mpg 컬럼을 최소최대 Scaling을 진행한 후 0.7보다 큰 값을 가지는 레코드 수를 구하라\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\nfrom sklearn.preprocessing import MinMaxScaler\nmscaler = MinMaxScaler()\ndf['mpg'] = mscaler.fit_transform(df[['mpg']])\nprint(len(df[df['mpg']&gt;0.7]))\n\n# 공식 : (x-min) / (max-min)\n# min_mpg = df['mpg'].min()\n# max_mpg = df['mpg'].max()\n# df['mpg'] = (df['mpg'] - min_mpg)/(max_mpg - min_mpg)\n# cond1 = (df['mpg']&gt;0.7)\n# print(len(df[cond1]))\n\n5\n\n\n\n# 문제 14\n# wt 컬럼에 대해 상자그림 기준으로 이상치의 개수를 구하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\n# 이상치 : Q1, Q3 로부터 1.5*IQR을 넘어가는 값\nQ1 = df['wt'].quantile(.25)\nQ3 = df['wt'].quantile(.75)\nIQR = Q3-Q1\n\nupper = Q3 + (1.5*IQR)\nlower = Q1 - (1.5*IQR)\n\ncond1 = (df['wt'] &gt; upper)\ncond2 = (df['wt'] &lt; lower)\n\nprint(len(df[cond1] + df[cond2]))\n\n3\n\n\n\n# 문제 15\n# 판매수 컬럼의 결측치를 최소값으로 대체하고\n# 결측치가 있을 때와 최소값으로 대체했을 때\n# 평균값의 차이를 절대값으로 정수로 출력하시오\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    '판매수' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf\n\n\n\n\n\n\n\n\n날짜\n제품\n판매수\n개당수익\n\n\n\n\n0\n20220103\nA\n3.0\n300\n\n\n1\n20220105\nB\nNaN\n400\n\n\n2\nNone\nNone\n5.0\n500\n\n\n3\n20230127\nB\n10.0\n600\n\n\n4\n20220203\nA\n10.0\n400\n\n\n5\n20220205\nNone\n10.0\n500\n\n\n6\n20230210\nA\n15.0\n500\n\n\n7\n20230223\nB\n15.0\n600\n\n\n8\n20230312\nA\n20.0\n600\n\n\n9\n20230422\nB\nNaN\n700\n\n\n10\n20220505\nA\n30.0\n600\n\n\n11\n20230511\nA\n40.0\n600\n\n\n\n\n\n\n\n\n# (풀이)\n# 데이터 복사\ndf2 = df.copy()\n# 최소값으로 결측치 대체\nmin = df['판매수'].min()\ndf2['판매수'] = df2['판매수'].fillna(min)\n\n# 결측치가 있을때, 대체했을때 평균\nm_yes = df['판매수'].mean()\nm_no = df2['판매수'].mean()\n\nprint(round(abs(m_yes - m_no)))\n\n2\n\n\n\n# 문제 16\n# vs변수가 0이 아닌 차량 중에 mpg 값이 가장 큰 차량의 hp 값을 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\ncond1 = df[df['vs']!=0]\n# mpg 값이 가장 큰 차량(내림차순)\ncond1 = cond1.sort_values('mpg',ascending=False)\n# cond1\nprint(cond1['hp'].iloc[0])\n\n65\n\n\n\n# 문제 17\n# gear 변수값이 3, 4인 두 그룹의 hp 표준편차값의 차이를 절대값으로 \n# 소수점 첫째자리까지 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\n# 두개 그룹으로 필터링\ncond1 = df[df['gear']==3]\ncond2 = df[df['gear']==4]\n# std 구하기\nstd3 = cond1['hp'].std()\nstd4 = cond2['hp'].std()\n\nprint(round(abs(std3 - std4),1))\n\n21.8\n\n\n\n# 문제 18\n# gear 변수의 갑별로 그룹화하여 mpg 평균값을 산출하고\n# 평균값이 높은 그룹의 mpg 제3사분위수 값을 구하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\ndf['gear'].unique()\ngear3 = df[df['gear']==3]\ngear4 = df[df['gear']==4]\ngear5 = df[df['gear']==5]\n\nm_3 = gear3['mpg'].mean()\nm_4 = gear4['mpg'].mean()\nm_5 = gear5['mpg'].mean()\n\n(m_3, m_4, m_5) # m_4의 평균값이 가장 큼\nQ3 = gear4['mpg'].quantile(.75)\nprint(Q3)\n\n28.075\n\n\n\n# (풀이_v2)\n# gear, mpg 변수만 필터링\ndf = df.loc[:, ['gear', 'mpg']]\n\n# gear 변수로 그룹화하여 mpg 평균값 보기\n# print(df.groupby('gear').mean()) # gear 4그룹이 제일 높음\n\n# gear=4인 그룹의 mpg Q3 구하기\ngear4 = df[df['gear']==4]\nprint(gear4['mpg'].quantile(.75))\n\n28.075\n\n\n\n# 문제 19\n# hp 항목의 상위 7번째 값으로 상위 7개 값을 변환한 후,\n# hp가 150 이상인 데이터를 추출하여 hp의 평균값을 반올림하여 정수로 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\n# hp 열 기준으로 내림차순 정렬\ndf = df.sort_values('hp', ascending=False)\n\n# 인덱스 초기화 - 내림차순으로 정렬시 최초의 인덱스로 있기에\ndf = df.reset_index(drop=True) # drop=True 는 기존 index 삭제\n# print(df)\n\n# hp 상위 7번째 값 확인\ntop7 = df['hp'].loc[6] # top7 = 205\n\n# np.where 활용\nimport numpy as np\ndf['hp'] = np.where(df['hp'] &gt;= 205, top7, df['hp'])\n# np.where(조건, 조건에 해당할 때 값, 그렇지 않을 때 값)\n\n# hp 150이상인 데이터\ncond1 = (df['hp']&gt;=150)\ndf = df[cond1]\nprint(round(df['hp'].mean()))\n\n187\n\n\n\n# 문제 20\n# car 변수에 Merc 무구가 포함된 자동차의 mpg 평균값을 정수로 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\ndf2 = df[df['car'].str.contains('Merc')] # 문자열 추출 알아두기\nprint(round(df2['mpg'].mean()))\n\n# 시험환경에서 답구하는 방법(reset_index() 사용 후)\n# 시험에서는 car가 index로 되어 있음\n# df.reset_index(inplace=True)\n# df2 = df[df['index'].str.contains(\"Merc\")] \n# print(round(df2['mpg'].mean()))\n\n19"
  },
  {
    "objectID": "OpenData_Analysis/실기_1유형.html#핵심문제-27개-2127",
    "href": "OpenData_Analysis/실기_1유형.html#핵심문제-27개-2127",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "✅ 핵심문제 27개 (21~27)",
    "text": "✅ 핵심문제 27개 (21~27)\n\n# 문제 21\n# 22년 1분기 A제품의 매출액을 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    '판매수' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n\n\n\n날짜\n제품\n판매수\n개당수익\n\n\n\n\n0\n20220103\nA\n3\n300\n\n\n1\n20220105\nB\n5\n400\n\n\n2\n20230105\nA\n5\n500\n\n\n3\n20230127\nB\n10\n600\n\n\n4\n20220203\nA\n10\n400\n\n\n\n\n\n\n\n\n# # (풀이)\n# cond1 = df[df['날짜']&lt;='20220431']\n# cond2 = cond1[cond1['제품']=='A']\n# cond2['매출액'] = (cond2['판매수']*cond2['개당수익'])\n# cond2\n\n# print(cond2['매출액'].sum())\n\n\n# (풀이_v2)\n# 데이터 타입 datetime으로 변경\ndf['날짜'] = pd.to_datetime(df['날짜'])\n\n# 년, 월, 일 변수(열) 추가하기\ndf['year'] = df['날짜'].dt.year\ndf['month'] = df['날짜'].dt.month\ndf['day'] = df['날짜'].dt.day\n\n# 매출액 변수 추가하기\ndf['매출액'] = df['판매수'] * df['개당수익']\n\n# 22년으로 필터링\ndf = df[df['year']==2022]\ndf.head()\n\n# 1,2,3월 매출액 계산\nm1 = df[df['month']==1]['매출액'].sum()\nm2 = df[df['month']==2]['매출액'].sum()\nm3 = df[df['month']==3]['매출액'].sum()\nprint(m1+m2+m3)\n\n11900\n\n\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['날짜'] = pd.to_datetime(df['날짜'])\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['year'] = df['날짜'].dt.year\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['month'] = df['날짜'].dt.month\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['day'] = df['날짜'].dt.day\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['매출액'] = df['판매수'] * df['개당수익']\n\n\n\n# 문제 22\n# 22년과 23년의 총 매출액 차이를 절대값으로 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    '판매수' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n\n\n\n날짜\n제품\n판매수\n개당수익\n\n\n\n\n0\n20220103\nA\n3\n300\n\n\n1\n20220105\nB\n5\n400\n\n\n2\n20230105\nA\n5\n500\n\n\n3\n20230127\nB\n10\n600\n\n\n4\n20220203\nA\n10\n400\n\n\n\n\n\n\n\n\n# (풀이)\n# 데이터 타입 datetime으로 변경\ndf['날짜'] = pd.to_datetime(df['날짜'])\n\n# 년 변수(열) 추가하기\ndf['year'] = df['날짜'].dt.year\n\n# 매출액 변수 추가하기\ndf['매출액'] = df['판매수'] * df['개당수익']\n\ncond22 = df[df['year']==2022]\ncond23 = df[df['year']==2023]\n\nprint(abs((cond22['매출액'].sum()) - (cond23['매출액'].sum())))\n\n48600\n\n\n\n# 문제 23\n# 23년 총 매출액이 큰 제품의 23년 판매수를 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    '판매수' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n\n\n\n날짜\n제품\n판매수\n개당수익\n\n\n\n\n0\n20220103\nA\n3\n300\n\n\n1\n20220105\nB\n5\n400\n\n\n2\n20230105\nA\n5\n500\n\n\n3\n20230127\nB\n10\n600\n\n\n4\n20220203\nA\n10\n400\n\n\n\n\n\n\n\n\n# 데이터 타입 datetime으로 변경\ndf['날짜'] = pd.to_datetime(df['날짜'])\n\n# 년 변수(열) 추가하기\ndf['year'] = df['날짜'].dt.year\n\n# 매출액 변수 추가하기\ndf['매출액'] = df['판매수'] * df['개당수익']\n\ndf = df[df['year']==2023]\n# df.head()\n\n# 23년 A 매출액과 B 매출액 별도로 구하기\n# A 매출액\ndf_a = df[df['제품']=='A']\na_sales = df_a['매출액'].sum()\n# print(a_sales) # 46000\n\n# B 매출액\ndf_b = df[df['제품']=='B']\nb_sales = df_b['매출액'].sum()\n# print(b_sales) # 32500\n\n# A 제품의 23년 판매수\na_sum = df_a['판매수'].sum()\nprint(a_sum)\n\n80\n\n\n\n# 문제 24\n# 매출액이 4천원 초과, 1만원 미만인 데이터 수를 출력하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    '판매수' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n\n\n\n날짜\n제품\n판매수\n개당수익\n\n\n\n\n0\n20220103\nA\n3\n300\n\n\n1\n20220105\nB\n5\n400\n\n\n2\n20230105\nA\n5\n500\n\n\n3\n20230127\nB\n10\n600\n\n\n4\n20220203\nA\n10\n400\n\n\n\n\n\n\n\n\n# (풀이)\ndf['매출액'] = df['판매수'] * df['개당수익']\ncond1 = df['매출액']&gt;4000\ncond2 = df['매출액']&lt;10000\n\ndf = df[cond1&cond2]\nprint(len(df))\n\n4\n\n\n\n# 문제 25\n# 23년 9월 24일 16:00~22:00 사이에 전체 제품의 판매수를 구하시오\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600]})\ntime = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf['time'] = time\ndf = df[['time', '물품', '판매수', '개당수익']]\ndf\n\n\n\n\n\n\n\n\ntime\n물품\n판매수\n개당수익\n\n\n\n\n0\n2023-09-24 12:25:00\nA\n5\n500\n\n\n1\n2023-09-24 16:48:25\nB\n10\n600\n\n\n2\n2023-09-24 21:11:50\nA\n15\n500\n\n\n3\n2023-09-25 01:35:15\nB\n15\n600\n\n\n4\n2023-09-25 05:58:40\nA\n20\n600\n\n\n5\n2023-09-25 10:22:05\nB\n25\n700\n\n\n6\n2023-09-25 14:45:30\nA\n40\n600\n\n\n\n\n\n\n\n\n# (풀이)\n# 컬럼과 인덱스에 둘다 time 변수를 놓고 풀이\n# 데이터 타입 datetime으로 변경 (항상 df.info()로 데이터 타입 확인할 것)\ndf['time'] = pd.to_datetime(df['time'])\n\n# index 새로 지정\ndf = df.set_index('time', drop=False) # default는 True\n\n# 9월 24일 필터링 // df['변수'].between( , )\ndf_after = df[df['time'].between('2023-09-24', '2023-09-25')] # 25일은 미포함\n\n# 시간 필터링 16:00 ~ 22:00 (주의: 시간이 index에 위치해야 함)\ndf = df_after.between_time(start_time='16:00', end_time='22:00') # 포함 기준\n\n# 전체 판매수 구하기\nprint(df['판매수'].sum())\n\n25\n\n\n\n# (풀이2) // 위에 df 새로 불러와서 실행해보기\n# 데이터 타입 datetime으로 변경\ndf['time'] = pd.to_datetime(df['time'])\n\n# index 새로 지정\ndf = df.set_index('time')\n\n# loc 함수로 필터링\ndf = df.loc[(df.index &gt;= '2023-09-24 16:00:00') & (df.index &lt;= '2023-09-24 22:00:00')]\n\n# 전체 판매수 구하기\nprint(df['판매수'].sum())\n\n25\n\n\n\n# 문제 26\n# 9월 25일 00:10~12:00까지의 B물품의 매출액 총합을 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600]})\ndf['time'] = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf = df[['time', '물품', '판매수', '개당수익']]\ndf = df.set_index('time',drop=True)\ndf\n\n\n\n\n\n\n\n\n물품\n판매수\n개당수익\n\n\ntime\n\n\n\n\n\n\n\n2023-09-24 12:25:00\nA\n5\n500\n\n\n2023-09-24 16:48:25\nB\n10\n600\n\n\n2023-09-24 21:11:50\nA\n15\n500\n\n\n2023-09-25 01:35:15\nB\n15\n600\n\n\n2023-09-25 05:58:40\nA\n20\n600\n\n\n2023-09-25 10:22:05\nB\n25\n700\n\n\n2023-09-25 14:45:30\nA\n40\n600\n\n\n\n\n\n\n\n\n# (풀이)\n# 매출액 변수 추가\ndf['매출액'] = df['판매수'] * df['개당수익']\n\n# loc 함수로 필터링\ndf = df.loc[(df.index &gt;= '2023-09-25 00:10:00') & (df.index &lt;= '2023-09-25 12:00:00')]\n\n# B 물품의 매출액 총합\ncond1 = df[df['물품']=='B']\nprint(cond1['매출액'].sum())\n\n26500\n\n\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\1645811142.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['매출액'] = df['판매수'] * df['개당수익']\n\n\n\n# 문제 27\n# 9월 24일 12:00~24:00까지의 A물품의 매출액 총합을 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600]})\ndf['time'] = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf = df[['time', '물품', '판매수', '개당수익']]\ndf = df.set_index('time',drop=True)\ndf\n\n\n\n\n\n\n\n\n물품\n판매수\n개당수익\n\n\ntime\n\n\n\n\n\n\n\n2023-09-24 12:25:00\nA\n5\n500\n\n\n2023-09-24 16:48:25\nB\n10\n600\n\n\n2023-09-24 21:11:50\nA\n15\n500\n\n\n2023-09-25 01:35:15\nB\n15\n600\n\n\n2023-09-25 05:58:40\nA\n20\n600\n\n\n2023-09-25 10:22:05\nB\n25\n700\n\n\n2023-09-25 14:45:30\nA\n40\n600\n\n\n\n\n\n\n\n\n# (풀이)\n# 매출액 변수 추가\ndf['매출액'] = df['판매수'] * df['개당수익']\n\n# loc 함수로 필터링\ndf = df.loc[(df.index &gt;= '2023-09-24 12:00:00') & (df.index &lt; '2023-09-25 00:00:00')]\n\n# A 물품의 매출액 총합\ncond1 = df[df['물품']=='A']\nprint(cond1['매출액'].sum())\n\n10000"
  },
  {
    "objectID": "Data_Visualization.html",
    "href": "Data_Visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "Data_Mining/2022-06-07-last.html",
    "href": "Data_Mining/2022-06-07-last.html",
    "title": "COVID-19 Analysis & Visualization",
    "section": "",
    "text": "COVID-19 Analysis & Visualization\n\n\n서론 \n\n분석 배경 및 목적 \n\n분석 배경 \n분석 배경 - 2022년 6월 현재 코로나의 상황은 매일 10000명의 확진자가 나오고 있는 상황이지만 코로나가 처음 발병하고 나서와는 조금은 다른 반응이다. 최근 정부에서는 집단 면역이 90% 이상 형성이 되어있으며 확진자의 추세 또한 감소세를 보이고 있는 상황에서 2020년 01월부터 2020년 06월까지 수집된 해당 데이터를 기반으로 과연 과거와 현재의 차이는 얼마나 있고 당시 정부와 뉴스에서 주장하던 코로나에 대한 정보는 과연 타당하였고 올바른 정보였는지 궁금하여 해당 주제를 선정하여 분석을 진행하게 되었습니다.\n\n\n분석 목적 \n분석 목적 - 자료를 제공한 데이콘에서는 해당 자료들은 이용해서 다음과 같은 인공지능 AI를 활용 코로나 확산 방지와 예방을 위한 인사이트 / 시각화 발굴. 이라는 목적을 가지고 진행을 하였습니다. 해서 저는 당시 기간동안 가장 많이 확진된 연령층과 주된 감염 원인과 그 이유에 대해서 알아보고, 어떤 연령층에게 가장 치명적인 질병이었느지와 당시 정부의 방역 대책은 타당하였는지 에 대해서 목적을 가지고 해당 분석을 진행하였습니다.\n\n\n\n데이터 소개 \n\n\n데이터 카테고리 \n\nCase Data\n\n\nCase: 한국의 COVID-19 감염 사례 데이터\n\n\nPatient Data\n\n\nPatientInfo: 한국의 코로나19 환자 역학 데이터\nPatientRoute: 국내 코로나19 환자 경로 데이터\n\n\nTime Series Data\n\n\nTime: 한국의 코로나19 상태의 시계열 데이터\nTimeAge: 한국의 연령별 코로나19 현황 시계열 데이터\nTimeGender: 한국의 성별에 따른 코로나19 현황의 시계열 데이터\nTimeProvince: 한국의 지역별 코로나19 현황 시계열 데이터\n\n\nAdditional Data\n\n\nRegion: 대한민국 내 지역의 위치 및 통계 자료\nWeather: 한국 지역의 날씨 데이터\nSearchTrend: 국내 최대 포털사이트 네이버에서 검색된 키워드의 트렌드 데이터\nSeoulFloating: 대한민국 서울 유동인구 데이터(SK텔레콤 빅데이터 허브에서)\nPolicy: 한국의 코로나19에 대한 정부 정책 데이터\n\n\n\n데이터 형태 \n\n색상이 의미하는 것은 비슷한 속성을 가지고 있다는 것입니다.\n열 사이에 선이 연결되어 있다는 것은 열의 값이 부분적으로 공유됨을 의미합니다.\n점선은 약한 관련성을 의미합니다.\n\nhttps://user-images.githubusercontent.com/50820635/86225695-8dca0580-bbc5-11ea-9e9b-b0ca33414d8a.PNG\n\n\n데이터 세부 설명 \n\nimport numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('../notebook/coronavirusdataset'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n../notebook/coronavirusdataset\\Case.csv\n../notebook/coronavirusdataset\\PatientInfo.csv\n../notebook/coronavirusdataset\\PatientRoute.csv\n../notebook/coronavirusdataset\\Policy.csv\n../notebook/coronavirusdataset\\Region.csv\n../notebook/coronavirusdataset\\SearchTrend.csv\n../notebook/coronavirusdataset\\SeoulFloating.csv\n../notebook/coronavirusdataset\\Time.csv\n../notebook/coronavirusdataset\\TimeAge.csv\n../notebook/coronavirusdataset\\TimeGender.csv\n../notebook/coronavirusdataset\\TimeProvince.csv\n../notebook/coronavirusdataset\\Weather.csv\n\n\n\npath = '../notebook/coronavirusdataset/'\n\ncase = p_info = pd.read_csv(path+'Case.csv')\npatientinfo = pd.read_csv(path+'PatientInfo.csv')\npatientroute = pd.read_csv(path+'PatientRoute.csv')\ntime = pd.read_csv(path+'Time.csv')\ntimeage = pd.read_csv(path+'TimeAge.csv')\ntimegender = pd.read_csv(path+'TimeGender.csv')\ntimeprovince = pd.read_csv(path+'TimeProvince.csv')\nregion = pd.read_csv(path+'Region.csv')\nweather = pd.read_csv(path+'Weather.csv')\nsearchtrend = pd.read_csv(path+'SearchTrend.csv')\nseoulfloating = pd.read_csv(path+'SeoulFloating.csv')\npolicy = pd.read_csv(path+'Policy.csv')\n\n\nCase\n\n한국의 COVID-19 감염 사례 데이터\n\ncase_id: the ID of the infection case\n\n\ncase_id(7) = region_code(5) + case_number(2)\nYou can check the region_code in ‘Region.csv’\nprovince: Special City / Metropolitan City / Province(-do)\ncity: City(-si) / Country (-gun) / District (-gu)\n\nThe value ‘from other city’ means that where the group infection started is other city.\n\ngroup: TRUE: group infection / FALSE: not group\n\nIf the value is ‘TRUE’ in this column, the value of ‘infection_cases’ means the name of group.\nThe values named ‘contact with patient’, ‘overseas inflow’ and ‘etc’ are not group infection.\n\ninfection_case: the infection case (the name of group or other cases)\n\nThe value ‘overseas inflow’ means that the infection is from other country.\nThe value ‘etc’ includes individual cases, cases where relevance classification is ongoing after investigation, and cases under investigation.\n\nconfirmed: the accumulated number of the confirmed\nlatitude: the latitude of the group (WGS84)\nlongitude: the longitude of the group (WGS84)\n\n\ncase.head()\n\n\n\n\n\n\n\n\ncase_id\nprovince\ncity\ngroup\ninfection_case\nconfirmed\nlatitude\nlongitude\n\n\n\n\n0\n1000001\nSeoul\nYongsan-gu\nTrue\nItaewon Clubs\n139\n37.538621\n126.992652\n\n\n1\n1000002\nSeoul\nGwanak-gu\nTrue\nRichway\n119\n37.48208\n126.901384\n\n\n2\n1000003\nSeoul\nGuro-gu\nTrue\nGuro-gu Call Center\n95\n37.508163\n126.884387\n\n\n3\n1000004\nSeoul\nYangcheon-gu\nTrue\nYangcheon Table Tennis Club\n43\n37.546061\n126.874209\n\n\n4\n1000005\nSeoul\nDobong-gu\nTrue\nDay Care Center\n43\n37.679422\n127.044374\n\n\n\n\n\n\n\n\nPatientInfo\n\n한국의 코로나19 환자 역학 데이터\n\npatient_id: the ID of the patient\n\n\npatient_id(10) = region_code(5) + patient_number(5)\nYou can check the region_code in ‘Region.csv’\nThere are two types of the patient_number\n\nlocal_num: The number given by the local government.\nglobal_num: The number given by the KCDC\n\nsex: the sex of the patient\nage: the age of the patient\n\n0s: 0 ~ 9\n10s: 10 ~ 19 …\n90s: 90 ~ 99\n100s: 100 ~ 109\n\ncountry: the country of the patient\nprovince: the province of the patient\ncity: the city of the patient\ninfection_case: the case of infection\ninfected_by: the ID of who infected the patient\n\nThis column refers to the ‘patient_id’ column.\n\ncontact_number: the number of contacts with people\nsymptom_onset_date: the date of symptom onset\nconfirmed_date: the date of being confirmed\nreleased_date: the date of being released\ndeceased_date: the date of being deceased\nstate: isolated / released / deceased\n\nisolated: being isolated in the hospital\nreleased: being released from the hospital\ndeceased: being deceased\n\n\n\npatientinfo.head()\n\n\n\n\n\n\n\n\npatient_id\nsex\nage\ncountry\nprovince\ncity\ninfection_case\ninfected_by\ncontact_number\nsymptom_onset_date\nconfirmed_date\nreleased_date\ndeceased_date\nstate\n\n\n\n\n0\n1000000001\nmale\n50s\nKorea\nSeoul\nGangseo-gu\noverseas inflow\nNaN\n75\n2020-01-22\n2020-01-23\n2020-02-05\nNaN\nreleased\n\n\n1\n1000000002\nmale\n30s\nKorea\nSeoul\nJungnang-gu\noverseas inflow\nNaN\n31\nNaN\n2020-01-30\n2020-03-02\nNaN\nreleased\n\n\n2\n1000000003\nmale\n50s\nKorea\nSeoul\nJongno-gu\ncontact with patient\n2002000001\n17\nNaN\n2020-01-30\n2020-02-19\nNaN\nreleased\n\n\n3\n1000000004\nmale\n20s\nKorea\nSeoul\nMapo-gu\noverseas inflow\nNaN\n9\n2020-01-26\n2020-01-30\n2020-02-15\nNaN\nreleased\n\n\n4\n1000000005\nfemale\n20s\nKorea\nSeoul\nSeongbuk-gu\ncontact with patient\n1000000002\n2\nNaN\n2020-01-31\n2020-02-24\nNaN\nreleased\n\n\n\n\n\n\n\n\nPatientRoute\n\n한국의 코로나19 환자 경로 데이터\n\npatient_id: the ID of the patient\ndate: YYYY-MM-DD\nprovince: Special City / Metropolitan City / Province(-do)\ncity: City(-si) / Country (-gun) / District (-gu)\nlatitude: the latitude of the visit (WGS84)\nlongitude: the longitude of the visit (WGS84)\n\n\npatientroute.head()\n\n\n\n\n\n\n\n\npatient_id\nglobal_num\ndate\nprovince\ncity\ntype\nlatitude\nlongitude\n\n\n\n\n0\n1000000001\n2.0\n2020-01-22\nGyeonggi-do\nGimpo-si\nairport\n37.615246\n126.715632\n\n\n1\n1000000001\n2.0\n2020-01-24\nSeoul\nJung-gu\nhospital\n37.567241\n127.005659\n\n\n2\n1000000002\n5.0\n2020-01-25\nSeoul\nSeongbuk-gu\netc\n37.592560\n127.017048\n\n\n3\n1000000002\n5.0\n2020-01-26\nSeoul\nSeongbuk-gu\nstore\n37.591810\n127.016822\n\n\n4\n1000000002\n5.0\n2020-01-26\nSeoul\nSeongdong-gu\npublic_transportation\n37.563992\n127.029534\n\n\n\n\n\n\n\n\nTime\n\n한국의 COVID-19 상태의 시계열 데이터\n\ndate: YYYY-MM-DD\ntime: Time (0 = AM 12:00 / 16 = PM 04:00)\n\nThe time for KCDC to open the information has been changed from PM 04:00 to AM 12:00 since March 2nd.\n\ntest: the accumulated number of tests\n\nA test is a diagnosis of an infection.\n\nnegative: the accumulated number of negative results\nconfirmed: the accumulated number of positive results\nreleased: the accumulated number of releases\ndeceased: the accumulated number of deceases\n\n\ntime.head()\n\n\n\n\n\n\n\n\ndate\ntime\ntest\nnegative\nconfirmed\nreleased\ndeceased\n\n\n\n\n0\n2020-01-20\n16\n1\n0\n1\n0\n0\n\n\n1\n2020-01-21\n16\n1\n0\n1\n0\n0\n\n\n2\n2020-01-22\n16\n4\n3\n1\n0\n0\n\n\n3\n2020-01-23\n16\n22\n21\n1\n0\n0\n\n\n4\n2020-01-24\n16\n27\n25\n2\n0\n0\n\n\n\n\n\n\n\n\nTimeAge\n\n한국의 연령별 코로나19 현황 시계열 데이터\n\ndate: YYYY-MM-DD\n\nThe status in terms of the age has been presented since March 2nd.\n\ntime: Time\nage: the age of patients\nconfirmed: the accumulated number of the confirmed\ndeceased: the accumulated number of the deceased\n\n\ntimeage.head()\n\n\n\n\n\n\n\n\ndate\ntime\nage\nconfirmed\ndeceased\n\n\n\n\n0\n2020-03-02\n0\n0s\n32\n0\n\n\n1\n2020-03-02\n0\n10s\n169\n0\n\n\n2\n2020-03-02\n0\n20s\n1235\n0\n\n\n3\n2020-03-02\n0\n30s\n506\n1\n\n\n4\n2020-03-02\n0\n40s\n633\n1\n\n\n\n\n\n\n\n\nTimeGender\n\n한국의 성별에 따른 COVID-19 현황의 시계열 데이터\n\ndate: YYYY-MM-DD\n\nThe status in terms of the gender has been presented since March 2nd.\n\ntime: Time\nsex: the gender of patients\nconfirmed: the accumulated number of the confirmed\ndeceased: the accumulated number of the deceased\n\n\ntimegender.head()\n\n\n\n\n\n\n\n\ndate\ntime\nsex\nconfirmed\ndeceased\n\n\n\n\n0\n2020-03-02\n0\nmale\n1591\n13\n\n\n1\n2020-03-02\n0\nfemale\n2621\n9\n\n\n2\n2020-03-03\n0\nmale\n1810\n16\n\n\n3\n2020-03-03\n0\nfemale\n3002\n12\n\n\n4\n2020-03-04\n0\nmale\n1996\n20\n\n\n\n\n\n\n\n\nTimeProvince\n\n한국의 지역별 코로나19 현황 시계열 데이터\n\ndate: YYYY-MM-DD\ntime: Time\nprovince: the province of South Korea\nconfirmed: the accumulated number of the confirmed in the province\n\nThe confirmed status in terms of the provinces has been presented since Feburary 21th.\nThe value before Feburary 21th can be different.\n\nreleased: the accumulated number of the released in the province\n\nThe confirmed status in terms of the provinces has been presented since March 5th. -The value before March 5th can be different.\n\ndeceased: the accumulated number of the deceased in the province\n\nThe confirmed status in terms of the provinces has been presented since March 5th.\nThe value before March 5th can be different.\n\n\n\ntimeprovince.head()\n\n\n\n\n\n\n\n\ndate\ntime\nprovince\nconfirmed\nreleased\ndeceased\n\n\n\n\n0\n2020-01-20\n16\nSeoul\n0\n0\n0\n\n\n1\n2020-01-20\n16\nBusan\n0\n0\n0\n\n\n2\n2020-01-20\n16\nDaegu\n0\n0\n0\n\n\n3\n2020-01-20\n16\nIncheon\n1\n0\n0\n\n\n4\n2020-01-20\n16\nGwangju\n0\n0\n0\n\n\n\n\n\n\n\n\nRegion\n\n대한민국 내 지역의 위치 및 통계 자료\n\ncode: the code of the region\nprovince: Special City / Metropolitan City / Province(-do)\ncity: City(-si) / Country (-gun) / District (-gu)\nlatitude: the latitude of the visit (WGS84)\nlongitude: the longitude of the visit (WGS84)\nelementary_school_count: the number of elementary schools\nkindergarten_count: the number of kindergartens\nuniversity_count: the number of universities\nacademy_ratio: the ratio of academies\nelderly_population_ratio: the ratio of the elderly population\nelderly_alone_ratio: the ratio of elderly households living alone\nnursing_home_count: the number of nursing homes\n\nSource of the statistic: KOSTAT (Statistics Korea)\n\nregion.head()\n\n\n\n\n\n\n\n\ncode\nprovince\ncity\nlatitude\nlongitude\nelementary_school_count\nkindergarten_count\nuniversity_count\nacademy_ratio\nelderly_population_ratio\nelderly_alone_ratio\nnursing_home_count\n\n\n\n\n0\n10000\nSeoul\nSeoul\n37.566953\n126.977977\n607\n830\n48\n1.44\n15.38\n5.8\n22739\n\n\n1\n10010\nSeoul\nGangnam-gu\n37.518421\n127.047222\n33\n38\n0\n4.18\n13.17\n4.3\n3088\n\n\n2\n10020\nSeoul\nGangdong-gu\n37.530492\n127.123837\n27\n32\n0\n1.54\n14.55\n5.4\n1023\n\n\n3\n10030\nSeoul\nGangbuk-gu\n37.639938\n127.025508\n14\n21\n0\n0.67\n19.49\n8.5\n628\n\n\n4\n10040\nSeoul\nGangseo-gu\n37.551166\n126.849506\n36\n56\n1\n1.17\n14.39\n5.7\n1080\n\n\n\n\n\n\n\n\nWeather\n\n한국 지역의 날씨 데이터\n\ncode: the code of the region\nprovince: Special City / Metropolitan City / Province(-do)\ndate: YYYY-MM-DD\navg_temp: the average temperature\nmin_temp: the lowest temperature\nmax_temp: the highest temperature\nprecipitation: the daily precipitation\nmax_wind_speed: the maximum wind speed\nmost_wind_direction: the most frequent wind direction\navg_relative_humidity: the average relative humidity\n\nSource of the weather data: KMA (Korea Meteorological Administration)\n\nweather.head()\n\n\n\n\n\n\n\n\ncode\nprovince\ndate\navg_temp\nmin_temp\nmax_temp\nprecipitation\nmax_wind_speed\nmost_wind_direction\navg_relative_humidity\n\n\n\n\n0\n10000\nSeoul\n2016-01-01\n1.2\n-3.3\n4.0\n0.0\n3.5\n90.0\n73.0\n\n\n1\n11000\nBusan\n2016-01-01\n5.3\n1.1\n10.9\n0.0\n7.4\n340.0\n52.1\n\n\n2\n12000\nDaegu\n2016-01-01\n1.7\n-4.0\n8.0\n0.0\n3.7\n270.0\n70.5\n\n\n3\n13000\nGwangju\n2016-01-01\n3.2\n-1.5\n8.1\n0.0\n2.7\n230.0\n73.1\n\n\n4\n14000\nIncheon\n2016-01-01\n3.1\n-0.4\n5.7\n0.0\n5.3\n180.0\n83.9\n\n\n\n\n\n\n\n\nSearchTrend\n\n국내 최대 포털인 네이버에서 검색된 키워드의 트렌드 데이터\n\ndate: YYYY-MM-DD\ncold: the search volume of ‘cold’ in Korean language\n\nThe unit means relative value by setting the highest search volume in the period to 100.\n\nflu: the search volume of ‘flu’ in Korean language\n\nSame as above.\n\npneumonia: the search volume of ‘pneumonia’ in Korean language -Same as above.\ncoronavirus: the search volume of ‘coronavirus’ in Korean language -Same as above.\n\nSource of the data: NAVER DataLab\n\nsearchtrend.head()\n\n\n\n\n\n\n\n\ndate\ncold\nflu\npneumonia\ncoronavirus\n\n\n\n\n0\n2016-01-01\n0.11663\n0.05590\n0.15726\n0.00736\n\n\n1\n2016-01-02\n0.13372\n0.17135\n0.20826\n0.00890\n\n\n2\n2016-01-03\n0.14917\n0.22317\n0.19326\n0.00845\n\n\n3\n2016-01-04\n0.17463\n0.18626\n0.29008\n0.01145\n\n\n4\n2016-01-05\n0.17226\n0.15072\n0.24562\n0.01381\n\n\n\n\n\n\n\n\nSeoulFloating\n\n대한민국 서울 유동인구 데이터(SK텔레콤 빅데이터 허브에서)\n\ndate: YYYY-MM-DD\nhour: Hour\nbirth_year: the birth year of the floating population\nsext: he sex of the floating population\nprovince: Special City / Metropolitan City / Province(-do)\ncity: City(-si) / Country (-gun) / District (-gu)\nfp_num: the number of floating population\n\nSource of the data: SKT Big Data Hub\n\nseoulfloating.head()\n\n\n\n\n\n\n\n\ndate\nhour\nbirth_year\nsex\nprovince\ncity\nfp_num\n\n\n\n\n0\n2020-01-01\n0\n20\nfemale\nSeoul\nDobong-gu\n19140\n\n\n1\n2020-01-01\n0\n20\nmale\nSeoul\nDobong-gu\n19950\n\n\n2\n2020-01-01\n0\n20\nfemale\nSeoul\nDongdaemun-gu\n25450\n\n\n3\n2020-01-01\n0\n20\nmale\nSeoul\nDongdaemun-gu\n27050\n\n\n4\n2020-01-01\n0\n20\nfemale\nSeoul\nDongjag-gu\n28880\n\n\n\n\n\n\n\n\nPolicy\n\n한국의 COVID-19에 대한 정부 정책 데이터\n\npolicy_id: the ID of the policy\ncountry: the country that implemented the policy\ntype: the type of the policy\ngov_policy: the policy of the government\ndetail: the detail of the policy\nstart_date: the start date of the policy\nend_date: the end date of the policy\n\n\npolicy.head()\n\n\n\n\n\n\n\n\npolicy_id\ncountry\ntype\ngov_policy\ndetail\nstart_date\nend_date\n\n\n\n\n0\n1\nKorea\nAlert\nInfectious Disease Alert Level\nLevel 1 (Blue)\n2020-01-03\n2020-01-19\n\n\n1\n2\nKorea\nAlert\nInfectious Disease Alert Level\nLevel 2 (Yellow)\n2020-01-20\n2020-01-27\n\n\n2\n3\nKorea\nAlert\nInfectious Disease Alert Level\nLevel 3 (Orange)\n2020-01-28\n2020-02-22\n\n\n3\n4\nKorea\nAlert\nInfectious Disease Alert Level\nLevel 4 (Red)\n2020-02-23\nNaN\n\n\n4\n5\nKorea\nImmigration\nSpecial Immigration Procedure\nfrom China\n2020-02-04\nNaN\n\n\n\n\n\n\n\n\n\n\n본론 \n\n주제1 - 어떤 연령층이 가장 많이 확진되었는가? \n\n주제1 - EDA \n\n# import packages - 사용할 패키지 불러오기\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom matplotlib import pyplot as plt\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\n\ntime.head() #time 데이터의 상위 5개를 확인\n\n\n\n\n\n\n\n\ndate\ntime\ntest\nnegative\nconfirmed\nreleased\ndeceased\n\n\n\n\n0\n2020-01-20\n16\n1\n0\n1\n0\n0\n\n\n1\n2020-01-21\n16\n1\n0\n1\n0\n0\n\n\n2\n2020-01-22\n16\n4\n3\n1\n0\n0\n\n\n3\n2020-01-23\n16\n22\n21\n1\n0\n0\n\n\n4\n2020-01-24\n16\n27\n25\n2\n0\n0\n\n\n\n\n\n\n\n\n#시간의 흐름에 따른 확진자 추이\nfig = go.Figure() #빈 도화지를 만든다는 개념\n\nfig.add_trace(go.Scatter(x=time['date'], y=time['confirmed'],\n                    mode='lines', \n                    name='확진(confirmed)')) \n                    #Scatter 형태의 플랏으로 x축은 time데이터의 date컬럼을 사용하고 y축은 time데이터의 confirmed 컬럼을 사용하고 표현 방법은 line이며 선의 이름은 확진으로 지정\n\nfig.update_layout(title='시간의 흐름에 따른 확진자 추이',\n                   xaxis_title='Date',\n                   yaxis_title='Number') #그래프의 제목과 x축 y축의 이름을 지정\n\nfig.show() #그래프를 출력해서 보이도록\n\n\n                                                \n\n\n해당 그래프를 보면 시간의 흐름에 따른 누적 확진자의 수를 나타낸것으로 20년 3월부터 가파른 경사를 보이면서 우상향해서 증가하는 추세를 보이고 있습니다\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=time['date'], y=time['confirmed'],\n                    mode='lines', \n                    name='확진(confirmed)'))\n\nfig.add_trace(go.Scatter(x=time['date'], y=time['released'],\n                    mode='lines', \n                    name='해제(released)'))\n                    \nfig.add_trace(go.Scatter(x=time['date'], y=time['deceased'],\n                    mode='lines', \n                    name='사망(deceased)'))\n\nfig.update_layout(title='시간의 흐름에 따른 코로나의 추이',\n                   xaxis_title='Date',\n                   yaxis_title='Number')\n\nfig.show()\n\n\n                                                \n\n\n다음 그래프는 시간의 흐름에 따른 코로나의 추이로 확진자와 격리해제자의 추이와 사망자의 추이에 대해서 보여주고 있으며 확진자와 격리해제자의 추이는 유사하게 우상향하는 모습을 보여주고 있으며 사망자는 확진자와 격리자의 수에 비해서는 적어서 눈에 보이는 변화는 없습니다.\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=time['date'], y=time['confirmed'],\n                    mode='lines', \n                    name='확진(confirmed)'))\n\nfig.add_trace(go.Scatter(x=time['date'],y=time['negative'],\n             mode='lines', name='음성(Negative)'))\n\nfig.add_trace(go.Scatter(x=time['date'],y=time['test'],\n             mode='markers', name='검사(Test)'))\n\nfig.update_layout(title='시간의 흐름에 따른 코로나의 검사 추이',\n                   xaxis_title='Date',\n                   yaxis_title='Number')\n\nfig.show()\n\n\n                                                \n\n\n해당 그래프는 시간의 흐름에 따른 코로나의 검사 추이로 검사수와 음성의 수가 거의 붙어서 우상향하는 모습이고 확진자는 이에 비해 변동이 없어 보이는 모습입니다. 이를 통해 검사를 많이 했지만 이에 비해서 확진이 된 정도는 상당히 적음을 알 수 있습니다.\n\nfig = go.Figure()\nfig.add_trace(go.Bar(x=time['date'],y=time['confirmed'].diff(), \n                     name='confirmed', marker_color='rgba(152, 0, 0, .8)'))\n\nfig.update_layout(title='일단위 확진자 수',\n                   xaxis_title='Date',\n                   yaxis_title='Number')\n\nfig.show()\n\n\n                                                \n\n\n다음은 일단위 확진자의 수를 보여주고 있습니다. 20년 3월 인근에서 800여명 까지 일일 확진되는 모습을 보이고 점차 감소하는 모습을 보이는 형태입니다\n\n\n주제1 - 연령대별 확진 비율 \n\ndisplay(timeage.head()) #timeage 데이터셋의 기본적인 형태를 파악하기 위해 상위 5개의 행만 추출\ndisplay(timeage.age.unique()) #timeage 데이터셋에서 age 컬럼에서 어떤 연령층이 있는지 unique 함수를 통해서 추출\n\n\n\n\n\n\n\n\ndate\ntime\nage\nconfirmed\ndeceased\n\n\n\n\n0\n2020-03-02\n0\n0s\n32\n0\n\n\n1\n2020-03-02\n0\n10s\n169\n0\n\n\n2\n2020-03-02\n0\n20s\n1235\n0\n\n\n3\n2020-03-02\n0\n30s\n506\n1\n\n\n4\n2020-03-02\n0\n40s\n633\n1\n\n\n\n\n\n\n\narray(['0s', '10s', '20s', '30s', '40s', '50s', '60s', '70s', '80s'],\n      dtype=object)\n\n\n\nage_list = timeage.age.unique()\nage_list #앞에서 설명한 연령대를 따로 추출하여 age_list라는 곳에 할당을 시킴 \n\narray(['0s', '10s', '20s', '30s', '40s', '50s', '60s', '70s', '80s'],\n      dtype=object)\n\n\n\nfig, ax = plt.subplots(figsize = (13,7)) #도화지(Figure : fig)를 깔고 그래프를 그릴 구역(Axes : ax)을 정의합니다. figsize를 통해서 도화지의 크기를 지정해준다\n#이는 objection oriented API으로 그래프의 각 부분을 객체로 지정하고 그리는 유형이다\nsns.barplot(age_list,timeage.confirmed[-9:])\nax.set_xlabel('age',size=13) #연령\nax.set_ylabel('number of case',size=13) #케이스의 횟수\nplt.title('Confirmed Cases by Age')\nplt.show()\n\nc:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning:\n\nPass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n\n\n\n\n\n\n연령대별로 분석을 해본결과 20대가 압도적으로 많은 수를 차지하고 있는 형태의 플랏을 볼 수가 있다\n2020년 연령대별 인구\n\nhttps://kosis.kr/visual/populationKorea/experienceYard/populationPyramid.do?mb=N&menuId=M_3_2\n\n해당 자료를 이용해서 인구수와 확진 비율을 확인하여 과연 20대가 인구수가 많아서 이렇게 많이 확진이 되었는가에 대해서 알아본다\n\nage_order = pd.DataFrame() #빈 데이터 프레임을 생성\nage_order['age']  = age_list #앞서 생성한 age_list를 새로 만드는 데이터 프레임에 age라는 이름의 컬럼으로 할당\nage_order['population'] = [4054901, 4769187, 7037893, 7174782, 8257903, 8575336, 6476602, 3598811, 1657942] #population이라는 컬럼에 통계청 홈페이지에서 확인한 값을 입력\nage_order['proportion'] = round(age_order['population']/sum(age_order['population'])*100,2) \n#인구 비율을 구하기 위해 모든 인구수를 더하고 각 연령별로 나누고 소수점으로 나오는것을 방지하기 위해 100을 곱하고 소수점 2번째 자리까지 표현이 되도록 설정\nage_order = age_order.sort_values('age') #age를 기준으로 재정렬\nage_order.set_index(np.arange(1,10),inplace=True) #인덱스의 설정을 1~10순으로 들어가도록 설정하고 본래 있던것은 대체해서 사용하도록\nage_order\n\n\n\n\n\n\n\n\nage\npopulation\nproportion\n\n\n\n\n1\n0s\n4054901\n7.86\n\n\n2\n10s\n4769187\n9.24\n\n\n3\n20s\n7037893\n13.64\n\n\n4\n30s\n7174782\n13.90\n\n\n5\n40s\n8257903\n16.00\n\n\n6\n50s\n8575336\n16.62\n\n\n7\n60s\n6476602\n12.55\n\n\n8\n70s\n3598811\n6.97\n\n\n9\n80s\n1657942\n3.21\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(13, 7)) \nplt.title('Korea Age Proportion', fontsize=17)\nsns.barplot(age_list, age_order.proportion[-9:])\nax.set_xlabel('Age', size=13)\nax.set_ylabel('Rate (%)', size=13)\nplt.show() \n#한국의 2020년 연령별 인구의 수를 나타낸 표이다 \n\nc:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning:\n\nPass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n\n\n\n\n\n\n예상과는 다르게 20대가 가장 많은 인구수를 가지고 있는 연령대가 아닌 40,50대가 가장 인구수가 많은 연령대임을 알 수 있다. 이로 20대 인구수가 다른 연령층에 비해 많기 때문에 확진이 많이 된것은 아니다.\n\nconfirmed_by_population = age_order.sort_values('age') #'age'라는 컬럼으로 정렬\nconfirmed_by_population['confirmed'] = list(timeage[-9:].confirmed) #confirmed라는 컬럼을 만들고 timeage의 해당 리스트를 할당 시킴\n\n\nconfirmed_by_population['confirmed_ratio'] = confirmed_by_population['confirmed']/confirmed_by_population['population'] *100 #인구비율에 따른 확진 비율 컬럼 추가\ndisplay(confirmed_by_population)\n\n\n\n\n\n\n\n\nage\npopulation\nproportion\nconfirmed\nconfirmed_ratio\n\n\n\n\n1\n0s\n4054901\n7.86\n193\n0.004760\n\n\n2\n10s\n4769187\n9.24\n708\n0.014845\n\n\n3\n20s\n7037893\n13.64\n3362\n0.047770\n\n\n4\n30s\n7174782\n13.90\n1496\n0.020851\n\n\n5\n40s\n8257903\n16.00\n1681\n0.020356\n\n\n6\n50s\n8575336\n16.62\n2286\n0.026658\n\n\n7\n60s\n6476602\n12.55\n1668\n0.025754\n\n\n8\n70s\n3598811\n6.97\n850\n0.023619\n\n\n9\n80s\n1657942\n3.21\n556\n0.033536\n\n\n\n\n\n\n\n\n## 3. Plot confirmed rate by age\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Population-adjusted Confirmed Rate by Age', fontsize=17)\nsns.barplot(age_list, confirmed_by_population.confirmed_ratio[-9:])\nax.set_xlabel('Age', size=13)\nax.set_ylabel('Confirmed rate (%)', size=13)\nplt.show() #인구 비율에 따른 확진 확률\n\nc:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning:\n\nPass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n\n\n\n\n\n\n인구 비율에 따른 확진자의 수를 보아도 20대가 인구수가 많은 연령대인 40,50대 보다도 확연하게 많은 것을 알 수 있으며 오히려 80대 이상의 연령대가 차지하는 비율이 증가하였다.\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(23, 7)) #1행 2열의 도화지를 생성\n\n## 1. Confirmed Cases by Age\nax[0].set_title('Confirmed Cases by Age', fontsize=15)\nax[0].bar(age_list, confirmed_by_population.confirmed)\n\n## 2. Population-adjusted Confirmed Rate\nax[1].set_title('Population-adjusted Confirmed Rate', fontsize=15)\nax[1].bar(age_list, confirmed_by_population.confirmed_ratio)\n\nplt.show() \n\n\n\n\n다음 두개의 플랏을 보면 앞선 그래프에서는 20대의 확진 확률이 다른 연령대에 비해서 앞도적으로 높았지만 인구의 비율에 따른 확진의 비율을 나타내는 두번째 플랏을 보면 아직도 20대가 다른 연령대에 비해서 높기는 하지만 첫번째 그래프에 비해서는 조금은 낮아진 모습과 60~80대까지 연령층의 비중이 조금은 증가 했다는 사실을 두개의 플랏을 비교하면서 알 수있습니다 따라서 저는 다른 연령에 비해 압도적으로 많은 확진 비율을 가지고 있는 20대에 대해서 집중적으로 분석을 해보도록 하겠습니다.\n\n\n주제1 - 연령대별 확진 경로 \n\npatientinfo.head()\n\n\n\n\n\n\n\n\npatient_id\nsex\nage\ncountry\nprovince\ncity\ninfection_case\ninfected_by\ncontact_number\nsymptom_onset_date\nconfirmed_date\nreleased_date\ndeceased_date\nstate\n\n\n\n\n0\n1000000001\nmale\n50s\nKorea\nSeoul\nGangseo-gu\noverseas inflow\nNaN\n75\n2020-01-22\n2020-01-23\n2020-02-05\nNaN\nreleased\n\n\n1\n1000000002\nmale\n30s\nKorea\nSeoul\nJungnang-gu\noverseas inflow\nNaN\n31\nNaN\n2020-01-30\n2020-03-02\nNaN\nreleased\n\n\n2\n1000000003\nmale\n50s\nKorea\nSeoul\nJongno-gu\ncontact with patient\n2002000001\n17\nNaN\n2020-01-30\n2020-02-19\nNaN\nreleased\n\n\n3\n1000000004\nmale\n20s\nKorea\nSeoul\nMapo-gu\noverseas inflow\nNaN\n9\n2020-01-26\n2020-01-30\n2020-02-15\nNaN\nreleased\n\n\n4\n1000000005\nfemale\n20s\nKorea\nSeoul\nSeongbuk-gu\ncontact with patient\n1000000002\n2\nNaN\n2020-01-31\n2020-02-24\nNaN\nreleased\n\n\n\n\n\n\n\n\npatientinfo['infection_case'] = patientinfo['infection_case'].astype(str).apply(lambda x:x.split()[0])\n#PatientInfo['infection_case']\ninfectionCase = patientinfo.pivot_table(index='infection_case',columns='age',\nvalues='patient_id',aggfunc=\"count\")\n#infectionCase\n#전체 감염 케이스\npatientTotal = infectionCase.fillna(0).sum(axis=1)\npatientTotal = patientTotal.sort_values(ascending = False)[:5]\n# 20대 감염 케이스\npatient20s = infectionCase['20s'].dropna()\npatient20sTop = patient20s.sort_values(ascending=False)[:5]\n\n\ndisplay(patientTotal)\ndisplay(patient20sTop)\n\ninfection_case\ncontact     1112.0\nnan          827.0\noverseas     653.0\netc          638.0\nGuro-gu      112.0\ndtype: float64\n\n\ninfection_case\noverseas       269.0\nnan            221.0\ncontact        172.0\netc            127.0\nShincheonji     41.0\nName: 20s, dtype: float64\n\n\n\nfig = go.Figure()\nfig.add_trace(go.Pie(labels=patientTotal.index, values=patientTotal.values,\n                     textinfo='label+percent'))\n\nfig.update_layout(title='Confirmed infection case Total AGe')\n\nfig.show()\n\n\n                                                \n\n\n전체 연령측의 감염원인에 대해서 본다면 접촉에 의한 확진이 33% 해외 입국이 19% 그외 nan과 etc가 각각 24,19%의 비율을 차지하고 있다\n\nfig = go.Figure()\nfig.add_trace(go.Pie(labels=patient20sTop.index, values=patient20sTop.values,\n                     textinfo='label+percent'))\n\nfig.update_layout(title='Confirmed infection case 20s AGe')\n\nfig.show()\n\n\n                                                \n\n\n20대 연령의 그룹은 전체연령에 비해 해외입국과 nan이 각각 32 26%를 차지 하고 있다. 그러나 해당 데이터에는 원인을 알수 없는 nan데이가 전체의 1/4가량을 차지 하기 때문에 정확한 분석을 하기 어렵다\n그렇다면 확진된 20대가 많이 돌아다닌 장소에 대해서 patientinfo 데이터를 이용해서 찾아보겠습니다\n\npatientroute = pd.read_csv(path+'PatientRoute.csv')\n\n\npatientroute.head()\n\n\n\n\n\n\n\n\npatient_id\nglobal_num\ndate\nprovince\ncity\ntype\nlatitude\nlongitude\n\n\n\n\n0\n1000000001\n2.0\n2020-01-22\nGyeonggi-do\nGimpo-si\nairport\n37.615246\n126.715632\n\n\n1\n1000000001\n2.0\n2020-01-24\nSeoul\nJung-gu\nhospital\n37.567241\n127.005659\n\n\n2\n1000000002\n5.0\n2020-01-25\nSeoul\nSeongbuk-gu\netc\n37.592560\n127.017048\n\n\n3\n1000000002\n5.0\n2020-01-26\nSeoul\nSeongbuk-gu\nstore\n37.591810\n127.016822\n\n\n4\n1000000002\n5.0\n2020-01-26\nSeoul\nSeongdong-gu\npublic_transportation\n37.563992\n127.029534\n\n\n\n\n\n\n\n\npatientroute[['patient_id','date','type']] #필요한 컬럼만 선택\n\n\n\n\n\n\n\n\npatient_id\ndate\ntype\n\n\n\n\n0\n1000000001\n2020-01-22\nairport\n\n\n1\n1000000001\n2020-01-24\nhospital\n\n\n2\n1000000002\n2020-01-25\netc\n\n\n3\n1000000002\n2020-01-26\nstore\n\n\n4\n1000000002\n2020-01-26\npublic_transportation\n\n\n...\n...\n...\n...\n\n\n6709\n6100000090\n2020-03-24\nairport\n\n\n6710\n6100000090\n2020-03-24\nairport\n\n\n6711\n6100000090\n2020-03-25\nstore\n\n\n6712\n6100000090\n2020-03-25\nhospital\n\n\n6713\n6100000090\n2020-03-25\nstore\n\n\n\n\n6714 rows × 3 columns\n\n\n\n\nplaces = patientroute.type.unique()\nsimproute = patientroute[['patient_id','date','type']]\nagedf = patientinfo[['patient_id','age']]\nsimproutewage = pd.merge(simproute,agedf,how='left')\nfiplot = simproutewage.set_index('type')\nfiplot_count = fiplot.groupby('type').count().patient_id.sort_values()\n\n\nfig = fiplot_count.iplot(asFigure = True, kind='bar')\nfig.show()\n\n\n                                                \n\n\n해당 그래프는 전체 연령의 확진 원인에 대한 것을 카운트 시킨 결과로 etc와 hospital이 가장 많은 것을 보이나 병원은 확진자가 이상증세를 느끼고 찾아가는 당연한 경로이므로 제외를 하고 etc 또한 어느 곳에 다녀왔는지 정확하게 알 수 없어서 제외를 하고 다시 진행을 해보겠습니다.\n\ntwtfi = fiplot[fiplot.age == '20s']\nuntwtfi = fiplot[fiplot.age != '20s']\ntwt = twtfi.groupby('type').count().patient_id\nuntwt = untwtfi.groupby('type').count().patient_id\ntwt = twt[~twt.index.isin( ['etc','hospital'])]\nuntwt = untwt[~untwt.index.isin( ['etc','hospital'])]\nfig = go.Figure()\nfig.add_trace(go.Bar(x = twt.index, \n                     y = twt,\n                     name = '20s',\n                     marker_color='indianred'))\nfig.add_trace(go.Bar(x = untwt.index, \n                     y = untwt,\n                     name = 'except 20s',\n                     marker_color='lightsalmon'))\nfig.update_layout(barmode='group', xaxis_tickangle=-45)\nfig.show()\n\n\n                                                \n\n\n다음 그래프는 20대와 그외의 연령층이 확진된 원인에 대한것으로 앞서 말한것 처럼 etc와 hospital은 가장 많은 비율을 차지 하지만 분석을 하는데 크게 도움이 되지 않는다고 판단을 하여 제외를 하고 진행을 한 결과 이다.\n공통적으로 많이 방문하는 store과 church는 비슷한 양상을 보여주고 있습니다. 하지만, 20대는 restaurant, pc방, cafe, bar 등에서 훨씬 많은 방문비율을 확인할 수 있었습니다.\n\n\n\n주제2 - 코로나는 누구에게 가장 치명적인가 \n\n주제2 - 연령별로 확진자의 치명률 \n\ntime.head()\n\n\n\n\n\n\n\n\ndate\ntime\ntest\nnegative\nconfirmed\nreleased\ndeceased\n\n\n\n\n0\n2020-01-20\n16\n1\n0\n1\n0\n0\n\n\n1\n2020-01-21\n16\n1\n0\n1\n0\n0\n\n\n2\n2020-01-22\n16\n4\n3\n1\n0\n0\n\n\n3\n2020-01-23\n16\n22\n21\n1\n0\n0\n\n\n4\n2020-01-24\n16\n27\n25\n2\n0\n0\n\n\n\n\n\n\n\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=time['date'], y=time['deceased'],\n                    mode='lines', # Line plot만 그리기\n                    name='사망(deceased)'))\n\nfig.update_layout(title='시간의 흐름에 따른 확진자 사망 추이',\n                   xaxis_title='Date',\n                   yaxis_title='Number')\n\nfig.show()\n\n\n                                                \n\n\n해당 그래프는 시간의 흐름에 따른 확진자의 사망 추이로서 2020년 3월 부터 계속해서 우상향하는 모습을 볼 수 있다\n\nfig, ax = plt.subplots(figsize = (13,7)) #도화지(Figure : fig)를 깔고 그래프를 그릴 구역(Axes : ax)을 정의합니다. figsize를 통해서 도화지의 크기를 지정해준다\n#이는 objection oriented API으로 그래프의 각 부분을 객체로 지정하고 그리는 유형이다\nsns.barplot(age_list,timeage.deceased[-9:])\nax.set_xlabel('age',size=13) #연령\nax.set_ylabel('number of case',size=13) #케이스의 횟수\nplt.title('Deceased Cases by Age')\nplt.show()\n\nc:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning:\n\nPass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n\n\n\n\n\n\n나이가 많아질수록 사망자의 비율이 높다 과연 나이가 많아질수록 인구수가 많아져서 이러한 현상이 나오는 지 인구 비율에 따른 사망자에 대해서 다시 한번 살펴보자\n\nconfirmed_by_population = age_order.sort_values('age')\nconfirmed_by_population['deceased'] = list(timeage[-9:].deceased)\n\n# 2. Get confirmed ratio regarding population\nconfirmed_by_population['deceased_ratio'] = confirmed_by_population['deceased']/confirmed_by_population['population'] *100\ndisplay(confirmed_by_population)\n\n\n\n\n\n\n\n\nage\npopulation\nproportion\ndeceased\ndeceased_ratio\n\n\n\n\n1\n0s\n4054901\n7.86\n0\n0.000000\n\n\n2\n10s\n4769187\n9.24\n0\n0.000000\n\n\n3\n20s\n7037893\n13.64\n0\n0.000000\n\n\n4\n30s\n7174782\n13.90\n2\n0.000028\n\n\n5\n40s\n8257903\n16.00\n3\n0.000036\n\n\n6\n50s\n8575336\n16.62\n15\n0.000175\n\n\n7\n60s\n6476602\n12.55\n41\n0.000633\n\n\n8\n70s\n3598811\n6.97\n82\n0.002279\n\n\n9\n80s\n1657942\n3.21\n139\n0.008384\n\n\n\n\n\n\n\n\n## 3. Plot confirmed rate by age\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Population-adjusted Deceased Rate by Age', fontsize=17)\nsns.barplot(age_list, confirmed_by_population.deceased_ratio[-9:])\nax.set_xlabel('Age', size=13)\nax.set_ylabel('Deceased rate (%)', size=13)\nplt.show() #인구 비율에 따른 확진 확률\n\nc:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning:\n\nPass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n\n\n\n\n\n\n인구 비율에 따른 사망자의 비율을 살펴본 결과이다 0~20대 까지는 사망자는 없으며 30대부터 확진으로 인한 사망자가 존재한다. 그러나 80대 이상의 연령층의 경우는 가장 많은 인구수를 가진 연령층도 아니지만 사망자의 비중이 가장 높은 것을 볼 수 있다. 이로 코로나 바이러스는 고연령층 일수록 가장 치명적인 질병임을 예측할 수 있다.\n그렇다면 고연령층의 확진 원인에 대해서 알아보자\n\n\n주제2 - 연령대의 확진 원인 \n\naged_pat = patientinfo[(patientinfo['age'] == '60s')|(patientinfo['age'] == '70s')|\n                (patientinfo['age'] == '80s')][['province','age','infection_case']]\n                \naged_inf = pd.DataFrame(aged_pat['infection_case'].value_counts())\n#고연령측의 확진 원인\n\n\npatientinfo['infection_case'] = patientinfo['infection_case'].astype(str).apply(lambda x:x.split()[0])\n#PatientInfo['infection_case']\ninfectionCase = patientinfo.pivot_table(index='infection_case',columns='age',\nvalues='patient_id',aggfunc=\"count\")\n#infectionCase\n#전체 감염 케이스\npatientTotal = infectionCase.fillna(0).sum(axis=1)\npatientTotal = patientTotal.sort_values(ascending = False)[:5]\n# 60대 감염 케이스\npatient60s = infectionCase['60s'].dropna()\npatient60sTop = patient60s.sort_values(ascending=False)[:5]\n# 70대 감염 케이스\npatient70s = infectionCase['70s'].dropna()\npatient70sTop = patient70s.sort_values(ascending=False)[:5]\n# 80대 감염 케이스\npatient80s = infectionCase['80s'].dropna()\npatient80sTop = patient80s.sort_values(ascending=False)[:5]\n\n\nfig = go.Figure()\nfig.add_trace(go.Pie(labels=patient60sTop.index, values=patient60sTop.values,\n                     textinfo='label+percent'))\n\nfig.update_layout(title='Confirmed infection case 60s AGe')\n\nfig.show()\n\n\n                                                \n\n\n\nfig = go.Figure()\nfig.add_trace(go.Pie(labels=patient70sTop.index, values=patient70sTop.values,\n                     textinfo='label+percent'))\n\nfig.update_layout(title='Confirmed infection case 70s AGe')\n\nfig.show()\n\n\n                                                \n\n\n\nfig = go.Figure()\nfig.add_trace(go.Pie(labels=patient80sTop.index, values=patient80sTop.values,\n                     textinfo='label+percent'))\n\nfig.update_layout(title='Confirmed infection case 80s AGe')\n\nfig.show()\n\n\n                                                \n\n\n60,70,80대의 확진 원인을 보니 nan과 etc가 많기는 하지만 다른 연령대에 비해서 contact가 많다는 사실을 알수 있다. 그래도 nan과 etc가 많아 확진의 주요 원인을 접촉에 의해서 라고 단정할 수는 없다\n그렇다면 나이가 많은 사람들은 완치 기간이 길어서 치명률이 높은 것이지 이에 대한 관계에 대해 알아보았습니다\n\n\n주제2 - 연령대별 회복기간 \n\npatientinfo\n\n\n\n\n\n\n\n\npatient_id\nsex\nage\ncountry\nprovince\ncity\ninfection_case\ninfected_by\ncontact_number\nsymptom_onset_date\nconfirmed_date\nreleased_date\ndeceased_date\nstate\n\n\n\n\n0\n1000000001\nmale\n50s\nKorea\nSeoul\nGangseo-gu\noverseas\nNaN\n75\n2020-01-22\n2020-01-23\n2020-02-05\nNaN\nreleased\n\n\n1\n1000000002\nmale\n30s\nKorea\nSeoul\nJungnang-gu\noverseas\nNaN\n31\nNaN\n2020-01-30\n2020-03-02\nNaN\nreleased\n\n\n2\n1000000003\nmale\n50s\nKorea\nSeoul\nJongno-gu\ncontact\n2002000001\n17\nNaN\n2020-01-30\n2020-02-19\nNaN\nreleased\n\n\n3\n1000000004\nmale\n20s\nKorea\nSeoul\nMapo-gu\noverseas\nNaN\n9\n2020-01-26\n2020-01-30\n2020-02-15\nNaN\nreleased\n\n\n4\n1000000005\nfemale\n20s\nKorea\nSeoul\nSeongbuk-gu\ncontact\n1000000002\n2\nNaN\n2020-01-31\n2020-02-24\nNaN\nreleased\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5160\n7000000015\nfemale\n30s\nKorea\nJeju-do\nJeju-do\noverseas\nNaN\n25\nNaN\n2020-05-30\n2020-06-13\nNaN\nreleased\n\n\n5161\n7000000016\nNaN\nNaN\nKorea\nJeju-do\nJeju-do\noverseas\nNaN\nNaN\nNaN\n2020-06-16\n2020-06-24\nNaN\nreleased\n\n\n5162\n7000000017\nNaN\nNaN\nBangladesh\nJeju-do\nJeju-do\noverseas\nNaN\n72\nNaN\n2020-06-18\nNaN\nNaN\nisolated\n\n\n5163\n7000000018\nNaN\nNaN\nBangladesh\nJeju-do\nJeju-do\noverseas\nNaN\nNaN\nNaN\n2020-06-18\nNaN\nNaN\nisolated\n\n\n5164\n7000000019\nNaN\nNaN\nBangladesh\nJeju-do\nJeju-do\noverseas\nNaN\nNaN\nNaN\n2020-06-18\nNaN\nNaN\nisolated\n\n\n\n\n5165 rows × 14 columns\n\n\n\n\nfrom datetime import datetime\n\n\npat_rel = patientinfo[['age','confirmed_date','released_date']]\n#pat_rel['diff'] = pat_rel.released_date - pat_rel.confirmed_date\nstr(pat_rel.confirmed_date)\npat_rel.released_date = pd.to_datetime(pat_rel['released_date'], format='%Y %m %d')\npat_rel.confirmed_date = pd.to_datetime(pat_rel['confirmed_date'], format='%Y %m %d')\npat_rel['diff'] = pat_rel.released_date - pat_rel.confirmed_date\npat_rel = pat_rel[:5161] #격리 날짜 없는 것 삭제\ndisplay(pat_rel)\n\n\n\n\n\n\n\n\nage\nconfirmed_date\nreleased_date\ndiff\n\n\n\n\n0\n50s\n2020-01-23\n2020-02-05\n13 days\n\n\n1\n30s\n2020-01-30\n2020-03-02\n32 days\n\n\n2\n50s\n2020-01-30\n2020-02-19\n20 days\n\n\n3\n20s\n2020-01-30\n2020-02-15\n16 days\n\n\n4\n20s\n2020-01-31\n2020-02-24\n24 days\n\n\n...\n...\n...\n...\n...\n\n\n5156\n30s\n2020-04-03\n2020-05-19\n46 days\n\n\n5157\n20s\n2020-04-03\n2020-05-05\n32 days\n\n\n5158\n10s\n2020-04-14\n2020-04-26\n12 days\n\n\n5159\n30s\n2020-05-09\n2020-06-12\n34 days\n\n\n5160\n30s\n2020-05-30\n2020-06-13\n14 days\n\n\n\n\n5161 rows × 4 columns\n\n\n\n\ndisplay(pat_rel['diff'].mean()) \ndisplay(pat_rel['diff'].min()) \ndisplay(pat_rel['diff'].max()) \n\nTimedelta('24 days 17:48:39.041614123')\n\n\nTimedelta('0 days 00:00:00')\n\n\nTimedelta('114 days 00:00:00')\n\n\n평균 완치일은 24일\n최대 완치일은 114일 입니다.\n\npat_rel['over_avg'] = np.where(pat_rel['diff']&gt;'24 days 17:48:39.041614123',1,0)\nover_av_released = pat_rel[pat_rel['over_avg']==1]\nunder_av_released = pat_rel[pat_rel['over_avg']==0]\n\nover_av=pd.DataFrame(over_av_released['age'].value_counts().sort_index()).reset_index()\nunder_av=pd.DataFrame(under_av_released['age'].value_counts().sort_index()).reset_index()\n\n#연령대층별로 감염자수가 확연히 다르기때문에 각 연령층별의 비율로 계산\nunder_av['per']=under_av['age']/(under_av['age']+over_av['age']) \nover_av['per']=over_av['age']/(under_av['age']+over_av['age'])\n\n#컬럼 재정리\nunder_av.columns=['age', 'count', 'under_per']\nover_av.columns=['age', 'count', 'over_per']\n\n\nover_av = pd.DataFrame({'age':['0s','10s','20s','30s','40s','50s','60s','70s','80s','90s'],\n                             'count':[7,20,156,90,86,119,90,46,39,8],\n                             'over_per':[0.106061,0.006289,0.026212,0.264856,0.172414,0.135647,0.232877,0.326087,0.2598887,0.487500]})\n\n\nfig = go.Figure()\n\nfig.add_trace(go.Bar(x=under_av.under_per, y=under_av.age, name='빠른 완치 기간',\n                     orientation='h'))\n\nfig.add_trace(go.Bar(x=over_av.over_per, y=over_av.age, name='오랜 완치 기간',\n                     text=over_av.over_per, texttemplate='%{x:.1%}', textposition='inside',\n                    textfont=dict(color='white'),\n                    orientation='h'))\nfig.update_layout(barmode='stack',\n                  paper_bgcolor='rgb(248, 248, 255)',\n                  plot_bgcolor='rgb(248, 248, 255)',\n                 )\nfig.update_layout(title='연령대에 따른 회복 기간')\n\nfig.show()\n\n\n                                                \n\n\n그래프를 본다면 0~20대의 연령츠은 10% 미만으로 평균보다 빠른 완치 기간을 가지고 있음을 알 수 있습니다. 고연령층인 70,80,90대의 경우 32,26,48%로 다른 연령층에 비해서 높기는 하지만 과반을 넘지 않기에 고연령층이라고 모두가 장기간의 회복 기간을 가진다고 판단하기 어렵습니다.\n\n\n\n주제3 - 정부의 정책은 타당했는가? \n\n주제3 - 감염병 경보 단계 공표 시점 \n\npolicy.head()\n\n\n\n\n\n\n\n\npolicy_id\ncountry\ntype\ngov_policy\ndetail\nstart_date\nend_date\n\n\n\n\n0\n1\nKorea\nAlert\nInfectious Disease Alert Level\nLevel 1 (Blue)\n2020-01-03\n2020-01-19\n\n\n1\n2\nKorea\nAlert\nInfectious Disease Alert Level\nLevel 2 (Yellow)\n2020-01-20\n2020-01-27\n\n\n2\n3\nKorea\nAlert\nInfectious Disease Alert Level\nLevel 3 (Orange)\n2020-01-28\n2020-02-22\n\n\n3\n4\nKorea\nAlert\nInfectious Disease Alert Level\nLevel 4 (Red)\n2020-02-23\nNaN\n\n\n4\n5\nKorea\nImmigration\nSpecial Immigration Procedure\nfrom China\n2020-02-04\nNaN\n\n\n\n\n\n\n\n\npolicy.isna().sum() #여기서 end_date의 NA값이 너무 많은 것을 알 수 있다. 따라서 기점을 start_date로 지정하고 사용을 하겠다.\n\npolicy_id      0\ncountry        0\ntype           0\ngov_policy     0\ndetail         2\nstart_date     0\nend_date      37\ndtype: int64\n\n\n\npolicy_alerts = policy[policy.type == 'Alert']\ndisplay(policy_alerts)\n\n\n\n\n\n\n\n\npolicy_id\ncountry\ntype\ngov_policy\ndetail\nstart_date\nend_date\n\n\n\n\n0\n1\nKorea\nAlert\nInfectious Disease Alert Level\nLevel 1 (Blue)\n2020-01-03\n2020-01-19\n\n\n1\n2\nKorea\nAlert\nInfectious Disease Alert Level\nLevel 2 (Yellow)\n2020-01-20\n2020-01-27\n\n\n2\n3\nKorea\nAlert\nInfectious Disease Alert Level\nLevel 3 (Orange)\n2020-01-28\n2020-02-22\n\n\n3\n4\nKorea\nAlert\nInfectious Disease Alert Level\nLevel 4 (Red)\n2020-02-23\nNaN\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(13,7))\nplt.title(\"Infection Disease Alert Level\",size=17)\nplt.plot(time.date.unique(), time.confirmed.diff(),\n         color = 'gray', lw = 3)\nax.set_xticks(ax.get_xticks()[::int(len(time.date.unique())/8)])\n\nfor day, color in zip(policy_alerts.start_date.values[1:], ['yellow','orange','red']):\n    ax.axvline(day, ls=':', color=color, lw=4)\nax.legend(['confirmed','alert level 2','alert level 3','alert level 4'])\nplt.xlabel('date')\nplt.ylabel('Number of Confirmed')\nplt.show()\n\n\n\n\n다음은 감염병의 경보 단계 별로 해당 시점과 확진자의 일일 추이를 나타낸 것으로 2,3단계는 발생을 하고 국내에 들어온 시점에 공표가 되었고 가장 강력한 단계인 4단계는 일일 확진자가 정점에 이르기 전에 공표가 된 사실을 알수 있습니다.\n그렇다면 정부의 거리두기는 어떠하였는지 알아보겠습니다.\n\n\n주제3 - 정부의 거리두기 공표 시점 \n\npolicy_social = policy[policy.type == 'Social'][:-1]\ndisplay(policy_social)\n\n\n\n\n\n\n\n\npolicy_id\ncountry\ntype\ngov_policy\ndetail\nstart_date\nend_date\n\n\n\n\n28\n29\nKorea\nSocial\nSocial Distancing Campaign\nStrong\n2020-02-29\n2020-03-21\n\n\n29\n30\nKorea\nSocial\nSocial Distancing Campaign\nStrong\n2020-03-22\n2020-04-19\n\n\n30\n31\nKorea\nSocial\nSocial Distancing Campaign\nWeak\n2020-04-20\n2020-05-05\n\n\n31\n32\nKorea\nSocial\nSocial Distancing Campaign\nWeak(1st)\n2020-05-06\nNaN\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(13,7))\nplt.title(\"Social Distancing Campaign\",size=17)\nplt.plot(time.date.unique(), time.confirmed.diff(),\n         color = 'gray', lw = 3)\nax.set_xticks(ax.get_xticks()[::int(len(time.date.unique())/8)])\n\nfor day, color in zip(policy_social.start_date.values[:], ['red','red','orange']):\n    ax.axvline(day, ls=':', color=color, lw=4)\nax.legend(['confirmed','strong','strong','weak'])\nplt.xlabel('date')\nplt.ylabel('Number of Confirmed')\nplt.show()\n\n\n\n\n3월 23일에 감염병 경고가 4단계 발표 되고 나서 3월 29일부터 사회적 거리두기를 진행 하였습니다. 두번의 강한 거리두기를 유지하고 4월 20일부터 거리두기의 단계를 낮췄습니다. 거리두기를 확진자의 정점에 다다르는 시점에서 발표하고 그 이후 감소하는 일일 확진자의 수를 확인할 수 있다 따라서 분석을 하고 있는 해당 기간 동안은 정부의 방역 정책은 성공을 했다고 할 수 있다\n\n\n\n\n결론 \n\n20대가 분석 기간동안 가장 많이 확진이 된 연령층이었으며, 20대의 인구수가 다른 연령층에 비해서 많아서가 아닌 다른 연령층에 비해서 활동 반경이 넓고 활발하며 불필요한 방문 지역과 장소에 자주 방문을 하였기 때문에 20대가 가장 많이 확진된 연령층이라는 결론을 내렸습니다.\n코로나 바이러스는 고연령층이 될수록 더욱 치명적이라는 사실을 얻었습니다. 치명률이 인구 대비 비율로 살펴보아도 80대 이상이 가장 압도적으로 많았고 60,70대 또한 적지 않은 치명률을 가지고 있음을 알게 되었으며, 완치까지 걸리는 기간은 고연령층일수록 저연령층에 비해서 평균 이상으로 오래 걸리기는 하였으나 과반이상이거나 다른 연령층에 비해서 조금은 많치만 크게 상관성이 있어보지는 않았습니다.\n정부의 거리두기는 당시 신천지로 인해서 일일 확진자가 800명 가량 나오던 시점에 공표되고 그 이후로 분석기간 동안에는 감소세를 보였습니다. 이로 정부의 거리두기는 적어도 당시에는 타당했다라는 결론을 내리게 되었습니다.\n\n코로나 종식 및 예방을 위해서는 해외 유입에 의한 확진자를 차단해야합니다. 현재 입국자에 대한 검사 및 2주 자가격리 등 많은 노력이 진행되고 있습니다. 하지만, 그럼에도 유의사항을 잘 따르지 않는 일부 인원에 의해서 신천지와 같은 큰 집단 감염이 발생될 수 있다는 사실을 잊지 말아야합니다. 따라서, 코로나에 대한 경각심과 인식을 잘 심어주어야하며, 특히나 가장 안일하게 생각하는 20대의 인식 변화를 이끌어야 할 것입니다. 또한, 20대의 행동 패턴 및 방문 경로를 바탕으로 감염 위험이 있는 업종은 특히나 더욱 신경써서 사회적 거리두기, 마스크 착용, 손세정제와 손씻기 등을 더욱 권장하도록 해야합니다.\n분석을 하면서 느낀 한계점은 자료에 etc나 NaN으로 표시된 자료의 형태가 많아서 정확한 원인들을 찾기에 어려움이 있었습니다.\n자료 출처 및 참고 출처 - https://dacon.io/competitions/official/235590/overview/description (데이콘 - 코로나 데이터 시각화 AI 경진대회) - https://www.kaggle.com/datasets/kimjihoo/coronavirusdataset (kaggle - Data Science for COVID-19 in South Korea) - https://chancoding.tistory.com/119 (plotly line plot) - https://plotly.com/python/pie-charts/(about pie plot)"
  },
  {
    "objectID": "Data_Mining/2022-05-26-exercise-manipulating-geospatial-data.html",
    "href": "Data_Mining/2022-05-26-exercise-manipulating-geospatial-data.html",
    "title": "Manipulating Geospatial Data",
    "section": "",
    "text": "Manipulating Geospatial Data\n\nThis notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link.\n\n\nimport math\nimport pandas as pd\nimport geopandas as gpd\n#from geopy.geocoders import Nominatim            # What you'd normally run\nfrom learntools.geospatial.tools import Nominatim # Just for this exercise\n\nimport folium \nfrom folium import Marker\nfrom folium.plugins import MarkerCluster\n\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.geospatial.ex4 import *\n\nYou’ll use the embed_map() function from the previous exercise to visualize your maps.\n\ndef embed_map(m, file_name):\n    from IPython.display import IFrame\n    m.save(file_name)\n    return IFrame(file_name, width='100%', height='500px')\n\n\nExercises\n\n1) Geocode the missing locations.\nRun the next code cell to create a DataFrame starbucks containing Starbucks locations in the state of California.\n\n# Load and preview Starbucks locations in California\nstarbucks = pd.read_csv(\"../input/geospatial-learn-course-data/starbucks_locations.csv\")\nstarbucks.head()\n\nMost of the stores have known (latitude, longitude) locations. But, all of the locations in the city of Berkeley are missing.\n\n# How many rows in each column have missing values?\nprint(starbucks.isnull().sum())\n\n# View rows with missing locations\nrows_with_missing = starbucks[starbucks[\"City\"]==\"Berkeley\"]\nrows_with_missing\n\nUse the code cell below to fill in these values with the Nominatim geocoder.\nNote that in the tutorial, we used Nominatim() (from geopy.geocoders) to geocode values, and this is what you can use in your own projects outside of this course.\n튜토리얼에서 우리는 값을 지오코딩하기 위해 Nominatim()(geopy.geocoders에서)을 사용했으며 이것은 이 과정 이외의 자신의 프로젝트에서 사용할 수 있는 것입니다.\nIn this exercise, you will use a slightly different function Nominatim() (from learntools.geospatial.tools). This function was imported at the top of the notebook and works identically to the function from GeoPandas.\n이 연습에서는 약간 다른 함수 Nominatim()(learntools.geospatial.tools에서)을 사용합니다. 이 기능은 노트북 상단에서 가져온 것으로 GeoPandas의 기능과 동일하게 작동합니다.\nSo, in other words, as long as: - you don’t change the import statements at the top of the notebook, and - you call the geocoding function as geocode() in the code cell below,\nyour code will work as intended!\n\n# Create the geocoder\ngeolocator = Nominatim(user_agent=\"kaggle_learn\")\n\n# Your code here\n\ndef my_geocoder(row):\n    point = geolocator.geocode(row).point\n    return pd.Series({'Latitude': point.latitude, 'Longitude': point.longitude})\n\nberkeley_locations = rows_with_missing.apply(lambda x: my_geocoder(x['Address']), axis=1)\nstarbucks.update(berkeley_locations)\n\n# Check your answer\nq_1.check()\n\n\n# Line below will give you solution code\n#q_1.solution()\n\n\n\n2) View Berkeley locations.\nLet’s take a look at the locations you just found. Visualize the (latitude, longitude) locations in Berkeley in the OpenStreetMap style.\n방금 찾은 위치를 살펴보겠습니다. OpenStreetMap 스타일로 버클리의 (위도, 경도) 위치를 시각화합니다.\n\n# Create a base map\nm_2 = folium.Map(location=[37.88,-122.26], zoom_start=13)\n\n# Your code here: Add a marker for each Berkeley location\nfor idx, row in starbucks[starbucks[\"City\"]=='Berkeley'].iterrows():\n    Marker([row['Latitude'], row['Longitude']]).add_to(m_2)\n\n# Uncomment to see a hint\n#q_2.a.hint()\n\n# Show the map\nembed_map(m_2, 'q_2.html')\n\n\n# Get credit for your work after you have created a map\nq_2.a.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_2.a.solution()\n\nConsidering only the five locations in Berkeley, how many of the (latitude, longitude) locations seem potentially correct (are located in the correct city)?\n\n# View the solution (Run this code cell to receive credit!)\nq_2.b.solution()\n\n\n\n3) Consolidate your data.\nRun the code below to load a GeoDataFrame CA_counties containing the name, area (in square kilometers), and a unique id (in the “GEOID” column) for each county in the state of California. The “geometry” column contains a polygon with county boundaries.\n\nCA_counties = gpd.read_file(\"../input/geospatial-learn-course-data/CA_county_boundaries/CA_county_boundaries/CA_county_boundaries.shp\")\nCA_counties.head()\n\nNext, we create three DataFrames: - CA_pop contains an estimate of the population of each county. - CA_high_earners contains the number of households with an income of at least $150,000 per year. - CA_median_age contains the median age for each county.\n\nCA_pop = pd.read_csv(\"../input/geospatial-learn-course-data/CA_county_population.csv\", index_col=\"GEOID\")\nCA_high_earners = pd.read_csv(\"../input/geospatial-learn-course-data/CA_county_high_earners.csv\", index_col=\"GEOID\")\nCA_median_age = pd.read_csv(\"../input/geospatial-learn-course-data/CA_county_median_age.csv\", index_col=\"GEOID\")\n\nUse the next code cell to join the CA_counties GeoDataFrame with CA_pop, CA_high_earners, and CA_median_age.\nName the resultant GeoDataFrame CA_stats, and make sure it has 8 columns: “GEOID”, “name”, “area_sqkm”, “geometry”, “population”, “high_earners”, and “median_age”. Also, make sure the CRS is set to {'init': 'epsg:4326'}.\n결과 GeoDataFrame의 이름을 CA_stats로 지정하고 “GEOID”, “name”, “area_sqkm”, “geometry”, “population”, “high_earners” 및 “median_age”의 8개 열이 있는지 확인합니다. 또한 CRS가 {'init': 'epsg:4326'}으로 설정되어 있는지 확인하십시오.\n\n# Your code here\ncols_to_add = CA_pop.join([CA_high_earners, CA_median_age]).reset_index()\nCA_stats = CA_counties.merge(cols_to_add, on=\"GEOID\")\n\n# Check your answer\nq_3.check()\n\n\n# Lines below will give you a hint or solution code\n#q_3.hint()\n#q_3.solution()\n\nNow that we have all of the data in one place, it’s much easier to calculate statistics that use a combination of columns. Run the next code cell to create a “density” column with the population density.\n\nCA_stats[\"density\"] = CA_stats[\"population\"] / CA_stats[\"area_sqkm\"]\n\n\n\n4) Which counties look promising?\nCollapsing all of the information into a single GeoDataFrame also makes it much easier to select counties that meet specific criteria.\nUse the next code cell to create a GeoDataFrame sel_counties that contains a subset of the rows (and all of the columns) from the CA_stats GeoDataFrame. In particular, you should select counties where:\n다음 코드 셀을 사용하여 CA_stats GeoDataFrame에서 행(및 모든 열)의 하위 집합을 포함하는 GeoDataFrame sel_counties를 만듭니다. 특히 다음과 같은 카운티를 선택해야 합니다.\n\nthere are at least 100,000 households making $150,000 per year,\nthe median age is less than 38.5, and\nthe density of inhabitants is at least 285 (per square kilometer).\n\nAdditionally, selected counties should satisfy at least one of the following criteria: - there are at least 500,000 households making $150,000 per year, - the median age is less than 35.5, or - the density of inhabitants is at least 1400 (per square kilometer).\n\n# Your code here\nsel_counties = CA_stats[((CA_stats.high_earners &gt; 100000) &\n                         (CA_stats.median_age &lt; 38.5) &\n                         (CA_stats.density &gt; 285) &\n                         ((CA_stats.median_age &lt; 35.5) |\n                         (CA_stats.density &gt; 1400) |\n                         (CA_stats.high_earners &gt; 500000)))]\n\n# Check your answer\nq_4.check()\n\n\n# Lines below will give you a hint or solution code\n#q_4.hint()\n#q_4.solution()\n\n\n\n5) How many stores did you identify?\nWhen looking for the next Starbucks Reserve Roastery location, you’d like to consider all of the stores within the counties that you selected. So, how many stores are within the selected counties?\nTo prepare to answer this question, run the next code cell to create a GeoDataFrame starbucks_gdf with all of the starbucks locations.\n\nstarbucks_gdf = gpd.GeoDataFrame(starbucks, geometry=gpd.points_from_xy(starbucks.Longitude, starbucks.Latitude))\nstarbucks_gdf.crs = {'init': 'epsg:4326'}\n\nSo, how many stores are in the counties you selected?\n그렇다면 선택한 카운티에는 몇 개의 매장이 있습니까?\n\n# Fill in your answer\nlocations_of_interest = gpd.sjoin(starbucks_gdf, sel_counties)\nnum_stores = len(locations_of_interest)\n\n# Check your answer\nq_5.check()\n\n\n# Lines below will give you a hint or solution code\n#q_5.hint()\n#q_5.solution()\n\n\n\n6) Visualize the store locations.\nCreate a map that shows the locations of the stores that you identified in the previous question.\n이전 질문에서 식별한 상점의 위치를 ​​보여주는 지도를 만드십시오.\n\n# Create a base map\nm_6 = folium.Map(location=[37,-120], zoom_start=6)\n\n# Your code here: show selected store locations\nmc = MarkerCluster()\n\nlocations_of_interest = gpd.sjoin(starbucks_gdf, sel_counties)\nfor idx, row in locations_of_interest.iterrows():\n    if not math.isnan(row['Longitude']) and not math.isnan(row['Latitude']):\n        mc.add_child(folium.Marker([row['Latitude'], row['Longitude']]))\n\nm_6.add_child(mc)\n\n# Uncomment to see a hint\n#q_6.hint()\n\n# Show the map\nembed_map(m_6, 'q_6.html')\n\n\n# Get credit for your work after you have created a map\n#q_6.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_6.solution()\n\n\n\n\nKeep going\nLearn about how proximity analysis can help you to understand the relationships between points on a map.\n\nHave questions or comments? Visit the course discussion forum to chat with other learners."
  },
  {
    "objectID": "Data_Mining/2022-05-24-exercise-your-first-map.html",
    "href": "Data_Mining/2022-05-24-exercise-your-first-map.html",
    "title": "Your First Map",
    "section": "",
    "text": "Your First Map\n\nThis notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link.\n\n\nimport geopandas as gpd\n\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.geospatial.ex1 import *\n\n\n1) Get the data.\nUse the next cell to load the shapefile located at loans_filepath to create a GeoDataFrame world_loans.\n다음 셀을 사용하여 loans_filepath에 있는 shapefile을 로드하여 GeoDataFrame world_loans를 생성합니다.\n\nloans_filepath = \"../input/geospatial-learn-course-data/kiva_loans/kiva_loans/kiva_loans.shp\"\n\n# Your code here: Load the data\nworld_loans = gpd.read_file(loans_filepath)\n\n# Check your answer\nq_1.check()\n\n# Uncomment to view the first five rows of the data\n#world_loans.head()\n\n\n\n2) Plot the data.\nRun the next code cell without changes to load a GeoDataFrame world containing country boundaries.\n변경 없이 다음 코드 셀을 실행하여 국가 경계가 포함된 GeoDataFrame ’world’를 로드합니다.\n\n# Lines below will give you a hint or solution code\n#q_1.hint()\n#q_1.solution()\n\n\n# This dataset is provided in GeoPandas\nworld_filepath = gpd.datasets.get_path('naturalearth_lowres')\nworld = gpd.read_file(world_filepath)\nworld.head()\n\nUse the world and world_loans GeoDataFrames to visualize Kiva loan locations across the world.\nworld 및 world_loans GeoDataFrames를 사용하여 전 세계의 Kiva loan locations를 시각화합니다.\n\n# Your code here\nax = world.plot(figsize=(20,20), color='none', edgecolor='gainsboro',zorder=3)\nworld_loans.plot(color='skyblue', markersize=2,ax=ax)\n# Uncomment to see a hint\n#q_2.hint()\n\n\n# Get credit for your work after you have created a map\nq_2.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_2.solution()\n\n\n\n3) Select loans based in the Philippines.\nNext, you’ll focus on loans that are based in the Philippines. Use the next code cell to create a GeoDataFrame PHL_loans which contains all rows from world_loans with loans that are based in the Philippines.\n다음으로 필리핀에 기반을 둔 대출에 중점을 둘 것입니다. 다음 코드 셀을 사용하여 필리핀에 기반을 둔 대출이 있는 world_loans의 모든 행을 포함하는 GeoDataFrame PHL_loans를 만듭니다.\n\n# Your code here\nPHL_loans = world_loans.loc[world_loans.country==\"Philippines\"].copy()\n\n\n# Check your answer\nq_3.check()\n\n\n# Lines below will give you a hint or solution code\n#q_3.hint()\n#q_3.solution()\n\n\n\n4) Understand loans in the Philippines.\nRun the next code cell without changes to load a GeoDataFrame PHL containing boundaries for all islands in the Philippines.\n필리핀의 모든 섬에 대한 경계를 포함하는 GeoDataFrame PHL을 로드하려면 변경 없이 다음 코드 셀을 실행하십시오.\n\n# Load a KML file containing island boundaries\ngpd.io.file.fiona.drvsupport.supported_drivers['KML'] = 'rw'\nPHL = gpd.read_file(\"../input/geospatial-learn-course-data/Philippines_AL258.kml\", driver='KML')\nPHL.head()\n\nUse the PHL and PHL_loans GeoDataFrames to visualize loans in the Philippines.\n‘PHL’ 및 ‘PHL_loans’ GeoDataFrames를 사용하여 필리핀의 대출을 시각화합니다.\n\n# Your code here\nax = PHL.plot(figsize=(20,20), color='none', edgecolor='gainsboro', zorder=3)\nPHL_loans.plot(color='skyblue', markersize=2, ax=ax)\n\n# Uncomment to see a hint\n#q_4.a.hint()\n\n\n# Get credit for your work after you have created a map\nq_4.a.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_4.a.solution()\n\nCan you identify any islands where it might be useful to recruit new Field Partners? Do any islands currently look outside of Kiva’s reach?\nYou might find this map useful to answer the question.\n\n# View the solution (Run this code cell to receive credit!)\nq_4.b.solution()\n\n\n\nKeep going\nContinue to learn about coordinate reference systems.\n\nHave questions or comments? Visit the course discussion forum to chat with other learners."
  },
  {
    "objectID": "Data_Mining/2022-05-24-exercise-coordinate-reference-systems.html",
    "href": "Data_Mining/2022-05-24-exercise-coordinate-reference-systems.html",
    "title": "Coordinate Refrence Systems",
    "section": "",
    "text": "Coordinate Reference Systems\n\nThis notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link.\n\n\nimport pandas as pd\nimport geopandas as gpd\n\nfrom shapely.geometry import LineString\n\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.geospatial.ex2 import *\n\n\nExercises\n\n1) Load the data.\nRun the next code cell (without changes) to load the GPS data into a pandas DataFrame birds_df.\n다음 코드 셀(변경 없이)을 실행하여 GPS 데이터를 pandas DataFrame birds_df에 로드합니다.\n\n# Load the data and print the first 5 rows\nbirds_df = pd.read_csv(\"../input/geospatial-learn-course-data/purple_martin.csv\", parse_dates=['timestamp'])\nprint(\"There are {} different birds in the dataset.\".format(birds_df[\"tag-local-identifier\"].nunique()))\nbirds_df.head()\n\nThere are 11 birds in the dataset, where each bird is identified by a unique value in the “tag-local-identifier” column. Each bird has several measurements, collected at different times of the year.\nUse the next code cell to create a GeoDataFrame birds.\n- birds should have all of the columns from birds_df, along with a “geometry” column that contains Point objects with (longitude, latitude) locations.\n-birds에는 birds_df의 모든 열과 함께 (경도, 위도) 위치가 있는 Point 개체가 포함된 “geometry” 열이 있어야 합니다. - Set the CRS of birds to {'init': 'epsg:4326'}.\n\n# Your code here: Create the GeoDataFrame\nbirds = gpd.GeoDataFrame(birds_df, geometry=gpd.points_from_xy(birds_df[\"location-long\"], birds_df[\"location-lat\"]))\n\n# Your code here: Set the CRS to {'init': 'epsg:4326'}\nbirds.crs = {'init' :'epsg:4326'}\n\n# Check your answer\nq_1.check()\n\n\n# Lines below will give you a hint or solution code\n#q_1.hint()\n#q_1.solution()\n\n\n\n2) Plot the data.\nNext, we load in the 'naturalearth_lowres' dataset from GeoPandas, and set americas to a GeoDataFrame containing the boundaries of all countries in the Americas (both North and South America). Run the next code cell without changes.\n다음으로 GeoPandas에서 naturalearth_lowres 데이터 세트를 로드하고 americas를 미주(북미와 남미 모두)의 모든 국가 경계를 포함하는 GeoDataFrame으로 설정합니다. 변경 없이 다음 코드 셀을 실행합니다.\n\n# Load a GeoDataFrame with country boundaries in North/South America, print the first 5 rows\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\namericas = world.loc[world['continent'].isin(['North America', 'South America'])]\namericas.head()\n\nUse the next code cell to create a single plot that shows both: (1) the country boundaries in the americas GeoDataFrame, and (2) all of the points in the birds_gdf GeoDataFrame.\n다음 코드 셀을 사용하여 (1) americas GeoDataFrame의 국가 경계와 (2) birds_gdf GeoDataFrame의 모든 점을 모두 표시하는 단일 플롯을 만듭니다.\nDon’t worry about any special styling here; just create a preliminary plot, as a quick sanity check that all of the data was loaded properly. In particular, you don’t have to worry about color-coding the points to differentiate between birds, and you don’t have to differentiate starting points from ending points. We’ll do that in the next part of the exercise.\n\n# Your code here\nax = americas.plot(figsize=(8,8), color='red', linestyle=':', edgecolor='black')\namericas.plot(markersize=1, ax=ax)\n# Uncomment to see a hint\n#q_2.hint()\n\n\n# Get credit for your work after you have created a map\nq_2.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_2.solution()\n\n\n\n3) Where does each bird start and end its journey? (Part 1)\nNow, we’re ready to look more closely at each bird’s path. Run the next code cell to create two GeoDataFrames: - path_gdf contains LineString objects that show the path of each bird. It uses the LineString() method to create a LineString object from a list of Point objects. - path_gdf에는 각 새의 경로를 표시하는 LineString 개체가 포함되어 있습니다. LineString() 메서드를 사용하여 Point 개체 목록에서 LineString 개체를 만듭니다. - start_gdf contains the starting points for each bird. - start_gdf는 각 새의 시작 지점을 포함합니다.\n\n# GeoDataFrame showing path for each bird\npath_df = birds.groupby(\"tag-local-identifier\")['geometry'].apply(list).apply(lambda x: LineString(x)).reset_index()\npath_gdf = gpd.GeoDataFrame(path_df, geometry=path_df.geometry)\npath_gdf.crs = {'init' :'epsg:4326'}\n\n# GeoDataFrame showing starting point for each bird\nstart_df = birds.groupby(\"tag-local-identifier\")['geometry'].apply(list).apply(lambda x: x[0]).reset_index()\nstart_gdf = gpd.GeoDataFrame(start_df, geometry=start_df.geometry)\nstart_gdf.crs = {'init' :'epsg:4326'}\n\n# Show first five rows of GeoDataFrame\nstart_gdf.head()\n\nUse the next code cell to create a GeoDataFrame end_gdf containing the final location of each bird.\n- The format should be identical to that of start_gdf, with two columns (“tag-local-identifier” and “geometry”), where the “geometry” column contains Point objects. - 형식은 두 개의 열(“tag-local-identifier” 및 “geometry”)이 있는 start_gdf'의 형식과 동일해야 합니다. 여기서 \"geometry\" 열은 Point 개체를 포함합니다. - Set the CRS ofend_gdfto{‘init’: ‘epsg:4326’}. -end_gdf의 CRS를{‘init’: ‘epsg:4326’}`으로 설정합니다.\n\n# Your code here\nend_df = birds.groupby(\"tag-local-identifier\")['geometry'].apply(list).apply(lambda x: x[-1]).reset_index()\nend_gdf = gpd.GeoDataFrame(end_df, geometry=end_df.geometry)\nend_gdf.crs = {'init' :'epsg:4326'}\n\n# Check your answer\nq_3.check()\n\n\n# Lines below will give you a hint or solution code\n#q_3.hint()\n#q_3.solution()\n\n\n\n4) Where does each bird start and end its journey? (Part 2)\nUse the GeoDataFrames from the question above (path_gdf, start_gdf, and end_gdf) to visualize the paths of all birds on a single map. You may also want to use the americas GeoDataFrame.\n위 질문의 GeoDataFrames(path_gdf, start_gdf, end_gdf)를 사용하여 단일 지도에서 모든 새의 경로를 시각화하세요. americas GeoDataFrame을 사용할 수도 있습니다.\n\n# Your code here\nax = americas.plot(figsize=(10,10), color='none', edgecolor='gainsboro', zorder=3)\n\n# Add wild lands, campsites, and foot trails to the base map\nstart_gdf.plot(color='lightgreen', ax=ax)\npath_gdf.plot(color='maroon', markersize=2, ax=ax)\nend_gdf.plot(color='black', markersize=1, ax=ax) \n\n# Uncomment to see a hint\n#q_4.hint()\n\n\n# Get credit for your work after you have created a map\nq_4.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_4.solution()\n\n\n\n5) Where are the protected areas in South America? (Part 1)\nIt looks like all of the birds end up somewhere in South America. But are they going to protected areas?\nIn the next code cell, you’ll create a GeoDataFrame protected_areas containing the locations of all of the protected areas in South America. The corresponding shapefile is located at filepath protected_filepath.\n다음 코드 셀에서는 남미의 모든 보호 지역 위치를 포함하는 GeoDataFrame protected_areas를 생성합니다. 해당 shapefile은 파일 경로 protected_filepath에 있습니다.\n\n# Path of the shapefile to load\nprotected_filepath = \"../input/geospatial-learn-course-data/SAPA_Aug2019-shapefile/SAPA_Aug2019-shapefile/SAPA_Aug2019-shapefile-polygons.shp\"\n\n# Your code here\nprotected_areas = gpd.read_file(protected_filepath)\n\n# Check your answer\nq_5.check()\n\n\n# Lines below will give you a hint or solution code\n#q_5.hint()\n#q_5.solution()\n\n\n\n6) Where are the protected areas in South America? (Part 2)\nCreate a plot that uses the protected_areas GeoDataFrame to show the locations of the protected areas in South America. (You’ll notice that some protected areas are on land, while others are in marine waters.)\n‘protected_areas’ GeoDataFrame을 사용하여 남아메리카의 보호 지역 위치를 표시하는 플롯을 만듭니다. (일부 보호 구역은 육지에 있고 다른 보호 구역은 바다에 있음을 알 수 있습니다.)\n\n# Country boundaries in South America\nsouth_america = americas.loc[americas['continent']=='South America']\n\n# Your code here: plot protected areas in South America\nax = south_america.plot(figsize=(8,8), color='whitesmoke', edgecolor='black')\nprotected_areas.plot(markersize=1, ax=ax,alpha=0.4)\n\n# Uncomment to see a hint\n#q_6.hint()\n\n\n# Get credit for your work after you have created a map\nq_6.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_6.solution()\n\n\n\n7) What percentage of South America is protected?\nYou’re interested in determining what percentage of South America is protected, so that you know how much of South America is suitable for the birds.\nAs a first step, you calculate the total area of all protected lands in South America (not including marine area). To do this, you use the “REP_AREA” and “REP_M_AREA” columns, which contain the total area and total marine area, respectively, in square kilometers.\nRun the code cell below without changes.\n\nP_Area = sum(protected_areas['REP_AREA']-protected_areas['REP_M_AREA'])\nprint(\"South America has {} square kilometers of protected areas.\".format(P_Area))\n\nThen, to finish the calculation, you’ll use the south_america GeoDataFrame.\n\nsouth_america.head()\n\nCalculate the total area of South America by following these steps: - Calculate the area of each country using the area attribute of each polygon (with EPSG 3035 as the CRS), and add up the results. The calculated area will be in units of square meters. - 각 폴리곤(CRS로 EPSG 3035 사용)의 ‘area’ 속성을 사용하여 각 국가의 면적을 계산하고 결과를 합산합니다. 계산된 면적은 평방 미터 단위입니다. - Convert your answer to have units of square kilometeters. - 평방 킬로미터 단위가 되도록 답을 변환하십시오.\n\n# Your code here: Calculate the total area of South America (in square kilometers)\ntotalArea = sum(south_america.geometry.to_crs(epsg=3035).area)/10**6\ntotalArea \n# Check your answer\nq_7.check()\n\n\n# Lines below will give you a hint or solution code\n#q_7.hint()\n#q_7.solution()\n\nRun the code cell below to calculate the percentage of South America that is protected.\n\n# What percentage of South America is protected?\npercentage_protected = P_Area/totalArea\nprint('Approximately {}% of South America is protected.'.format(round(percentage_protected*100, 2)))\n\n\n\n8) Where are the birds in South America?\nSo, are the birds in protected areas?\n그렇다면 새들은 보호 구역에 있습니까?\nCreate a plot that shows for all birds, all of the locations where they were discovered in South America. Also plot the locations of all protected areas in South America.\n모든 새, 남미에서 발견된 모든 위치를 보여주는 플롯을 만듭니다. 또한 남아메리카의 모든 보호 지역의 위치를 ​​표시합니다.\nTo exclude protected areas that are purely marine areas (with no land component), you can use the “MARINE” column (and plot only the rows in protected_areas[protected_areas['MARINE']!='2'], instead of every row in the protected_areas GeoDataFrame).\n순수한 해양 지역(토지 구성요소 없음)인 보호 지역을 제외하려면 “MARINE” 열을 사용할 수 있습니다(그리고 protected_areas[protected_areas['MARINE']!='2'] protected_areas GeoDataFrame의 모든 행).\n\n# Your code here\nax = south_america.plot(figsize=(8,8), color='whitesmoke', edgecolor='black')\nprotected_areas[protected_areas['MARINE']!='2'].plot(ax=ax, alpha=0.4, zorder=1)\nbirds[birds.geometry.y &lt; 0].plot(ax=ax, color='red', alpha=0.6, markersize=10, zorder=2)\n\n# Uncomment to see a hint\n#q_8.hint()\n\n\n# Get credit for your work after you have created a map\nq_8.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_8.solution()\n\n\n\n\nKeep going\nCreate stunning interactive maps with your geospatial data.\n\nHave questions or comments? Visit the course discussion forum to chat with other learners."
  },
  {
    "objectID": "Data_Mining/2022-03-18-Python Language Basics.html",
    "href": "Data_Mining/2022-03-18-Python Language Basics.html",
    "title": "Python 기초",
    "section": "",
    "text": "basic 기본 코드 실습\n\n\nPython language bascis\n\nlanguage semantics\n\nIdentation, not braces // 들여쓰기\nfor x in array: if x &lt; pivot : less.append(x) else: greater.append(x)\na=5; b=6; c=7\n\na=5; b=6; c=7\n\n\nc\n\n7\n\n\n\n\nEverything is an object // 오브젝트를 기본 할당하고 동작함\n\n\nComments\nresult =[] for line in file_handle: # keep the empty line for now # if len(line) == 0: # continue result.append(line.replace(‘foo’,‘bar’)) print(“Reach this line”) #simple statis report\n\n\nFunction and object method calls\nresult = f(x,y,z) g()\nobj.some_method(x,y,z) result = f(a,b,c,d=5,e=‘foo’)\n\n\nVariables and argument passing\n\na=[1,2,3]\n\n\nb=a\n\n\na.append(4) #뒤에 붙여줌 \nb\n\n[1, 2, 3, 4]\n\n\n\na=5\ntype(a) #int - integer - 수치형 \n\nint\n\n\n\na='foo'\ntype(a)\n\nstr\n\n\n\n'5'+5\n\nTypeError: can only concatenate str (not \"int\") to str\n\n\n\na=4.5\nb=2\n# String formatting, to be visted later\nprint('a is {0}, b is {1}'.format(type(a),type(b)))\na/b\n\na is &lt;class 'float'&gt;, b is &lt;class 'int'&gt;\n\n\n2.25\n\n\n\na=5\nisinstance(a,int)\n\nTrue\n\n\n\na=5; b=4.5\nisinstance(a,(int,float))\nisinstance(b,(int,float))\n\nTrue\n\n\n\na='foo' #Tab 키를 누르면 다양한 옵션이 나오면서 대문자화 카운트등 여러옵션 존재\n\n\na.&lt;Press Tab&gt;\n\nSyntaxError: invalid syntax (3084391244.py, line 1)\n\n\n\na.upper()\n\n'FOO'\n\n\n\ngetattr(a,'split') #object에 속한 속성을 가지고 온다 \n\n&lt;function str.split(sep=None, maxsplit=-1)&gt;\n\n\n\n\n\nDuck typing\n\ndef isiterable(obj):\n    try:\n        iter(obj)\n        return True\n    except TypeError: # not literable\n        return False\n\n\nisiterable('a string') \n\nTrue\n\n\n\nisiterable([1,2,3])\n\nTrue\n\n\n\nisiterable(5)\n\nFalse\n\n\nif not isinstance(x,list) and isiterable(x): x=list(x)\n\n\nImports\nIn Python a module is simply a file with the .py extension containing Python code. Suppose that we had the following module:\nIf you wanted to access the variables and functions defined in some_module.py, from another file in the same directory we could do:\n\nimport some_module #같은 디렉토리의 .py파일을 불러와서 사용가능 \n\nresult = some_module.f(5)\nresult\n\n7\n\n\n\npi = some_module.PI\npi\n\n3.14159\n\n\nOr equivalently:\n\nfrom some_module import f,g,PI\nresult = g(5,PI)\nresult\n\n8.14159\n\n\nBy using the as keyword you can give imports different variable names:\n\nimport some_module as sm\nfrom some_module import PI as pi, g as gf\n\nr1 = sm.f(pi)\nr2 = gf(6,pi)\n\n\nr1\n\n5.14159\n\n\n\nr2\n\n9.14159\n\n\n\n\nBinary operators and comparisons\nMost of the binary math operations and comparisons are as you might expect:\n\n5-7\n12+21.5\n5 &lt;= 2\n\nFalse\n\n\n\na = [1,2,3]\nb=a\nc=list()\na is b\na is not c \n\nTrue\n\n\n\na == c\n\nFalse\n\n\n\na = None\na is None\n\nTrue\n\n\n\nMutable and immutable objects\nMost object in Python such as list, dict, NumPy arrays, and most user-defined types(classes), are mutable.\nThis means that the object or values that they contain can be modified\n\na_list = ['foo',2,[4,5]]\na_list[2] = (3,4)\na_list\n\n['foo', 2, (3, 4)]\n\n\nOthers, like strings and tuples, are immutable:\n\na_tuple = (3,5,(4,5))\na_tuple[1] = 'four' #tuple은 변경이 불가능 하다 / list는 가능  \n\nTypeError: 'tuple' object does not support item assignment\n\n\n\n\n\nScalar Types\n\nNumeric types\nThe primary Python types for numbers are int and float. An int can store arbitrarily large numbers:\n\nival = 17239871\nival ** 6\n\n26254519291092456596965462913230729701102721\n\n\nFloating-point numbers are represented with the Python float type. Under the hood each one is a double-precision(64-bit) value. They can also be expressed with scientific notation:\n\nfval = 7.2343\nfval2 = 6.78e-5\n\n\n3/2\n\n1.5\n\n\n\ntype(3/2)\n\nfloat\n\n\n\n3//2 #몫\n\n1\n\n\n\ntype(3//2)\n\nint\n\n\n\n\n\nStrings\nMany people use Python for its powerful and flexible built-in string processing capabilities.\nYou can write string literals using either single quotes or double quotes:\n\na = 'one way of writing a string'\nb = 'another way'\n\nFor multiline strings with line breaks, you can use triple qutoes, either ’’’ or ““”\n\nc = \"\"\" \nThis is a longer string that\nspans multiple lines\n\"\"\"\n\n\nc\n\n' \\nThis is a longer string that\\nspans multiple lines\\n'\n\n\nIt may surprise you that this string c actually contains for lines of text;\nthe line breaks after ““” and after lines are include in the string.\nWe can count the new line characters with the count method on c:\n\nc.count('\\n')\n\n3\n\n\nPython strings are immutable; you cannot modify a string:\n\na = 'this is a string'\na[10] = 'f' #변형불가 \n\nTypeError: 'str' object does not support item assignment\n\n\n\nb = a.replace('string','longer string')\nb\n\n'this is a longer string'\n\n\n\na #변형은 안되나 대체는 가능\n\n'this is a string'\n\n\nMany Python object can be converted to a string using the str function\n\na = 5.6\ns = str(a)\nprint(s)\n\n5.6\n\n\nStrings are a sequnce of Unicode characters and therefore can be treated like other sequences, such as lists and tuples(which we will explore in more detail in the next chapter):\n\ns='python'\nlist(s)\n\n['p', 'y', 't', 'h', 'o', 'n']\n\n\n\ns[:3]\n\n'pyt'\n\n\nThe syntax s[:3] is called slicing and is implemented for many kinds of Python sequences. This will be explained in moire detail later on, as it it used extensively in the book.\nThe backslash character  is an escape character, meaning that it is used to specify special characters like newline or Unicode characters. To write a string literal with backslashes, you need to escape them:\n\nprint('12\\n34') #\\n is Enter \n\n12\n34\n\n\n\ns = '12\\\\34' #백슬래쉬를 문자열로 바꾼다 \nprint(s)\n\n12\\34\n\n\nAdding two strings together concatenates them and produces a new string:\n\na='this is the first half '\nb='and this is the second half'\na+b\n\n'this is the first half and this is the second half'\n\n\nString templating or formatting is another important topic.\nThe number of ways tod do so has expanded with the advent of Python 3,\nand here I will briefly describe the mechanics of one of the main interfaces.\nString objects have a format method that can be used to substitute formatted arguments into the string, producting a new string:\n\ntemplate = '{0:.2f} {1:s} are worth US${2:d}'\ntemplate\n\n'{0:.2f} {1:s} are worth US${2:d}'\n\n\n\n{0:.2f} means to format the first argument as a floating-point number with two decimal places.\n{1:s} means to format the second argument as a string.\n{2:d} means to format the third argument as an exact integer\n\n\ntemplate.format(4.560, 'Argentine Pesos',1)\n\n'4.56 Argentine Pesos are worth US$1'\n\n\n\ntemplate.format(1263.23,'won',1)\n\n'1263.23 won are worth US$1'\n\n\n\n\nBooleans\n\nThe two boolean value in python are written as True as False.\n\n\nComprasions and other conditional expressions evaluate to either True or False.\n\n\nBoolean values are combined with the and or keywords:\n\nTrue and True\n\nTrue\n\n\n\nFalse or True\n\nTrue\n\n\n\n\nType casting\nThe str, bool, int, and float types are also functions that can be used to cast values\n\ns='3.14159'\nfval=float(s)\ntype(fval)\n\nfloat\n\n\n\nint(fval)\n\n3\n\n\n\nbool(fval)\n\nTrue\n\n\n\nbool(0)\n\nFalse\n\n\n\n\n\nNone\nNone is the Python null value type. If a function does not explicitly return a value, it implicitly returns None:\n\na = None\na is None\n\nTrue\n\n\n\nb = 5\nb is not None\n\nTrue\n\n\nNone is also a common default vlaue for function arguments:\n\ndef add_and_maybe_multiply(a,b,c=None):\n    result = a+b\n\n    if c is not None:\n        result = result * c\n    \n    return result\n\n\nadd_and_maybe_multiply(5,3)\n\n8\n\n\n\nadd_and_maybe_multiply(5,3,10)\n\n80\n\n\nWhile a technical point, it’s worth bearing in mind that None is not only a reserved keyword but also a unique instance of NoneType:\n\ntype(None)\n\nNoneType\n\n\n\n\nDates and times\nThe built-in Python datetime module provides datetime, date, and time types.\nThe datetime type, as you may imagine, combines the information stored in date and time and is the most commonly used:\n\nfrom datetime import datetime, date, time\ndt = datetime(2011,10,29,20,30,21) #year,month,day,hour,minute,second\ndt\n\ndatetime.datetime(2011, 10, 29, 20, 30, 21)\n\n\n\ndt.day\n\n29\n\n\n\ndt.minute\n\n30\n\n\nGiven a datetime instance, you can extract the equivalent date and time objects by calling methods on the datetime of the same name:\n\ndt.date()\n\ndatetime.date(2011, 10, 29)\n\n\n\ndt.time()\n\ndatetime.time(20, 30, 21)\n\n\nThe strfime method formats a datetime as a string:\n\ndt.strftime('%m/%d/%Y %H:%M')\n\n'10/29/2011 20:30'\n\n\n\ndt.strftime('%Y/%m/%d %H:%M')\n\n'2011/10/29 20:30'\n\n\nString can be converted (parsed) into datetime objects with the strptime function:\n\ndatetime.strptime('20091031',\"%Y%m%d\")\n\ndatetime.datetime(2009, 10, 31, 0, 0)\n\n\nWhen you are aggregating or otherwise grouping time series data, it will occasionally be useful to replace time fields of a series of datetimes-for example,replacing the minute and second fields with zero:\n\ndt.replace(minute=0,second=0)\n\ndatetime.datetime(2011, 10, 29, 20, 0)\n\n\n\ndt2 = datetime(2011,11,15,22,30) \ndelta =dt2 - dt #dt = datetime(2011,10,29,20,30,21)\ndelta\n\ndatetime.timedelta(days=17, seconds=7179)\n\n\n\ntype(delta)\n\ndatetime.timedelta\n\n\n\ndt\ndt + delta\n\ndatetime.datetime(2011, 11, 15, 22, 30)\n\n\n\n\n\nControl Flow\nPython has several built-in keywords for conditonal logic, loops, and other standard control flow concepts found in other porgramming languages.\n\nif, elif, and else\nThe if statement is one of the most well-known control flow statement types.\nIt checks a conditon that, if True, evaluates the code in the block that follows:\n\nx = -5\nif x &lt; 0:\n    print('It is negative')\n\nIt is negative\n\n\nAn if statement can be optionally followed by one or mor elif blocks and a catch all else block if all of the conditions are False\n\nx = -5\n\nif x &lt; 0 :\n    print('It is negative')\nelif x == 0 :\n    print('Equal to zero')\nelif 0 &lt; x &lt; 5 :\n    print('Postive but smaller than 5')\nelse :\n    print('Postive and larger than or equal to 5')\n\nIt is negative\n\n\nIf any of the conditions is True, no futher elif or else blocks will be reached.\nWith a compound condition using and or or, conditions are evaluated left to right and will short-circuit:\n\na = 5; b = 7\nc = 8; d = 4\nif a &lt; b or c &gt; d :\n    print('Made it')\n\nMade it\n\n\nIn this examplme, the comparison c &gt; d never get evaluated because the first compar- ison was True. It is also possible to chain comparisons:\n\n4 &gt; 3 &gt; 2 &gt; 1\n\nTrue\n\n\n\n3 &gt; 5 or 2 &gt; 1\n\nTrue\n\n\n\n3&gt;5&gt;2&gt;1\n\nFalse\n\n\n\n\nfor loops\nfor loops are fir iterating over a collection (like a list or tuple) or an iterater. They standard syntax for a for loop is:\nfor value in collection: # do something with value\nYou can advance a for loop to the next iteration, skipping the remainder of the block, using the continue keyword.\nConsider this code, which sums up integers in a list and skips None values\n\nsequnce = [1,2,None,4,None,5]\ntotal = 0\n\nfor value in sequnce:\n    total += value\n\nTypeError: unsupported operand type(s) for +=: 'int' and 'NoneType'\n\n\n\nsequnce = [1,2,None,4,None,5]\ntotal = 0\n\nfor value in sequnce:\n    if value is None:\n        continue\n    total += value\n\n\ntotal\n\n12\n\n\nA for loop cna be exited altogether with the break keyword. This code sums ele- ments of the list until a 5 is reached:\n\nsequnce = [1,2,0,4,6,5,2,1]\ntotal_until_5 = 0\n\nfor value in sequnce:\n    if value == 5:\n        break\n    total_until_5 += value\n\n\ntotal_until_5 #값이 5가 되면 멈추는 조건으로 1+2+0+4+6까지 계산후 5가 오기에 for문이 종료된다 \n\n13\n\n\nThe break keyword only terminates the innermost for loop; any outer for loops will continue to run:\n\nfor i in range(4):\n    for j in range(4):\n        if j&gt;i:\n            break\n        print((i,j))\n\n(0, 0)\n(1, 0)\n(1, 1)\n(2, 0)\n(2, 1)\n(2, 2)\n(3, 0)\n(3, 1)\n(3, 2)\n(3, 3)\n\n\nAs we will see in more detail, if the elements in the collection or iterator are sequences (tuples or list, say), they can be conveniently unpacked into variables in the for loop statement:\nfor a,b,c in iterator: # do something\n\nfor a,b,c in [[1,2,3],[4,5,6],[7,8,9]]:\n    print(a,b,c)\n\n1 2 3\n4 5 6\n7 8 9\n\n\n\nWhile loops\nA while looops specifies a conditon and a block o f code that is to be excused until the condition evaluates to False or the loops is explicitly ended with break:\n\nx = 256\ntotal = 0\nwhile x &gt; 0:\n    if total &gt; 500 :\n        break\n    total += x\n    x = x //2\n\n\ntotal #504\n\n504\n\n\n\nx\n\n4\n\n\n\n256+128+64+32+16+8\n\n504\n\n\n\n\npass\npass is the “no-op”(No Operation) statement in Python. It can be used in block where no action is to be taken (or as a placeholder for code not yer implemented); it is only required becauese Python uese whitespace to delimit blocks:\n\nx = -1\n\nif x &lt; 0:\n    print('negative')\nelif x == 0:\n    # TODO: put something smart here\n    pass\nelse:\n    print('Postive!')\n\nnegative\n\n\n\n\nrange\nThe range function returns an iterator that yields a sequnce of evenly spaced intergers:\n\nrange(10)\n\nrange(0, 10)\n\n\n\nlist(range(10))\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\nBoth a start,end,and step(Which may be negative) can be given:\n\nlist(range(0,20,2)) #등차수열\n\n[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n\n\n\nlist(range(5,0,-1)) #리버스 인덱스\n\n[5, 4, 3, 2, 1]\n\n\nAs you can see, range prodices integers up to but not including the endpoint.\nA common use of range is for iterating through sequcnes by index:\n\nseq = [1,2,3,4]\nfor i in range(len(seq)):\n    val = seq[i]\n\n\nval\n\n4\n\n\nWhile you can use fuctions loke list to store all the integers generated by range in some other data structure, often the default iterator form will be what you want. This snippet sums all numbers form 0 to 99,999 that are multiples of 3 or 5:\n\nsum = 0\nfor i in range(10000) :\n    # % is the modulo operator\n    if i % 3 == 0 or i % 5 == 0:\n        sum += 1\n\n\n\n\nTernary expressions\nvalue = true - expr if conditon else false - expr\nHere, true-expr and false-expr cna be any Python expressions. It has the identical effect as the more verbose:\nif conditon: value = true-expr else: value = false-expr\n\nx = 5\n'Non-negative' if x &gt;= 0 else 'Negative'\n\n'Non-negative'\n\n\n\nx = 5\n\na = 100 if x&gt;= 0 else -100\na\n\n100"
  },
  {
    "objectID": "BigData_Analysis.html",
    "href": "BigData_Analysis.html",
    "title": "Big Data Analysis Engineer",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n빅분기 실기 - 3유형 문제풀이(1)\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nsklearn\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n빅분기 실기 - 3유형 문제풀이(2)\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nsklearn\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n빅분기 실기 - 3유형 문제풀이(3)\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nscipy\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n빅분기 실기 - 3유형 문제풀이(4)\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nscipy\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n빅분기 실기 - 3유형 문제풀이(5)\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nscipy\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n빅분기 실기 - 3유형 문제풀이(6)\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nscipy\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n빅분기 실기 - 2유형 문제풀이(5)\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nsklearn\n\n\n\n\n\n\n\n\n\n\n\nOct 23, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n빅분기 실기 - 2유형 문제풀이(1)\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nsklearn\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n빅분기 실기 - 2유형 문제풀이(2)\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nsklearn\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n빅분기 실기 - 2유형 문제풀이(3)\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nsklearn\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n빅분기 실기 - 2유형 문제풀이(4)\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nsklearn\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n빅분기 실기 - 1유형 문제풀이\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n빅분기 실기 - 파이썬 기초\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_6.html",
    "href": "BigData_Analysis/실기_3유형_6.html",
    "title": "빅분기 실기 - 3유형 문제풀이(6)",
    "section": "",
    "text": "실기 3 유형(6)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_6.html#타이타닉-데이터-불러오기생존자-예측-데이터",
    "href": "BigData_Analysis/실기_3유형_6.html#타이타닉-데이터-불러오기생존자-예측-데이터",
    "title": "빅분기 실기 - 3유형 문제풀이(6)",
    "section": "타이타닉 데이터 불러오기(생존자 예측 데이터)",
    "text": "타이타닉 데이터 불러오기(생존자 예측 데이터)\n\n# 데이터 불러오기\nimport pandas as pd\nimport numpy as np\n\n# Seaborn의 내장 타이타닉 데이터셋을 불러옵니다.\nimport seaborn as sns\ndf = sns.load_dataset('titanic')\n\n\nprint(df.head())\n\n   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n0         0       3    male  22.0      1      0   7.2500        S  Third   \n1         1       1  female  38.0      1      0  71.2833        C  First   \n2         1       3  female  26.0      0      0   7.9250        S  Third   \n3         1       1  female  35.0      1      0  53.1000        S  First   \n4         0       3    male  35.0      0      0   8.0500        S  Third   \n\n     who  adult_male deck  embark_town alive  alone  \n0    man        True  NaN  Southampton    no  False  \n1  woman       False    C    Cherbourg   yes  False  \n2  woman       False  NaN  Southampton   yes   True  \n3  woman       False    C  Southampton   yes  False  \n4    man        True  NaN  Southampton    no   True  \n\n\n\n# 분석 데이터 설정\ndf = df[['survived', 'sex', 'sibsp', 'fare']] \n# sex:성별, sibsp:탑승한 부모 및 자녀수 fare:요금\n\nprint(df.head())\n\n   survived     sex  sibsp     fare\n0         0    male      1   7.2500\n1         1  female      1  71.2833\n2         1  female      0   7.9250\n3         1  female      1  53.1000\n4         0    male      0   8.0500\n\n\n\n로지스틱 회귀식 : P(1일 확률) = 1 / (1+exp(-f(x))\n\nf(x) = b0 + b1x1 + b2x2 + b3x3\nln(P/1-P) = b0 + b1x1 + b2x2 + b3x3\n(P = 생존할 확률, x1=sex, x2=sibsp, x3=fare)\n\n\n# 데이터 전처리\n# 변수처리\n# 문자형 타입의 데이터의 경우 숫자로 변경해준다.\n# *** 실제 시험에서 지시사항을 따를 것 ***\n\n# 성별을 map 함수를 활용해서 각각 1과 0에 할당한다. (여성을 1, 남성을 0)\n# (실제 시험의 지시 조건에 따를 것)\ndf['sex'] = df['sex'].map({'female':1,\n                          'male':0})\nprint(df.head())\n\n   survived  sex  sibsp     fare\n0         0    0      1   7.2500\n1         1    1      1  71.2833\n2         1    1      0   7.9250\n3         1    1      1  53.1000\n4         0    0      0   8.0500\n\n\n\nprint(df.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 4 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   survived  891 non-null    int64  \n 1   sex       891 non-null    int64  \n 2   sibsp     891 non-null    int64  \n 3   fare      891 non-null    float64\ndtypes: float64(1), int64(3)\nmemory usage: 28.0 KB\nNone"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_6.html#sklearn-라이브러리-활용",
    "href": "BigData_Analysis/실기_3유형_6.html#sklearn-라이브러리-활용",
    "title": "빅분기 실기 - 3유형 문제풀이(6)",
    "section": "1. sklearn 라이브러리 활용",
    "text": "1. sklearn 라이브러리 활용\n\n# 독립변수와 종속변수 설정\nx = df.drop(['survived'], axis=1) # x = df[['sex', 'age', 'fare']]\ny = df['survived']\n\n(주의) LogisticRegrresion() 객체안에 반드시 penalty=None으로 입력해야 함\n\n# 모델링\nfrom sklearn.linear_model import LogisticRegression # 회귀는 LinearRegression\n\n# 반드시 penalty = None으로 입력할 것해야 함.  default='l2'\nmodel1 = LogisticRegression(penalty='none')\nmodel1.fit(x, y)\n\nLogisticRegression(penalty='none')\n\n\n\n# 로지스틱회귀분석 관련 지표 출력\n\n# 1.회귀계수 출력 : model.coef_\nprint(np.round(model1.coef_, 4))        # 전체 회귀계수\nprint(np.round(model1.coef_[0,0], 4))   # x1의 회귀계수\nprint(np.round(model1.coef_[0,1], 4))   # x2의 회귀계수\nprint(np.round(model1.coef_[0,2], 4))   # x3의 회귀계수\n\n# 2. 회귀계수(절편) : model.intercept_\nprint(np.round(model1.intercept_, 4))\n\n[[ 2.5668 -0.4017  0.0138]]\n2.5668\n-0.4017\n0.0138\n[-1.6964]\n\n\n\n로지스틱 회귀식 : P(1일 확률) = 1 / (1+exp(-f(x))\n\nf(x) = b0 + b1x1 + b2x2 + b3x3\nln(P/1-P) = b0 + b1x1 + b2x2 + b3x3\n(P = 생존할 확률, x1=sex, x2=sibsp, x3=fare)\n\n\n\n결과 : ln(P/1-P) = -1.6964 + 2.5668sex - 0.4017sibsp + 0.0138fare\n\n# 3-1. 로지스틱 회귀모형에서 sibsp 변수가 한단위 증가할 때 생존할 오즈가 몇 배 \n#      증가하는지 반올림하여 소수점 셋째 자리까지 구하시오.\n\n# exp(b2) 를 구하면 된다.\nresult = np.exp(model1.coef_[0,1]) #인덱싱 주의하세요.\nprint(round(result,3))\n\n# 해석 : sibsp 변수가 한 단위 증가할 때 생존할 오즈가 0.669배 증가한다.\n#        생존할 오즈가 33% 감소한다.(생존할 확률이 감소한다.)\n\n0.669\n\n\n\n# 3-2. 로지스틱 회귀모형에서 여성일 경우 남성에 비해 오즈가 몇 배 증가하는지\n#      반올림하여 소수점 셋째 자리까지 구하시오\n\n# exp(b2) 를 구하면 된다.\nresult2 = np.exp(model1.coef_[0,0]) #인덱싱 주의하세요.\nprint(round(result2,3))\n\n# 해석 : 여성일 경우 남성에 비해 생존할 오즈가 13.024배 증가한다.\n#        생존할 오즈가 13배 증가한다.(생존할 확률이 증가한다.)\n\n13.024"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_6.html#statsmodels-라이브러리-사용",
    "href": "BigData_Analysis/실기_3유형_6.html#statsmodels-라이브러리-사용",
    "title": "빅분기 실기 - 3유형 문제풀이(6)",
    "section": "2. statsmodels 라이브러리 사용",
    "text": "2. statsmodels 라이브러리 사용\n(주의) 실제 오즈가 몇 배 증가했는지 계산하는 문제가 나온다면\nsklearn 라이브러리를 사용하여 회귀계수를 직접구해서 계산할 것(소수점이 결과값에 영향을 줄 수 있음)\n\n# 모델링\nimport statsmodels.api as sm\n\nx = sm.add_constant(x)       # 주의 : 상수항 추가해줘야 함\nmodel2 = sm.Logit(y,x).fit() # 주의할 것 : y, x 순으로 입력해야 함\nsummary = model2.summary()\nprint(summary)\n\nOptimization terminated successfully.\n         Current function value: 0.483846\n         Iterations 6\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:               survived   No. Observations:                  891\nModel:                          Logit   Df Residuals:                      887\nMethod:                           MLE   Df Model:                            3\nDate:                Wed, 29 Nov 2023   Pseudo R-squ.:                  0.2734\nTime:                        19:06:26   Log-Likelihood:                -431.11\nconverged:                       True   LL-Null:                       -593.33\nCovariance Type:            nonrobust   LLR p-value:                 5.094e-70\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -1.6964      0.129    -13.134      0.000      -1.950      -1.443\nsex            2.5668      0.179     14.321      0.000       2.216       2.918\nsibsp         -0.4017      0.095     -4.222      0.000      -0.588      -0.215\nfare           0.0138      0.003      5.367      0.000       0.009       0.019\n==============================================================================\n\n\n\n(결과 비교해보기) 두 라이브러리 모두 같은 결과값을 출력\n\n회귀식 : P(1일 확률) = 1 / (1+exp(-f(x))\nf(x) = b0 + b1x1 + b2x2 + b3x3\nln(P/1-P) = b0 + b1x1 + b2x2 + b3x3\n(P=생존할 확률, x1=sex, x2=sibsp, x3=fare)\n\n1. sklearn : ln(P/1-P) = -1.6964 + 2.5668sex - 0.4017sibsp + 0.0138fare\n2. statsmodel : ln(P/1-P) = -1.6964 + 2.5668sex - 0.4017sibsp + 0.0138fare"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_4.html",
    "href": "BigData_Analysis/실기_3유형_4.html",
    "title": "빅분기 실기 - 3유형 문제풀이(4)",
    "section": "",
    "text": "실기 3 유형(4)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_4.html#분석-case",
    "href": "BigData_Analysis/실기_3유형_4.html#분석-case",
    "title": "빅분기 실기 - 3유형 문제풀이(4)",
    "section": "✅ 분석 Case",
    "text": "✅ 분석 Case\n\nCase 1. 적합도 검정 - 각 범주에 속할 확률이 같은지?\n\n\nCase 2. 독립성 검정 - 두개의 범주형 변수가 서로 독립인지?"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_4.html#가설검정-순서중요",
    "href": "BigData_Analysis/실기_3유형_4.html#가설검정-순서중요",
    "title": "빅분기 실기 - 3유형 문제풀이(4)",
    "section": "✅ 가설검정 순서(중요!!)",
    "text": "✅ 가설검정 순서(중요!!)\n\n\n가설검정\n유의수준 확인\n검정실시(통계량, p-value 확인)\n귀무가설 기각여부 결정(채택/기각)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_4.html#예제문제",
    "href": "BigData_Analysis/실기_3유형_4.html#예제문제",
    "title": "빅분기 실기 - 3유형 문제풀이(4)",
    "section": "✅ 예제문제",
    "text": "✅ 예제문제\n\nCase 1. 적합도 검정 - 각 범주에 속할 확률이 같은지?\n\n\n문제 1-1\n\n\n랜덤 박스에 상품 A, B, C, D가 들어있다.\n\n\n다음은 랜덤박스에서 100번 상품을 꺼냈을 떄의 상품 데이터라고 할 때\n\n\n상품이 동일한 비율로 들어있다고 할 수 있는지 검정해보시오.\n\nimport pandas as pd\nimport numpy as np\n\n\n# 데이터 생성\nrow1 = [30,20,15,35]\ndf = pd.DataFrame([row1], columns=['A','B','C','D'])\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n30\n20\n15\n35\n\n\n\n\n\n\n\n\n# 1. 가설검정\n# H0 : 랜덤박스에 상품 A,B,C,D가 동일한 비율로 들어있다.\n# H1 : 랜덤박스에 상품 A,B,C,D가 동일한 비율로 들어있지 않다.\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3. 검정실시(통계량, p-value)\nfrom scipy.stats import chisquare\n# chisquare(f_obs=f_obs, f_exp=f_exp)   # 관측값, 기대값\n\n# 관측값과 기대값 구하기\nf_obs = [30,20,15,35]\n# f_obs = df.iloc[0]\nf_exp = [25,25,25,25]\n\nstatistic, pvalue = chisquare(f_obs=f_obs, f_exp=f_exp)\nprint(statistic)\nprint(pvalue)\n\n10.0\n0.01856613546304325\n\n\n\n# 4. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값이 0.05보다 작기 떄문에 귀무가설을 기각한다.\n# 즉, 랜덤박스에 상품 A,B,C,D가 동일한 비율로 들어있지 않다고 할 수 있다.\n\n# 답 : 기각\n\n\n\n문제 1-1\n\n\n랜덤 박스에 상품 A, B, C가 들어있다.\n\n\n다음은 랜덤박스에서 150번 상품을 꺼냈을 떄의 상품 데이터라고 할 때\n\n\n상품별로 A 30%, B 15%, C 55% 비율로 들어있다고 할 수 있는지 검정해보시오.\n\nimport pandas as pd\nimport numpy as np\n\n\n# 데이터 생성\nrow1 = [50,25,75]\ndf = pd.DataFrame([row1], columns=['A','B','C'])\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n50\n25\n75\n\n\n\n\n\n\n\n\n# 1. 가설검정\n# H0 : 랜덤박스에 상품 A,B,C가 30%, 15%, 55%의 비율로 들어있다.\n# H1 : 랜덤박스에 상품 A,B,C가 30%, 15%, 55%의비율로 들어있지 않다.\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3. 검정실시(통계량, p-value)\nfrom scipy.stats import chisquare\n# chisquare(f_obs=f_obs, f_exp=f_exp)   # 관측값, 기대값\n\n# 관측값과 기대값 구하기\nf_obs = [50,25,75]\n# f_obs = df.iloc[0]\n\na = 150*0.3\nb = 150*0.15\nc = 150*0.55\nf_exp = [a,b,c]\n\nstatistic, pvalue = chisquare(f_obs=f_obs, f_exp=f_exp)\nprint(statistic)\nprint(pvalue)\n\n1.5151515151515151\n0.46880153914023537\n\n\n\n# 4. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값이 0.05보다 크기 떄문에 귀무가설을 채택한다.\n# 즉, 랜덤박스에 상품 A,B,C가 30%, 15%, 55%의 비율로 들어있다고 할 수 있다.\n\n# 답 : 채택\n\n\n\nCase 2. 독립성 검정 - 두개의 범주형 변수가 서로 독립인지?\n\n\n문제 2-1\n\n\n연령대에 따라 먹는 아이스크림의 차이가 있는지 독립성 검정을 실시하시오.\n\nimport pandas as pd\nimport numpy as np\n\n\n# 데이터 생성\nrow1, row2 = [200,190,250],[220,250,300]\ndf = pd.DataFrame([row1, row2], columns=['딸기','초코','바닐라']\n                  ,index=['10대', '20대'])\ndf\n\n\n\n\n\n\n\n\n딸기\n초코\n바닐라\n\n\n\n\n10대\n200\n190\n250\n\n\n20대\n220\n250\n300\n\n\n\n\n\n\n\n\n# 1. 가설설정\n# H0 : 연령대와 먹는 아이스크림의 종류는 서로 관련이 없다(두 변수는 서로 독립이다.)\n# H1 : 연령대와 먹는 아이스크림의 종류는 서로 관련이 있다(두 변수는 서로 독립이 아니다.)\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3.검정실시(통계량, p-value, 기대값 확인)\nfrom scipy.stats import chi2_contingency\n\nstatistic, pvalue, dof, expected = chi2_contingency(df)\n# 공식문서상에 : statistic(통계량), pvalue, dof(자유도), expected_freq(기대값)\n\n# 아래와 같이 입력해도 동일한 결과\n# statistic, pvalue, dof, expected = chi2_contingency([row1, row2])\n# statistic, pvalue, dof, expected = chi2_contingency([df.iloc[0], df.iloc[1]])\n\nprint(statistic)\nprint(pvalue)\nprint(dof) # 자유도 = (행-1) * (열*1)\nprint(np.round(expected, 2)) # 반올림하고 싶다면 np.round()\n\n1.708360126075226\n0.4256320394874311\n2\n[[190.64 199.72 249.65]\n [229.36 240.28 300.35]]\n\n\n\n# 4. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값이 0.05보다 크기 떄문에 귀무가설을 채택한다.\n# 즉, 연령대와 먹는 아이스크림의 종류는 서로 관련이 없다고 할 수 있다.\n\n# 답 : 채택\n\n\n\n(추가) 만약 데이터 형태가 다를경우?\n\n# (Case1) 만약 데이터가 아래와 같이 주어진다면?\ndf = pd.DataFrame({\n    '아이스크림' : ['딸기', '초코', '바닐라', '딸기', '초코', '바닐라'],\n    '연령' : ['10대','10대','10대','20대','20대','20대'],\n    '인원' : [200,190,250,220,250,300]\n})\ndf\n\n\n\n\n\n\n\n\n아이스크림\n연령\n인원\n\n\n\n\n0\n딸기\n10대\n200\n\n\n1\n초코\n10대\n190\n\n\n2\n바닐라\n10대\n250\n\n\n3\n딸기\n20대\n220\n\n\n4\n초코\n20대\n250\n\n\n5\n바닐라\n20대\n300\n\n\n\n\n\n\n\n\n# pd.crosstab(index= , columns= , values= , aggfunc=sum)\ntable = pd.crosstab(index=df['연령'] , columns=df['아이스크림'] ,\n                    values= df['인원'], aggfunc=sum)\ntable\n# 주의 : index, columns에 순서를 꼭 확인하기\n\n\n\n\n\n\n\n아이스크림\n딸기\n바닐라\n초코\n\n\n연령\n\n\n\n\n\n\n\n10대\n200\n250\n190\n\n\n20대\n220\n300\n250\n\n\n\n\n\n\n\n\n# 3.검정실시(통계량, p-value, 기대값 확인)\nfrom scipy.stats import chi2_contingency\n\nstatistic, pvalue, dof, expected = chi2_contingency(table)\n# 공식문서상에 : statistic(통계량), pvalue, dof(자유도), expected_freq(기대값)\n\nprint(statistic)\nprint(pvalue)\nprint(dof) # 자유도 = (행-1) * (열*1)\nprint(np.round(expected, 2)) # 반올림하고 싶다면 np.round()\n\n1.708360126075226\n0.4256320394874311\n2\n[[190.64 249.65 199.72]\n [229.36 300.35 240.28]]\n\n\n\n# (Case2) 만약 데이터가 아래와 같이 주어진다면?\n# (이해를 위한 참고용 입니다. 빈도수 카운팅)\ndf = pd.DataFrame({\n    '아이스크림' : ['딸기', '초코', '바닐라', '딸기', '초코', '바닐라'],\n    '연령' : ['10대','10대','10대','20대','20대','20대']\n})\ndf\n\n\n\n\n\n\n\n\n아이스크림\n연령\n\n\n\n\n0\n딸기\n10대\n\n\n1\n초코\n10대\n\n\n2\n바닐라\n10대\n\n\n3\n딸기\n20대\n\n\n4\n초코\n20대\n\n\n5\n바닐라\n20대\n\n\n\n\n\n\n\n\n# pd.crosstab(index, columns)\npd.crosstab(df['연령'],df['아이스크림'])\n\n\n\n\n\n\n\n아이스크림\n딸기\n바닐라\n초코\n\n\n연령\n\n\n\n\n\n\n\n10대\n1\n1\n1\n\n\n20대\n1\n1\n1\n\n\n\n\n\n\n\n\n\n문제2-2\n\n\n타이타닉에 데이터에서 성별(sex)과 생존여부(survied) 변수간\n\n\n독립성 검정을 실시하시오\n\nimport seaborn as sns\ndf = sns.load_dataset('titanic')\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 15 columns):\n #   Column       Non-Null Count  Dtype   \n---  ------       --------------  -----   \n 0   survived     891 non-null    int64   \n 1   pclass       891 non-null    int64   \n 2   sex          891 non-null    object  \n 3   age          714 non-null    float64 \n 4   sibsp        891 non-null    int64   \n 5   parch        891 non-null    int64   \n 6   fare         891 non-null    float64 \n 7   embarked     889 non-null    object  \n 8   class        891 non-null    category\n 9   who          891 non-null    object  \n 10  adult_male   891 non-null    bool    \n 11  deck         203 non-null    category\n 12  embark_town  889 non-null    object  \n 13  alive        891 non-null    object  \n 14  alone        891 non-null    bool    \ndtypes: bool(2), category(2), float64(2), int64(4), object(5)\nmemory usage: 80.7+ KB\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n\n\n\n\n\n\n# pd.crosstab(index, columns)\ntable = pd.crosstab(df['sex'], df['survived'])\nprint(table)\n\nsurvived    0    1\nsex               \nfemale     81  233\nmale      468  109\n\n\n\n# 1. 가설설정\n# H0 : 성별과 생존 여부는 서로 관련이 없다(두 변수는 서로 독립이다)\n# H1 : 성별과 생존 여부는 서로 관련이 있다(두 변수는 서로 독립이 아니다)\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3.검정실시(통계량, p-value, 기대값 확인)\nfrom scipy.stats import chi2_contingency\n\nstatistic, pvalue, dof, expected = chi2_contingency(table)\n# 공식문서상에 : statistic(통계량), pvalue, dof(자유도), expected_freq(기대값)\n\nprint(statistic)\nprint(pvalue)\nprint(dof) # 자유도 = (행-1) * (열*1)\nprint(np.round(expected, 2)) # 반올림하고 싶다면 np.round()\n\n260.71702016732104\n1.1973570627755645e-58\n1\n[[193.47 120.53]\n [355.53 221.47]]\n\n\n\n# 4. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값(1.1973570627755645e-58)이 0.05보다 작기 떄문에 귀무가설을 기각한다.\n# 즉, 성별과 생존 여부는 서로 관련이 있다고 할 수 있다.\n\n# 답 : 기각\n\n\n\n데이터를 변경해보면서 이해해봅시다.\n\n# 임의 데이터 생성\nsex, survived = [160,160], [250,220]\ntable = pd.DataFrame([sex, survived], columns=['0','1'], index=['female','male'])\nprint(table)\n\n          0    1\nfemale  160  160\nmale    250  220\n\n\n\n# 3.검정실시(통계량, p-value, 기대값 확인)\nfrom scipy.stats import chi2_contingency\n\nstatistic, pvalue, dof, expected = chi2_contingency(table)\n# 공식문서상에 : statistic(통계량), pvalue, dof(자유도), expected_freq(기대값)\n\nprint(statistic)\nprint(pvalue)\nprint(dof) # 자유도 = (행-1) * (열*1)\nprint(np.round(expected, 2)) # 반올림하고 싶다면 np.round()\n\n0.6541895872879862\n0.41861876333789727\n1\n[[166.08 153.92]\n [243.92 226.08]]\n\n\n\n# 4. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값이 0.05보다 크기 떄문에 귀무가설을 채택한다.\n# 즉, 성별과 생존 여부는 서로 관련이 없다고 할 수 있다.\n\n# 답 : 채택"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_2.html",
    "href": "BigData_Analysis/실기_3유형_2.html",
    "title": "빅분기 실기 - 3유형 문제풀이(2)",
    "section": "",
    "text": "실기 3 유형(2)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_2.html#검정방법",
    "href": "BigData_Analysis/실기_3유형_2.html#검정방법",
    "title": "빅분기 실기 - 3유형 문제풀이(2)",
    "section": "✅ 검정방법",
    "text": "✅ 검정방법"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_2.html#대응표본쌍체-동일한-객체의-전-vs-후-평균비교",
    "href": "BigData_Analysis/실기_3유형_2.html#대응표본쌍체-동일한-객체의-전-vs-후-평균비교",
    "title": "빅분기 실기 - 3유형 문제풀이(2)",
    "section": "1. 대응표본(쌍체) : 동일한 객체의 전 vs 후 평균비교",
    "text": "1. 대응표본(쌍체) : 동일한 객체의 전 vs 후 평균비교\n\n(정규성o) 대응표본(쌍체) t검정(paired t-test) : 동일한 객체의 전 vs 후 평균비교\n(정규성x) 윌콕슨 부호순위 검정(wilcoxon)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_2.html#독립표본-a-집단의-평균-vs-b-집단의-평균",
    "href": "BigData_Analysis/실기_3유형_2.html#독립표본-a-집단의-평균-vs-b-집단의-평균",
    "title": "빅분기 실기 - 3유형 문제풀이(2)",
    "section": "2. 독립표본 : A 집단의 평균 vs B 집단의 평균",
    "text": "2. 독립표본 : A 집단의 평균 vs B 집단의 평균\n\n(정규성o) 독립표본 t검정(2sample t-test)\n(정규성x) 윌콕슨 순위합 검정(ranksums)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_2.html#가설검정-순서중요",
    "href": "BigData_Analysis/실기_3유형_2.html#가설검정-순서중요",
    "title": "빅분기 실기 - 3유형 문제풀이(2)",
    "section": "✅ 가설검정 순서(중요!!)",
    "text": "✅ 가설검정 순서(중요!!)\n\n1. 대응표본(쌍체) t검정(paired t-test)\n\n\n가설검정\n유의수준 확인\n정규성 검정 (주의) 차이값에 대한 정규성\n검정실시(통계량, p-value 확인)\n귀무가설 기각여부 결정(채택/기각)\n\n\n\n\n2. 독립표본 t검정(2sample t-test)\n\n\n가설검정\n유의수준 확인\n정규성 검정 (주의) 두 집단 모두 정규성을 따를 경우\n등분산 검정\n검정실시(통계량, p-value 확인)\n귀무가설 기각여부 결정(채택/기각)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_2.html#예제문제",
    "href": "BigData_Analysis/실기_3유형_2.html#예제문제",
    "title": "빅분기 실기 - 3유형 문제풀이(2)",
    "section": "✅ 예제문제",
    "text": "✅ 예제문제\n\nCase 1) 대응표본(쌍체) t 검정(paired t-test)\n문제 1-1 다음은 혈압약을 먹기 전, 후의 혈압 데이터이다.\n혈압약을 먹기 전, 후의 차이가 있는지 쌍체 t검정을 실시하시오.\n(유의수준 5%)\n\nbefore : 혈압약을 먹기전 혈압, after : 혈압약을 먹은 후 혈압\nH_0(귀무가설) : after - before = 0\nH_1(대립가설) : after - before != 0\n\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom scipy.stats import shapiro\n\n\n# 데이터 만들기\ndf = pd.DataFrame( {\n    'before': [120, 135, 122, 124, 135, 122, 145, 160, 155, 142, 144, 135, 167],\n    'after' : [110, 132, 123, 119, 123, 115, 140, 162, 142, 138, 135, 142, 160] })\n\n\n# 1. 가설검정\n# H0 : 약을 먹기전과 먹은 후의 혈압 평균은 같다(효과가 없다)\n# H1 : 약을 먹기전과 먹은 후의 혈압 평균은 같지 않다(효과가 있다)\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3. 정규성 검정 (차이값에 대해 정규성 확인)\nstatistic, pvalue = stats.shapiro(df['after'] - df['before'])\nprint(round(statistic,4), round(pvalue,4))\n\n0.9589 0.7363\n\n\n\np-value 값(0.7363)이 유의수준 (0.05)보다 크다.\n귀무가설 (H0) 채택(정규성검정의 H0 : 정규분포를 따른다)\n\n\n# 4.1 (정규성o) 대응표본(쌍체) t검정(paired t-test)\nstatistic, pvalue = stats.ttest_rel(df['after'], df['before']\n                                    , alternative='two-sided')\nprint(round(statistic,4), round(pvalue,4))\n\n-3.1382 0.0086\n\n\n\n# 4.2 (정규성x) wilcoxon 부호순위 검정\nstatistic, pvalue = stats.wilcoxon(df['after']-df['before'],\n                                  alternative='two-sided')\nprint(round(statistic,4), round(pvalue,4))\n\n11.0 0.0134\n\n\n\n# 5. 귀무가설 기각 여부 결정(채택/기각)\n# p-value 값(0.0086)이 0.05보다 작기 떄문에 귀무가설을 기각한다\n# 즉, 약을 먹기 전과 먹은 후의 혈압은 같지 않다(효과가 있다)\n\n# 답 : 기각(H1)\n\n차이가 있는지 없는지 확인은 양측검정 // 증가, 감소를 확인하는 것은 단측검정\n문제 1-2\n다음은 혈압약을 먹기 전, 후의 혈압 데이터이다.\n혈압약을 먹은 후 혈압이 감소했는지 확인하기 위해 쌍체 t 검정을 실시하시오\n(유의수준 5%)\n\nbefore : 혈압약을 먹기전 혈압, after : 혈압약을 먹은 후의 혈압\nH0(귀무가설) : after - before &gt;=0\nH1(대립가설) : after - before &lt; 0\n\n\n# 데이터 만들기\ndf = pd.DataFrame( {\n    'before': [120, 135, 122, 124, 135, 122, 145, 160, 155, 142, 144, 135, 167],\n    'after' : [110, 132, 123, 119, 123, 115, 140, 162, 142, 138, 135, 142, 160] })\nprint(df.head(3))\n\n   before  after\n0     120    110\n1     135    132\n2     122    123\n\n\n\n# 1. 가설검정\n# H0 : 약을 먹은 후 혈압이 같거나 증가했다. (after - before &gt;= 0)\n# H1 : 약을 먹은 후 혈압이 감소했다.        (after - before &lt;  0) \n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\nstatistic, pvalue = stats.shapiro(df['after']-df['before'])\nprint(round(statistic, 4), round(pvalue ,4))\n\n0.9589 0.7363\n\n\np-value가 0.05보다 크다 귀무가설 채택\n\n# 4.1 (정규성o) 대응표본(쌍체) t검정(paired t-test)\nstatistic, pvalue = stats.ttest_rel(df['after'], df['before']\n                                    , alternative='less')\nprint(round(statistic,4), round(pvalue,4))\n\n-3.1382 0.0043\n\n\n\n# 4.2 (정규성x) wilcoxon 부호순위 검정\nstatistic, pvalue = stats.wilcoxon(df['after']-df['before'],\n                                  alternative='less')\nprint(round(statistic,4), round(pvalue,4))\n\n11.0 0.0067\n\n\n\n# 5. 귀무가설 기각 여부 결정(채택/기각)\n# p-value 값(0.0043)이 0.05보다 작기 떄문에 귀무가설을 기각한다\n# 즉, 약을 먹은 후 혈압이 감소했다고 할 수 있다.(효과가 있다)\n\n# 답 : 기각(H1)\n\n\n\nCase 2) 독립표본 t검정(2sample t-test)\n문제 2-1\n다음은 A그룹과 B그룹 인원의 혈압 데이터이다.\n두 그룹의 혈압평균이 다르다고 할 수 있는지 독립표본 t검정을 실시하시오.\n(유의수준 5%)\n\nA : A그룹 인원의 혈압, B : B그룹 인원의 혈압\nH0(귀무가설) : A = B\nH1(대립가설) : A != B\n\n\n# 데이터 만들기\ndf = pd.DataFrame( {\n    'A': [120, 135, 122, 124, 135, 122, 145, 160, 155, 142, 144, 135, 167],\n    'B' : [110, 132, 123, 119, 123, 115, 140, 162, 142, 138, 135, 142, 160] })\nprint(df.head(3))\n\n     A    B\n0  120  110\n1  135  132\n2  122  123\n\n\n\n# 1. 가설검정\n# H0(귀무가설) : A그룹과 B그룹의 혈압 평균은 같다.      (A = B)\n# H1(대립가설) : A그룹과 B그룹의 혈압 평균은 같지 않다. (A != B)\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3. 정규성 검정\n# H0(귀무가설) : 정규분포를 따른다.\n# H1(대립가설) : 정규분포를 따르지 않는다.\n\nstatisticA, pvalueA = stats.shapiro(df['A'])\nstatisticB, pvalueB = stats.shapiro(df['B'])\nprint(round(statisticA, 4), round(pvalueA, 4))\nprint(round(statisticB, 4), round(pvalueB, 4))\n\n0.9314 0.3559\n0.9498 0.5956\n\n\n\np-value 값(0.3559, 0.5956)이 유의수준(0.05)보다 크다.\n귀무가설(H0) 채택\n만약 하나라도 정규분포를 따르지 않는다면 비모수 검정방법을 써야 함\n(윌콕슨의 순위합 검정 ranksums)\n\n\n# 4. 등분산성 검정\n# H0(귀무가설) : 등분산 한다.\n# H1(대립가설) : 등분산 하지 않는다.\nstatistic, pvalue = stats.bartlett(df['A'], df['B'])\nprint(round(statistic, 4), round(pvalue, 4))\n\n0.0279 0.8673\n\n\n\np-value 값이 유의수준(0.05) 보다 크다.\n귀무가설(H0) 채택 =&gt; 등분산성을 따른다고 할 수 있다.\n\n\n# 5.1 (정규성o, 등분산성 o/x) t검정\nstatistic, pvalue = stats.ttest_ind(df['A'], df['B'],\n                                   equal_var = True,\n                                   alternative='two-sided')\n# 만약 등분산 하지 않으면 False로 설정\nprint(round(statistic, 4), round(pvalue, 4))\n\n0.8192 0.4207\n\n\n\n# 5.2 (정규성x)윌콕슨의 순위합 검정\nstatistic, pvalue = stats.ranksums(df['A'], df['B'],\n                                   alternative='two-sided')\nprint(round(statistic, 4), round(pvalue, 4))\n\n0.8462 0.3975\n\n\n\n# 6. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값이 0.05보다 크기 떄문에 귀무가설을 채택한다\n# 즉, A그룹과 B그룹의 혈압 평균은 같다고 할 수 있다.\n\n# 답 : 채택(H0)\n\n\n# (참고) 평균데이터 확인\nprint(round(df['A'].mean(), 4))\nprint(round(df['B'].mean(), 4))\n\n138.9231\n133.9231\n\n\n문제 2-2\n다음은 A그룹과 B그룹 인원의 혈압 데이터이다.\nA그룹의 혈압 평균이 B그룹보다 크다고 할 수 있는지 독립표본 t검정을 실시하시오.\n(유의수준 5%)\n\nA : A그룹 인원의 혈압, B : B그룹 인원의 혈압\nH0(귀무가설) : A - B &lt;= 0 (or A &lt;= B)\nH1(대립가설) : A - B &gt; 0 (or A &gt; B)\n\n\n# 데이터 만들기\ndf = pd.DataFrame( {\n    'A': [120, 135, 122, 124, 135, 122, 145, 160, 155, 142, 144, 135, 167],\n    'B' : [110, 132, 123, 119, 123, 115, 140, 162, 142, 138, 135, 142, 160] })\nprint(df.head(3))\n\n     A    B\n0  120  110\n1  135  132\n2  122  123\n\n\n\n# 1. 가설검정\n# H0(귀무가설) : A그룹의 혈압 평균이 B그룹보다 작거나 같다. (A &lt;= B)\n# H1(대립가설) : A그룹의 혈압 평균이 B그룹보다 크다.        (A &gt;  B)\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3. 정규성 검정(차이값에 대해 정규성 확인)\n# H0(귀무가설) : 정규분포를 따른다.\n# H1(대립가설) : 정규분포를 따르지 않는다.\n\nstatisticA, pvalueA = stats.shapiro(df['A'])\nstatisticB, pvalueB = stats.shapiro(df['B'])\nprint(round(statisticA, 4), round(pvalueA, 4))\nprint(round(statisticB, 4), round(pvalueB, 4))\n\n0.9314 0.3559\n0.9498 0.5956\n\n\n\np-value 값(0.3559, 0.5956)이 유의수준(0.05)보다 크다.\n귀무가설(H0) 채택\n만약 하나라도 정규분포를 따르지 않는다면 비모수 검정방법을 써야 함\n(윌콕슨의 순위합 검정 ranksums)\n\n\n# 4. 등분산성 검정\n# H0(귀무가설) : 등분산 한다.\n# H1(대립가설) : 등분산 하지 않는다.\nstatistic, pvalue = stats.bartlett(df['A'], df['B'])\nprint(round(statistic, 4), round(pvalue, 4))\n\n0.0279 0.8673\n\n\n\n# 5.1 (정규성o, 등분산성 o/x) t검정\nstatistic, pvalue = stats.ttest_ind(df['A'], df['B'],\n                                   equal_var = True,\n                                   alternative='greater')\n# 만약 등분산 하지 않으면 False로 설정\nprint(round(statistic, 4), round(pvalue, 4))\n\n0.8192 0.2104\n\n\n\n# 5.2 (정규성x)윌콕슨의 순위합 검정\nstatistic, pvalue = stats.ranksums(df['A'], df['B'],\n                                   alternative='greater')\n# A그룹의 평균값이 B그룹의 평균값보다 클 수 있는가를 대립가설(H1)으로 설정했기에 greater\nprint(round(statistic, 4), round(pvalue, 4))\n\n0.8462 0.1987\n\n\n\n# 6. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값이 0.05보다 크기 떄문에 귀무가설을 채택한다\n# 즉, A그룹의 혈압 평균이 B그룹보다 작거나 같다고 할 수 있다.\n# (A그룹의 혈압 평균이 B그룹보다 크다고 할 수 없다)\n\n# 답 : 채택(H0)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_5.html",
    "href": "BigData_Analysis/실기_2유형_5.html",
    "title": "빅분기 실기 - 2유형 문제풀이(5)",
    "section": "",
    "text": "실기 2 유형(5)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_5.html#데이터-분석-순서",
    "href": "BigData_Analysis/실기_2유형_5.html#데이터-분석-순서",
    "title": "빅분기 실기 - 2유형 문제풀이(5)",
    "section": "✅ 데이터 분석 순서",
    "text": "✅ 데이터 분석 순서\n\n1. 라이브러리 및 데이터 확인\n\n\n2. 데이터 탐색(EDA)\n\n\n3. 데이터 전처리 및 분리\n\n\n4. 모델링 및 성능평가\n\n\n5. 예측값 제출"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_5.html#라이브러리-및-데이터-확인-1",
    "href": "BigData_Analysis/실기_2유형_5.html#라이브러리-및-데이터-확인-1",
    "title": "빅분기 실기 - 2유형 문제풀이(5)",
    "section": "✅ 1. 라이브러리 및 데이터 확인",
    "text": "✅ 1. 라이브러리 및 데이터 확인\n\nimport pandas as pd\nimport numpy as np\n\n\n###############  복사 영역  ###############\n# 실기 시험 데이터셋으로 셋팅하기 (수정금지)\n\n# Seaborn의 내장 타이타닉 데이터셋을 불러옵니다.\nimport seaborn as sns\ndf = sns.load_dataset('titanic')\n\nx = df.drop('survived', axis=1)\ny = df['survived']\n\n# 실기 시험 데이터셋으로 셋팅하기 (수정금지)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, \n                                                    stratify=y,\n                                                    random_state=2023)\n\nx_test = pd.DataFrame(x_test)\nx_train = pd.DataFrame(x_train)\ny_train = pd.DataFrame(y_train)\ny_test = pd.DataFrame(y_test) # 평가용\n\nx_test.reset_index()\ny_train.columns = ['target'] \ny_test.columns = ['target'] \n###############  복사 영역  ###############\n\n\n타이타닉 생존자 예측 문제\n\n\n- 데이터의 결측치, 중복 변수값에 대해 처리하고\n\n\n- 분류모델을 사용하여 Accuracy, F1 score, AUC 값을 산출하시오.\n\n\n데이터 설명\n\n\nsurvival : 0 = No, 1 = Yes -pclass : 객실 등급(1,2,3) -sex : 성별 -age : 나이 -sibsp : 타이타닉호에 탑승한 형제/배우자의 수 -parch : 타이타닉호에 탑승한 부모/자녀의 수 -fare : 요금 -embarked : 탑승지 이름(C, Q, S) Cherbourg / Queenstown / Southampton -(중복)class : 객실 등급(First, Second, Third) -who : man, women, child -adult_male : 성인남자인지 여부(True=성인남자, False 그외) -deck : 선실번호 첫 알파벳(A,B,C,D,E,F,G) -(중복) embark_town : 탑승지 이름(Cherbourg, Queenstown, Southampton) -(중복) alive : 생존여부(no:사망, yes:생존) -alone : 혼자 탑승했는지 여부(True=혼자, False=가족과 함께)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_5.html#데이터-탐색eda-1",
    "href": "BigData_Analysis/실기_2유형_5.html#데이터-탐색eda-1",
    "title": "빅분기 실기 - 2유형 문제풀이(5)",
    "section": "✅ 2. 데이터 탐색(EDA)",
    "text": "✅ 2. 데이터 탐색(EDA)\n\n# 데이터의 행/열 확인\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\n\n(712, 14)\n(179, 14)\n(712, 1)\n\n\n\n# 초기 데이터 확인\n\nprint(x_train.head())\nprint(x_test.head())\nprint(y_train.head())\n\n     pclass     sex   age  sibsp  parch   fare embarked   class    who  \\\n3         1  female  35.0      1      0  53.10        S   First  woman   \n517       3    male   NaN      0      0  24.15        Q   Third    man   \n861       2    male  21.0      1      0  11.50        S  Second    man   \n487       1    male  58.0      0      0  29.70        C   First    man   \n58        2  female   5.0      1      2  27.75        S  Second  child   \n\n     adult_male deck  embark_town alive  alone  \n3         False    C  Southampton   yes  False  \n517        True  NaN   Queenstown    no   True  \n861        True  NaN  Southampton    no  False  \n487        True    B    Cherbourg    no   True  \n58        False  NaN  Southampton   yes  False  \n     pclass     sex   age  sibsp  parch      fare embarked   class    who  \\\n800       2    male  34.0      0      0   13.0000        S  Second    man   \n341       1  female  24.0      3      2  263.0000        S   First  woman   \n413       2    male   NaN      0      0    0.0000        S  Second    man   \n575       3    male  19.0      0      0   14.5000        S   Third    man   \n202       3    male  34.0      0      0    6.4958        S   Third    man   \n\n     adult_male deck  embark_town alive  alone  \n800        True  NaN  Southampton    no   True  \n341       False    C  Southampton   yes  False  \n413        True  NaN  Southampton    no   True  \n575        True  NaN  Southampton    no   True  \n202        True  NaN  Southampton    no   True  \n     target\n3         1\n517       0\n861       0\n487       0\n58        1\n\n\n\n# 변수명과 데이터 타입이 매칭이 되는지, 결측치가 있는지 확인해보세요\n\nprint(x_train.info())\nprint(x_test.info())\nprint(y_train.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 712 entries, 3 to 608\nData columns (total 14 columns):\n #   Column       Non-Null Count  Dtype   \n---  ------       --------------  -----   \n 0   pclass       712 non-null    int64   \n 1   sex          712 non-null    object  \n 2   age          579 non-null    float64 \n 3   sibsp        712 non-null    int64   \n 4   parch        712 non-null    int64   \n 5   fare         712 non-null    float64 \n 6   embarked     710 non-null    object  \n 7   class        712 non-null    category\n 8   who          712 non-null    object  \n 9   adult_male   712 non-null    bool    \n 10  deck         164 non-null    category\n 11  embark_town  710 non-null    object  \n 12  alive        712 non-null    object  \n 13  alone        712 non-null    bool    \ndtypes: bool(2), category(2), float64(2), int64(3), object(5)\nmemory usage: 64.4+ KB\nNone\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 179 entries, 800 to 410\nData columns (total 14 columns):\n #   Column       Non-Null Count  Dtype   \n---  ------       --------------  -----   \n 0   pclass       179 non-null    int64   \n 1   sex          179 non-null    object  \n 2   age          135 non-null    float64 \n 3   sibsp        179 non-null    int64   \n 4   parch        179 non-null    int64   \n 5   fare         179 non-null    float64 \n 6   embarked     179 non-null    object  \n 7   class        179 non-null    category\n 8   who          179 non-null    object  \n 9   adult_male   179 non-null    bool    \n 10  deck         39 non-null     category\n 11  embark_town  179 non-null    object  \n 12  alive        179 non-null    object  \n 13  alone        179 non-null    bool    \ndtypes: bool(2), category(2), float64(2), int64(3), object(5)\nmemory usage: 16.6+ KB\nNone\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 712 entries, 3 to 608\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype\n---  ------  --------------  -----\n 0   target  712 non-null    int64\ndtypes: int64(1)\nmemory usage: 11.1 KB\nNone\n\n\n\n# x_train 과 x_test 데이터의 기초통계량을 잘 비교해보세요\n\nprint(x_train.describe())\nprint(x_test.describe())\nprint(y_train.describe())\n\n           pclass         age       sibsp       parch        fare\ncount  712.000000  579.000000  712.000000  712.000000  712.000000\nmean     2.307584   29.479568    0.518258    0.372191   31.741836\nstd      0.834926   14.355304    1.094522    0.792341   45.403910\nmin      1.000000    0.420000    0.000000    0.000000    0.000000\n25%      2.000000   20.000000    0.000000    0.000000    7.895800\n50%      3.000000   28.000000    0.000000    0.000000   14.454200\n75%      3.000000   38.000000    1.000000    0.000000   31.275000\nmax      3.000000   74.000000    8.000000    6.000000  512.329200\n           pclass         age       sibsp       parch        fare\ncount  179.000000  135.000000  179.000000  179.000000  179.000000\nmean     2.312849   30.640741    0.541899    0.418994   34.043364\nstd      0.842950   15.258427    1.137797    0.859760   64.097184\nmin      1.000000    1.000000    0.000000    0.000000    0.000000\n25%      2.000000   22.000000    0.000000    0.000000    7.925000\n50%      3.000000   29.000000    0.000000    0.000000   14.500000\n75%      3.000000   39.000000    1.000000    0.000000   30.285400\nmax      3.000000   80.000000    8.000000    5.000000  512.329200\n           target\ncount  712.000000\nmean     0.383427\nstd      0.486563\nmin      0.000000\n25%      0.000000\n50%      0.000000\n75%      1.000000\nmax      1.000000\n\n\n\n# object, category 데이터도 추가확인\n\nprint(x_train.describe(include=\"object\"))\nprint(x_test.describe(include=\"object\"))\n\nprint(x_train.describe(include=\"category\"))\nprint(x_test.describe(include=\"category\"))\n\n         sex embarked  who  embark_town alive\ncount    712      710  712          710   712\nunique     2        3    3            3     2\ntop     male        S  man  Southampton    no\nfreq     469      518  432          518   439\n         sex embarked  who  embark_town alive\ncount    179      179  179          179   179\nunique     2        3    3            3     2\ntop     male        S  man  Southampton    no\nfreq     108      126  105          126   110\n        class deck\ncount     712  164\nunique      3    7\ntop     Third    C\nfreq      391   47\n        class deck\ncount     179   39\nunique      3    7\ntop     Third    C\nfreq      100   12\n\n\n\n# y 데이터도 구체적으로 살펴보세요\nprint(y_train.head())\n\n     target\n3         1\n517       0\n861       0\n487       0\n58        1\n\n\n\n# y 데이터도 구체적으로 살펴보세요\nprint(y_train.value_counts())\n\ntarget\n0         439\n1         273\ndtype: int64"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_5.html#데이터-전처리-및-분리-1",
    "href": "BigData_Analysis/실기_2유형_5.html#데이터-전처리-및-분리-1",
    "title": "빅분기 실기 - 2유형 문제풀이(5)",
    "section": "✅ 3. 데이터 전처리 및 분리",
    "text": "✅ 3. 데이터 전처리 및 분리\n\n1) 결측치, 2) 이상치, 3) 변수 처리하기\n\n# 결측치 확인\n\nprint(x_train.isnull().sum())\nprint(x_test.isnull().sum())\nprint(y_train.isnull().sum())\n\npclass           0\nsex              0\nage            133\nsibsp            0\nparch            0\nfare             0\nembarked         2\nclass            0\nwho              0\nadult_male       0\ndeck           548\nembark_town      2\nalive            0\nalone            0\ndtype: int64\npclass           0\nsex              0\nage             44\nsibsp            0\nparch            0\nfare             0\nembarked         0\nclass            0\nwho              0\nadult_male       0\ndeck           140\nembark_town      0\nalive            0\nalone            0\ndtype: int64\ntarget    0\ndtype: int64\n\n\n\n# 결측치 제거\n# df = df.dropna()\n# print(df)\n\n# 참고사항\n# print(df.dropna().shape) # 행 기준으로 삭제\n\n\n# 결측치 대체\n# x_train(712, 14) : age(133), embarked(2), deck(548), embark_town(2)\n# x_test(179, 14) : age(44), deck(140)\n\n# 변수 제거\n# (중복)class\n# (중복) embark_town\n# (중복) alive \n# (결측치 다수) deck\n\n\n# 중복변수 제거\nx_train = x_train.drop(['class', 'embark_town', 'alive', 'deck'], axis=1)\nx_test = x_test.drop(['class', 'embark_town', 'alive', 'deck'], axis=1)\n\n\n# 변수 제거 확인\nprint(x_train.info())\nprint(x_test.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 712 entries, 3 to 608\nData columns (total 10 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   pclass      712 non-null    int64  \n 1   sex         712 non-null    object \n 2   age         579 non-null    float64\n 3   sibsp       712 non-null    int64  \n 4   parch       712 non-null    int64  \n 5   fare        712 non-null    float64\n 6   embarked    710 non-null    object \n 7   who         712 non-null    object \n 8   adult_male  712 non-null    bool   \n 9   alone       712 non-null    bool   \ndtypes: bool(2), float64(2), int64(3), object(3)\nmemory usage: 51.5+ KB\nNone\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 179 entries, 800 to 410\nData columns (total 10 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   pclass      179 non-null    int64  \n 1   sex         179 non-null    object \n 2   age         135 non-null    float64\n 3   sibsp       179 non-null    int64  \n 4   parch       179 non-null    int64  \n 5   fare        179 non-null    float64\n 6   embarked    179 non-null    object \n 7   who         179 non-null    object \n 8   adult_male  179 non-null    bool   \n 9   alone       179 non-null    bool   \ndtypes: bool(2), float64(2), int64(3), object(3)\nmemory usage: 12.9+ KB\nNone\n\n\n\n# 결측치 대체\n# x_train(712, 14) : age(133), embarked(2)\n# x_test(179, 14) : age(44)\n\n# age 변수 \nmed_age = x_train['age'].median()\nx_train['age'] = x_train['age'].fillna(med_age)\nx_test['age'] = x_test['age'].fillna(med_age) # train data의 중앙값으로 \n\n# embarked(object 타입은 대체시 주로 최빈값으로)\nmode_et = x_train['embarked'].mode()\nx_train['embarked'] = x_train['embarked'].fillna(mode_et[0]) # 최빈값 [0] 주의\n\n\n# 결측치 대체 여부 확인\nprint(x_train.isnull().sum())\nprint(x_test.isnull().sum())\n\npclass        0\nsex           0\nage           0\nsibsp         0\nparch         0\nfare          0\nembarked      0\nwho           0\nadult_male    0\nalone         0\ndtype: int64\npclass        0\nsex           0\nage           0\nsibsp         0\nparch         0\nfare          0\nembarked      0\nwho           0\nadult_male    0\nalone         0\ndtype: int64\n\n\n\n# 변수처리(원핫인코딩) - object 변수만 적용됨\nx_train = pd.get_dummies(x_train)\nx_test = pd.get_dummies(x_test)\n\nprint(x_train.info())\nprint(x_test.info())\n\n# advanced 버전 사용\nx_train_ad = x_train.copy()\nx_test_ad = x_test.copy()\ny_train_ad = y_train.copy()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 712 entries, 3 to 608\nData columns (total 15 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   pclass      712 non-null    int64  \n 1   age         712 non-null    float64\n 2   sibsp       712 non-null    int64  \n 3   parch       712 non-null    int64  \n 4   fare        712 non-null    float64\n 5   adult_male  712 non-null    bool   \n 6   alone       712 non-null    bool   \n 7   sex_female  712 non-null    uint8  \n 8   sex_male    712 non-null    uint8  \n 9   embarked_C  712 non-null    uint8  \n 10  embarked_Q  712 non-null    uint8  \n 11  embarked_S  712 non-null    uint8  \n 12  who_child   712 non-null    uint8  \n 13  who_man     712 non-null    uint8  \n 14  who_woman   712 non-null    uint8  \ndtypes: bool(2), float64(2), int64(3), uint8(8)\nmemory usage: 40.3 KB\nNone\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 179 entries, 800 to 410\nData columns (total 15 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   pclass      179 non-null    int64  \n 1   age         179 non-null    float64\n 2   sibsp       179 non-null    int64  \n 3   parch       179 non-null    int64  \n 4   fare        179 non-null    float64\n 5   adult_male  179 non-null    bool   \n 6   alone       179 non-null    bool   \n 7   sex_female  179 non-null    uint8  \n 8   sex_male    179 non-null    uint8  \n 9   embarked_C  179 non-null    uint8  \n 10  embarked_Q  179 non-null    uint8  \n 11  embarked_S  179 non-null    uint8  \n 12  who_child   179 non-null    uint8  \n 13  who_man     179 non-null    uint8  \n 14  who_woman   179 non-null    uint8  \ndtypes: bool(2), float64(2), int64(3), uint8(8)\nmemory usage: 10.1 KB\nNone\n\n\n\n# (참고사항)원핫인코딩 후 변수의 수가 다른 경우\n# =&gt; x_test의 변수의 수가 x_train 보다 많은 경우 (혹은 그 반대인 경우)\n# 원핫인코딩 후 Feature 수가 다를 경우\n# x_train = pd.get_dummies(x_train)\n# x_test = pd.get_dummies(x_test)\n# x_train.info()\n# x_test.info()\n# 해결방법(x_test의 변수가 수가 더 많은 경우의 코드)\n# x_train = x_train.reindex(columns = x_test.columns, fill_value=0)\n# x_train.info()\n\n\n\n데이터 분리\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_train,\n                                                 y_train['target'],\n                                                 test_size = 0.2,\n                                                 stratify = y_train['target'],\n                                                 random_state = 2023)\nprint(x_train.shape)\nprint(x_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)\n\n(569, 15)\n(143, 15)\n(569,)\n(143,)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_5.html#모델링-및-성능평가-1",
    "href": "BigData_Analysis/실기_2유형_5.html#모델링-및-성능평가-1",
    "title": "빅분기 실기 - 2유형 문제풀이(5)",
    "section": "✅ 4. 모델링 및 성능평가",
    "text": "✅ 4. 모델링 및 성능평가\n\n# 랜덤포레스트 모델 사용 (참고: 회귀모델은 RandomForestRegressor)\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(random_state = 2023)\nmodel.fit(x_train, y_train)\n\nRandomForestClassifier(random_state=2023)\n\n\n\n# 모델을 사용하여 테스트 데이터 예측\ny_pred = model.predict(x_val)\n\n\n# 모델 성능 평가 (accuracy, f1 score, AUC 등)\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score\nacc = accuracy_score(y_val, y_pred)  # (실제값, 예측값)\nf1 = f1_score(y_val, y_pred)         # (실제값, 예측값)\nauc = roc_auc_score(y_val, y_pred)   # (실제값, 예측값)\n\n\nprint(acc) # 정확도(Accuracy)\nprint(f1) # f1 score\nprint(auc) # AUC\n\n0.8531468531468531\n0.8108108108108109\n0.8465909090909092\n\n\n\n# 참고사항\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_val, y_pred)\nprint(cm)\n\n# ####    예측\n# ####   0  1\n# 실제 0 TN FP\n# 실제 1 FN TP\n\n[[77 11]\n [10 45]]\n\n\n\n실제 test 셋으로 성능평가를 한다면?\n\n# 모델을 사용하여 테스트 데이터 예측\ny_pred_f = model.predict(x_test)\n# 모델 성능 평가 (정확도, F1 score, AUC)\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\nacc_f = accuracy_score(y_test, y_pred_f) # (실제값, 예측값)\nf1_f = f1_score(y_test, y_pred_f) # (실제값, 예측값)\nauc_f = roc_auc_score(y_test, y_pred_f) # (실제값,예측값)\n\n\nprint(acc_f) # 정확도(Accuracy)\nprint(f1_f) #f1 score\nprint(auc_f) #AUC\n\n0.7821229050279329\n0.7153284671532847\n0.7687088274044797"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_5.html#예측값-제출-1",
    "href": "BigData_Analysis/실기_2유형_5.html#예측값-제출-1",
    "title": "빅분기 실기 - 2유형 문제풀이(5)",
    "section": "✅ 5. 예측값 제출",
    "text": "✅ 5. 예측값 제출\n\n(주의) x_test 를 모델에 넣어 나온 예측값을 제출해야 함\n\n#(실기시험 안내사항)\n# 아래 코드 예측변수와 수험번호를 개인별로 변경하여 활용\n# pd.DataFrame({'cust_id':cust_id, 'target':y_result}).to_csv('0030000.csv', index=False)\n\n# 모델을 사용하여 테스트 데이터 예측\n\n# 1. 특정 클래스로 분류할 경우 (predict)\ny_result = model.predict(x_test)\nprint(y_result[:5])\n\n# 2. 특정 클래스로 분류될 확률을 구할 경우 (predict_proba)\ny_result_prob = model.predict_proba(x_test)\nprint(y_result_prob[:5])\n\n# 이해해보기\nresult_prob = pd.DataFrame({\n    'result' : y_result,\n    'prob_0' : y_result_prob[:,0]\n})\n# setosa 일 확률 : y_result_prob[:, 0]\n# 그 외 종일 확률 : y_result_prob[:, 1]\n\nprint(result_prob[:5])\n\n[1 1 0 0 0]\n[[0.32 0.68]\n [0.24 0.76]\n [1.   0.  ]\n [0.93 0.07]\n [0.93 0.07]]\n   result  prob_0\n0       1    0.32\n1       1    0.24\n2       0    1.00\n3       0    0.93\n4       0    0.93\n\n\n\n# ★tip : 데이터를 저장한 다음 불러와서 제대로 제출했는지 확인해보자\n# pd.DataFrame({'result':y_result}).to_csv('수험번호.csv', index=False)\n# df2 = pd.read_csv(\"수험번호.csv\")\n# print(df2.head())"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html",
    "href": "BigData_Analysis/실기_2유형_3.html",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "",
    "text": "실기 2 유형(3)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html#데이터-분석-순서",
    "href": "BigData_Analysis/실기_2유형_3.html#데이터-분석-순서",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "✅ 데이터 분석 순서",
    "text": "✅ 데이터 분석 순서\n\n1. 라이브러리 및 데이터 확인\n\n\n2. 데이터 탐색(EDA)\n\n\n3. 데이터 전처리 및 분리\n\n\n4. 모델링 및 성능평가\n\n\n5. 예측값 제출"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html#라이브러리-및-데이터-확인-1",
    "href": "BigData_Analysis/실기_2유형_3.html#라이브러리-및-데이터-확인-1",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "✅ 1. 라이브러리 및 데이터 확인",
    "text": "✅ 1. 라이브러리 및 데이터 확인\n\nimport pandas as pd\nimport numpy as np\n\n\n###### 실기환경 복사 영역 ######\n# 실기 시험 데이터셋으로 셋팅하기 (수정금지)\n\nimport seaborn as sns\n# tips 데이터셋 로드\ndf = sns.load_dataset('tips')\n\nx = df.drop(['tip'], axis=1)\ny = df['tip']\n\n\n# 실시 시험 데이터셋으로 셋팅하기 (수정금지)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2,\n                                                   random_state = 2023)\n\nx_test = pd.DataFrame(x_test.reset_index())\nx_train = pd.DataFrame(x_train.reset_index())\ny_train = pd.DataFrame(y_train.reset_index())\n\nx_test.rename(columns = {'index':'cust_id'}, inplace=True)\nx_train.rename(columns = {'index':'cust_id'}, inplace=True)\ny_train.columns = ['cust_id', 'target']\n###### 실기환경 복사 영역 ######"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html#레스토랑의-예측-문제",
    "href": "BigData_Analysis/실기_2유형_3.html#레스토랑의-예측-문제",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "레스토랑의 예측 문제",
    "text": "레스토랑의 예측 문제"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html#데이터의-결측치-이상치-변수에-대해-처리하고",
    "href": "BigData_Analysis/실기_2유형_3.html#데이터의-결측치-이상치-변수에-대해-처리하고",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "- 데이터의 결측치, 이상치, 변수에 대해 처리하고",
    "text": "- 데이터의 결측치, 이상치, 변수에 대해 처리하고"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html#회귀모델을-사용하여-rsq-mse-값을-산출하시오",
    "href": "BigData_Analysis/실기_2유형_3.html#회귀모델을-사용하여-rsq-mse-값을-산출하시오",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "- 회귀모델을 사용하여 Rsq, MSE 값을 산출하시오",
    "text": "- 회귀모델을 사용하여 Rsq, MSE 값을 산출하시오"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html#데이터셋-설명",
    "href": "BigData_Analysis/실기_2유형_3.html#데이터셋-설명",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "데이터셋 설명",
    "text": "데이터셋 설명\n\n\ntotal_bill(): 손님의 식사 총 청구액\ntip(팁): 팁의 양\nsex(성별): 손님의 성별\nsmoker(흡연자): 손님의 흡연 여부(“Yes” 또는 “No”)\nday(요일): 식사가 이루어진 요일\ntime(시간): 점심 또는 저녁 중 언제 식사가 이루어졌는지\nsize(인원 수): 식사에 참석한 인원 수"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html#데이터-탐색eda-1",
    "href": "BigData_Analysis/실기_2유형_3.html#데이터-탐색eda-1",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "✅ 2. 데이터 탐색(EDA)",
    "text": "✅ 2. 데이터 탐색(EDA)\n\n# 데이터의 행/열 확인\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\n\n(195, 7)\n(49, 7)\n(195, 2)\n\n\n\n# 초기 데이터 확인\n\nprint(x_train.head(3))\nprint(x_test.head(3))\nprint(y_train.head(3))\n\n   cust_id  total_bill     sex smoker  day    time  size\n0      158       13.39  Female     No  Sun  Dinner     2\n1      186       20.90  Female    Yes  Sun  Dinner     3\n2       21       20.29  Female     No  Sat  Dinner     2\n   cust_id  total_bill     sex smoker  day    time  size\n0      154       19.77    Male     No  Sun  Dinner     4\n1        4       24.59  Female     No  Sun  Dinner     4\n2       30        9.55    Male     No  Sat  Dinner     2\n   cust_id  target\n0      158    2.61\n1      186    3.50\n2       21    2.75\n\n\n\n# 변수명과 데이터 타입이 매칭이 되는지, 결측치가 있는지 확인해보세요\n\nprint(x_train.info())\nprint(x_test.info())\nprint(y_train.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 195 entries, 0 to 194\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   cust_id     195 non-null    int64   \n 1   total_bill  195 non-null    float64 \n 2   sex         195 non-null    category\n 3   smoker      195 non-null    category\n 4   day         195 non-null    category\n 5   time        195 non-null    category\n 6   size        195 non-null    int64   \ndtypes: category(4), float64(1), int64(2)\nmemory usage: 6.0 KB\nNone\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 49 entries, 0 to 48\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   cust_id     49 non-null     int64   \n 1   total_bill  49 non-null     float64 \n 2   sex         49 non-null     category\n 3   smoker      49 non-null     category\n 4   day         49 non-null     category\n 5   time        49 non-null     category\n 6   size        49 non-null     int64   \ndtypes: category(4), float64(1), int64(2)\nmemory usage: 2.0 KB\nNone\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 195 entries, 0 to 194\nData columns (total 2 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   cust_id  195 non-null    int64  \n 1   target   195 non-null    float64\ndtypes: float64(1), int64(1)\nmemory usage: 3.2 KB\nNone\n\n\n\n# x_train 과 x_test 데이터의 기초통계량을 잘 비교해보세요\n\nprint(x_train.describe())\nprint(x_test.describe())\nprint(y_train.describe())\n\n          cust_id  total_bill        size\ncount  195.000000  195.000000  195.000000\nmean   122.056410   20.054667    2.543590\nstd     70.668034    8.961645    0.942631\nmin      0.000000    3.070000    1.000000\n25%     59.500000   13.420000    2.000000\n50%    121.000000   17.920000    2.000000\n75%    182.500000   24.395000    3.000000\nmax    243.000000   50.810000    6.000000\n          cust_id  total_bill       size\ncount   49.000000   49.000000  49.000000\nmean   119.285714   18.716531   2.673469\nstd     70.918674    8.669864   0.987162\nmin      2.000000    5.750000   2.000000\n25%     62.000000   12.740000   2.000000\n50%    123.000000   16.660000   2.000000\n75%    180.000000   21.010000   3.000000\nmax    239.000000   44.300000   6.000000\n          cust_id      target\ncount  195.000000  195.000000\nmean   122.056410    3.021692\nstd     70.668034    1.402690\nmin      0.000000    1.000000\n25%     59.500000    2.000000\n50%    121.000000    2.920000\n75%    182.500000    3.530000\nmax    243.000000   10.000000\n\n\n\n# object, category 데이터도 추가 확인\n# print(x_trian.describe(include='object'))\n# print(x_test.describe(include='object'))\n\nprint(x_train.describe(include='category'))\nprint(x_test.describe(include='category'))\n\n         sex smoker  day    time\ncount    195    195  195     195\nunique     2      2    4       2\ntop     Male     No  Sat  Dinner\nfreq     125    120   71     142\n         sex smoker  day    time\ncount     49     49   49      49\nunique     2      2    4       2\ntop     Male     No  Sat  Dinner\nfreq      32     31   16      34\n\n\n\n# y데이터도 구체적으로 살펴보세요\nprint(y_train.head())\n\n   cust_id  target\n0      158    2.61\n1      186    3.50\n2       21    2.75\n3       74    2.20\n4       43    1.32\n\n\n\n# y데이터도 구체적으로 살펴보세요\nprint(y_train.describe().T)\n\n         count        mean        std  min   25%     50%     75%    max\ncust_id  195.0  122.056410  70.668034  0.0  59.5  121.00  182.50  243.0\ntarget   195.0    3.021692   1.402690  1.0   2.0    2.92    3.53   10.0"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html#데이터-전처리-및-분리-1",
    "href": "BigData_Analysis/실기_2유형_3.html#데이터-전처리-및-분리-1",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "✅ 3. 데이터 전처리 및 분리",
    "text": "✅ 3. 데이터 전처리 및 분리\n\n1) 결측치, 2) 이상치, 3) 변수 처리하기\n\n# 결측치 확인\n\nprint(x_train.isnull().sum())\nprint(x_test.isnull().sum())\nprint(y_train.isnull().sum())\n\ncust_id       0\ntotal_bill    0\nsex           0\nsmoker        0\nday           0\ntime          0\nsize          0\ndtype: int64\ncust_id       0\ntotal_bill    0\nsex           0\nsmoker        0\nday           0\ntime          0\nsize          0\ndtype: int64\ncust_id    0\ntarget     0\ndtype: int64\n\n\n\n# 결측치 제거\n# df = df.dropna()\n# print(df)\n\n# 참고사항\n# print(df.dropna().shape) # 행 기준으로 삭제\n\n\n# 결측치 대체 (평균값, 중앙값, 최빈값)\n# 연속형 변수 :  중앙값, 평균값\n# df['변수명'].median()\n# df['변수명'].mean()\n\n# 범주형 변수 :  최빈값\n# df['변수명'].mode()\n\n\n# df['변수명'] = df['변수명'].fillna(대체할값)\n\n\n# 이상치 대체\n# (참고) df['변수명'] = np.where(df['변수명'] &gt;= 5, 대체할 값, df['변수명'])\n\n\n# 변수처리\n\n# 불필요한 변수 제거\n# df = df.drop(columns = ['변수1', '변수2'])\n# df = df.drop(['변수1', '변수2'], axis=1)\n\n# 필요시 변수 추가(파생변수 생성)\n# df['파생변수명'] = df['A'] * df['B'] (파생변수 생성 수식)\n\n# 원핫인코딩(가변수 처리)\n# x_train = pd.get_dummies(x_train)\n# x_test = pd.get_dummies(x_test)\n\n# print(x_train.info())\n# print(x_test.info())\n\n\n# 변수처리\n\n# 불필요한 변수(columns) 제거\n# cust_id 는 불필요한 변수이므로 제거합니다.\n# 단, test셋의 cust_id가 나중에 제출이 필요하기 떄문에 별도로 저장\n\ncust_id = x_test['cust_id'].copy()\n\n# 각 데이터에서 cust_id 변수 제거\nx_train = x_train.drop(columns = ['cust_id']) # drop(columns = ['변수1', '변수2'])\nx_test = x_test.drop(columns = ['cust_id'])\n\n\n# 변수처리(원핫인코딩)\nprint(x_train.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 195 entries, 0 to 194\nData columns (total 6 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   total_bill  195 non-null    float64 \n 1   sex         195 non-null    category\n 2   smoker      195 non-null    category\n 3   day         195 non-null    category\n 4   time        195 non-null    category\n 5   size        195 non-null    int64   \ndtypes: category(4), float64(1), int64(1)\nmemory usage: 4.5 KB\nNone\n\n\n\n# 변수처리(원핫인코딩)\nx_train = pd.get_dummies(x_train)\nx_test = pd.get_dummies(x_test)\n\nprint(x_train.info())\nprint(x_test.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 195 entries, 0 to 194\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   total_bill   195 non-null    float64\n 1   size         195 non-null    int64  \n 2   sex_Male     195 non-null    uint8  \n 3   sex_Female   195 non-null    uint8  \n 4   smoker_Yes   195 non-null    uint8  \n 5   smoker_No    195 non-null    uint8  \n 6   day_Thur     195 non-null    uint8  \n 7   day_Fri      195 non-null    uint8  \n 8   day_Sat      195 non-null    uint8  \n 9   day_Sun      195 non-null    uint8  \n 10  time_Lunch   195 non-null    uint8  \n 11  time_Dinner  195 non-null    uint8  \ndtypes: float64(1), int64(1), uint8(10)\nmemory usage: 5.1 KB\nNone\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 49 entries, 0 to 48\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   total_bill   49 non-null     float64\n 1   size         49 non-null     int64  \n 2   sex_Male     49 non-null     uint8  \n 3   sex_Female   49 non-null     uint8  \n 4   smoker_Yes   49 non-null     uint8  \n 5   smoker_No    49 non-null     uint8  \n 6   day_Thur     49 non-null     uint8  \n 7   day_Fri      49 non-null     uint8  \n 8   day_Sat      49 non-null     uint8  \n 9   day_Sun      49 non-null     uint8  \n 10  time_Lunch   49 non-null     uint8  \n 11  time_Dinner  49 non-null     uint8  \ndtypes: float64(1), int64(1), uint8(10)\nmemory usage: 1.4 KB\nNone\n\n\n\n\n데이터 분리\n\n# 데이터를 훈련 세트와 검증용 세트로 분할 (80% 훈련, 20% 검증용)\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train['target'],\n                                                   test_size = 0.2,\n                                                   random_state = 23)\n# 분류 모델에서는 층화(starify)를 할 필요가 없다. 연속형인 경우에만 사용\n\nprint(x_train.shape)\nprint(x_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)\n\n(156, 12)\n(39, 12)\n(156,)\n(39,)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html#모델링-밑-성능평가",
    "href": "BigData_Analysis/실기_2유형_3.html#모델링-밑-성능평가",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "✅ 4. 모델링 밑 성능평가",
    "text": "✅ 4. 모델링 밑 성능평가\n\n# 랜덤포레스트 모델 사용 (참고: 분류모델은 RandomForestClassifier)\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(random_state=2023)\nmodel.fit(x_train, y_train)\n\nRandomForestRegressor(random_state=2023)\n\n\n\n# 모델을 사용하여 테스트 데이터 예측\ny_pred = model.predict(x_val)\n\n\n# 모델 성능 평가 (R-squared, MSE 등)\nfrom sklearn.metrics import r2_score, mean_squared_error\nr2 = r2_score(y_val, y_pred)            # (실제값, 예측값)\nmse = mean_squared_error(y_val, y_pred) # (실제값, 예측값)\n\n\n# MSE\nprint(mse)\n\n0.9812277338461534\n\n\n\n# RMSE\nrmse = mse**0.5\nprint(rmse)\n\n0.990569398803614\n\n\n\n# R2 score(R-squared)\nprint(r2)\n\n0.4286497615634072"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html#예측값-제출-1",
    "href": "BigData_Analysis/실기_2유형_3.html#예측값-제출-1",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "✅ 5. 예측값 제출",
    "text": "✅ 5. 예측값 제출\n\n(주의) x_test 를 모델에 넣어 나온 예측값을 제출해야함\n\n#(실기시험 안내사항)\n# 아래 코드 예측변수와 수험번호를 개인별로 변경하여 활용\n# pd.DataFrame({'cust_id':cust_id, 'target':y_result}).to_csv('0030000.csv', index=False)\n\n# 모델을 사용하여 테스트 데이터 예측\ny_result = model.predict(x_test)\nresult = pd.DataFrame({'cust_id':cust_id, 'target':y_result})\nprint(result[:5])\n\n   cust_id  target\n0      154  3.2266\n1        4  4.1160\n2       30  1.8966\n3       75  1.8735\n4       33  3.0267\n\n\n\n# ★tip : 데이터를 저장한 다음 불러와서 제대로 제출했는지 확인해보자\n# pd.DataFrame({'result':y_result}).to_csv('수험번호.csv', index=False)\n# df2 = pd.read_csv(\"수험번호.csv\")\n# print(df2.head())"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_1.html",
    "href": "BigData_Analysis/실기_2유형_1.html",
    "title": "빅분기 실기 - 2유형 문제풀이(1)",
    "section": "",
    "text": "실기 2 유형(1)\n# !pip install IPython\nfrom IPython.display import Image\nImage('실기_2(1).png')\n지도학습만 현재까지 나옴\nImage('실기_2(2).png')"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_1.html#제2유형_연습하기_와인종류분류",
    "href": "BigData_Analysis/실기_2유형_1.html#제2유형_연습하기_와인종류분류",
    "title": "빅분기 실기 - 2유형 문제풀이(1)",
    "section": "제2유형_연습하기_와인종류분류",
    "text": "제2유형_연습하기_와인종류분류"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_1.html#데이터-분석-분서",
    "href": "BigData_Analysis/실기_2유형_1.html#데이터-분석-분서",
    "title": "빅분기 실기 - 2유형 문제풀이(1)",
    "section": "✅ 데이터 분석 분서",
    "text": "✅ 데이터 분석 분서\n\n1. 라이브러리 및 데이터 확인\n\n\n2. 데이터 탐색(EDA)\n\n\n3. 데이터 전처리 및 분리\n\n\n4. 모델링 및 성능평가\n\n\n5. 예측값 제출"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_1.html#라이브러리-및-데이터-확인-1",
    "href": "BigData_Analysis/실기_2유형_1.html#라이브러리-및-데이터-확인-1",
    "title": "빅분기 실기 - 2유형 문제풀이(1)",
    "section": "✅ 1. 라이브러리 및 데이터 확인",
    "text": "✅ 1. 라이브러리 및 데이터 확인\n\nimport pandas as pd\nimport numpy as np\n\n\n####### 실기환경 복사 영역 #######\nimport pandas as pd\nimport numpy as np\n# 실기 시험 데이터셋으로 세팅하기 (수정금지)\n\nfrom sklearn.datasets import load_wine\n# 와인 데이터셋을 로드합니다\nwine = load_wine()\nx = pd.DataFrame(wine.data, columns=wine.feature_names)\ny = pd.DataFrame(wine.target)\n\n# 실기 시험 데이터셋으로 세팅하기 (수정금지)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2,\n                                                   stratify=y,\n                                                   random_state=2023)\nx_test = pd.DataFrame(x_test)\nx_train = pd.DataFrame(x_train)\ny_train = pd.DataFrame(y_train)\n\nx_test.reset_index()\ny_train.columns = ['target']\n####### 실기환경 복사 영역 #######\n\n🍷 와인의 종류를 분류해보자\n\n데이터의 결측치, 이상치에 대해 처리하고\n분류모델을 사용하여 정확도, F1 score, AUC 값을 산출하시오\n제출은 result 변수에 담아 양식에 맞게 제출하시오\n\n\n# 데이터 설명\n# print(wine.DESCR)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_1.html#데이터-탐색eda-1",
    "href": "BigData_Analysis/실기_2유형_1.html#데이터-탐색eda-1",
    "title": "빅분기 실기 - 2유형 문제풀이(1)",
    "section": "✅ 2. 데이터 탐색(EDA)",
    "text": "✅ 2. 데이터 탐색(EDA)\n\n# 데이터 행/열 확인\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\n\n(142, 13)\n(36, 13)\n(142, 1)\n\n\n\n# 초기 데이터 확인\nprint(x_train.head(3))\nprint(x_test.head(3))\nprint(y_train.head(3))\n\n     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n52     13.82        1.75  2.42               14.0      111.0           3.88   \n146    13.88        5.04  2.23               20.0       80.0           0.98   \n44     13.05        1.77  2.10               17.0      107.0           3.00   \n\n     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n52         3.74                  0.32             1.87             7.05  1.01   \n146        0.34                  0.40             0.68             4.90  0.58   \n44         3.00                  0.28             2.03             5.04  0.88   \n\n     od280/od315_of_diluted_wines  proline  \n52                           3.26   1190.0  \n146                          1.33    415.0  \n44                           3.35    885.0  \n     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n168    13.58        2.58  2.69               24.5      105.0           1.55   \n144    12.25        3.88  2.20               18.5      112.0           1.38   \n151    12.79        2.67  2.48               22.0      112.0           1.48   \n\n     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n168        0.84                  0.39             1.54             8.66  0.74   \n144        0.78                  0.29             1.14             8.21  0.65   \n151        1.36                  0.24             1.26            10.80  0.48   \n\n     od280/od315_of_diluted_wines  proline  \n168                          1.80    750.0  \n144                          2.00    855.0  \n151                          1.47    480.0  \n     target\n52        0\n146       2\n44        0\n\n\n\n# 변수명과 데이터 타입이 매칭이 되는지, 결측치가 있는지 확인\nprint(x_train.info())\nprint(x_test.info())\nprint(y_train.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 142 entries, 52 to 115\nData columns (total 13 columns):\n #   Column                        Non-Null Count  Dtype  \n---  ------                        --------------  -----  \n 0   alcohol                       142 non-null    float64\n 1   malic_acid                    142 non-null    float64\n 2   ash                           142 non-null    float64\n 3   alcalinity_of_ash             142 non-null    float64\n 4   magnesium                     142 non-null    float64\n 5   total_phenols                 142 non-null    float64\n 6   flavanoids                    142 non-null    float64\n 7   nonflavanoid_phenols          142 non-null    float64\n 8   proanthocyanins               142 non-null    float64\n 9   color_intensity               142 non-null    float64\n 10  hue                           142 non-null    float64\n 11  od280/od315_of_diluted_wines  142 non-null    float64\n 12  proline                       142 non-null    float64\ndtypes: float64(13)\nmemory usage: 15.5 KB\nNone\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 36 entries, 168 to 55\nData columns (total 13 columns):\n #   Column                        Non-Null Count  Dtype  \n---  ------                        --------------  -----  \n 0   alcohol                       36 non-null     float64\n 1   malic_acid                    36 non-null     float64\n 2   ash                           36 non-null     float64\n 3   alcalinity_of_ash             36 non-null     float64\n 4   magnesium                     36 non-null     float64\n 5   total_phenols                 36 non-null     float64\n 6   flavanoids                    36 non-null     float64\n 7   nonflavanoid_phenols          36 non-null     float64\n 8   proanthocyanins               36 non-null     float64\n 9   color_intensity               36 non-null     float64\n 10  hue                           36 non-null     float64\n 11  od280/od315_of_diluted_wines  36 non-null     float64\n 12  proline                       36 non-null     float64\ndtypes: float64(13)\nmemory usage: 3.9 KB\nNone\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 142 entries, 52 to 115\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype\n---  ------  --------------  -----\n 0   target  142 non-null    int32\ndtypes: int32(1)\nmemory usage: 1.7 KB\nNone\n\n\n\n# x_train과 x_test 데이터의 기초통계량을 잘 비교해보세요\nprint(x_train.describe())\nprint(x_test.describe())\nprint(y_train.describe())\n\n          alcohol  malic_acid         ash  alcalinity_of_ash   magnesium  \\\ncount  142.000000  142.000000  142.000000         142.000000  142.000000   \nmean    13.025915    2.354296    2.340211          19.354225   98.732394   \nstd      0.812423    1.142722    0.279910           3.476825   13.581859   \nmin     11.030000    0.740000    1.360000          10.600000   70.000000   \n25%     12.370000    1.610000    2.190000          16.800000   88.000000   \n50%     13.050000    1.820000    2.320000          19.300000   97.000000   \n75%     13.685000    3.115000    2.510000          21.500000  106.750000   \nmax     14.830000    5.800000    3.230000          30.000000  151.000000   \n\n       total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\ncount     142.000000  142.000000            142.000000       142.000000   \nmean        2.303592    2.043592              0.361479         1.575070   \nstd         0.633955    1.033597              0.124627         0.576798   \nmin         0.980000    0.340000              0.140000         0.410000   \n25%         1.757500    1.227500              0.270000         1.242500   \n50%         2.335000    2.100000              0.325000         1.555000   \n75%         2.800000    2.917500              0.437500         1.950000   \nmax         3.880000    5.080000              0.630000         3.580000   \n\n       color_intensity         hue  od280/od315_of_diluted_wines      proline  \ncount       142.000000  142.000000                    142.000000   142.000000  \nmean          5.005070    0.950394                      2.603592   742.112676  \nstd           2.150186    0.220736                      0.709751   317.057395  \nmin           1.280000    0.540000                      1.270000   290.000000  \n25%           3.300000    0.782500                      1.922500   496.250000  \n50%           4.850000    0.960000                      2.780000   660.000000  \n75%           6.122500    1.097500                      3.170000   981.250000  \nmax          13.000000    1.710000                      3.920000  1680.000000  \n         alcohol  malic_acid        ash  alcalinity_of_ash   magnesium  \\\ncount  36.000000   36.000000  36.000000           36.00000   36.000000   \nmean   12.900833    2.265556   2.470278           20.05000  103.722222   \nstd     0.813112    1.021943   0.226066            2.70275   16.371772   \nmin    11.640000    0.890000   2.000000           14.60000   84.000000   \n25%    12.230000    1.592500   2.300000           18.00000   91.500000   \n50%    12.835000    1.885000   2.470000           19.50000  101.000000   \n75%    13.635000    2.792500   2.605000           21.70000  112.000000   \nmax    14.390000    4.950000   3.220000           26.50000  162.000000   \n\n       total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\ncount      36.000000   36.000000             36.000000        36.000000   \nmean        2.261667    1.972778              0.363333         1.653333   \nstd         0.600259    0.858882              0.125516         0.558012   \nmin         1.350000    0.660000              0.130000         0.840000   \n25%         1.715000    1.175000              0.267500         1.320000   \n50%         2.420000    2.175000              0.395000         1.550000   \n75%         2.602500    2.682500              0.435000         1.972500   \nmax         3.850000    3.490000              0.660000         3.280000   \n\n       color_intensity        hue  od280/od315_of_diluted_wines     proline  \ncount        36.000000  36.000000                     36.000000    36.00000  \nmean          5.267222   0.985278                      2.643611   765.75000  \nstd           2.915076   0.258694                      0.720100   309.94851  \nmin           2.080000   0.480000                      1.290000   278.00000  \n25%           2.875000   0.787500                      2.037500   542.50000  \n50%           4.325000   0.985000                      2.790000   682.50000  \n75%           6.900000   1.167500                      3.192500   996.25000  \nmax          11.750000   1.450000                      4.000000  1480.00000  \n           target\ncount  142.000000\nmean     0.936620\nstd      0.773816\nmin      0.000000\n25%      0.000000\n50%      1.000000\n75%      2.000000\nmax      2.000000\n\n\n\n# y 데이터도 구체적으로 살펴볼 필요있음\nprint(y_train.head())\n\nprint(y_train.value_counts())\n\n     target\n52        0\n146       2\n44        0\n67        1\n43        0\ntarget\n1         57\n0         47\n2         38\ndtype: int64"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_1.html#데이터-전처리-및-분리-1",
    "href": "BigData_Analysis/실기_2유형_1.html#데이터-전처리-및-분리-1",
    "title": "빅분기 실기 - 2유형 문제풀이(1)",
    "section": "✅ 3. 데이터 전처리 및 분리",
    "text": "✅ 3. 데이터 전처리 및 분리\n\n1) 결측치, 2) 이상치, 3) 변수 처리하기\n\n# 결측치 확인\nprint(x_train.isnull().sum())\nprint(x_test.isnull().sum())\nprint(y_train.isnull().sum())\n\nalcohol                         0\nmalic_acid                      0\nash                             0\nalcalinity_of_ash               0\nmagnesium                       0\ntotal_phenols                   0\nflavanoids                      0\nnonflavanoid_phenols            0\nproanthocyanins                 0\ncolor_intensity                 0\nhue                             0\nod280/od315_of_diluted_wines    0\nproline                         0\ndtype: int64\nalcohol                         0\nmalic_acid                      0\nash                             0\nalcalinity_of_ash               0\nmagnesium                       0\ntotal_phenols                   0\nflavanoids                      0\nnonflavanoid_phenols            0\nproanthocyanins                 0\ncolor_intensity                 0\nhue                             0\nod280/od315_of_diluted_wines    0\nproline                         0\ndtype: int64\ntarget    0\ndtype: int64\n\n\n\n# 결측치 제거\n# df = df.dropna()\n# print(df)\n\n# 참고사항\n# print(df.dropna().shape) # 행기준으로 삭제\n\n\n# 결측치 대체(평균값, 중앙값, 최빈값)\n# 연속형 변수 : 중앙값, 평균값\n#     - df['변수명'].median()\n#     - df['변수명'].mean()\n# 범주형 변수 : 최빈값\n\n# df['변수명'] = df['변수명'].fillna(대체할 값)\n\n\n# 이상치 대체(예시)\n# df['변수명'] = np.where(df['변수명']&gt;=5, 대체할 값, df['변수명'])\n\n\n# 변수처리\n\n# 불필요한 변수 제거\n# df = df.drop(columns = ['변수1', '변수2'])\n# df = df.drop(['변수1', '변수2'], axis=1)\n\n# 필요시 변수 추가(파생변수 생성)\n# df['파생변수명'] = df['A'] * df['B'] (파생변수 생성 수식)\n\n# 원핫인코딩(가변수 처리)\n# x_train = pd.get_dummies(x_train)\n# x_test = pd.get_dummies(x_test)\n# print(x_train.info())\n# print(x_test.info())\n\n\n\n데이터 분리\n\n# 데이터를 훈련 세트와 검증용 세트로 분할 (80% 훈련, 20% 검증용)\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train['target'],\n                                                   test_size = 0.2,\n                                                   stratify = y_train['target'],\n                                                   random_state = 2023)\nprint(x_train.shape)\nprint(x_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)\n\n(113, 13)\n(29, 13)\n(113,)\n(29,)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_1.html#모델링-및-성능평가-1",
    "href": "BigData_Analysis/실기_2유형_1.html#모델링-및-성능평가-1",
    "title": "빅분기 실기 - 2유형 문제풀이(1)",
    "section": "✅ 4. 모델링 및 성능평가",
    "text": "✅ 4. 모델링 및 성능평가\n\n# 랜덤포레스트 모델 사용 (참고: 회귀모델은 RandomForestRegressor)\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nmodel.fit(x_train, y_train)\n\nRandomForestClassifier()\n\n\n\n# 모델을 사용하여 테스트 데이터 예측\ny_pred = model.predict(x_val)\n\n\n# 모델 성늘 평가 (정확도, F1 score, 민감도, 특이도 등)\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score\nacc = accuracy_score(y_val, y_pred)              # (실제값, 예측값)\nf1 = f1_score(y_val, y_pred, average = 'macro')  # (실제값, 예측값)\n# auc = roc_auc_score(y_val, y_pred)             # (실제값, 예측값)  * AUC는 이진분류\n\n\n# 정확도(Accuracy)\nprint(acc)\n\n1.0\n\n\n\n# macro f1 score\nprint(f1)\n\n1.0"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_1.html#예측값-제출-1",
    "href": "BigData_Analysis/실기_2유형_1.html#예측값-제출-1",
    "title": "빅분기 실기 - 2유형 문제풀이(1)",
    "section": "✅ 5. 예측값 제출",
    "text": "✅ 5. 예측값 제출\n\n(주의) test 셋을 모델에 넣어 나온 예측값을 제출해야함\n\n# (실기시험 안내사항)\n# 아래 코드 예측변수와 수험번호를 개인별로 변경하여 활용\n# pd.DataFrame({ 'result':y_result }).to_csv('수험번호_csv', index=False)\n\n# 모델을 사용하여 테스트 데이터 예측\n\n# 1. 특정 클래스로 분류할 경우 (predict)\ny_result = model.predict(x_test)\nprint(y_result[:5])\n\n# 2. 특정 클래스로 분류될 확률을 구할 경우 (predict_proba)\ny_result_prob = model.predict_proba(x_test)\nprint(y_result_prob[:5])\n\n# 이해해보기\nresult_prob = pd.DataFrame({\n    'result' : y_result,\n    'prob_0' : y_result_prob[:, 0],\n    'prob_1' : y_result_prob[:, 1],\n    'prob_2' : y_result_prob[:, 2],\n})\n# Class 0일 확률 : y_result_prob[:, 0]\n# Class 1일 확률 : y_result_prob[:, 1]\n# Class 2일 확률 : y_result_prob[:, 2]\n\nprint(result_prob[:5])\n\n[2 2 2 0 1]\n[[0.01 0.05 0.94]\n [0.13 0.17 0.7 ]\n [0.   0.08 0.92]\n [0.99 0.01 0.  ]\n [0.08 0.81 0.11]]\n   result  prob_0  prob_1  prob_2\n0       2    0.01    0.05    0.94\n1       2    0.13    0.17    0.70\n2       2    0.00    0.08    0.92\n3       0    0.99    0.01    0.00\n4       1    0.08    0.81    0.11"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "BigData_Analysis/실기_1유형.html",
    "href": "BigData_Analysis/실기_1유형.html",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "",
    "text": "실기 1 유형"
  },
  {
    "objectID": "BigData_Analysis/실기_1유형.html#데이터-다루기-유형",
    "href": "BigData_Analysis/실기_1유형.html#데이터-다루기-유형",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "✅ 데이터 다루기 유형",
    "text": "✅ 데이터 다루기 유형\n\n\n데이터 타입(object, int, float, bool 등)\n기초통계량(평균, 중앙값, 사분위수, IQR, 표준편차 등)\n데이터 인덱싱, 필터링, 정렬, 변경 등\n중복값, 결측치, 이상치 처리 (제거 or 대체)\n데이터 Scaling (데이터 표준화(z), 데이터 정규화(min-max))\n데이터 합치기\n날짜/시간 데이터, index 다루기"
  },
  {
    "objectID": "BigData_Analysis/실기_1유형.html#핵심문제-27개-110",
    "href": "BigData_Analysis/실기_1유형.html#핵심문제-27개-110",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "✅ 핵심문제 27개 (1~10)",
    "text": "✅ 핵심문제 27개 (1~10)\n\n# 데이터 불러오기\nimport pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"mtcars.txt\")\ndf.head()\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\nMazda RX4\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\nHornet 4 Drive\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n4\nHornet Sportabout\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n\n\n\n\n# 문제 1\n# mpg 변수의 제 1사분위수를 구하고 정수값으로 출력하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\nQ1 = df['mpg'].quantile(.25)\nprint(round(Q1))\n\n15\n\n\n\n# 문제 2\n# mpg 값이 19이상 21이하인 데이터의 수를 구하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이 1)\ncond1 = (df[(df['mpg']&gt;=19) & (df['mpg']&lt;=21)])\nprint(len(cond1))\n\n# (풀이 2)\n# cond1 = (df['mpg']&gt;=19)\n# cond2 = (df['mpg']&lt;=21)\n# print(len(df[cond1&cond2]))\n\n5\n\n\n\n# 문제 3\n# hp 변수의 IQR 값을 구하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\nQ1 = df['hp'].quantile(.25)\nQ3 = df['hp'].quantile(.75)\nIQR = Q3 - Q1\nprint(IQR)\n\n83.5\n\n\n\n# 문제 4 \n# wt 변수의 상위 10개 값의 총합을 구하여 소수점을 버리고 정수로 출력하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\ntop10 = df.sort_values('wt', ascending=False)\nsum_top10 = sum(top10['wt'].head(10))\nprint(int(sum_top10))\n\n# top_10 = df.sort_values('wt',ascending=False).head(10)\n# print(int(sum(top_10['wt']))) #주의: 소수점 반올림이 아니라 버리는 문제\n\n42\n\n\n\n# 문제 5\n# 전체 자동차에서 cyl가 6인 비율이 얼마인지 소수점 첫째짜리까지 출력하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\ncond1 = len(df[df['cyl']==6])\ncond2 = len(df['cyl'])\nprint(round(cond1/cond2, 1))\n\n0.2\n\n\n\n# 문제 6\n# 첫번째 행부터 순서대로 10개 뽑은 후 mpg 열의 평균값을 반올림하여 정수로 출력하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\ncond1 = df.head(10) # df[:10], df.loc[0:9]\nmean_mpg = cond1['mpg'].mean()\nprint(round(mean_mpg))\n\n20\n\n\n\n# 문제 7\n# 첫번째 행부터 순서대로 50% 까지 데이터를 뽑아 wt 변수의 중앙값을 구하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\n# 50% 데이터에 해당하는 행의 수 (정수로 구하기)\np50 = int(len(df)*0.5)\ndf50 = df[:p50]\nprint(df50['wt'].median())\n\n# len(df)/2\n# cond1 = df[:17]\n# cond2 = cond1['wt'].median()\n# print(cond2)\n\n3.44\n\n\n\n# 문제 8\n# 결측값이 있는 데이터의 수를 출력하시오\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    '판매수' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n\n\n\n날짜\n제품\n판매수\n개당수익\n\n\n\n\n0\n20220103\nA\n3.0\n300\n\n\n1\n20220105\nB\nNaN\n400\n\n\n2\nNone\nNone\n5.0\n500\n\n\n3\n20230127\nB\n10.0\n600\n\n\n4\n20220203\nA\n10.0\n400\n\n\n\n\n\n\n\n\n# (풀이)\nm_value = df.isnull().sum()\nprint(m_value.sum())\n\n5\n\n\n\n# 문제 9 \n# '판매수' 컬럼의 결측값을 판매수의 중앙값으로 대체하고 판매수의 \n#  평균값을 정수로 출력하시오\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    '판매수' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\n\n\n# (풀이)\ndf[df['판매수'].isnull()]\nmedian = df['판매수'].median()\ndf['판매수'] = df['판매수'].fillna(median)\nmean = df['판매수'].mean()\nprint(int(mean))\n\n15\n\n\n\n# 문제 10\n# 판매수 컬럼에 결측치가 있는 행을 제거하고\n# 첫번째 행부터 순서대로 50%까지 데이터를 추출하여\n# 판매수 변수의 Q1(제1사분위수) 값을 정수로 출력하시오\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    '판매수' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\n\n\n# (풀이)\n# 결측치 삭제(행 기준)\ndf = df['판매수'].dropna()\n# 첫번째 행부터 순서대로 50%까지의 데이터 추출\nper50 = int(len(df)*0.5)\ndf = df[:per50]\n# 값구하기\nprint(round(df.quantile(.25)))\n\n# df[df['판매수'].isnull()]\n# df['판매수'] = df['판매수'].dropna()\n# int(len(df['판매수'])/2)\n# cond1 = df['판매수'].head(6)\n# Q1 = cond1.quantile(.25)\n# print(int(Q1))\n\n5"
  },
  {
    "objectID": "BigData_Analysis/실기_1유형.html#핵심문제-27개-1120",
    "href": "BigData_Analysis/실기_1유형.html#핵심문제-27개-1120",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "✅ 핵심문제 27개 (11~20)",
    "text": "✅ 핵심문제 27개 (11~20)\n\ndf = pd.read_csv('mtcars.txt')\ndf.head()\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\nMazda RX4\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\nHornet 4 Drive\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n4\nHornet Sportabout\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n\n\n\n\n# 문제 11\n# cyl가 4인 자동차와 6인 자동차 그룹의 mpg 평균값이 차이를 절대값으로 정수로 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\ncyl_4 = df[df['cyl']==4]\ncyl_4_mean = cyl_4['mpg'].mean()\n\ncyl_6 = df[df['cyl']==6]\ncyl_6_mean = cyl_6['mpg'].mean()\n\nprint(round(abs(cyl_4_mean - cyl_6_mean)))\n\n7\n\n\n\n# 문제 12\n# hp 변수에 대해 데이터표준화(Z-score)를 진행하고 이상치의 수를 구하시오\n# (단, 이상치는 Z값이 1.5를 초과하거나 -1.5미만인 값이다)\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\n# Z = (X-평균) / 표준편차\nstd = df['hp'].std()\nmean_hp = df['hp'].mean()\ndf['zscore'] = (df['hp']-mean_hp)/std\n\ncond1 = (df['zscore']&gt;1.5)\ncond2 = (df['zscore']&lt;-1.5)\nprint(len(df[cond1] + df[cond2]))\n\n2\n\n\n\n# 문제 13\n# mpg 컬럼을 최소최대 Scaling을 진행한 후 0.7보다 큰 값을 가지는 레코드 수를 구하라\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\nfrom sklearn.preprocessing import MinMaxScaler\nmscaler = MinMaxScaler()\ndf['mpg'] = mscaler.fit_transform(df[['mpg']])\nprint(len(df[df['mpg']&gt;0.7]))\n\n# 공식 : (x-min) / (max-min)\n# min_mpg = df['mpg'].min()\n# max_mpg = df['mpg'].max()\n# df['mpg'] = (df['mpg'] - min_mpg)/(max_mpg - min_mpg)\n# cond1 = (df['mpg']&gt;0.7)\n# print(len(df[cond1]))\n\n5\n\n\n\n# 문제 14\n# wt 컬럼에 대해 상자그림 기준으로 이상치의 개수를 구하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\n# 이상치 : Q1, Q3 로부터 1.5*IQR을 넘어가는 값\nQ1 = df['wt'].quantile(.25)\nQ3 = df['wt'].quantile(.75)\nIQR = Q3-Q1\n\nupper = Q3 + (1.5*IQR)\nlower = Q1 - (1.5*IQR)\n\ncond1 = (df['wt'] &gt; upper)\ncond2 = (df['wt'] &lt; lower)\n\nprint(len(df[cond1] + df[cond2]))\n\n3\n\n\n\n# 문제 15\n# 판매수 컬럼의 결측치를 최소값으로 대체하고\n# 결측치가 있을 때와 최소값으로 대체했을 때\n# 평균값의 차이를 절대값으로 정수로 출력하시오\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    '판매수' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf\n\n\n\n\n\n\n\n\n날짜\n제품\n판매수\n개당수익\n\n\n\n\n0\n20220103\nA\n3.0\n300\n\n\n1\n20220105\nB\nNaN\n400\n\n\n2\nNone\nNone\n5.0\n500\n\n\n3\n20230127\nB\n10.0\n600\n\n\n4\n20220203\nA\n10.0\n400\n\n\n5\n20220205\nNone\n10.0\n500\n\n\n6\n20230210\nA\n15.0\n500\n\n\n7\n20230223\nB\n15.0\n600\n\n\n8\n20230312\nA\n20.0\n600\n\n\n9\n20230422\nB\nNaN\n700\n\n\n10\n20220505\nA\n30.0\n600\n\n\n11\n20230511\nA\n40.0\n600\n\n\n\n\n\n\n\n\n# (풀이)\n# 데이터 복사\ndf2 = df.copy()\n# 최소값으로 결측치 대체\nmin = df['판매수'].min()\ndf2['판매수'] = df2['판매수'].fillna(min)\n\n# 결측치가 있을때, 대체했을때 평균\nm_yes = df['판매수'].mean()\nm_no = df2['판매수'].mean()\n\nprint(round(abs(m_yes - m_no)))\n\n2\n\n\n\n# 문제 16\n# vs변수가 0이 아닌 차량 중에 mpg 값이 가장 큰 차량의 hp 값을 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\ncond1 = df[df['vs']!=0]\n# mpg 값이 가장 큰 차량(내림차순)\ncond1 = cond1.sort_values('mpg',ascending=False)\n# cond1\nprint(cond1['hp'].iloc[0])\n\n65\n\n\n\n# 문제 17\n# gear 변수값이 3, 4인 두 그룹의 hp 표준편차값의 차이를 절대값으로 \n# 소수점 첫째자리까지 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\n# 두개 그룹으로 필터링\ncond1 = df[df['gear']==3]\ncond2 = df[df['gear']==4]\n# std 구하기\nstd3 = cond1['hp'].std()\nstd4 = cond2['hp'].std()\n\nprint(round(abs(std3 - std4),1))\n\n21.8\n\n\n\n# 문제 18\n# gear 변수의 갑별로 그룹화하여 mpg 평균값을 산출하고\n# 평균값이 높은 그룹의 mpg 제3사분위수 값을 구하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\ndf['gear'].unique()\ngear3 = df[df['gear']==3]\ngear4 = df[df['gear']==4]\ngear5 = df[df['gear']==5]\n\nm_3 = gear3['mpg'].mean()\nm_4 = gear4['mpg'].mean()\nm_5 = gear5['mpg'].mean()\n\n(m_3, m_4, m_5) # m_4의 평균값이 가장 큼\nQ3 = gear4['mpg'].quantile(.75)\nprint(Q3)\n\n28.075\n\n\n\n# (풀이_v2)\n# gear, mpg 변수만 필터링\ndf = df.loc[:, ['gear', 'mpg']]\n\n# gear 변수로 그룹화하여 mpg 평균값 보기\n# print(df.groupby('gear').mean()) # gear 4그룹이 제일 높음\n\n# gear=4인 그룹의 mpg Q3 구하기\ngear4 = df[df['gear']==4]\nprint(gear4['mpg'].quantile(.75))\n\n28.075\n\n\n\n# 문제 19\n# hp 항목의 상위 7번째 값으로 상위 7개 값을 변환한 후,\n# hp가 150 이상인 데이터를 추출하여 hp의 평균값을 반올림하여 정수로 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\n# hp 열 기준으로 내림차순 정렬\ndf = df.sort_values('hp', ascending=False)\n\n# 인덱스 초기화 - 내림차순으로 정렬시 최초의 인덱스로 있기에\ndf = df.reset_index(drop=True) # drop=True 는 기존 index 삭제\n# print(df)\n\n# hp 상위 7번째 값 확인\ntop7 = df['hp'].loc[6] # top7 = 205\n\n# np.where 활용\nimport numpy as np\ndf['hp'] = np.where(df['hp'] &gt;= 205, top7, df['hp'])\n# np.where(조건, 조건에 해당할 때 값, 그렇지 않을 때 값)\n\n# hp 150이상인 데이터\ncond1 = (df['hp']&gt;=150)\ndf = df[cond1]\nprint(round(df['hp'].mean()))\n\n187\n\n\n\n# 문제 20\n# car 변수에 Merc 무구가 포함된 자동차의 mpg 평균값을 정수로 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\ndf2 = df[df['car'].str.contains('Merc')] # 문자열 추출 알아두기\nprint(round(df2['mpg'].mean()))\n\n# 시험환경에서 답구하는 방법(reset_index() 사용 후)\n# 시험에서는 car가 index로 되어 있음\n# df.reset_index(inplace=True)\n# df2 = df[df['index'].str.contains(\"Merc\")] \n# print(round(df2['mpg'].mean()))\n\n19"
  },
  {
    "objectID": "BigData_Analysis/실기_1유형.html#핵심문제-27개-2127",
    "href": "BigData_Analysis/실기_1유형.html#핵심문제-27개-2127",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "✅ 핵심문제 27개 (21~27)",
    "text": "✅ 핵심문제 27개 (21~27)\n\n# 문제 21\n# 22년 1분기 A제품의 매출액을 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    '판매수' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n\n\n\n날짜\n제품\n판매수\n개당수익\n\n\n\n\n0\n20220103\nA\n3\n300\n\n\n1\n20220105\nB\n5\n400\n\n\n2\n20230105\nA\n5\n500\n\n\n3\n20230127\nB\n10\n600\n\n\n4\n20220203\nA\n10\n400\n\n\n\n\n\n\n\n\n# # (풀이)\n# cond1 = df[df['날짜']&lt;='20220431']\n# cond2 = cond1[cond1['제품']=='A']\n# cond2['매출액'] = (cond2['판매수']*cond2['개당수익'])\n# cond2\n\n# print(cond2['매출액'].sum())\n\n\n# (풀이_v2)\n# 데이터 타입 datetime으로 변경\ndf['날짜'] = pd.to_datetime(df['날짜'])\n\n# 년, 월, 일 변수(열) 추가하기\ndf['year'] = df['날짜'].dt.year\ndf['month'] = df['날짜'].dt.month\ndf['day'] = df['날짜'].dt.day\n\n# 매출액 변수 추가하기\ndf['매출액'] = df['판매수'] * df['개당수익']\n\n# 22년으로 필터링\ndf = df[df['year']==2022]\ndf.head()\n\n# 1,2,3월 매출액 계산\nm1 = df[df['month']==1]['매출액'].sum()\nm2 = df[df['month']==2]['매출액'].sum()\nm3 = df[df['month']==3]['매출액'].sum()\nprint(m1+m2+m3)\n\n11900\n\n\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['날짜'] = pd.to_datetime(df['날짜'])\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['year'] = df['날짜'].dt.year\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['month'] = df['날짜'].dt.month\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['day'] = df['날짜'].dt.day\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['매출액'] = df['판매수'] * df['개당수익']\n\n\n\n# 문제 22\n# 22년과 23년의 총 매출액 차이를 절대값으로 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    '판매수' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n\n\n\n날짜\n제품\n판매수\n개당수익\n\n\n\n\n0\n20220103\nA\n3\n300\n\n\n1\n20220105\nB\n5\n400\n\n\n2\n20230105\nA\n5\n500\n\n\n3\n20230127\nB\n10\n600\n\n\n4\n20220203\nA\n10\n400\n\n\n\n\n\n\n\n\n# (풀이)\n# 데이터 타입 datetime으로 변경\ndf['날짜'] = pd.to_datetime(df['날짜'])\n\n# 년 변수(열) 추가하기\ndf['year'] = df['날짜'].dt.year\n\n# 매출액 변수 추가하기\ndf['매출액'] = df['판매수'] * df['개당수익']\n\ncond22 = df[df['year']==2022]\ncond23 = df[df['year']==2023]\n\nprint(abs((cond22['매출액'].sum()) - (cond23['매출액'].sum())))\n\n48600\n\n\n\n# 문제 23\n# 23년 총 매출액이 큰 제품의 23년 판매수를 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    '판매수' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n\n\n\n날짜\n제품\n판매수\n개당수익\n\n\n\n\n0\n20220103\nA\n3\n300\n\n\n1\n20220105\nB\n5\n400\n\n\n2\n20230105\nA\n5\n500\n\n\n3\n20230127\nB\n10\n600\n\n\n4\n20220203\nA\n10\n400\n\n\n\n\n\n\n\n\n# 데이터 타입 datetime으로 변경\ndf['날짜'] = pd.to_datetime(df['날짜'])\n\n# 년 변수(열) 추가하기\ndf['year'] = df['날짜'].dt.year\n\n# 매출액 변수 추가하기\ndf['매출액'] = df['판매수'] * df['개당수익']\n\ndf = df[df['year']==2023]\n# df.head()\n\n# 23년 A 매출액과 B 매출액 별도로 구하기\n# A 매출액\ndf_a = df[df['제품']=='A']\na_sales = df_a['매출액'].sum()\n# print(a_sales) # 46000\n\n# B 매출액\ndf_b = df[df['제품']=='B']\nb_sales = df_b['매출액'].sum()\n# print(b_sales) # 32500\n\n# A 제품의 23년 판매수\na_sum = df_a['판매수'].sum()\nprint(a_sum)\n\n80\n\n\n\n# 문제 24\n# 매출액이 4천원 초과, 1만원 미만인 데이터 수를 출력하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    '판매수' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n\n\n\n날짜\n제품\n판매수\n개당수익\n\n\n\n\n0\n20220103\nA\n3\n300\n\n\n1\n20220105\nB\n5\n400\n\n\n2\n20230105\nA\n5\n500\n\n\n3\n20230127\nB\n10\n600\n\n\n4\n20220203\nA\n10\n400\n\n\n\n\n\n\n\n\n# (풀이)\ndf['매출액'] = df['판매수'] * df['개당수익']\ncond1 = df['매출액']&gt;4000\ncond2 = df['매출액']&lt;10000\n\ndf = df[cond1&cond2]\nprint(len(df))\n\n4\n\n\n\n# 문제 25\n# 23년 9월 24일 16:00~22:00 사이에 전체 제품의 판매수를 구하시오\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600]})\ntime = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf['time'] = time\ndf = df[['time', '물품', '판매수', '개당수익']]\ndf\n\n\n\n\n\n\n\n\ntime\n물품\n판매수\n개당수익\n\n\n\n\n0\n2023-09-24 12:25:00\nA\n5\n500\n\n\n1\n2023-09-24 16:48:25\nB\n10\n600\n\n\n2\n2023-09-24 21:11:50\nA\n15\n500\n\n\n3\n2023-09-25 01:35:15\nB\n15\n600\n\n\n4\n2023-09-25 05:58:40\nA\n20\n600\n\n\n5\n2023-09-25 10:22:05\nB\n25\n700\n\n\n6\n2023-09-25 14:45:30\nA\n40\n600\n\n\n\n\n\n\n\n\n# (풀이)\n# 컬럼과 인덱스에 둘다 time 변수를 놓고 풀이\n# 데이터 타입 datetime으로 변경 (항상 df.info()로 데이터 타입 확인할 것)\ndf['time'] = pd.to_datetime(df['time'])\n\n# index 새로 지정\ndf = df.set_index('time', drop=False) # default는 True\n\n# 9월 24일 필터링 // df['변수'].between( , )\ndf_after = df[df['time'].between('2023-09-24', '2023-09-25')] # 25일은 미포함\n\n# 시간 필터링 16:00 ~ 22:00 (주의: 시간이 index에 위치해야 함)\ndf = df_after.between_time(start_time='16:00', end_time='22:00') # 포함 기준\n\n# 전체 판매수 구하기\nprint(df['판매수'].sum())\n\n25\n\n\n\n# (풀이2) // 위에 df 새로 불러와서 실행해보기\n# 데이터 타입 datetime으로 변경\ndf['time'] = pd.to_datetime(df['time'])\n\n# index 새로 지정\ndf = df.set_index('time')\n\n# loc 함수로 필터링\ndf = df.loc[(df.index &gt;= '2023-09-24 16:00:00') & (df.index &lt;= '2023-09-24 22:00:00')]\n\n# 전체 판매수 구하기\nprint(df['판매수'].sum())\n\n25\n\n\n\n# 문제 26\n# 9월 25일 00:10~12:00까지의 B물품의 매출액 총합을 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600]})\ndf['time'] = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf = df[['time', '물품', '판매수', '개당수익']]\ndf = df.set_index('time',drop=True)\ndf\n\n\n\n\n\n\n\n\n물품\n판매수\n개당수익\n\n\ntime\n\n\n\n\n\n\n\n2023-09-24 12:25:00\nA\n5\n500\n\n\n2023-09-24 16:48:25\nB\n10\n600\n\n\n2023-09-24 21:11:50\nA\n15\n500\n\n\n2023-09-25 01:35:15\nB\n15\n600\n\n\n2023-09-25 05:58:40\nA\n20\n600\n\n\n2023-09-25 10:22:05\nB\n25\n700\n\n\n2023-09-25 14:45:30\nA\n40\n600\n\n\n\n\n\n\n\n\n# (풀이)\n# 매출액 변수 추가\ndf['매출액'] = df['판매수'] * df['개당수익']\n\n# loc 함수로 필터링\ndf = df.loc[(df.index &gt;= '2023-09-25 00:10:00') & (df.index &lt;= '2023-09-25 12:00:00')]\n\n# B 물품의 매출액 총합\ncond1 = df[df['물품']=='B']\nprint(cond1['매출액'].sum())\n\n26500\n\n\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\1645811142.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['매출액'] = df['판매수'] * df['개당수익']\n\n\n\n# 문제 27\n# 9월 24일 12:00~24:00까지의 A물품의 매출액 총합을 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600]})\ndf['time'] = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf = df[['time', '물품', '판매수', '개당수익']]\ndf = df.set_index('time',drop=True)\ndf\n\n\n\n\n\n\n\n\n물품\n판매수\n개당수익\n\n\ntime\n\n\n\n\n\n\n\n2023-09-24 12:25:00\nA\n5\n500\n\n\n2023-09-24 16:48:25\nB\n10\n600\n\n\n2023-09-24 21:11:50\nA\n15\n500\n\n\n2023-09-25 01:35:15\nB\n15\n600\n\n\n2023-09-25 05:58:40\nA\n20\n600\n\n\n2023-09-25 10:22:05\nB\n25\n700\n\n\n2023-09-25 14:45:30\nA\n40\n600\n\n\n\n\n\n\n\n\n# (풀이)\n# 매출액 변수 추가\ndf['매출액'] = df['판매수'] * df['개당수익']\n\n# loc 함수로 필터링\ndf = df.loc[(df.index &gt;= '2023-09-24 12:00:00') & (df.index &lt; '2023-09-25 00:00:00')]\n\n# A 물품의 매출액 총합\ncond1 = df[df['물품']=='A']\nprint(cond1['매출액'].sum())\n\n10000"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_2.html",
    "href": "BigData_Analysis/실기_2유형_2.html",
    "title": "빅분기 실기 - 2유형 문제풀이(2)",
    "section": "",
    "text": "실기 2 유형(2)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_2.html#데이터-분석-순서",
    "href": "BigData_Analysis/실기_2유형_2.html#데이터-분석-순서",
    "title": "빅분기 실기 - 2유형 문제풀이(2)",
    "section": "✅ 데이터 분석 순서",
    "text": "✅ 데이터 분석 순서\n\n1. 라이브러리 및 데이터 확인\n\n\n2. 데이터 탐색(EDA)\n\n\n3. 데이터 전처리 및 분리\n\n\n4. 모델링 및 성능평가\n\n\n5. 예측값 제출"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_2.html#라이브러리-및-데이터-확인-1",
    "href": "BigData_Analysis/실기_2유형_2.html#라이브러리-및-데이터-확인-1",
    "title": "빅분기 실기 - 2유형 문제풀이(2)",
    "section": "✅ 1. 라이브러리 및 데이터 확인",
    "text": "✅ 1. 라이브러리 및 데이터 확인\n\nimport pandas as pd\nimport numpy as np\n\n\n###### 실기환경 복사 영역 ######\nimport pandas as pd\nimport numpy as np\n# 실기 시험 데이터셋으로 셋팅하기 (수정금지)\nfrom sklearn.datasets import load_diabetes\n# diabetes 데이터셋 로드\ndiabetes = load_diabetes()\nx = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\ny = pd.DataFrame(diabetes.target)\n\n# 실시 시험 데이터셋으로 셋팅하기 (수정금지)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2,\n                                                   random_state = 2023)\n\nx_test = pd.DataFrame(x_test.reset_index())\nx_train = pd.DataFrame(x_train.reset_index())\ny_train = pd.DataFrame(y_train.reset_index())\n\nx_test.rename(columns = {'index':'cust_id'}, inplace=True)\nx_train.rename(columns = {'index':'cust_id'}, inplace=True)\ny_train.columns = ['cust_id', 'target']\n###### 실기환경 복사 영역 ######\n\n\n# from sklearn.datasets import load_diabetes\n# diabetes = load_diabetes()\n# x = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n# y = pd.DataFrame(diabetes.target)\n\n# from sklearn.model_selection import train_test_split\n# x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,\n#                                                 random_state=2023)\n\n# x_test = pd.DataFrame(x_test.reset_index())\n# x_train = pd.DataFrame(x_train.reset_index())\n# y_train = pd.DataFrame(y_train.reset_index())\n\n# x_test.rename(columns={'index':'cust_id'},inplace=True)\n# x_train.rename(columns={'index':'cust_id'},inplace=True)\n# y_train.columns=['cust']\n\n\n🏥 당뇨병 환자의 질병 진행정도를 예측해보자\n\n\n- 데이터의 결측치, 이상치, 변수들에 대해 전처리하고\n\n\n- 회귀모델을 사용하여 Rsq, MSE 값을 산출하시오\n\n\n- 제출은 cust_id, target 변수를 가진 dataframe 형태로 제출하시오\n\n# 데이터 설명\nprint(diabetes.DESCR)\n\n.. _diabetes_dataset:\n\nDiabetes dataset\n----------------\n\nTen baseline variables, age, sex, body mass index, average blood\npressure, and six blood serum measurements were obtained for each of n =\n442 diabetes patients, as well as the response of interest, a\nquantitative measure of disease progression one year after baseline.\n\n**Data Set Characteristics:**\n\n  :Number of Instances: 442\n\n  :Number of Attributes: First 10 columns are numeric predictive values\n\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n\n  :Attribute Information:\n      - age     age in years\n      - sex\n      - bmi     body mass index\n      - bp      average blood pressure\n      - s1      tc, total serum cholesterol\n      - s2      ldl, low-density lipoproteins\n      - s3      hdl, high-density lipoproteins\n      - s4      tch, total cholesterol / HDL\n      - s5      ltg, possibly log of serum triglycerides level\n      - s6      glu, blood sugar level\n\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n\nSource URL:\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n\nFor more information see:\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_2.html#데이터-탐색eda-1",
    "href": "BigData_Analysis/실기_2유형_2.html#데이터-탐색eda-1",
    "title": "빅분기 실기 - 2유형 문제풀이(2)",
    "section": "✅ 2. 데이터 탐색(EDA)",
    "text": "✅ 2. 데이터 탐색(EDA)\n\n# 데이터의 행/열 확인\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\n\n(353, 11)\n(89, 11)\n(353, 2)\n\n\n\n# 초기 데이터 확인\n\nprint(x_train.head(3))\nprint(x_test.head(3))\nprint(y_train.head(3))\n\n   cust_id       age       sex       bmi        bp        s1        s2  \\\n0        4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596   \n1      318  0.088931 -0.044642  0.006728  0.025315  0.030078  0.008707   \n2      301 -0.001882  0.050680 -0.024529  0.052858  0.027326  0.030001   \n\n         s3        s4        s5        s6  \n0  0.008142 -0.002592 -0.031991 -0.046641  \n1  0.063367 -0.039493  0.009436  0.032059  \n2  0.030232 -0.002592 -0.021394  0.036201  \n   cust_id       age       sex       bmi        bp        s1        s2  \\\n0      280  0.009016  0.050680  0.018584  0.039087  0.017694  0.010586   \n1      412  0.074401 -0.044642  0.085408  0.063187  0.014942  0.013091   \n2       68  0.038076  0.050680 -0.029918 -0.040099 -0.033216 -0.024174   \n\n         s3        s4        s5        s6  \n0  0.019187 -0.002592  0.016305 -0.017646  \n1  0.015505 -0.002592  0.006209  0.085907  \n2 -0.010266 -0.002592 -0.012908  0.003064  \n   cust_id  target\n0        4   135.0\n1      318   109.0\n2      301    65.0\n\n\n\n# 변수명과 데이터 타입이 매칭이 되는지, 결측치가 있는지 확인해보세요\n\nprint(x_train.info())\nprint(x_test.info())\nprint(y_train.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 353 entries, 0 to 352\nData columns (total 11 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   cust_id  353 non-null    int64  \n 1   age      353 non-null    float64\n 2   sex      353 non-null    float64\n 3   bmi      353 non-null    float64\n 4   bp       353 non-null    float64\n 5   s1       353 non-null    float64\n 6   s2       353 non-null    float64\n 7   s3       353 non-null    float64\n 8   s4       353 non-null    float64\n 9   s5       353 non-null    float64\n 10  s6       353 non-null    float64\ndtypes: float64(10), int64(1)\nmemory usage: 30.5 KB\nNone\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 89 entries, 0 to 88\nData columns (total 11 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   cust_id  89 non-null     int64  \n 1   age      89 non-null     float64\n 2   sex      89 non-null     float64\n 3   bmi      89 non-null     float64\n 4   bp       89 non-null     float64\n 5   s1       89 non-null     float64\n 6   s2       89 non-null     float64\n 7   s3       89 non-null     float64\n 8   s4       89 non-null     float64\n 9   s5       89 non-null     float64\n 10  s6       89 non-null     float64\ndtypes: float64(10), int64(1)\nmemory usage: 7.8 KB\nNone\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 353 entries, 0 to 352\nData columns (total 2 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   cust_id  353 non-null    int64  \n 1   target   353 non-null    float64\ndtypes: float64(1), int64(1)\nmemory usage: 5.6 KB\nNone\n\n\n\n# x_train 과 x_test 데이터의 기초통계량을 잘 비교해보세요\n\nprint(x_train.describe())\nprint(x_test.describe())\nprint(y_train.describe())\n\n          cust_id         age         sex         bmi          bp          s1  \\\ncount  353.000000  353.000000  353.000000  353.000000  353.000000  353.000000   \nmean   212.634561    0.000804    0.000724    0.000640   -0.000326    0.001179   \nstd    126.668903    0.047617    0.047673    0.048141    0.046585    0.047891   \nmin      0.000000   -0.107226   -0.044642   -0.084886   -0.112400   -0.126781   \n25%    105.000000   -0.038207   -0.044642   -0.035307   -0.033214   -0.033216   \n50%    210.000000    0.005383   -0.044642   -0.006206   -0.005671   -0.002945   \n75%    322.000000    0.038076    0.050680    0.030440    0.032201    0.027326   \nmax    441.000000    0.110727    0.050680    0.170555    0.125158    0.153914   \n\n               s2          s3          s4          s5          s6  \ncount  353.000000  353.000000  353.000000  353.000000  353.000000  \nmean     0.001110   -0.000452    0.000901    0.001446    0.000589  \nstd      0.048248    0.048600    0.048045    0.047160    0.048122  \nmin     -0.115613   -0.102307   -0.076395   -0.126097   -0.137767  \n25%     -0.029184   -0.039719   -0.039493   -0.033249   -0.034215  \n50%     -0.001314   -0.006584   -0.002592    0.000271    0.003064  \n75%      0.031567    0.030232    0.034309    0.033657    0.032059  \nmax      0.198788    0.181179    0.185234    0.133599    0.135612  \n          cust_id        age        sex        bmi         bp         s1  \\\ncount   89.000000  89.000000  89.000000  89.000000  89.000000  89.000000   \nmean   251.696629  -0.003188  -0.002871  -0.002537   0.001292  -0.004676   \nstd    127.901365   0.047761   0.047563   0.045665   0.051777   0.046493   \nmin      9.000000  -0.099961  -0.044642  -0.090275  -0.108957  -0.091006   \n25%    148.000000  -0.034575  -0.044642  -0.030996  -0.036656  -0.037344   \n50%    280.000000  -0.001882  -0.044642  -0.009439  -0.005671  -0.009825   \n75%    366.000000   0.030811   0.050680   0.034751   0.042530   0.031454   \nmax    436.000000   0.096197   0.050680   0.137143   0.132044   0.119515   \n\n              s2         s3         s4         s5         s6  \ncount  89.000000  89.000000  89.000000  89.000000  89.000000  \nmean   -0.004401   0.001792  -0.003575  -0.005737  -0.002334  \nstd     0.045030   0.043723   0.045980   0.049252   0.045757  \nmin    -0.089935  -0.080217  -0.076395  -0.104365  -0.129483  \n25%    -0.030437  -0.028674  -0.039493  -0.038459  -0.030072  \n50%    -0.014153  -0.002903  -0.002592  -0.014956  -0.005220  \n75%     0.020607   0.022869   0.003312   0.024053   0.019633  \nmax     0.130208   0.122273   0.141322   0.133599   0.135612  \n          cust_id      target\ncount  353.000000  353.000000\nmean   212.634561  152.943343\nstd    126.668903   75.324692\nmin      0.000000   37.000000\n25%    105.000000   90.000000\n50%    210.000000  141.000000\n75%    322.000000  208.000000\nmax    441.000000  346.000000\n\n\n\n# y 데이터도 구체적으로 살펴보세요\nprint(y_train.head())\n\n   cust_id  target\n0        4   135.0\n1      318   109.0\n2      301    65.0\n3      189    79.0\n4      288    80.0\n\n\n\n# y 데이터도 구체적으로 살펴보세요\nprint(y_train.describe())\n\n          cust_id      target\ncount  353.000000  353.000000\nmean   212.634561  152.943343\nstd    126.668903   75.324692\nmin      0.000000   37.000000\n25%    105.000000   90.000000\n50%    210.000000  141.000000\n75%    322.000000  208.000000\nmax    441.000000  346.000000"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_2.html#데이터-전처리-및-분리-1",
    "href": "BigData_Analysis/실기_2유형_2.html#데이터-전처리-및-분리-1",
    "title": "빅분기 실기 - 2유형 문제풀이(2)",
    "section": "✅ 3. 데이터 전처리 및 분리",
    "text": "✅ 3. 데이터 전처리 및 분리\n\n1) 결측치, 2) 이상치, 3) 변수처리하기\n\n# 결측치 확인\n\nprint(x_train.isnull().sum())\nprint(x_test.isnull().sum())\nprint(y_train.isnull().sum())\n\ncust_id    0\nage        0\nsex        0\nbmi        0\nbp         0\ns1         0\ns2         0\ns3         0\ns4         0\ns5         0\ns6         0\ndtype: int64\ncust_id    0\nage        0\nsex        0\nbmi        0\nbp         0\ns1         0\ns2         0\ns3         0\ns4         0\ns5         0\ns6         0\ndtype: int64\ncust_id    0\ntarget     0\ndtype: int64\n\n\n\n# 결측치 제거\n# df = df.dropna()\n# print(df)\n\n# 참고사항\n# print(df.dropna().shape) # 행 기준으로 삭제\n\n\n# 결측치 대체 (평균값, 중앙값, 최빈값)\n# 연속형 변수 :  중앙값, 평균값\n# df['변수명'].median()\n# df['변수명'].mean()\n\n# 범주형 변수 :  최빈값\n# df['변수명'].mode()\n\n\n# df['변수명'] = df['변수명'].fillna(대체할값)\n\n\n# 이상치 대체\n# (참고) df['변수명'] = np.where(df['변수명'] &gt;= 5, 대체할 값, df['변수명'])\n\n\n# 변수처리\n\n# 불필요한 변수 제거\n# df = df.drop(columns = ['변수1', '변수2'])\n# df = df.drop(['변수1', '변수2'], axis=1)\n\n# 필요시 변수 추가(파생변수 생성)\n# df['파생변수명'] = df['A'] * df['B'] (파생변수 생성 수식)\n\n# 원핫인코딩(가변수 처리)\n# x_train = pd.get_dummies(x_train)\n# x_test = pd.get_dummies(x_test)\n\n# print(x_train.info())\n# print(x_test.info())\n\n\n# 변수처리\n\n# 불필요한 변수(columns) 제거\n# cust_id 는 불필요한 변수이므로 제거합니다.\n# 단, test셋의 cust_id가 나중에 제출이 필요하기 떄문에 별도로 저장\n\ncust_id = x_test['cust_id'].copy()\n\n# 각 데이터에서 cust_id 변수 제거\nx_train = x_train.drop(columns = ['cust_id']) # drop(columns = ['변수1', '변수2'])\nx_test = x_test.drop(columns = ['cust_id'])\n\n\n\n데이터 분리\n\n# 데이터를 훈련 세트와 검증용 세트로 분할 (80% 훈련, 20% 검증용)\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train['target'],\n                                                   test_size = 0.2,\n                                                   random_state = 23)\n# 분류 모델에서는 층화(starify)를 할 필요가 없다. 연속형인 경우에만 사용\n\nprint(x_train.shape)\nprint(x_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)\n\n(282, 10)\n(71, 10)\n(282,)\n(71,)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_2.html#모델링-및-성능평가-1",
    "href": "BigData_Analysis/실기_2유형_2.html#모델링-및-성능평가-1",
    "title": "빅분기 실기 - 2유형 문제풀이(2)",
    "section": "✅ 4. 모델링 및 성능평가",
    "text": "✅ 4. 모델링 및 성능평가\n\n# 랜덤포레스 모델 사용 (참고 : 분류모델은 RandomForestClassifier)\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(random_state = 2023)\nmodel.fit(x_train, y_train)\n\nRandomForestRegressor(random_state=2023)\n\n\n\n# 모델을 사용하여 테스트 데이터 예측\ny_pred = model.predict(x_val)\n\n\n# 모델 성능 평가 (평균 제곱 오차 및 R-squared)\nfrom sklearn.metrics import mean_squared_error, r2_score\nmse = mean_squared_error(y_val, y_pred) # (실제값, 예측값)\nr2 = r2_score(y_val, y_pred) # (실제값, 예측값)\n\n\n# MSE(mean_squared_error, 평균 제곱 오차)\nprint(mse)\n\n2571.6276845070424\n\n\n\n# R2 score(R-squared)\nprint(r2)\n\n0.5235611874726152\n\n\n\n# RMSE (root mean squared error)\nrmse = mse ** 0.5\nprint(rmse)\n\n50.71121852713699"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_2.html#예측값-제출-1",
    "href": "BigData_Analysis/실기_2유형_2.html#예측값-제출-1",
    "title": "빅분기 실기 - 2유형 문제풀이(2)",
    "section": "✅ 5. 예측값 제출",
    "text": "✅ 5. 예측값 제출\n\n(주의) x_test 를 모델에 넣어 나온 예측값을 제출해야함\n\n#(실기시험 안내사항)\n# 아래 코드 예측변수와 수험번호를 개인별로 변경하여 활용\n# pd.DataFrame({'cust_id':cust_id, 'target':y_result}).to_csv('0030000.csv', index=False)\n\n# 모델을 사용하여 테스트 데이터 예측\ny_result = model.predict(x_test)\nresult = pd.DataFrame({'cust_id':cust_id, 'target':y_result})\nprint(result[:5])\n\n   cust_id  target\n0      280  186.51\n1      412  255.92\n2       68   77.97\n3      324  184.22\n4      101  111.14\n\n\n\n# ★tip : 데이터를 저장한 다음 불러와서 제대로 제출했는지 확인해보자\n# pd.DataFrame({'result':y_result}).to_csv('수험번호.csv', index=False)\n# df2 = pd.read_csv(\"수험번호.csv\")\n# print(df2.head())"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html",
    "href": "BigData_Analysis/실기_2유형_4.html",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "",
    "text": "실기 2 유형(4)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html#데이터-분석-순서",
    "href": "BigData_Analysis/실기_2유형_4.html#데이터-분석-순서",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "✅ 데이터 분석 순서",
    "text": "✅ 데이터 분석 순서\n\n1. 라이브러리 및 데이터 확인\n\n\n2. 데이터 탐색(EDA)\n\n\n3. 데이터 전처리 및 분리\n\n\n4. 모델링 및 성능평가\n\n\n5. 예측값 제출"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html#라이브러리-및-데이터-확인-1",
    "href": "BigData_Analysis/실기_2유형_4.html#라이브러리-및-데이터-확인-1",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "✅ 1. 라이브러리 및 데이터 확인",
    "text": "✅ 1. 라이브러리 및 데이터 확인\n\nimport pandas as pd\nimport numpy as np\n\n\n###############  실기환경 복사 영역  ###############\nimport pandas as pd\nimport numpy as np\n# 실기 시험 데이터셋으로 셋팅하기 (수정금지)\nfrom sklearn.datasets import load_iris\n# Iris 데이터셋을 로드\niris = load_iris()\nx = pd.DataFrame(iris.data, columns=['sepal_length', 'sepal_width',\n                                     'petal_length', 'petal_width'])\ny = iris.target   # 'setosa'=0, 'versicolor'=1, 'virginica'=2\ny = np.where(y&gt;0, 1, 0) # setosa 종은 0, 나머지 종은 1로 변경 // 이진분류\n\n# 실기 시험 데이터셋으로 셋팅하기 (수정금지)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, \n                                                    stratify=y,\n                                                    random_state=2023)\n\nx_test = pd.DataFrame(x_test)\nx_train = pd.DataFrame(x_train)\ny_train = pd.DataFrame(y_train)\ny_train.columns = ['species']\n\n# 결측치 삽입\nx_test['sepal_length'].iloc[0] = None  \nx_train['sepal_length'].iloc[0] = None\n# 이상치 삽입\nx_train['sepal_width'].iloc[0] = 150\n###############  실기환경 복사 영역  ###############"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html#붓꽃iris의-종species을-분류해보자",
    "href": "BigData_Analysis/실기_2유형_4.html#붓꽃iris의-종species을-분류해보자",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "붓꽃(iris)의 종(Species)을 분류해보자",
    "text": "붓꽃(iris)의 종(Species)을 분류해보자"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html#데이터의-결측치-이상치에-대해-처리하고",
    "href": "BigData_Analysis/실기_2유형_4.html#데이터의-결측치-이상치에-대해-처리하고",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "- 데이터의 결측치, 이상치에 대해 처리하고",
    "text": "- 데이터의 결측치, 이상치에 대해 처리하고"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html#분류모델을-사용하여-정확도-f1-score-auc-값을-산출하시오",
    "href": "BigData_Analysis/실기_2유형_4.html#분류모델을-사용하여-정확도-f1-score-auc-값을-산출하시오",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "- 분류모델을 사용하여 정확도, F1 score, AUC 값을 산출하시오",
    "text": "- 분류모델을 사용하여 정확도, F1 score, AUC 값을 산출하시오"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html#제출은-result-변수에-담아-양식에-맞게-제출하시오",
    "href": "BigData_Analysis/실기_2유형_4.html#제출은-result-변수에-담아-양식에-맞게-제출하시오",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "- 제출은 result 변수에 담아 양식에 맞게 제출하시오",
    "text": "- 제출은 result 변수에 담아 양식에 맞게 제출하시오\n\n# 데이터 설명\nprint(iris.DESCR)\n\n.. _iris_dataset:\n\nIris plants dataset\n--------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 150 (50 in each of three classes)\n    :Number of Attributes: 4 numeric, predictive attributes and the class\n    :Attribute Information:\n        - sepal length in cm\n        - sepal width in cm\n        - petal length in cm\n        - petal width in cm\n        - class:\n                - Iris-Setosa\n                - Iris-Versicolour\n                - Iris-Virginica\n                \n    :Summary Statistics:\n\n    ============== ==== ==== ======= ===== ====================\n                    Min  Max   Mean    SD   Class Correlation\n    ============== ==== ==== ======= ===== ====================\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n    ============== ==== ==== ======= ===== ====================\n\n    :Missing Attribute Values: None\n    :Class Distribution: 33.3% for each of 3 classes.\n    :Creator: R.A. Fisher\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n    :Date: July, 1988\n\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\nfrom Fisher's paper. Note that it's the same as in R, but not as in the UCI\nMachine Learning Repository, which has two wrong data points.\n\nThis is perhaps the best known database to be found in the\npattern recognition literature.  Fisher's paper is a classic in the field and\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\ndata set contains 3 classes of 50 instances each, where each class refers to a\ntype of iris plant.  One class is linearly separable from the other 2; the\nlatter are NOT linearly separable from each other.\n\n.. topic:: References\n\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n     Mathematical Statistics\" (John Wiley, NY, 1950).\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n     Structure and Classification Rule for Recognition in Partially Exposed\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n     on Information Theory, May 1972, 431-433.\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n     conceptual clustering system finds 3 classes in the data.\n   - Many, many more ..."
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html#데이터-탐색eda-1",
    "href": "BigData_Analysis/실기_2유형_4.html#데이터-탐색eda-1",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "✅ 2. 데이터 탐색(EDA)",
    "text": "✅ 2. 데이터 탐색(EDA)\n\n# 데이터의 행/열 확인\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\n\n(120, 4)\n(30, 4)\n(120, 1)\n\n\n\n# 초기 데이터 확인\n\nprint(x_train.head(3))\nprint(x_test.head(3))\nprint(y_train.head(3))\n\n    sepal_length  sepal_width  petal_length  petal_width\n2            NaN        150.0           1.3          0.2\n49           5.0          3.3           1.4          0.2\n66           5.6          3.0           4.5          1.5\n     sepal_length  sepal_width  petal_length  petal_width\n93            NaN          2.3           3.3          1.0\n69            5.6          2.5           3.9          1.1\n137           6.4          3.1           5.5          1.8\n   species\n0        0\n1        0\n2        1\n\n\n\n# 변수명과 데이터 타입이 매칭이 되는지, 결측치가 있는지 확인\n\nprint(x_train.info())\nprint(x_test.info())\nprint(y_train.info()) # 현재 train, test에 결측치를 하나씩 넣었기에 확인 가능\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 120 entries, 2 to 44\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   sepal_length  119 non-null    float64\n 1   sepal_width   120 non-null    float64\n 2   petal_length  120 non-null    float64\n 3   petal_width   120 non-null    float64\ndtypes: float64(4)\nmemory usage: 4.7 KB\nNone\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 30 entries, 93 to 55\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   sepal_length  29 non-null     float64\n 1   sepal_width   30 non-null     float64\n 2   petal_length  30 non-null     float64\n 3   petal_width   30 non-null     float64\ndtypes: float64(4)\nmemory usage: 1.2 KB\nNone\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 120 entries, 0 to 119\nData columns (total 1 columns):\n #   Column   Non-Null Count  Dtype\n---  ------   --------------  -----\n 0   species  120 non-null    int32\ndtypes: int32(1)\nmemory usage: 608.0 bytes\nNone\n\n\n\n# x_train 과 x_test 데이터의 기초통계량을 잘 비교해보세요\n\nprint(x_train.describe())\nprint(x_test.describe())\nprint(y_train.describe())\n\n       sepal_length  sepal_width  petal_length  petal_width\ncount    119.000000     120.0000    120.000000   120.000000\nmean       5.920168       4.2950      3.816667     1.226667\nstd        0.841667      13.4191      1.798848     0.780512\nmin        4.300000       2.2000      1.100000     0.100000\n25%        5.150000       2.8000      1.575000     0.300000\n50%        6.000000       3.0000      4.400000     1.350000\n75%        6.500000       3.4000      5.225000     1.800000\nmax        7.900000     150.0000      6.900000     2.500000\n       sepal_length  sepal_width  petal_length  petal_width\ncount     29.000000    30.000000     30.000000     30.00000\nmean       5.596552     3.000000      3.523333      1.09000\nstd        0.709367     0.522593      1.631518      0.68549\nmin        4.600000     2.000000      1.000000      0.10000\n25%        5.000000     2.625000      1.600000      0.35000\n50%        5.500000     3.000000      4.050000      1.15000\n75%        5.900000     3.300000      4.925000      1.57500\nmax        7.600000     4.200000      6.600000      2.30000\n          species\ncount  120.000000\nmean     0.666667\nstd      0.473381\nmin      0.000000\n25%      0.000000\n50%      1.000000\n75%      1.000000\nmax      1.000000\n\n\n\n# y 데이터도 구체적으로 살펴보세요\nprint(y_train.head())\n\n   species\n0        0\n1        0\n2        1\n3        1\n4        1\n\n\n\n# y 데이터도 구체적으로 살펴보세요\nprint(y_train.value_counts())\n\nspecies\n1          80\n0          40\ndtype: int64"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html#데이터-전처리-및-분리-1",
    "href": "BigData_Analysis/실기_2유형_4.html#데이터-전처리-및-분리-1",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "✅ 3. 데이터 전처리 및 분리",
    "text": "✅ 3. 데이터 전처리 및 분리\n\n1) 결측치, 2) 이상치, 3) 변수 처리하기\n\n# 결측치 확인\nprint(x_train.isnull().sum())\nprint(x_test.isnull().sum())\nprint(y_train.isnull().sum())\n\nsepal_length    1\nsepal_width     0\npetal_length    0\npetal_width     0\ndtype: int64\nsepal_length    1\nsepal_width     0\npetal_length    0\npetal_width     0\ndtype: int64\nspecies    0\ndtype: int64\n\n\n\n# 결측치 제거 // 데이터의 수가 많은 경우\n# df = df.dropna()\n# print(df)\n\n# 참고사항\n# print(df.dropna().shape) # 행 기준으로 삭제\n\n\n# 결측치 대체(평균값, 중앙값, 최빈값)\n\n# 연속형 변수 : 중앙값, 평균값\n#  - df['변수명'].median()\n#  - df['변수명'].mean()\n\n# 범수형 변수 :  최빈값\n\n# df['변수명'] = df['변수명'].fillna(대체할 값)\n\n\n# 결측치 대체(중앙값)\n# ** 주의사항 : train 데이터의 중앙값으로 test 데이터도 변경해줘야 함 **\n\nmedian = x_train['sepal_length'].median()\nx_train['sepal_length'] = x_train['sepal_length'].fillna(median)\nx_test['sepal_length'] = x_test['sepal_length'].fillna(median)\n\n\n# 이상치 확인\ncond1 = (x_train['sepal_width']&gt;=10)\nprint(len(x_train[cond1]))\n\n1\n\n\n\n# 이상치 대체\n# (참고) df['변수명'] = np.where( df['변수명'] &gt;= 5, 대체할 값, df['변수명'])\n\n# 예를 들어 'sepal_width' 값이 10이 넘으면 이상치라고 가정해본다명\n# 이상치를 제외한 Max 값을 구해서 대체해보자\ncond1 = (x_train['sepal_width'] &lt;= 10)\nmax_sw = x_train[cond1]['sepal_width'].max()\nprint(max_sw)\n\nx_train['sepal_width'] = np.where(x_train['sepal_width'] &gt;=10, max_sw,\n                                 x_train['sepal_width'])\nprint(x_train.describe())\n\n4.4\n       sepal_length  sepal_width  petal_length  petal_width\ncount    120.000000   120.000000    120.000000   120.000000\nmean       5.920833     3.081667      3.816667     1.226667\nstd        0.838155     0.429966      1.798848     0.780512\nmin        4.300000     2.200000      1.100000     0.100000\n25%        5.175000     2.800000      1.575000     0.300000\n50%        6.000000     3.000000      4.400000     1.350000\n75%        6.500000     3.400000      5.225000     1.800000\nmax        7.900000     4.400000      6.900000     2.500000\n\n\n\n# 변수처리\n\n# 불필요한 변수 제거\n# df = df.drop(columns = ['변수1', '변수2'])\n# df = df.drop(['변수1', '변수2'], axis = 1)\n\n# 필요시 변수 추가(파생변수 생성)\n# df['파생변수명'] = df['A'] * df['B'] (파생변수 생성 수식)\n\n# 원핫인코딩(가변수 처리)\n# x_train = pd.get_dummies(x_train)\n# x_test = pd.get_dummies(x_test)\n# print(x_train.info())\n# print(x_test.info())\n\n\n\n데이터 분리\n\n#  데이터를 훈련 세트와 검증용 세트로 분할 (80% 훈련, 20% 검증)\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_train,\n                                                  y_train['species'],\n                                                  test_size=0.2,\n                                                  stratify = y_train['species'],\n                                                  random_state = 2023)\nprint(x_train.shape)\nprint(x_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)\n\n(96, 4)\n(24, 4)\n(96,)\n(24,)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html#모델링-및-성능평가-1",
    "href": "BigData_Analysis/실기_2유형_4.html#모델링-및-성능평가-1",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "✅ 4. 모델링 및 성능평가",
    "text": "✅ 4. 모델링 및 성능평가\n\n# 랜덤포레스트 모델 사용 (참고 : 회귀모델은 RandomForestRegressor)\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nmodel.fit(x_train, y_train)\n\nRandomForestClassifier()\n\n\n\n# 모델을 사용하여 테스트 데이터 예측\ny_pred = model.predict(x_val)\n\n\n# 모델 성능 평가 (accuracy, f1 score, AUC 등)\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score\nacc = accuracy_score(y_val, y_pred)  # (실제값, 예측값)\nf1 = f1_score(y_val, y_pred)         # (실제값, 예측값)\n# 다중분류인 경우 f1  f1_score(y_val, y_pred, average = 'macro')\nauc = roc_auc_score(y_val, y_pred)   # (실제값, 예측값)\n\n\n# 정확도(Accuracy)\nprint(acc)\n\n1.0\n\n\n\n# F1 Score\nprint(f1)\n\n1.0\n\n\n\n# AUC\nprint(auc)\n\n1.0\n\n\n\n# 참고사항\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_val, y_pred)\nprint(cm)\n\n# #####  예측\n# #####  0  1\n# 실제 0 TN FP\n# 실제 1 FN TP\n\n[[ 8  0]\n [ 0 16]]"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html#예측값-제출-1",
    "href": "BigData_Analysis/실기_2유형_4.html#예측값-제출-1",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "✅ 5. 예측값 제출",
    "text": "✅ 5. 예측값 제출\n\n(주의) x_test 를 모델에 넣어 나온 예측값을 제출해야 함\n\n#(실기시험 안내사항)\n# 아래 코드 예측변수와 수험번호를 개인별로 변경하여 활용\n# pd.DataFrame({'cust_id':cust_id, 'target':y_result}).to_csv('0030000.csv', index=False)\n\n# 모델을 사용하여 테스트 데이터 예측\n\n# 1. 특정 클래스로 분류할 경우 (predict)\ny_result = model.predict(x_test)\nprint(y_result[:5])\n\n# 2. 특정 클래스로 분류될 확률을 구할 경우 (predict_proba)\ny_result_prob = model.predict_proba(x_test)\nprint(y_result_prob[:5])\n\n# 이해해보기\nresult_prob = pd.DataFrame({\n    'result' : y_result,\n    'prob_0' : y_result_prob[:,0]\n})\n# setosa 일 확률 : y_result_prob[:, 0]\n# 그 외 종일 확률 : y_result_prob[:, 1]\n\nprint(result_prob[:5])\n\n[1 1 1 0 1]\n[[0.   1.  ]\n [0.   1.  ]\n [0.   1.  ]\n [1.   0.  ]\n [0.04 0.96]]\n   result  prob_0\n0       1    0.00\n1       1    0.00\n2       1    0.00\n3       0    1.00\n4       1    0.04\n\n\n\n# ★tip : 데이터를 저장한 다음 불러와서 제대로 제출했는지 확인해보자\n# pd.DataFrame({'result':y_result}).to_csv('수험번호.csv', index=False)\n# df2 = pd.read_csv(\"수험번호.csv\")\n# print(df2.head())"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_1.html",
    "href": "BigData_Analysis/실기_3유형_1.html",
    "title": "빅분기 실기 - 3유형 문제풀이(1)",
    "section": "",
    "text": "실기 3 유형(1)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_1.html#검정방법",
    "href": "BigData_Analysis/실기_3유형_1.html#검정방법",
    "title": "빅분기 실기 - 3유형 문제풀이(1)",
    "section": "✅ 검정방법",
    "text": "✅ 검정방법\n\n1) (정규성o) 단일표본 t검정(1sample t-test)\n\n\n2) (정규성x) 윌콕슨 부호순위 검정"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_1.html#가설검정-순서중요",
    "href": "BigData_Analysis/실기_3유형_1.html#가설검정-순서중요",
    "title": "빅분기 실기 - 3유형 문제풀이(1)",
    "section": "✅ 가설검정 순서(중요!!)",
    "text": "✅ 가설검정 순서(중요!!)\n\n\n가설검정\n유의수준 확인\n정규성 검정\n검정실시(통계량, p-value 확인)\n귀무가설 기각여부 결정(채택/기각)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_1.html#데이터-불러오기",
    "href": "BigData_Analysis/실기_3유형_1.html#데이터-불러오기",
    "title": "빅분기 실기 - 3유형 문제풀이(1)",
    "section": "✅ 데이터 불러오기",
    "text": "✅ 데이터 불러오기\n\nimport pandas as pd\nimport numpy as np\n\n\n# 데이터 불러오기 mtcars\ndf = pd.read_csv('mtcars.txt')\ndf.head(3)\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\nMazda RX4\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n\n\n\n\n\n\n예제문제\n\n\n1. mtcars 데이터셋의 mpg열 데이터의 평균이 20과 같다고 할 수 있는지 검정하시오. (유의수준 5%)\n\nimport scipy.stats as stats\nfrom scipy.stats import shapiro\n\n\n# 1. 가설검정\n# H_0 : mpg 열의 평균이 20과 같다\n# H_1 : mpg 열의 평균이 20과 같지 않다\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3. 정규성 검정\n# H_0(귀무가설) : 정규분포를 따른다\n# H_1(대립가설) : 정규분포를 따르지 않는다\nstatistic, pvalue = stats.shapiro(df['mpg'])\nprint(round(statistic, 4), round(pvalue, 4))\n\nresult = stats.shapiro(df['mpg'])\nprint(result)\n\n0.9476 0.1229\nShapiroResult(statistic=0.9475648403167725, pvalue=0.1228824257850647)\n\n\n\np-value 값이 유의수준(0.05) 보다 크다. -&gt; 귀무가설(H_0) 채택\n(만약 정규분포를 따르지 않는다면 비모수 검정방법을 써야 함(윌콕슨의 부호순위 검정)\n\n\n# 4.1 (정규성 만족 o) t-검정 실시\nstatistic, pvalue = stats.ttest_1samp(df['mpg'], popmean=20,\n                                      alternative='two-sided') # default: two-sided\n                                      # H_1 : 왼쪽값이 오른쪽 값과 같지 않다\nprint(round(statistic, 4), round(pvalue, 4))\n# alternative (대립가설: H_1) 옵션 : 'two-sided', 'greater', 'less'\n\n0.0851 0.9328\n\n\n\n# 4.2 (정규성 만족 x) Wilcoxon 부호순위 검정\nstatistic, pvalue = stats.wilcoxon(df['mpg']-20,alternative='two-sided') \nprint(round(statistic, 4), round(pvalue, 4))\n\n249.0 0.7891\n\n\n\n# 5. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값이 0.05보다 크기 때문에(0.9328) 귀무가설을 채택한다\n# 즉, mpg 열의 평균이 20과 같다고 할 수 있다\n\n# 답 : 채택\n\n\n# 실제로 평균을 구해보면\ndf['mpg'].mean()\n\n20.090624999999996\n\n\n\n\n2. mtcars 데이터셋의 mpg 열 데이터의 평균이 17보다 크다고 할 수 있는지 검정하시오. (유의수준 5%)\n\n# 1. 가설검정\n# H_0 : mpg 열의 평균이 17보다 작거나 같다(mpg mean &lt;= 17)\n# H_1 : mpg 열의 평균이 17보다 크다(mpg mean &gt;17)\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3. 정규성 검정\n# H_0(귀무가설) : 정규분포를 따른다\n# H_1(대립가설) : 정규분포를 따르지 않는다\nstatistic, pvalue = stats.shapiro(df['mpg'])\nprint(round(statistic, 4), round(pvalue, 4))\n\n0.9476 0.1229\n\n\n\n# 4.1 (정규성 만족 o) t-검정 실시\nstatistic, pvalue = stats.ttest_1samp(df['mpg'], popmean=17,\n                                      alternative='greater') # alternative(대립가설)\n# H_1: 왼쪽값(df['mpg'].mean)이 오른쪽값(popmean)보다 크다\nprint(round(statistic, 4), round(pvalue, 4))\n\n2.9008 0.0034\n\n\n\n# 4.2 (정규성 만족 x) Wilcoxon 부호순위 검정\nstatistic, pvalue = stats.wilcoxon(df['mpg']-17,alternative='greater') \nprint(round(statistic, 4), round(pvalue, 4))\n\n395.5 0.0066\n\n\n\n# 5. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값이 0.05보다 작기 때문에(0.0034) 귀무가설을 기각한다(대립가설 채택)\n# 즉, mpg 열의 평균이 17보다 크다고 할 수 있다\n\n# 답 : 기각\n\n\n\n3. mtcars 데이터셋의 mpg 열 데이터의 평균이 17보다 작다고 할 수 있는지 검정하시오. (유의수준 5%)\n\n# 1. 가설검정\n# H_0 : mpg 열의 평균이 17보다 크거나 같다(mpg mean &gt;= 17)\n# H_1 : mpg 열의 평균이 17보다 작다(mpg mean &lt;17)\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3. 정규성 검정\nstatistic, pvalue = stats.shapiro(df['mpg'])\nprint(round(statistic, 4), round(pvalue, 4))\n\n0.9476 0.1229\n\n\n\n# 4.1 (정규성 만족 o) t-검정 실시\nstatistic, pvalue = stats.ttest_1samp(df['mpg'], popmean=17,\n                                      alternative='less') # alternative(대립가설)\n# H_1: 왼쪽값(df['mpg'].mean)이 오른쪽값(popmean)보다 작다\nprint(round(statistic, 4), round(pvalue, 4))\n\n2.9008 0.9966\n\n\n\n# 4.2 (정규성 만족 x) Wilcoxon 부호순위 검정\nstatistic, pvalue = stats.wilcoxon(df['mpg']-17,alternative='less') \nprint(round(statistic, 4), round(pvalue, 4))\n\n395.5 0.9938\n\n\n\n# 5. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값이 0.05보다 크기 때문에(0.9966) 귀무가설을 채택한다\n# 즉, mpg 열의 평균이 17보다 크거나 같다고 할 수 있다\n\n# 답 : 채택"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_3.html",
    "href": "BigData_Analysis/실기_3유형_3.html",
    "title": "빅분기 실기 - 3유형 문제풀이(3)",
    "section": "",
    "text": "실기 3 유형(3)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_3.html#검정방법",
    "href": "BigData_Analysis/실기_3유형_3.html#검정방법",
    "title": "빅분기 실기 - 3유형 문제풀이(3)",
    "section": "✅ 검정방법",
    "text": "✅ 검정방법"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_3.html#분산분석anova-a집단-vs-b집단-vs-c집단-vs",
    "href": "BigData_Analysis/실기_3유형_3.html#분산분석anova-a집단-vs-b집단-vs-c집단-vs",
    "title": "빅분기 실기 - 3유형 문제풀이(3)",
    "section": "1. 분산분석(ANOVA) : A집단 vs B집단 vs C집단 vs …",
    "text": "1. 분산분석(ANOVA) : A집단 vs B집단 vs C집단 vs …\n\n(정규성o) ANOVA 분석\n(정규성x) 크루스칼-왈리스 검정(kruskal-wallis test)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_3.html#가설검정-순서중요",
    "href": "BigData_Analysis/실기_3유형_3.html#가설검정-순서중요",
    "title": "빅분기 실기 - 3유형 문제풀이(3)",
    "section": "✅ 가설검정 순서(중요!!)",
    "text": "✅ 가설검정 순서(중요!!)\n\n\n가설검정\n유의수준 확인\n정규성 검정 (주의) 집단 모두 정규성을 따를 경우!\n등분산 검정\n검정실시(통계량, p-value 확인) (주의) 등분산여부 확인\n귀무가설 기각여부 결정(채택/기각)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_3.html#예제문제",
    "href": "BigData_Analysis/실기_3유형_3.html#예제문제",
    "title": "빅분기 실기 - 3유형 문제풀이(3)",
    "section": "✅ 예제문제",
    "text": "✅ 예제문제\n\n문제 1-1\n다음은 A, B, C 그룹 인원 성적 데이터이다.\n세 그룹의 성적 평균이 같다고 할 수 있는지 ANOVA 분석을 실시하시오.\n(유의수준 5%)\n\nA, B, C : 각 그룹 인원의 성적\nH0(귀무가설) : A(평균) = B(평균) = C(평균)\nH1(대립가설) : Not H0 (적어도 하나는 같지 않다)\n\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom scipy.stats import shapiro\n\n\n# 데이터 만들기\ndf = pd.DataFrame( {\n    'A': [120, 135, 122, 124, 135, 122, 145, 160, 155, 142, 144, 135, 167],\n    'B': [110, 132, 123, 119, 123, 115, 140, 162, 142, 138, 135, 142, 160],\n    'C': [130, 120, 115, 122, 133, 144, 122, 120, 110, 134, 125, 122, 122]})\nprint(df.head(3))\n\n     A    B    C\n0  120  110  130\n1  135  132  120\n2  122  123  115\n\n\n\n# 1. 가설검정\n# H0 : 세 그룹 성적의 평균값이 같다. (A(평균) = B(평균) = C(평균))\n# H1 : 세 그룹의 성적 평균값이 적어도 하나는 같지 않다. (not H0)\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3. 정규성 검정\nprint(stats.shapiro(df['A']))\nprint(stats.shapiro(df['B']))\nprint(stats.shapiro(df['C']))\n\n# statistic, pvalue = stats.shapiro(df['A'])\n# print(round(statistic,4), round(pvalue,4))\n\nShapiroResult(statistic=0.9314376711845398, pvalue=0.35585272312164307)\nShapiroResult(statistic=0.9498201012611389, pvalue=0.5955665707588196)\nShapiroResult(statistic=0.9396706223487854, pvalue=0.45265132188796997)\n\n\n\n세 집단 모두 p-value 값이 유의수준(0.05)보다 크다\n귀무가설(H0) 채택 =&gt; 정규분포를 따른다고 할 수 있다.\n만약 하나라도 정규분포를 따르지 않는다면 비모수 검정방법을 써야함\n(크루스칼-왈리스 검정)\n\n\n# 4. 등분산성 검정\n# H0(귀무가설) : 등분산 한다\n# H1(대립가설) : 등분산 하지 않는다\nprint(stats.bartlett(df['A'], df['B'], df['C']))\n\nBartlettResult(statistic=4.222248448848066, pvalue=0.12110174433684852)\n\n\n\np-value 값이 유의수준(0.05)보다 크다\n귀무가설(H0) 채택 =&gt; 등분산 한다고 할 수 있다.\n\n\n# 5.1 (정규성o, 등분산성 o) 분산분석(F_oneway)\nimport scipy.stats as stats\nstatistic, pvalue = stats.f_oneway(df['A'], df['B'], df['C'])\n# 주의 : 데이터가 각각 들어가야 함(밑에 예제와 비교해볼 것)\n\nprint(round(statistic,4), round(pvalue,4))\n\n3.6971 0.0346\n\n\n\n# 5.2 (정규성o, 등분산성 x) Welch_ANOVA 분석\n# import pingouin as pg     # pingouin 패키지 미지원\n# pg.welch_anova(dv = \"그룹변수명\", between=\"성적데이터\", data=데이터)\n# pg.welch_anova(df['A'], df['B'], df['C']) 형태로 분석불가\n\n\n# 5.3 (정규성x, 등분산성 x) 크루스 왈리스 검정\nimport scipy.stats as stats\nstatistic, pvalue = stats.kruskal(df['A'], df['B'], df['C'])\n\nprint(round(statistic,4), round(pvalue,4))\n\n6.897 0.0318\n\n\n\n# 6. 귀무가설 기각 여부 결정(채택/기각)\n# p-value 값(0.0346)이 0.05보다 작기 떄문에 귀무가설을 기각한다\n# 즉, A, B, C 그룹의 성적 평균이 같다고 할 수 없다.\n\n# 답 : 기각(H1)\n\n\n\n문제 1-2 데이터 형태가 다른 경우\n\n# 데이터 만들기\ndf2 = pd.DataFrame( {\n    '항목': ['A','A','A','A','A','A','A','A','A','A','A','A','A',\n           'B','B','B','B','B','B','B','B','B','B','B','B','B',\n           'C','C','C','C','C','C','C','C','C','C','C','C','C',],\n    'value': [120, 135, 122, 124, 135, 122, 145, 160, 155, 142, 144, 135, 167,\n             110, 132, 123, 119, 123, 115, 140, 162, 142, 138, 135, 142, 160,\n             130, 120, 115, 122, 133, 144, 122, 120, 110, 134, 125, 122, 122]\n    })\nprint(df2.head(3))\n\n  항목  value\n0  A    120\n1  A    135\n2  A    122\n\n\n\n# 각각 필터링해서 변수명에 저장하고 분석 진행\na = df2[ df2['항목']=='A' ]['value']\nb = df2[ df2['항목']=='B' ]['value']\nc = df2[ df2['항목']=='C' ]['value']\n\n\n# 분산분석(F_oneway)\nimport scipy.stats as stats\nstatistic, pvalue = stats.f_oneway(a, b, c)\nprint(round(statistic,4), round(pvalue,4))\n\n3.6971 0.0346"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_5.html",
    "href": "BigData_Analysis/실기_3유형_5.html",
    "title": "빅분기 실기 - 3유형 문제풀이(5)",
    "section": "",
    "text": "실기 3 유형(5)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_5.html#다중회귀분석",
    "href": "BigData_Analysis/실기_3유형_5.html#다중회귀분석",
    "title": "빅분기 실기 - 3유형 문제풀이(5)",
    "section": "✅ 다중회귀분석",
    "text": "✅ 다중회귀분석\n\nimport pandas as pd\nimport numpy as np\n\n\n당뇨병 환자의 질병 진행정도 데이터셋\n\n###############  실기환경 복사 영역  ###############\nimport pandas as pd\nimport numpy as np\n# 실기 시험 데이터셋으로 셋팅하기 (수정금지)\nfrom sklearn.datasets import load_diabetes\n# diabetes 데이터셋 로드\ndiabetes = load_diabetes()\nx = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\ny = pd.DataFrame(diabetes.target)\ny.columns = ['target'] \n###############  실기환경 복사 영역  ###############\n\n\n# 데이터 설명\nprint(diabetes.DESCR)\n\n.. _diabetes_dataset:\n\nDiabetes dataset\n----------------\n\nTen baseline variables, age, sex, body mass index, average blood\npressure, and six blood serum measurements were obtained for each of n =\n442 diabetes patients, as well as the response of interest, a\nquantitative measure of disease progression one year after baseline.\n\n**Data Set Characteristics:**\n\n  :Number of Instances: 442\n\n  :Number of Attributes: First 10 columns are numeric predictive values\n\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n\n  :Attribute Information:\n      - age     age in years\n      - sex\n      - bmi     body mass index\n      - bp      average blood pressure\n      - s1      tc, total serum cholesterol\n      - s2      ldl, low-density lipoproteins\n      - s3      hdl, high-density lipoproteins\n      - s4      tch, total cholesterol / HDL\n      - s5      ltg, possibly log of serum triglycerides level\n      - s6      glu, blood sugar level\n\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n\nSource URL:\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n\nFor more information see:\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n\n\n\n\n1. sklearn 라이브러리 활용\n\n# sklearn 라이브러리 활용\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\n\n# 독립변수와 종속변수 설정\nx = x[['age','sex','bmi']]\nprint(x.head())\nprint(y.head())\n\n        age       sex       bmi\n0  0.038076  0.050680  0.061696\n1 -0.001882 -0.044642 -0.051474\n2  0.085299  0.050680  0.044451\n3 -0.089063 -0.044642 -0.011595\n4  0.005383 -0.044642 -0.036385\n----------------------------------\n   target\n0   151.0\n1    75.0\n2   141.0\n3   206.0\n4   135.0\n\n\n\n회귀식 : y = b0 + b1x1 + b2x2 + b3x3\n(x1=age, x2=sex, x3=bmi)\n\n\n# 모델링 \nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(x, y)\n\nLinearRegression()\n\n\n\n# 회귀분석 관련 지표 출력\n\n# 1. Rsq(결정계수) : model.score(x,y)\nmodel.score(x, y)\nprint(round(model.score(x,y),2))\n\n0.35\n\n\n\n# 2. 회귀계수 출력 : model.coef_\nprint(np.round(model.coef_, 2))        # 전체 회귀계수\nprint(np.round(model.coef_[0,0], 2))   # x1의 회귀계수\nprint(np.round(model.coef_[0,1], 2))   # x2의 회귀계수\nprint(np.round(model.coef_[0,2], 2))   # x3의 회귀계수\n\n[[138.9  -36.14 926.91]]\n138.9\n-36.14\n926.91\n\n\n\n# 3. 회귀계수(절편) : model.intercept_\nprint(np.round(model.intercept_, 2))\n\n[152.13]\n\n\n\n회귀식 : y = b0 + b1x1 + b2x2 + b3x3\n(x1=age, x2=sex, x3=bmi)\n\n\n\n결과 : 152.13 + 138.9age -36.14sex + 926.91bmi"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_5.html#statsmodels-라이브러리-사용",
    "href": "BigData_Analysis/실기_3유형_5.html#statsmodels-라이브러리-사용",
    "title": "빅분기 실기 - 3유형 문제풀이(5)",
    "section": "2. statsmodels 라이브러리 사용",
    "text": "2. statsmodels 라이브러리 사용\n\n###############  실기환경 복사 영역  ###############\nimport pandas as pd\nimport numpy as np\n# 실기 시험 데이터셋으로 셋팅하기 (수정금지)\nfrom sklearn.datasets import load_diabetes\n# diabetes 데이터셋 로드\ndiabetes = load_diabetes()\nx = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\ny = pd.DataFrame(diabetes.target)\ny.columns = ['target'] \n###############  실기환경 복사 영역  ###############\n\n\n# statsmodel.formula 활용\nimport statsmodels.api as sm\n# 독립변수와 종속변수 설정\nx = x[['age','sex','bmi']]\ny = y[['target']]\nprint(x.head())\nprint(y.head())\n\n        age       sex       bmi\n0  0.038076  0.050680  0.061696\n1 -0.001882 -0.044642 -0.051474\n2  0.085299  0.050680  0.044451\n3 -0.089063 -0.044642 -0.011595\n4  0.005383 -0.044642 -0.036385\n   target\n0   151.0\n1    75.0\n2   141.0\n3   206.0\n4   135.0\n\n\n\n# 모델링\nimport statsmodels.api as sm\n\nx = sm.add_constant(x)        # 주의 : 상수항 추가해줘야 함\nmodel = sm.OLS(y, x).fit()\n# y_pred = model.predict(x)\nsummary = model.summary()\nprint(summary)\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 target   R-squared:                       0.351\nModel:                            OLS   Adj. R-squared:                  0.346\nMethod:                 Least Squares   F-statistic:                     78.94\nDate:                Wed, 29 Nov 2023   Prob (F-statistic):           7.77e-41\nTime:                        14:29:14   Log-Likelihood:                -2451.6\nNo. Observations:                 442   AIC:                             4911.\nDf Residuals:                     438   BIC:                             4928.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        152.1335      2.964     51.321      0.000     146.307     157.960\nage          138.9039     64.254      2.162      0.031      12.618     265.189\nsex          -36.1353     63.391     -0.570      0.569    -160.724      88.453\nbmi          926.9120     63.525     14.591      0.000     802.061    1051.763\n==============================================================================\nOmnibus:                       14.687   Durbin-Watson:                   1.851\nProb(Omnibus):                  0.001   Jarque-Bera (JB):                8.290\nSkew:                           0.150   Prob(JB):                       0.0158\nKurtosis:                       2.400   Cond. No.                         23.7\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# 1. Rsq(결정계수)\n# r2 = 0.351\n\n# 2. 회귀계수\n# age = 138.9039\n# sex = -36.1353\n# bmi = 926.9120\n\n# 3. 회귀계수(절편)\n# const = 152.1335\n\n# 4. 회귀식 p-value\n# pvalue = 7.77e-41 # 0에 가까운 작은 값"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_5.html#결과-비교해보기-두-라이브러리-모두-같은-결과값을-출력",
    "href": "BigData_Analysis/실기_3유형_5.html#결과-비교해보기-두-라이브러리-모두-같은-결과값을-출력",
    "title": "빅분기 실기 - 3유형 문제풀이(5)",
    "section": "(결과 비교해보기) 두 라이브러리 모두 같은 결과값을 출력",
    "text": "(결과 비교해보기) 두 라이브러리 모두 같은 결과값을 출력\n\n회귀식 : y = b0 + b1x1 + b2x2 + b3x3\n(x1=age, x2=sex, x3=bmi)\n\n\n1. sklearn : y = 152.13 + 138.9age -36.14sex + 926.91bmi\n\n\n2. statsmodel : y = 152.13 + 138.9age -36.14sex + 926.91bmi"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_5.html#상관분석",
    "href": "BigData_Analysis/실기_3유형_5.html#상관분석",
    "title": "빅분기 실기 - 3유형 문제풀이(5)",
    "section": "✅ 상관분석",
    "text": "✅ 상관분석\n\n###############  실기환경 복사 영역  ###############\nimport pandas as pd\nimport numpy as np\n# 실기 시험 데이터셋으로 셋팅하기 (수정금지)\nfrom sklearn.datasets import load_diabetes\n# diabetes 데이터셋 로드\ndiabetes = load_diabetes()\nx = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\ny = pd.DataFrame(diabetes.target)\ny.columns = ['target'] \n###############  실기환경 복사 영역  ###############\n\n\n# 상관분석을 할 2가지 변수 설정\nx = x['bmi']\ny = y['target']\nprint(x.head())\nprint(y.head())\n\n0    0.061696\n1   -0.051474\n2    0.044451\n3   -0.011595\n4   -0.036385\nName: bmi, dtype: float64\n0    151.0\n1     75.0\n2    141.0\n3    206.0\n4    135.0\nName: target, dtype: float64\n\n\n\n# 라이브러리 불러오기\nfrom scipy.stats import pearsonr\n\n# 상관계수에 대한 검정실시\nr, pvalue = pearsonr(x, y)\n\n# 가설검정\n# H0 : 두 변수간 선형관계가 존재하지 않는다 (p=0)\n# H1 : 두 변수간 선형관계가 존재한다 (p!=0)\n\n# 1. 상관계수\nprint(round(r,2))\n\n# 2. p-value\nprint(round(pvalue,2))\n\n# 3. 검정통계량\n# 통계량은 별로돌 구해야 함 (T = r*root(n-2) / root(1-2r))\n# r = 상관계수\n# n = 데이터의 개수\n\nn = len(x) # 데이터 수\nr2 = r**2  # 상관꼐수의 제곱  \nstatistic = r * ((n-2)**0.5) / ((1-r2)**0.5)\n\nprint(round(statistic,2)) # 통계량값이 크면 p-value 값이 작아진다\n\n# 4. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값이 0.05보다 작기 떄문에 귀무가설을 기각한다.\n# 즉, 두 변수간 선형관계가 존재한다고 할 수 있다.(상관계수가 0이 아니다)\n\n# 답 : 기각\n\n0.59\n0.0\n15.19"
  },
  {
    "objectID": "BigData_Analysis/실기_파이썬기초.html",
    "href": "BigData_Analysis/실기_파이썬기초.html",
    "title": "빅분기 실기 - 파이썬 기초",
    "section": "",
    "text": "파이썬 기초"
  },
  {
    "objectID": "BigData_Analysis/실기_파이썬기초.html#데이터-타입object-int-float-bool-등",
    "href": "BigData_Analysis/실기_파이썬기초.html#데이터-타입object-int-float-bool-등",
    "title": "빅분기 실기 - 파이썬 기초",
    "section": "1.데이터 타입(object, int, float, bool 등)",
    "text": "1.데이터 타입(object, int, float, bool 등)\n\n# 데이터 타입 확인\ndf.dtypes\n\ncar      object\nmpg     float64\ncyl       int64\ndisp    float64\nhp        int64\ndrat    float64\nwt      float64\nqsec    float64\nvs        int64\nam        int64\ngear      int64\ncarb      int64\ndtype: object\n\n\n\n# 데이터 타입 변경 (1개)\ndf1 = df.copy()\ndf1 = df1.astype({'cyl':'object'})\nprint(df1.dtypes)\n\ncar      object\nmpg     float64\ncyl      object\ndisp    float64\nhp        int64\ndrat    float64\nwt      float64\nqsec    float64\nvs        int64\nam        int64\ngear      int64\ncarb      int64\ndtype: object\n\n\n\n# 데이터 타입 변경 (2개 이상)\ndf1 = df.copy()\ndf1 = df1.astype({'cyl':'int', 'gear':'object'})\nprint(df1.dtypes)\n\ncar      object\nmpg     float64\ncyl       int32\ndisp    float64\nhp        int64\ndrat    float64\nwt      float64\nqsec    float64\nvs        int64\nam        int64\ngear     object\ncarb      int64\ndtype: object\n\n\n\n#df1['cyl']\n\n\ndf1['cyl'].value_counts()\n\n8    14\n4    11\n6     7\nName: cyl, dtype: int64"
  },
  {
    "objectID": "BigData_Analysis/실기_파이썬기초.html#기초통계량평균-중앙값-사분위수-iqr-표준편차-등",
    "href": "BigData_Analysis/실기_파이썬기초.html#기초통계량평균-중앙값-사분위수-iqr-표준편차-등",
    "title": "빅분기 실기 - 파이썬 기초",
    "section": "2. 기초통계량(평균, 중앙값, 사분위수, IQR, 표준편차 등)",
    "text": "2. 기초통계량(평균, 중앙값, 사분위수, IQR, 표준편차 등)\n\n# Import CSV mtcars\ndf = pd.read_csv('mtcars.txt')\ndf.head()\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\nMazda RX4\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\nHornet 4 Drive\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n4\nHornet Sportabout\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n\n\n\n\ndf.shape # (행, 열)\n\n(32, 12)\n\n\n\n# 평균값 구하기\nmpg_mean = df['mpg'].mean()\nprint(mpg_mean)\n\n20.090624999999996\n\n\n\n# 중앙값 구하기\nmpg_median = df['mpg'].median()\nprint(mpg_median)\n\n19.2\n\n\n\n# 최빈값 구하기\ncyl_mode = df['cyl'].mode()\nprint(cyl_mode)\n\n0    8\nName: cyl, dtype: int64\n\n\n\ndf['cyl'].value_counts()\n\n8    14\n4    11\n6     7\nName: cyl, dtype: int64\n\n\n\n# 분산\nmpg_var = df['mpg'].var()\nprint(mpg_var)\n\n36.32410282258065\n\n\n\n# 표준편차\nmpg_std = df['mpg'].std()\nprint(mpg_std)\n\n6.026948052089105\n\n\n\n# IQR (Q3 - Q1)\nQ1 = df['mpg'].quantile(.25)\nprint(Q1)\n\n15.425\n\n\n\nQ3 = df['mpg'].quantile(.75)\nprint(Q3)\n\n22.8\n\n\n\nIQR = Q3 - Q1\nprint(IQR)\n\n7.375\n\n\n\nQ2 = df['mpg'].quantile(.5)\nprint(Q2)\nprint(df['mpg'].median()) # 2사분위수와 중앙값은 동일한 값을 출력 \n\n19.2\n19.2\n\n\n\n# 범위(Range) = 최대값 - 최소값\nmpg_max = df['mpg'].max()\nprint(mpg_max)\n\n33.9\n\n\n\nmpg_min = df['mpg'].min()\nprint(mpg_min)\n\n10.4\n\n\n\nmpg_range = mpg_max - mpg_min\nprint(mpg_range)\n\n23.5\n\n\n\n1) 분포의 비대칭도\n\n# 왜도\nmpg_skew = df['mpg'].skew()\nprint(mpg_skew)\n\n0.6723771376290805\n\n\n\n# 첨도\nmpg_kurt = df['mpg'].kurt()\nprint(mpg_kurt)\n\n-0.0220062914240855\n\n\n\n\n2) 기타(합계, 절대값, 데이터 수 등)\n\n# 합계\nmpg_sum = df['mpg'].sum()\nprint(mpg_sum)\n\n642.9000000000001\n\n\n\n# 절대값\nIQR2 = Q1 - Q3\nprint(IQR2)\nprint(abs(IQR2))\n\n-7.375\n7.375\n\n\n\n# 데이터 수\nprint(len(df['mpg']))\n\n32\n\n\n\n\n3) 그룹화하여 계산하기 (groupby 활용)\n\nimport seaborn as sns\ndf = sns.load_dataset('iris')\ndf.head()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\n# species 별로 각 변수의 평균 구해보기\ndf.groupby('species').mean()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\nspecies\n\n\n\n\n\n\n\n\nsetosa\n5.006\n3.428\n1.462\n0.246\n\n\nversicolor\n5.936\n2.770\n4.260\n1.326\n\n\nvirginica\n6.588\n2.974\n5.552\n2.026\n\n\n\n\n\n\n\n\ndf.groupby('species').median()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\nspecies\n\n\n\n\n\n\n\n\nsetosa\n5.0\n3.4\n1.50\n0.2\n\n\nversicolor\n5.9\n2.8\n4.35\n1.3\n\n\nvirginica\n6.5\n3.0\n5.55\n2.0"
  },
  {
    "objectID": "BigData_Analysis/실기_파이썬기초.html#데이터-인덱싱-필터링-정렬-변경-등",
    "href": "BigData_Analysis/실기_파이썬기초.html#데이터-인덱싱-필터링-정렬-변경-등",
    "title": "빅분기 실기 - 파이썬 기초",
    "section": "3. 데이터 인덱싱, 필터링, 정렬, 변경 등",
    "text": "3. 데이터 인덱싱, 필터링, 정렬, 변경 등\n\ndf = pd.read_csv('mtcars.txt')\ndf.head()\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\nMazda RX4\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\nHornet 4 Drive\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n4\nHornet Sportabout\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n\n\n\n\n1) 데이터 인덱싱\n인덱싱 - 행, 열을 기준으로 데이터값을 기준으로 뽑는 것\n\n# 행/열 인덱싱 : df.loc['행', '열']\ndf.loc[3, 'mpg'] # 인덱싱은 0부터 시작임 \n\n21.4\n\n\n\n# 열만 인덱싱\ndf.loc[:, 'mpg'].head() # ':' 는 전체를 가지고 올 때(빈칸으로 두면 에러)\n\n0    21.0\n1    21.0\n2    22.8\n3    21.4\n4    18.7\nName: mpg, dtype: float64\n\n\n\ndf.loc[0:3, ['mpg', 'cyl', 'disp']] # 행에서 0~3번 사이의 인덱스를 가지고 와라\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\n\n\n\n\n0\n21.0\n6\n160.0\n\n\n1\n21.0\n6\n160.0\n\n\n2\n22.8\n4\n108.0\n\n\n3\n21.4\n6\n258.0\n\n\n\n\n\n\n\n\ndf.loc[0:3, 'mpg':'disp']  # 열에서 mpg와 disp 사이의 변수를 가지고 와라\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\n\n\n\n\n0\n21.0\n6\n160.0\n\n\n1\n21.0\n6\n160.0\n\n\n2\n22.8\n4\n108.0\n\n\n3\n21.4\n6\n258.0\n\n\n\n\n\n\n\n\n# 앞에서 n행 인덱싱\ndf.head(2)\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\nMazda RX4\n21.0\n6\n160.0\n110\n3.9\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.9\n2.875\n17.02\n0\n1\n4\n4\n\n\n\n\n\n\n\n\n# 뒤에서 n행 인덱싱\ndf.tail(3)\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n29\nFerrari Dino\n19.7\n6\n145.0\n175\n3.62\n2.77\n15.5\n0\n1\n5\n6\n\n\n30\nMaserati Bora\n15.0\n8\n301.0\n335\n3.54\n3.57\n14.6\n0\n1\n5\n8\n\n\n31\nVolvo 142E\n21.4\n4\n121.0\n109\n4.11\n2.78\n18.6\n1\n1\n4\n2\n\n\n\n\n\n\n\n\n\n2) 열(Columns) 추가/제거\n\n# 열 선택\ndf_cyl = df['cyl']\ndf_cyl.head(3) # df.cyl.head(3) 같은 결과 비추\n\n0    6\n1    6\n2    4\nName: cyl, dtype: int64\n\n\n\ndf_new = df[['cyl','mpg']]\ndf_new.head(3)\n\n\n\n\n\n\n\n\ncyl\nmpg\n\n\n\n\n0\n6\n21.0\n\n\n1\n6\n21.0\n\n\n2\n4\n22.8\n\n\n\n\n\n\n\n\n# 열 제거\ndf.drop(columns = ['car','mpg','cyl']).head(3)\n\n\n\n\n\n\n\n\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n\n\n\n\n\n\n# 열 추가\ndf2 = df.copy()\ndf2['new'] = df['mpg'] + 10 # 새로운 컬럼으로 생김\ndf2.head(3)\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\nnew\n\n\n\n\n0\nMazda RX4\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n31.0\n\n\n1\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n31.0\n\n\n2\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n32.8\n\n\n\n\n\n\n\n\n\n3) 데이터 필터링\n\n# 1개 조건 필터링\n# cyl = 4 인 데이터의 수 \ncond1 = (df['cyl'] == 4)\nlen(df[cond1])\n\n# cyl_4 = df[df['cyl']==4]\n# print(len(cyl_4))\n\n11\n\n\n\n# mpg 가 22 이상인 데이터 수\ncond2 = (df['mpg'] &gt;= 22)\nlen(df[cond2])\n\n9\n\n\n\n# 2개 조건 필터링 \ndf[cond1 & cond2]\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n2\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n7\nMerc 240D\n24.4\n4\n146.7\n62\n3.69\n3.190\n20.00\n1\n0\n4\n2\n\n\n8\nMerc 230\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2\n\n\n17\nFiat 128\n32.4\n4\n78.7\n66\n4.08\n2.200\n19.47\n1\n1\n4\n1\n\n\n18\nHonda Civic\n30.4\n4\n75.7\n52\n4.93\n1.615\n18.52\n1\n1\n4\n2\n\n\n19\nToyota Corolla\n33.9\n4\n71.1\n65\n4.22\n1.835\n19.90\n1\n1\n4\n1\n\n\n25\nFiat X1-9\n27.3\n4\n79.0\n66\n4.08\n1.935\n18.90\n1\n1\n4\n1\n\n\n26\nPorsche 914-2\n26.0\n4\n120.3\n91\n4.43\n2.140\n16.70\n0\n1\n5\n2\n\n\n27\nLotus Europa\n30.4\n4\n95.1\n113\n3.77\n1.513\n16.90\n1\n1\n5\n2\n\n\n\n\n\n\n\n\n# 2개 조건 필터링 후 데이터 개수 (and)\nprint(len(df[cond1 & cond2])) # print() 함수를 이용해서 개수를 출력해야 인정됨 \n\n9\n\n\n\n# 2개 조건 필터링 후 데이터 개수 (or)\ndf[cond1 | cond2]\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n2\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n7\nMerc 240D\n24.4\n4\n146.7\n62\n3.69\n3.190\n20.00\n1\n0\n4\n2\n\n\n8\nMerc 230\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2\n\n\n17\nFiat 128\n32.4\n4\n78.7\n66\n4.08\n2.200\n19.47\n1\n1\n4\n1\n\n\n18\nHonda Civic\n30.4\n4\n75.7\n52\n4.93\n1.615\n18.52\n1\n1\n4\n2\n\n\n19\nToyota Corolla\n33.9\n4\n71.1\n65\n4.22\n1.835\n19.90\n1\n1\n4\n1\n\n\n20\nToyota Corona\n21.5\n4\n120.1\n97\n3.70\n2.465\n20.01\n1\n0\n3\n1\n\n\n25\nFiat X1-9\n27.3\n4\n79.0\n66\n4.08\n1.935\n18.90\n1\n1\n4\n1\n\n\n26\nPorsche 914-2\n26.0\n4\n120.3\n91\n4.43\n2.140\n16.70\n0\n1\n5\n2\n\n\n27\nLotus Europa\n30.4\n4\n95.1\n113\n3.77\n1.513\n16.90\n1\n1\n5\n2\n\n\n31\nVolvo 142E\n21.4\n4\n121.0\n109\n4.11\n2.780\n18.60\n1\n1\n4\n2\n\n\n\n\n\n\n\n\nprint(len(df[cond1 | cond2]))\n\n11\n\n\n\n# 한번에 코딩할 경우\nprint(len(df[(df['cyl'] == 4) & (df['mpg'] &gt;= 22)]))\nprint(len(df[(df['cyl'] == 4) | (df['mpg'] &gt;= 22)])) # 이렇게 한번에 하면 실수할 가능성 존재 cond1,2를 만들어서 하는 것 추천\n\n9\n11\n\n\n\n\n4) 데이터 정렬\n\n# 내림차순 정렬 (위에서부터 내려간다)\ndf.sort_values('mpg', ascending=False).head()\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n19\nToyota Corolla\n33.9\n4\n71.1\n65\n4.22\n1.835\n19.90\n1\n1\n4\n1\n\n\n17\nFiat 128\n32.4\n4\n78.7\n66\n4.08\n2.200\n19.47\n1\n1\n4\n1\n\n\n27\nLotus Europa\n30.4\n4\n95.1\n113\n3.77\n1.513\n16.90\n1\n1\n5\n2\n\n\n18\nHonda Civic\n30.4\n4\n75.7\n52\n4.93\n1.615\n18.52\n1\n1\n4\n2\n\n\n25\nFiat X1-9\n27.3\n4\n79.0\n66\n4.08\n1.935\n18.90\n1\n1\n4\n1\n\n\n\n\n\n\n\n\n# 오름차순 정렬 (아래에서부터 올라간다)\ndf.sort_values('mpg', ascending=True).head()\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n15\nLincoln Continental\n10.4\n8\n460.0\n215\n3.00\n5.424\n17.82\n0\n0\n3\n4\n\n\n14\nCadillac Fleetwood\n10.4\n8\n472.0\n205\n2.93\n5.250\n17.98\n0\n0\n3\n4\n\n\n23\nCamaro Z28\n13.3\n8\n350.0\n245\n3.73\n3.840\n15.41\n0\n0\n3\n4\n\n\n6\nDuster 360\n14.3\n8\n360.0\n245\n3.21\n3.570\n15.84\n0\n0\n3\n4\n\n\n16\nChrysler Imperial\n14.7\n8\n440.0\n230\n3.23\n5.345\n17.42\n0\n0\n3\n4\n\n\n\n\n\n\n\n\n\n5) 데이터 변경 (조건문)\n\nimport numpy as np \ndf = pd.read_csv(\"mtcars.txt\")\n# np.where 활용\n# hp 변수 값 중에서 205가 넘는 값은 205로 처리하고, 나머지는 그대로 유지\ndf['hp'] = np.where(df['hp'] &gt; 205, 205, df['hp'])\n\n# 내림차순 정렬 (위에서부터 내려간다)\ndf.sort_values('hp', ascending=False).head(10)\n\n# 활용 : 이상치를 Max 값이나 Min 값으로 대체할 경우 조건문 활용\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n16\nChrysler Imperial\n14.7\n8\n440.0\n205\n3.23\n5.345\n17.42\n0\n0\n3\n4\n\n\n30\nMaserati Bora\n15.0\n8\n301.0\n205\n3.54\n3.570\n14.60\n0\n1\n5\n8\n\n\n28\nFord Pantera L\n15.8\n8\n351.0\n205\n4.22\n3.170\n14.50\n0\n1\n5\n4\n\n\n6\nDuster 360\n14.3\n8\n360.0\n205\n3.21\n3.570\n15.84\n0\n0\n3\n4\n\n\n23\nCamaro Z28\n13.3\n8\n350.0\n205\n3.73\n3.840\n15.41\n0\n0\n3\n4\n\n\n15\nLincoln Continental\n10.4\n8\n460.0\n205\n3.00\n5.424\n17.82\n0\n0\n3\n4\n\n\n14\nCadillac Fleetwood\n10.4\n8\n472.0\n205\n2.93\n5.250\n17.98\n0\n0\n3\n4\n\n\n13\nMerc 450SLC\n15.2\n8\n275.8\n180\n3.07\n3.780\n18.00\n0\n0\n3\n3\n\n\n11\nMerc 450SE\n16.4\n8\n275.8\n180\n3.07\n4.070\n17.40\n0\n0\n3\n3\n\n\n12\nMerc 450SL\n17.3\n8\n275.8\n180\n3.07\n3.730\n17.60\n0\n0\n3\n3"
  },
  {
    "objectID": "BigData_Analysis/실기_파이썬기초.html#결측치-이상치-중복값-처리제거-or-대체",
    "href": "BigData_Analysis/실기_파이썬기초.html#결측치-이상치-중복값-처리제거-or-대체",
    "title": "빅분기 실기 - 파이썬 기초",
    "section": "4. 결측치, 이상치, 중복값 처리(제거 or 대체)",
    "text": "4. 결측치, 이상치, 중복값 처리(제거 or 대체)\n\n🚢 데이터 불러오기 (타이타닉 데이터셋)\n\n종속변수(y) : 생존 여부 (0 사망, 1 생존)\n\n\n독립변수(x) : pclass, sex, age 등의 탑승자 정보(변수)\n\n\nimport seaborn as sns\n# 데이터셋 목록 : sns.get_dataset_names()\n# 타이타닉 데이터 불러오기\ndf = sns.load_dataset('titanic')\n\n\ndf.head()\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n\n\n\n\n\n\ndf.shape\n\n(891, 15)\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 15 columns):\n #   Column       Non-Null Count  Dtype   \n---  ------       --------------  -----   \n 0   survived     891 non-null    int64   \n 1   pclass       891 non-null    int64   \n 2   sex          891 non-null    object  \n 3   age          714 non-null    float64 \n 4   sibsp        891 non-null    int64   \n 5   parch        891 non-null    int64   \n 6   fare         891 non-null    float64 \n 7   embarked     889 non-null    object  \n 8   class        891 non-null    category\n 9   who          891 non-null    object  \n 10  adult_male   891 non-null    bool    \n 11  deck         203 non-null    category\n 12  embark_town  889 non-null    object  \n 13  alive        891 non-null    object  \n 14  alone        891 non-null    bool    \ndtypes: bool(2), category(2), float64(2), int64(4), object(5)\nmemory usage: 80.7+ KB\n\n\n\n\n1) 결측치 확인 및 처리\n\n# 결측치 확인 \ndf.isnull().sum()\n\nsurvived         0\npclass           0\nsex              0\nage            177\nsibsp            0\nparch            0\nfare             0\nembarked         2\nclass            0\nwho              0\nadult_male       0\ndeck           688\nembark_town      2\nalive            0\nalone            0\ndtype: int64\n\n\n\n# 결측치 제거\nprint(df.dropna(axis=0).shape) # 행 기준, default는 axis = 0 따로 설정 안해도 됨\nprint(df.dropna(axis=1).shape) # 열 기준\n\n(182, 15)\n(891, 11)\n\n\n\n# 결측치 대체\n# 데이터 복사 \ndf2 = df.copy()\ndf2 = pd.DataFrame(df) # df를 데이터 프레임 형태로 변환\n\n\n# 1. 중앙값/평균값 등으로 대체\n\n# 먼저 중앙값을 구합니다\nmedian_age = df2['age'].median()\nprint(median_age)\n\n# 평균으로 대체할 경우 \n# mean_age = df2['age'].mean()\n\n28.0\n\n\n\n# 구한 중앙값으로 결측치를 대체합니다.\ndf2['age'] = df['age'].fillna(median_age)\n\n\n# 결측치가 잘 대체되었는지 확인합니다\ndf2.isnull().sum()\n\nsurvived         0\npclass           0\nsex              0\nage              0\nsibsp            0\nparch            0\nfare             0\nembarked         2\nclass            0\nwho              0\nadult_male       0\ndeck           688\nembark_town      2\nalive            0\nalone            0\ndtype: int64\n\n\n\nprint(df['age'].mean()) # 원본 데이터\nprint(df2['age'].mean()) # 중앙값으로 대체한 데이터 \n\n29.69911764705882\n29.36158249158249\n\n\n\n# 중복값 확인\ndf.drop_duplicates().shape\n\n(784, 15)\n\n\n\n\n2) 이상치 확인 및 처리\n\n\n✔ 상자그림 활용 (이상치: Q1, Q3로부터 1.5*IQR을 초과하는 값)\n\n# 타이타닉 데이터 불러오기\ndf = sns.load_dataset('titanic')\n\n# (참고) 상자그림\nsns.boxplot(df['age'])\n\nC:\\Users\\Hyunsoo Kim\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n  warnings.warn(\n\n\n&lt;AxesSubplot:xlabel='age'&gt;\n\n\n\n\n\n\n# Q1, Q3, IQR 구하기\nQ1 = df['age'].quantile(.25)\nQ3 = df['age'].quantile(.75)\nIQR = Q3 - Q1\nprint(Q1, Q3, IQR)\n\n20.125 38.0 17.875\n\n\n\nupper = Q3 + 1.5*IQR\nlower = Q1 - 1.5*IQR\nprint(upper, lower)\n\n64.8125 -6.6875\n\n\n\n# 문제 : age 변수의 이상치를 제외한 데이터 수는? (상자그림 기준)\ncond1 = (df['age'] &lt;= upper)\ncond2 = (df['age'] &gt;= lower)\nprint(len(df[cond1 & cond2]))\nprint(len(df[cond1]))\nprint(len(df))\n\n703\n703\n891\n\n\n\n# 문제 : age 변수의 이상치를 제외한 데이터셋 확인(상자그림 기준)\ndf_new = df[cond1 & cond2]\ndf_new\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n885\n0\n3\nfemale\n39.0\n0\n5\n29.1250\nQ\nThird\nwoman\nFalse\nNaN\nQueenstown\nno\nFalse\n\n\n886\n0\n2\nmale\n27.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n887\n1\n1\nfemale\n19.0\n0\n0\n30.0000\nS\nFirst\nwoman\nFalse\nB\nSouthampton\nyes\nTrue\n\n\n889\n1\n1\nmale\n26.0\n0\n0\n30.0000\nC\nFirst\nman\nTrue\nC\nCherbourg\nyes\nTrue\n\n\n890\n0\n3\nmale\n32.0\n0\n0\n7.7500\nQ\nThird\nman\nTrue\nNaN\nQueenstown\nno\nTrue\n\n\n\n\n703 rows × 15 columns\n\n\n\n\n\n✔ 표준정규분포 활용(이상치 : \\(\\pm\\) 3Z 값을 넘어가는 값)\n\n# 데이터 표준화, Z = (개별값 -  평균) / 표준편차\n\n\nmean_age = df['age'].mean()\nstd_age = df['age'].std()\nprint(mean_age)\nprint(std_age)\n\n29.69911764705882\n14.526497332334044\n\n\n\nznorm = (df['age']-mean_age) / std_age\nznorm\n\n0     -0.530005\n1      0.571430\n2     -0.254646\n3      0.364911\n4      0.364911\n         ...   \n886   -0.185807\n887   -0.736524\n888         NaN\n889   -0.254646\n890    0.158392\nName: age, Length: 891, dtype: float64\n\n\n\n# 문제 : 이상치의 개수는 몇개인가? (: ±3Z 기준)\n\n\ncond1 = (znorm &gt; 3)\nlen(df[cond1])\n\n2\n\n\n\ncond2 = (znorm &lt; -3)\nlen(df[cond2])\n\n0\n\n\n\nprint(len(df[cond1]) + len(df[cond2]))\n\n2\n\n\n\n\n3) 중복값 제거\n\n# 데이터 불러오기\ndf = sns.load_dataset('titanic')\n\n\ndf.shape\n\n(891, 15)\n\n\n\ndf1 = df.copy()\ndf1 = df1.drop_duplicates()\nprint(df1.shape)\n# (주의) 예제에서는 중복값이 있어서 제거했지만,\n# 중복값이 나올 수 있는 상황이변 제거할 필요없음\n\n(784, 15)"
  },
  {
    "objectID": "BigData_Analysis/실기_파이썬기초.html#데이터-scaling데이터-표준화-정규화",
    "href": "BigData_Analysis/실기_파이썬기초.html#데이터-scaling데이터-표준화-정규화",
    "title": "빅분기 실기 - 파이썬 기초",
    "section": "✅ 5. 데이터 scaling(데이터 표준화, 정규화)",
    "text": "✅ 5. 데이터 scaling(데이터 표준화, 정규화)\n\n1) 데이터 표준화(Z-score normalization)\n\ndf = pd.read_csv(\"mtcars.txt\")\ndf.head()\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\nMazda RX4\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\nHornet 4 Drive\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n4\nHornet Sportabout\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\nzscaler = StandardScaler() # 변수명은 사용하기 편한 변수명으로 사용\ndf['mpg'] = zscaler.fit_transform(df[['mpg']])\ndf.head()\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\nMazda RX4\n0.153299\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\nMazda RX4 Wag\n0.153299\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\nDatsun 710\n0.456737\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\nHornet 4 Drive\n0.220730\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n4\nHornet Sportabout\n-0.234427\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n\n\n\n\n# 확인\nprint(df['mpg'].mean(), df['mpg'].std())\n\n-5.48172618408671e-16 1.016001016001524\n\n\n\n\n2) 데이터 정규화(min-max normalization)\n\ndf = pd.read_csv(\"mtcars.txt\")\ndf.head\n\nfrom sklearn.preprocessing import MinMaxScaler\nmscaler = MinMaxScaler()\ndf['mpg'] = mscaler.fit_transform(df[['mpg']])\ndf.head()\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\nMazda RX4\n0.451064\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\nMazda RX4 Wag\n0.451064\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\nDatsun 710\n0.527660\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\nHornet 4 Drive\n0.468085\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n4\nHornet Sportabout\n0.353191\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n\n\n\n\n# 확인\nprint(df['mpg'].min(), df['mpg'].max())\n\n0.0 1.0"
  },
  {
    "objectID": "BigData_Analysis/실기_파이썬기초.html#데이터-합치기",
    "href": "BigData_Analysis/실기_파이썬기초.html#데이터-합치기",
    "title": "빅분기 실기 - 파이썬 기초",
    "section": "6. 데이터 합치기",
    "text": "6. 데이터 합치기\n\n# 행, 열 방향으로 데이터 합치기\ndf = sns.load_dataset('iris')\ndf.head()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\n# 데이터 2개로 분리\ndf1 = df.loc[0:30, ] # 0~30행 데이터\ndf2 = df.loc[31:60, ] # 31~60행 데이터 \n\n\ndf1.head()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\ndf2.head()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n31\n5.4\n3.4\n1.5\n0.4\nsetosa\n\n\n32\n5.2\n4.1\n1.5\n0.1\nsetosa\n\n\n33\n5.5\n4.2\n1.4\n0.2\nsetosa\n\n\n34\n4.9\n3.1\n1.5\n0.2\nsetosa\n\n\n35\n5.0\n3.2\n1.2\n0.2\nsetosa\n\n\n\n\n\n\n\n\ndf_sum = pd.concat([df1, df2], axis=0) # 행 방향으로 결합 (위, 아래)\nprint(df_sum.head())\nprint(df_sum.shape)\n\n   sepal_length  sepal_width  petal_length  petal_width species\n0           5.1          3.5           1.4          0.2  setosa\n1           4.9          3.0           1.4          0.2  setosa\n2           4.7          3.2           1.3          0.2  setosa\n3           4.6          3.1           1.5          0.2  setosa\n4           5.0          3.6           1.4          0.2  setosa\n(61, 5)\n\n\n\n# 데이터 2개로 나누기\ndf1 = df.loc[:, 'sepal_length':'petal_length'] # 1~3열 추출 데이터\ndf2 = df.loc[:, ['petal_width','species']] # 4~5열 추출 데이터\n\n\ndf_sum = pd.concat([df1, df2], axis=1) # 열 방향으로 결합 (좌, 우)\ndf_sum.head()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa"
  },
  {
    "objectID": "BigData_Analysis/실기_파이썬기초.html#날짜시간-데이터-index-다루기",
    "href": "BigData_Analysis/실기_파이썬기초.html#날짜시간-데이터-index-다루기",
    "title": "빅분기 실기 - 파이썬 기초",
    "section": "7. 날짜/시간 데이터, index 다루기",
    "text": "7. 날짜/시간 데이터, index 다루기\n\n1) 날짜 다루기\n\n# 데이터 만들기\ndf = pd.DataFrame({\n    '날짜' : ['20230105','20230105','20230223','20230223',\n            '20230312','20230422','20230511'],\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600] })\ndf\n\n\n\n\n\n\n\n\n날짜\n물품\n판매수\n개당수익\n\n\n\n\n0\n20230105\nA\n5\n500\n\n\n1\n20230105\nB\n10\n600\n\n\n2\n20230223\nA\n15\n500\n\n\n3\n20230223\nB\n15\n600\n\n\n4\n20230312\nA\n20\n600\n\n\n5\n20230422\nB\n25\n700\n\n\n6\n20230511\nA\n40\n600\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 7 entries, 0 to 6\nData columns (total 4 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   날짜      7 non-null      object\n 1   물품      7 non-null      object\n 2   판매수     7 non-null      int64 \n 3   개당수익    7 non-null      int64 \ndtypes: int64(2), object(2)\nmemory usage: 352.0+ bytes\n\n\n\n# 데이터 타입 datetime으로 변경\ndf['날짜'] = pd.to_datetime(df['날짜'])\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 7 entries, 0 to 6\nData columns (total 4 columns):\n #   Column  Non-Null Count  Dtype         \n---  ------  --------------  -----         \n 0   날짜      7 non-null      datetime64[ns]\n 1   물품      7 non-null      object        \n 2   판매수     7 non-null      int64         \n 3   개당수익    7 non-null      int64         \ndtypes: datetime64[ns](1), int64(2), object(1)\nmemory usage: 352.0+ bytes\n\n\n\n# 년, 월, 일 변수(열) 추가하기\ndf['year'] = df['날짜'].dt.year\ndf['month'] = df['날짜'].dt.month\ndf['day'] = df['날짜'].dt.day\ndf\n\n\n\n\n\n\n\n\n날짜\n물품\n판매수\n개당수익\nyear\nmonth\nday\n\n\n\n\n0\n2023-01-05\nA\n5\n500\n2023\n1\n5\n\n\n1\n2023-01-05\nB\n10\n600\n2023\n1\n5\n\n\n2\n2023-02-23\nA\n15\n500\n2023\n2\n23\n\n\n3\n2023-02-23\nB\n15\n600\n2023\n2\n23\n\n\n4\n2023-03-12\nA\n20\n600\n2023\n3\n12\n\n\n5\n2023-04-22\nB\n25\n700\n2023\n4\n22\n\n\n6\n2023-05-11\nA\n40\n600\n2023\n5\n11\n\n\n\n\n\n\n\n\n# 날짜 구간 필터링\ndf[df['날짜'].between('2023-01-01', '2023-01-31')] # 1월 31일은 미포함\n\n\n\n\n\n\n\n\n날짜\n물품\n판매수\n개당수익\nyear\nmonth\nday\n\n\n\n\n0\n2023-01-05\nA\n5\n500\n2023\n1\n5\n\n\n1\n2023-01-05\nB\n10\n600\n2023\n1\n5\n\n\n\n\n\n\n\n\n# 날짜를 인덱스로 설정후 loc 함수 사용\n# 데이터 만들기 \ndf = pd.DataFrame({\n    '날짜' : ['20230105','20230105','20230223','20230223',\n            '20230312','20230422','20230511'],\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600] })\n\n# 데이터 타입 datetime으로 변경(필수)\ndf['날짜'] = pd.to_datetime(df['날짜'])\n\ndf = df.set_index('날짜', drop=True) # drop=True(디폴트) or False\ndf.head(3)\n\n\n\n\n\n\n\n\n물품\n판매수\n개당수익\n\n\n날짜\n\n\n\n\n\n\n\n2023-01-05\nA\n5\n500\n\n\n2023-01-05\nB\n10\n600\n\n\n2023-02-23\nA\n15\n500\n\n\n\n\n\n\n\n\nprint(df.loc['2023-01-05':'2023-02-23']) # 둘다 기간 포함\nprint(df.loc[ (df.index&gt;='2023-01-05') & (df.index&lt;='2023-02-23') ])\n\n           물품  판매수  개당수익\n날짜                      \n2023-01-05  A    5   500\n2023-01-05  B   10   600\n2023-02-23  A   15   500\n2023-02-23  B   15   600\n           물품  판매수  개당수익\n날짜                      \n2023-01-05  A    5   500\n2023-01-05  B   10   600\n2023-02-23  A   15   500\n2023-02-23  B   15   600\n\n\n\n\n2) 시간 다루기\n\n# 시간 데이터 만들기 \ndf = pd.DataFrame({\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600] })\ntime = pd.date_range('2023-09-24 12:25:00', '2023-09-25 14:45:30', periods=7)\ndf['time'] = time\ndf = df[['time','물품','판매수','개당수익']]\ndf\n\n\n\n\n\n\n\n\ntime\n물품\n판매수\n개당수익\n\n\n\n\n0\n2023-09-24 12:25:00\nA\n5\n500\n\n\n1\n2023-09-24 16:48:25\nB\n10\n600\n\n\n2\n2023-09-24 21:11:50\nA\n15\n500\n\n\n3\n2023-09-25 01:35:15\nB\n15\n600\n\n\n4\n2023-09-25 05:58:40\nA\n20\n600\n\n\n5\n2023-09-25 10:22:05\nB\n25\n700\n\n\n6\n2023-09-25 14:45:30\nA\n40\n600\n\n\n\n\n\n\n\n\n# index 초기화 (인덱스를 컬럼으로)\n# df = df.reset_index()\n# df\n\n\n# index 새로 지정\ndf = df.set_index('time')\ndf\n\n\n\n\n\n\n\n\n물품\n판매수\n개당수익\n\n\ntime\n\n\n\n\n\n\n\n2023-09-24 12:25:00\nA\n5\n500\n\n\n2023-09-24 16:48:25\nB\n10\n600\n\n\n2023-09-24 21:11:50\nA\n15\n500\n\n\n2023-09-25 01:35:15\nB\n15\n600\n\n\n2023-09-25 05:58:40\nA\n20\n600\n\n\n2023-09-25 10:22:05\nB\n25\n700\n\n\n2023-09-25 14:45:30\nA\n40\n600\n\n\n\n\n\n\n\n\n# 시간 데이터 다루기(주의: 시간이 index에 위치해야 함)\ndf.between_time(start_time='12:25', end_time='21:00') #시간 시작, 끝 모두 포함\n# include_start=False, include_end=False 옵션을 시작, 끝 시간 제외 가능\n\n\n\n\n\n\n\n\n물품\n판매수\n개당수익\n\n\ntime\n\n\n\n\n\n\n\n2023-09-24 12:25:00\nA\n5\n500\n\n\n2023-09-24 16:48:25\nB\n10\n600\n\n\n2023-09-25 14:45:30\nA\n40\n600\n\n\n\n\n\n\n\n\n# 날짜를 인덱스로 설정후 loc 함수 사용\nprint(df.loc['2023-09-24 12:25:00':'2023-09-24 21:11:50']) # 둘다 포함\nprint(df.loc[ (df.index&gt;='2023-09-24 12:25:00') & (df.index&lt;='2023-09-24 21:11:50') ])\n\n                    물품  판매수  개당수익\ntime                             \n2023-09-24 12:25:00  A    5   500\n2023-09-24 16:48:25  B   10   600\n2023-09-24 21:11:50  A   15   500\n                    물품  판매수  개당수익\ntime                             \n2023-09-24 12:25:00  A    5   500\n2023-09-24 16:48:25  B   10   600\n2023-09-24 21:11:50  A   15   500"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html",
    "href": "Data_Mining/2022-03-04-numpy.html",
    "title": "Numpy 기본",
    "section": "",
    "text": "numpy 기본 코드 실습\n도구 - 넘파이(NumPy)\n*넘파이(NumPy)는 파이썬의 과학 컴퓨팅을 위한 기본 라이브러리입니다. 넘파이의 핵심은 강력한 N-차원 배열 객체입니다. 또한 선형 대수, 푸리에(Fourier) 변환, 유사 난수 생성과 같은 유용한 함수들도 제공합니다.”"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.zeros",
    "href": "Data_Mining/2022-03-04-numpy.html#np.zeros",
    "title": "Numpy 기본",
    "section": "np.zeros",
    "text": "np.zeros\nzeros 함수는 0으로 채워진 배열을 만듭니다:\n\nnp.zeros(5)\n\narray([0., 0., 0., 0., 0.])\n\n\n2D 배열(즉, 행렬)을 만들려면 원하는 행과 열의 크기를 튜플로 전달합니다. 예를 들어 다음은 \\(3 \\times 4\\) 크기의 행렬입니다:\n\nnp.zeros((3,4))\n\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#용어",
    "href": "Data_Mining/2022-03-04-numpy.html#용어",
    "title": "Numpy 기본",
    "section": "용어",
    "text": "용어\n\n넘파이에서 각 차원을 축(axis) 이라고 합니다\n축의 개수를 랭크(rank) 라고 합니다.\n\n예를 들어, 위의 \\(3 \\times 4\\) 행렬은 랭크 2인 배열입니다(즉 2차원입니다).\n첫 번째 축의 길이는 3이고 두 번째 축의 길이는 4입니다.\n\n배열의 축 길이를 배열의 크기(shape)라고 합니다.\n\n예를 들어, 위 행렬의 크기는 (3, 4)입니다.\n랭크는 크기의 길이와 같습니다.\n\n배열의 사이즈(size)는 전체 원소의 개수입니다. 축의 길이를 모두 곱해서 구할 수 있습니다(가령, \\(3 \\times 4=12\\)).\n\n\na = np.zeros((3,4))\na\n\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])\n\n\n\na.shape\n\n(3, 4)\n\n\n\na.ndim  # len(a.shape)와 같습니다\n\n2\n\n\n\na.size\n\n12"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#n-차원-배열",
    "href": "Data_Mining/2022-03-04-numpy.html#n-차원-배열",
    "title": "Numpy 기본",
    "section": "N-차원 배열",
    "text": "N-차원 배열\n임의의 랭크 수를 가진 N-차원 배열을 만들 수 있습니다. 예를 들어, 다음은 크기가 (2,3,4)인 3D 배열(랭크=3)입니다:\n\nnp.zeros((2,2,5))\n\narray([[[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]],\n\n       [[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#배열-타입",
    "href": "Data_Mining/2022-03-04-numpy.html#배열-타입",
    "title": "Numpy 기본",
    "section": "배열 타입",
    "text": "배열 타입\n넘파이 배열의 타입은 ndarray입니다:\n\ntype(np.zeros((3,4)))\n\nnumpy.ndarray"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.ones",
    "href": "Data_Mining/2022-03-04-numpy.html#np.ones",
    "title": "Numpy 기본",
    "section": "np.ones",
    "text": "np.ones\nndarray를 만들 수 있는 넘파이 함수가 많습니다.\n다음은 1로 채워진 \\(3 \\times 4\\) 크기의 행렬입니다:\n\nnp.ones((3,4))\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.full",
    "href": "Data_Mining/2022-03-04-numpy.html#np.full",
    "title": "Numpy 기본",
    "section": "np.full",
    "text": "np.full\n주어진 값으로 지정된 크기의 배열을 초기화합니다. 다음은 π로 채워진 \\(3 \\times 4\\) 크기의 행렬입니다.\n\nnp.full((3,4), np.pi)\n\narray([[3.14159265, 3.14159265, 3.14159265, 3.14159265],\n       [3.14159265, 3.14159265, 3.14159265, 3.14159265],\n       [3.14159265, 3.14159265, 3.14159265, 3.14159265]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.empty",
    "href": "Data_Mining/2022-03-04-numpy.html#np.empty",
    "title": "Numpy 기본",
    "section": "np.empty",
    "text": "np.empty\n초기화되지 않은 \\(2 \\times 3\\) 크기의 배열을 만듭니다(배열의 내용은 예측이 불가능하며 메모리 상황에 따라 달라집니다):\n\nnp.empty((2,3))\n\narray([[9.6677106e-317, 0.0000000e+000, 0.0000000e+000],\n       [0.0000000e+000, 0.0000000e+000, 0.0000000e+000]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.array",
    "href": "Data_Mining/2022-03-04-numpy.html#np.array",
    "title": "Numpy 기본",
    "section": "np.array",
    "text": "np.array\narray 함수는 파이썬 리스트를 사용하여 ndarray를 초기화합니다:\n\nnp.array([[1,2,3,4], [10, 20, 30, 40]])\n\narray([[ 1,  2,  3,  4],\n       [10, 20, 30, 40]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.arange",
    "href": "Data_Mining/2022-03-04-numpy.html#np.arange",
    "title": "Numpy 기본",
    "section": "np.arange",
    "text": "np.arange\n파이썬의 기본 range 함수와 비슷한 넘파이 arange 함수를 사용하여 ndarray를 만들 수 있습니다:\n\nnp.arange(1, 5)\n\narray([1, 2, 3, 4])\n\n\n부동 소수도 가능합니다:\n\nnp.arange(1.0, 5.0)\n\narray([1., 2., 3., 4.])\n\n\n파이썬의 기본 range 함수처럼 건너 뛰는 정도를 지정할 수 있습니다:\n\nnp.arange(1, 5, 0.5)\n\narray([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5])\n\n\n부동 소수를 사용하면 원소의 개수가 일정하지 않을 수 있습니다. 예를 들면 다음과 같습니다:\n\nprint(np.arange(0, 5/3, 1/3)) # 부동 소수 오차 때문에, 최댓값은 4/3 또는 5/3이 됩니다.\nprint(np.arange(0, 5/3, 0.333333333))\nprint(np.arange(0, 5/3, 0.333333334))\n\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]\n[0.         0.33333333 0.66666667 1.         1.33333334]"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.linspace",
    "href": "Data_Mining/2022-03-04-numpy.html#np.linspace",
    "title": "Numpy 기본",
    "section": "np.linspace",
    "text": "np.linspace\n이런 이유로 부동 소수를 사용할 땐 arange 대신에 linspace 함수를 사용하는 것이 좋습니다. linspace 함수는 지정된 개수만큼 두 값 사이를 나눈 배열을 반환합니다(arange와는 다르게 최댓값이 포함됩니다):\n\nprint(np.linspace(0, 5/3, 6))\n\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.rand와-np.randn",
    "href": "Data_Mining/2022-03-04-numpy.html#np.rand와-np.randn",
    "title": "Numpy 기본",
    "section": "np.rand와 np.randn",
    "text": "np.rand와 np.randn\n넘파이의 random 모듈에는 ndarray를 랜덤한 값으로 초기화할 수 있는 함수들이 많이 있습니다. 예를 들어, 다음은 (균등 분포인) 0과 1사이의 랜덤한 부동 소수로 \\(3 \\times 4\\) 행렬을 초기화합니다:\n\nnp.random.rand(3,4)\n\narray([[0.37892456, 0.17966937, 0.38206837, 0.34922123],\n       [0.80462136, 0.9845914 , 0.9416127 , 0.28305275],\n       [0.21201033, 0.54891417, 0.03781613, 0.4369229 ]])\n\n\n다음은 평균이 0이고 분산이 1인 일변량 정규 분포(가우시안 분포)에서 샘플링한 랜덤한 부동 소수를 담은 \\(3 \\times 4\\) 행렬입니다:\n\nnp.random.randn(3,4)\n\narray([[ 0.83811287, -0.57131751, -0.4381827 ,  1.1485899 ],\n       [ 1.45316084, -0.47259181, -1.23426057, -0.0669813 ],\n       [ 1.01003549,  1.04381736, -0.93060038,  2.39043293]])\n\n\n이 분포의 모양을 알려면 맷플롯립을 사용해 그려보는 것이 좋습니다(더 자세한 것은 맷플롯립 튜토리얼을 참고하세요):\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\n\nplt.hist(np.random.rand(100000), density=True, bins=100, histtype=\"step\", color=\"blue\", label=\"rand\")\nplt.hist(np.random.randn(100000), density=True, bins=100, histtype=\"step\", color=\"red\", label=\"randn\")\nplt.axis([-2.5, 2.5, 0, 1.1])\nplt.legend(loc = \"upper left\")\nplt.title(\"Random distributions\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Density\")\nplt.show()"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.fromfunction",
    "href": "Data_Mining/2022-03-04-numpy.html#np.fromfunction",
    "title": "Numpy 기본",
    "section": "np.fromfunction",
    "text": "np.fromfunction\n함수를 사용하여 ndarray를 초기화할 수도 있습니다:\n\ndef my_function(z, y, x):\n    return x + 10 * y + 100 * z\n\nnp.fromfunction(my_function, (3, 2, 10))\n\narray([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.],\n        [ 10.,  11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.]],\n\n       [[100., 101., 102., 103., 104., 105., 106., 107., 108., 109.],\n        [110., 111., 112., 113., 114., 115., 116., 117., 118., 119.]],\n\n       [[200., 201., 202., 203., 204., 205., 206., 207., 208., 209.],\n        [210., 211., 212., 213., 214., 215., 216., 217., 218., 219.]]])\n\n\n넘파이는 먼저 크기가 (3, 2, 10)인 세 개의 ndarray(차원마다 하나씩)를 만듭니다. 각 배열은 축을 따라 좌표 값과 같은 값을 가집니다. 예를 들어, z 축에 있는 배열의 모든 원소는 z-축의 값과 같습니다:\n[[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n\n [[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]]\n\n [[ 2.  2.  2.  2.  2.  2.  2.  2.  2.  2.]\n  [ 2.  2.  2.  2.  2.  2.  2.  2.  2.  2.]]]\n위의 식 x + 10 * y + 100 * z에서 x, y, z는 사실 ndarray입니다(배열의 산술 연산에 대해서는 아래에서 설명합니다). 중요한 점은 함수 my_function이 원소마다 호출되는 것이 아니고 딱 한 번 호출된다는 점입니다. 그래서 매우 효율적으로 초기화할 수 있습니다."
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#dtype",
    "href": "Data_Mining/2022-03-04-numpy.html#dtype",
    "title": "Numpy 기본",
    "section": "dtype",
    "text": "dtype\n넘파이의 ndarray는 모든 원소가 동일한 타입(보통 숫자)을 가지기 때문에 효율적입니다. dtype 속성으로 쉽게 데이터 타입을 확인할 수 있습니다:\n\nc = np.arange(1, 5)\nprint(c.dtype, c)\n\nint64 [1 2 3 4]\n\n\n\nc = np.arange(1.0, 5.0)\nprint(c.dtype, c)\n\nfloat64 [1. 2. 3. 4.]\n\n\n넘파이가 데이터 타입을 결정하도록 내버려 두는 대신 dtype 매개변수를 사용해서 배열을 만들 때 명시적으로 지정할 수 있습니다:\n\nd = np.arange(1, 5, dtype=np.complex64)\nprint(d.dtype, d)\n\ncomplex64 [1.+0.j 2.+0.j 3.+0.j 4.+0.j]\n\n\n가능한 데이터 타입은 int8, int16, int32, int64, uint8|16|32|64, float16|32|64, complex64|128가 있습니다. 전체 리스트는 온라인 문서를 참고하세요."
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#itemsize",
    "href": "Data_Mining/2022-03-04-numpy.html#itemsize",
    "title": "Numpy 기본",
    "section": "itemsize",
    "text": "itemsize\nitemsize 속성은 각 아이템의 크기(바이트)를 반환합니다:\n\ne = np.arange(1, 5, dtype=np.complex64)\ne.itemsize\n\n8"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#data-버퍼",
    "href": "Data_Mining/2022-03-04-numpy.html#data-버퍼",
    "title": "Numpy 기본",
    "section": "data 버퍼",
    "text": "data 버퍼\n배열의 데이터는 1차원 바이트 버퍼로 메모리에 저장됩니다. data 속성을 사용해 참조할 수 있습니다(사용할 일은 거의 없겠지만요).\n\nf = np.array([[1,2],[1000, 2000]], dtype=np.int32)\nf.data\n\n&lt;memory at 0x7f97929dd790&gt;\n\n\n파이썬 2에서는 f.data가 버퍼이고 파이썬 3에서는 memoryview입니다.\n\nif (hasattr(f.data, \"tobytes\")):\n    data_bytes = f.data.tobytes() # python 3\nelse:\n    data_bytes = memoryview(f.data).tobytes() # python 2\n\ndata_bytes\n\nb'\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\xe8\\x03\\x00\\x00\\xd0\\x07\\x00\\x00'\n\n\n여러 개의 ndarray가 데이터 버퍼를 공유할 수 있습니다. 하나를 수정하면 다른 것도 바뀝니다. 잠시 후에 예를 살펴 보겠습니다."
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#자신을-변경",
    "href": "Data_Mining/2022-03-04-numpy.html#자신을-변경",
    "title": "Numpy 기본",
    "section": "자신을 변경",
    "text": "자신을 변경\nndarray의 shape 속성을 지정하면 간단히 크기를 바꿀 수 있습니다. 배열의 원소 개수는 동일하게 유지됩니다.\n\ng = np.arange(24)\nprint(g)\nprint(\"랭크:\", g.ndim)\n\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n랭크: 1\n\n\n\ng.shape = (6, 4)\nprint(g)\nprint(\"랭크:\", g.ndim)\n\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]\n [12 13 14 15]\n [16 17 18 19]\n [20 21 22 23]]\n랭크: 2\n\n\n\ng.shape = (2, 3, 4)\nprint(g)\nprint(\"랭크:\", g.ndim)\n\n[[[ 0  1  2  3]\n  [ 4  5  6  7]\n  [ 8  9 10 11]]\n\n [[12 13 14 15]\n  [16 17 18 19]\n  [20 21 22 23]]]\n랭크: 3"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#reshape",
    "href": "Data_Mining/2022-03-04-numpy.html#reshape",
    "title": "Numpy 기본",
    "section": "reshape",
    "text": "reshape\nreshape 함수는 동일한 데이터를 가리키는 새로운 ndarray 객체를 반환합니다. 한 배열을 수정하면 다른 것도 함께 바뀝니다.\n\ng2 = g.reshape(4,6)\nprint(g2)\nprint(\"랭크:\", g2.ndim)\n\n[[ 0  1  2  3  4  5]\n [ 6  7  8  9 10 11]\n [12 13 14 15 16 17]\n [18 19 20 21 22 23]]\n랭크: 2\n\n\n행 1, 열 2의 원소를 999로 설정합니다(인덱싱 방식은 아래를 참고하세요).\n\ng2[1, 2] = 999\ng2\n\narray([[  0,   1,   2,   3,   4,   5],\n       [  6,   7, 999,   9,  10,  11],\n       [ 12,  13,  14,  15,  16,  17],\n       [ 18,  19,  20,  21,  22,  23]])\n\n\n이에 상응하는 g의 원소도 수정됩니다.\n\ng\n\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [999,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#ravel",
    "href": "Data_Mining/2022-03-04-numpy.html#ravel",
    "title": "Numpy 기본",
    "section": "ravel",
    "text": "ravel\n마지막으로 ravel 함수는 동일한 데이터를 가리키는 새로운 1차원 ndarray를 반환합니다:\n\ng.ravel()\n\narray([  0,   1,   2,   3,   4,   5,   6,   7, 999,   9,  10,  11,  12,\n        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#규칙-1",
    "href": "Data_Mining/2022-03-04-numpy.html#규칙-1",
    "title": "Numpy 기본",
    "section": "규칙 1",
    "text": "규칙 1\n배열의 랭크가 동일하지 않으면 랭크가 맞을 때까지 랭크가 작은 배열 앞에 1을 추가합니다.\n\nh = np.arange(5).reshape(1, 1, 5)\nh\n\narray([[[0, 1, 2, 3, 4]]])\n\n\n여기에 (1,1,5) 크기의 3D 배열에 (5,) 크기의 1D 배열을 더해 보죠. 브로드캐스팅의 규칙 1이 적용됩니다!\n\nh + [10, 20, 30, 40, 50]  # 다음과 동일합니다: h + [[[10, 20, 30, 40, 50]]]\n\narray([[[10, 21, 32, 43, 54]]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#규칙-2",
    "href": "Data_Mining/2022-03-04-numpy.html#규칙-2",
    "title": "Numpy 기본",
    "section": "규칙 2",
    "text": "규칙 2\n특정 차원이 1인 배열은 그 차원에서 크기가 가장 큰 배열의 크기에 맞춰 동작합니다. 배열의 원소가 차원을 따라 반복됩니다.\n\nk = np.arange(6).reshape(2, 3)\nk\n\narray([[0, 1, 2],\n       [3, 4, 5]])\n\n\n(2,3) 크기의 2D ndarray에 (2,1) 크기의 2D 배열을 더해 보죠. 넘파이는 브로드캐스팅 규칙 2를 적용합니다:\n\nk + [[100], [200]]  # 다음과 같습니다: k + [[100, 100, 100], [200, 200, 200]]\n\narray([[100, 101, 102],\n       [203, 204, 205]])\n\n\n규칙 1과 2를 합치면 다음과 같이 동작합니다:\n\nk + [100, 200, 300]  # 규칙 1 적용: [[100, 200, 300]], 규칙 2 적용: [[100, 200, 300], [100, 200, 300]]\n\narray([[100, 201, 302],\n       [103, 204, 305]])\n\n\n또 매우 간단히 다음 처럼 해도 됩니다:\n\nk + 1000  # 다음과 같습니다: k + [[1000, 1000, 1000], [1000, 1000, 1000]]\n\narray([[1000, 1001, 1002],\n       [1003, 1004, 1005]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#규칙-3",
    "href": "Data_Mining/2022-03-04-numpy.html#규칙-3",
    "title": "Numpy 기본",
    "section": "규칙 3",
    "text": "규칙 3\n규칙 1 & 2을 적용했을 때 모든 배열의 크기가 맞아야 합니다.\n\ntry:\n    k + [33, 44]\nexcept ValueError as e:\n    print(e)\n\noperands could not be broadcast together with shapes (2,3) (2,) \n\n\n브로드캐스팅 규칙은 산술 연산 뿐만 아니라 넘파이 연산에서 많이 사용됩니다. 아래에서 더 보도록 하죠. 브로드캐스팅에 관한 더 자세한 정보는 온라인 문서를 참고하세요."
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#업캐스팅",
    "href": "Data_Mining/2022-03-04-numpy.html#업캐스팅",
    "title": "Numpy 기본",
    "section": "업캐스팅",
    "text": "업캐스팅\ndtype이 다른 배열을 합칠 때 넘파이는 (실제 값에 상관없이) 모든 값을 다룰 수 있는 타입으로 업캐스팅합니다.\n\nk1 = np.arange(0, 5, dtype=np.uint8)\nprint(k1.dtype, k1)\n\nuint8 [0 1 2 3 4]\n\n\n\nk2 = k1 + np.array([5, 6, 7, 8, 9], dtype=np.int8)\nprint(k2.dtype, k2)\n\nint16 [ 5  7  9 11 13]\n\n\n모든 int8과 uint8 값(-128에서 255까지)을 표현하기 위해 int16이 필요합니다. 이 코드에서는 uint8이면 충분하지만 업캐스팅되었습니다.\n\nk3 = k1 + 1.5\nprint(k3.dtype, k3)\n\nfloat64 [1.5 2.5 3.5 4.5 5.5]"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#ndarray-메서드",
    "href": "Data_Mining/2022-03-04-numpy.html#ndarray-메서드",
    "title": "Numpy 기본",
    "section": "ndarray 메서드",
    "text": "ndarray 메서드\n일부 함수는 ndarray 메서드로 제공됩니다. 예를 들면:\n\na = np.array([[-2.5, 3.1, 7], [10, 11, 12]])\nprint(a)\nprint(\"평균 =\", a.mean())\n\n[[-2.5  3.1  7. ]\n [10.  11.  12. ]]\n평균 = 6.766666666666667\n\n\n이 명령은 크기에 상관없이 ndarray에 있는 모든 원소의 평균을 계산합니다.\n다음은 유용한 ndarray 메서드입니다:\n\nfor func in (a.min, a.max, a.sum, a.prod, a.std, a.var):\n    print(func.__name__, \"=\", func())\n\nmin = -2.5\nmax = 12.0\nsum = 40.6\nprod = -71610.0\nstd = 5.084835843520964\nvar = 25.855555555555554\n\n\n이 함수들은 선택적으로 매개변수 axis를 사용합니다. 지정된 축을 따라 원소에 연산을 적용하는데 사용합니다. 예를 들면:\n\nc=np.arange(24).reshape(2,3,4)\nc\n\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])\n\n\n\nc.sum(axis=0)  # 첫 번째 축을 따라 더함, 결과는 3x4 배열\n\narray([[12, 14, 16, 18],\n       [20, 22, 24, 26],\n       [28, 30, 32, 34]])\n\n\n\nc.sum(axis=1)  # 두 번째 축을 따라 더함, 결과는 2x4 배열\n\narray([[12, 15, 18, 21],\n       [48, 51, 54, 57]])\n\n\n여러 축에 대해서 더할 수도 있습니다:\n\nc.sum(axis=(0,2))  # 첫 번째 축과 세 번째 축을 따라 더함, 결과는 (3,) 배열\n\narray([ 60,  92, 124])\n\n\n\n0+1+2+3 + 12+13+14+15, 4+5+6+7 + 16+17+18+19, 8+9+10+11 + 20+21+22+23\n\n(60, 92, 124)"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#일반-함수",
    "href": "Data_Mining/2022-03-04-numpy.html#일반-함수",
    "title": "Numpy 기본",
    "section": "일반 함수",
    "text": "일반 함수\n넘파이는 일반 함수(universal function) 또는 ufunc라고 부르는 원소별 함수를 제공합니다. 예를 들면 square 함수는 원본 ndarray를 복사하여 각 원소를 제곱한 새로운 ndarray 객체를 반환합니다:\n\na = np.array([[-2.5, 3.1, 7], [10, 11, 12]])\nnp.square(a)\n\narray([[  6.25,   9.61,  49.  ],\n       [100.  , 121.  , 144.  ]])\n\n\n다음은 유용한 단항 일반 함수들입니다:\n\nprint(\"원본 ndarray\")\nprint(a)\nfor func in (np.abs, np.sqrt, np.exp, np.log, np.sign, np.ceil, np.modf, np.isnan, np.cos):\n    print(\"\\n\", func.__name__)\n    print(func(a))\n\n원본 ndarray\n[[-2.5  3.1  7. ]\n [10.  11.  12. ]]\n\n absolute\n[[ 2.5  3.1  7. ]\n [10.  11.  12. ]]\n\n sqrt\n[[       nan 1.76068169 2.64575131]\n [3.16227766 3.31662479 3.46410162]]\n\n exp\n[[8.20849986e-02 2.21979513e+01 1.09663316e+03]\n [2.20264658e+04 5.98741417e+04 1.62754791e+05]]\n\n log\n[[       nan 1.13140211 1.94591015]\n [2.30258509 2.39789527 2.48490665]]\n\n sign\n[[-1.  1.  1.]\n [ 1.  1.  1.]]\n\n ceil\n[[-2.  4.  7.]\n [10. 11. 12.]]\n\n modf\n(array([[-0.5,  0.1,  0. ],\n       [ 0. ,  0. ,  0. ]]), array([[-2.,  3.,  7.],\n       [10., 11., 12.]]))\n\n isnan\n[[False False False]\n [False False False]]\n\n cos\n[[-0.80114362 -0.99913515  0.75390225]\n [-0.83907153  0.0044257   0.84385396]]\n\n\nRuntimeWarning: invalid value encountered in sqrt\n  print(func(a))\n&lt;ipython-input-59-d791c8e37e6f&gt;:5: RuntimeWarning: invalid value encountered in log\n  print(func(a))"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#이항-일반-함수",
    "href": "Data_Mining/2022-03-04-numpy.html#이항-일반-함수",
    "title": "Numpy 기본",
    "section": "이항 일반 함수",
    "text": "이항 일반 함수\n두 개의 ndarray에 원소별로 적용되는 이항 함수도 많습니다. 두 배열이 동일한 크기가 아니면 브로드캐스팅 규칙이 적용됩니다:\n\na = np.array([1, -2, 3, 4])\nb = np.array([2, 8, -1, 7])\nnp.add(a, b)  # a + b 와 동일\n\narray([ 3,  6,  2, 11])\n\n\n\nnp.greater(a, b)  # a &gt; b 와 동일\n\narray([False, False,  True, False])\n\n\n\nnp.maximum(a, b)\n\narray([2, 8, 3, 7])\n\n\n\nnp.copysign(a, b)\n\narray([ 1.,  2., -3.,  4.])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#차원-배열",
    "href": "Data_Mining/2022-03-04-numpy.html#차원-배열",
    "title": "Numpy 기본",
    "section": "1차원 배열",
    "text": "1차원 배열\n1차원 넘파이 배열은 보통의 파이썬 배열과 비슷하게 사용할 수 있습니다:\n\na = np.array([1, 5, 3, 19, 13, 7, 3])\na[3]\n\n19\n\n\n\na[2:5]\n\narray([ 3, 19, 13])\n\n\n\na[2:-1]\n\narray([ 3, 19, 13,  7])\n\n\n\na[:2]\n\narray([1, 5])\n\n\n\na[2::2]\n\narray([ 3, 13,  3])\n\n\n\na[::-1]\n\narray([ 3,  7, 13, 19,  3,  5,  1])\n\n\n물론 원소를 수정할 수 있죠:\n\na[3]=999\na\n\narray([  1,   5,   3, 999,  13,   7,   3])\n\n\n슬라이싱을 사용해 ndarray를 수정할 수 있습니다:\n\na[2:5] = [997, 998, 999]\na\n\narray([  1,   5, 997, 998, 999,   7,   3])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#보통의-파이썬-배열과-차이점",
    "href": "Data_Mining/2022-03-04-numpy.html#보통의-파이썬-배열과-차이점",
    "title": "Numpy 기본",
    "section": "보통의 파이썬 배열과 차이점",
    "text": "보통의 파이썬 배열과 차이점\n보통의 파이썬 배열과 대조적으로 ndarray 슬라이싱에 하나의 값을 할당하면 슬라이싱 전체에 복사됩니다. 위에서 언급한 브로드캐스팅 덕택입니다.\n\na[2:5] = -1\na\n\narray([ 1,  5, -1, -1, -1,  7,  3])\n\n\n또한 이런 식으로 ndarray 크기를 늘리거나 줄일 수 없습니다:\n\ntry:\n    a[2:5] = [1,2,3,4,5,6]  # 너무 길어요\nexcept ValueError as e:\n    print(e)\n\ncannot copy sequence with size 6 to array axis with dimension 3\n\n\n원소를 삭제할 수도 없습니다:\n\ntry:\n    del a[2:5]\nexcept ValueError as e:\n    print(e)\n\ncannot delete array elements\n\n\n중요한 점은 ndarray의 슬라이싱은 같은 데이터 버퍼를 바라보는 뷰(view)입니다. 슬라이싱된 객체를 수정하면 실제 원본 ndarray가 수정됩니다!\n\na_slice = a[2:6]\na_slice[1] = 1000\na  # 원본 배열이 수정됩니다!\n\narray([   1,    5,   -1, 1000,   -1,    7,    3])\n\n\n\na[3] = 2000\na_slice  # 비슷하게 원본 배열을 수정하면 슬라이싱 객체에도 반영됩니다!\n\narray([  -1, 2000,   -1,    7])\n\n\n데이터를 복사하려면 copy 메서드를 사용해야 합니다:\n\nanother_slice = a[2:6].copy()\nanother_slice[1] = 3000\na  # 원본 배열이 수정되지 않습니다\n\narray([   1,    5,   -1, 2000,   -1,    7,    3])\n\n\n\na[3] = 4000\nanother_slice  # 마찬가지로 원본 배열을 수정해도 복사된 배열은 바뀌지 않습니다\n\narray([  -1, 3000,   -1,    7])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#다차원-배열",
    "href": "Data_Mining/2022-03-04-numpy.html#다차원-배열",
    "title": "Numpy 기본",
    "section": "다차원 배열",
    "text": "다차원 배열\n다차원 배열은 비슷한 방식으로 각 축을 따라 인덱싱 또는 슬라이싱해서 사용합니다. 콤마로 구분합니다:\n\nb = np.arange(48).reshape(4, 12)\nb\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\n\n\n\nb[1, 2]  # 행 1, 열 2\n\n14\n\n\n\nb[1, :]  # 행 1, 모든 열\n\narray([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\n\n\n\nb[:, 1]  # 모든 행, 열 1\n\narray([ 1, 13, 25, 37])\n\n\n주의: 다음 두 표현에는 미묘한 차이가 있습니다:\n\nb[1, :]\n\narray([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\n\n\n\nb[1:2, :]\n\narray([[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]])\n\n\n첫 번째 표현식은 (12,) 크기인 1D 배열로 행이 하나입니다. 두 번째는 (1, 12) 크기인 2D 배열로 같은 행을 반환합니다."
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#팬시-인덱싱fancy-indexing",
    "href": "Data_Mining/2022-03-04-numpy.html#팬시-인덱싱fancy-indexing",
    "title": "Numpy 기본",
    "section": "팬시 인덱싱(Fancy indexing)",
    "text": "팬시 인덱싱(Fancy indexing)\n관심 대상의 인덱스 리스트를 지정할 수도 있습니다. 이를 팬시 인덱싱이라고 부릅니다.\n\nb[(0,2), 2:5]  # 행 0과 2, 열 2에서 4(5-1)까지\n\narray([[ 2,  3,  4],\n       [26, 27, 28]])\n\n\n\nb[:, (-1, 2, -1)]  # 모든 행, 열 -1 (마지막), 2와 -1 (다시 반대 방향으로)\n\narray([[11,  2, 11],\n       [23, 14, 23],\n       [35, 26, 35],\n       [47, 38, 47]])\n\n\n여러 개의 인덱스 리스트를 지정하면 인덱스에 맞는 값이 포함된 1D ndarray를 반환됩니다.\n\nb[(-1, 2, -1, 2), (5, 9, 1, 9)]  # returns a 1D array with b[-1, 5], b[2, 9], b[-1, 1] and b[2, 9] (again)\n\narray([41, 33, 37, 33])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#고차원",
    "href": "Data_Mining/2022-03-04-numpy.html#고차원",
    "title": "Numpy 기본",
    "section": "고차원",
    "text": "고차원\n고차원에서도 동일한 방식이 적용됩니다. 몇 가지 예를 살펴 보겠습니다:\n\nc = b.reshape(4,2,6)\nc\n\narray([[[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11]],\n\n       [[12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23]],\n\n       [[24, 25, 26, 27, 28, 29],\n        [30, 31, 32, 33, 34, 35]],\n\n       [[36, 37, 38, 39, 40, 41],\n        [42, 43, 44, 45, 46, 47]]])\n\n\n\nc[2, 1, 4]  # 행렬 2, 행 1, 열 4\n\n34\n\n\n\nc[2, :, 3]  # 행렬 2, 모든 행, 열 3\n\narray([27, 33])\n\n\n어떤 축에 대한 인덱스를 지정하지 않으면 이 축의 모든 원소가 반환됩니다:\n\nc[2, 1]  # 행렬 2, 행 1, 모든 열이 반환됩니다. c[2, 1, :]와 동일합니다.\n\narray([30, 31, 32, 33, 34, 35])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#생략-부호-...",
    "href": "Data_Mining/2022-03-04-numpy.html#생략-부호-...",
    "title": "Numpy 기본",
    "section": "생략 부호 (...)",
    "text": "생략 부호 (...)\n생략 부호(...)를 쓰면 모든 지정하지 않은 축의 원소를 포함합니다.\n\nc[2, ...]  #  행렬 2, 모든 행, 모든 열. c[2, :, :]와 동일\n\narray([[24, 25, 26, 27, 28, 29],\n       [30, 31, 32, 33, 34, 35]])\n\n\n\nc[2, 1, ...]  # 행렬 2, 행 1, 모든 열. c[2, 1, :]와 동일\n\narray([30, 31, 32, 33, 34, 35])\n\n\n\nc[2, ..., 3]  # 행렬 2, 모든 행, 열 3. c[2, :, 3]와 동일\n\narray([27, 33])\n\n\n\nc[..., 3]  # 모든 행렬, 모든 행, 열 3. c[:, :, 3]와 동일\n\narray([[ 3,  9],\n       [15, 21],\n       [27, 33],\n       [39, 45]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#불리언-인덱싱",
    "href": "Data_Mining/2022-03-04-numpy.html#불리언-인덱싱",
    "title": "Numpy 기본",
    "section": "불리언 인덱싱",
    "text": "불리언 인덱싱\n불리언 값을 가진 ndarray를 사용해 축의 인덱스를 지정할 수 있습니다.\n\nb = np.arange(48).reshape(4, 12)\nb\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\n\n\n\nrows_on = np.array([True, False, True, False])\nb[rows_on, :]  # 행 0과 2, 모든 열. b[(0, 2), :]와 동일\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]])\n\n\n\ncols_on = np.array([False, True, False] * 4)\nb[:, cols_on]  # 모든 행, 열 1, 4, 7, 10\n\narray([[ 1,  4,  7, 10],\n       [13, 16, 19, 22],\n       [25, 28, 31, 34],\n       [37, 40, 43, 46]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.ix_",
    "href": "Data_Mining/2022-03-04-numpy.html#np.ix_",
    "title": "Numpy 기본",
    "section": "np.ix_",
    "text": "np.ix_\n여러 축에 걸쳐서는 불리언 인덱싱을 사용할 수 없고 ix_ 함수를 사용합니다:\n\nb[np.ix_(rows_on, cols_on)]\n\narray([[ 1,  4,  7, 10],\n       [25, 28, 31, 34]])\n\n\n\nnp.ix_(rows_on, cols_on)\n\n(array([[0],\n        [2]]),\n array([[ 1,  4,  7, 10]]))\n\n\nndarray와 같은 크기의 불리언 배열을 사용하면 해당 위치가 True인 모든 원소를 담은 1D 배열이 반환됩니다. 일반적으로 조건 연산자와 함께 사용합니다:\n\nb[b % 3 == 1]\n\narray([ 1,  4,  7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#vstack",
    "href": "Data_Mining/2022-03-04-numpy.html#vstack",
    "title": "Numpy 기본",
    "section": "vstack",
    "text": "vstack\nvstack 함수를 사용하여 수직으로 쌓아보죠:\n\nq4 = np.vstack((q1, q2, q3))\nq4\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.]])\n\n\n\nq4.shape\n\n(10, 4)\n\n\nq1, q2, q3가 모두 같은 크기이므로 가능합니다(수직으로 쌓기 때문에 수직 축은 크기가 달라도 됩니다)."
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#hstack",
    "href": "Data_Mining/2022-03-04-numpy.html#hstack",
    "title": "Numpy 기본",
    "section": "hstack",
    "text": "hstack\nhstack을 사용해 수평으로도 쌓을 수 있습니다:\n\nq5 = np.hstack((q1, q3))\nq5\n\narray([[1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.]])\n\n\n\nq5.shape\n\n(3, 8)\n\n\nq1과 q3가 모두 3개의 행을 가지고 있기 때문에 가능합니다. q2는 4개의 행을 가지고 있기 때문에 q1, q3와 수평으로 쌓을 수 없습니다:\n\ntry:\n    q5 = np.hstack((q1, q2, q3))\nexcept ValueError as e:\n    print(e)\n\nall the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 3 and the array at index 1 has size 4"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#concatenate",
    "href": "Data_Mining/2022-03-04-numpy.html#concatenate",
    "title": "Numpy 기본",
    "section": "concatenate",
    "text": "concatenate\nconcatenate 함수는 지정한 축으로도 배열을 쌓습니다.\n\nq7 = np.concatenate((q1, q2, q3), axis=0)  # vstack과 동일\nq7\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.]])\n\n\n\nq7.shape\n\n(10, 4)\n\n\n예상했겠지만 hstack은 axis=1으로 concatenate를 호출하는 것과 같습니다."
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#stack",
    "href": "Data_Mining/2022-03-04-numpy.html#stack",
    "title": "Numpy 기본",
    "section": "stack",
    "text": "stack\nstack 함수는 새로운 축을 따라 배열을 쌓습니다. 모든 배열은 같은 크기를 가져야 합니다.\n\nq8 = np.stack((q1, q3))\nq8\n\narray([[[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]],\n\n       [[3., 3., 3., 3.],\n        [3., 3., 3., 3.],\n        [3., 3., 3., 3.]]])\n\n\n\nq8.shape\n\n(2, 3, 4)"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#행렬-전치",
    "href": "Data_Mining/2022-03-04-numpy.html#행렬-전치",
    "title": "Numpy 기본",
    "section": "행렬 전치",
    "text": "행렬 전치\nT 속성은 랭크가 2보다 크거나 같을 때 transpose()를 호출하는 것과 같습니다:\n\nm1 = np.arange(10).reshape(2,5)\nm1\n\narray([[0, 1, 2, 3, 4],\n       [5, 6, 7, 8, 9]])\n\n\n\nm1.T\n\narray([[0, 5],\n       [1, 6],\n       [2, 7],\n       [3, 8],\n       [4, 9]])\n\n\nT 속성은 랭크가 0이거나 1인 배열에는 아무런 영향을 미치지 않습니다:\n\nm2 = np.arange(5)\nm2\n\narray([0, 1, 2, 3, 4])\n\n\n\nm2.T\n\narray([0, 1, 2, 3, 4])\n\n\n먼저 1D 배열을 하나의 행이 있는 행렬(2D)로 바꾼다음 전치를 수행할 수 있습니다:\n\nm2r = m2.reshape(1,5)\nm2r\n\narray([[0, 1, 2, 3, 4]])\n\n\n\nm2r.T\n\narray([[0],\n       [1],\n       [2],\n       [3],\n       [4]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#행렬-곱셈",
    "href": "Data_Mining/2022-03-04-numpy.html#행렬-곱셈",
    "title": "Numpy 기본",
    "section": "행렬 곱셈",
    "text": "행렬 곱셈\n두 개의 행렬을 만들어 dot 메서드로 행렬 곱셈을 실행해 보죠.\n\nn1 = np.arange(10).reshape(2, 5)\nn1\n\narray([[0, 1, 2, 3, 4],\n       [5, 6, 7, 8, 9]])\n\n\n\nn2 = np.arange(15).reshape(5,3)\nn2\n\narray([[ 0,  1,  2],\n       [ 3,  4,  5],\n       [ 6,  7,  8],\n       [ 9, 10, 11],\n       [12, 13, 14]])\n\n\n\nn1.dot(n2)\n\narray([[ 90, 100, 110],\n       [240, 275, 310]])\n\n\n주의: 앞서 언급한 것처럼 n1*n2는 행렬 곱셈이 아니라 원소별 곱셈(또는 아다마르 곱이라 부릅니다)입니다."
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#역행렬과-유사-역행렬",
    "href": "Data_Mining/2022-03-04-numpy.html#역행렬과-유사-역행렬",
    "title": "Numpy 기본",
    "section": "역행렬과 유사 역행렬",
    "text": "역행렬과 유사 역행렬\nnumpy.linalg 모듈 안에 많은 선형 대수 함수들이 있습니다. 특히 inv 함수는 정방 행렬의 역행렬을 계산합니다:\n\nimport numpy.linalg as linalg\n\nm3 = np.array([[1,2,3],[5,7,11],[21,29,31]])\nm3\n\narray([[ 1,  2,  3],\n       [ 5,  7, 11],\n       [21, 29, 31]])\n\n\n\nlinalg.inv(m3)\n\narray([[-2.31818182,  0.56818182,  0.02272727],\n       [ 1.72727273, -0.72727273,  0.09090909],\n       [-0.04545455,  0.29545455, -0.06818182]])\n\n\npinv 함수를 사용하여 유사 역행렬을 계산할 수도 있습니다:\n\nlinalg.pinv(m3)\n\narray([[-2.31818182,  0.56818182,  0.02272727],\n       [ 1.72727273, -0.72727273,  0.09090909],\n       [-0.04545455,  0.29545455, -0.06818182]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#단위-행렬",
    "href": "Data_Mining/2022-03-04-numpy.html#단위-행렬",
    "title": "Numpy 기본",
    "section": "단위 행렬",
    "text": "단위 행렬\n행렬과 그 행렬의 역행렬을 곱하면 단위 행렬이 됩니다(작은 소숫점 오차가 있습니다):\n\nm3.dot(linalg.inv(m3))\n\narray([[ 1.00000000e+00, -1.66533454e-16,  0.00000000e+00],\n       [ 6.31439345e-16,  1.00000000e+00, -1.38777878e-16],\n       [ 5.21110932e-15, -2.38697950e-15,  1.00000000e+00]])\n\n\neye 함수는 NxN 크기의 단위 행렬을 만듭니다:\n\nnp.eye(3)\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#qr-분해",
    "href": "Data_Mining/2022-03-04-numpy.html#qr-분해",
    "title": "Numpy 기본",
    "section": "QR 분해",
    "text": "QR 분해\nqr 함수는 행렬을 QR 분해합니다:\n\nq, r = linalg.qr(m3)\nq\n\narray([[-0.04627448,  0.98786672,  0.14824986],\n       [-0.23137241,  0.13377362, -0.96362411],\n       [-0.97176411, -0.07889213,  0.22237479]])\n\n\n\nr\n\narray([[-21.61018278, -29.89331494, -32.80860727],\n       [  0.        ,   0.62427688,   1.9894538 ],\n       [  0.        ,   0.        ,  -3.26149699]])\n\n\n\nq.dot(r)  # q.r는 m3와 같습니다\n\narray([[ 1.,  2.,  3.],\n       [ 5.,  7., 11.],\n       [21., 29., 31.]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#행렬식",
    "href": "Data_Mining/2022-03-04-numpy.html#행렬식",
    "title": "Numpy 기본",
    "section": "행렬식",
    "text": "행렬식\ndet 함수는 행렬식을 계산합니다:\n\nlinalg.det(m3)  # 행렬식 계산\n\n43.99999999999997"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#고윳값과-고유벡터",
    "href": "Data_Mining/2022-03-04-numpy.html#고윳값과-고유벡터",
    "title": "Numpy 기본",
    "section": "고윳값과 고유벡터",
    "text": "고윳값과 고유벡터\neig 함수는 정방 행렬의 고윳값과 고유벡터를 계산합니다:\n\neigenvalues, eigenvectors = linalg.eig(m3)\neigenvalues # λ\n\narray([42.26600592, -0.35798416, -2.90802176])\n\n\n\neigenvectors # v\n\narray([[-0.08381182, -0.76283526, -0.18913107],\n       [-0.3075286 ,  0.64133975, -0.6853186 ],\n       [-0.94784057, -0.08225377,  0.70325518]])\n\n\n\nm3.dot(eigenvectors) - eigenvalues * eigenvectors  # m3.v - λ*v = 0\n\narray([[ 8.88178420e-15,  2.22044605e-16, -3.10862447e-15],\n       [ 3.55271368e-15,  2.02615702e-15, -1.11022302e-15],\n       [ 3.55271368e-14,  3.33413852e-15, -8.43769499e-15]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#특잇값-분해",
    "href": "Data_Mining/2022-03-04-numpy.html#특잇값-분해",
    "title": "Numpy 기본",
    "section": "특잇값 분해",
    "text": "특잇값 분해\nsvd 함수는 행렬을 입력으로 받아 그 행렬의 특잇값 분해를 반환합니다:\n\nm4 = np.array([[1,0,0,0,2], [0,0,3,0,0], [0,0,0,0,0], [0,2,0,0,0]])\nm4\n\narray([[1, 0, 0, 0, 2],\n       [0, 0, 3, 0, 0],\n       [0, 0, 0, 0, 0],\n       [0, 2, 0, 0, 0]])\n\n\n\nU, S_diag, V = linalg.svd(m4)\nU\n\narray([[ 0.,  1.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0., -1.],\n       [ 0.,  0.,  1.,  0.]])\n\n\n\nS_diag\n\narray([3.        , 2.23606798, 2.        , 0.        ])\n\n\nsvd 함수는 Σ의 대각 원소 값만 반환합니다. 전체 Σ 행렬은 다음과 같이 만듭니다:\n\nS = np.zeros((4, 5))\nS[np.diag_indices(4)] = S_diag\nS  # Σ\n\narray([[3.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 2.23606798, 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 2.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ]])\n\n\n\nV\n\narray([[-0.        ,  0.        ,  1.        , -0.        ,  0.        ],\n       [ 0.4472136 ,  0.        ,  0.        ,  0.        ,  0.89442719],\n       [-0.        ,  1.        ,  0.        , -0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ],\n       [-0.89442719,  0.        ,  0.        ,  0.        ,  0.4472136 ]])\n\n\n\nU.dot(S).dot(V) # U.Σ.V == m4\n\narray([[1., 0., 0., 0., 2.],\n       [0., 0., 3., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 2., 0., 0., 0.]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#대각원소와-대각합",
    "href": "Data_Mining/2022-03-04-numpy.html#대각원소와-대각합",
    "title": "Numpy 기본",
    "section": "대각원소와 대각합",
    "text": "대각원소와 대각합\n\nnp.diag(m3)  # m3의 대각 원소입니다(왼쪽 위에서 오른쪽 아래)\n\narray([ 1,  7, 31])\n\n\n\nnp.trace(m3)  # np.diag(m3).sum()와 같습니다\n\n39"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#선형-방정식-풀기",
    "href": "Data_Mining/2022-03-04-numpy.html#선형-방정식-풀기",
    "title": "Numpy 기본",
    "section": "선형 방정식 풀기",
    "text": "선형 방정식 풀기\nsolve 함수는 다음과 같은 선형 방정식을 풉니다:\n\n\\(2x + 6y = 6\\)\n\\(5x + 3y = -9\\)\n\n\ncoeffs  = np.array([[2, 6], [5, 3]])\ndepvars = np.array([6, -9])\nsolution = linalg.solve(coeffs, depvars)\nsolution\n\narray([-3.,  2.])\n\n\nsolution을 확인해 보죠:\n\ncoeffs.dot(solution), depvars  # 네 같네요\n\n(array([ 6., -9.]), array([ 6, -9]))\n\n\n좋습니다! 다른 방식으로도 solution을 확인해 보죠:\n\nnp.allclose(coeffs.dot(solution), depvars)\n\nTrue"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#바이너리-.npy-포맷",
    "href": "Data_Mining/2022-03-04-numpy.html#바이너리-.npy-포맷",
    "title": "Numpy 기본",
    "section": "바이너리 .npy 포맷",
    "text": "바이너리 .npy 포맷\n랜덤 배열을 만들고 저장해 보죠.\n\na = np.random.rand(2,3)\na\n\narray([[0.5435938 , 0.92886307, 0.01535158],\n       [0.4157283 , 0.9102127 , 0.55129708]])\n\n\n\nnp.save(\"my_array\", a)\n\n끝입니다! 파일 이름의 확장자를 지정하지 않았기 때문에 넘파이는 자동으로 .npy를 붙입니다. 파일 내용을 확인해 보겠습니다:\n\nwith open(\"my_array.npy\", \"rb\") as f:\n    content = f.read()\n\ncontent\n\nb\"\\x93NUMPY\\x01\\x00v\\x00{'descr': '&lt;f8', 'fortran_order': False, 'shape': (2, 3), }                                                          \\nY\\xc1\\xfc\\xd0\\x1ee\\xe1?\\xde{3\\t?\\xb9\\xed?\\x80V\\x08\\xef\\xa5p\\x8f?\\x96I}\\xe0J\\x9b\\xda?\\xe0U\\xfaav \\xed?\\xd8\\xe50\\xc59\\xa4\\xe1?\"\n\n\n이 파일을 넘파이 배열로 로드하려면 load 함수를 사용합니다:\n\na_loaded = np.load(\"my_array.npy\")\na_loaded\n\narray([[0.5435938 , 0.92886307, 0.01535158],\n       [0.4157283 , 0.9102127 , 0.55129708]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#텍스트-포맷",
    "href": "Data_Mining/2022-03-04-numpy.html#텍스트-포맷",
    "title": "Numpy 기본",
    "section": "텍스트 포맷",
    "text": "텍스트 포맷\n배열을 텍스트 포맷으로 저장해 보죠:\n\nnp.savetxt(\"my_array.csv\", a)\n\n파일 내용을 확인해 보겠습니다:\n\nwith open(\"my_array.csv\", \"rt\") as f:\n    print(f.read())\n\n5.435937959464737235e-01 9.288630656918674955e-01 1.535157809943688001e-02\n4.157283012656532994e-01 9.102126992826775620e-01 5.512970782648904944e-01\n\n\n\n이 파일은 탭으로 구분된 CSV 파일입니다. 다른 구분자를 지정할 수도 있습니다:\n\nnp.savetxt(\"my_array.csv\", a, delimiter=\",\")\n\n이 파일을 로드하려면 loadtxt 함수를 사용합니다:\n\na_loaded = np.loadtxt(\"my_array.csv\", delimiter=\",\")\na_loaded\n\narray([[0.5435938 , 0.92886307, 0.01535158],\n       [0.4157283 , 0.9102127 , 0.55129708]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#압축된-.npz-포맷",
    "href": "Data_Mining/2022-03-04-numpy.html#압축된-.npz-포맷",
    "title": "Numpy 기본",
    "section": "압축된 .npz 포맷",
    "text": "압축된 .npz 포맷\n여러 개의 배열을 압축된 한 파일로 저장하는 것도 가능합니다:\n\nb = np.arange(24, dtype=np.uint8).reshape(2, 3, 4)\nb\n\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]], dtype=uint8)\n\n\n\nnp.savez(\"my_arrays\", my_a=a, my_b=b)\n\n파일 내용을 확인해 보죠. .npz 파일 확장자가 자동으로 추가되었습니다.\n\nwith open(\"my_arrays.npz\", \"rb\") as f:\n    content = f.read()\n\nrepr(content)[:180] + \"[...]\"\n\n'b\"PK\\\\x03\\\\x04\\\\x14\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00!\\\\x00\\\\x063\\\\xcf\\\\xb9\\\\xb0\\\\x00\\\\x00\\\\x00\\\\xb0\\\\x00\\\\x00\\\\x00\\\\x08\\\\x00\\\\x14\\\\x00my_a.npy\\\\x01\\\\x00\\\\x10\\\\x00\\\\xb0\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\xb0\\\\x00\\\\x00\\\\x[...]'\n\n\n다음과 같이 이 파일을 로드할 수 있습니다:\n\nmy_arrays = np.load(\"my_arrays.npz\")\nmy_arrays\n\n&lt;numpy.lib.npyio.NpzFile at 0x7f9791c73d60&gt;\n\n\n게으른 로딩을 수행하는 딕셔너리와 유사한 객체입니다:\n\nmy_arrays.keys()\n\nKeysView(&lt;numpy.lib.npyio.NpzFile object at 0x7f9791c73d60&gt;)\n\n\n\nmy_arrays[\"my_a\"]\n\narray([[0.5435938 , 0.92886307, 0.01535158],\n       [0.4157283 , 0.9102127 , 0.55129708]])"
  },
  {
    "objectID": "Data_Mining/2022-05-07-matplotlib and seaborn.html",
    "href": "Data_Mining/2022-05-07-matplotlib and seaborn.html",
    "title": "Visualization_with_seaborn_and_matplotlib",
    "section": "",
    "text": "seaborn과 matplotlib의 시각화\n코드 출처 - 이제현님 블로그"
  },
  {
    "objectID": "Data_Mining/2022-05-07-matplotlib and seaborn.html#visualization-with-seaborn",
    "href": "Data_Mining/2022-05-07-matplotlib and seaborn.html#visualization-with-seaborn",
    "title": "Visualization_with_seaborn_and_matplotlib",
    "section": "Visualization with seaborn",
    "text": "Visualization with seaborn\nseaborn은 python의 시각화 라이브러리인 matplolib를 기반으로 제작된 라이브러리입니다.\n\nimport seaborn as sns\nsns.set()\nsns.set(style=\"darkgrid\")\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nplt.rcParams['figure.figsize']=(5,5) #figure의 사이즈를 설정이 가능하다 \n\n\nLoding dataset\n\n# 사용할 데이터 불러오기 \ndata_BM = pd.read_csv('bigmart_data.csv')\ndata_BM = data_BM.dropna(how=\"any\") #NA값 제거\ndata_BM[\"Visibility_Scaled\"] = data_BM[\"Item_Visibility\"] * 100 #Visibility_Scaled 컬럼의 값들에 100 곱해줌\ndata_BM.head()\n\n\n\n\n\n\n\n\nItem_Identifier\nItem_Weight\nItem_Fat_Content\nItem_Visibility\nItem_Type\nItem_MRP\nOutlet_Identifier\nOutlet_Establishment_Year\nOutlet_Size\nOutlet_Location_Type\nOutlet_Type\nItem_Outlet_Sales\nVisibility_Scaled\n\n\n\n\n0\nFDA15\n9.300\nLow Fat\n0.016047\nDairy\n249.8092\nOUT049\n1999\nMedium\nTier 1\nSupermarket Type1\n3735.1380\n1.604730\n\n\n1\nDRC01\n5.920\nRegular\n0.019278\nSoft Drinks\n48.2692\nOUT018\n2009\nMedium\nTier 3\nSupermarket Type2\n443.4228\n1.927822\n\n\n2\nFDN15\n17.500\nLow Fat\n0.016760\nMeat\n141.6180\nOUT049\n1999\nMedium\nTier 1\nSupermarket Type1\n2097.2700\n1.676007\n\n\n4\nNCD19\n8.930\nLow Fat\n0.000000\nHousehold\n53.8614\nOUT013\n1987\nHigh\nTier 3\nSupermarket Type1\n994.7052\n0.000000\n\n\n5\nFDP36\n10.395\nRegular\n0.000000\nBaking Goods\n51.4008\nOUT018\n2009\nMedium\nTier 3\nSupermarket Type2\n556.6088\n0.000000\n\n\n\n\n\n\n\n\ndata_BM.describe() #이상치가 있는지 확인\n\n\n\n\n\n\n\n\nItem_Weight\nItem_Visibility\nItem_MRP\nOutlet_Establishment_Year\nItem_Outlet_Sales\nVisibility_Scaled\n\n\n\n\ncount\n4650.000000\n4650.000000\n4650.000000\n4650.000000\n4650.000000\n4650.000000\n\n\nmean\n12.898675\n0.060700\n141.716328\n1999.190538\n2272.037489\n6.070048\n\n\nstd\n4.670973\n0.044607\n62.420534\n7.388800\n1497.964740\n4.460652\n\n\nmin\n4.555000\n0.000000\n31.490000\n1987.000000\n69.243200\n0.000000\n\n\n25%\n8.770000\n0.025968\n94.409400\n1997.000000\n1125.202000\n2.596789\n\n\n50%\n12.650000\n0.049655\n142.979900\n1999.000000\n1939.808300\n4.965549\n\n\n75%\n17.000000\n0.088736\n186.614150\n2004.000000\n3111.616300\n8.873565\n\n\nmax\n21.350000\n0.188323\n266.888400\n2009.000000\n10256.649000\n18.832266"
  },
  {
    "objectID": "Data_Mining/2022-05-07-matplotlib and seaborn.html#creating-basic-plots",
    "href": "Data_Mining/2022-05-07-matplotlib and seaborn.html#creating-basic-plots",
    "title": "Visualization_with_seaborn_and_matplotlib",
    "section": "1. Creating basic plots",
    "text": "1. Creating basic plots\nmatplotlib에서 여러 줄이 필요한 한 줄로 seaborn에서 몇 가지 기본 플롯을 만드는 방법을 살펴보겠습니다.\n\nLine Chart\n\n일부 데이터 세트의 경우 한 변수의 변화를 시간의 함수로 이해하거나 이와 유사한 연속 변수를 이해하고자 할 수 있습니다.\nseaborn에서 이는 lineplot() 함수로 직접 또는 kind=\"line\":을 설정하여 relplot()으로 수행할 수 있습니다.\n\n\nsns.lineplot(x=\"Item_Weight\", y=\"Item_MRP\",data=data_BM[:50]); #처음부터 50번째 까지의 데이터만 사용한다\n\n\n\n\n\n\nBar Chart\n\nSeaborn에서는 barplot 기능을 사용하여 간단하게 막대 차트를 생성할 수 있습니다.\nmatplotlib에서 동일한 결과를 얻으려면 데이터 범주를 현명하게 그룹화하기 위해 추가 코드를 작성해야 했습니다.\n그리고 나서 플롯이 올바르게 나오도록 훨씬 더 많은 코드를 작성해야 했습니다.\n\n\nsns.barplot(x=\"Item_Type\", y=\"Item_MRP\", data=data_BM[:5])\n\n&lt;AxesSubplot:xlabel='Item_Type', ylabel='Item_MRP'&gt;\n\n\n\n\n\n\n\nHistogram\n\ndistplot()을 사용하여 seaborn에서 히스토그램을 만들 수 있습니다. 사용할 수 있는 여러 옵션이 있으며 노트북에서 더 자세히 살펴보겠습니다.\n\n\nsns.distplot(data_BM['Item_MRP'])\n\n&lt;AxesSubplot:xlabel='Item_MRP', ylabel='Density'&gt;\n\n\n\n\n\n\n\nBox plots\n\nSeaborn에서 boxplot을 생성하기 위해 boxplot()을 사용할 수 있습니다.\n\n\nsns.boxplot(data_BM['Item_Outlet_Sales'], orient='vertical') \n\n&lt;AxesSubplot:xlabel='Item_Outlet_Sales'&gt;\n\n\n\n\n\n\n\nViolin plot\n\n바이올린 플롯은 상자 및 수염 플롯과 유사한 역할을 합니다.\n이는 하나(또는 그 이상) 범주형 변수의 여러 수준에 걸친 정량적 데이터의 분포를 보여줌으로써 해당 분포를 비교할 수 있습니다.\n모든 플롯 구성 요소가 실제 데이터 포인트에 해당하는 상자 플롯과 달리 바이올린 플롯은 기본 분포의 커널 밀도 추정을 특징으로 합니다.\nseaborn에서 violinplot()을 사용하여 바이올린 플롯을 만들 수 있습니다.\n\n\nsns.violinplot(data_BM['Item_Outlet_Sales'], orient='vertical', color='skyblue')\n\n&lt;AxesSubplot:xlabel='Item_Outlet_Sales'&gt;\n\n\n\n\n\n\n\nScatter plot\n\n각 포인트는 데이터 세트의 관찰을 나타내는 포인트 클라우드를 사용하여 두 변수의 분포를 나타냅니다.\n이 묘사를 통해 눈은 그들 사이에 의미 있는 관계가 있는지 여부에 대한 상당한 양의 정보를 추론할 수 있습니다.\nrelplot()을 kind=scatter 옵션과 함께 사용하여 seaborn에서 산점도를 그릴 수 있습니다.\n\n\nsns.relplot(x=\"Item_MRP\", y=\"Item_Outlet_Sales\", data=data_BM[:200], kind=\"scatter\"); \n#sns.relplot(x=\"Item_MRP\", y=\"Item_Outlet_Sales\", data=data_BM[:200], kind=\"line\"); #kind의 설정으로 표현 설정을 바꿈\n\n\n\n\n\n\nHue semantic\n세 번째 변수에 따라 점을 색칠하여 플롯에 다른 차원을 추가할 수도 있습니다. Seaborn에서는 이것을 “hue semantic” 사용이라고 합니다.\n\nsns.relplot(x=\"Item_MRP\", y=\"Item_Outlet_Sales\", hue=\"Item_Type\",data=data_BM[:200]); #hue로 item_type에 대한 플롯을 그려줌 \n\n\n\n\n\nsns.lineplot(x=\"Item_Weight\", y=\"Item_MRP\",hue='Outlet_Size',data=data_BM[:150]);\n\n\n\n\n\n\nBubble plot\n\nhue semantic 활용하여 Item_Visibility별로 거품을 색칠함과 동시에 개별 거품의 크기로 사용합니다.\n\n\n# Bubble plot\nsns.relplot(x=\"Item_MRP\", y=\"Item_Outlet_Sales\", data=data_BM[:200], kind=\"scatter\", size=\"Visibility_Scaled\", hue=\"Visibility_Scaled\");\n\n\n\n\n\n\nCategory wise sub plot\n\nSeaborn에서 카테고리를 기반으로 플롯을 만들 수도 있습니다.\n각 Outlet_Size에 대한 산점도를 만들었습니다.\n\n\nsns.relplot(x=\"Item_Weight\", y=\"Item_Visibility\",\n            hue='Outlet_Size',style='Outlet_Size',\n            col='Outlet_Size',data=data_BM[:100]);"
  },
  {
    "objectID": "Data_Mining/2022-05-07-matplotlib and seaborn.html#advance-categorical-plots-in-seaborn",
    "href": "Data_Mining/2022-05-07-matplotlib and seaborn.html#advance-categorical-plots-in-seaborn",
    "title": "Visualization_with_seaborn_and_matplotlib",
    "section": "2. Advance categorical plots in seaborn",
    "text": "2. Advance categorical plots in seaborn\n범주형 변수의 경우 seaborn에 세 가지 다른 패밀리가 있습니다.\ncatplot()에서 데이터의 기본 표현은 산점도를 사용합니다.\n\na. Categorical scatterplots\n\n\nStrip plot\n\n하나의 변수가 범주형인 산점도를 그립니다.\ncatplot()에서 kind=strip을 전달하여 생성할 수 있습니다.\n\n\nsns.catplot(x=\"Outlet_Size\", y=\"Item_Outlet_Sales\", kind='strip',data=data_BM[:250]);\n\n\n\n\n\n\nSwarm plot\n\n이 함수는 stripplot()과 유사하지만 점이 겹치지 않도록 조정됩니다(범주형 축을 따라만).\n이렇게 하면 값 분포를 더 잘 표현할 수 있지만 많은 수의 관측치에 대해서는 잘 확장되지 않습니다. 이러한 스타일의 플롯은 때때로 “beeswarm”이라고 불립니다.\ncatplot()에서 kind=swarm을 전달하여 이를 생성할 수 있습니다.\n\n\nsns.catplot(x=\"Outlet_Size\", y=\"Item_Outlet_Sales\", kind='swarm',data=data_BM[:250]);\n\n\n\n\n\n\nb. Categorical distribution plots\n\n\nBox Plots\n\n상자 그림은 극단값과 함께 분포의 3사분위수 값을 보여줍니다.\n“whiskers”은 하위 및 상위 사분위수의 1.5 IQR 내에 있는 점으로 확장되고 이 범위를 벗어나는 관찰은 독립적으로 표시됩니다.\n즉, 상자 그림의 각 값은 데이터의 실제 관측값에 해당합니다.\n\n\nsns.catplot(x=\"Outlet_Size\", y=\"Item_Outlet_Sales\",kind=\"box\",data=data_BM);\n\n\n\n\n\n\nViolin Plots\n\nsns.catplot(x=\"Outlet_Size\", y=\"Item_Outlet_Sales\",kind=\"violin\",data=data_BM);\n\n\n\n\n\n\nPoint Plot\n\nsns.catplot(x=\"Outlet_Size\", y=\"Item_Outlet_Sales\",kind=\"point\",data=data_BM); #y축은 연속형 플랏 x축은 범주형 \n\n\n\n\n\n\nBar plots\n\nsns.catplot(x=\"Outlet_Size\", y=\"Item_Outlet_Sales\",kind=\"bar\",data=data_BM);"
  },
  {
    "objectID": "Data_Mining/2022-05-07-matplotlib and seaborn.html#density-plots",
    "href": "Data_Mining/2022-05-07-matplotlib and seaborn.html#density-plots",
    "title": "Visualization_with_seaborn_and_matplotlib",
    "section": "3. Density Plots",
    "text": "3. Density Plots\n히스토그램 대신 Seaborn이 sn.kdeplot으로 수행하는 커널 밀도 추정을 사용하여 분포의 부드러운 추정치를 얻을 수 있습니다.:\n\n# distribution of Item Visibility\nplt.figure(figsize=(5,5))\nsns.kdeplot(data_BM['Item_Visibility'], shade=True);\n\n\n\n\n\nHistogram and Density Plot\ndistplot을 사용하여 히스토그램과 KDE를 결합할 수 있습니다.:\n\nplt.figure(figsize=(10,10))\nsns.distplot(data_BM['Item_Outlet_Sales']);"
  },
  {
    "objectID": "Data_Mining/2022-05-07-matplotlib and seaborn.html#pair-plots",
    "href": "Data_Mining/2022-05-07-matplotlib and seaborn.html#pair-plots",
    "title": "Visualization_with_seaborn_and_matplotlib",
    "section": "4. Pair plots",
    "text": "4. Pair plots\n\n조인트 플롯을 더 큰 차원의 데이터세트로 일반화하면 쌍 플롯으로 끝납니다. 이것은 모든 값 쌍을 서로에 대해 플롯하려는 경우 다차원 데이터 간의 상관 관계를 탐색하는 데 매우 유용합니다.\n세 가지 붓꽃 종의 꽃잎과 꽃받침 측정값을 나열하는 잘 알려진 Iris 데이터 세트를 사용하여 이것을 시연할 것입니다.\n\n\niris = sns.load_dataset(\"iris\")\nsns.pairplot(iris, hue='species', height=2.5); #둘다 연속형이면 scatter플랏으로 보여줌"
  },
  {
    "objectID": "Data_Mining/2022-05-07-matplotlib and seaborn.html#seaborn-and-matplotlib",
    "href": "Data_Mining/2022-05-07-matplotlib and seaborn.html#seaborn-and-matplotlib",
    "title": "Visualization_with_seaborn_and_matplotlib",
    "section": "Seaborn and Matplotlib",
    "text": "Seaborn and Matplotlib\n\n1.1 Load data\n\n예제로 사용할 펭귄 데이터를 불러옵니다.\nseaborn에 내장되어 있습니다.\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npenguins = sns.load_dataset(\"penguins\")\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n\n\n\n\n\n\n\n1.2 Figure and Axes\n\nmatplotlib으로 도화지figure를 깔고 축공간axes를 만듭니다.\n1 x 2 축공간을 구성합니다.\n\n\nfig, axes = plt.subplots(ncols=2, figsize=(8,4))\n\nfig.tight_layout()\n\n\n\n\n\n\n1.3 plot with matplotlib\n\nmatplotlib 기능을 이용해서 산점도를 그립니다.\n\nx축은 부리 길이 bill length\ny축은 부리 위 아래 두께 bill depth\n색상은 종species로 합니다.\nAdelie, Chinstrap, Gentoo이 있습니다.\n\n두 축공간 중 왼쪽에만 그립니다.\n\n\nfig, axes = plt.subplots(ncols=2,figsize=(8,4))\n\nspecies_u = penguins[\"species\"].unique()\n\nfor i, s in enumerate(species_u):\n    axes[0].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s],\n                    penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s],\n                    c=f\"C{i}\", label=s, alpha=0.3)\n    \naxes[0].legend(species_u, title=\"species\")\naxes[0].set_xlabel(\"Bill Length (mm)\")\naxes[0].set_ylabel(\"Bill Depth (mm)\")\n\n# plt.show()\nfig.tight_layout()\n\n\n\n\n\n\n1.4 Plot with seaborn\n\n단 세 줄로 거의 동일한 그림이 나왔습니다.\n\nscatter plot의 점 크기만 살짝 작습니다.\nlabel의 투명도만 살짝 다릅니다.\n\nseaborn 명령 scatterplot()을 그대로 사용했습니다.\nx축과 y축 label도 바꾸었습니다.\n\nax=axes[1] 인자에서 볼 수 있듯, 존재하는 axes에 그림만 얹었습니다.\nmatplotlib 틀 + seaborn 그림 이므로, matplotlib 명령이 모두 통합니다.\n\n\n\nfig, axes = plt.subplots(ncols=2,figsize=(8,4))\n\nspecies_u = penguins[\"species\"].unique()\n\n# plot 0 : matplotlib\n\nfor i, s in enumerate(species_u):\n    axes[0].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s],\n                    penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s],\n                    c=f\"C{i}\", label=s, alpha=0.3)\n    \naxes[0].legend(species_u, title=\"species\")\naxes[0].set_xlabel(\"Bill Length (mm)\")\naxes[0].set_ylabel(\"Bill Depth (mm)\")\n\n\n# plot 1 : seaborn\nsns.scatterplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", data=penguins, alpha=0.3, ax=axes[1])\naxes[1].set_xlabel(\"Bill Length (mm)\")\naxes[1].set_ylabel(\"Bill Depth (mm)\")\n\nfig.tight_layout()\n\n\n\n\n\n\n1.5 matplotlib + seaborn & seaborn + matplotlib\n\nmatplotlib과 seaborn이 자유롭게 섞일 수 있습니다.\n\nmatplotlib 산점도 위에 seaborn 추세선을 얹을 수 있고,\nseaborn 산점도 위에 matplotlib 중심점을 얹을 수 있습니다.\n\n\n\nfig, axes = plt.subplots(ncols=2, figsize=(8, 4))\n\nspecies_u = penguins[\"species\"].unique()\n\n# plot 0 : matplotlib + seaborn\nfor i, s in enumerate(species_u):\n    # matplotlib 산점도\n    axes[0].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s],\n                   penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s],\n                   c=f\"C{i}\", label=s, alpha=0.3\n                  )\n                  \n    # seaborn 추세선\n    sns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=penguins.loc[penguins[\"species\"]==s], \n                scatter=False, ax=axes[0])\n    \naxes[0].legend(species_u, title=\"species\")\naxes[0].set_xlabel(\"Bill Length (mm)\")\naxes[0].set_ylabel(\"Bill Depth (mm)\")\n\n# plot 1 : seaborn + matplotlib\n# seaborn 산점도\nsns.scatterplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", data=penguins, alpha=0.3, ax=axes[1])\naxes[1].set_xlabel(\"Bill Length (mm)\")\naxes[1].set_ylabel(\"Bill Depth (mm)\")\n\nfor i, s in enumerate(species_u):\n    # matplotlib 중심점\n    axes[1].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s].mean(),\n                   penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s].mean(),\n                   c=f\"C{i}\", alpha=1, marker=\"x\", s=100\n                  )\n\nfig.tight_layout()\n\n\n\n\n\n\n1.6 seaborn + seaborn + matplotlib\n\nseaborn scatterplot + seaborn kdeplot + matplotlib text입니다\n\n\nfig, ax = plt.subplots(figsize=(6,5))\n\n# plot 0: scatter plot\nsns.scatterplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", color=\"k\", data=penguins, alpha=0.3, ax=ax, legend=False)\n\n# plot 1: kde plot\nsns.kdeplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", data=penguins, alpha=0.5, ax=ax, legend=False)\n\n# text:\nspecies_u = penguins[\"species\"].unique()\nfor i, s in enumerate(species_u):\n    ax.text(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s].mean(),\n            penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s].mean(),\n            s = s, fontdict={\"fontsize\":14, \"fontweight\":\"bold\",\"color\":\"k\"}\n            )\n\nax.set_xlabel(\"Bill Length (mm)\")\nax.set_ylabel(\"Bill Depth (mm)\")\n\nfig.tight_layout()"
  },
  {
    "objectID": "Data_Mining/2022-05-24-exercise-interactive-maps.html",
    "href": "Data_Mining/2022-05-24-exercise-interactive-maps.html",
    "title": "Interactive Maps",
    "section": "",
    "text": "Interactive Maps\n\nThis notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link.\n\n\nimport pandas as pd\nimport geopandas as gpd\n\nimport folium\nfrom folium import Choropleth\nfrom folium.plugins import HeatMap\n\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.geospatial.ex3 import *\n\nWe define a function embed_map() for displaying interactive maps. It accepts two arguments: the variable containing the map, and the name of the HTML file where the map will be saved.\nThis function ensures that the maps are visible in all web browsers.\n\ndef embed_map(m, file_name):\n    from IPython.display import IFrame\n    m.save(file_name)\n    return IFrame(file_name, width='100%', height='500px')\n\n\nExercises\n\n1) Do earthquakes coincide with plate boundaries?\nRun the code cell below to create a DataFrame plate_boundaries that shows global plate boundaries. The “coordinates” column is a list of (latitude, longitude) locations along the boundaries.\n아래 코드 셀을 실행하여 전역 플레이트 경계를 표시하는 DataFrame plate_boundaries를 만듭니다. “좌표” 열은 경계를 따라 (위도, 경도) 위치의 목록입니다.\n\nplate_boundaries = gpd.read_file(\"../input/geospatial-learn-course-data/Plate_Boundaries/Plate_Boundaries/Plate_Boundaries.shp\")\nplate_boundaries['coordinates'] = plate_boundaries.apply(lambda x: [(b,a) for (a,b) in list(x.geometry.coords)], axis='columns')\nplate_boundaries.drop('geometry', axis=1, inplace=True)\n\nplate_boundaries.head()\n\nNext, run the code cell below without changes to load the historical earthquake data into a DataFrame earthquakes.\n\n# Load the data and print the first 5 rows\nearthquakes = pd.read_csv(\"../input/geospatial-learn-course-data/earthquakes1970-2014.csv\", parse_dates=[\"DateTime\"])\nearthquakes.head()\n\nThe code cell below visualizes the plate boundaries on a map. Use all of the earthquake data to add a heatmap to the same map, to determine whether earthquakes coincide with plate boundaries.\n아래 코드 셀은 지도에서 판 경계를 시각화합니다. 모든 지진 데이터를 사용하여 동일한 지도에 히트맵을 추가하여 지진이 판 경계와 일치하는지 확인합니다.\n\n# Create a base map with plate boundaries\nm_1 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)\nfor i in range(len(plate_boundaries)):\n    folium.PolyLine(locations=plate_boundaries.coordinates.iloc[i], weight=2, color='black').add_to(m_1)\n\n# Your code here: Add a heatmap to the map\nHeatMap(data=earthquakes[['Latitude', 'Longitude']], radius=10).add_to(m_1)\n\n# Uncomment to see a hint\n#q_1.a.hint()\n\n# Show the map\nembed_map(m_1, 'q_1.html')\n\n\n# Get credit for your work after you have created a map\nq_1.a.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_1.a.solution()\n\nSo, given the map above, do earthquakes coincide with plate boundaries?\n\n# View the solution (Run this code cell to receive credit!)\nq_1.b.solution()\n\n\n\n2) Is there a relationship between earthquake depth and proximity to a plate boundary in Japan?\nYou recently read that the depth of earthquakes tells us important information about the structure of the earth. You’re interested to see if there are any intereresting global patterns, and you’d also like to understand how depth varies in Japan.\n최근에 지진의 깊이가 중요 정보를 알려준다는 내용을 읽었습니다. -news_science_products) 지구의 구조에 대해. 흥미로운 글로벌 패턴이 있는지 확인하고 싶고 일본의 깊이가 어떻게 다른지 이해하고 싶습니다.\n\n# Create a base map with plate boundaries\nm_2 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)\nfor i in range(len(plate_boundaries)):\n    folium.PolyLine(locations=plate_boundaries.coordinates.iloc[i], weight=2, color='black').add_to(m_2)\n    \n# Your code here: Add a map to visualize earthquake depth\ndef color_producer(val):\n    if val &lt; 50:\n        return 'forestgreen'\n    else:\n        return 'darkred'\n\n\nfor i in range(0,len(earthquakes)):\n    folium.Circle(\n        location=[earthquakes.iloc[i]['Latitude'], earthquakes.iloc[i]['Longitude']],\n        radius=2000,\n        color=color_producer(earthquakes.iloc[i]['Depth'])).add_to(m_2)\n    \n# Uncomment to see a hint\nq_2.a.hint()\n\n# View the map\nembed_map(m_2, 'q_2.html')\n\n\n# Get credit for your work after you have created a map\nq_2.a.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_2.a.solution()\n\nCan you detect a relationship between proximity to a plate boundary and earthquake depth? Does this pattern hold globally? In Japan?\n\n# View the solution (Run this code cell to receive credit!)\nq_2.b.solution()\n\n\n\n3) Which prefectures have high population density?\nRun the next code cell (without changes) to create a GeoDataFrame prefectures that contains the geographical boundaries of Japanese prefectures.\n다음 코드 셀을 변경 없이 실행하여 일본 현의 지리적 경계를 포함하는 GeoDataFrame ’현’을 만듭니다.\n\n# GeoDataFrame with prefecture boundaries\nprefectures = gpd.read_file(\"../input/geospatial-learn-course-data/japan-prefecture-boundaries/japan-prefecture-boundaries/japan-prefecture-boundaries.shp\")\nprefectures.set_index('prefecture', inplace=True)\nprefectures.head()\n\nThe next code cell creates a DataFrame stats containing the population, area (in square kilometers), and population density (per square kilometer) for each Japanese prefecture. Run the code cell without changes.\n다음 코드 셀은 각 일본 현에 대한 인구, 면적(제곱 킬로미터 단위) 및 인구 밀도(제곱 킬로미터당)를 포함하는 DataFrame ’통계’를 생성합니다. 변경 없이 코드 셀을 실행합니다.\n\n# DataFrame containing population of each prefecture\npopulation = pd.read_csv(\"../input/geospatial-learn-course-data/japan-prefecture-population.csv\")\npopulation.set_index('prefecture', inplace=True)\n\n# Calculate area (in square kilometers) of each prefecture\narea_sqkm = pd.Series(prefectures.geometry.to_crs(epsg=32654).area / 10**6, name='area_sqkm')\nstats = population.join(area_sqkm)\n\n# Add density (per square kilometer) of each prefecture\nstats['density'] = stats[\"population\"] / stats[\"area_sqkm\"]\nstats.head()\n\nUse the next code cell to create a choropleth map to visualize population density.\n다음 코드 셀을 사용하여 등치 지도를 만들어 인구 밀도를 시각화합니다.\n\n# Create a base map\nm_3 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)\n\n# Your code here: create a choropleth map to visualize population density\nChoropleth(geo_data=prefectures['geometry'].__geo_interface__, \n           data=stats['density'], \n           key_on=\"feature.id\", \n           fill_color='YlGnBu', \n           legend_name='population density'\n          ).add_to(m_3)\n\n# Uncomment to see a hint\n#q_3.a.hint()\n\n# View the map\nembed_map(m_3, 'q_3.html')\n\n\n# Get credit for your work after you have created a map\nq_3.a.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_3.a.solution()\n\nWhich three prefectures have relatively higher density than the others? Are they spread throughout the country, or all located in roughly the same geographical region? (If you’re unfamiliar with Japanese geography, you might find this map useful to answer the questions.)\n\n# View the solution (Run this code cell to receive credit!)\nq_3.b.solution()\n\n\n\n4) Which high-density prefecture is prone to high-magnitude earthquakes?\nCreate a map to suggest one prefecture that might benefit from earthquake reinforcement. Your map should visualize both density and earthquake magnitude.\n지진 보강의 혜택을 받을 수 있는 한 현을 제안하는 지도를 만듭니다. 지도는 밀도와 지진 규모를 모두 시각화해야 합니다.\n\n# Create a base map\nm_4 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)\n\n# Your code here: create a map\ndef color_producer(magnitude):\n    if magnitude &gt; 6.5:\n        return 'red'\n    else:\n        return 'green'\n\nChoropleth(\n    geo_data=prefectures['geometry'].__geo_interface__,\n    data=stats['density'],\n    key_on=\"feature.id\",\n    fill_color='BuPu',\n    legend_name='Population density (per square kilometer)').add_to(m_4)\n\nfor i in range(0,len(earthquakes)):\n    folium.Circle(\n        location=[earthquakes.iloc[i]['Latitude'], earthquakes.iloc[i]['Longitude']],\n        popup=(\"{} ({})\").format(\n            earthquakes.iloc[i]['Magnitude'],\n            earthquakes.iloc[i]['DateTime'].year),\n        radius=earthquakes.iloc[i]['Magnitude']**5.5,\n        color=color_producer(earthquakes.iloc[i]['Magnitude'])).add_to(m_4)\n# Uncomment to see a hint\n#q_4.a.hint()\n\n# View the map\nembed_map(m_4, 'q_4.html')\n\n\n# Get credit for your work after you have created a map\nq_4.a.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_4.a.solution()\n\nWhich prefecture do you recommend for extra earthquake reinforcement?\n\n# View the solution (Run this code cell to receive credit!)\nq_4.b.solution()\n\n\n\n\nKeep going\nLearn how to convert names of places to geographic coordinates with geocoding. You’ll also explore special ways to join information from multiple GeoDataFrames.\n\nHave questions or comments? Visit the course discussion forum to chat with other learners."
  },
  {
    "objectID": "Data_Mining/2022-05-25-newyorktaxi.html",
    "href": "Data_Mining/2022-05-25-newyorktaxi.html",
    "title": "NYC taxi",
    "section": "",
    "text": "NYC taxi"
  },
  {
    "objectID": "Data_Mining/2022-05-25-newyorktaxi.html#visual-analytics-with-python",
    "href": "Data_Mining/2022-05-25-newyorktaxi.html#visual-analytics-with-python",
    "title": "NYC taxi",
    "section": "Visual Analytics with Python",
    "text": "Visual Analytics with Python\n강의자료 출처 - kaggle.com\nIn this script we will explore the spatial and temporal behavior of the people of New York as can be inferred by examining their cab usage.\nThe main fields of this dataset are taxi pickup time and location, as well as dropoff location and trip duration. There is a total of around 1.4 Million trips in the dataset that took place during the first half of 2016.\nWe will study how the patterns of cab usage change throughout the year, throughout the week and throughout the day, and we will focus on difference between weekdays and weekends.\n\n%matplotlib inline\n\nfrom sklearn import decomposition\nfrom scipy import stats\nfrom sklearn import cluster\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nmatplotlib.style.use('fivethirtyeight')\nmatplotlib.rcParams['font.size'] = 12\nmatplotlib.rcParams['figure.figsize'] = (10,10)\n\n\nLoad data and preprocess measurements to sensible units\n\ndata_frame = pd.read_csv('train.csv')\n\n\ndata_frame.describe()\n\n\n\n\n\n\n\n\nvendor_id\npassenger_count\npickup_longitude\npickup_latitude\ndropoff_longitude\ndropoff_latitude\ntrip_duration\n\n\n\n\ncount\n1.458644e+06\n1.458644e+06\n1.458644e+06\n1.458644e+06\n1.458644e+06\n1.458644e+06\n1.458644e+06\n\n\nmean\n1.534950e+00\n1.664530e+00\n-7.397349e+01\n4.075092e+01\n-7.397342e+01\n4.075180e+01\n9.594923e+02\n\n\nstd\n4.987772e-01\n1.314242e+00\n7.090186e-02\n3.288119e-02\n7.064327e-02\n3.589056e-02\n5.237432e+03\n\n\nmin\n1.000000e+00\n0.000000e+00\n-1.219333e+02\n3.435970e+01\n-1.219333e+02\n3.218114e+01\n1.000000e+00\n\n\n25%\n1.000000e+00\n1.000000e+00\n-7.399187e+01\n4.073735e+01\n-7.399133e+01\n4.073588e+01\n3.970000e+02\n\n\n50%\n2.000000e+00\n1.000000e+00\n-7.398174e+01\n4.075410e+01\n-7.397975e+01\n4.075452e+01\n6.620000e+02\n\n\n75%\n2.000000e+00\n2.000000e+00\n-7.396733e+01\n4.076836e+01\n-7.396301e+01\n4.076981e+01\n1.075000e+03\n\n\nmax\n2.000000e+00\n9.000000e+00\n-6.133553e+01\n5.188108e+01\n-6.133553e+01\n4.392103e+01\n3.526282e+06\n\n\n\n\n\n\n\n\ndata_frame.shape\n\n(1458644, 11)\n\n\n\nnp.max(data_frame['trip_duration']) #이렇게 보면서 이상치가 있는 것을 눈으로 봄 3526282(전처리전) --&gt; 4764(전처리후)\n\n3526282\n\n\n\n# remove obvious outliers / 위경도 좌표가 튀는 것을 제거 \nallLat  = np.array(list(data_frame['pickup_latitude'])  + list(data_frame['dropoff_latitude'])) #두개의 컬럼을 리스트로 합쳐줌 \nallLong = np.array(list(data_frame['pickup_longitude']) + list(data_frame['dropoff_longitude']))\n\nlongLimits = [np.percentile(allLong, 0.3), np.percentile(allLong, 99.7)]\nlatLimits  = [np.percentile(allLat , 0.3), np.percentile(allLat , 99.7)]\ndurLimits  = [np.percentile(data_frame['trip_duration'], 0.4), np.percentile(data_frame['trip_duration'], 99.7)]\n\ndata_frame = data_frame[(data_frame['pickup_latitude']   &gt;= latLimits[0] ) & (data_frame['pickup_latitude']   &lt;= latLimits[1]) ] #이상치 제거\ndata_frame = data_frame[(data_frame['dropoff_latitude']  &gt;= latLimits[0] ) & (data_frame['dropoff_latitude']  &lt;= latLimits[1]) ]\ndata_frame = data_frame[(data_frame['pickup_longitude']  &gt;= longLimits[0]) & (data_frame['pickup_longitude']  &lt;= longLimits[1])]\ndata_frame = data_frame[(data_frame['dropoff_longitude'] &gt;= longLimits[0]) & (data_frame['dropoff_longitude'] &lt;= longLimits[1])]\ndata_frame = data_frame[(data_frame['trip_duration']     &gt;= durLimits[0] ) & (data_frame['trip_duration']     &lt;= durLimits[1]) ]\ndata_frame = data_frame.reset_index(drop=True)\n\n\n# convert fields to sensible units\nmedianLat  = np.percentile(allLat,50)\nmedianLong = np.percentile(allLong,50)\n\nlatMultiplier  = 111.32\nlongMultiplier = np.cos(medianLat*(np.pi/180.0)) * 111.32\n\ndata_frame['duration [min]'] = data_frame['trip_duration']/60.0\ndata_frame['src lat [km]']   = latMultiplier  * (data_frame['pickup_latitude']   - medianLat)\ndata_frame['src long [km]']  = longMultiplier * (data_frame['pickup_longitude']  - medianLong)\ndata_frame['dst lat [km]']   = latMultiplier  * (data_frame['dropoff_latitude']  - medianLat)\ndata_frame['dst long [km]']  = longMultiplier * (data_frame['dropoff_longitude'] - medianLong)\n\nallLat  = np.array(list(data_frame['src lat [km]'])  + list(data_frame['dst lat [km]']))\nallLong = np.array(list(data_frame['src long [km]']) + list(data_frame['dst long [km]']))\n\n\n\nPlot the histograms of trip duration, latitude and longitude\n\nfig, axArray = plt.subplots(nrows=1,ncols=3,figsize=(13,4))\naxArray[0].hist(data_frame['duration [min]'],80);\naxArray[0].set_xlabel('trip duration [min]'); axArray[0].set_ylabel('counts')\naxArray[1].hist(allLat ,80); axArray[1].set_xlabel('latitude [km]')\naxArray[2].hist(allLong,80); axArray[2].set_xlabel('longitude [km]')\n#위경도는 큰의미는 없지만 플랏으로 이상치가 있는지 시각적으로 확인 \n\nText(0.5, 0, 'longitude [km]')\n\n\n\n\n\n\n\nPlot the trip Duration vs. the Aerial Distance between pickup and dropoff\n\ndata_frame['log duration']       = np.log1p(data_frame['duration [min]'])\ndata_frame['euclidian distance'] = np.sqrt((data_frame['src lat [km]']  - data_frame['dst lat [km]'] )**2 +\n                                       (data_frame['src long [km]'] - data_frame['dst long [km]'])**2)\n\nfig, axArray = plt.subplots(nrows=1,ncols=2,figsize=(13,6))\naxArray[0].scatter(data_frame['euclidian distance'], data_frame['duration [min]'],c='b',s=5,alpha=0.01);\naxArray[0].set_xlabel('Aerial Euclidian Distance [km]'); axArray[0].set_ylabel('Duration [min]')\naxArray[0].set_xlim(data_frame['euclidian distance'].min(),data_frame['euclidian distance'].max())\naxArray[0].set_ylim(data_frame['duration [min]'].min(),data_frame['duration [min]'].max())\naxArray[0].set_title('trip Duration vs Aerial trip Distance')\n\naxArray[1].scatter(data_frame['euclidian distance'], data_frame['log duration'],c='b',s=5,alpha=0.01);\naxArray[1].set_xlabel('Aerial Euclidian Distance [km]'); axArray[1].set_ylabel('log(1+Duration) [log(min)]')\naxArray[1].set_xlim(data_frame['euclidian distance'].min(),data_frame['euclidian distance'].max())\naxArray[1].set_ylim(data_frame['log duration'].min(),data_frame['log duration'].max())\naxArray[1].set_title('log of trip Duration vs Aerial trip Distance')\n\nText(0.5, 1.0, 'log of trip Duration vs Aerial trip Distance')\n\n\n\n\n\n\n\nExercise 1\n위의 Scatter plot은 Point가 너무 많이 존재하여 좋은 시각화가 아닐 수 있습니다. 보다 효율적인 시각화 방안을 제시해 보세요.\nmatplotlib, seaborn, plotly 등 다양한 패키지를 활용할 수 있습니다\n\nimport seaborn as sns\n\nsns.jointplot(x=data_frame['euclidian distance'], \n              y=data_frame['duration [min]'], \n              kind=\"hex\", \n              color=\"#4CB391\",\n              xlim = (0,30),\n              ylim = (0,70))\n\n\n\n\n\np = sns.jointplot(x=np.log1p(data_frame['euclidian distance']),\n              y=data_frame['log duration'], \n              kind=\"hex\", \n              color=\"#4CB391\") #log를 취해 주어서 히트맵을 변환시켜 보여줌 \n\np.ax_joint.set_xlabel('log(1+Aerial Euclidian Distance) [log(km)]')\np.ax_joint.set_ylabel('log(1+Duration) [log(min)]')\n\nText(8.060000000000002, 0.5, 'log(1+Duration) [log(min)]')\n\n\n\n\n\nWe can see that the trip distance defines the lower bound on trip duration, as one would expect.\n\n\nPlot spatial density plot of the pickup and dropoff locations\n\nimageSize = (700,700)\nlongRange = [-5,19]\nlatRange = [-13,11]\n\nallLatInds  = imageSize[0] - (imageSize[0] * (allLat  - latRange[0])  / (latRange[1]  - latRange[0]) ).astype(int)\nallLongInds =                (imageSize[1] * (allLong - longRange[0]) / (longRange[1] - longRange[0])).astype(int)\n\nlocationDensityImage = np.zeros(imageSize)\nfor latInd, longInd in zip(allLatInds,allLongInds):\n    locationDensityImage[latInd,longInd] += 1\n\nfig, ax = plt.subplots(nrows=1,ncols=1,figsize=(12,12))\nax.imshow(np.log(locationDensityImage+1),cmap='inferno')\nax.set_axis_off()\n\n\n\n\n\n\nExercise 2\nfolium이나 pydeck을 사용하여 택시의 승차지점, 하차지점을 시각화 해봅시다. 디서 승객이 많이 타고 내리는지를 확인할 수 있습니까?\n\ndata_frame.head()\n\n\n\n\n\n\n\n\nid\nvendor_id\npickup_datetime\ndropoff_datetime\npassenger_count\npickup_longitude\npickup_latitude\ndropoff_longitude\ndropoff_latitude\nstore_and_fwd_flag\ntrip_duration\nduration [min]\nsrc lat [km]\nsrc long [km]\ndst lat [km]\ndst long [km]\nlog duration\neuclidian distance\n\n\n\n\n0\nid2875421\n2\n2016-03-14 17:24:55\n2016-03-14 17:32:30\n1\n-73.982155\n40.767937\n-73.964630\n40.765602\nN\n455\n7.583333\n1.516008\n-0.110015\n1.256121\n1.367786\n2.149822\n1.500479\n\n\n1\nid2377394\n1\n2016-06-12 00:43:35\n2016-06-12 00:54:38\n1\n-73.980415\n40.738564\n-73.999481\n40.731152\nN\n663\n11.050000\n-1.753813\n0.036672\n-2.578912\n-1.571088\n2.489065\n1.807119\n\n\n2\nid3858529\n2\n2016-01-19 11:35:24\n2016-01-19 12:10:48\n1\n-73.979027\n40.763939\n-74.005333\n40.710087\nN\n2124\n35.400000\n1.070973\n0.153763\n-4.923841\n-2.064547\n3.594569\n6.392080\n\n\n3\nid3504673\n2\n2016-04-06 19:32:31\n2016-04-06 19:39:40\n1\n-74.010040\n40.719971\n-74.012268\n40.706718\nN\n429\n7.150000\n-3.823568\n-2.461500\n-5.298809\n-2.649362\n2.098018\n1.487155\n\n\n4\nid2181028\n2\n2016-03-26 13:30:55\n2016-03-26 13:38:10\n1\n-73.973053\n40.793209\n-73.972923\n40.782520\nN\n435\n7.250000\n4.329328\n0.657515\n3.139453\n0.668452\n2.110213\n1.189925\n\n\n\n\n\n\n\n\ndata_frame.columns\n\nIndex(['id', 'vendor_id', 'pickup_datetime', 'dropoff_datetime',\n       'passenger_count', 'pickup_longitude', 'pickup_latitude',\n       'dropoff_longitude', 'dropoff_latitude', 'store_and_fwd_flag',\n       'trip_duration', 'duration [min]', 'src lat [km]', 'src long [km]',\n       'dst lat [km]', 'dst long [km]', 'log duration', 'euclidian distance'],\n      dtype='object')\n\n\n\nimport pydeck as pdk\n\n\n# Define a layer to display on a map\nlayer = pdk.Layer(\n    \"HexagonLayer\",\n    data_frame[:1000],\n    get_position=[\"pickup_longitude\", \"pickup_latitude\"],\n    auto_highlight=True,\n    elevation_scale=50,\n    pickable=True,\n    elevation_range=[0, 50],\n    extruded=True,\n    coverage=1,\n)\n\n# Set the viewport location\nview_lon = np.mean(data_frame[\"pickup_longitude\"])\nview_lat = np.mean(data_frame[\"pickup_latitude\"])\n\nview_state = pdk.ViewState(\n    longitude=view_lon, latitude=view_lat, zoom=10, min_zoom=5, max_zoom=15, pitch=40.5, bearing=-27.36,\n)\n\n# Render\nr = pdk.Deck(layers=[layer], initial_view_state=view_state)\nr.show()\n#r.to_html(\"hexagon_ny_taxi.html\")\n\n\n\n\n\nlayer = pdk.Layer(\n    \"HeatmapLayer\",\n    data_frame[:1000],\n    get_position=[\"pickup_longitude\", \"pickup_latitude\"],\n    auto_highlight=True,\n    elevation_scale=50,\n    pickable=True,\n    elevation_range=[0, 50],\n    extruded=True,\n    coverage=1,\n)\n\n# Set the viewport location\nview_lon = np.mean(data_frame[\"pickup_longitude\"])\nview_lat = np.mean(data_frame[\"pickup_latitude\"])\n\nview_state = pdk.ViewState(\n    longitude=view_lon, latitude=view_lat, zoom=10, min_zoom=5, max_zoom=15, pitch=40.5, bearing=-27.36,\n)\n\n# Render\nr = pdk.Deck(layers=[layer], initial_view_state=view_state)\nr.show()\n\n\n\n\n\n#folium 을 이용해서도 예제로 해보기 \n\n\n\nClosing in on Manhattan\n\nimageSizeMan = (720,480)\nlatRangeMan = [-8,10]\nlongRangeMan = [-5,7]\n\nindToKeep  = np.logical_and(allLat &gt; latRangeMan[0], allLat &lt; latRangeMan[1])\nindToKeep  = np.logical_and(indToKeep, np.logical_and(allLong &gt; longRangeMan[0], allLong &lt; longRangeMan[1]))\nallLatMan  = allLat[indToKeep]\nallLongMan = allLong[indToKeep]\n\nallLatIndsMan  = (imageSizeMan[0]-1) - (imageSizeMan[0] * (allLatMan  - latRangeMan[0])\n                                                        / (latRangeMan[1] - latRangeMan[0])).astype(int)\nallLongIndsMan =                       (imageSizeMan[1] * (allLongMan - longRangeMan[0])\n                                                        / (longRangeMan[1] - longRangeMan[0])).astype(int)\n\nlocationDensityImageMan = np.zeros(imageSizeMan)\nfor latInd, longInd in zip(allLatIndsMan,allLongIndsMan):\n    locationDensityImageMan[latInd,longInd] += 1\n\nfig, ax = plt.subplots(nrows=1,ncols=1,figsize=(12,18))\nax.imshow(np.log(locationDensityImageMan+1),cmap='inferno')\nax.set_axis_off()\n\n\n\n\n\n\nCluster the Trips and Look at their distribution\n\npickupTime = pd.to_datetime(data_frame['pickup_datetime'])\n\ndata_frame['src hourOfDay'] = (pickupTime.dt.hour*60.0 + pickupTime.dt.minute)   / 60.0\ndata_frame['dst hourOfDay'] = data_frame['src hourOfDay'] + data_frame['duration [min]'] / 60.0\n\ndata_frame['dayOfWeek']     = pickupTime.dt.weekday\ndata_frame['hourOfWeek']    = data_frame['dayOfWeek']*24.0 + data_frame['src hourOfDay']\n\ndata_frame['monthOfYear']   = pickupTime.dt.month\ndata_frame['dayOfYear']     = pickupTime.dt.dayofyear\ndata_frame['weekOfYear']    = pickupTime.dt.weekofyear\ndata_frame['hourOfYear']    = data_frame['dayOfYear']*24.0 + data_frame['src hourOfDay']\n\nFutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n  data_frame['weekOfYear']    = pickupTime.dt.weekofyear\n\n\n\ntripAttributes = np.array(data_frame.loc[:,['src lat [km]','src long [km]','dst lat [km]','dst long [km]','duration [min]']])\nmeanTripAttr = tripAttributes.mean(axis=0)\nstdTripAttr  = tripAttributes.std(axis=0)\ntripAttributes = stats.zscore(tripAttributes, axis=0)\n\nnumClusters = 40\nTripKmeansModel = cluster.MiniBatchKMeans(n_clusters=numClusters, batch_size=120000, n_init=100, random_state=1)\nclusterInds = TripKmeansModel.fit_predict(tripAttributes)\n\nclusterTotalCounts, _ = np.histogram(clusterInds, bins=numClusters)\nsortedClusterInds = np.flipud(np.argsort(clusterTotalCounts))\n\nplt.figure(figsize=(12,4)); plt.title('Cluster Histogram of all trip')\nplt.bar(range(1,numClusters+1),clusterTotalCounts[sortedClusterInds])\nplt.ylabel('Frequency [counts]'); plt.xlabel('Cluster index (sorted by cluster frequency)')\nplt.xlim(0,numClusters+1)\n\n(0.0, 41.0)\n\n\n\n\n\n\n\nPlot typical Trips on the Map\n\ndef ConvertToImageCoords(latCoord, longCoord, latRange, longRange, imageSize):\n    latInds  = imageSize[0] - (imageSize[0] * (latCoord  - latRange[0])  / (latRange[1]  - latRange[0]) ).astype(int)\n    longInds =                (imageSize[1] * (longCoord - longRange[0]) / (longRange[1] - longRange[0])).astype(int)\n\n    return latInds, longInds\n\ntemplateTrips = TripKmeansModel.cluster_centers_ * np.tile(stdTripAttr,(numClusters,1)) + np.tile(meanTripAttr,(numClusters,1))\n\nsrcCoords = templateTrips[:,:2]\ndstCoords = templateTrips[:,2:4]\n\nsrcImCoords = ConvertToImageCoords(srcCoords[:,0],srcCoords[:,1], latRange, longRange, imageSize)\ndstImCoords = ConvertToImageCoords(dstCoords[:,0],dstCoords[:,1], latRange, longRange, imageSize)\n\nplt.figure(figsize=(12,12))\nplt.imshow(np.log(locationDensityImage+1),cmap='inferno'); plt.grid('off')\nplt.scatter(srcImCoords[1],srcImCoords[0],c='m',s=200,alpha=0.8)\nplt.scatter(dstImCoords[1],dstImCoords[0],c='g',s=200,alpha=0.8)\n\nfor i in range(len(srcImCoords[0])):\n    plt.arrow(srcImCoords[1][i],srcImCoords[0][i], dstImCoords[1][i]-srcImCoords[1][i], dstImCoords[0][i]-srcImCoords[0][i],\n              edgecolor='c', facecolor='c', width=0.8,alpha=0.4,head_width=10.0,head_length=10.0,length\n\nSyntaxError: unexpected EOF while parsing (&lt;ipython-input-22-39197d099a2f&gt;, line 22)\n\n\n\n\nCalculate the trip distribution for different hours of the weekday\n\nhoursOfDay = np.sort(data_frame['src hourOfDay'].astype(int).unique())\nclusterDistributionHourOfDay_weekday = np.zeros((len(hoursOfDay),numClusters))\nfor k, hour in enumerate(hoursOfDay):\n    slectedInds = (data_frame['src hourOfDay'].astype(int) == hour) & (data_frame['dayOfWeek'] &lt;= 4)\n    currDistribution, _ = np.histogram(clusterInds[slectedInds], bins=numClusters)\n    clusterDistributionHourOfDay_weekday[k,:] = currDistribution[sortedClusterInds]\n\nfig, ax = plt.subplots(nrows=1,ncols=1,figsize=(12,6))\nax.set_title('Trip Distribution during Weekdays', fontsize=12)\nax.imshow(clusterDistributionHourOfDay_weekday); ax.grid('off')\nax.set_xlabel('Trip Cluster'); ax.set_ylabel('Hour of Day')\n\n\n\nCalculate the trip distribution for different hours of the weekend\n\nhoursOfDay = np.sort(data_frame['src hourOfDay'].astype(int).unique())\nclusterDistributionHourOfDay_weekend = np.zeros((len(hoursOfDay),numClusters))\nfor k, hour in enumerate(hoursOfDay):\n    slectedInds = (data_frame['src hourOfDay'].astype(int) == hour) & (data_frame['dayOfWeek'] &gt;= 5)\n    currDistribution, _ = np.histogram(clusterInds[slectedInds], bins=numClusters)\n    clusterDistributionHourOfDay_weekend[k,:] = currDistribution[sortedClusterInds]\n\nfig, ax = plt.subplots(nrows=1,ncols=1,figsize=(12,6))\nax.set_title('Trip Distribution during Weekends', fontsize=12)\nax.imshow(clusterDistributionHourOfDay_weekend); ax.grid('off')\nax.set_xlabel('Trip Cluster'); ax.set_ylabel('Hour of Day')\n\n\n\nCalculate the trip distribution for day of week\n\ndaysOfWeek = np.sort(data_frame['dayOfWeek'].unique())\nclusterDistributionDayOfWeek = np.zeros((len(daysOfWeek),numClusters))\nfor k, day in enumerate(daysOfWeek):\n    slectedInds = data_frame['dayOfWeek'] == day\n    currDistribution, _ = np.histogram(clusterInds[slectedInds], bins=numClusters)\n    clusterDistributionDayOfWeek[k,:] = currDistribution[sortedClusterInds]\n\nplt.figure(figsize=(12,5)); plt.title('Trip Distribution throughout the Week')\nplt.imshow(clusterDistributionDayOfWeek); plt.grid('off')\nplt.xlabel('Trip Cluster'); plt.ylabel('Day of Week')\n\n\n\nCalculate the trip distribution for day of year\n\ndaysOfYear = data_frame['dayOfYear'].unique()\ndaysOfYear = np.sort(daysOfYear)\nclusterDistributionDayOfYear = np.zeros((len(daysOfYear),numClusters))\nfor k, day in enumerate(daysOfYear):\n    slectedInds = data_frame['dayOfYear'] == day\n    currDistribution, _ = np.histogram(clusterInds[slectedInds], bins=numClusters)\n    clusterDistributionDayOfYear[k,:] = currDistribution[sortedClusterInds]\n\nfig, ax = plt.subplots(nrows=1,ncols=1,figsize=(10,16))\nax.set_title('Trip Distribution throughout the Year', fontsize=12)\nax.imshow(clusterDistributionDayOfYear); ax.grid('off')\nax.set_xlabel('Trip Cluster'); ax.set_ylabel('Day of Year')\nax.annotate('Large Snowstorm', color='r', fontsize=15 ,xy=(5, 21), xytext=(20, 17),\n            arrowprops=dict(facecolor='red', shrink=0.03))\nax.annotate('Memorial Day', color='r', fontsize=15, xy=(5, 151), xytext=(20, 157),\n            arrowprops=dict(facecolor='red', shrink=0.03))\n\n\n\nComputing PCA coefficients\n\nhoursOfYear = np.sort(data_frame['hourOfYear'].astype(int).unique())\nclusterDistributionHourOfYear = np.zeros((len(range(hoursOfYear[0],hoursOfYear[-1])),numClusters))\ndayOfYearVec  = np.zeros(clusterDistributionHourOfYear.shape[0])\nweekdayVec    = np.zeros(clusterDistributionHourOfYear.shape[0])\nweekOfYearVec = np.zeros(clusterDistributionHourOfYear.shape[0])\nfor k, hour in enumerate(hoursOfYear):\n    slectedInds = data_frame['hourOfYear'].astype(int) == hour\n    currDistribution, _ = np.histogram(clusterInds[slectedInds], bins=numClusters)\n    clusterDistributionHourOfYear[k,:] = currDistribution[sortedClusterInds]\n\n    dayOfYearVec[k]  = data_frame[slectedInds]['dayOfYear'].mean()\n    weekdayVec[k]    = data_frame[slectedInds]['dayOfWeek'].mean()\n    weekOfYearVec[k] = data_frame[slectedInds]['weekOfYear'].mean()\n\nnumComponents = 3\nTripDistributionPCAModel = decomposition.PCA(n_components=numComponents,whiten=True, random_state=1)\ncompactClusterDistributionHourOfYear = TripDistributionPCAModel.fit_transform(clusterDistributionHourOfYear)\n\n\n\nCollect traces for all weeks of year\n\nlistOfFullWeeks = []\nfor uniqueVal in np.unique(weekOfYearVec):\n    if (weekOfYearVec == uniqueVal).sum() == 24*7:\n        listOfFullWeeks.append(uniqueVal)\n\nweeklyTraces = np.zeros((24*7,numComponents,len(listOfFullWeeks)))\nfor k, weekInd in enumerate(listOfFullWeeks):\n    weeklyTraces[:,:,k] = compactClusterDistributionHourOfYear[weekOfYearVec == weekInd,:]\n\nfig, axArray = plt.subplots(nrows=numComponents,ncols=1,sharex=True, figsize=(10,10))\nfig.suptitle('PCA coefficients during the Week', fontsize=25)\nfor PC_coeff in range(numComponents):\n    meanTrace = weeklyTraces[:,PC_coeff,:].mean(axis=1)\n    axArray[PC_coeff].plot(weeklyTraces[:,PC_coeff,:],'red',linewidth=1.5)\n    axArray[PC_coeff].plot(meanTrace,'k',linewidth=2.5)\n    axArray[PC_coeff].set_ylabel('PC %d coeff' %(PC_coeff+1))\n    axArray[PC_coeff].vlines([0,23,47,71,95,119,143,167], weeklyTraces[:,PC_coeff,:].min(), weeklyTraces[:,PC_coeff,:].max(), colors='black', lw=2)\n\naxArray[PC_coeff].set_xlabel('hours since start of week')\naxArray[PC_coeff].set_xlim(-0.9,24*7-0.1)\n\n\n\nExamine what different PC coefficients mean by looking at their trip template distributions\n\nfig, axArray = plt.subplots(nrows=numComponents,ncols=1,sharex=True, figsize=(12,11))\nfig.suptitle('Trip Distribution PCA Components', fontsize=25)\nfor PC_coeff in range(numComponents):\n    tripTemplateDistributionDifference = TripDistributionPCAModel.components_[PC_coeff,:] * \\\n                                         TripDistributionPCAModel.explained_variance_[PC_coeff]\n    axArray[PC_coeff].bar(range(1,numClusters+1),tripTemplateDistributionDifference)\n    axArray[PC_coeff].set_title('PCA %d component' %(PC_coeff+1))\n    axArray[PC_coeff].set_ylabel('delta frequency [counts]')\n    \naxArray[PC_coeff].set_xlabel('cluster index (sorted by cluster frequency)')\naxArray[PC_coeff].set_xlim(0,numClusters+0.5)\n\naxArray[1].hlines([-25,25], 0, numClusters+0.5, colors='r', lw=0.7)\naxArray[2].hlines([-11,11], 0, numClusters+0.5, colors='r', lw=0.7)\n\nWe can see that the first PCA component looks very similar to the overall trip distribution, suggesting that it’s mainly a “gain” component that controls just the number of total trips in that period of time."
  },
  {
    "objectID": "Data_Mining/2022-05-26-exercise-proximity-analysis.html",
    "href": "Data_Mining/2022-05-26-exercise-proximity-analysis.html",
    "title": "Proximity Analysis",
    "section": "",
    "text": "Proximity Analysis\n\nThis notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link.\n\n\nimport math\nimport geopandas as gpd\nimport pandas as pd\nfrom shapely.geometry import MultiPolygon\n\nimport folium\nfrom folium import Choropleth, Marker\nfrom folium.plugins import HeatMap, MarkerCluster\n\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.geospatial.ex5 import *\n\nYou’ll use the embed_map() function to visualize your maps.\n\ndef embed_map(m, file_name):\n    from IPython.display import IFrame\n    m.save(file_name)\n    return IFrame(file_name, width='100%', height='500px')\n\n\nExercises\n\n1) Visualize the collision data.\nRun the code cell below to load a GeoDataFrame collisions tracking major motor vehicle collisions in 2013-2018.\n\ncollisions = gpd.read_file(\"../input/geospatial-learn-course-data/NYPD_Motor_Vehicle_Collisions/NYPD_Motor_Vehicle_Collisions/NYPD_Motor_Vehicle_Collisions.shp\")\ncollisions.head()\n\nUse the “LATITUDE” and “LONGITUDE” columns to create an interactive map to visualize the collision data. What type of map do you think is most effective?\n“LATITUDE” 및 “LONGITUDE” 열을 사용하여 충돌 데이터를 시각화하는 대화형 맵을 만듭니다. 어떤 유형의 지도가 가장 효과적이라고 생각하십니까?\n\nm_1 = folium.Map(location=[40.7, -74], zoom_start=11) \n\n# Your code here: Visualize the collision data\nHeatMap(data=collisions[['LATITUDE', 'LONGITUDE']], radius=9).add_to(m_1)\n\n# Uncomment to see a hint\n#q_1.hint()\n\n# Show the map\nembed_map(m_1, \"q_1.html\")\n\n\n# Get credit for your work after you have created a map\n#q_1.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_1.solution()\n\n\n\n2) Understand hospital coverage.\nRun the next code cell to load the hospital data.\n\nhospitals = gpd.read_file(\"../input/geospatial-learn-course-data/nyu_2451_34494/nyu_2451_34494/nyu_2451_34494.shp\")\nhospitals.head()\n\nUse the “latitude” and “longitude” columns to visualize the hospital locations.\n“위도” 및 “경도” 열을 사용하여 병원 위치를 시각화합니다.\n\nm_2 = folium.Map(location=[40.7, -74], zoom_start=11) \n\n# Your code here: Visualize the hospital locations\nfor idx, row in hospitals.iterrows():\n    Marker([row['latitude'], row['longitude']], popup=row['name']).add_to(m_2)\n    \n# Uncomment to see a hint\n#q_2.hint()\n        \n# Show the map\nembed_map(m_2, \"q_2.html\")\n\n\n# Get credit for your work after you have created a map\nq_2.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_2.solution()\n\n\n\n3) When was the closest hospital more than 10 kilometers away?\nCreate a DataFrame outside_range containing all rows from collisions with crashes that occurred more than 10 kilometers from the closest hospital.\n가장 가까운 병원에서 10km 이상 떨어진 곳에서 발생한 충돌이 있는 ’충돌’의 모든 행을 포함하는 DataFrame ’outside_range’를 만듭니다.\nNote that both hospitals and collisions have EPSG 2263 as the coordinate reference system, and EPSG 2263 has units of meters.\n‘병원’과 ’충돌’ 모두 좌표 참조 시스템으로 EPSG 2263을 사용하고 EPSG 2263은 미터 단위를 사용합니다.\n\n# Your code here\ncoverage = gpd.GeoDataFrame(geometry=hospitals.geometry).buffer(10000)\nmy_union = coverage.geometry.unary_union\noutside_range = collisions.loc[~collisions[\"geometry\"].apply(lambda x: my_union.contains(x))]\n\n# Check your answer\nq_3.check()\n\n\n# Lines below will give you a hint or solution code\n#q_3.hint()\n#q_3.solution()\n\nThe next code cell calculates the percentage of collisions that occurred more than 10 kilometers away from the closest hospital.\n\npercentage = round(100*len(outside_range)/len(collisions), 2)\nprint(\"Percentage of collisions more than 10 km away from the closest hospital: {}%\".format(percentage))\n\n\n\n4) Make a recommender.\nWhen collisions occur in distant locations, it becomes even more vital that injured persons are transported to the nearest available hospital.\n멀리 떨어진 곳에서 충돌이 발생하면 부상자를 가장 가까운 병원으로 이송하는 것이 더욱 중요해집니다.\nWith this in mind, you decide to create a recommender that: - takes the location of the crash (in EPSG 2263) as input, - finds the closest hospital (where distance calculations are done in EPSG 2263), and - returns the name of the closest hospital.\n\ndef best_hospital(collision_location):\n    idx_min = hospitals.geometry.distance(collision_location).idxmin()\n    my_hospital = hospitals.iloc[idx_min]\n    # Your code here\n    name = my_hospital[\"name\"]\n    return name\n\n# Test your function: this should suggest CALVARY HOSPITAL INC\nprint(best_hospital(outside_range.geometry.iloc[0]))\n\n# Check your answer\nq_4.check()\n\n\n# Lines below will give you a hint or solution code\n#q_4.hint()\n#q_4.solution()\n\n\n\n5) Which hospital is under the highest demand?\nConsidering only collisions in the outside_range DataFrame, which hospital is most recommended?\noutside_range DataFrame에서 충돌만 고려한다면 어느 병원을 가장 추천하는가?\nYour answer should be a Python string that exactly matches the name of the hospital returned by the function you created in 4).\n\n# Your code here\nhighest_demand = outside_range.geometry.apply(best_hospital).value_counts().idxmax()\n\n# Check your answer\nq_5.check()\n\n\n# Lines below will give you a hint or solution code\n#q_5.hint()\n#q_5.solution()\n\n\n\n6) Where should the city construct new hospitals?\nRun the next code cell (without changes) to visualize hospital locations, in addition to collisions that occurred more than 10 kilometers away from the closest hospital.\n다음 코드 셀(변경 없이)을 실행하여 가장 가까운 병원에서 10km 이상 떨어진 곳에서 발생한 충돌 외에도 병원 위치를 시각화합니다.\n\nm_6 = folium.Map(location=[40.7, -74], zoom_start=11) \n\ncoverage = gpd.GeoDataFrame(geometry=hospitals.geometry).buffer(10000)\nfolium.GeoJson(coverage.geometry.to_crs(epsg=4326)).add_to(m_6)\nHeatMap(data=outside_range[['LATITUDE', 'LONGITUDE']], radius=9).add_to(m_6)\nfolium.LatLngPopup().add_to(m_6)\n\nembed_map(m_6, 'm_6.html')\n\nClick anywhere on the map to see a pop-up with the corresponding location in latitude and longitude.\nThe city of New York reaches out to you for help with deciding locations for two brand new hospitals. They specifically want your help with identifying locations to bring the calculated percentage from step 3) to less than ten percent. Using the map (and without worrying about zoning laws or what potential buildings would have to be removed in order to build the hospitals), can you identify two locations that would help the city accomplish this goal?\nPut the proposed latitude and longitude for hospital 1 in lat_1 and long_1, respectively. (Likewise for hospital 2.)\n병원 1에 대해 제안된 위도와 경도를 각각 lat_1과 long_1에 넣습니다. (병원2도 마찬가지)\nThen, run the rest of the cell as-is to see the effect of the new hospitals. Your answer will be marked correct, if the two new hospitals bring the percentage to less than ten percent.\n그런 다음 나머지 셀을 그대로 실행하여 새 병원의 효과를 확인하십시오. 두 개의 새로운 병원에서 백분율을 10% 미만으로 낮추면 답이 정답으로 표시됩니다.\n\n# Your answer here: proposed location of hospital 1\nlat_1 = 40.6714\nlong_1 = -73.8492\n\n# Your answer here: proposed location of hospital 2\nlat_2 = 40.6702\nlong_2 = -73.7612\n\n# Do not modify the code below this line\ntry:\n    new_df = pd.DataFrame(\n        {'Latitude': [lat_1, lat_2],\n         'Longitude': [long_1, long_2]})\n    new_gdf = gpd.GeoDataFrame(new_df, geometry=gpd.points_from_xy(new_df.Longitude, new_df.Latitude))\n    new_gdf.crs = {'init' :'epsg:4326'}\n    new_gdf = new_gdf.to_crs(epsg=2263)\n    # get new percentage\n    new_coverage = gpd.GeoDataFrame(geometry=new_gdf.geometry).buffer(10000)\n    new_my_union = new_coverage.geometry.unary_union\n    new_outside_range = outside_range.loc[~outside_range[\"geometry\"].apply(lambda x: new_my_union.contains(x))]\n    new_percentage = round(100*len(new_outside_range)/len(collisions), 2)\n    print(\"(NEW) Percentage of collisions more than 10 km away from the closest hospital: {}%\".format(new_percentage))\n    # Did you help the city to meet its goal?\n    q_6.check()\n    # make the map\n    m = folium.Map(location=[40.7, -74], zoom_start=11) \n    folium.GeoJson(coverage.geometry.to_crs(epsg=4326)).add_to(m)\n    folium.GeoJson(new_coverage.geometry.to_crs(epsg=4326)).add_to(m)\n    for idx, row in new_gdf.iterrows():\n        Marker([row['Latitude'], row['Longitude']]).add_to(m)\n    HeatMap(data=new_outside_range[['LATITUDE', 'LONGITUDE']], radius=9).add_to(m)\n    folium.LatLngPopup().add_to(m)\n    display(embed_map(m, 'q_6.html'))\nexcept:\n    q_6.hint()\n\n\n# Uncomment to see one potential answer \n#q_6.solution()\n\n\n\n\nCongratulations!\nYou have just completed the Geospatial Analysis micro-course! Great job!\n\nHave questions or comments? Visit the course discussion forum to chat with other learners."
  },
  {
    "objectID": "Data_Mining.html",
    "href": "Data_Mining.html",
    "title": "Data Mining",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nCOVID-19 Analysis & Visualization\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\n\n\n\n\n\n\n\n\n\nJun 7, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\nManipulating Geospatial Data\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nFolium\n\n\nGeopandas\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\nProximity Analysis\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nFolium\n\n\nGeopandas\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\nCoordinate Refrence Systems\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nGeopandas\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\nInteractive Maps\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nGeopandas\n\n\nFolium\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\nYour First Map\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nGeopandas\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\nNYC taxi\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nScipy\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\nVisualization_with_seaborn_and_matplotlib\n\n\n\n\n\n\n\nPython\n\n\nSeaborn\n\n\nMatplotlib\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\nPython 기초\n\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nMar 18, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\nNumpy 기본\n\n\n\n\n\n\n\nPython\n\n\nNumpy\n\n\n\n\n\n\n\n\n\n\n\nMar 4, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Profile",
    "section": "",
    "text": "한남대학교 비즈니스통계학과 | 2018.03 ~ 2021.02\n한남대학교 빅데이터응용학과 | 2021.03 ~ 2024.02 \n\n\n\n\n\n과학기술정보통신부 장관상 | 2022.09\n한국수자원공사 사장상 | 2022.10 \n\n\n\n\n\n데이터 청년 캠퍼스 | 2022.08 ~ 2022.09\n빅리더 AI 산학협력 | 2022.08 ~ 2022.10\n멋쟁이사자처럼 11기 | 2023.03 ~ 2023.12\n태블로 신병훈련소 21기 | 2023.10 ~ 2023.11\n\n\n\n\n\n데이터 분석 준전문가 (ADsP) | 2023.06\nSQL 개발자 (SQLD) | 2023.10\n빅데이터분석기사_필기 (BAE) | 2023.10"
  },
  {
    "objectID": "index.html#hyunsoo-kim",
    "href": "index.html#hyunsoo-kim",
    "title": "Profile",
    "section": "",
    "text": "한남대학교 비즈니스통계학과 | 2018.03 ~ 2021.02\n한남대학교 빅데이터응용학과 | 2021.03 ~ 2024.02 \n\n\n\n\n\n과학기술정보통신부 장관상 | 2022.09\n한국수자원공사 사장상 | 2022.10 \n\n\n\n\n\n데이터 청년 캠퍼스 | 2022.08 ~ 2022.09\n빅리더 AI 산학협력 | 2022.08 ~ 2022.10\n멋쟁이사자처럼 11기 | 2023.03 ~ 2023.12\n태블로 신병훈련소 21기 | 2023.10 ~ 2023.11\n\n\n\n\n\n데이터 분석 준전문가 (ADsP) | 2023.06\nSQL 개발자 (SQLD) | 2023.10\n빅데이터분석기사_필기 (BAE) | 2023.10"
  },
  {
    "objectID": "OpenData_Analysis.html",
    "href": "OpenData_Analysis.html",
    "title": "OpenData Analysis",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n빅분기 실기 - 1유형 문제풀이\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "R_Basic.html",
    "href": "R_Basic.html",
    "title": "R Basic",
    "section": "",
    "text": "모두를 위한 R 데이터 분석 입문\n\n\n\n\n2 + 3  # 2 더하기 3\n## [1] 5\n(3 + 6) * 8\n## [1] 72\n2 ^ 3  # 2의 세제곱\n## [1] 8\n8 %% 3\n## [1] 2\n\n\n7 + 4\n## [1] 11\n\n\nlog(10) + 5 # 로그함수\n## [1] 7.302585\nsqrt(25) # 제곱근\n## [1] 5\nmax(5, 3, 2) # 가장 큰 값\n## [1] 5\n\n\na &lt;- 10\nb &lt;- 20\nc &lt;- a+b\nprint(c)\n## [1] 30\n\n\na &lt;- 125\na\n## [1] 125\nprint(a)\n## [1] 125\n\n\na &lt;- 10 # a에 숫자 저장\nb &lt;- 20\na + b # a+b의 결과 출력\n## [1] 30\na &lt;- \"A\" # a에 문자 저장\na + b # a+b의 결과 출력. 에러 발생\n## Error in a + b: non-numeric argument to binary operator\n\n\nx &lt;- c(1, 2, 3) # 숫자형 벡터\ny &lt;- c(\"a\", \"b\", \"c\") # 문자형 벡터\nz &lt;- c(TRUE, TRUE, FALSE, TRUE) # 논리형 벡터\nx ; y ;z\n## [1] 1 2 3\n## [1] \"a\" \"b\" \"c\"\n## [1]  TRUE  TRUE FALSE  TRUE\n\n\nw &lt;- c(1, 2, 3, \"a\", \"b\", \"c\")\nw\n## [1] \"1\" \"2\" \"3\" \"a\" \"b\" \"c\"\n\n\nv1 &lt;- 50:90\nv1\n##  [1] 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74\n## [26] 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90\nv2 &lt;- c(1, 2, 5, 50:90)\nv2\n##  [1]  1  2  5 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n## [26] 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90\n\n\nv3 &lt;- seq(1, 101, 3)\nv3\n##  [1]   1   4   7  10  13  16  19  22  25  28  31  34  37  40  43  46  49  52  55\n## [20]  58  61  64  67  70  73  76  79  82  85  88  91  94  97 100\nv4 &lt;- seq(0.1, 1.0, 0.1)\nv4\n##  [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n\n\nv5 &lt;- rep(1, times = 5) # 1을 5번 반복\nv5\n## [1] 1 1 1 1 1\nv6 &lt;- rep(1:5, times = 3) # 1에서 5까지 3번 반복\nv6\n##  [1] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5\nv7 &lt;- rep(c(1, 5, 9), times = 3) # 1, 5, 9를 3번 반복\nv7\n## [1] 1 5 9 1 5 9 1 5 9\nv8 &lt;- rep(1:5, each = 3) # 1에서 5를 각각 3번 반복\nv8\n##  [1] 1 1 1 2 2 2 3 3 3 4 4 4 5 5 5\n\nrep(1:3, each = 3, times = 3)\n##  [1] 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3\nrep(1:3, times = 3, each = 3)\n##  [1] 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3\n\n\nscore &lt;- c(90, 85, 70) # 성적\nscore\n## [1] 90 85 70\nnames(score) # score에 저장된 값들의 이름을 보이시오\n## NULL\nnames(score) &lt;- c(\"John\", \"Tom\", \"Jane\") # 값들에 이름을 부여\nnames(score) # score에 저장된 값들의 이름을 보이시오\n## [1] \"John\" \"Tom\"  \"Jane\"\nscore # 이름과 함께 값이 출력\n## John  Tom Jane \n##   90   85   70\n\n\nd &lt;- c(1, 4, 3, 7, 8)\nd[1]\n## [1] 1\nd[2]\n## [1] 4\nd[3]\n## [1] 3\nd[4]\n## [1] 7\nd[5]\n## [1] 8\nd[6]\n## [1] NA\nd[c(2, 4)]\n## [1] 4 7\n\n\nd &lt;- c(1, 4, 3, 7, 8)\nd[c(1, 3, 5)] # 1, 3, 5번째 값 출력\n## [1] 1 3 8\nd[1:3] # 처음 세 개의 값 출력\n## [1] 1 4 3\nd[seq(1, 5, 2)] # 홀수 번째 값 출력\n## [1] 1 3 8\nd[-2] # 2번째 값 제외하고 출력\n## [1] 1 3 7 8\nd[-c(3:5)] # 3~5번째 값은 제외하고 출력\n## [1] 1 4\n\n\nGNP &lt;- c(2000, 2450, 960)\nGNP\n## [1] 2000 2450  960\nnames(GNP) &lt;- c(\"Korea\", \"Japan\", \"Nepal\")\nGNP\n## Korea Japan Nepal \n##  2000  2450   960\nGNP[1]\n## Korea \n##  2000\nGNP[\"Korea\"]\n## Korea \n##  2000\nGNP_NEW &lt;- GNP[c(\"Korea\", \"Nepal\")]\nGNP_NEW\n## Korea Nepal \n##  2000   960\n\n\nv1 &lt;- c(1, 5, 7, 8, 9)\nv1\n## [1] 1 5 7 8 9\nv1[2] &lt;- 3 # v1의 2번째 값을 3으로 변경\nv1\n## [1] 1 3 7 8 9\nv1[c(1, 5)] &lt;- c(10, 20) # v1의 1, 5번째 값을 각각 10, 20으로 변경\nv1\n## [1] 10  3  7  8 20\n\n\nd &lt;- c(1, 4, 3, 7, 8)\n2 * d\n## [1]  2  8  6 14 16\nd - 5\n## [1] -4 -1 -2  2  3\n3 * d + 4\n## [1]  7 16 13 25 28\n\n\nx &lt;- c(1, 2, 3)\ny &lt;- c(4, 5, 6)\nx + y # 대응하는 원소끼리 더하여 출력\n## [1] 5 7 9\nx * y # 대응하는 원소끼리 곱하여 출력\n## [1]  4 10 18\nz &lt;- x + y # x, y를 더하여 z에 저장\nz\n## [1] 5 7 9\n\n\nd &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\nsum(d) # d에 포함된 값들의 합\n## [1] 55\nsum(2 * d) # d에 포함된 값들에 2를 곱한 후 합한 값\n## [1] 110\nlength(d) # d에 포함된 값들의 개수\n## [1] 10\nmean(d[1:5]) # 1~5번째 값들의 평균\n## [1] 3\nmax(d) # d에 포함된 값들의 최댓값\n## [1] 10\nmin(d) # d에 포함된 값들의 최솟값\n## [1] 1\nsort(d) # 오름차순 정렬\n##  [1]  1  2  3  4  5  6  7  8  9 10\nsort(d, decreasing = FALSE) # 오름차순 정렬\n##  [1]  1  2  3  4  5  6  7  8  9 10\nsort(d, decreasing = TRUE) # 내림차순 정렬\n##  [1] 10  9  8  7  6  5  4  3  2  1\nsort(d, TRUE) # 내림차순 정렬\n##  [1] 10  9  8  7  6  5  4  3  2  1\n\nv1 &lt;- median(d)\nv1\n## [1] 5.5\nv2 &lt;- sum(d) / length(d)\nv2\n## [1] 5.5\nmean(d)\n## [1] 5.5\n\n\nd &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9)\nd &gt;= 5\n## [1] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\nd[d &gt; 5] # 5보다 큰 값\n## [1] 6 7 8 9\nsum(d &gt; 5) # 5보다 큰 값의 개수를 출력\n## [1] 4\nsum(d[d &gt; 5]) # 5보다 큰 값의 합계를 출력\n## [1] 30\nd == 5\n## [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n\ncondi &lt;- d &gt; 5 & d &lt; 8 # 조건을 변수에 저장\nd[condi] # 조건에 맞는 값들을 선택\n## [1] 6 7\nd[d &gt; 5 & d &lt; 8]\n## [1] 6 7\n\n\nds &lt;- c(90, 85, 70, 84)\nmy.info &lt;- list(name = 'Tom', age = 60, status = TRUE, score = ds)\nmy.info # 리스트에 저장된 내용을 모두 출력\n## $name\n## [1] \"Tom\"\n## \n## $age\n## [1] 60\n## \n## $status\n## [1] TRUE\n## \n## $score\n## [1] 90 85 70 84\nmy.info[1] # 이름이랑 내용 다 출력\n## $name\n## [1] \"Tom\"\nmy.info[[1]] # 리스트의 첫 번째 값을 출력\n## [1] \"Tom\"\nmy.info$name # 리스트에서 값의 이름이 name인 값을 출력\n## [1] \"Tom\"\nmy.info[[4]] # 리스트의 네 번째 값을 출력\n## [1] 90 85 70 84\n\n\nbt &lt;- c('A', 'B', 'B', 'O', 'AB', 'A') # 문자형 벡터 bt 정의\nbt.new &lt;- factor(bt) # 팩터 bt.new 정의\nbt # 벡터 bt의 내용 출력\n## [1] \"A\"  \"B\"  \"B\"  \"O\"  \"AB\" \"A\"\nbt.new # 팩터 bt.new의 내용 출력\n## [1] A  B  B  O  AB A \n## Levels: A AB B O\nbt[5] # 벡터 bt의 5번째 값 출력\n## [1] \"AB\"\nbt.new[5] # 팩터 bt.new의 5번째 값 출력\n## [1] AB\n## Levels: A AB B O\nlevels(bt.new) # 팩터에 저장된 값의 종류를 출력\n## [1] \"A\"  \"AB\" \"B\"  \"O\"\nas.integer(bt.new) # 팩터의 문자값을 숫자로 바꾸어 출력\n## [1] 1 3 3 4 2 1\nbt.new[7] &lt;- 'B' # 팩터 bt.new의 7번째에 'B' 저장\nbt.new[8] &lt;- 'C' # 팩터 bt.new의 8번째에 'C' 저장\n## Warning in `[&lt;-.factor`(`*tmp*`, 8, value = \"C\"): invalid factor level, NA\n## generated\nbt.new # 팩터 bt.new의 내용 출력\n## [1] A    B    B    O    AB   A    B    &lt;NA&gt;\n## Levels: A AB B O\n\n\n\n\n\nz &lt;- matrix(1:20, nrow = 4, ncol = 5)\nz # 매트릭스 z의 내용을 출력\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\n\n\nz2 &lt;- matrix(1:20, nrow = 4, ncol = 5, byrow = T)\nz2 # 매트릭스 z2의 내용을 출력\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    2    3    4    5\n## [2,]    6    7    8    9   10\n## [3,]   11   12   13   14   15\n## [4,]   16   17   18   19   20\n\nz &lt;- matrix(1:16, nrow = 4, ncol = 5)\n## Warning in matrix(1:16, nrow = 4, ncol = 5): data length [16] is not a\n## sub-multiple or multiple of the number of columns [5]\nz\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13    1\n## [2,]    2    6   10   14    2\n## [3,]    3    7   11   15    3\n## [4,]    4    8   12   16    4\n\n\nx &lt;- 1:4 # 벡터 x 생성\ny &lt;- 5:8 # 벡터 y 생성\nz &lt;- matrix(1:20, nrow = 4, ncol = 5) # 매트릭스 z 생성\n\nm1 &lt;- cbind(x, y) # x와 y를 열 방향으로 결합하여 매트릭스 생성\nm1 # 매트릭스 m1의 내용을 출력\n##      x y\n## [1,] 1 5\n## [2,] 2 6\n## [3,] 3 7\n## [4,] 4 8\nm2 &lt;- rbind(x, y) # x와 y를 행 방향으로 결합하여 매트릭스 생성\nm2 # 매트릭스 m2의 내용을 출력\n##   [,1] [,2] [,3] [,4]\n## x    1    2    3    4\n## y    5    6    7    8\nm3 &lt;- rbind(m2, x) # m2와 벡터 x를 행 방향으로 결합\nm3 # 매트릭스 m3의 내용을 출력\n##   [,1] [,2] [,3] [,4]\n## x    1    2    3    4\n## y    5    6    7    8\n## x    1    2    3    4\nm4 &lt;- cbind(z, x) # 매트릭스 z와 벡터 x를 열 방향으로 결합\nm4 # 매트릭스 m4의 내용을 출력\n##                   x\n## [1,] 1 5  9 13 17 1\n## [2,] 2 6 10 14 18 2\n## [3,] 3 7 11 15 19 3\n## [4,] 4 8 12 16 20 4\n\nx &lt;- 1:5\nm5 &lt;- cbind(z, x)\n## Warning in cbind(z, x): number of rows of result is not a multiple of vector\n## length (arg 2)\nm5\n##                   x\n## [1,] 1 5  9 13 17 1\n## [2,] 2 6 10 14 18 2\n## [3,] 3 7 11 15 19 3\n## [4,] 4 8 12 16 20 4\n\n\nz &lt;- matrix(1:20, nrow = 4, ncol = 5) # 매트릭스 z 생성\nz # 매트릭스 z의 내용 출력\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\n\nz[2, 3] # 2행 3열에 있는 값\n## [1] 10\nz[1, 4] # 1행 4열에 있는 값\n## [1] 13\nz[2, ] # 2행에 있는 모든 값\n## [1]  2  6 10 14 18\nz[, 4] # 4열에 있는 모든 값\n## [1] 13 14 15 16\nz[, ]\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\nz\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\n\n\nz &lt;- matrix(1:20, nrow = 4, ncol = 5) # 매트릭스 z 생성\nz # 매트릭스 z의 내용 출력\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\n\nz[2, 1:3] # 2행의 값 중 1~3열에 있는 값\n## [1]  2  6 10\nz[1, c(1, 2, 4)] # 1행의 값 중 1, 2, 4열에 있는 값\n## [1]  1  5 13\nz[1:2, ] # 1, 2행에 있는 모든 값\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\nz[, c(1, 4)] # 1, 4열에 있는 모든 값\n##      [,1] [,2]\n## [1,]    1   13\n## [2,]    2   14\n## [3,]    3   15\n## [4,]    4   16\n\n\nscore &lt;- matrix(c(90, 85, 69, 78,\n                  85, 96, 49, 95,\n                  90, 80, 70, 60),\n                nrow = 4,\n                ncol = 3)\nscore\n##      [,1] [,2] [,3]\n## [1,]   90   85   90\n## [2,]   85   96   80\n## [3,]   69   49   70\n## [4,]   78   95   60\nrownames(score) &lt;- c('John', 'Tom', 'Mark', 'Jane')\ncolnames(score) &lt;- c('English', 'Math', 'Science')\nscore\n##      English Math Science\n## John      90   85      90\n## Tom       85   96      80\n## Mark      69   49      70\n## Jane      78   95      60\n\n\nscore['John', 'Math'] # John의 수학 성적\n## [1] 85\nscore['Tom', c('Math', 'Science')] # Tom의 수학, 과학 성적\n##    Math Science \n##      96      80\nscore['Mark', ] # Mark의 모든 과목 성적\n## English    Math Science \n##      69      49      70\nscore[, 'English'] # 모든 학생의 영어 성적\n## John  Tom Mark Jane \n##   90   85   69   78\nrownames(score) # score의 행의 이름\n## [1] \"John\" \"Tom\"  \"Mark\" \"Jane\"\ncolnames(score) # score의 열의 이름\n## [1] \"English\" \"Math\"    \"Science\"\ncolnames(score)[2] # score의 열의 이름 중 두 번째 값\n## [1] \"Math\"\n\n\ncity &lt;- c(\"Seoul\", \"Tokyo\", \"Washington\") # 문자로 이루어진 벡터\nrank &lt;- c(1, 3, 2) # 숫자로 이루어진 벡터\ncity.info &lt;- data.frame(city, rank) # 데이터프레임 생성\ncity.info # city.info의 내용 출력\n##         city rank\n## 1      Seoul    1\n## 2      Tokyo    3\n## 3 Washington    2\n\n\n# iris\niris[, c(1:2)] # 1, 2열의 모든 데이터\n##     Sepal.Length Sepal.Width\n## 1            5.1         3.5\n## 2            4.9         3.0\n## 3            4.7         3.2\n## 4            4.6         3.1\n## 5            5.0         3.6\n## 6            5.4         3.9\n## 7            4.6         3.4\n## 8            5.0         3.4\n## 9            4.4         2.9\n## 10           4.9         3.1\n## 11           5.4         3.7\n## 12           4.8         3.4\n## 13           4.8         3.0\n## 14           4.3         3.0\n## 15           5.8         4.0\n## 16           5.7         4.4\n## 17           5.4         3.9\n## 18           5.1         3.5\n## 19           5.7         3.8\n## 20           5.1         3.8\n## 21           5.4         3.4\n## 22           5.1         3.7\n## 23           4.6         3.6\n## 24           5.1         3.3\n## 25           4.8         3.4\n## 26           5.0         3.0\n## 27           5.0         3.4\n## 28           5.2         3.5\n## 29           5.2         3.4\n## 30           4.7         3.2\n## 31           4.8         3.1\n## 32           5.4         3.4\n## 33           5.2         4.1\n## 34           5.5         4.2\n## 35           4.9         3.1\n## 36           5.0         3.2\n## 37           5.5         3.5\n## 38           4.9         3.6\n## 39           4.4         3.0\n## 40           5.1         3.4\n## 41           5.0         3.5\n## 42           4.5         2.3\n## 43           4.4         3.2\n## 44           5.0         3.5\n## 45           5.1         3.8\n## 46           4.8         3.0\n## 47           5.1         3.8\n## 48           4.6         3.2\n## 49           5.3         3.7\n## 50           5.0         3.3\n## 51           7.0         3.2\n## 52           6.4         3.2\n## 53           6.9         3.1\n## 54           5.5         2.3\n## 55           6.5         2.8\n## 56           5.7         2.8\n## 57           6.3         3.3\n## 58           4.9         2.4\n## 59           6.6         2.9\n## 60           5.2         2.7\n## 61           5.0         2.0\n## 62           5.9         3.0\n## 63           6.0         2.2\n## 64           6.1         2.9\n## 65           5.6         2.9\n## 66           6.7         3.1\n## 67           5.6         3.0\n## 68           5.8         2.7\n## 69           6.2         2.2\n## 70           5.6         2.5\n## 71           5.9         3.2\n## 72           6.1         2.8\n## 73           6.3         2.5\n## 74           6.1         2.8\n## 75           6.4         2.9\n## 76           6.6         3.0\n## 77           6.8         2.8\n## 78           6.7         3.0\n## 79           6.0         2.9\n## 80           5.7         2.6\n## 81           5.5         2.4\n## 82           5.5         2.4\n## 83           5.8         2.7\n## 84           6.0         2.7\n## 85           5.4         3.0\n## 86           6.0         3.4\n## 87           6.7         3.1\n## 88           6.3         2.3\n## 89           5.6         3.0\n## 90           5.5         2.5\n## 91           5.5         2.6\n## 92           6.1         3.0\n## 93           5.8         2.6\n## 94           5.0         2.3\n## 95           5.6         2.7\n## 96           5.7         3.0\n## 97           5.7         2.9\n## 98           6.2         2.9\n## 99           5.1         2.5\n## 100          5.7         2.8\n## 101          6.3         3.3\n## 102          5.8         2.7\n## 103          7.1         3.0\n## 104          6.3         2.9\n## 105          6.5         3.0\n## 106          7.6         3.0\n## 107          4.9         2.5\n## 108          7.3         2.9\n## 109          6.7         2.5\n## 110          7.2         3.6\n## 111          6.5         3.2\n## 112          6.4         2.7\n## 113          6.8         3.0\n## 114          5.7         2.5\n## 115          5.8         2.8\n## 116          6.4         3.2\n## 117          6.5         3.0\n## 118          7.7         3.8\n## 119          7.7         2.6\n## 120          6.0         2.2\n## 121          6.9         3.2\n## 122          5.6         2.8\n## 123          7.7         2.8\n## 124          6.3         2.7\n## 125          6.7         3.3\n## 126          7.2         3.2\n## 127          6.2         2.8\n## 128          6.1         3.0\n## 129          6.4         2.8\n## 130          7.2         3.0\n## 131          7.4         2.8\n## 132          7.9         3.8\n## 133          6.4         2.8\n## 134          6.3         2.8\n## 135          6.1         2.6\n## 136          7.7         3.0\n## 137          6.3         3.4\n## 138          6.4         3.1\n## 139          6.0         3.0\n## 140          6.9         3.1\n## 141          6.7         3.1\n## 142          6.9         3.1\n## 143          5.8         2.7\n## 144          6.8         3.2\n## 145          6.7         3.3\n## 146          6.7         3.0\n## 147          6.3         2.5\n## 148          6.5         3.0\n## 149          6.2         3.4\n## 150          5.9         3.0\niris[, c(1, 3, 5)] # 1, 3, 5열의 모든 데이터\n##     Sepal.Length Petal.Length    Species\n## 1            5.1          1.4     setosa\n## 2            4.9          1.4     setosa\n## 3            4.7          1.3     setosa\n## 4            4.6          1.5     setosa\n## 5            5.0          1.4     setosa\n## 6            5.4          1.7     setosa\n## 7            4.6          1.4     setosa\n## 8            5.0          1.5     setosa\n## 9            4.4          1.4     setosa\n## 10           4.9          1.5     setosa\n## 11           5.4          1.5     setosa\n## 12           4.8          1.6     setosa\n## 13           4.8          1.4     setosa\n## 14           4.3          1.1     setosa\n## 15           5.8          1.2     setosa\n## 16           5.7          1.5     setosa\n## 17           5.4          1.3     setosa\n## 18           5.1          1.4     setosa\n## 19           5.7          1.7     setosa\n## 20           5.1          1.5     setosa\n## 21           5.4          1.7     setosa\n## 22           5.1          1.5     setosa\n## 23           4.6          1.0     setosa\n## 24           5.1          1.7     setosa\n## 25           4.8          1.9     setosa\n## 26           5.0          1.6     setosa\n## 27           5.0          1.6     setosa\n## 28           5.2          1.5     setosa\n## 29           5.2          1.4     setosa\n## 30           4.7          1.6     setosa\n## 31           4.8          1.6     setosa\n## 32           5.4          1.5     setosa\n## 33           5.2          1.5     setosa\n## 34           5.5          1.4     setosa\n## 35           4.9          1.5     setosa\n## 36           5.0          1.2     setosa\n## 37           5.5          1.3     setosa\n## 38           4.9          1.4     setosa\n## 39           4.4          1.3     setosa\n## 40           5.1          1.5     setosa\n## 41           5.0          1.3     setosa\n## 42           4.5          1.3     setosa\n## 43           4.4          1.3     setosa\n## 44           5.0          1.6     setosa\n## 45           5.1          1.9     setosa\n## 46           4.8          1.4     setosa\n## 47           5.1          1.6     setosa\n## 48           4.6          1.4     setosa\n## 49           5.3          1.5     setosa\n## 50           5.0          1.4     setosa\n## 51           7.0          4.7 versicolor\n## 52           6.4          4.5 versicolor\n## 53           6.9          4.9 versicolor\n## 54           5.5          4.0 versicolor\n## 55           6.5          4.6 versicolor\n## 56           5.7          4.5 versicolor\n## 57           6.3          4.7 versicolor\n## 58           4.9          3.3 versicolor\n## 59           6.6          4.6 versicolor\n## 60           5.2          3.9 versicolor\n## 61           5.0          3.5 versicolor\n## 62           5.9          4.2 versicolor\n## 63           6.0          4.0 versicolor\n## 64           6.1          4.7 versicolor\n## 65           5.6          3.6 versicolor\n## 66           6.7          4.4 versicolor\n## 67           5.6          4.5 versicolor\n## 68           5.8          4.1 versicolor\n## 69           6.2          4.5 versicolor\n## 70           5.6          3.9 versicolor\n## 71           5.9          4.8 versicolor\n## 72           6.1          4.0 versicolor\n## 73           6.3          4.9 versicolor\n## 74           6.1          4.7 versicolor\n## 75           6.4          4.3 versicolor\n## 76           6.6          4.4 versicolor\n## 77           6.8          4.8 versicolor\n## 78           6.7          5.0 versicolor\n## 79           6.0          4.5 versicolor\n## 80           5.7          3.5 versicolor\n## 81           5.5          3.8 versicolor\n## 82           5.5          3.7 versicolor\n## 83           5.8          3.9 versicolor\n## 84           6.0          5.1 versicolor\n## 85           5.4          4.5 versicolor\n## 86           6.0          4.5 versicolor\n## 87           6.7          4.7 versicolor\n## 88           6.3          4.4 versicolor\n## 89           5.6          4.1 versicolor\n## 90           5.5          4.0 versicolor\n## 91           5.5          4.4 versicolor\n## 92           6.1          4.6 versicolor\n## 93           5.8          4.0 versicolor\n## 94           5.0          3.3 versicolor\n## 95           5.6          4.2 versicolor\n## 96           5.7          4.2 versicolor\n## 97           5.7          4.2 versicolor\n## 98           6.2          4.3 versicolor\n## 99           5.1          3.0 versicolor\n## 100          5.7          4.1 versicolor\n## 101          6.3          6.0  virginica\n## 102          5.8          5.1  virginica\n## 103          7.1          5.9  virginica\n## 104          6.3          5.6  virginica\n## 105          6.5          5.8  virginica\n## 106          7.6          6.6  virginica\n## 107          4.9          4.5  virginica\n## 108          7.3          6.3  virginica\n## 109          6.7          5.8  virginica\n## 110          7.2          6.1  virginica\n## 111          6.5          5.1  virginica\n## 112          6.4          5.3  virginica\n## 113          6.8          5.5  virginica\n## 114          5.7          5.0  virginica\n## 115          5.8          5.1  virginica\n## 116          6.4          5.3  virginica\n## 117          6.5          5.5  virginica\n## 118          7.7          6.7  virginica\n## 119          7.7          6.9  virginica\n## 120          6.0          5.0  virginica\n## 121          6.9          5.7  virginica\n## 122          5.6          4.9  virginica\n## 123          7.7          6.7  virginica\n## 124          6.3          4.9  virginica\n## 125          6.7          5.7  virginica\n## 126          7.2          6.0  virginica\n## 127          6.2          4.8  virginica\n## 128          6.1          4.9  virginica\n## 129          6.4          5.6  virginica\n## 130          7.2          5.8  virginica\n## 131          7.4          6.1  virginica\n## 132          7.9          6.4  virginica\n## 133          6.4          5.6  virginica\n## 134          6.3          5.1  virginica\n## 135          6.1          5.6  virginica\n## 136          7.7          6.1  virginica\n## 137          6.3          5.6  virginica\n## 138          6.4          5.5  virginica\n## 139          6.0          4.8  virginica\n## 140          6.9          5.4  virginica\n## 141          6.7          5.6  virginica\n## 142          6.9          5.1  virginica\n## 143          5.8          5.1  virginica\n## 144          6.8          5.9  virginica\n## 145          6.7          5.7  virginica\n## 146          6.7          5.2  virginica\n## 147          6.3          5.0  virginica\n## 148          6.5          5.2  virginica\n## 149          6.2          5.4  virginica\n## 150          5.9          5.1  virginica\niris[, c(\"Sepal.Length\", \"Species\")] # 1, 5열의 모든 데이터\n##     Sepal.Length    Species\n## 1            5.1     setosa\n## 2            4.9     setosa\n## 3            4.7     setosa\n## 4            4.6     setosa\n## 5            5.0     setosa\n## 6            5.4     setosa\n## 7            4.6     setosa\n## 8            5.0     setosa\n## 9            4.4     setosa\n## 10           4.9     setosa\n## 11           5.4     setosa\n## 12           4.8     setosa\n## 13           4.8     setosa\n## 14           4.3     setosa\n## 15           5.8     setosa\n## 16           5.7     setosa\n## 17           5.4     setosa\n## 18           5.1     setosa\n## 19           5.7     setosa\n## 20           5.1     setosa\n## 21           5.4     setosa\n## 22           5.1     setosa\n## 23           4.6     setosa\n## 24           5.1     setosa\n## 25           4.8     setosa\n## 26           5.0     setosa\n## 27           5.0     setosa\n## 28           5.2     setosa\n## 29           5.2     setosa\n## 30           4.7     setosa\n## 31           4.8     setosa\n## 32           5.4     setosa\n## 33           5.2     setosa\n## 34           5.5     setosa\n## 35           4.9     setosa\n## 36           5.0     setosa\n## 37           5.5     setosa\n## 38           4.9     setosa\n## 39           4.4     setosa\n## 40           5.1     setosa\n## 41           5.0     setosa\n## 42           4.5     setosa\n## 43           4.4     setosa\n## 44           5.0     setosa\n## 45           5.1     setosa\n## 46           4.8     setosa\n## 47           5.1     setosa\n## 48           4.6     setosa\n## 49           5.3     setosa\n## 50           5.0     setosa\n## 51           7.0 versicolor\n## 52           6.4 versicolor\n## 53           6.9 versicolor\n## 54           5.5 versicolor\n## 55           6.5 versicolor\n## 56           5.7 versicolor\n## 57           6.3 versicolor\n## 58           4.9 versicolor\n## 59           6.6 versicolor\n## 60           5.2 versicolor\n## 61           5.0 versicolor\n## 62           5.9 versicolor\n## 63           6.0 versicolor\n## 64           6.1 versicolor\n## 65           5.6 versicolor\n## 66           6.7 versicolor\n## 67           5.6 versicolor\n## 68           5.8 versicolor\n## 69           6.2 versicolor\n## 70           5.6 versicolor\n## 71           5.9 versicolor\n## 72           6.1 versicolor\n## 73           6.3 versicolor\n## 74           6.1 versicolor\n## 75           6.4 versicolor\n## 76           6.6 versicolor\n## 77           6.8 versicolor\n## 78           6.7 versicolor\n## 79           6.0 versicolor\n## 80           5.7 versicolor\n## 81           5.5 versicolor\n## 82           5.5 versicolor\n## 83           5.8 versicolor\n## 84           6.0 versicolor\n## 85           5.4 versicolor\n## 86           6.0 versicolor\n## 87           6.7 versicolor\n## 88           6.3 versicolor\n## 89           5.6 versicolor\n## 90           5.5 versicolor\n## 91           5.5 versicolor\n## 92           6.1 versicolor\n## 93           5.8 versicolor\n## 94           5.0 versicolor\n## 95           5.6 versicolor\n## 96           5.7 versicolor\n## 97           5.7 versicolor\n## 98           6.2 versicolor\n## 99           5.1 versicolor\n## 100          5.7 versicolor\n## 101          6.3  virginica\n## 102          5.8  virginica\n## 103          7.1  virginica\n## 104          6.3  virginica\n## 105          6.5  virginica\n## 106          7.6  virginica\n## 107          4.9  virginica\n## 108          7.3  virginica\n## 109          6.7  virginica\n## 110          7.2  virginica\n## 111          6.5  virginica\n## 112          6.4  virginica\n## 113          6.8  virginica\n## 114          5.7  virginica\n## 115          5.8  virginica\n## 116          6.4  virginica\n## 117          6.5  virginica\n## 118          7.7  virginica\n## 119          7.7  virginica\n## 120          6.0  virginica\n## 121          6.9  virginica\n## 122          5.6  virginica\n## 123          7.7  virginica\n## 124          6.3  virginica\n## 125          6.7  virginica\n## 126          7.2  virginica\n## 127          6.2  virginica\n## 128          6.1  virginica\n## 129          6.4  virginica\n## 130          7.2  virginica\n## 131          7.4  virginica\n## 132          7.9  virginica\n## 133          6.4  virginica\n## 134          6.3  virginica\n## 135          6.1  virginica\n## 136          7.7  virginica\n## 137          6.3  virginica\n## 138          6.4  virginica\n## 139          6.0  virginica\n## 140          6.9  virginica\n## 141          6.7  virginica\n## 142          6.9  virginica\n## 143          5.8  virginica\n## 144          6.8  virginica\n## 145          6.7  virginica\n## 146          6.7  virginica\n## 147          6.3  virginica\n## 148          6.5  virginica\n## 149          6.2  virginica\n## 150          5.9  virginica\niris[1:5, ] # 1~5행의 모든 데이터\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\niris[1:5, c(1, 3)] # 1~5행의 데이터 중 1, 3열의 데이터\n##   Sepal.Length Petal.Length\n## 1          5.1          1.4\n## 2          4.9          1.4\n## 3          4.7          1.3\n## 4          4.6          1.5\n## 5          5.0          1.4\n\n\ndim(iris) # 행과 열의 개수 출력\n## [1] 150   5\nnrow(iris) # 행의 개수 출력\n## [1] 150\nncol(iris) # 열의 개수 출력\n## [1] 5\ncolnames(iris) # 열 이름 출력, names()와 결과 동일\n## [1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"\nhead(iris) # 데이터셋의 앞부분 일부 출력\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\ntail(iris) # 데이터셋의 뒷부분 일부 출력\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n## 145          6.7         3.3          5.7         2.5 virginica\n## 146          6.7         3.0          5.2         2.3 virginica\n## 147          6.3         2.5          5.0         1.9 virginica\n## 148          6.5         3.0          5.2         2.0 virginica\n## 149          6.2         3.4          5.4         2.3 virginica\n## 150          5.9         3.0          5.1         1.8 virginica\n\nhead(iris, 10)\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1           5.1         3.5          1.4         0.2  setosa\n## 2           4.9         3.0          1.4         0.2  setosa\n## 3           4.7         3.2          1.3         0.2  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\n## 5           5.0         3.6          1.4         0.2  setosa\n## 6           5.4         3.9          1.7         0.4  setosa\n## 7           4.6         3.4          1.4         0.3  setosa\n## 8           5.0         3.4          1.5         0.2  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\ntail(iris, 20)\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n## 131          7.4         2.8          6.1         1.9 virginica\n## 132          7.9         3.8          6.4         2.0 virginica\n## 133          6.4         2.8          5.6         2.2 virginica\n## 134          6.3         2.8          5.1         1.5 virginica\n## 135          6.1         2.6          5.6         1.4 virginica\n## 136          7.7         3.0          6.1         2.3 virginica\n## 137          6.3         3.4          5.6         2.4 virginica\n## 138          6.4         3.1          5.5         1.8 virginica\n## 139          6.0         3.0          4.8         1.8 virginica\n## 140          6.9         3.1          5.4         2.1 virginica\n## 141          6.7         3.1          5.6         2.4 virginica\n## 142          6.9         3.1          5.1         2.3 virginica\n## 143          5.8         2.7          5.1         1.9 virginica\n## 144          6.8         3.2          5.9         2.3 virginica\n## 145          6.7         3.3          5.7         2.5 virginica\n## 146          6.7         3.0          5.2         2.3 virginica\n## 147          6.3         2.5          5.0         1.9 virginica\n## 148          6.5         3.0          5.2         2.0 virginica\n## 149          6.2         3.4          5.4         2.3 virginica\n## 150          5.9         3.0          5.1         1.8 virginica\n\n\nstr(iris) # 데이터셋 요약 정보 보기\n## 'data.frame':    150 obs. of  5 variables:\n##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n##  $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\niris[, 5] # 품종 데이터 보기\n##   [1] setosa     setosa     setosa     setosa     setosa     setosa    \n##   [7] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [13] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [19] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [25] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [31] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [37] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [43] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [49] setosa     setosa     versicolor versicolor versicolor versicolor\n##  [55] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [61] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [67] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [73] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [79] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [85] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [91] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [97] versicolor versicolor versicolor versicolor virginica  virginica \n## [103] virginica  virginica  virginica  virginica  virginica  virginica \n## [109] virginica  virginica  virginica  virginica  virginica  virginica \n## [115] virginica  virginica  virginica  virginica  virginica  virginica \n## [121] virginica  virginica  virginica  virginica  virginica  virginica \n## [127] virginica  virginica  virginica  virginica  virginica  virginica \n## [133] virginica  virginica  virginica  virginica  virginica  virginica \n## [139] virginica  virginica  virginica  virginica  virginica  virginica \n## [145] virginica  virginica  virginica  virginica  virginica  virginica \n## Levels: setosa versicolor virginica\nunique(iris[, 5]) # 품종의 종류 보기(중복 제거)\n## [1] setosa     versicolor virginica \n## Levels: setosa versicolor virginica\ntable(iris[, \"Species\"]) # 품종의 종류별 행의 개수 세기\n## \n##     setosa versicolor  virginica \n##         50         50         50\n\n\niris[, -5]\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width\n## 1            5.1         3.5          1.4         0.2\n## 2            4.9         3.0          1.4         0.2\n## 3            4.7         3.2          1.3         0.2\n## 4            4.6         3.1          1.5         0.2\n## 5            5.0         3.6          1.4         0.2\n## 6            5.4         3.9          1.7         0.4\n## 7            4.6         3.4          1.4         0.3\n## 8            5.0         3.4          1.5         0.2\n## 9            4.4         2.9          1.4         0.2\n## 10           4.9         3.1          1.5         0.1\n## 11           5.4         3.7          1.5         0.2\n## 12           4.8         3.4          1.6         0.2\n## 13           4.8         3.0          1.4         0.1\n## 14           4.3         3.0          1.1         0.1\n## 15           5.8         4.0          1.2         0.2\n## 16           5.7         4.4          1.5         0.4\n## 17           5.4         3.9          1.3         0.4\n## 18           5.1         3.5          1.4         0.3\n## 19           5.7         3.8          1.7         0.3\n## 20           5.1         3.8          1.5         0.3\n## 21           5.4         3.4          1.7         0.2\n## 22           5.1         3.7          1.5         0.4\n## 23           4.6         3.6          1.0         0.2\n## 24           5.1         3.3          1.7         0.5\n## 25           4.8         3.4          1.9         0.2\n## 26           5.0         3.0          1.6         0.2\n## 27           5.0         3.4          1.6         0.4\n## 28           5.2         3.5          1.5         0.2\n## 29           5.2         3.4          1.4         0.2\n## 30           4.7         3.2          1.6         0.2\n## 31           4.8         3.1          1.6         0.2\n## 32           5.4         3.4          1.5         0.4\n## 33           5.2         4.1          1.5         0.1\n## 34           5.5         4.2          1.4         0.2\n## 35           4.9         3.1          1.5         0.2\n## 36           5.0         3.2          1.2         0.2\n## 37           5.5         3.5          1.3         0.2\n## 38           4.9         3.6          1.4         0.1\n## 39           4.4         3.0          1.3         0.2\n## 40           5.1         3.4          1.5         0.2\n## 41           5.0         3.5          1.3         0.3\n## 42           4.5         2.3          1.3         0.3\n## 43           4.4         3.2          1.3         0.2\n## 44           5.0         3.5          1.6         0.6\n## 45           5.1         3.8          1.9         0.4\n## 46           4.8         3.0          1.4         0.3\n## 47           5.1         3.8          1.6         0.2\n## 48           4.6         3.2          1.4         0.2\n## 49           5.3         3.7          1.5         0.2\n## 50           5.0         3.3          1.4         0.2\n## 51           7.0         3.2          4.7         1.4\n## 52           6.4         3.2          4.5         1.5\n## 53           6.9         3.1          4.9         1.5\n## 54           5.5         2.3          4.0         1.3\n## 55           6.5         2.8          4.6         1.5\n## 56           5.7         2.8          4.5         1.3\n## 57           6.3         3.3          4.7         1.6\n## 58           4.9         2.4          3.3         1.0\n## 59           6.6         2.9          4.6         1.3\n## 60           5.2         2.7          3.9         1.4\n## 61           5.0         2.0          3.5         1.0\n## 62           5.9         3.0          4.2         1.5\n## 63           6.0         2.2          4.0         1.0\n## 64           6.1         2.9          4.7         1.4\n## 65           5.6         2.9          3.6         1.3\n## 66           6.7         3.1          4.4         1.4\n## 67           5.6         3.0          4.5         1.5\n## 68           5.8         2.7          4.1         1.0\n## 69           6.2         2.2          4.5         1.5\n## 70           5.6         2.5          3.9         1.1\n## 71           5.9         3.2          4.8         1.8\n## 72           6.1         2.8          4.0         1.3\n## 73           6.3         2.5          4.9         1.5\n## 74           6.1         2.8          4.7         1.2\n## 75           6.4         2.9          4.3         1.3\n## 76           6.6         3.0          4.4         1.4\n## 77           6.8         2.8          4.8         1.4\n## 78           6.7         3.0          5.0         1.7\n## 79           6.0         2.9          4.5         1.5\n## 80           5.7         2.6          3.5         1.0\n## 81           5.5         2.4          3.8         1.1\n## 82           5.5         2.4          3.7         1.0\n## 83           5.8         2.7          3.9         1.2\n## 84           6.0         2.7          5.1         1.6\n## 85           5.4         3.0          4.5         1.5\n## 86           6.0         3.4          4.5         1.6\n## 87           6.7         3.1          4.7         1.5\n## 88           6.3         2.3          4.4         1.3\n## 89           5.6         3.0          4.1         1.3\n## 90           5.5         2.5          4.0         1.3\n## 91           5.5         2.6          4.4         1.2\n## 92           6.1         3.0          4.6         1.4\n## 93           5.8         2.6          4.0         1.2\n## 94           5.0         2.3          3.3         1.0\n## 95           5.6         2.7          4.2         1.3\n## 96           5.7         3.0          4.2         1.2\n## 97           5.7         2.9          4.2         1.3\n## 98           6.2         2.9          4.3         1.3\n## 99           5.1         2.5          3.0         1.1\n## 100          5.7         2.8          4.1         1.3\n## 101          6.3         3.3          6.0         2.5\n## 102          5.8         2.7          5.1         1.9\n## 103          7.1         3.0          5.9         2.1\n## 104          6.3         2.9          5.6         1.8\n## 105          6.5         3.0          5.8         2.2\n## 106          7.6         3.0          6.6         2.1\n## 107          4.9         2.5          4.5         1.7\n## 108          7.3         2.9          6.3         1.8\n## 109          6.7         2.5          5.8         1.8\n## 110          7.2         3.6          6.1         2.5\n## 111          6.5         3.2          5.1         2.0\n## 112          6.4         2.7          5.3         1.9\n## 113          6.8         3.0          5.5         2.1\n## 114          5.7         2.5          5.0         2.0\n## 115          5.8         2.8          5.1         2.4\n## 116          6.4         3.2          5.3         2.3\n## 117          6.5         3.0          5.5         1.8\n## 118          7.7         3.8          6.7         2.2\n## 119          7.7         2.6          6.9         2.3\n## 120          6.0         2.2          5.0         1.5\n## 121          6.9         3.2          5.7         2.3\n## 122          5.6         2.8          4.9         2.0\n## 123          7.7         2.8          6.7         2.0\n## 124          6.3         2.7          4.9         1.8\n## 125          6.7         3.3          5.7         2.1\n## 126          7.2         3.2          6.0         1.8\n## 127          6.2         2.8          4.8         1.8\n## 128          6.1         3.0          4.9         1.8\n## 129          6.4         2.8          5.6         2.1\n## 130          7.2         3.0          5.8         1.6\n## 131          7.4         2.8          6.1         1.9\n## 132          7.9         3.8          6.4         2.0\n## 133          6.4         2.8          5.6         2.2\n## 134          6.3         2.8          5.1         1.5\n## 135          6.1         2.6          5.6         1.4\n## 136          7.7         3.0          6.1         2.3\n## 137          6.3         3.4          5.6         2.4\n## 138          6.4         3.1          5.5         1.8\n## 139          6.0         3.0          4.8         1.8\n## 140          6.9         3.1          5.4         2.1\n## 141          6.7         3.1          5.6         2.4\n## 142          6.9         3.1          5.1         2.3\n## 143          5.8         2.7          5.1         1.9\n## 144          6.8         3.2          5.9         2.3\n## 145          6.7         3.3          5.7         2.5\n## 146          6.7         3.0          5.2         2.3\n## 147          6.3         2.5          5.0         1.9\n## 148          6.5         3.0          5.2         2.0\n## 149          6.2         3.4          5.4         2.3\n## 150          5.9         3.0          5.1         1.8\ncolSums(iris[, -5]) # 열별 합계\n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n##        876.5        458.6        563.7        179.9\ncolMeans(iris[, -5]) # 열별 평균\n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n##     5.843333     3.057333     3.758000     1.199333\nrowSums(iris[, -5]) # 행별 합계\n##   [1] 10.2  9.5  9.4  9.4 10.2 11.4  9.7 10.1  8.9  9.6 10.8 10.0  9.3  8.5 11.2\n##  [16] 12.0 11.0 10.3 11.5 10.7 10.7 10.7  9.4 10.6 10.3  9.8 10.4 10.4 10.2  9.7\n##  [31]  9.7 10.7 10.9 11.3  9.7  9.6 10.5 10.0  8.9 10.2 10.1  8.4  9.1 10.7 11.2\n##  [46]  9.5 10.7  9.4 10.7  9.9 16.3 15.6 16.4 13.1 15.4 14.3 15.9 11.6 15.4 13.2\n##  [61] 11.5 14.6 13.2 15.1 13.4 15.6 14.6 13.6 14.4 13.1 15.7 14.2 15.2 14.8 14.9\n##  [76] 15.4 15.8 16.4 14.9 12.8 12.8 12.6 13.6 15.4 14.4 15.5 16.0 14.3 14.0 13.3\n##  [91] 13.7 15.1 13.6 11.6 13.8 14.1 14.1 14.7 11.7 13.9 18.1 15.5 18.1 16.6 17.5\n## [106] 19.3 13.6 18.3 16.8 19.4 16.8 16.3 17.4 15.2 16.1 17.2 16.8 20.4 19.5 14.7\n## [121] 18.1 15.3 19.2 15.7 17.8 18.2 15.6 15.8 16.9 17.6 18.2 20.1 17.0 15.7 15.7\n## [136] 19.1 17.7 16.8 15.6 17.5 17.8 17.4 15.5 18.2 18.2 17.2 15.7 16.7 17.3 15.8\nrowMeans(iris[, -5]) # 행별 평균\n##   [1] 2.550 2.375 2.350 2.350 2.550 2.850 2.425 2.525 2.225 2.400 2.700 2.500\n##  [13] 2.325 2.125 2.800 3.000 2.750 2.575 2.875 2.675 2.675 2.675 2.350 2.650\n##  [25] 2.575 2.450 2.600 2.600 2.550 2.425 2.425 2.675 2.725 2.825 2.425 2.400\n##  [37] 2.625 2.500 2.225 2.550 2.525 2.100 2.275 2.675 2.800 2.375 2.675 2.350\n##  [49] 2.675 2.475 4.075 3.900 4.100 3.275 3.850 3.575 3.975 2.900 3.850 3.300\n##  [61] 2.875 3.650 3.300 3.775 3.350 3.900 3.650 3.400 3.600 3.275 3.925 3.550\n##  [73] 3.800 3.700 3.725 3.850 3.950 4.100 3.725 3.200 3.200 3.150 3.400 3.850\n##  [85] 3.600 3.875 4.000 3.575 3.500 3.325 3.425 3.775 3.400 2.900 3.450 3.525\n##  [97] 3.525 3.675 2.925 3.475 4.525 3.875 4.525 4.150 4.375 4.825 3.400 4.575\n## [109] 4.200 4.850 4.200 4.075 4.350 3.800 4.025 4.300 4.200 5.100 4.875 3.675\n## [121] 4.525 3.825 4.800 3.925 4.450 4.550 3.900 3.950 4.225 4.400 4.550 5.025\n## [133] 4.250 3.925 3.925 4.775 4.425 4.200 3.900 4.375 4.450 4.350 3.875 4.550\n## [145] 4.550 4.300 3.925 4.175 4.325 3.950\n\n\nz &lt;- matrix(1:20, nrow = 4, ncol = 5)\nz\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\nt(z) # 행과열 방향 전환\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    2    3    4\n## [2,]    5    6    7    8\n## [3,]    9   10   11   12\n## [4,]   13   14   15   16\n## [5,]   17   18   19   20\n\n\nIR.1 &lt;- subset(iris, Species == \"setosa\")\nIR.1\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1           5.1         3.5          1.4         0.2  setosa\n## 2           4.9         3.0          1.4         0.2  setosa\n## 3           4.7         3.2          1.3         0.2  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\n## 5           5.0         3.6          1.4         0.2  setosa\n## 6           5.4         3.9          1.7         0.4  setosa\n## 7           4.6         3.4          1.4         0.3  setosa\n## 8           5.0         3.4          1.5         0.2  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\n## 11          5.4         3.7          1.5         0.2  setosa\n## 12          4.8         3.4          1.6         0.2  setosa\n## 13          4.8         3.0          1.4         0.1  setosa\n## 14          4.3         3.0          1.1         0.1  setosa\n## 15          5.8         4.0          1.2         0.2  setosa\n## 16          5.7         4.4          1.5         0.4  setosa\n## 17          5.4         3.9          1.3         0.4  setosa\n## 18          5.1         3.5          1.4         0.3  setosa\n## 19          5.7         3.8          1.7         0.3  setosa\n## 20          5.1         3.8          1.5         0.3  setosa\n## 21          5.4         3.4          1.7         0.2  setosa\n## 22          5.1         3.7          1.5         0.4  setosa\n## 23          4.6         3.6          1.0         0.2  setosa\n## 24          5.1         3.3          1.7         0.5  setosa\n## 25          4.8         3.4          1.9         0.2  setosa\n## 26          5.0         3.0          1.6         0.2  setosa\n## 27          5.0         3.4          1.6         0.4  setosa\n## 28          5.2         3.5          1.5         0.2  setosa\n## 29          5.2         3.4          1.4         0.2  setosa\n## 30          4.7         3.2          1.6         0.2  setosa\n## 31          4.8         3.1          1.6         0.2  setosa\n## 32          5.4         3.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\n## 35          4.9         3.1          1.5         0.2  setosa\n## 36          5.0         3.2          1.2         0.2  setosa\n## 37          5.5         3.5          1.3         0.2  setosa\n## 38          4.9         3.6          1.4         0.1  setosa\n## 39          4.4         3.0          1.3         0.2  setosa\n## 40          5.1         3.4          1.5         0.2  setosa\n## 41          5.0         3.5          1.3         0.3  setosa\n## 42          4.5         2.3          1.3         0.3  setosa\n## 43          4.4         3.2          1.3         0.2  setosa\n## 44          5.0         3.5          1.6         0.6  setosa\n## 45          5.1         3.8          1.9         0.4  setosa\n## 46          4.8         3.0          1.4         0.3  setosa\n## 47          5.1         3.8          1.6         0.2  setosa\n## 48          4.6         3.2          1.4         0.2  setosa\n## 49          5.3         3.7          1.5         0.2  setosa\n## 50          5.0         3.3          1.4         0.2  setosa\n\nIR.2 &lt;- subset(iris, Sepal.Length &gt; 5.0 & Sepal.Width &gt; 4.0)\nIR.2\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 16          5.7         4.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\nIR.2[, c(2, 4)] # 2, 4열의 값만 추출\n##    Sepal.Width Petal.Width\n## 16         4.4         0.4\n## 33         4.1         0.1\n## 34         4.2         0.2\n\nIR.3 &lt;- subset(iris, Sepal.Length &gt; 5.0 | Sepal.Width &gt; 4.0)\nIR.3\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 1            5.1         3.5          1.4         0.2     setosa\n## 6            5.4         3.9          1.7         0.4     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 15           5.8         4.0          1.2         0.2     setosa\n## 16           5.7         4.4          1.5         0.4     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 34           5.5         4.2          1.4         0.2     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 49           5.3         3.7          1.5         0.2     setosa\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 114          5.7         2.5          5.0         2.0  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 120          6.0         2.2          5.0         1.5  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 122          5.6         2.8          4.9         2.0  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 127          6.2         2.8          4.8         1.8  virginica\n## 128          6.1         3.0          4.9         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\n\n\na &lt;- matrix(1:20, 4, 5)\nb &lt;- matrix(21:40, 4, 5)\na ; b\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   21   25   29   33   37\n## [2,]   22   26   30   34   38\n## [3,]   23   27   31   35   39\n## [4,]   24   28   32   36   40\n\n2 * a # 매트릭스 a에 저장된 값들에 2를 곱하기\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    2   10   18   26   34\n## [2,]    4   12   20   28   36\n## [3,]    6   14   22   30   38\n## [4,]    8   16   24   32   40\nb - 5\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   16   20   24   28   32\n## [2,]   17   21   25   29   33\n## [3,]   18   22   26   30   34\n## [4,]   19   23   27   31   35\n2 * a + 3 * b\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   65   85  105  125  145\n## [2,]   70   90  110  130  150\n## [3,]   75   95  115  135  155\n## [4,]   80  100  120  140  160\n\na + b\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   22   30   38   46   54\n## [2,]   24   32   40   48   56\n## [3,]   26   34   42   50   58\n## [4,]   28   36   44   52   60\nb - a\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   20   20   20   20   20\n## [2,]   20   20   20   20   20\n## [3,]   20   20   20   20   20\n## [4,]   20   20   20   20   20\nb / a\n##           [,1]     [,2]     [,3]     [,4]     [,5]\n## [1,] 21.000000 5.000000 3.222222 2.538462 2.176471\n## [2,] 11.000000 4.333333 3.000000 2.428571 2.111111\n## [3,]  7.666667 3.857143 2.818182 2.333333 2.052632\n## [4,]  6.000000 3.500000 2.666667 2.250000 2.000000\na * b\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   21  125  261  429  629\n## [2,]   44  156  300  476  684\n## [3,]   69  189  341  525  741\n## [4,]   96  224  384  576  800\n\na &lt;- a * 3\na\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    3   15   27   39   51\n## [2,]    6   18   30   42   54\n## [3,]    9   21   33   45   57\n## [4,]   12   24   36   48   60\nb &lt;- b - 5\nb\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   16   20   24   28   32\n## [2,]   17   21   25   29   33\n## [3,]   18   22   26   30   34\n## [4,]   19   23   27   31   35\n\n\nclass(iris) # iris 데이터셋의 자료구조 확인\n## [1] \"data.frame\"\nclass(state.x77) # state.x77 데이터셋의 자료구조 확인\n## [1] \"matrix\" \"array\"\nis.matrix(iris) # 데이터셋이 매트릭스인지를 확인하는 함수\n## [1] FALSE\nis.data.frame(iris) # 데이터셋이 데이터프레임인지를 확인하는 함수\n## [1] TRUE\nis.matrix(state.x77)\n## [1] TRUE\nis.data.frame(state.x77)\n## [1] FALSE\n\n\n# 매트릭스를 데이터프레임으로 변환\nst &lt;- data.frame(state.x77)\nhead(st)\n##            Population Income Illiteracy Life.Exp Murder HS.Grad Frost   Area\n## Alabama          3615   3624        2.1    69.05   15.1    41.3    20  50708\n## Alaska            365   6315        1.5    69.31   11.3    66.7   152 566432\n## Arizona          2212   4530        1.8    70.55    7.8    58.1    15 113417\n## Arkansas         2110   3378        1.9    70.66   10.1    39.9    65  51945\n## California      21198   5114        1.1    71.71   10.3    62.6    20 156361\n## Colorado         2541   4884        0.7    72.06    6.8    63.9   166 103766\nclass(st)\n## [1] \"data.frame\"\n\n\niris[, \"Species\"] # 결과=벡터. 매트릭스와 데이터프레임 모두 가능\n##   [1] setosa     setosa     setosa     setosa     setosa     setosa    \n##   [7] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [13] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [19] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [25] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [31] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [37] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [43] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [49] setosa     setosa     versicolor versicolor versicolor versicolor\n##  [55] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [61] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [67] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [73] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [79] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [85] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [91] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [97] versicolor versicolor versicolor versicolor virginica  virginica \n## [103] virginica  virginica  virginica  virginica  virginica  virginica \n## [109] virginica  virginica  virginica  virginica  virginica  virginica \n## [115] virginica  virginica  virginica  virginica  virginica  virginica \n## [121] virginica  virginica  virginica  virginica  virginica  virginica \n## [127] virginica  virginica  virginica  virginica  virginica  virginica \n## [133] virginica  virginica  virginica  virginica  virginica  virginica \n## [139] virginica  virginica  virginica  virginica  virginica  virginica \n## [145] virginica  virginica  virginica  virginica  virginica  virginica \n## Levels: setosa versicolor virginica\niris[, 5] # 결과=벡터. 매트릭스와 데이터프레임 모두 가능\n##   [1] setosa     setosa     setosa     setosa     setosa     setosa    \n##   [7] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [13] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [19] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [25] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [31] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [37] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [43] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [49] setosa     setosa     versicolor versicolor versicolor versicolor\n##  [55] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [61] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [67] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [73] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [79] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [85] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [91] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [97] versicolor versicolor versicolor versicolor virginica  virginica \n## [103] virginica  virginica  virginica  virginica  virginica  virginica \n## [109] virginica  virginica  virginica  virginica  virginica  virginica \n## [115] virginica  virginica  virginica  virginica  virginica  virginica \n## [121] virginica  virginica  virginica  virginica  virginica  virginica \n## [127] virginica  virginica  virginica  virginica  virginica  virginica \n## [133] virginica  virginica  virginica  virginica  virginica  virginica \n## [139] virginica  virginica  virginica  virginica  virginica  virginica \n## [145] virginica  virginica  virginica  virginica  virginica  virginica \n## Levels: setosa versicolor virginica\niris[\"Species\"] # 결과=데이터프레임. 데이터프레임만 가능\n##        Species\n## 1       setosa\n## 2       setosa\n## 3       setosa\n## 4       setosa\n## 5       setosa\n## 6       setosa\n## 7       setosa\n## 8       setosa\n## 9       setosa\n## 10      setosa\n## 11      setosa\n## 12      setosa\n## 13      setosa\n## 14      setosa\n## 15      setosa\n## 16      setosa\n## 17      setosa\n## 18      setosa\n## 19      setosa\n## 20      setosa\n## 21      setosa\n## 22      setosa\n## 23      setosa\n## 24      setosa\n## 25      setosa\n## 26      setosa\n## 27      setosa\n## 28      setosa\n## 29      setosa\n## 30      setosa\n## 31      setosa\n## 32      setosa\n## 33      setosa\n## 34      setosa\n## 35      setosa\n## 36      setosa\n## 37      setosa\n## 38      setosa\n## 39      setosa\n## 40      setosa\n## 41      setosa\n## 42      setosa\n## 43      setosa\n## 44      setosa\n## 45      setosa\n## 46      setosa\n## 47      setosa\n## 48      setosa\n## 49      setosa\n## 50      setosa\n## 51  versicolor\n## 52  versicolor\n## 53  versicolor\n## 54  versicolor\n## 55  versicolor\n## 56  versicolor\n## 57  versicolor\n## 58  versicolor\n## 59  versicolor\n## 60  versicolor\n## 61  versicolor\n## 62  versicolor\n## 63  versicolor\n## 64  versicolor\n## 65  versicolor\n## 66  versicolor\n## 67  versicolor\n## 68  versicolor\n## 69  versicolor\n## 70  versicolor\n## 71  versicolor\n## 72  versicolor\n## 73  versicolor\n## 74  versicolor\n## 75  versicolor\n## 76  versicolor\n## 77  versicolor\n## 78  versicolor\n## 79  versicolor\n## 80  versicolor\n## 81  versicolor\n## 82  versicolor\n## 83  versicolor\n## 84  versicolor\n## 85  versicolor\n## 86  versicolor\n## 87  versicolor\n## 88  versicolor\n## 89  versicolor\n## 90  versicolor\n## 91  versicolor\n## 92  versicolor\n## 93  versicolor\n## 94  versicolor\n## 95  versicolor\n## 96  versicolor\n## 97  versicolor\n## 98  versicolor\n## 99  versicolor\n## 100 versicolor\n## 101  virginica\n## 102  virginica\n## 103  virginica\n## 104  virginica\n## 105  virginica\n## 106  virginica\n## 107  virginica\n## 108  virginica\n## 109  virginica\n## 110  virginica\n## 111  virginica\n## 112  virginica\n## 113  virginica\n## 114  virginica\n## 115  virginica\n## 116  virginica\n## 117  virginica\n## 118  virginica\n## 119  virginica\n## 120  virginica\n## 121  virginica\n## 122  virginica\n## 123  virginica\n## 124  virginica\n## 125  virginica\n## 126  virginica\n## 127  virginica\n## 128  virginica\n## 129  virginica\n## 130  virginica\n## 131  virginica\n## 132  virginica\n## 133  virginica\n## 134  virginica\n## 135  virginica\n## 136  virginica\n## 137  virginica\n## 138  virginica\n## 139  virginica\n## 140  virginica\n## 141  virginica\n## 142  virginica\n## 143  virginica\n## 144  virginica\n## 145  virginica\n## 146  virginica\n## 147  virginica\n## 148  virginica\n## 149  virginica\n## 150  virginica\niris[5] # 결과=데이터프레임. 데이터프레임만 가능\n##        Species\n## 1       setosa\n## 2       setosa\n## 3       setosa\n## 4       setosa\n## 5       setosa\n## 6       setosa\n## 7       setosa\n## 8       setosa\n## 9       setosa\n## 10      setosa\n## 11      setosa\n## 12      setosa\n## 13      setosa\n## 14      setosa\n## 15      setosa\n## 16      setosa\n## 17      setosa\n## 18      setosa\n## 19      setosa\n## 20      setosa\n## 21      setosa\n## 22      setosa\n## 23      setosa\n## 24      setosa\n## 25      setosa\n## 26      setosa\n## 27      setosa\n## 28      setosa\n## 29      setosa\n## 30      setosa\n## 31      setosa\n## 32      setosa\n## 33      setosa\n## 34      setosa\n## 35      setosa\n## 36      setosa\n## 37      setosa\n## 38      setosa\n## 39      setosa\n## 40      setosa\n## 41      setosa\n## 42      setosa\n## 43      setosa\n## 44      setosa\n## 45      setosa\n## 46      setosa\n## 47      setosa\n## 48      setosa\n## 49      setosa\n## 50      setosa\n## 51  versicolor\n## 52  versicolor\n## 53  versicolor\n## 54  versicolor\n## 55  versicolor\n## 56  versicolor\n## 57  versicolor\n## 58  versicolor\n## 59  versicolor\n## 60  versicolor\n## 61  versicolor\n## 62  versicolor\n## 63  versicolor\n## 64  versicolor\n## 65  versicolor\n## 66  versicolor\n## 67  versicolor\n## 68  versicolor\n## 69  versicolor\n## 70  versicolor\n## 71  versicolor\n## 72  versicolor\n## 73  versicolor\n## 74  versicolor\n## 75  versicolor\n## 76  versicolor\n## 77  versicolor\n## 78  versicolor\n## 79  versicolor\n## 80  versicolor\n## 81  versicolor\n## 82  versicolor\n## 83  versicolor\n## 84  versicolor\n## 85  versicolor\n## 86  versicolor\n## 87  versicolor\n## 88  versicolor\n## 89  versicolor\n## 90  versicolor\n## 91  versicolor\n## 92  versicolor\n## 93  versicolor\n## 94  versicolor\n## 95  versicolor\n## 96  versicolor\n## 97  versicolor\n## 98  versicolor\n## 99  versicolor\n## 100 versicolor\n## 101  virginica\n## 102  virginica\n## 103  virginica\n## 104  virginica\n## 105  virginica\n## 106  virginica\n## 107  virginica\n## 108  virginica\n## 109  virginica\n## 110  virginica\n## 111  virginica\n## 112  virginica\n## 113  virginica\n## 114  virginica\n## 115  virginica\n## 116  virginica\n## 117  virginica\n## 118  virginica\n## 119  virginica\n## 120  virginica\n## 121  virginica\n## 122  virginica\n## 123  virginica\n## 124  virginica\n## 125  virginica\n## 126  virginica\n## 127  virginica\n## 128  virginica\n## 129  virginica\n## 130  virginica\n## 131  virginica\n## 132  virginica\n## 133  virginica\n## 134  virginica\n## 135  virginica\n## 136  virginica\n## 137  virginica\n## 138  virginica\n## 139  virginica\n## 140  virginica\n## 141  virginica\n## 142  virginica\n## 143  virginica\n## 144  virginica\n## 145  virginica\n## 146  virginica\n## 147  virginica\n## 148  virginica\n## 149  virginica\n## 150  virginica\niris$Species # 결과=벡터. 데이터프레임만 가능\n##   [1] setosa     setosa     setosa     setosa     setosa     setosa    \n##   [7] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [13] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [19] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [25] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [31] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [37] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [43] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [49] setosa     setosa     versicolor versicolor versicolor versicolor\n##  [55] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [61] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [67] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [73] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [79] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [85] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [91] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [97] versicolor versicolor versicolor versicolor virginica  virginica \n## [103] virginica  virginica  virginica  virginica  virginica  virginica \n## [109] virginica  virginica  virginica  virginica  virginica  virginica \n## [115] virginica  virginica  virginica  virginica  virginica  virginica \n## [121] virginica  virginica  virginica  virginica  virginica  virginica \n## [127] virginica  virginica  virginica  virginica  virginica  virginica \n## [133] virginica  virginica  virginica  virginica  virginica  virginica \n## [139] virginica  virginica  virginica  virginica  virginica  virginica \n## [145] virginica  virginica  virginica  virginica  virginica  virginica \n## Levels: setosa versicolor virginica\n\n\ngetwd()\n## [1] \"C:/Users/Hyunsoo Kim/Desktop/senior_grade/blog/my-quarto-website\"\n# setwd(\"G:/내 드라이브/202202/R_Basic/data\") # 작업 폴더 지정\nair &lt;- read.csv(\"./R_Basic/data/airquality.csv\", header = T) # .csv 파일 읽기\nhead(air)\n##                                    version.https...git.lfs.github.com.spec.v1\n## 1 oid sha256:6fdc84af524856a54abe063336bfea6511e9fb5dfcd2ec6e1dfa9e1e4d8c7357\n## 2                                                                   size 3044\n\n\nmy.iris &lt;- subset(iris, Species = 'Setosa') # Setosa 품종 데이터만 추출\n## Warning: In subset.data.frame(iris, Species = \"Setosa\") :\n##  extra argument 'Species' will be disregarded\nwrite.csv(my.iris, \"./R_Basic/data/my_iris_1.csv\") # .csv 파일에 저장하기\n\n\n\n\n\njob.type &lt;- 'A'\nif (job.type == 'B') {\n    bonus &lt;- 200 # 직무 유형이 B일 때 실행\n} else {\n    bonus &lt;- 100 # 직무 유형이 B가 아닌 나머지 경우 실행\n}\nprint(bonus)\n## [1] 100\n\n\njob.type &lt;- 'B'\nbonus &lt;- 100\nif (job.type == 'A') {\n    bonus &lt;- 200 # 직무 유형이 A일 때 실행\n}\nprint(bonus)\n## [1] 100\n\n\nscore &lt;- 85\n\nif (score &gt; 90) {\n    grade &lt;- 'A'\n} else if (score &gt; 80) {\n    grade &lt;- 'B'\n} else if (score &gt; 70) {\n    grade &lt;- 'C'\n} else if (score &gt; 60) {\n    grade &lt;- 'D'\n} else {\n    grade &lt;- 'F'\n}\n\nprint(grade)\n## [1] \"B\"\n\n\na &lt;- 10\nb &lt;- 20\nif (a &gt; 5 & b &gt; 5) {    # and 사용\n    print(a + b)\n}\n## [1] 30\n\nif (a &gt; 5 | b &gt; 30) {   # or 사용\n    print(a * b)\n}\n## [1] 200\n\nif (a &gt; 5 & b &gt; 30) {\n    print(a * b)\n}\n\nif (a &gt; 20 | b &gt; 30) {\n    print(a * b)\n}\n\nif (a &gt; 20 & b &gt; 15) {\n    print(a * b)\n}\n\nr_basic &lt;- 70\npython_basic &lt;- 82\n\nif (r_basic &gt; 80 & python_basic &gt; 80) {\n    grade &lt;- \"Excellent\"\n} else {\n    grade &lt;- \"Good\"\n}\ngrade\n## [1] \"Good\"\n\n\na &lt;- 10\nb &lt;- 20\n\nif (a &gt; b) {\n    c &lt;- a\n} else {\n    c &lt;- b\n}\nprint(c)\n## [1] 20\n\na &lt;- 10\nb &lt;- 20\n\nc &lt;- ifelse(a &gt; b, a, b)\nprint(c)\n## [1] 20\n\n\nfor(i in 1:5) {\n    print('*')\n}\n## [1] \"*\"\n## [1] \"*\"\n## [1] \"*\"\n## [1] \"*\"\n## [1] \"*\"\n\nfor (i in 1:5) {\n    print(i)\n}\n## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\n\nfor (i in 1:5) {\n    a &lt;- i * 2\n    print(a)\n}\n## [1] 2\n## [1] 4\n## [1] 6\n## [1] 8\n## [1] 10\n\n# for (i in 1:10000) {\n#     a &lt;- i * 2\n#     print(a)\n# }\n\n# for (i in 1:10000) {\n#     a &lt;- i * 2 / 1521 + 10000\n#     print(a)\n# }\n\n\nfor (i in 6:10) {\n    print(i)\n}\n## [1] 6\n## [1] 7\n## [1] 8\n## [1] 9\n## [1] 10\n\n\nfor(i in 1:9) {\n    cat('2 *', i, '=', 2 * i, '\\n')\n}\n## 2 * 1 = 2 \n## 2 * 2 = 4 \n## 2 * 3 = 6 \n## 2 * 4 = 8 \n## 2 * 5 = 10 \n## 2 * 6 = 12 \n## 2 * 7 = 14 \n## 2 * 8 = 16 \n## 2 * 9 = 18\n\nfor (i in 1:9) {\n    cat('2 *', i, '=', 2 * i)\n}\n## 2 * 1 = 22 * 2 = 42 * 3 = 62 * 4 = 82 * 5 = 102 * 6 = 122 * 7 = 142 * 8 = 162 * 9 = 18\n\nfor (i in 1:9) {\n    j &lt;- i:10\n    print(j)\n}\n##  [1]  1  2  3  4  5  6  7  8  9 10\n## [1]  2  3  4  5  6  7  8  9 10\n## [1]  3  4  5  6  7  8  9 10\n## [1]  4  5  6  7  8  9 10\n## [1]  5  6  7  8  9 10\n## [1]  6  7  8  9 10\n## [1]  7  8  9 10\n## [1]  8  9 10\n## [1]  9 10\n\n\nfor(i in 1:20) {\n    if (i %% 2 == 0) {  # 짝수인지 확인\n        print(i)\n    }\n}\n## [1] 2\n## [1] 4\n## [1] 6\n## [1] 8\n## [1] 10\n## [1] 12\n## [1] 14\n## [1] 16\n## [1] 18\n## [1] 20\n\n\nsum &lt;- 0\nfor (i in 1:100) {\n    sum &lt;- sum + i  # sum에 i 값을 누적\n}\nprint(sum)\n## [1] 5050\n\nsum &lt;- 0\nfor (i in 1:100) {\n    sum &lt;- sum + i\n    print(c(sum, i))\n}\n## [1] 1 1\n## [1] 3 2\n## [1] 6 3\n## [1] 10  4\n## [1] 15  5\n## [1] 21  6\n## [1] 28  7\n## [1] 36  8\n## [1] 45  9\n## [1] 55 10\n## [1] 66 11\n## [1] 78 12\n## [1] 91 13\n## [1] 105  14\n## [1] 120  15\n## [1] 136  16\n## [1] 153  17\n## [1] 171  18\n## [1] 190  19\n## [1] 210  20\n## [1] 231  21\n## [1] 253  22\n## [1] 276  23\n## [1] 300  24\n## [1] 325  25\n## [1] 351  26\n## [1] 378  27\n## [1] 406  28\n## [1] 435  29\n## [1] 465  30\n## [1] 496  31\n## [1] 528  32\n## [1] 561  33\n## [1] 595  34\n## [1] 630  35\n## [1] 666  36\n## [1] 703  37\n## [1] 741  38\n## [1] 780  39\n## [1] 820  40\n## [1] 861  41\n## [1] 903  42\n## [1] 946  43\n## [1] 990  44\n## [1] 1035   45\n## [1] 1081   46\n## [1] 1128   47\n## [1] 1176   48\n## [1] 1225   49\n## [1] 1275   50\n## [1] 1326   51\n## [1] 1378   52\n## [1] 1431   53\n## [1] 1485   54\n## [1] 1540   55\n## [1] 1596   56\n## [1] 1653   57\n## [1] 1711   58\n## [1] 1770   59\n## [1] 1830   60\n## [1] 1891   61\n## [1] 1953   62\n## [1] 2016   63\n## [1] 2080   64\n## [1] 2145   65\n## [1] 2211   66\n## [1] 2278   67\n## [1] 2346   68\n## [1] 2415   69\n## [1] 2485   70\n## [1] 2556   71\n## [1] 2628   72\n## [1] 2701   73\n## [1] 2775   74\n## [1] 2850   75\n## [1] 2926   76\n## [1] 3003   77\n## [1] 3081   78\n## [1] 3160   79\n## [1] 3240   80\n## [1] 3321   81\n## [1] 3403   82\n## [1] 3486   83\n## [1] 3570   84\n## [1] 3655   85\n## [1] 3741   86\n## [1] 3828   87\n## [1] 3916   88\n## [1] 4005   89\n## [1] 4095   90\n## [1] 4186   91\n## [1] 4278   92\n## [1] 4371   93\n## [1] 4465   94\n## [1] 4560   95\n## [1] 4656   96\n## [1] 4753   97\n## [1] 4851   98\n## [1] 4950   99\n## [1] 5050  100\nprint(sum)\n## [1] 5050\n\nsum &lt;- 0\nfor (i in 1:100) {\n    print(c(sum, i))\n    sum &lt;- sum + i\n}\n## [1] 0 1\n## [1] 1 2\n## [1] 3 3\n## [1] 6 4\n## [1] 10  5\n## [1] 15  6\n## [1] 21  7\n## [1] 28  8\n## [1] 36  9\n## [1] 45 10\n## [1] 55 11\n## [1] 66 12\n## [1] 78 13\n## [1] 91 14\n## [1] 105  15\n## [1] 120  16\n## [1] 136  17\n## [1] 153  18\n## [1] 171  19\n## [1] 190  20\n## [1] 210  21\n## [1] 231  22\n## [1] 253  23\n## [1] 276  24\n## [1] 300  25\n## [1] 325  26\n## [1] 351  27\n## [1] 378  28\n## [1] 406  29\n## [1] 435  30\n## [1] 465  31\n## [1] 496  32\n## [1] 528  33\n## [1] 561  34\n## [1] 595  35\n## [1] 630  36\n## [1] 666  37\n## [1] 703  38\n## [1] 741  39\n## [1] 780  40\n## [1] 820  41\n## [1] 861  42\n## [1] 903  43\n## [1] 946  44\n## [1] 990  45\n## [1] 1035   46\n## [1] 1081   47\n## [1] 1128   48\n## [1] 1176   49\n## [1] 1225   50\n## [1] 1275   51\n## [1] 1326   52\n## [1] 1378   53\n## [1] 1431   54\n## [1] 1485   55\n## [1] 1540   56\n## [1] 1596   57\n## [1] 1653   58\n## [1] 1711   59\n## [1] 1770   60\n## [1] 1830   61\n## [1] 1891   62\n## [1] 1953   63\n## [1] 2016   64\n## [1] 2080   65\n## [1] 2145   66\n## [1] 2211   67\n## [1] 2278   68\n## [1] 2346   69\n## [1] 2415   70\n## [1] 2485   71\n## [1] 2556   72\n## [1] 2628   73\n## [1] 2701   74\n## [1] 2775   75\n## [1] 2850   76\n## [1] 2926   77\n## [1] 3003   78\n## [1] 3081   79\n## [1] 3160   80\n## [1] 3240   81\n## [1] 3321   82\n## [1] 3403   83\n## [1] 3486   84\n## [1] 3570   85\n## [1] 3655   86\n## [1] 3741   87\n## [1] 3828   88\n## [1] 3916   89\n## [1] 4005   90\n## [1] 4095   91\n## [1] 4186   92\n## [1] 4278   93\n## [1] 4371   94\n## [1] 4465   95\n## [1] 4560   96\n## [1] 4656   97\n## [1] 4753   98\n## [1] 4851   99\n## [1] 4950  100\nprint(sum)\n## [1] 5050\n\n\nnorow &lt;- nrow(iris)                             # iris의 행의 수\nmylabel &lt;- c()                                  # 비어 있는 벡터 선언\nfor (i in 1:norow) {\n    if (iris$Petal.Length[i] &lt;= 1.6) {          # 꽃잎의 길이에 따라 레이블 결정\n        mylabel[i] &lt;- 'L'\n    } else if (iris$Petal.Length[i] &gt;= 5.1) {\n        mylabel[i] &lt;- 'H'\n    } else {\n        mylabel[i] &lt;- 'M'\n    }\n    print(c(iris$Petal.Length[i], mylabel))\n}\n## [1] \"1.4\" \"L\"  \n## [1] \"1.4\" \"L\"   \"L\"  \n## [1] \"1.3\" \"L\"   \"L\"   \"L\"  \n## [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"  \n## [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [1] \"1.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"  \n## [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"  \n## [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"  \n##  [1] \"1.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"  \n##  [1] \"1.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"  \n##  [1] \"1.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"  \n##  [1] \"1\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\"\n##  [1] \"1.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"  \n##  [1] \"1.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"  \n##  [1] \"4.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"  \n##  [1] \"4.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\"\n##  [1] \"4.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"  \n##  [1] \"3.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"  \n##  [1] \"4.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"  \n##  [1] \"4\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [58] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [1] \"4.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [58] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [1] \"4.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"  \n##  [1] \"4.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"  \n##  [1] \"4.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"5\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [58] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [77] \"M\" \"M\" \"M\"\n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"  \n##  [1] \"4.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [58] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [77] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [1] \"4.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [58] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [77] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [1] \"3.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [97] \"M\"  \n##  [1] \"4.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [97] \"M\"   \"M\"  \n##  [1] \"4.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [97] \"M\"   \"M\"   \"M\"  \n##   [1] \"3\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##   [1] \"4.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##   [1] \"6\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\"\n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"  \n##   [1] \"5.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"6.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n##   [1] \"6.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"  \n##   [1] \"5.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"  \n##   [1] \"6.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"  \n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n## [109] \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"  \n##   [1] \"5.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"  \n##   [1] \"5.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"6.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"6.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n## [109] \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n##   [1] \"5.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"  \n##   [1] \"4.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"  \n##   [1] \"6.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"  \n##   [1] \"4.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"  \n##   [1] \"5.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"  \n##   [1] \"6\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n## [109] \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"M\" \"H\" \"M\" \"H\"\n## [127] \"H\"\n##   [1] \"4.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"  \n##   [1] \"4.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"  \n##   [1] \"5.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"  \n##   [1] \"6.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"6.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"  \n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"6.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"4.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n##   [1] \"5.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"  \n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"  \n##   [1] \"5.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"   \"H\"  \n##   [1] \"5.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"   \"H\"   \"H\"  \n##   [1] \"5\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n## [109] \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"M\" \"H\" \"M\" \"H\"\n## [127] \"H\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\"\n## [145] \"H\" \"H\" \"H\" \"M\"\n##   [1] \"5.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"   \"H\"   \"H\"   \"M\"   \"H\"  \n##   [1] \"5.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"  \n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"\nprint(mylabel)                                  # 레이블 출력\n##   [1] \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\"\n## [109] \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"M\" \"H\" \"M\" \"H\" \"H\"\n## [127] \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\"\n## [145] \"H\" \"H\" \"M\" \"H\" \"H\" \"H\"\nnewds &lt;- data.frame(iris$Petal.Length, mylabel) # 꽃잎의 길이와 레이블 결합\nhead(newds)                                     # 새로운 데이터셋 내용 출력\n##   iris.Petal.Length mylabel\n## 1               1.4       L\n## 2               1.4       L\n## 3               1.3       L\n## 4               1.5       L\n## 5               1.4       L\n## 6               1.7       M\n\n\nsum &lt;- 0\ni &lt;- 1\nwhile (i &lt;= 100) {\n    sum &lt;- sum + i      # sum에 i 값을 누적\n    i &lt;- i + 1          # i 값을 1 증가시킴\n    print(c(sum, i))\n}\n## [1] 1 2\n## [1] 3 3\n## [1] 6 4\n## [1] 10  5\n## [1] 15  6\n## [1] 21  7\n## [1] 28  8\n## [1] 36  9\n## [1] 45 10\n## [1] 55 11\n## [1] 66 12\n## [1] 78 13\n## [1] 91 14\n## [1] 105  15\n## [1] 120  16\n## [1] 136  17\n## [1] 153  18\n## [1] 171  19\n## [1] 190  20\n## [1] 210  21\n## [1] 231  22\n## [1] 253  23\n## [1] 276  24\n## [1] 300  25\n## [1] 325  26\n## [1] 351  27\n## [1] 378  28\n## [1] 406  29\n## [1] 435  30\n## [1] 465  31\n## [1] 496  32\n## [1] 528  33\n## [1] 561  34\n## [1] 595  35\n## [1] 630  36\n## [1] 666  37\n## [1] 703  38\n## [1] 741  39\n## [1] 780  40\n## [1] 820  41\n## [1] 861  42\n## [1] 903  43\n## [1] 946  44\n## [1] 990  45\n## [1] 1035   46\n## [1] 1081   47\n## [1] 1128   48\n## [1] 1176   49\n## [1] 1225   50\n## [1] 1275   51\n## [1] 1326   52\n## [1] 1378   53\n## [1] 1431   54\n## [1] 1485   55\n## [1] 1540   56\n## [1] 1596   57\n## [1] 1653   58\n## [1] 1711   59\n## [1] 1770   60\n## [1] 1830   61\n## [1] 1891   62\n## [1] 1953   63\n## [1] 2016   64\n## [1] 2080   65\n## [1] 2145   66\n## [1] 2211   67\n## [1] 2278   68\n## [1] 2346   69\n## [1] 2415   70\n## [1] 2485   71\n## [1] 2556   72\n## [1] 2628   73\n## [1] 2701   74\n## [1] 2775   75\n## [1] 2850   76\n## [1] 2926   77\n## [1] 3003   78\n## [1] 3081   79\n## [1] 3160   80\n## [1] 3240   81\n## [1] 3321   82\n## [1] 3403   83\n## [1] 3486   84\n## [1] 3570   85\n## [1] 3655   86\n## [1] 3741   87\n## [1] 3828   88\n## [1] 3916   89\n## [1] 4005   90\n## [1] 4095   91\n## [1] 4186   92\n## [1] 4278   93\n## [1] 4371   94\n## [1] 4465   95\n## [1] 4560   96\n## [1] 4656   97\n## [1] 4753   98\n## [1] 4851   99\n## [1] 4950  100\n## [1] 5050  101\nprint(sum)\n## [1] 5050\n\n#---------------------------------------#\n# 오류 없이 계속 실행됨\n# sum &lt;- 0\n# i &lt;- 1\n# while(i &gt;= 1) {\n#   sum &lt;- sum + i # sum에 i 값을 누적\n#   i &lt;- i + 1 # i 값을 1 증가시킴\n#   print(c(sum,i))\n# }\n# print(sum)\n#---------------------------------------#\n\n\nsum &lt;- 0\nfor (i in 1:10) {\n    sum &lt;- sum + i\n    print(c(sum, i))\n    if (i &gt;= 5)\n        break\n}\n## [1] 1 1\n## [1] 3 2\n## [1] 6 3\n## [1] 10  4\n## [1] 15  5\nsum\n## [1] 15\n\n\nsum &lt;- 0\nfor (i in 1:10) {\n    if (i %% 2 == 0)\n        next # %% = 나머지\n    sum &lt;- sum + i\n    print(c(sum, i))\n}\n## [1] 1 1\n## [1] 4 3\n## [1] 9 5\n## [1] 16  7\n## [1] 25  9\nsum\n## [1] 25\n\n\napply(iris[, 1:4], 1, mean) # row 방향으로 함수 적용\n##   [1] 2.550 2.375 2.350 2.350 2.550 2.850 2.425 2.525 2.225 2.400 2.700 2.500\n##  [13] 2.325 2.125 2.800 3.000 2.750 2.575 2.875 2.675 2.675 2.675 2.350 2.650\n##  [25] 2.575 2.450 2.600 2.600 2.550 2.425 2.425 2.675 2.725 2.825 2.425 2.400\n##  [37] 2.625 2.500 2.225 2.550 2.525 2.100 2.275 2.675 2.800 2.375 2.675 2.350\n##  [49] 2.675 2.475 4.075 3.900 4.100 3.275 3.850 3.575 3.975 2.900 3.850 3.300\n##  [61] 2.875 3.650 3.300 3.775 3.350 3.900 3.650 3.400 3.600 3.275 3.925 3.550\n##  [73] 3.800 3.700 3.725 3.850 3.950 4.100 3.725 3.200 3.200 3.150 3.400 3.850\n##  [85] 3.600 3.875 4.000 3.575 3.500 3.325 3.425 3.775 3.400 2.900 3.450 3.525\n##  [97] 3.525 3.675 2.925 3.475 4.525 3.875 4.525 4.150 4.375 4.825 3.400 4.575\n## [109] 4.200 4.850 4.200 4.075 4.350 3.800 4.025 4.300 4.200 5.100 4.875 3.675\n## [121] 4.525 3.825 4.800 3.925 4.450 4.550 3.900 3.950 4.225 4.400 4.550 5.025\n## [133] 4.250 3.925 3.925 4.775 4.425 4.200 3.900 4.375 4.450 4.350 3.875 4.550\n## [145] 4.550 4.300 3.925 4.175 4.325 3.950\napply(iris[, 1:4], 2, mean) # col 방향으로 함수 적용\n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n##     5.843333     3.057333     3.758000     1.199333\n\nresult &lt;- c()\nfor (i in 1:4) {\n    iris_col &lt;- iris[, i]\n    iris_col_mean_temp &lt;- mean(iris_col)\n    result &lt;- c(result, iris_col_mean_temp)\n}\nresult\n## [1] 5.843333 3.057333 3.758000 1.199333\n\n\nmymax &lt;- function(x, y) {\n    num.max &lt;- x\n    if (y &gt; x) {\n        num.max &lt;- y\n    }\n    return(num.max)\n}\n\n\nmymax(10, 15)\n## [1] 15\na &lt;- mymax(20, 15)\nb &lt;- mymax(31, 45)\nprint(a + b)\n## [1] 65\n\n\nmydiv &lt;- function(x, y = 2) {\n    result &lt;- x / y\n    return(result)\n}\n\nmydiv(x = 10, y = 3) # 매개변수 이름과 매개변수값을 쌍으로 입력\n## [1] 3.333333\nmydiv(10, 3) # 매개변수값만 입력\n## [1] 3.333333\nmydiv(10) # x에 대한 값만 입력(y 값이 생략됨)\n## [1] 5\n\n\nmyfunc &lt;- function(x, y) {\n    val.sum &lt;- x + y\n    val.mul &lt;- x * y\n    return(list(sum = val.sum, mul = val.mul))\n}\n\nresult &lt;- myfunc(5, 8)\nresult\n## $sum\n## [1] 13\n## \n## $mul\n## [1] 40\ns &lt;- result$sum # 5, 8의 합\nm &lt;- result$mul # 5, 8의 곱\ncat('5+8=', s, '\\n')\n## 5+8= 13\ncat('5*8=', m, '\\n')\n## 5*8= 40\n\n\ngetwd()\n## [1] \"C:/Users/Hyunsoo Kim/Desktop/senior_grade/blog/my-quarto-website\"\n# source(\"myfunc.R\") # myfunc.R 안에 있는 함수 실행\n\na &lt;- mydiv(20, 4) # 함수 호출\nb &lt;- mydiv(30, 4) # 함수 호출\na + b\n## [1] 12.5\nmydiv(mydiv(20, 2), 5) # 함수 호출\n## [1] 2\n\n\nscore &lt;- c(76, 84, 69, 50, 95, 60, 82, 71, 88, 84)\nwhich(score == 69) # 성적이 69인 학생은 몇 번째에 있나\n## [1] 3\nwhich(score &gt;= 85) # 성적이 85 이상인 학생은 몇 번째에 있나\n## [1] 5 9\n\nmax(score) # 최고 점수는 몇 점인가\n## [1] 95\nwhich.max(score) # 최고 점수는 몇 번째에 있나\n## [1] 5\nscore[which.max(score)] # 최고 점수는 몇 점인가\n## [1] 95\n\nmin(score) # 최저 점수는 몇 점인가\n## [1] 50\nwhich.min(score) # 최저 점수는 몇 번째에 있나\n## [1] 4\nscore[which.min(score)] # 최저 점수는 몇 점인가\n## [1] 50\n\n\nscore &lt;- c(76, 84, 69, 50, 95, 60, 82, 71, 88, 84)\nidx &lt;- which(score &lt;= 60) # 성적이 60 이하인 값들의 인덱스\nidx\n## [1] 4 6\nscore[idx]\n## [1] 50 60\nscore[idx] &lt;- 61 # 성적이 60 이하인 값들은 61점으로 성적 상향 조정\nscore # 상향 조정된 성적 확인\n##  [1] 76 84 69 61 95 61 82 71 88 84\n\nidx &lt;- which(score &gt;= 80) # 성적이 80 이상인 값들의 인덱스\nidx\n## [1]  2  5  7  9 10\nscore[idx]\n## [1] 84 95 82 88 84\nscore.high &lt;- score[idx] # 성적이 80 이상인 값들만 추출하여 저장\nscore.high # score.high의 내용 확인\n## [1] 84 95 82 88 84\n\n\niris\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 1            5.1         3.5          1.4         0.2     setosa\n## 2            4.9         3.0          1.4         0.2     setosa\n## 3            4.7         3.2          1.3         0.2     setosa\n## 4            4.6         3.1          1.5         0.2     setosa\n## 5            5.0         3.6          1.4         0.2     setosa\n## 6            5.4         3.9          1.7         0.4     setosa\n## 7            4.6         3.4          1.4         0.3     setosa\n## 8            5.0         3.4          1.5         0.2     setosa\n## 9            4.4         2.9          1.4         0.2     setosa\n## 10           4.9         3.1          1.5         0.1     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 12           4.8         3.4          1.6         0.2     setosa\n## 13           4.8         3.0          1.4         0.1     setosa\n## 14           4.3         3.0          1.1         0.1     setosa\n## 15           5.8         4.0          1.2         0.2     setosa\n## 16           5.7         4.4          1.5         0.4     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 23           4.6         3.6          1.0         0.2     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 25           4.8         3.4          1.9         0.2     setosa\n## 26           5.0         3.0          1.6         0.2     setosa\n## 27           5.0         3.4          1.6         0.4     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 30           4.7         3.2          1.6         0.2     setosa\n## 31           4.8         3.1          1.6         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 34           5.5         4.2          1.4         0.2     setosa\n## 35           4.9         3.1          1.5         0.2     setosa\n## 36           5.0         3.2          1.2         0.2     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 38           4.9         3.6          1.4         0.1     setosa\n## 39           4.4         3.0          1.3         0.2     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 41           5.0         3.5          1.3         0.3     setosa\n## 42           4.5         2.3          1.3         0.3     setosa\n## 43           4.4         3.2          1.3         0.2     setosa\n## 44           5.0         3.5          1.6         0.6     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\n## 46           4.8         3.0          1.4         0.3     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 48           4.6         3.2          1.4         0.2     setosa\n## 49           5.3         3.7          1.5         0.2     setosa\n## 50           5.0         3.3          1.4         0.2     setosa\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 107          4.9         2.5          4.5         1.7  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 114          5.7         2.5          5.0         2.0  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 120          6.0         2.2          5.0         1.5  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 122          5.6         2.8          4.9         2.0  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 127          6.2         2.8          4.8         1.8  virginica\n## 128          6.1         3.0          4.9         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\niris$Petal.Length\n##   [1] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 1.5 1.6 1.4 1.1 1.2 1.5 1.3 1.4\n##  [19] 1.7 1.5 1.7 1.5 1.0 1.7 1.9 1.6 1.6 1.5 1.4 1.6 1.6 1.5 1.5 1.4 1.5 1.2\n##  [37] 1.3 1.4 1.3 1.5 1.3 1.3 1.3 1.6 1.9 1.4 1.6 1.4 1.5 1.4 4.7 4.5 4.9 4.0\n##  [55] 4.6 4.5 4.7 3.3 4.6 3.9 3.5 4.2 4.0 4.7 3.6 4.4 4.5 4.1 4.5 3.9 4.8 4.0\n##  [73] 4.9 4.7 4.3 4.4 4.8 5.0 4.5 3.5 3.8 3.7 3.9 5.1 4.5 4.5 4.7 4.4 4.1 4.0\n##  [91] 4.4 4.6 4.0 3.3 4.2 4.2 4.2 4.3 3.0 4.1 6.0 5.1 5.9 5.6 5.8 6.6 4.5 6.3\n## [109] 5.8 6.1 5.1 5.3 5.5 5.0 5.1 5.3 5.5 6.7 6.9 5.0 5.7 4.9 6.7 4.9 5.7 6.0\n## [127] 4.8 4.9 5.6 5.8 6.1 6.4 5.6 5.1 5.6 6.1 5.6 5.5 4.8 5.4 5.6 5.1 5.1 5.9\n## [145] 5.7 5.2 5.0 5.2 5.4 5.1\niris$Petal.Length &gt; 5.0\n##   [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n##  [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [97] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n## [109]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n## [121]  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n## [133]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n## [145]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\nwhich(iris$Petal.Length &gt; 5.0)\n##  [1]  84 101 102 103 104 105 106 108 109 110 111 112 113 115 116 117 118 119 121\n## [20] 123 125 126 129 130 131 132 133 134 135 136 137 138 140 141 142 143 144 145\n## [39] 146 148 149 150\n\niris$Petal.Length[iris$Petal.Length &gt; 5.0]\n##  [1] 5.1 6.0 5.1 5.9 5.6 5.8 6.6 6.3 5.8 6.1 5.1 5.3 5.5 5.1 5.3 5.5 6.7 6.9 5.7\n## [20] 6.7 5.7 6.0 5.6 5.8 6.1 6.4 5.6 5.1 5.6 6.1 5.6 5.5 5.4 5.6 5.1 5.1 5.9 5.7\n## [39] 5.2 5.2 5.4 5.1\n\nidx &lt;- which(iris$Petal.Length &gt; 5.0) # 꽃잎의 길이가 5.0 이상인 값들의 인덱스\nidx\n##  [1]  84 101 102 103 104 105 106 108 109 110 111 112 113 115 116 117 118 119 121\n## [20] 123 125 126 129 130 131 132 133 134 135 136 137 138 140 141 142 143 144 145\n## [39] 146 148 149 150\niris.big &lt;- iris[idx, ] # 인덱스에 해당하는 값만 추출하여 저장\niris.big\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\n\n\n# 1~4열의 값 중 5보다 큰 값의 행과 열의 위치\nwhich(iris[, 1:4] &gt; 5.0)\n##   [1]   1   6  11  15  16  17  18  19  20  21  22  24  28  29  32  33  34  37\n##  [19]  40  45  47  49  51  52  53  54  55  56  57  59  60  62  63  64  65  66\n##  [37]  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84\n##  [55]  85  86  87  88  89  90  91  92  93  95  96  97  98  99 100 101 102 103\n##  [73] 104 105 106 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122\n##  [91] 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140\n## [109] 141 142 143 144 145 146 147 148 149 150 384 401 402 403 404 405 406 408\n## [127] 409 410 411 412 413 415 416 417 418 419 421 423 425 426 429 430 431 432\n## [145] 433 434 435 436 437 438 440 441 442 443 444 445 446 448 449 450\nwhich(iris[, 1:4] &gt; 5.0, arr.ind = TRUE) # arr.ind = TRUE : 조건에 맞는 인덱스까지 반환\n##        row col\n##   [1,]   1   1\n##   [2,]   6   1\n##   [3,]  11   1\n##   [4,]  15   1\n##   [5,]  16   1\n##   [6,]  17   1\n##   [7,]  18   1\n##   [8,]  19   1\n##   [9,]  20   1\n##  [10,]  21   1\n##  [11,]  22   1\n##  [12,]  24   1\n##  [13,]  28   1\n##  [14,]  29   1\n##  [15,]  32   1\n##  [16,]  33   1\n##  [17,]  34   1\n##  [18,]  37   1\n##  [19,]  40   1\n##  [20,]  45   1\n##  [21,]  47   1\n##  [22,]  49   1\n##  [23,]  51   1\n##  [24,]  52   1\n##  [25,]  53   1\n##  [26,]  54   1\n##  [27,]  55   1\n##  [28,]  56   1\n##  [29,]  57   1\n##  [30,]  59   1\n##  [31,]  60   1\n##  [32,]  62   1\n##  [33,]  63   1\n##  [34,]  64   1\n##  [35,]  65   1\n##  [36,]  66   1\n##  [37,]  67   1\n##  [38,]  68   1\n##  [39,]  69   1\n##  [40,]  70   1\n##  [41,]  71   1\n##  [42,]  72   1\n##  [43,]  73   1\n##  [44,]  74   1\n##  [45,]  75   1\n##  [46,]  76   1\n##  [47,]  77   1\n##  [48,]  78   1\n##  [49,]  79   1\n##  [50,]  80   1\n##  [51,]  81   1\n##  [52,]  82   1\n##  [53,]  83   1\n##  [54,]  84   1\n##  [55,]  85   1\n##  [56,]  86   1\n##  [57,]  87   1\n##  [58,]  88   1\n##  [59,]  89   1\n##  [60,]  90   1\n##  [61,]  91   1\n##  [62,]  92   1\n##  [63,]  93   1\n##  [64,]  95   1\n##  [65,]  96   1\n##  [66,]  97   1\n##  [67,]  98   1\n##  [68,]  99   1\n##  [69,] 100   1\n##  [70,] 101   1\n##  [71,] 102   1\n##  [72,] 103   1\n##  [73,] 104   1\n##  [74,] 105   1\n##  [75,] 106   1\n##  [76,] 108   1\n##  [77,] 109   1\n##  [78,] 110   1\n##  [79,] 111   1\n##  [80,] 112   1\n##  [81,] 113   1\n##  [82,] 114   1\n##  [83,] 115   1\n##  [84,] 116   1\n##  [85,] 117   1\n##  [86,] 118   1\n##  [87,] 119   1\n##  [88,] 120   1\n##  [89,] 121   1\n##  [90,] 122   1\n##  [91,] 123   1\n##  [92,] 124   1\n##  [93,] 125   1\n##  [94,] 126   1\n##  [95,] 127   1\n##  [96,] 128   1\n##  [97,] 129   1\n##  [98,] 130   1\n##  [99,] 131   1\n## [100,] 132   1\n## [101,] 133   1\n## [102,] 134   1\n## [103,] 135   1\n## [104,] 136   1\n## [105,] 137   1\n## [106,] 138   1\n## [107,] 139   1\n## [108,] 140   1\n## [109,] 141   1\n## [110,] 142   1\n## [111,] 143   1\n## [112,] 144   1\n## [113,] 145   1\n## [114,] 146   1\n## [115,] 147   1\n## [116,] 148   1\n## [117,] 149   1\n## [118,] 150   1\n## [119,]  84   3\n## [120,] 101   3\n## [121,] 102   3\n## [122,] 103   3\n## [123,] 104   3\n## [124,] 105   3\n## [125,] 106   3\n## [126,] 108   3\n## [127,] 109   3\n## [128,] 110   3\n## [129,] 111   3\n## [130,] 112   3\n## [131,] 113   3\n## [132,] 115   3\n## [133,] 116   3\n## [134,] 117   3\n## [135,] 118   3\n## [136,] 119   3\n## [137,] 121   3\n## [138,] 123   3\n## [139,] 125   3\n## [140,] 126   3\n## [141,] 129   3\n## [142,] 130   3\n## [143,] 131   3\n## [144,] 132   3\n## [145,] 133   3\n## [146,] 134   3\n## [147,] 135   3\n## [148,] 136   3\n## [149,] 137   3\n## [150,] 138   3\n## [151,] 140   3\n## [152,] 141   3\n## [153,] 142   3\n## [154,] 143   3\n## [155,] 144   3\n## [156,] 145   3\n## [157,] 146   3\n## [158,] 148   3\n## [159,] 149   3\n## [160,] 150   3\n\nidx &lt;- which(iris[, 1:4] &gt; 5.0, arr.ind = TRUE)\niris[idx[, 1], ]\n##       Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 1              5.1         3.5          1.4         0.2     setosa\n## 6              5.4         3.9          1.7         0.4     setosa\n## 11             5.4         3.7          1.5         0.2     setosa\n## 15             5.8         4.0          1.2         0.2     setosa\n## 16             5.7         4.4          1.5         0.4     setosa\n## 17             5.4         3.9          1.3         0.4     setosa\n## 18             5.1         3.5          1.4         0.3     setosa\n## 19             5.7         3.8          1.7         0.3     setosa\n## 20             5.1         3.8          1.5         0.3     setosa\n## 21             5.4         3.4          1.7         0.2     setosa\n## 22             5.1         3.7          1.5         0.4     setosa\n## 24             5.1         3.3          1.7         0.5     setosa\n## 28             5.2         3.5          1.5         0.2     setosa\n## 29             5.2         3.4          1.4         0.2     setosa\n## 32             5.4         3.4          1.5         0.4     setosa\n## 33             5.2         4.1          1.5         0.1     setosa\n## 34             5.5         4.2          1.4         0.2     setosa\n## 37             5.5         3.5          1.3         0.2     setosa\n## 40             5.1         3.4          1.5         0.2     setosa\n## 45             5.1         3.8          1.9         0.4     setosa\n## 47             5.1         3.8          1.6         0.2     setosa\n## 49             5.3         3.7          1.5         0.2     setosa\n## 51             7.0         3.2          4.7         1.4 versicolor\n## 52             6.4         3.2          4.5         1.5 versicolor\n## 53             6.9         3.1          4.9         1.5 versicolor\n## 54             5.5         2.3          4.0         1.3 versicolor\n## 55             6.5         2.8          4.6         1.5 versicolor\n## 56             5.7         2.8          4.5         1.3 versicolor\n## 57             6.3         3.3          4.7         1.6 versicolor\n## 59             6.6         2.9          4.6         1.3 versicolor\n## 60             5.2         2.7          3.9         1.4 versicolor\n## 62             5.9         3.0          4.2         1.5 versicolor\n## 63             6.0         2.2          4.0         1.0 versicolor\n## 64             6.1         2.9          4.7         1.4 versicolor\n## 65             5.6         2.9          3.6         1.3 versicolor\n## 66             6.7         3.1          4.4         1.4 versicolor\n## 67             5.6         3.0          4.5         1.5 versicolor\n## 68             5.8         2.7          4.1         1.0 versicolor\n## 69             6.2         2.2          4.5         1.5 versicolor\n## 70             5.6         2.5          3.9         1.1 versicolor\n## 71             5.9         3.2          4.8         1.8 versicolor\n## 72             6.1         2.8          4.0         1.3 versicolor\n## 73             6.3         2.5          4.9         1.5 versicolor\n## 74             6.1         2.8          4.7         1.2 versicolor\n## 75             6.4         2.9          4.3         1.3 versicolor\n## 76             6.6         3.0          4.4         1.4 versicolor\n## 77             6.8         2.8          4.8         1.4 versicolor\n## 78             6.7         3.0          5.0         1.7 versicolor\n## 79             6.0         2.9          4.5         1.5 versicolor\n## 80             5.7         2.6          3.5         1.0 versicolor\n## 81             5.5         2.4          3.8         1.1 versicolor\n## 82             5.5         2.4          3.7         1.0 versicolor\n## 83             5.8         2.7          3.9         1.2 versicolor\n## 84             6.0         2.7          5.1         1.6 versicolor\n## 85             5.4         3.0          4.5         1.5 versicolor\n## 86             6.0         3.4          4.5         1.6 versicolor\n## 87             6.7         3.1          4.7         1.5 versicolor\n## 88             6.3         2.3          4.4         1.3 versicolor\n## 89             5.6         3.0          4.1         1.3 versicolor\n## 90             5.5         2.5          4.0         1.3 versicolor\n## 91             5.5         2.6          4.4         1.2 versicolor\n## 92             6.1         3.0          4.6         1.4 versicolor\n## 93             5.8         2.6          4.0         1.2 versicolor\n## 95             5.6         2.7          4.2         1.3 versicolor\n## 96             5.7         3.0          4.2         1.2 versicolor\n## 97             5.7         2.9          4.2         1.3 versicolor\n## 98             6.2         2.9          4.3         1.3 versicolor\n## 99             5.1         2.5          3.0         1.1 versicolor\n## 100            5.7         2.8          4.1         1.3 versicolor\n## 101            6.3         3.3          6.0         2.5  virginica\n## 102            5.8         2.7          5.1         1.9  virginica\n## 103            7.1         3.0          5.9         2.1  virginica\n## 104            6.3         2.9          5.6         1.8  virginica\n## 105            6.5         3.0          5.8         2.2  virginica\n## 106            7.6         3.0          6.6         2.1  virginica\n## 108            7.3         2.9          6.3         1.8  virginica\n## 109            6.7         2.5          5.8         1.8  virginica\n## 110            7.2         3.6          6.1         2.5  virginica\n## 111            6.5         3.2          5.1         2.0  virginica\n## 112            6.4         2.7          5.3         1.9  virginica\n## 113            6.8         3.0          5.5         2.1  virginica\n## 114            5.7         2.5          5.0         2.0  virginica\n## 115            5.8         2.8          5.1         2.4  virginica\n## 116            6.4         3.2          5.3         2.3  virginica\n## 117            6.5         3.0          5.5         1.8  virginica\n## 118            7.7         3.8          6.7         2.2  virginica\n## 119            7.7         2.6          6.9         2.3  virginica\n## 120            6.0         2.2          5.0         1.5  virginica\n## 121            6.9         3.2          5.7         2.3  virginica\n## 122            5.6         2.8          4.9         2.0  virginica\n## 123            7.7         2.8          6.7         2.0  virginica\n## 124            6.3         2.7          4.9         1.8  virginica\n## 125            6.7         3.3          5.7         2.1  virginica\n## 126            7.2         3.2          6.0         1.8  virginica\n## 127            6.2         2.8          4.8         1.8  virginica\n## 128            6.1         3.0          4.9         1.8  virginica\n## 129            6.4         2.8          5.6         2.1  virginica\n## 130            7.2         3.0          5.8         1.6  virginica\n## 131            7.4         2.8          6.1         1.9  virginica\n## 132            7.9         3.8          6.4         2.0  virginica\n## 133            6.4         2.8          5.6         2.2  virginica\n## 134            6.3         2.8          5.1         1.5  virginica\n## 135            6.1         2.6          5.6         1.4  virginica\n## 136            7.7         3.0          6.1         2.3  virginica\n## 137            6.3         3.4          5.6         2.4  virginica\n## 138            6.4         3.1          5.5         1.8  virginica\n## 139            6.0         3.0          4.8         1.8  virginica\n## 140            6.9         3.1          5.4         2.1  virginica\n## 141            6.7         3.1          5.6         2.4  virginica\n## 142            6.9         3.1          5.1         2.3  virginica\n## 143            5.8         2.7          5.1         1.9  virginica\n## 144            6.8         3.2          5.9         2.3  virginica\n## 145            6.7         3.3          5.7         2.5  virginica\n## 146            6.7         3.0          5.2         2.3  virginica\n## 147            6.3         2.5          5.0         1.9  virginica\n## 148            6.5         3.0          5.2         2.0  virginica\n## 149            6.2         3.4          5.4         2.3  virginica\n## 150            5.9         3.0          5.1         1.8  virginica\n## 84.1           6.0         2.7          5.1         1.6 versicolor\n## 101.1          6.3         3.3          6.0         2.5  virginica\n## 102.1          5.8         2.7          5.1         1.9  virginica\n## 103.1          7.1         3.0          5.9         2.1  virginica\n## 104.1          6.3         2.9          5.6         1.8  virginica\n## 105.1          6.5         3.0          5.8         2.2  virginica\n## 106.1          7.6         3.0          6.6         2.1  virginica\n## 108.1          7.3         2.9          6.3         1.8  virginica\n## 109.1          6.7         2.5          5.8         1.8  virginica\n## 110.1          7.2         3.6          6.1         2.5  virginica\n## 111.1          6.5         3.2          5.1         2.0  virginica\n## 112.1          6.4         2.7          5.3         1.9  virginica\n## 113.1          6.8         3.0          5.5         2.1  virginica\n## 115.1          5.8         2.8          5.1         2.4  virginica\n## 116.1          6.4         3.2          5.3         2.3  virginica\n## 117.1          6.5         3.0          5.5         1.8  virginica\n## 118.1          7.7         3.8          6.7         2.2  virginica\n## 119.1          7.7         2.6          6.9         2.3  virginica\n## 121.1          6.9         3.2          5.7         2.3  virginica\n## 123.1          7.7         2.8          6.7         2.0  virginica\n## 125.1          6.7         3.3          5.7         2.1  virginica\n## 126.1          7.2         3.2          6.0         1.8  virginica\n## 129.1          6.4         2.8          5.6         2.1  virginica\n## 130.1          7.2         3.0          5.8         1.6  virginica\n## 131.1          7.4         2.8          6.1         1.9  virginica\n## 132.1          7.9         3.8          6.4         2.0  virginica\n## 133.1          6.4         2.8          5.6         2.2  virginica\n## 134.1          6.3         2.8          5.1         1.5  virginica\n## 135.1          6.1         2.6          5.6         1.4  virginica\n## 136.1          7.7         3.0          6.1         2.3  virginica\n## 137.1          6.3         3.4          5.6         2.4  virginica\n## 138.1          6.4         3.1          5.5         1.8  virginica\n## 140.1          6.9         3.1          5.4         2.1  virginica\n## 141.1          6.7         3.1          5.6         2.4  virginica\n## 142.1          6.9         3.1          5.1         2.3  virginica\n## 143.1          5.8         2.7          5.1         1.9  virginica\n## 144.1          6.8         3.2          5.9         2.3  virginica\n## 145.1          6.7         3.3          5.7         2.5  virginica\n## 146.1          6.7         3.0          5.2         2.3  virginica\n## 148.1          6.5         3.0          5.2         2.0  virginica\n## 149.1          6.2         3.4          5.4         2.3  virginica\n## 150.1          5.9         3.0          5.1         1.8  virginica\n\niris[, 1:4][idx]\n##   [1] 5.1 5.4 5.4 5.8 5.7 5.4 5.1 5.7 5.1 5.4 5.1 5.1 5.2 5.2 5.4 5.2 5.5 5.5\n##  [19] 5.1 5.1 5.1 5.3 7.0 6.4 6.9 5.5 6.5 5.7 6.3 6.6 5.2 5.9 6.0 6.1 5.6 6.7\n##  [37] 5.6 5.8 6.2 5.6 5.9 6.1 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7 5.5 5.5 5.8 6.0\n##  [55] 5.4 6.0 6.7 6.3 5.6 5.5 5.5 6.1 5.8 5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 7.1\n##  [73] 6.3 6.5 7.6 7.3 6.7 7.2 6.5 6.4 6.8 5.7 5.8 6.4 6.5 7.7 7.7 6.0 6.9 5.6\n##  [91] 7.7 6.3 6.7 7.2 6.2 6.1 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 6.3 6.4 6.0 6.9\n## [109] 6.7 6.9 5.8 6.8 6.7 6.7 6.3 6.5 6.2 5.9 5.1 6.0 5.1 5.9 5.6 5.8 6.6 6.3\n## [127] 5.8 6.1 5.1 5.3 5.5 5.1 5.3 5.5 6.7 6.9 5.7 6.7 5.7 6.0 5.6 5.8 6.1 6.4\n## [145] 5.6 5.1 5.6 6.1 5.6 5.5 5.4 5.6 5.1 5.1 5.9 5.7 5.2 5.2 5.4 5.1\n\n\n\n\n\nfavorite &lt;- c('WINTER', 'SUMMER', 'SPRING', 'SUMMER', 'SUMMER',\n              'FALL', 'FALL', 'SUMMER', 'SPRING', 'SPRING')\nfavorite # favorite의 내용 출력\n##  [1] \"WINTER\" \"SUMMER\" \"SPRING\" \"SUMMER\" \"SUMMER\" \"FALL\"   \"FALL\"   \"SUMMER\"\n##  [9] \"SPRING\" \"SPRING\"\ntable(favorite) # 도수분포표 계산\n## favorite\n##   FALL SPRING SUMMER WINTER \n##      2      3      4      1\nlength(favorite)\n## [1] 10\ntable(favorite) / length(favorite) # 비율 출력\n## favorite\n##   FALL SPRING SUMMER WINTER \n##    0.2    0.3    0.4    0.1\n\n\nds &lt;- table(favorite)\nds\n## favorite\n##   FALL SPRING SUMMER WINTER \n##      2      3      4      1\nbarplot(ds, main = 'favorite season')\n\n\n\n\n\nds &lt;- table(favorite)\nds\n## favorite\n##   FALL SPRING SUMMER WINTER \n##      2      3      4      1\npie(ds, main = 'favorite season')\n\n\n\n\n\nfavorite.color &lt;- c(2, 3, 2, 1, 1, 2, 2, 1, 3, 2, 1, 3, 2, 1, 2)\nds &lt;- table(favorite.color)\nds\n## favorite.color\n## 1 2 3 \n## 5 7 3\nbarplot(ds, main = 'favorite color')\n\n\n\ncolors &lt;- c('green', 'red', 'blue')\nnames(ds) &lt;- colors # 자료값 1, 2, 3을 green, red, blue로 변경\nds\n## green   red  blue \n##     5     7     3\nbarplot(ds, main = 'favorite color', col = colors) # 색 지정 막대그래프\n\n\n\nbarplot(ds, main = 'favorite color', col = c('green', 'red', 'blue'))\npie(ds, main = 'favorite color', col = colors) # 색 지정 원그래프\n\n\n\n\n\nweight &lt;- c(60, 62, 64, 65, 68, 69)\nweight.heavy &lt;- c(weight, 120)\nweight\n## [1] 60 62 64 65 68 69\nweight.heavy\n## [1]  60  62  64  65  68  69 120\n\nmean(weight) # 평균\n## [1] 64.66667\nmean(weight.heavy) # 평균\n## [1] 72.57143\n\nmedian(weight) # 중앙값\n## [1] 64.5\nmedian(weight.heavy) # 중앙값\n## [1] 65\n\nmean(weight, trim = 0.2) # 절사평균(상하위 20% 제외)\n## [1] 64.75\nmean(weight.heavy, trim = 0.2) # 절사평균(상하위 20% 제외)\n## [1] 65.6\n\n\nmydata &lt;- c(60, 62, 64, 65, 68, 69, 120)\nquantile(mydata)\n##    0%   25%   50%   75%  100% \n##  60.0  63.0  65.0  68.5 120.0\nquantile(mydata, (0:10) / 10) # 10% 단위로 구간을 나누어 계산\n##    0%   10%   20%   30%   40%   50%   60%   70%   80%   90%  100% \n##  60.0  61.2  62.4  63.6  64.4  65.0  66.8  68.2  68.8  89.4 120.0\nsummary(mydata) # 최소값, 중앙값, 평균값, 3분위 값, 최대값\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   60.00   63.00   65.00   72.57   68.50  120.00\n\nmydata &lt;- 0:1000\nquantile(mydata)\n##   0%  25%  50%  75% 100% \n##    0  250  500  750 1000\nquantile(mydata, (0:10) / 10)\n##   0%  10%  20%  30%  40%  50%  60%  70%  80%  90% 100% \n##    0  100  200  300  400  500  600  700  800  900 1000\nsummary(mydata)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##       0     250     500     500     750    1000\n?quantile\n## starting httpd help server ... done\n\n\nmydata &lt;- c(60, 62, 64, 65, 68, 69, 120)\nvar(mydata) # 분산\n## [1] 447.2857\nsd(mydata) # 표준편차\n## [1] 21.14913\nrange(mydata) # 값의 범위\n## [1]  60 120\ndiff(range(mydata)) # 최대값, 최소값의 차이\n## [1] 60\n\n\ndist &lt;- cars[, 2] # 자동차 제동거리\nhist(dist,                            # 자료(data)\n     main = \"Histogram for 제동거리\", # 제목\n     xlab = \"제동거리\",               # x축 레이블\n     ylab = \"빈도수\",                 # y축 레이블\n     border = \"blue\",                 # 막대 테두리색\n     col = rainbow(10),               # 막대 색\n     las = 2,                         # x축 글씨 방향(0~3)\n     breaks = seq(0, 120, 10))        # 막대 개수 조절\n\n\n\n\n\ndist &lt;- cars[,2] # 자동차 제동거리(단위: 피트(ft))\nboxplot(dist, main = \"자동차 제동거리\") # ★★★★★\n\n\n\n\n\nboxplot.stats(dist)\n## $stats\n## [1]  2 26 36 56 93\n## \n## $n\n## [1] 50\n## \n## $conf\n## [1] 29.29663 42.70337\n## \n## $out\n## [1] 120\nboxplot.stats(dist)$stats\n## [1]  2 26 36 56 93\nboxplot.stats(dist)$stats[4]\n## [1] 56\n\n\nboxplot(Petal.Length ~ Species, data = iris, main = \"품종별 꽃잎의 길이\")\n\n\n\n\npar(mfrow = c(1, 3)) # 1*3 가상화면 분할\n\nbarplot(\n    table(mtcars$carb),\n    main = \"Barplot of Carburetors\",\n    xlab = \"#of carburetors\",\n    ylab = \"frequency\",\n    col = \"blue\"\n)\n\nbarplot(\n    table(mtcars$cyl),\n    main = \"Barplot of Cylender\",\n    xlab = \"#of cylender\",\n    ylab = \"frequency\",\n    col = \"red\"\n)\n\nbarplot(\n    table(mtcars$gear),\n    main = \"Barplot of Grar\",\n    xlab = \"#of gears\",\n    ylab = \"frequency\",\n    col = \"green\"\n)\n\n\n\n\npar(mfrow = c(1, 1)) # 가상화면 분할 해제\n\n\n\n\n\nwt &lt;- mtcars$wt                 # 중량 자료\nmpg &lt;- mtcars$mpg               # 연비 자료\nplot(wt, mpg,                   # 2개 변수(x축, y축)\n     main = \"중량-연비 그래프\", # 제목\n     xlab = \"중량\",             # x축 레이블\n     ylab = \"연비(MPG)\",        # y축 레이블\n     col = \"red\",               # point의 color\n     pch = 11)                  # point의 종류\n\n\n\n\n\nvars &lt;- c(\"mpg\", \"disp\", \"drat\", \"wt\") # 대상 변수(연비, 배기량, 후방차측 비율, 중량)\ntarget &lt;- mtcars[, vars]\nhead(target)\n##                    mpg disp drat    wt\n## Mazda RX4         21.0  160 3.90 2.620\n## Mazda RX4 Wag     21.0  160 3.90 2.875\n## Datsun 710        22.8  108 3.85 2.320\n## Hornet 4 Drive    21.4  258 3.08 3.215\n## Hornet Sportabout 18.7  360 3.15 3.440\n## Valiant           18.1  225 2.76 3.460\npairs(target, main = \"Multi Plots\")    # 대상 데이터\n\n\n\n\n\niris.2 &lt;- iris[, 3:4]              # 데이터 준비\npoint &lt;- as.numeric(iris$Species)  # 점의 모양\npoint                              # point 내용 출력\n##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n##  [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n##  [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3\n## [112] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n## [149] 3 3\ncolor &lt;- c(\"red\", \"green\", \"blue\") # 점의 컬러\nplot(iris.2,\n     main = \"Iris plot\",\n     pch = c(point),\n     col = color[point])\n\n\n\n\n\nbeers = c(5, 2, 9, 8, 3, 7, 3, 5, 3, 5) # 자료 입력\nbal &lt;- c(0.1, 0.03, 0.19, 0.12, 0.04, 0.0095, 0.07, 0.06, 0.02, 0.05)\ntbl &lt;- data.frame(beers, bal)           # 데이터프레임 생성\ntbl\n##    beers    bal\n## 1      5 0.1000\n## 2      2 0.0300\n## 3      9 0.1900\n## 4      8 0.1200\n## 5      3 0.0400\n## 6      7 0.0095\n## 7      3 0.0700\n## 8      5 0.0600\n## 9      3 0.0200\n## 10     5 0.0500\nplot(bal ~ beers, data = tbl)           # 산점도 plot(beers, bal)\nres &lt;- lm(bal ~ beers, data = tbl)      # 회귀식 도출\nabline(res)                             # 회귀선 그리기\n\n\n\ncor(beers, bal)                         # 상관계수 계산\n## [1] 0.6797025\n\n\ncor(iris[, 1:4]) # 4개 변수 간 상관성 분석\n##              Sepal.Length Sepal.Width Petal.Length Petal.Width\n## Sepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411\n## Sepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259\n## Petal.Length    0.8717538  -0.4284401    1.0000000   0.9628654\n## Petal.Width     0.8179411  -0.3661259    0.9628654   1.0000000\n\n\nmonth = 1:12 # 자료 입력\nlate = c(5, 8, 7, 9, 4, 6, 12, 13, 8, 6, 6, 4) # 자료 입력\nplot(month,                # x data\n     late,                 # y data\n     main = \"지각생 통계\", # 제목\n     type = \"l\",           # 그래프의 종류 선택(알파벳)\n     lty = 1,              # 선의 종류(line type) 선택\n     lwd = 1,              # 선의 굵기 선택\n     xlab = \"Month\",       # x축 레이블\n     ylab = \"Late cnt\")    # y축 레이블\n\n\n\n\n\nmonth = 1:12\nlate1 = c(5, 8, 7, 9, 4, 6, 12, 13, 8, 6, 6, 4)\nlate2 = c(4, 6, 5, 8, 7, 8, 10, 11, 6, 5, 7, 3)\nplot(month,                  # x data\n     late1,                  # y data\n     main = \"Late Students\",\n     type = \"b\",             # 그래프의 종류 선택(알파벳)\n     lty = 1,                # 선의 종류(line type) 선택\n     col = \"red\",            # 선의 색 선택\n     xlab = \"Month\",         # x축 레이블\n     ylab = \"Late cnt\",      # y축 레이블\n     ylim = c(1, 15))        # y축 값의 (하한, 상한)\n\nlines(month,                 # x data\n      late2,                 # y data\n      type = \"b\",            # 선의 종류(line type) 선택\n      col = \"blue\")          # 선의 색 선택\n\n\n\n\n\n## (1) 분석 대상 데이터셋 준비\n# install.packages(\"mlbench\")\nlibrary(mlbench)\ndata(\"BostonHousing\")\nmyds &lt;- BostonHousing[, c(\"crim\", \"rm\", \"dis\", \"tax\", \"medv\")]\n\n## (2) grp 변수 추가 ★★★★★\ngrp &lt;- c()\nfor (i in 1:nrow(myds)) {\n    # myds$medv 값에 따라 그룹 분류\n    if (myds$medv[i] &gt;= 25.0) {\n        grp[i] &lt;- \"H\"\n    } else if (myds$medv[i] &lt;= 17.0) {\n        grp[i] &lt;- \"L\"\n    } else {\n        grp[i] &lt;- \"M\"\n    }\n}\ngrp &lt;- factor(grp) # 문자 벡터를 팩터 타입으로 변경\ngrp &lt;- factor(grp, levels = c(\"H\", \"M\", \"L\")) # 레벨의 순서를 H, L, M -&gt; H, M, L\n\nmyds &lt;- data.frame(myds, grp) # myds에 grp 열 추가\n\n## (3) 데이터셋의 형태와 기본적인 내용 파악\nstr(myds)\n## 'data.frame':    506 obs. of  6 variables:\n##  $ crim: num  0.00632 0.02731 0.02729 0.03237 0.06905 ...\n##  $ rm  : num  6.58 6.42 7.18 7 7.15 ...\n##  $ dis : num  4.09 4.97 4.97 6.06 6.06 ...\n##  $ tax : num  296 242 242 222 222 222 311 311 311 311 ...\n##  $ medv: num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...\n##  $ grp : Factor w/ 3 levels \"H\",\"M\",\"L\": 2 2 1 1 1 1 2 1 3 2 ...\nhead(myds)\n##      crim    rm    dis tax medv grp\n## 1 0.00632 6.575 4.0900 296 24.0   M\n## 2 0.02731 6.421 4.9671 242 21.6   M\n## 3 0.02729 7.185 4.9671 242 34.7   H\n## 4 0.03237 6.998 6.0622 222 33.4   H\n## 5 0.06905 7.147 6.0622 222 36.2   H\n## 6 0.02985 6.430 6.0622 222 28.7   H\ntable(myds$grp) # 주택 가격 그룹별 분포\n## \n##   H   M   L \n## 132 247 127\n\n## (4) 히스토그램에 의한 관측값의 분포 확인\npar(mfrow = c(2, 3)) # 2*3 가상화면 분할\nfor (i in 1:5) {\n    hist(myds[, i], main = colnames(myds)[i], col = \"yellow\")\n}\npar(mfrow = c(1, 1)) # 2*3 가상화면 분할 해제\n\n\n\n\n## (5) 상자그림에 의한 관측값의 분포 확인\npar(mfrow = c(2, 3)) # 2*3 가상화면 분할\nfor (i in 1:5) {\n    boxplot(myds[, i], main = colnames(myds)[i])\n}\npar(mfrow = c(1, 1)) # 2*3 가상화면 분할 해제\n\n\n\n\n## (6) 그룹별 관측값 분포의 확인\nboxplot(myds$crim ~ myds$grp, main = \"1인당 범죄율\")\n\n\n\nboxplot(myds$rm ~ myds$grp, main = \"방의 개수\")\n\n\n\nboxplot(myds$dis ~ myds$grp, main = \"직업 센터까지의 거리\")\n\n\n\nboxplot(myds$tax ~ myds$grp, main = \"재산세율\")\n\n\n\n\n## (7) 다중 산점도를 통한 변수 간 상관 관계의 확인\npairs(myds[, -6]) # 6번째 열 제거(grp)\npairs(myds[, 1:5])\n\n\n\n\n## (8) 그룹 정보를 포함한 변수 간 상관 관계의 확인\npoint &lt;- as.integer(myds$grp) # 점의 모양 지정\ncolor &lt;- c(\"red\", \"green\", \"blue\") # 점의 색 지정\npairs(myds[, -6], pch = point, col = color[point])\n\n\n\n\n## (9) 변수 간 상관계수의 확인\ncor(myds[, -6])\n##            crim         rm        dis        tax       medv\n## crim  1.0000000 -0.2192467 -0.3796701  0.5827643 -0.3883046\n## rm   -0.2192467  1.0000000  0.2052462 -0.2920478  0.6953599\n## dis  -0.3796701  0.2052462  1.0000000 -0.5344316  0.2499287\n## tax   0.5827643 -0.2920478 -0.5344316  1.0000000 -0.4685359\n## medv -0.3883046  0.6953599  0.2499287 -0.4685359  1.0000000\ncor(myds[1:5])\n##            crim         rm        dis        tax       medv\n## crim  1.0000000 -0.2192467 -0.3796701  0.5827643 -0.3883046\n## rm   -0.2192467  1.0000000  0.2052462 -0.2920478  0.6953599\n## dis  -0.3796701  0.2052462  1.0000000 -0.5344316  0.2499287\n## tax   0.5827643 -0.2920478 -0.5344316  1.0000000 -0.4685359\n## medv -0.3883046  0.6953599  0.2499287 -0.4685359  1.0000000\n\n\n\n\n\nz &lt;- c(1, 2, 3, NA, 5, NA, 8)   # 결측값이 포함된 벡터 z\nsum(z)                          # 정상 계산이 안 됨\n## [1] NA\nis.na(z)                        # NA 여부 확인\n## [1] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE\nsum(is.na(z))                   # NA의 개수 확인\n## [1] 2\nsum(z, na.rm = TRUE)            # NA를 제외하고 합계를 계산\n## [1] 19\n\n\nz1 &lt;- c(1, 2, 3, NA, 5, NA, 8)          # 결측값이 포함된 벡터 z1\nz2 &lt;- c(5, 8, 1, NA, 3, NA, 7)          # 결측값이 포함된 벡터 z2\nz1[is.na(z1)] &lt;- 0                      # NA를 0으로 치환\nz1\n## [1] 1 2 3 0 5 0 8\n\nz1[is.na(z1)] &lt;- mean(z1, na.rm = TRUE) # NA를 z1의 평균값으로 치환\nz1\n## [1] 1 2 3 0 5 0 8\n\nz3 &lt;- as.vector(na.omit(z2))            # NA를 제거하고 새로운 벡터 생성\nz3\n## [1] 5 8 1 3 7\n\n\n# NA를 포함하는 test 데이터 생성\nx &lt;- iris\nx[1, 2] &lt;- NA\nx[1, 3] &lt;- NA\nx[2, 3] &lt;- NA\nx[3, 4] &lt;- NA\nhead(x)\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1          NA           NA         0.2  setosa\n## 2          4.9         3.0           NA         0.2  setosa\n## 3          4.7         3.2          1.3          NA  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\n\n\n# for문을 이용한 방법 ★★★★★\nfor (i in 1:ncol(x)) {\n    this.na &lt;- is.na(x[, i])\n    cat(colnames(x)[i], \"\\t\", sum(this.na), \"\\n\")\n}\n## Sepal.Length      0 \n## Sepal.Width   1 \n## Petal.Length      2 \n## Petal.Width   1 \n## Species   0\n\n# apply를 이용한 방법\ncol_na &lt;- function(y) {\n    return(sum(is.na(y)))\n}\n\nna_count &lt;- apply(x, 2, FUN = col_na)\nna_count\n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n##            0            1            2            1            0\n\n\nrowSums(is.na(x))           # 행별 NA의 개수\n##   [1] 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n##  [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n##  [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n## [112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n## [149] 0 0\nsum(rowSums(is.na(x)) &gt; 0)  # NA가 포함된 행의 개수\n## [1] 3\n\nsum(is.na(x))               # 데이터셋 전체에서 NA 개수\n## [1] 4\n\n\nhead(x)\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1          NA           NA         0.2  setosa\n## 2          4.9         3.0           NA         0.2  setosa\n## 3          4.7         3.2          1.3          NA  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\nx[!complete.cases(x), ]     # NA가 포함된 행들 출력\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1          NA           NA         0.2  setosa\n## 2          4.9         3.0           NA         0.2  setosa\n## 3          4.7         3.2          1.3          NA  setosa\ny &lt;- x[complete.cases(x), ] # NA가 포함된 행들 제거\nhead(y)                     # 새로운 데이터셋 y의 내용 확인\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\n## 7          4.6         3.4          1.4         0.3  setosa\n## 8          5.0         3.4          1.5         0.2  setosa\n## 9          4.4         2.9          1.4         0.2  setosa\n\n\nst &lt;- data.frame(state.x77)\nboxplot(st$Income)\n\n\n\nboxplot.stats(st$Income)\n## $stats\n## [1] 3098 3983 4519 4815 5348\n## \n## $n\n## [1] 50\n## \n## $conf\n## [1] 4333.093 4704.907\n## \n## $out\n## [1] 6315\n# stats (각 변수의 최소값, 1사분위수, 2사분위수, 3사분위수, 최대값이 저장되어 있는 행렬)\n# n (각 그룹마다의 관측값 수를 저장한 벡터)\n# conf (중앙값의 95% 신뢰구간, median+-1.58*IQR/(n)^0.5)\n# out (이상치)\nboxplot.stats(st$Income)$out\n## [1] 6315\n\n\nout.val &lt;- boxplot.stats(st$Income)$out     # 특이값 추출\n\nst$Income %in% out.val\n##  [1] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [49] FALSE FALSE\nst$Income == out.val\n##  [1] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [49] FALSE FALSE\n\nst$Income[st$Income %in% out.val] &lt;- NA     # 특이값을 NA로 대체\nst$Income[st$Income == out.val] &lt;- NA\n\nhead(st)\n##            Population Income Illiteracy Life.Exp Murder HS.Grad Frost   Area\n## Alabama          3615   3624        2.1    69.05   15.1    41.3    20  50708\n## Alaska            365     NA        1.5    69.31   11.3    66.7   152 566432\n## Arizona          2212   4530        1.8    70.55    7.8    58.1    15 113417\n## Arkansas         2110   3378        1.9    70.66   10.1    39.9    65  51945\n## California      21198   5114        1.1    71.71   10.3    62.6    20 156361\n## Colorado         2541   4884        0.7    72.06    6.8    63.9   166 103766\nnewdata &lt;- st[complete.cases(st), ]         # NA가 포함된 행 제거 ★★★★★\nhead(newdata)\n##             Population Income Illiteracy Life.Exp Murder HS.Grad Frost   Area\n## Alabama           3615   3624        2.1    69.05   15.1    41.3    20  50708\n## Arizona           2212   4530        1.8    70.55    7.8    58.1    15 113417\n## Arkansas          2110   3378        1.9    70.66   10.1    39.9    65  51945\n## California       21198   5114        1.1    71.71   10.3    62.6    20 156361\n## Colorado          2541   4884        0.7    72.06    6.8    63.9   166 103766\n## Connecticut       3100   5348        1.1    72.48    3.1    56.0   139   4862\n\n\nv1 &lt;- c(1, 7, 6, 8, 4, 2, 3)\norder(v1)\n## [1] 1 6 7 5 3 2 4\n\nv1 &lt;- sort(v1) # 오름차순\nv1\n## [1] 1 2 3 4 6 7 8\nv1[order(v1)]\n## [1] 1 2 3 4 6 7 8\n\nv2 &lt;- sort(v1, decreasing = T) # 내림차순\nv2\n## [1] 8 7 6 4 3 2 1\n\n\nhead(iris)\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\norder(iris$Sepal.Length)\n##   [1]  14   9  39  43  42   4   7  23  48   3  30  12  13  25  31  46   2  10\n##  [19]  35  38  58 107   5   8  26  27  36  41  44  50  61  94   1  18  20  22\n##  [37]  24  40  45  47  99  28  29  33  60  49   6  11  17  21  32  85  34  37\n##  [55]  54  81  82  90  91  65  67  70  89  95 122  16  19  56  80  96  97 100\n##  [73] 114  15  68  83  93 102 115 143  62  71 150  63  79  84  86 120 139  64\n##  [91]  72  74  92 128 135  69  98 127 149  57  73  88 101 104 124 134 137 147\n## [109]  52  75 112 116 129 133 138  55 105 111 117 148  59  76  66  78  87 109\n## [127] 125 141 145 146  77 113 144  53 121 140 142  51 103 110 126 130 108 131\n## [145] 106 118 119 123 136 132\niris[order(iris$Sepal.Length), ]                    # 오름차순으로 정렬\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 14           4.3         3.0          1.1         0.1     setosa\n## 9            4.4         2.9          1.4         0.2     setosa\n## 39           4.4         3.0          1.3         0.2     setosa\n## 43           4.4         3.2          1.3         0.2     setosa\n## 42           4.5         2.3          1.3         0.3     setosa\n## 4            4.6         3.1          1.5         0.2     setosa\n## 7            4.6         3.4          1.4         0.3     setosa\n## 23           4.6         3.6          1.0         0.2     setosa\n## 48           4.6         3.2          1.4         0.2     setosa\n## 3            4.7         3.2          1.3         0.2     setosa\n## 30           4.7         3.2          1.6         0.2     setosa\n## 12           4.8         3.4          1.6         0.2     setosa\n## 13           4.8         3.0          1.4         0.1     setosa\n## 25           4.8         3.4          1.9         0.2     setosa\n## 31           4.8         3.1          1.6         0.2     setosa\n## 46           4.8         3.0          1.4         0.3     setosa\n## 2            4.9         3.0          1.4         0.2     setosa\n## 10           4.9         3.1          1.5         0.1     setosa\n## 35           4.9         3.1          1.5         0.2     setosa\n## 38           4.9         3.6          1.4         0.1     setosa\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 107          4.9         2.5          4.5         1.7  virginica\n## 5            5.0         3.6          1.4         0.2     setosa\n## 8            5.0         3.4          1.5         0.2     setosa\n## 26           5.0         3.0          1.6         0.2     setosa\n## 27           5.0         3.4          1.6         0.4     setosa\n## 36           5.0         3.2          1.2         0.2     setosa\n## 41           5.0         3.5          1.3         0.3     setosa\n## 44           5.0         3.5          1.6         0.6     setosa\n## 50           5.0         3.3          1.4         0.2     setosa\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 1            5.1         3.5          1.4         0.2     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 28           5.2         3.5          1.5         0.2     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 49           5.3         3.7          1.5         0.2     setosa\n## 6            5.4         3.9          1.7         0.4     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 34           5.5         4.2          1.4         0.2     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 122          5.6         2.8          4.9         2.0  virginica\n## 16           5.7         4.4          1.5         0.4     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 114          5.7         2.5          5.0         2.0  virginica\n## 15           5.8         4.0          1.2         0.2     setosa\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 102          5.8         2.7          5.1         1.9  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 150          5.9         3.0          5.1         1.8  virginica\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 120          6.0         2.2          5.0         1.5  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 128          6.1         3.0          4.9         1.8  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 127          6.2         2.8          4.8         1.8  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 112          6.4         2.7          5.3         1.9  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 105          6.5         3.0          5.8         2.2  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 109          6.7         2.5          5.8         1.8  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 113          6.8         3.0          5.5         2.1  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 121          6.9         3.2          5.7         2.3  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 103          7.1         3.0          5.9         2.1  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\niris[order(iris$Sepal.Length, decreasing = T), ]    # 내림차순으로 정렬\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 132          7.9         3.8          6.4         2.0  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 121          6.9         3.2          5.7         2.3  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 113          6.8         3.0          5.5         2.1  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 109          6.7         2.5          5.8         1.8  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 105          6.5         3.0          5.8         2.2  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 112          6.4         2.7          5.3         1.9  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 127          6.2         2.8          4.8         1.8  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 128          6.1         3.0          4.9         1.8  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 120          6.0         2.2          5.0         1.5  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 150          5.9         3.0          5.1         1.8  virginica\n## 15           5.8         4.0          1.2         0.2     setosa\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 102          5.8         2.7          5.1         1.9  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 16           5.7         4.4          1.5         0.4     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 114          5.7         2.5          5.0         2.0  virginica\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 122          5.6         2.8          4.9         2.0  virginica\n## 34           5.5         4.2          1.4         0.2     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 6            5.4         3.9          1.7         0.4     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 49           5.3         3.7          1.5         0.2     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 1            5.1         3.5          1.4         0.2     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 5            5.0         3.6          1.4         0.2     setosa\n## 8            5.0         3.4          1.5         0.2     setosa\n## 26           5.0         3.0          1.6         0.2     setosa\n## 27           5.0         3.4          1.6         0.4     setosa\n## 36           5.0         3.2          1.2         0.2     setosa\n## 41           5.0         3.5          1.3         0.3     setosa\n## 44           5.0         3.5          1.6         0.6     setosa\n## 50           5.0         3.3          1.4         0.2     setosa\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 2            4.9         3.0          1.4         0.2     setosa\n## 10           4.9         3.1          1.5         0.1     setosa\n## 35           4.9         3.1          1.5         0.2     setosa\n## 38           4.9         3.6          1.4         0.1     setosa\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 107          4.9         2.5          4.5         1.7  virginica\n## 12           4.8         3.4          1.6         0.2     setosa\n## 13           4.8         3.0          1.4         0.1     setosa\n## 25           4.8         3.4          1.9         0.2     setosa\n## 31           4.8         3.1          1.6         0.2     setosa\n## 46           4.8         3.0          1.4         0.3     setosa\n## 3            4.7         3.2          1.3         0.2     setosa\n## 30           4.7         3.2          1.6         0.2     setosa\n## 4            4.6         3.1          1.5         0.2     setosa\n## 7            4.6         3.4          1.4         0.3     setosa\n## 23           4.6         3.6          1.0         0.2     setosa\n## 48           4.6         3.2          1.4         0.2     setosa\n## 42           4.5         2.3          1.3         0.3     setosa\n## 9            4.4         2.9          1.4         0.2     setosa\n## 39           4.4         3.0          1.3         0.2     setosa\n## 43           4.4         3.2          1.3         0.2     setosa\n## 14           4.3         3.0          1.1         0.1     setosa\n\niris.new &lt;- iris[order(iris$Sepal.Length), ]        # 정렬된 데이터를 저장\nhead(iris.new)\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 14          4.3         3.0          1.1         0.1  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 39          4.4         3.0          1.3         0.2  setosa\n## 43          4.4         3.2          1.3         0.2  setosa\n## 42          4.5         2.3          1.3         0.3  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\niris[order(iris$Species,-iris$Petal.Length, decreasing = T), ] # 정렬 기준이 2개\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 107          4.9         2.5          4.5         1.7  virginica\n## 127          6.2         2.8          4.8         1.8  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 122          5.6         2.8          4.9         2.0  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 128          6.1         3.0          4.9         1.8  virginica\n## 114          5.7         2.5          5.0         2.0  virginica\n## 120          6.0         2.2          5.0         1.5  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 101          6.3         3.3          6.0         2.5  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 23           4.6         3.6          1.0         0.2     setosa\n## 14           4.3         3.0          1.1         0.1     setosa\n## 15           5.8         4.0          1.2         0.2     setosa\n## 36           5.0         3.2          1.2         0.2     setosa\n## 3            4.7         3.2          1.3         0.2     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 39           4.4         3.0          1.3         0.2     setosa\n## 41           5.0         3.5          1.3         0.3     setosa\n## 42           4.5         2.3          1.3         0.3     setosa\n## 43           4.4         3.2          1.3         0.2     setosa\n## 1            5.1         3.5          1.4         0.2     setosa\n## 2            4.9         3.0          1.4         0.2     setosa\n## 5            5.0         3.6          1.4         0.2     setosa\n## 7            4.6         3.4          1.4         0.3     setosa\n## 9            4.4         2.9          1.4         0.2     setosa\n## 13           4.8         3.0          1.4         0.1     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 34           5.5         4.2          1.4         0.2     setosa\n## 38           4.9         3.6          1.4         0.1     setosa\n## 46           4.8         3.0          1.4         0.3     setosa\n## 48           4.6         3.2          1.4         0.2     setosa\n## 50           5.0         3.3          1.4         0.2     setosa\n## 4            4.6         3.1          1.5         0.2     setosa\n## 8            5.0         3.4          1.5         0.2     setosa\n## 10           4.9         3.1          1.5         0.1     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 16           5.7         4.4          1.5         0.4     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 35           4.9         3.1          1.5         0.2     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 49           5.3         3.7          1.5         0.2     setosa\n## 12           4.8         3.4          1.6         0.2     setosa\n## 26           5.0         3.0          1.6         0.2     setosa\n## 27           5.0         3.4          1.6         0.4     setosa\n## 30           4.7         3.2          1.6         0.2     setosa\n## 31           4.8         3.1          1.6         0.2     setosa\n## 44           5.0         3.5          1.6         0.6     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 6            5.4         3.9          1.7         0.4     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 25           4.8         3.4          1.9         0.2     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\niris[order(iris$Species, decreasing = T, iris$Petal.Length), ]\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 119          7.7         2.6          6.9         2.3  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 101          6.3         3.3          6.0         2.5  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\n## 114          5.7         2.5          5.0         2.0  virginica\n## 120          6.0         2.2          5.0         1.5  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 122          5.6         2.8          4.9         2.0  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 128          6.1         3.0          4.9         1.8  virginica\n## 127          6.2         2.8          4.8         1.8  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 107          4.9         2.5          4.5         1.7  virginica\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 25           4.8         3.4          1.9         0.2     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\n## 6            5.4         3.9          1.7         0.4     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 12           4.8         3.4          1.6         0.2     setosa\n## 26           5.0         3.0          1.6         0.2     setosa\n## 27           5.0         3.4          1.6         0.4     setosa\n## 30           4.7         3.2          1.6         0.2     setosa\n## 31           4.8         3.1          1.6         0.2     setosa\n## 44           5.0         3.5          1.6         0.6     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 4            4.6         3.1          1.5         0.2     setosa\n## 8            5.0         3.4          1.5         0.2     setosa\n## 10           4.9         3.1          1.5         0.1     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 16           5.7         4.4          1.5         0.4     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 35           4.9         3.1          1.5         0.2     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 49           5.3         3.7          1.5         0.2     setosa\n## 1            5.1         3.5          1.4         0.2     setosa\n## 2            4.9         3.0          1.4         0.2     setosa\n## 5            5.0         3.6          1.4         0.2     setosa\n## 7            4.6         3.4          1.4         0.3     setosa\n## 9            4.4         2.9          1.4         0.2     setosa\n## 13           4.8         3.0          1.4         0.1     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 34           5.5         4.2          1.4         0.2     setosa\n## 38           4.9         3.6          1.4         0.1     setosa\n## 46           4.8         3.0          1.4         0.3     setosa\n## 48           4.6         3.2          1.4         0.2     setosa\n## 50           5.0         3.3          1.4         0.2     setosa\n## 3            4.7         3.2          1.3         0.2     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 39           4.4         3.0          1.3         0.2     setosa\n## 41           5.0         3.5          1.3         0.3     setosa\n## 42           4.5         2.3          1.3         0.3     setosa\n## 43           4.4         3.2          1.3         0.2     setosa\n## 15           5.8         4.0          1.2         0.2     setosa\n## 36           5.0         3.2          1.2         0.2     setosa\n## 14           4.3         3.0          1.1         0.1     setosa\n## 23           4.6         3.6          1.0         0.2     setosa\n\n\nsp &lt;- split(iris, iris$Species) # 품종별로 데이터 분리\nsp                              # 분리 결과 확인\n## $setosa\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1           5.1         3.5          1.4         0.2  setosa\n## 2           4.9         3.0          1.4         0.2  setosa\n## 3           4.7         3.2          1.3         0.2  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\n## 5           5.0         3.6          1.4         0.2  setosa\n## 6           5.4         3.9          1.7         0.4  setosa\n## 7           4.6         3.4          1.4         0.3  setosa\n## 8           5.0         3.4          1.5         0.2  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\n## 11          5.4         3.7          1.5         0.2  setosa\n## 12          4.8         3.4          1.6         0.2  setosa\n## 13          4.8         3.0          1.4         0.1  setosa\n## 14          4.3         3.0          1.1         0.1  setosa\n## 15          5.8         4.0          1.2         0.2  setosa\n## 16          5.7         4.4          1.5         0.4  setosa\n## 17          5.4         3.9          1.3         0.4  setosa\n## 18          5.1         3.5          1.4         0.3  setosa\n## 19          5.7         3.8          1.7         0.3  setosa\n## 20          5.1         3.8          1.5         0.3  setosa\n## 21          5.4         3.4          1.7         0.2  setosa\n## 22          5.1         3.7          1.5         0.4  setosa\n## 23          4.6         3.6          1.0         0.2  setosa\n## 24          5.1         3.3          1.7         0.5  setosa\n## 25          4.8         3.4          1.9         0.2  setosa\n## 26          5.0         3.0          1.6         0.2  setosa\n## 27          5.0         3.4          1.6         0.4  setosa\n## 28          5.2         3.5          1.5         0.2  setosa\n## 29          5.2         3.4          1.4         0.2  setosa\n## 30          4.7         3.2          1.6         0.2  setosa\n## 31          4.8         3.1          1.6         0.2  setosa\n## 32          5.4         3.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\n## 35          4.9         3.1          1.5         0.2  setosa\n## 36          5.0         3.2          1.2         0.2  setosa\n## 37          5.5         3.5          1.3         0.2  setosa\n## 38          4.9         3.6          1.4         0.1  setosa\n## 39          4.4         3.0          1.3         0.2  setosa\n## 40          5.1         3.4          1.5         0.2  setosa\n## 41          5.0         3.5          1.3         0.3  setosa\n## 42          4.5         2.3          1.3         0.3  setosa\n## 43          4.4         3.2          1.3         0.2  setosa\n## 44          5.0         3.5          1.6         0.6  setosa\n## 45          5.1         3.8          1.9         0.4  setosa\n## 46          4.8         3.0          1.4         0.3  setosa\n## 47          5.1         3.8          1.6         0.2  setosa\n## 48          4.6         3.2          1.4         0.2  setosa\n## 49          5.3         3.7          1.5         0.2  setosa\n## 50          5.0         3.3          1.4         0.2  setosa\n## \n## $versicolor\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## \n## $virginica\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n## 101          6.3         3.3          6.0         2.5 virginica\n## 102          5.8         2.7          5.1         1.9 virginica\n## 103          7.1         3.0          5.9         2.1 virginica\n## 104          6.3         2.9          5.6         1.8 virginica\n## 105          6.5         3.0          5.8         2.2 virginica\n## 106          7.6         3.0          6.6         2.1 virginica\n## 107          4.9         2.5          4.5         1.7 virginica\n## 108          7.3         2.9          6.3         1.8 virginica\n## 109          6.7         2.5          5.8         1.8 virginica\n## 110          7.2         3.6          6.1         2.5 virginica\n## 111          6.5         3.2          5.1         2.0 virginica\n## 112          6.4         2.7          5.3         1.9 virginica\n## 113          6.8         3.0          5.5         2.1 virginica\n## 114          5.7         2.5          5.0         2.0 virginica\n## 115          5.8         2.8          5.1         2.4 virginica\n## 116          6.4         3.2          5.3         2.3 virginica\n## 117          6.5         3.0          5.5         1.8 virginica\n## 118          7.7         3.8          6.7         2.2 virginica\n## 119          7.7         2.6          6.9         2.3 virginica\n## 120          6.0         2.2          5.0         1.5 virginica\n## 121          6.9         3.2          5.7         2.3 virginica\n## 122          5.6         2.8          4.9         2.0 virginica\n## 123          7.7         2.8          6.7         2.0 virginica\n## 124          6.3         2.7          4.9         1.8 virginica\n## 125          6.7         3.3          5.7         2.1 virginica\n## 126          7.2         3.2          6.0         1.8 virginica\n## 127          6.2         2.8          4.8         1.8 virginica\n## 128          6.1         3.0          4.9         1.8 virginica\n## 129          6.4         2.8          5.6         2.1 virginica\n## 130          7.2         3.0          5.8         1.6 virginica\n## 131          7.4         2.8          6.1         1.9 virginica\n## 132          7.9         3.8          6.4         2.0 virginica\n## 133          6.4         2.8          5.6         2.2 virginica\n## 134          6.3         2.8          5.1         1.5 virginica\n## 135          6.1         2.6          5.6         1.4 virginica\n## 136          7.7         3.0          6.1         2.3 virginica\n## 137          6.3         3.4          5.6         2.4 virginica\n## 138          6.4         3.1          5.5         1.8 virginica\n## 139          6.0         3.0          4.8         1.8 virginica\n## 140          6.9         3.1          5.4         2.1 virginica\n## 141          6.7         3.1          5.6         2.4 virginica\n## 142          6.9         3.1          5.1         2.3 virginica\n## 143          5.8         2.7          5.1         1.9 virginica\n## 144          6.8         3.2          5.9         2.3 virginica\n## 145          6.7         3.3          5.7         2.5 virginica\n## 146          6.7         3.0          5.2         2.3 virginica\n## 147          6.3         2.5          5.0         1.9 virginica\n## 148          6.5         3.0          5.2         2.0 virginica\n## 149          6.2         3.4          5.4         2.3 virginica\n## 150          5.9         3.0          5.1         1.8 virginica\nsummary(sp)                     # 분리 결과 요약\n##            Length Class      Mode\n## setosa     5      data.frame list\n## versicolor 5      data.frame list\n## virginica  5      data.frame list\nsp$setosa                       # setosa 품종의 데이터 확인\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1           5.1         3.5          1.4         0.2  setosa\n## 2           4.9         3.0          1.4         0.2  setosa\n## 3           4.7         3.2          1.3         0.2  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\n## 5           5.0         3.6          1.4         0.2  setosa\n## 6           5.4         3.9          1.7         0.4  setosa\n## 7           4.6         3.4          1.4         0.3  setosa\n## 8           5.0         3.4          1.5         0.2  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\n## 11          5.4         3.7          1.5         0.2  setosa\n## 12          4.8         3.4          1.6         0.2  setosa\n## 13          4.8         3.0          1.4         0.1  setosa\n## 14          4.3         3.0          1.1         0.1  setosa\n## 15          5.8         4.0          1.2         0.2  setosa\n## 16          5.7         4.4          1.5         0.4  setosa\n## 17          5.4         3.9          1.3         0.4  setosa\n## 18          5.1         3.5          1.4         0.3  setosa\n## 19          5.7         3.8          1.7         0.3  setosa\n## 20          5.1         3.8          1.5         0.3  setosa\n## 21          5.4         3.4          1.7         0.2  setosa\n## 22          5.1         3.7          1.5         0.4  setosa\n## 23          4.6         3.6          1.0         0.2  setosa\n## 24          5.1         3.3          1.7         0.5  setosa\n## 25          4.8         3.4          1.9         0.2  setosa\n## 26          5.0         3.0          1.6         0.2  setosa\n## 27          5.0         3.4          1.6         0.4  setosa\n## 28          5.2         3.5          1.5         0.2  setosa\n## 29          5.2         3.4          1.4         0.2  setosa\n## 30          4.7         3.2          1.6         0.2  setosa\n## 31          4.8         3.1          1.6         0.2  setosa\n## 32          5.4         3.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\n## 35          4.9         3.1          1.5         0.2  setosa\n## 36          5.0         3.2          1.2         0.2  setosa\n## 37          5.5         3.5          1.3         0.2  setosa\n## 38          4.9         3.6          1.4         0.1  setosa\n## 39          4.4         3.0          1.3         0.2  setosa\n## 40          5.1         3.4          1.5         0.2  setosa\n## 41          5.0         3.5          1.3         0.3  setosa\n## 42          4.5         2.3          1.3         0.3  setosa\n## 43          4.4         3.2          1.3         0.2  setosa\n## 44          5.0         3.5          1.6         0.6  setosa\n## 45          5.1         3.8          1.9         0.4  setosa\n## 46          4.8         3.0          1.4         0.3  setosa\n## 47          5.1         3.8          1.6         0.2  setosa\n## 48          4.6         3.2          1.4         0.2  setosa\n## 49          5.3         3.7          1.5         0.2  setosa\n## 50          5.0         3.3          1.4         0.2  setosa\nsetosa &lt;- sp$setosa\n\n\nsubset(iris, Species == \"setosa\")\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1           5.1         3.5          1.4         0.2  setosa\n## 2           4.9         3.0          1.4         0.2  setosa\n## 3           4.7         3.2          1.3         0.2  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\n## 5           5.0         3.6          1.4         0.2  setosa\n## 6           5.4         3.9          1.7         0.4  setosa\n## 7           4.6         3.4          1.4         0.3  setosa\n## 8           5.0         3.4          1.5         0.2  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\n## 11          5.4         3.7          1.5         0.2  setosa\n## 12          4.8         3.4          1.6         0.2  setosa\n## 13          4.8         3.0          1.4         0.1  setosa\n## 14          4.3         3.0          1.1         0.1  setosa\n## 15          5.8         4.0          1.2         0.2  setosa\n## 16          5.7         4.4          1.5         0.4  setosa\n## 17          5.4         3.9          1.3         0.4  setosa\n## 18          5.1         3.5          1.4         0.3  setosa\n## 19          5.7         3.8          1.7         0.3  setosa\n## 20          5.1         3.8          1.5         0.3  setosa\n## 21          5.4         3.4          1.7         0.2  setosa\n## 22          5.1         3.7          1.5         0.4  setosa\n## 23          4.6         3.6          1.0         0.2  setosa\n## 24          5.1         3.3          1.7         0.5  setosa\n## 25          4.8         3.4          1.9         0.2  setosa\n## 26          5.0         3.0          1.6         0.2  setosa\n## 27          5.0         3.4          1.6         0.4  setosa\n## 28          5.2         3.5          1.5         0.2  setosa\n## 29          5.2         3.4          1.4         0.2  setosa\n## 30          4.7         3.2          1.6         0.2  setosa\n## 31          4.8         3.1          1.6         0.2  setosa\n## 32          5.4         3.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\n## 35          4.9         3.1          1.5         0.2  setosa\n## 36          5.0         3.2          1.2         0.2  setosa\n## 37          5.5         3.5          1.3         0.2  setosa\n## 38          4.9         3.6          1.4         0.1  setosa\n## 39          4.4         3.0          1.3         0.2  setosa\n## 40          5.1         3.4          1.5         0.2  setosa\n## 41          5.0         3.5          1.3         0.3  setosa\n## 42          4.5         2.3          1.3         0.3  setosa\n## 43          4.4         3.2          1.3         0.2  setosa\n## 44          5.0         3.5          1.6         0.6  setosa\n## 45          5.1         3.8          1.9         0.4  setosa\n## 46          4.8         3.0          1.4         0.3  setosa\n## 47          5.1         3.8          1.6         0.2  setosa\n## 48          4.6         3.2          1.4         0.2  setosa\n## 49          5.3         3.7          1.5         0.2  setosa\n## 50          5.0         3.3          1.4         0.2  setosa\nsubset(iris, Sepal.Length &gt; 7.5)\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n## 106          7.6         3.0          6.6         2.1 virginica\n## 118          7.7         3.8          6.7         2.2 virginica\n## 119          7.7         2.6          6.9         2.3 virginica\n## 123          7.7         2.8          6.7         2.0 virginica\n## 132          7.9         3.8          6.4         2.0 virginica\n## 136          7.7         3.0          6.1         2.3 virginica\nsubset(iris, Sepal.Length &gt; 5.1 &\n           Sepal.Width &gt; 3.9)\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 15          5.8         4.0          1.2         0.2  setosa\n## 16          5.7         4.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\nsubset(iris, Sepal.Length &gt; 5.1 |\n           Sepal.Width &gt; 3.9)\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 6            5.4         3.9          1.7         0.4     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 15           5.8         4.0          1.2         0.2     setosa\n## 16           5.7         4.4          1.5         0.4     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 34           5.5         4.2          1.4         0.2     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 49           5.3         3.7          1.5         0.2     setosa\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 114          5.7         2.5          5.0         2.0  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 120          6.0         2.2          5.0         1.5  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 122          5.6         2.8          4.9         2.0  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 127          6.2         2.8          4.8         1.8  virginica\n## 128          6.1         3.0          4.9         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\nsubset(iris, Sepal.Length &gt; 7.6,\n       select = c(Petal.Length, Petal.Width))\n##     Petal.Length Petal.Width\n## 118          6.7         2.2\n## 119          6.9         2.3\n## 123          6.7         2.0\n## 132          6.4         2.0\n## 136          6.1         2.3\n\n\nx &lt;- 1:10\nsample(x, size = 5, replace = FALSE) # 비복원추출\n## [1] 7 8 6 2 5\nsample(x, size = 5, replace = TRUE)\n## [1] 1 7 8 9 4\n\nx &lt;- 1:45\nsample(x, size = 6, replace = FALSE)\n## [1] 41 40  4  9 15 33\n\n\nidx &lt;- sample(1:nrow(iris), size = 50,\n              replace = FALSE)\niris.50 &lt;- iris[idx, ]  # 50개의 행 추출\ndim(iris.50)            # 행과 열의 개수 확인\n## [1] 50  5\nhead(iris.50)\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 19           5.7         3.8          1.7         0.3     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 109          6.7         2.5          5.8         1.8  virginica\n## 42           4.5         2.3          1.3         0.3     setosa\n## 121          6.9         3.2          5.7         2.3  virginica\n\n\nsample(1:20, size = 5)\n## [1] 17 14  6 13  2\nsample(1:20, size = 5)\n## [1] 13 17  1 20 15\nsample(1:20, size = 5)\n## [1]  2  7 12 16 17\n\n# 같은 값이 추출되도록 고정시키고 싶다면\n# set.seed() 함수를 이용하여 seed값을 지정해주면 된다.\nset.seed(100)\nsample(1:20, size = 5)\n## [1] 10  6 16 14 12\nset.seed(100)\nsample(1:20, size = 5)\n## [1] 10  6 16 14 12\nset.seed(100)\nsample(1:20, size = 5)\n## [1] 10  6 16 14 12\n\n\ncombn(1:5, 3) # 1~5에서 3개를 뽑는 조합\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n## [1,]    1    1    1    1    1    1    2    2    2     3\n## [2,]    2    2    2    3    3    4    3    3    4     4\n## [3,]    3    4    5    4    5    5    4    5    5     5\n\nx = c(\"red\", \"green\", \"blue\", \"black\", \"white\")\ncom &lt;- combn(x, 2) # x의 원소를 2개씩 뽑는 조합\ncom\n##      [,1]    [,2]   [,3]    [,4]    [,5]    [,6]    [,7]    [,8]    [,9]   \n## [1,] \"red\"   \"red\"  \"red\"   \"red\"   \"green\" \"green\" \"green\" \"blue\"  \"blue\" \n## [2,] \"green\" \"blue\" \"black\" \"white\" \"blue\"  \"black\" \"white\" \"black\" \"white\"\n##      [,10]  \n## [1,] \"black\"\n## [2,] \"white\"\n\nfor (i in 1:ncol(com)) {\n    # 조합을 출력\n    cat(com[, i], \"\\n\")\n}\n## red green \n## red blue \n## red black \n## red white \n## green blue \n## green black \n## green white \n## blue black \n## blue white \n## black white\n\n\n# aggregate(data, by = '기준이 되는 컬럼', FUN)\nagg &lt;- aggregate(iris[, -5], by = list(iris$Species), FUN = mean)\nagg\n##      Group.1 Sepal.Length Sepal.Width Petal.Length Petal.Width\n## 1     setosa        5.006       3.428        1.462       0.246\n## 2 versicolor        5.936       2.770        4.260       1.326\n## 3  virginica        6.588       2.974        5.552       2.026\n\n\n# aggregate는 데이터의 특정 컬럼을 기준으로 통계량을 구해주는 함수\nagg &lt;- aggregate(iris[, -5], by = list(표준편차 = iris$Species), FUN = sd)\nagg\n##     표준편차 Sepal.Length Sepal.Width Petal.Length Petal.Width\n## 1     setosa    0.3524897   0.3790644    0.1736640   0.1053856\n## 2 versicolor    0.5161711   0.3137983    0.4699110   0.1977527\n## 3  virginica    0.6358796   0.3224966    0.5518947   0.2746501\n\n\nhead(mtcars)\n##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\nagg &lt;- aggregate(mtcars, by = list(cyl = mtcars$cyl, vs = mtcars$vs), FUN = max)\nagg\n##   cyl vs  mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## 1   4  0 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n## 2   6  0 21.0   6 160.0 175 3.90 2.875 17.02  0  1    5    6\n## 3   8  0 19.2   8 472.0 335 4.22 5.424 18.00  0  1    5    8\n## 4   4  1 33.9   4 146.7 113 4.93 3.190 22.90  1  1    5    2\n## 5   6  1 21.4   6 258.0 123 3.92 3.460 20.22  1  0    4    4\n\nagg &lt;- aggregate(mtcars, by = list(cyl = mtcars$cyl, vs = mtcars$vs), FUN = mean)\nagg\n##   cyl vs      mpg cyl   disp       hp     drat       wt     qsec vs        am\n## 1   4  0 26.00000   4 120.30  91.0000 4.430000 2.140000 16.70000  0 1.0000000\n## 2   6  0 20.56667   6 155.00 131.6667 3.806667 2.755000 16.32667  0 1.0000000\n## 3   8  0 15.10000   8 353.10 209.2143 3.229286 3.999214 16.77214  0 0.1428571\n## 4   4  1 26.73000   4 103.62  81.8000 4.035000 2.300300 19.38100  1 0.7000000\n## 5   6  1 19.12500   6 204.55 115.2500 3.420000 3.388750 19.21500  1 0.0000000\n##       gear     carb\n## 1 5.000000 2.000000\n## 2 4.333333 4.666667\n## 3 3.285714 3.500000\n## 4 4.000000 1.500000\n## 5 3.500000 2.500000\n\n\nx &lt;- data.frame(name = c(\"a\", \"b\", \"c\"), math = c(90, 80, 40))\ny &lt;- data.frame(name = c(\"a\", \"b\", \"d\"), korean = c(75, 60, 90))\nx ; y\n##   name math\n## 1    a   90\n## 2    b   80\n## 3    c   40\n##   name korean\n## 1    a     75\n## 2    b     60\n## 3    d     90\n\n\nz &lt;- merge(x, y, by = c(\"name\"))\nz\n##   name math korean\n## 1    a   90     75\n## 2    b   80     60\n\n\nmerge(x, y, all.x = T)  # 첫 번째 데이터셋의 행들은 모두 표시되도록\n##   name math korean\n## 1    a   90     75\n## 2    b   80     60\n## 3    c   40     NA\nmerge(x, y, all.y = T)  # 두 번째 데이터셋의 행들은 모두 표시되도록\n##   name math korean\n## 1    a   90     75\n## 2    b   80     60\n## 3    d   NA     90\nmerge(x, y, all = T)    # 두 데이터셋의 모든 행들이 표시되도록\n##   name math korean\n## 1    a   90     75\n## 2    b   80     60\n## 3    c   40     NA\n## 4    d   NA     90\n\n\nx &lt;- data.frame(name = c(\"a\", \"b\", \"c\"), math = c(90, 80, 40))\ny &lt;- data.frame(sname = c(\"a\", \"b\", \"d\"), korean = c(75, 60, 90))\nx # 병합 기준 열의 이름이 name\n##   name math\n## 1    a   90\n## 2    b   80\n## 3    c   40\ny # 병합 기준 열의 이름이 sname\n##   sname korean\n## 1     a     75\n## 2     b     60\n## 3     d     90\nmerge(x, y, by.x = c(\"name\"), by.y = c(\"sname\"))\n##   name math korean\n## 1    a   90     75\n## 2    b   80     60"
  },
  {
    "objectID": "R_Basic.html#장.-변수와-벡터",
    "href": "R_Basic.html#장.-변수와-벡터",
    "title": "R Basic",
    "section": "",
    "text": "2 + 3  # 2 더하기 3\n## [1] 5\n(3 + 6) * 8\n## [1] 72\n2 ^ 3  # 2의 세제곱\n## [1] 8\n8 %% 3\n## [1] 2\n\n\n7 + 4\n## [1] 11\n\n\nlog(10) + 5 # 로그함수\n## [1] 7.302585\nsqrt(25) # 제곱근\n## [1] 5\nmax(5, 3, 2) # 가장 큰 값\n## [1] 5\n\n\na &lt;- 10\nb &lt;- 20\nc &lt;- a+b\nprint(c)\n## [1] 30\n\n\na &lt;- 125\na\n## [1] 125\nprint(a)\n## [1] 125\n\n\na &lt;- 10 # a에 숫자 저장\nb &lt;- 20\na + b # a+b의 결과 출력\n## [1] 30\na &lt;- \"A\" # a에 문자 저장\na + b # a+b의 결과 출력. 에러 발생\n## Error in a + b: non-numeric argument to binary operator\n\n\nx &lt;- c(1, 2, 3) # 숫자형 벡터\ny &lt;- c(\"a\", \"b\", \"c\") # 문자형 벡터\nz &lt;- c(TRUE, TRUE, FALSE, TRUE) # 논리형 벡터\nx ; y ;z\n## [1] 1 2 3\n## [1] \"a\" \"b\" \"c\"\n## [1]  TRUE  TRUE FALSE  TRUE\n\n\nw &lt;- c(1, 2, 3, \"a\", \"b\", \"c\")\nw\n## [1] \"1\" \"2\" \"3\" \"a\" \"b\" \"c\"\n\n\nv1 &lt;- 50:90\nv1\n##  [1] 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74\n## [26] 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90\nv2 &lt;- c(1, 2, 5, 50:90)\nv2\n##  [1]  1  2  5 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n## [26] 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90\n\n\nv3 &lt;- seq(1, 101, 3)\nv3\n##  [1]   1   4   7  10  13  16  19  22  25  28  31  34  37  40  43  46  49  52  55\n## [20]  58  61  64  67  70  73  76  79  82  85  88  91  94  97 100\nv4 &lt;- seq(0.1, 1.0, 0.1)\nv4\n##  [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n\n\nv5 &lt;- rep(1, times = 5) # 1을 5번 반복\nv5\n## [1] 1 1 1 1 1\nv6 &lt;- rep(1:5, times = 3) # 1에서 5까지 3번 반복\nv6\n##  [1] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5\nv7 &lt;- rep(c(1, 5, 9), times = 3) # 1, 5, 9를 3번 반복\nv7\n## [1] 1 5 9 1 5 9 1 5 9\nv8 &lt;- rep(1:5, each = 3) # 1에서 5를 각각 3번 반복\nv8\n##  [1] 1 1 1 2 2 2 3 3 3 4 4 4 5 5 5\n\nrep(1:3, each = 3, times = 3)\n##  [1] 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3\nrep(1:3, times = 3, each = 3)\n##  [1] 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3\n\n\nscore &lt;- c(90, 85, 70) # 성적\nscore\n## [1] 90 85 70\nnames(score) # score에 저장된 값들의 이름을 보이시오\n## NULL\nnames(score) &lt;- c(\"John\", \"Tom\", \"Jane\") # 값들에 이름을 부여\nnames(score) # score에 저장된 값들의 이름을 보이시오\n## [1] \"John\" \"Tom\"  \"Jane\"\nscore # 이름과 함께 값이 출력\n## John  Tom Jane \n##   90   85   70\n\n\nd &lt;- c(1, 4, 3, 7, 8)\nd[1]\n## [1] 1\nd[2]\n## [1] 4\nd[3]\n## [1] 3\nd[4]\n## [1] 7\nd[5]\n## [1] 8\nd[6]\n## [1] NA\nd[c(2, 4)]\n## [1] 4 7\n\n\nd &lt;- c(1, 4, 3, 7, 8)\nd[c(1, 3, 5)] # 1, 3, 5번째 값 출력\n## [1] 1 3 8\nd[1:3] # 처음 세 개의 값 출력\n## [1] 1 4 3\nd[seq(1, 5, 2)] # 홀수 번째 값 출력\n## [1] 1 3 8\nd[-2] # 2번째 값 제외하고 출력\n## [1] 1 3 7 8\nd[-c(3:5)] # 3~5번째 값은 제외하고 출력\n## [1] 1 4\n\n\nGNP &lt;- c(2000, 2450, 960)\nGNP\n## [1] 2000 2450  960\nnames(GNP) &lt;- c(\"Korea\", \"Japan\", \"Nepal\")\nGNP\n## Korea Japan Nepal \n##  2000  2450   960\nGNP[1]\n## Korea \n##  2000\nGNP[\"Korea\"]\n## Korea \n##  2000\nGNP_NEW &lt;- GNP[c(\"Korea\", \"Nepal\")]\nGNP_NEW\n## Korea Nepal \n##  2000   960\n\n\nv1 &lt;- c(1, 5, 7, 8, 9)\nv1\n## [1] 1 5 7 8 9\nv1[2] &lt;- 3 # v1의 2번째 값을 3으로 변경\nv1\n## [1] 1 3 7 8 9\nv1[c(1, 5)] &lt;- c(10, 20) # v1의 1, 5번째 값을 각각 10, 20으로 변경\nv1\n## [1] 10  3  7  8 20\n\n\nd &lt;- c(1, 4, 3, 7, 8)\n2 * d\n## [1]  2  8  6 14 16\nd - 5\n## [1] -4 -1 -2  2  3\n3 * d + 4\n## [1]  7 16 13 25 28\n\n\nx &lt;- c(1, 2, 3)\ny &lt;- c(4, 5, 6)\nx + y # 대응하는 원소끼리 더하여 출력\n## [1] 5 7 9\nx * y # 대응하는 원소끼리 곱하여 출력\n## [1]  4 10 18\nz &lt;- x + y # x, y를 더하여 z에 저장\nz\n## [1] 5 7 9\n\n\nd &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\nsum(d) # d에 포함된 값들의 합\n## [1] 55\nsum(2 * d) # d에 포함된 값들에 2를 곱한 후 합한 값\n## [1] 110\nlength(d) # d에 포함된 값들의 개수\n## [1] 10\nmean(d[1:5]) # 1~5번째 값들의 평균\n## [1] 3\nmax(d) # d에 포함된 값들의 최댓값\n## [1] 10\nmin(d) # d에 포함된 값들의 최솟값\n## [1] 1\nsort(d) # 오름차순 정렬\n##  [1]  1  2  3  4  5  6  7  8  9 10\nsort(d, decreasing = FALSE) # 오름차순 정렬\n##  [1]  1  2  3  4  5  6  7  8  9 10\nsort(d, decreasing = TRUE) # 내림차순 정렬\n##  [1] 10  9  8  7  6  5  4  3  2  1\nsort(d, TRUE) # 내림차순 정렬\n##  [1] 10  9  8  7  6  5  4  3  2  1\n\nv1 &lt;- median(d)\nv1\n## [1] 5.5\nv2 &lt;- sum(d) / length(d)\nv2\n## [1] 5.5\nmean(d)\n## [1] 5.5\n\n\nd &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9)\nd &gt;= 5\n## [1] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\nd[d &gt; 5] # 5보다 큰 값\n## [1] 6 7 8 9\nsum(d &gt; 5) # 5보다 큰 값의 개수를 출력\n## [1] 4\nsum(d[d &gt; 5]) # 5보다 큰 값의 합계를 출력\n## [1] 30\nd == 5\n## [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n\ncondi &lt;- d &gt; 5 & d &lt; 8 # 조건을 변수에 저장\nd[condi] # 조건에 맞는 값들을 선택\n## [1] 6 7\nd[d &gt; 5 & d &lt; 8]\n## [1] 6 7\n\n\nds &lt;- c(90, 85, 70, 84)\nmy.info &lt;- list(name = 'Tom', age = 60, status = TRUE, score = ds)\nmy.info # 리스트에 저장된 내용을 모두 출력\n## $name\n## [1] \"Tom\"\n## \n## $age\n## [1] 60\n## \n## $status\n## [1] TRUE\n## \n## $score\n## [1] 90 85 70 84\nmy.info[1] # 이름이랑 내용 다 출력\n## $name\n## [1] \"Tom\"\nmy.info[[1]] # 리스트의 첫 번째 값을 출력\n## [1] \"Tom\"\nmy.info$name # 리스트에서 값의 이름이 name인 값을 출력\n## [1] \"Tom\"\nmy.info[[4]] # 리스트의 네 번째 값을 출력\n## [1] 90 85 70 84\n\n\nbt &lt;- c('A', 'B', 'B', 'O', 'AB', 'A') # 문자형 벡터 bt 정의\nbt.new &lt;- factor(bt) # 팩터 bt.new 정의\nbt # 벡터 bt의 내용 출력\n## [1] \"A\"  \"B\"  \"B\"  \"O\"  \"AB\" \"A\"\nbt.new # 팩터 bt.new의 내용 출력\n## [1] A  B  B  O  AB A \n## Levels: A AB B O\nbt[5] # 벡터 bt의 5번째 값 출력\n## [1] \"AB\"\nbt.new[5] # 팩터 bt.new의 5번째 값 출력\n## [1] AB\n## Levels: A AB B O\nlevels(bt.new) # 팩터에 저장된 값의 종류를 출력\n## [1] \"A\"  \"AB\" \"B\"  \"O\"\nas.integer(bt.new) # 팩터의 문자값을 숫자로 바꾸어 출력\n## [1] 1 3 3 4 2 1\nbt.new[7] &lt;- 'B' # 팩터 bt.new의 7번째에 'B' 저장\nbt.new[8] &lt;- 'C' # 팩터 bt.new의 8번째에 'C' 저장\n## Warning in `[&lt;-.factor`(`*tmp*`, 8, value = \"C\"): invalid factor level, NA\n## generated\nbt.new # 팩터 bt.new의 내용 출력\n## [1] A    B    B    O    AB   A    B    &lt;NA&gt;\n## Levels: A AB B O"
  },
  {
    "objectID": "R_Basic.html#장.-매트릭스와-데이터프레임",
    "href": "R_Basic.html#장.-매트릭스와-데이터프레임",
    "title": "R Basic",
    "section": "",
    "text": "z &lt;- matrix(1:20, nrow = 4, ncol = 5)\nz # 매트릭스 z의 내용을 출력\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\n\n\nz2 &lt;- matrix(1:20, nrow = 4, ncol = 5, byrow = T)\nz2 # 매트릭스 z2의 내용을 출력\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    2    3    4    5\n## [2,]    6    7    8    9   10\n## [3,]   11   12   13   14   15\n## [4,]   16   17   18   19   20\n\nz &lt;- matrix(1:16, nrow = 4, ncol = 5)\n## Warning in matrix(1:16, nrow = 4, ncol = 5): data length [16] is not a\n## sub-multiple or multiple of the number of columns [5]\nz\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13    1\n## [2,]    2    6   10   14    2\n## [3,]    3    7   11   15    3\n## [4,]    4    8   12   16    4\n\n\nx &lt;- 1:4 # 벡터 x 생성\ny &lt;- 5:8 # 벡터 y 생성\nz &lt;- matrix(1:20, nrow = 4, ncol = 5) # 매트릭스 z 생성\n\nm1 &lt;- cbind(x, y) # x와 y를 열 방향으로 결합하여 매트릭스 생성\nm1 # 매트릭스 m1의 내용을 출력\n##      x y\n## [1,] 1 5\n## [2,] 2 6\n## [3,] 3 7\n## [4,] 4 8\nm2 &lt;- rbind(x, y) # x와 y를 행 방향으로 결합하여 매트릭스 생성\nm2 # 매트릭스 m2의 내용을 출력\n##   [,1] [,2] [,3] [,4]\n## x    1    2    3    4\n## y    5    6    7    8\nm3 &lt;- rbind(m2, x) # m2와 벡터 x를 행 방향으로 결합\nm3 # 매트릭스 m3의 내용을 출력\n##   [,1] [,2] [,3] [,4]\n## x    1    2    3    4\n## y    5    6    7    8\n## x    1    2    3    4\nm4 &lt;- cbind(z, x) # 매트릭스 z와 벡터 x를 열 방향으로 결합\nm4 # 매트릭스 m4의 내용을 출력\n##                   x\n## [1,] 1 5  9 13 17 1\n## [2,] 2 6 10 14 18 2\n## [3,] 3 7 11 15 19 3\n## [4,] 4 8 12 16 20 4\n\nx &lt;- 1:5\nm5 &lt;- cbind(z, x)\n## Warning in cbind(z, x): number of rows of result is not a multiple of vector\n## length (arg 2)\nm5\n##                   x\n## [1,] 1 5  9 13 17 1\n## [2,] 2 6 10 14 18 2\n## [3,] 3 7 11 15 19 3\n## [4,] 4 8 12 16 20 4\n\n\nz &lt;- matrix(1:20, nrow = 4, ncol = 5) # 매트릭스 z 생성\nz # 매트릭스 z의 내용 출력\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\n\nz[2, 3] # 2행 3열에 있는 값\n## [1] 10\nz[1, 4] # 1행 4열에 있는 값\n## [1] 13\nz[2, ] # 2행에 있는 모든 값\n## [1]  2  6 10 14 18\nz[, 4] # 4열에 있는 모든 값\n## [1] 13 14 15 16\nz[, ]\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\nz\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\n\n\nz &lt;- matrix(1:20, nrow = 4, ncol = 5) # 매트릭스 z 생성\nz # 매트릭스 z의 내용 출력\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\n\nz[2, 1:3] # 2행의 값 중 1~3열에 있는 값\n## [1]  2  6 10\nz[1, c(1, 2, 4)] # 1행의 값 중 1, 2, 4열에 있는 값\n## [1]  1  5 13\nz[1:2, ] # 1, 2행에 있는 모든 값\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\nz[, c(1, 4)] # 1, 4열에 있는 모든 값\n##      [,1] [,2]\n## [1,]    1   13\n## [2,]    2   14\n## [3,]    3   15\n## [4,]    4   16\n\n\nscore &lt;- matrix(c(90, 85, 69, 78,\n                  85, 96, 49, 95,\n                  90, 80, 70, 60),\n                nrow = 4,\n                ncol = 3)\nscore\n##      [,1] [,2] [,3]\n## [1,]   90   85   90\n## [2,]   85   96   80\n## [3,]   69   49   70\n## [4,]   78   95   60\nrownames(score) &lt;- c('John', 'Tom', 'Mark', 'Jane')\ncolnames(score) &lt;- c('English', 'Math', 'Science')\nscore\n##      English Math Science\n## John      90   85      90\n## Tom       85   96      80\n## Mark      69   49      70\n## Jane      78   95      60\n\n\nscore['John', 'Math'] # John의 수학 성적\n## [1] 85\nscore['Tom', c('Math', 'Science')] # Tom의 수학, 과학 성적\n##    Math Science \n##      96      80\nscore['Mark', ] # Mark의 모든 과목 성적\n## English    Math Science \n##      69      49      70\nscore[, 'English'] # 모든 학생의 영어 성적\n## John  Tom Mark Jane \n##   90   85   69   78\nrownames(score) # score의 행의 이름\n## [1] \"John\" \"Tom\"  \"Mark\" \"Jane\"\ncolnames(score) # score의 열의 이름\n## [1] \"English\" \"Math\"    \"Science\"\ncolnames(score)[2] # score의 열의 이름 중 두 번째 값\n## [1] \"Math\"\n\n\ncity &lt;- c(\"Seoul\", \"Tokyo\", \"Washington\") # 문자로 이루어진 벡터\nrank &lt;- c(1, 3, 2) # 숫자로 이루어진 벡터\ncity.info &lt;- data.frame(city, rank) # 데이터프레임 생성\ncity.info # city.info의 내용 출력\n##         city rank\n## 1      Seoul    1\n## 2      Tokyo    3\n## 3 Washington    2\n\n\n# iris\niris[, c(1:2)] # 1, 2열의 모든 데이터\n##     Sepal.Length Sepal.Width\n## 1            5.1         3.5\n## 2            4.9         3.0\n## 3            4.7         3.2\n## 4            4.6         3.1\n## 5            5.0         3.6\n## 6            5.4         3.9\n## 7            4.6         3.4\n## 8            5.0         3.4\n## 9            4.4         2.9\n## 10           4.9         3.1\n## 11           5.4         3.7\n## 12           4.8         3.4\n## 13           4.8         3.0\n## 14           4.3         3.0\n## 15           5.8         4.0\n## 16           5.7         4.4\n## 17           5.4         3.9\n## 18           5.1         3.5\n## 19           5.7         3.8\n## 20           5.1         3.8\n## 21           5.4         3.4\n## 22           5.1         3.7\n## 23           4.6         3.6\n## 24           5.1         3.3\n## 25           4.8         3.4\n## 26           5.0         3.0\n## 27           5.0         3.4\n## 28           5.2         3.5\n## 29           5.2         3.4\n## 30           4.7         3.2\n## 31           4.8         3.1\n## 32           5.4         3.4\n## 33           5.2         4.1\n## 34           5.5         4.2\n## 35           4.9         3.1\n## 36           5.0         3.2\n## 37           5.5         3.5\n## 38           4.9         3.6\n## 39           4.4         3.0\n## 40           5.1         3.4\n## 41           5.0         3.5\n## 42           4.5         2.3\n## 43           4.4         3.2\n## 44           5.0         3.5\n## 45           5.1         3.8\n## 46           4.8         3.0\n## 47           5.1         3.8\n## 48           4.6         3.2\n## 49           5.3         3.7\n## 50           5.0         3.3\n## 51           7.0         3.2\n## 52           6.4         3.2\n## 53           6.9         3.1\n## 54           5.5         2.3\n## 55           6.5         2.8\n## 56           5.7         2.8\n## 57           6.3         3.3\n## 58           4.9         2.4\n## 59           6.6         2.9\n## 60           5.2         2.7\n## 61           5.0         2.0\n## 62           5.9         3.0\n## 63           6.0         2.2\n## 64           6.1         2.9\n## 65           5.6         2.9\n## 66           6.7         3.1\n## 67           5.6         3.0\n## 68           5.8         2.7\n## 69           6.2         2.2\n## 70           5.6         2.5\n## 71           5.9         3.2\n## 72           6.1         2.8\n## 73           6.3         2.5\n## 74           6.1         2.8\n## 75           6.4         2.9\n## 76           6.6         3.0\n## 77           6.8         2.8\n## 78           6.7         3.0\n## 79           6.0         2.9\n## 80           5.7         2.6\n## 81           5.5         2.4\n## 82           5.5         2.4\n## 83           5.8         2.7\n## 84           6.0         2.7\n## 85           5.4         3.0\n## 86           6.0         3.4\n## 87           6.7         3.1\n## 88           6.3         2.3\n## 89           5.6         3.0\n## 90           5.5         2.5\n## 91           5.5         2.6\n## 92           6.1         3.0\n## 93           5.8         2.6\n## 94           5.0         2.3\n## 95           5.6         2.7\n## 96           5.7         3.0\n## 97           5.7         2.9\n## 98           6.2         2.9\n## 99           5.1         2.5\n## 100          5.7         2.8\n## 101          6.3         3.3\n## 102          5.8         2.7\n## 103          7.1         3.0\n## 104          6.3         2.9\n## 105          6.5         3.0\n## 106          7.6         3.0\n## 107          4.9         2.5\n## 108          7.3         2.9\n## 109          6.7         2.5\n## 110          7.2         3.6\n## 111          6.5         3.2\n## 112          6.4         2.7\n## 113          6.8         3.0\n## 114          5.7         2.5\n## 115          5.8         2.8\n## 116          6.4         3.2\n## 117          6.5         3.0\n## 118          7.7         3.8\n## 119          7.7         2.6\n## 120          6.0         2.2\n## 121          6.9         3.2\n## 122          5.6         2.8\n## 123          7.7         2.8\n## 124          6.3         2.7\n## 125          6.7         3.3\n## 126          7.2         3.2\n## 127          6.2         2.8\n## 128          6.1         3.0\n## 129          6.4         2.8\n## 130          7.2         3.0\n## 131          7.4         2.8\n## 132          7.9         3.8\n## 133          6.4         2.8\n## 134          6.3         2.8\n## 135          6.1         2.6\n## 136          7.7         3.0\n## 137          6.3         3.4\n## 138          6.4         3.1\n## 139          6.0         3.0\n## 140          6.9         3.1\n## 141          6.7         3.1\n## 142          6.9         3.1\n## 143          5.8         2.7\n## 144          6.8         3.2\n## 145          6.7         3.3\n## 146          6.7         3.0\n## 147          6.3         2.5\n## 148          6.5         3.0\n## 149          6.2         3.4\n## 150          5.9         3.0\niris[, c(1, 3, 5)] # 1, 3, 5열의 모든 데이터\n##     Sepal.Length Petal.Length    Species\n## 1            5.1          1.4     setosa\n## 2            4.9          1.4     setosa\n## 3            4.7          1.3     setosa\n## 4            4.6          1.5     setosa\n## 5            5.0          1.4     setosa\n## 6            5.4          1.7     setosa\n## 7            4.6          1.4     setosa\n## 8            5.0          1.5     setosa\n## 9            4.4          1.4     setosa\n## 10           4.9          1.5     setosa\n## 11           5.4          1.5     setosa\n## 12           4.8          1.6     setosa\n## 13           4.8          1.4     setosa\n## 14           4.3          1.1     setosa\n## 15           5.8          1.2     setosa\n## 16           5.7          1.5     setosa\n## 17           5.4          1.3     setosa\n## 18           5.1          1.4     setosa\n## 19           5.7          1.7     setosa\n## 20           5.1          1.5     setosa\n## 21           5.4          1.7     setosa\n## 22           5.1          1.5     setosa\n## 23           4.6          1.0     setosa\n## 24           5.1          1.7     setosa\n## 25           4.8          1.9     setosa\n## 26           5.0          1.6     setosa\n## 27           5.0          1.6     setosa\n## 28           5.2          1.5     setosa\n## 29           5.2          1.4     setosa\n## 30           4.7          1.6     setosa\n## 31           4.8          1.6     setosa\n## 32           5.4          1.5     setosa\n## 33           5.2          1.5     setosa\n## 34           5.5          1.4     setosa\n## 35           4.9          1.5     setosa\n## 36           5.0          1.2     setosa\n## 37           5.5          1.3     setosa\n## 38           4.9          1.4     setosa\n## 39           4.4          1.3     setosa\n## 40           5.1          1.5     setosa\n## 41           5.0          1.3     setosa\n## 42           4.5          1.3     setosa\n## 43           4.4          1.3     setosa\n## 44           5.0          1.6     setosa\n## 45           5.1          1.9     setosa\n## 46           4.8          1.4     setosa\n## 47           5.1          1.6     setosa\n## 48           4.6          1.4     setosa\n## 49           5.3          1.5     setosa\n## 50           5.0          1.4     setosa\n## 51           7.0          4.7 versicolor\n## 52           6.4          4.5 versicolor\n## 53           6.9          4.9 versicolor\n## 54           5.5          4.0 versicolor\n## 55           6.5          4.6 versicolor\n## 56           5.7          4.5 versicolor\n## 57           6.3          4.7 versicolor\n## 58           4.9          3.3 versicolor\n## 59           6.6          4.6 versicolor\n## 60           5.2          3.9 versicolor\n## 61           5.0          3.5 versicolor\n## 62           5.9          4.2 versicolor\n## 63           6.0          4.0 versicolor\n## 64           6.1          4.7 versicolor\n## 65           5.6          3.6 versicolor\n## 66           6.7          4.4 versicolor\n## 67           5.6          4.5 versicolor\n## 68           5.8          4.1 versicolor\n## 69           6.2          4.5 versicolor\n## 70           5.6          3.9 versicolor\n## 71           5.9          4.8 versicolor\n## 72           6.1          4.0 versicolor\n## 73           6.3          4.9 versicolor\n## 74           6.1          4.7 versicolor\n## 75           6.4          4.3 versicolor\n## 76           6.6          4.4 versicolor\n## 77           6.8          4.8 versicolor\n## 78           6.7          5.0 versicolor\n## 79           6.0          4.5 versicolor\n## 80           5.7          3.5 versicolor\n## 81           5.5          3.8 versicolor\n## 82           5.5          3.7 versicolor\n## 83           5.8          3.9 versicolor\n## 84           6.0          5.1 versicolor\n## 85           5.4          4.5 versicolor\n## 86           6.0          4.5 versicolor\n## 87           6.7          4.7 versicolor\n## 88           6.3          4.4 versicolor\n## 89           5.6          4.1 versicolor\n## 90           5.5          4.0 versicolor\n## 91           5.5          4.4 versicolor\n## 92           6.1          4.6 versicolor\n## 93           5.8          4.0 versicolor\n## 94           5.0          3.3 versicolor\n## 95           5.6          4.2 versicolor\n## 96           5.7          4.2 versicolor\n## 97           5.7          4.2 versicolor\n## 98           6.2          4.3 versicolor\n## 99           5.1          3.0 versicolor\n## 100          5.7          4.1 versicolor\n## 101          6.3          6.0  virginica\n## 102          5.8          5.1  virginica\n## 103          7.1          5.9  virginica\n## 104          6.3          5.6  virginica\n## 105          6.5          5.8  virginica\n## 106          7.6          6.6  virginica\n## 107          4.9          4.5  virginica\n## 108          7.3          6.3  virginica\n## 109          6.7          5.8  virginica\n## 110          7.2          6.1  virginica\n## 111          6.5          5.1  virginica\n## 112          6.4          5.3  virginica\n## 113          6.8          5.5  virginica\n## 114          5.7          5.0  virginica\n## 115          5.8          5.1  virginica\n## 116          6.4          5.3  virginica\n## 117          6.5          5.5  virginica\n## 118          7.7          6.7  virginica\n## 119          7.7          6.9  virginica\n## 120          6.0          5.0  virginica\n## 121          6.9          5.7  virginica\n## 122          5.6          4.9  virginica\n## 123          7.7          6.7  virginica\n## 124          6.3          4.9  virginica\n## 125          6.7          5.7  virginica\n## 126          7.2          6.0  virginica\n## 127          6.2          4.8  virginica\n## 128          6.1          4.9  virginica\n## 129          6.4          5.6  virginica\n## 130          7.2          5.8  virginica\n## 131          7.4          6.1  virginica\n## 132          7.9          6.4  virginica\n## 133          6.4          5.6  virginica\n## 134          6.3          5.1  virginica\n## 135          6.1          5.6  virginica\n## 136          7.7          6.1  virginica\n## 137          6.3          5.6  virginica\n## 138          6.4          5.5  virginica\n## 139          6.0          4.8  virginica\n## 140          6.9          5.4  virginica\n## 141          6.7          5.6  virginica\n## 142          6.9          5.1  virginica\n## 143          5.8          5.1  virginica\n## 144          6.8          5.9  virginica\n## 145          6.7          5.7  virginica\n## 146          6.7          5.2  virginica\n## 147          6.3          5.0  virginica\n## 148          6.5          5.2  virginica\n## 149          6.2          5.4  virginica\n## 150          5.9          5.1  virginica\niris[, c(\"Sepal.Length\", \"Species\")] # 1, 5열의 모든 데이터\n##     Sepal.Length    Species\n## 1            5.1     setosa\n## 2            4.9     setosa\n## 3            4.7     setosa\n## 4            4.6     setosa\n## 5            5.0     setosa\n## 6            5.4     setosa\n## 7            4.6     setosa\n## 8            5.0     setosa\n## 9            4.4     setosa\n## 10           4.9     setosa\n## 11           5.4     setosa\n## 12           4.8     setosa\n## 13           4.8     setosa\n## 14           4.3     setosa\n## 15           5.8     setosa\n## 16           5.7     setosa\n## 17           5.4     setosa\n## 18           5.1     setosa\n## 19           5.7     setosa\n## 20           5.1     setosa\n## 21           5.4     setosa\n## 22           5.1     setosa\n## 23           4.6     setosa\n## 24           5.1     setosa\n## 25           4.8     setosa\n## 26           5.0     setosa\n## 27           5.0     setosa\n## 28           5.2     setosa\n## 29           5.2     setosa\n## 30           4.7     setosa\n## 31           4.8     setosa\n## 32           5.4     setosa\n## 33           5.2     setosa\n## 34           5.5     setosa\n## 35           4.9     setosa\n## 36           5.0     setosa\n## 37           5.5     setosa\n## 38           4.9     setosa\n## 39           4.4     setosa\n## 40           5.1     setosa\n## 41           5.0     setosa\n## 42           4.5     setosa\n## 43           4.4     setosa\n## 44           5.0     setosa\n## 45           5.1     setosa\n## 46           4.8     setosa\n## 47           5.1     setosa\n## 48           4.6     setosa\n## 49           5.3     setosa\n## 50           5.0     setosa\n## 51           7.0 versicolor\n## 52           6.4 versicolor\n## 53           6.9 versicolor\n## 54           5.5 versicolor\n## 55           6.5 versicolor\n## 56           5.7 versicolor\n## 57           6.3 versicolor\n## 58           4.9 versicolor\n## 59           6.6 versicolor\n## 60           5.2 versicolor\n## 61           5.0 versicolor\n## 62           5.9 versicolor\n## 63           6.0 versicolor\n## 64           6.1 versicolor\n## 65           5.6 versicolor\n## 66           6.7 versicolor\n## 67           5.6 versicolor\n## 68           5.8 versicolor\n## 69           6.2 versicolor\n## 70           5.6 versicolor\n## 71           5.9 versicolor\n## 72           6.1 versicolor\n## 73           6.3 versicolor\n## 74           6.1 versicolor\n## 75           6.4 versicolor\n## 76           6.6 versicolor\n## 77           6.8 versicolor\n## 78           6.7 versicolor\n## 79           6.0 versicolor\n## 80           5.7 versicolor\n## 81           5.5 versicolor\n## 82           5.5 versicolor\n## 83           5.8 versicolor\n## 84           6.0 versicolor\n## 85           5.4 versicolor\n## 86           6.0 versicolor\n## 87           6.7 versicolor\n## 88           6.3 versicolor\n## 89           5.6 versicolor\n## 90           5.5 versicolor\n## 91           5.5 versicolor\n## 92           6.1 versicolor\n## 93           5.8 versicolor\n## 94           5.0 versicolor\n## 95           5.6 versicolor\n## 96           5.7 versicolor\n## 97           5.7 versicolor\n## 98           6.2 versicolor\n## 99           5.1 versicolor\n## 100          5.7 versicolor\n## 101          6.3  virginica\n## 102          5.8  virginica\n## 103          7.1  virginica\n## 104          6.3  virginica\n## 105          6.5  virginica\n## 106          7.6  virginica\n## 107          4.9  virginica\n## 108          7.3  virginica\n## 109          6.7  virginica\n## 110          7.2  virginica\n## 111          6.5  virginica\n## 112          6.4  virginica\n## 113          6.8  virginica\n## 114          5.7  virginica\n## 115          5.8  virginica\n## 116          6.4  virginica\n## 117          6.5  virginica\n## 118          7.7  virginica\n## 119          7.7  virginica\n## 120          6.0  virginica\n## 121          6.9  virginica\n## 122          5.6  virginica\n## 123          7.7  virginica\n## 124          6.3  virginica\n## 125          6.7  virginica\n## 126          7.2  virginica\n## 127          6.2  virginica\n## 128          6.1  virginica\n## 129          6.4  virginica\n## 130          7.2  virginica\n## 131          7.4  virginica\n## 132          7.9  virginica\n## 133          6.4  virginica\n## 134          6.3  virginica\n## 135          6.1  virginica\n## 136          7.7  virginica\n## 137          6.3  virginica\n## 138          6.4  virginica\n## 139          6.0  virginica\n## 140          6.9  virginica\n## 141          6.7  virginica\n## 142          6.9  virginica\n## 143          5.8  virginica\n## 144          6.8  virginica\n## 145          6.7  virginica\n## 146          6.7  virginica\n## 147          6.3  virginica\n## 148          6.5  virginica\n## 149          6.2  virginica\n## 150          5.9  virginica\niris[1:5, ] # 1~5행의 모든 데이터\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\niris[1:5, c(1, 3)] # 1~5행의 데이터 중 1, 3열의 데이터\n##   Sepal.Length Petal.Length\n## 1          5.1          1.4\n## 2          4.9          1.4\n## 3          4.7          1.3\n## 4          4.6          1.5\n## 5          5.0          1.4\n\n\ndim(iris) # 행과 열의 개수 출력\n## [1] 150   5\nnrow(iris) # 행의 개수 출력\n## [1] 150\nncol(iris) # 열의 개수 출력\n## [1] 5\ncolnames(iris) # 열 이름 출력, names()와 결과 동일\n## [1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"\nhead(iris) # 데이터셋의 앞부분 일부 출력\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\ntail(iris) # 데이터셋의 뒷부분 일부 출력\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n## 145          6.7         3.3          5.7         2.5 virginica\n## 146          6.7         3.0          5.2         2.3 virginica\n## 147          6.3         2.5          5.0         1.9 virginica\n## 148          6.5         3.0          5.2         2.0 virginica\n## 149          6.2         3.4          5.4         2.3 virginica\n## 150          5.9         3.0          5.1         1.8 virginica\n\nhead(iris, 10)\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1           5.1         3.5          1.4         0.2  setosa\n## 2           4.9         3.0          1.4         0.2  setosa\n## 3           4.7         3.2          1.3         0.2  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\n## 5           5.0         3.6          1.4         0.2  setosa\n## 6           5.4         3.9          1.7         0.4  setosa\n## 7           4.6         3.4          1.4         0.3  setosa\n## 8           5.0         3.4          1.5         0.2  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\ntail(iris, 20)\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n## 131          7.4         2.8          6.1         1.9 virginica\n## 132          7.9         3.8          6.4         2.0 virginica\n## 133          6.4         2.8          5.6         2.2 virginica\n## 134          6.3         2.8          5.1         1.5 virginica\n## 135          6.1         2.6          5.6         1.4 virginica\n## 136          7.7         3.0          6.1         2.3 virginica\n## 137          6.3         3.4          5.6         2.4 virginica\n## 138          6.4         3.1          5.5         1.8 virginica\n## 139          6.0         3.0          4.8         1.8 virginica\n## 140          6.9         3.1          5.4         2.1 virginica\n## 141          6.7         3.1          5.6         2.4 virginica\n## 142          6.9         3.1          5.1         2.3 virginica\n## 143          5.8         2.7          5.1         1.9 virginica\n## 144          6.8         3.2          5.9         2.3 virginica\n## 145          6.7         3.3          5.7         2.5 virginica\n## 146          6.7         3.0          5.2         2.3 virginica\n## 147          6.3         2.5          5.0         1.9 virginica\n## 148          6.5         3.0          5.2         2.0 virginica\n## 149          6.2         3.4          5.4         2.3 virginica\n## 150          5.9         3.0          5.1         1.8 virginica\n\n\nstr(iris) # 데이터셋 요약 정보 보기\n## 'data.frame':    150 obs. of  5 variables:\n##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n##  $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\niris[, 5] # 품종 데이터 보기\n##   [1] setosa     setosa     setosa     setosa     setosa     setosa    \n##   [7] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [13] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [19] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [25] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [31] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [37] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [43] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [49] setosa     setosa     versicolor versicolor versicolor versicolor\n##  [55] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [61] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [67] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [73] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [79] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [85] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [91] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [97] versicolor versicolor versicolor versicolor virginica  virginica \n## [103] virginica  virginica  virginica  virginica  virginica  virginica \n## [109] virginica  virginica  virginica  virginica  virginica  virginica \n## [115] virginica  virginica  virginica  virginica  virginica  virginica \n## [121] virginica  virginica  virginica  virginica  virginica  virginica \n## [127] virginica  virginica  virginica  virginica  virginica  virginica \n## [133] virginica  virginica  virginica  virginica  virginica  virginica \n## [139] virginica  virginica  virginica  virginica  virginica  virginica \n## [145] virginica  virginica  virginica  virginica  virginica  virginica \n## Levels: setosa versicolor virginica\nunique(iris[, 5]) # 품종의 종류 보기(중복 제거)\n## [1] setosa     versicolor virginica \n## Levels: setosa versicolor virginica\ntable(iris[, \"Species\"]) # 품종의 종류별 행의 개수 세기\n## \n##     setosa versicolor  virginica \n##         50         50         50\n\n\niris[, -5]\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width\n## 1            5.1         3.5          1.4         0.2\n## 2            4.9         3.0          1.4         0.2\n## 3            4.7         3.2          1.3         0.2\n## 4            4.6         3.1          1.5         0.2\n## 5            5.0         3.6          1.4         0.2\n## 6            5.4         3.9          1.7         0.4\n## 7            4.6         3.4          1.4         0.3\n## 8            5.0         3.4          1.5         0.2\n## 9            4.4         2.9          1.4         0.2\n## 10           4.9         3.1          1.5         0.1\n## 11           5.4         3.7          1.5         0.2\n## 12           4.8         3.4          1.6         0.2\n## 13           4.8         3.0          1.4         0.1\n## 14           4.3         3.0          1.1         0.1\n## 15           5.8         4.0          1.2         0.2\n## 16           5.7         4.4          1.5         0.4\n## 17           5.4         3.9          1.3         0.4\n## 18           5.1         3.5          1.4         0.3\n## 19           5.7         3.8          1.7         0.3\n## 20           5.1         3.8          1.5         0.3\n## 21           5.4         3.4          1.7         0.2\n## 22           5.1         3.7          1.5         0.4\n## 23           4.6         3.6          1.0         0.2\n## 24           5.1         3.3          1.7         0.5\n## 25           4.8         3.4          1.9         0.2\n## 26           5.0         3.0          1.6         0.2\n## 27           5.0         3.4          1.6         0.4\n## 28           5.2         3.5          1.5         0.2\n## 29           5.2         3.4          1.4         0.2\n## 30           4.7         3.2          1.6         0.2\n## 31           4.8         3.1          1.6         0.2\n## 32           5.4         3.4          1.5         0.4\n## 33           5.2         4.1          1.5         0.1\n## 34           5.5         4.2          1.4         0.2\n## 35           4.9         3.1          1.5         0.2\n## 36           5.0         3.2          1.2         0.2\n## 37           5.5         3.5          1.3         0.2\n## 38           4.9         3.6          1.4         0.1\n## 39           4.4         3.0          1.3         0.2\n## 40           5.1         3.4          1.5         0.2\n## 41           5.0         3.5          1.3         0.3\n## 42           4.5         2.3          1.3         0.3\n## 43           4.4         3.2          1.3         0.2\n## 44           5.0         3.5          1.6         0.6\n## 45           5.1         3.8          1.9         0.4\n## 46           4.8         3.0          1.4         0.3\n## 47           5.1         3.8          1.6         0.2\n## 48           4.6         3.2          1.4         0.2\n## 49           5.3         3.7          1.5         0.2\n## 50           5.0         3.3          1.4         0.2\n## 51           7.0         3.2          4.7         1.4\n## 52           6.4         3.2          4.5         1.5\n## 53           6.9         3.1          4.9         1.5\n## 54           5.5         2.3          4.0         1.3\n## 55           6.5         2.8          4.6         1.5\n## 56           5.7         2.8          4.5         1.3\n## 57           6.3         3.3          4.7         1.6\n## 58           4.9         2.4          3.3         1.0\n## 59           6.6         2.9          4.6         1.3\n## 60           5.2         2.7          3.9         1.4\n## 61           5.0         2.0          3.5         1.0\n## 62           5.9         3.0          4.2         1.5\n## 63           6.0         2.2          4.0         1.0\n## 64           6.1         2.9          4.7         1.4\n## 65           5.6         2.9          3.6         1.3\n## 66           6.7         3.1          4.4         1.4\n## 67           5.6         3.0          4.5         1.5\n## 68           5.8         2.7          4.1         1.0\n## 69           6.2         2.2          4.5         1.5\n## 70           5.6         2.5          3.9         1.1\n## 71           5.9         3.2          4.8         1.8\n## 72           6.1         2.8          4.0         1.3\n## 73           6.3         2.5          4.9         1.5\n## 74           6.1         2.8          4.7         1.2\n## 75           6.4         2.9          4.3         1.3\n## 76           6.6         3.0          4.4         1.4\n## 77           6.8         2.8          4.8         1.4\n## 78           6.7         3.0          5.0         1.7\n## 79           6.0         2.9          4.5         1.5\n## 80           5.7         2.6          3.5         1.0\n## 81           5.5         2.4          3.8         1.1\n## 82           5.5         2.4          3.7         1.0\n## 83           5.8         2.7          3.9         1.2\n## 84           6.0         2.7          5.1         1.6\n## 85           5.4         3.0          4.5         1.5\n## 86           6.0         3.4          4.5         1.6\n## 87           6.7         3.1          4.7         1.5\n## 88           6.3         2.3          4.4         1.3\n## 89           5.6         3.0          4.1         1.3\n## 90           5.5         2.5          4.0         1.3\n## 91           5.5         2.6          4.4         1.2\n## 92           6.1         3.0          4.6         1.4\n## 93           5.8         2.6          4.0         1.2\n## 94           5.0         2.3          3.3         1.0\n## 95           5.6         2.7          4.2         1.3\n## 96           5.7         3.0          4.2         1.2\n## 97           5.7         2.9          4.2         1.3\n## 98           6.2         2.9          4.3         1.3\n## 99           5.1         2.5          3.0         1.1\n## 100          5.7         2.8          4.1         1.3\n## 101          6.3         3.3          6.0         2.5\n## 102          5.8         2.7          5.1         1.9\n## 103          7.1         3.0          5.9         2.1\n## 104          6.3         2.9          5.6         1.8\n## 105          6.5         3.0          5.8         2.2\n## 106          7.6         3.0          6.6         2.1\n## 107          4.9         2.5          4.5         1.7\n## 108          7.3         2.9          6.3         1.8\n## 109          6.7         2.5          5.8         1.8\n## 110          7.2         3.6          6.1         2.5\n## 111          6.5         3.2          5.1         2.0\n## 112          6.4         2.7          5.3         1.9\n## 113          6.8         3.0          5.5         2.1\n## 114          5.7         2.5          5.0         2.0\n## 115          5.8         2.8          5.1         2.4\n## 116          6.4         3.2          5.3         2.3\n## 117          6.5         3.0          5.5         1.8\n## 118          7.7         3.8          6.7         2.2\n## 119          7.7         2.6          6.9         2.3\n## 120          6.0         2.2          5.0         1.5\n## 121          6.9         3.2          5.7         2.3\n## 122          5.6         2.8          4.9         2.0\n## 123          7.7         2.8          6.7         2.0\n## 124          6.3         2.7          4.9         1.8\n## 125          6.7         3.3          5.7         2.1\n## 126          7.2         3.2          6.0         1.8\n## 127          6.2         2.8          4.8         1.8\n## 128          6.1         3.0          4.9         1.8\n## 129          6.4         2.8          5.6         2.1\n## 130          7.2         3.0          5.8         1.6\n## 131          7.4         2.8          6.1         1.9\n## 132          7.9         3.8          6.4         2.0\n## 133          6.4         2.8          5.6         2.2\n## 134          6.3         2.8          5.1         1.5\n## 135          6.1         2.6          5.6         1.4\n## 136          7.7         3.0          6.1         2.3\n## 137          6.3         3.4          5.6         2.4\n## 138          6.4         3.1          5.5         1.8\n## 139          6.0         3.0          4.8         1.8\n## 140          6.9         3.1          5.4         2.1\n## 141          6.7         3.1          5.6         2.4\n## 142          6.9         3.1          5.1         2.3\n## 143          5.8         2.7          5.1         1.9\n## 144          6.8         3.2          5.9         2.3\n## 145          6.7         3.3          5.7         2.5\n## 146          6.7         3.0          5.2         2.3\n## 147          6.3         2.5          5.0         1.9\n## 148          6.5         3.0          5.2         2.0\n## 149          6.2         3.4          5.4         2.3\n## 150          5.9         3.0          5.1         1.8\ncolSums(iris[, -5]) # 열별 합계\n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n##        876.5        458.6        563.7        179.9\ncolMeans(iris[, -5]) # 열별 평균\n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n##     5.843333     3.057333     3.758000     1.199333\nrowSums(iris[, -5]) # 행별 합계\n##   [1] 10.2  9.5  9.4  9.4 10.2 11.4  9.7 10.1  8.9  9.6 10.8 10.0  9.3  8.5 11.2\n##  [16] 12.0 11.0 10.3 11.5 10.7 10.7 10.7  9.4 10.6 10.3  9.8 10.4 10.4 10.2  9.7\n##  [31]  9.7 10.7 10.9 11.3  9.7  9.6 10.5 10.0  8.9 10.2 10.1  8.4  9.1 10.7 11.2\n##  [46]  9.5 10.7  9.4 10.7  9.9 16.3 15.6 16.4 13.1 15.4 14.3 15.9 11.6 15.4 13.2\n##  [61] 11.5 14.6 13.2 15.1 13.4 15.6 14.6 13.6 14.4 13.1 15.7 14.2 15.2 14.8 14.9\n##  [76] 15.4 15.8 16.4 14.9 12.8 12.8 12.6 13.6 15.4 14.4 15.5 16.0 14.3 14.0 13.3\n##  [91] 13.7 15.1 13.6 11.6 13.8 14.1 14.1 14.7 11.7 13.9 18.1 15.5 18.1 16.6 17.5\n## [106] 19.3 13.6 18.3 16.8 19.4 16.8 16.3 17.4 15.2 16.1 17.2 16.8 20.4 19.5 14.7\n## [121] 18.1 15.3 19.2 15.7 17.8 18.2 15.6 15.8 16.9 17.6 18.2 20.1 17.0 15.7 15.7\n## [136] 19.1 17.7 16.8 15.6 17.5 17.8 17.4 15.5 18.2 18.2 17.2 15.7 16.7 17.3 15.8\nrowMeans(iris[, -5]) # 행별 평균\n##   [1] 2.550 2.375 2.350 2.350 2.550 2.850 2.425 2.525 2.225 2.400 2.700 2.500\n##  [13] 2.325 2.125 2.800 3.000 2.750 2.575 2.875 2.675 2.675 2.675 2.350 2.650\n##  [25] 2.575 2.450 2.600 2.600 2.550 2.425 2.425 2.675 2.725 2.825 2.425 2.400\n##  [37] 2.625 2.500 2.225 2.550 2.525 2.100 2.275 2.675 2.800 2.375 2.675 2.350\n##  [49] 2.675 2.475 4.075 3.900 4.100 3.275 3.850 3.575 3.975 2.900 3.850 3.300\n##  [61] 2.875 3.650 3.300 3.775 3.350 3.900 3.650 3.400 3.600 3.275 3.925 3.550\n##  [73] 3.800 3.700 3.725 3.850 3.950 4.100 3.725 3.200 3.200 3.150 3.400 3.850\n##  [85] 3.600 3.875 4.000 3.575 3.500 3.325 3.425 3.775 3.400 2.900 3.450 3.525\n##  [97] 3.525 3.675 2.925 3.475 4.525 3.875 4.525 4.150 4.375 4.825 3.400 4.575\n## [109] 4.200 4.850 4.200 4.075 4.350 3.800 4.025 4.300 4.200 5.100 4.875 3.675\n## [121] 4.525 3.825 4.800 3.925 4.450 4.550 3.900 3.950 4.225 4.400 4.550 5.025\n## [133] 4.250 3.925 3.925 4.775 4.425 4.200 3.900 4.375 4.450 4.350 3.875 4.550\n## [145] 4.550 4.300 3.925 4.175 4.325 3.950\n\n\nz &lt;- matrix(1:20, nrow = 4, ncol = 5)\nz\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\nt(z) # 행과열 방향 전환\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    2    3    4\n## [2,]    5    6    7    8\n## [3,]    9   10   11   12\n## [4,]   13   14   15   16\n## [5,]   17   18   19   20\n\n\nIR.1 &lt;- subset(iris, Species == \"setosa\")\nIR.1\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1           5.1         3.5          1.4         0.2  setosa\n## 2           4.9         3.0          1.4         0.2  setosa\n## 3           4.7         3.2          1.3         0.2  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\n## 5           5.0         3.6          1.4         0.2  setosa\n## 6           5.4         3.9          1.7         0.4  setosa\n## 7           4.6         3.4          1.4         0.3  setosa\n## 8           5.0         3.4          1.5         0.2  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\n## 11          5.4         3.7          1.5         0.2  setosa\n## 12          4.8         3.4          1.6         0.2  setosa\n## 13          4.8         3.0          1.4         0.1  setosa\n## 14          4.3         3.0          1.1         0.1  setosa\n## 15          5.8         4.0          1.2         0.2  setosa\n## 16          5.7         4.4          1.5         0.4  setosa\n## 17          5.4         3.9          1.3         0.4  setosa\n## 18          5.1         3.5          1.4         0.3  setosa\n## 19          5.7         3.8          1.7         0.3  setosa\n## 20          5.1         3.8          1.5         0.3  setosa\n## 21          5.4         3.4          1.7         0.2  setosa\n## 22          5.1         3.7          1.5         0.4  setosa\n## 23          4.6         3.6          1.0         0.2  setosa\n## 24          5.1         3.3          1.7         0.5  setosa\n## 25          4.8         3.4          1.9         0.2  setosa\n## 26          5.0         3.0          1.6         0.2  setosa\n## 27          5.0         3.4          1.6         0.4  setosa\n## 28          5.2         3.5          1.5         0.2  setosa\n## 29          5.2         3.4          1.4         0.2  setosa\n## 30          4.7         3.2          1.6         0.2  setosa\n## 31          4.8         3.1          1.6         0.2  setosa\n## 32          5.4         3.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\n## 35          4.9         3.1          1.5         0.2  setosa\n## 36          5.0         3.2          1.2         0.2  setosa\n## 37          5.5         3.5          1.3         0.2  setosa\n## 38          4.9         3.6          1.4         0.1  setosa\n## 39          4.4         3.0          1.3         0.2  setosa\n## 40          5.1         3.4          1.5         0.2  setosa\n## 41          5.0         3.5          1.3         0.3  setosa\n## 42          4.5         2.3          1.3         0.3  setosa\n## 43          4.4         3.2          1.3         0.2  setosa\n## 44          5.0         3.5          1.6         0.6  setosa\n## 45          5.1         3.8          1.9         0.4  setosa\n## 46          4.8         3.0          1.4         0.3  setosa\n## 47          5.1         3.8          1.6         0.2  setosa\n## 48          4.6         3.2          1.4         0.2  setosa\n## 49          5.3         3.7          1.5         0.2  setosa\n## 50          5.0         3.3          1.4         0.2  setosa\n\nIR.2 &lt;- subset(iris, Sepal.Length &gt; 5.0 & Sepal.Width &gt; 4.0)\nIR.2\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 16          5.7         4.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\nIR.2[, c(2, 4)] # 2, 4열의 값만 추출\n##    Sepal.Width Petal.Width\n## 16         4.4         0.4\n## 33         4.1         0.1\n## 34         4.2         0.2\n\nIR.3 &lt;- subset(iris, Sepal.Length &gt; 5.0 | Sepal.Width &gt; 4.0)\nIR.3\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 1            5.1         3.5          1.4         0.2     setosa\n## 6            5.4         3.9          1.7         0.4     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 15           5.8         4.0          1.2         0.2     setosa\n## 16           5.7         4.4          1.5         0.4     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 34           5.5         4.2          1.4         0.2     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 49           5.3         3.7          1.5         0.2     setosa\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 114          5.7         2.5          5.0         2.0  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 120          6.0         2.2          5.0         1.5  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 122          5.6         2.8          4.9         2.0  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 127          6.2         2.8          4.8         1.8  virginica\n## 128          6.1         3.0          4.9         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\n\n\na &lt;- matrix(1:20, 4, 5)\nb &lt;- matrix(21:40, 4, 5)\na ; b\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   21   25   29   33   37\n## [2,]   22   26   30   34   38\n## [3,]   23   27   31   35   39\n## [4,]   24   28   32   36   40\n\n2 * a # 매트릭스 a에 저장된 값들에 2를 곱하기\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    2   10   18   26   34\n## [2,]    4   12   20   28   36\n## [3,]    6   14   22   30   38\n## [4,]    8   16   24   32   40\nb - 5\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   16   20   24   28   32\n## [2,]   17   21   25   29   33\n## [3,]   18   22   26   30   34\n## [4,]   19   23   27   31   35\n2 * a + 3 * b\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   65   85  105  125  145\n## [2,]   70   90  110  130  150\n## [3,]   75   95  115  135  155\n## [4,]   80  100  120  140  160\n\na + b\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   22   30   38   46   54\n## [2,]   24   32   40   48   56\n## [3,]   26   34   42   50   58\n## [4,]   28   36   44   52   60\nb - a\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   20   20   20   20   20\n## [2,]   20   20   20   20   20\n## [3,]   20   20   20   20   20\n## [4,]   20   20   20   20   20\nb / a\n##           [,1]     [,2]     [,3]     [,4]     [,5]\n## [1,] 21.000000 5.000000 3.222222 2.538462 2.176471\n## [2,] 11.000000 4.333333 3.000000 2.428571 2.111111\n## [3,]  7.666667 3.857143 2.818182 2.333333 2.052632\n## [4,]  6.000000 3.500000 2.666667 2.250000 2.000000\na * b\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   21  125  261  429  629\n## [2,]   44  156  300  476  684\n## [3,]   69  189  341  525  741\n## [4,]   96  224  384  576  800\n\na &lt;- a * 3\na\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    3   15   27   39   51\n## [2,]    6   18   30   42   54\n## [3,]    9   21   33   45   57\n## [4,]   12   24   36   48   60\nb &lt;- b - 5\nb\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   16   20   24   28   32\n## [2,]   17   21   25   29   33\n## [3,]   18   22   26   30   34\n## [4,]   19   23   27   31   35\n\n\nclass(iris) # iris 데이터셋의 자료구조 확인\n## [1] \"data.frame\"\nclass(state.x77) # state.x77 데이터셋의 자료구조 확인\n## [1] \"matrix\" \"array\"\nis.matrix(iris) # 데이터셋이 매트릭스인지를 확인하는 함수\n## [1] FALSE\nis.data.frame(iris) # 데이터셋이 데이터프레임인지를 확인하는 함수\n## [1] TRUE\nis.matrix(state.x77)\n## [1] TRUE\nis.data.frame(state.x77)\n## [1] FALSE\n\n\n# 매트릭스를 데이터프레임으로 변환\nst &lt;- data.frame(state.x77)\nhead(st)\n##            Population Income Illiteracy Life.Exp Murder HS.Grad Frost   Area\n## Alabama          3615   3624        2.1    69.05   15.1    41.3    20  50708\n## Alaska            365   6315        1.5    69.31   11.3    66.7   152 566432\n## Arizona          2212   4530        1.8    70.55    7.8    58.1    15 113417\n## Arkansas         2110   3378        1.9    70.66   10.1    39.9    65  51945\n## California      21198   5114        1.1    71.71   10.3    62.6    20 156361\n## Colorado         2541   4884        0.7    72.06    6.8    63.9   166 103766\nclass(st)\n## [1] \"data.frame\"\n\n\niris[, \"Species\"] # 결과=벡터. 매트릭스와 데이터프레임 모두 가능\n##   [1] setosa     setosa     setosa     setosa     setosa     setosa    \n##   [7] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [13] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [19] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [25] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [31] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [37] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [43] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [49] setosa     setosa     versicolor versicolor versicolor versicolor\n##  [55] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [61] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [67] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [73] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [79] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [85] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [91] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [97] versicolor versicolor versicolor versicolor virginica  virginica \n## [103] virginica  virginica  virginica  virginica  virginica  virginica \n## [109] virginica  virginica  virginica  virginica  virginica  virginica \n## [115] virginica  virginica  virginica  virginica  virginica  virginica \n## [121] virginica  virginica  virginica  virginica  virginica  virginica \n## [127] virginica  virginica  virginica  virginica  virginica  virginica \n## [133] virginica  virginica  virginica  virginica  virginica  virginica \n## [139] virginica  virginica  virginica  virginica  virginica  virginica \n## [145] virginica  virginica  virginica  virginica  virginica  virginica \n## Levels: setosa versicolor virginica\niris[, 5] # 결과=벡터. 매트릭스와 데이터프레임 모두 가능\n##   [1] setosa     setosa     setosa     setosa     setosa     setosa    \n##   [7] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [13] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [19] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [25] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [31] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [37] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [43] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [49] setosa     setosa     versicolor versicolor versicolor versicolor\n##  [55] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [61] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [67] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [73] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [79] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [85] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [91] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [97] versicolor versicolor versicolor versicolor virginica  virginica \n## [103] virginica  virginica  virginica  virginica  virginica  virginica \n## [109] virginica  virginica  virginica  virginica  virginica  virginica \n## [115] virginica  virginica  virginica  virginica  virginica  virginica \n## [121] virginica  virginica  virginica  virginica  virginica  virginica \n## [127] virginica  virginica  virginica  virginica  virginica  virginica \n## [133] virginica  virginica  virginica  virginica  virginica  virginica \n## [139] virginica  virginica  virginica  virginica  virginica  virginica \n## [145] virginica  virginica  virginica  virginica  virginica  virginica \n## Levels: setosa versicolor virginica\niris[\"Species\"] # 결과=데이터프레임. 데이터프레임만 가능\n##        Species\n## 1       setosa\n## 2       setosa\n## 3       setosa\n## 4       setosa\n## 5       setosa\n## 6       setosa\n## 7       setosa\n## 8       setosa\n## 9       setosa\n## 10      setosa\n## 11      setosa\n## 12      setosa\n## 13      setosa\n## 14      setosa\n## 15      setosa\n## 16      setosa\n## 17      setosa\n## 18      setosa\n## 19      setosa\n## 20      setosa\n## 21      setosa\n## 22      setosa\n## 23      setosa\n## 24      setosa\n## 25      setosa\n## 26      setosa\n## 27      setosa\n## 28      setosa\n## 29      setosa\n## 30      setosa\n## 31      setosa\n## 32      setosa\n## 33      setosa\n## 34      setosa\n## 35      setosa\n## 36      setosa\n## 37      setosa\n## 38      setosa\n## 39      setosa\n## 40      setosa\n## 41      setosa\n## 42      setosa\n## 43      setosa\n## 44      setosa\n## 45      setosa\n## 46      setosa\n## 47      setosa\n## 48      setosa\n## 49      setosa\n## 50      setosa\n## 51  versicolor\n## 52  versicolor\n## 53  versicolor\n## 54  versicolor\n## 55  versicolor\n## 56  versicolor\n## 57  versicolor\n## 58  versicolor\n## 59  versicolor\n## 60  versicolor\n## 61  versicolor\n## 62  versicolor\n## 63  versicolor\n## 64  versicolor\n## 65  versicolor\n## 66  versicolor\n## 67  versicolor\n## 68  versicolor\n## 69  versicolor\n## 70  versicolor\n## 71  versicolor\n## 72  versicolor\n## 73  versicolor\n## 74  versicolor\n## 75  versicolor\n## 76  versicolor\n## 77  versicolor\n## 78  versicolor\n## 79  versicolor\n## 80  versicolor\n## 81  versicolor\n## 82  versicolor\n## 83  versicolor\n## 84  versicolor\n## 85  versicolor\n## 86  versicolor\n## 87  versicolor\n## 88  versicolor\n## 89  versicolor\n## 90  versicolor\n## 91  versicolor\n## 92  versicolor\n## 93  versicolor\n## 94  versicolor\n## 95  versicolor\n## 96  versicolor\n## 97  versicolor\n## 98  versicolor\n## 99  versicolor\n## 100 versicolor\n## 101  virginica\n## 102  virginica\n## 103  virginica\n## 104  virginica\n## 105  virginica\n## 106  virginica\n## 107  virginica\n## 108  virginica\n## 109  virginica\n## 110  virginica\n## 111  virginica\n## 112  virginica\n## 113  virginica\n## 114  virginica\n## 115  virginica\n## 116  virginica\n## 117  virginica\n## 118  virginica\n## 119  virginica\n## 120  virginica\n## 121  virginica\n## 122  virginica\n## 123  virginica\n## 124  virginica\n## 125  virginica\n## 126  virginica\n## 127  virginica\n## 128  virginica\n## 129  virginica\n## 130  virginica\n## 131  virginica\n## 132  virginica\n## 133  virginica\n## 134  virginica\n## 135  virginica\n## 136  virginica\n## 137  virginica\n## 138  virginica\n## 139  virginica\n## 140  virginica\n## 141  virginica\n## 142  virginica\n## 143  virginica\n## 144  virginica\n## 145  virginica\n## 146  virginica\n## 147  virginica\n## 148  virginica\n## 149  virginica\n## 150  virginica\niris[5] # 결과=데이터프레임. 데이터프레임만 가능\n##        Species\n## 1       setosa\n## 2       setosa\n## 3       setosa\n## 4       setosa\n## 5       setosa\n## 6       setosa\n## 7       setosa\n## 8       setosa\n## 9       setosa\n## 10      setosa\n## 11      setosa\n## 12      setosa\n## 13      setosa\n## 14      setosa\n## 15      setosa\n## 16      setosa\n## 17      setosa\n## 18      setosa\n## 19      setosa\n## 20      setosa\n## 21      setosa\n## 22      setosa\n## 23      setosa\n## 24      setosa\n## 25      setosa\n## 26      setosa\n## 27      setosa\n## 28      setosa\n## 29      setosa\n## 30      setosa\n## 31      setosa\n## 32      setosa\n## 33      setosa\n## 34      setosa\n## 35      setosa\n## 36      setosa\n## 37      setosa\n## 38      setosa\n## 39      setosa\n## 40      setosa\n## 41      setosa\n## 42      setosa\n## 43      setosa\n## 44      setosa\n## 45      setosa\n## 46      setosa\n## 47      setosa\n## 48      setosa\n## 49      setosa\n## 50      setosa\n## 51  versicolor\n## 52  versicolor\n## 53  versicolor\n## 54  versicolor\n## 55  versicolor\n## 56  versicolor\n## 57  versicolor\n## 58  versicolor\n## 59  versicolor\n## 60  versicolor\n## 61  versicolor\n## 62  versicolor\n## 63  versicolor\n## 64  versicolor\n## 65  versicolor\n## 66  versicolor\n## 67  versicolor\n## 68  versicolor\n## 69  versicolor\n## 70  versicolor\n## 71  versicolor\n## 72  versicolor\n## 73  versicolor\n## 74  versicolor\n## 75  versicolor\n## 76  versicolor\n## 77  versicolor\n## 78  versicolor\n## 79  versicolor\n## 80  versicolor\n## 81  versicolor\n## 82  versicolor\n## 83  versicolor\n## 84  versicolor\n## 85  versicolor\n## 86  versicolor\n## 87  versicolor\n## 88  versicolor\n## 89  versicolor\n## 90  versicolor\n## 91  versicolor\n## 92  versicolor\n## 93  versicolor\n## 94  versicolor\n## 95  versicolor\n## 96  versicolor\n## 97  versicolor\n## 98  versicolor\n## 99  versicolor\n## 100 versicolor\n## 101  virginica\n## 102  virginica\n## 103  virginica\n## 104  virginica\n## 105  virginica\n## 106  virginica\n## 107  virginica\n## 108  virginica\n## 109  virginica\n## 110  virginica\n## 111  virginica\n## 112  virginica\n## 113  virginica\n## 114  virginica\n## 115  virginica\n## 116  virginica\n## 117  virginica\n## 118  virginica\n## 119  virginica\n## 120  virginica\n## 121  virginica\n## 122  virginica\n## 123  virginica\n## 124  virginica\n## 125  virginica\n## 126  virginica\n## 127  virginica\n## 128  virginica\n## 129  virginica\n## 130  virginica\n## 131  virginica\n## 132  virginica\n## 133  virginica\n## 134  virginica\n## 135  virginica\n## 136  virginica\n## 137  virginica\n## 138  virginica\n## 139  virginica\n## 140  virginica\n## 141  virginica\n## 142  virginica\n## 143  virginica\n## 144  virginica\n## 145  virginica\n## 146  virginica\n## 147  virginica\n## 148  virginica\n## 149  virginica\n## 150  virginica\niris$Species # 결과=벡터. 데이터프레임만 가능\n##   [1] setosa     setosa     setosa     setosa     setosa     setosa    \n##   [7] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [13] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [19] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [25] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [31] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [37] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [43] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [49] setosa     setosa     versicolor versicolor versicolor versicolor\n##  [55] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [61] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [67] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [73] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [79] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [85] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [91] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [97] versicolor versicolor versicolor versicolor virginica  virginica \n## [103] virginica  virginica  virginica  virginica  virginica  virginica \n## [109] virginica  virginica  virginica  virginica  virginica  virginica \n## [115] virginica  virginica  virginica  virginica  virginica  virginica \n## [121] virginica  virginica  virginica  virginica  virginica  virginica \n## [127] virginica  virginica  virginica  virginica  virginica  virginica \n## [133] virginica  virginica  virginica  virginica  virginica  virginica \n## [139] virginica  virginica  virginica  virginica  virginica  virginica \n## [145] virginica  virginica  virginica  virginica  virginica  virginica \n## Levels: setosa versicolor virginica\n\n\ngetwd()\n## [1] \"C:/Users/Hyunsoo Kim/Desktop/senior_grade/blog/my-quarto-website\"\n# setwd(\"G:/내 드라이브/202202/R_Basic/data\") # 작업 폴더 지정\nair &lt;- read.csv(\"./R_Basic/data/airquality.csv\", header = T) # .csv 파일 읽기\nhead(air)\n##                                    version.https...git.lfs.github.com.spec.v1\n## 1 oid sha256:6fdc84af524856a54abe063336bfea6511e9fb5dfcd2ec6e1dfa9e1e4d8c7357\n## 2                                                                   size 3044\n\n\nmy.iris &lt;- subset(iris, Species = 'Setosa') # Setosa 품종 데이터만 추출\n## Warning: In subset.data.frame(iris, Species = \"Setosa\") :\n##  extra argument 'Species' will be disregarded\nwrite.csv(my.iris, \"./R_Basic/data/my_iris_1.csv\") # .csv 파일에 저장하기"
  },
  {
    "objectID": "R_Basic.html#장.-조건문-반복문-함수",
    "href": "R_Basic.html#장.-조건문-반복문-함수",
    "title": "R Basic",
    "section": "",
    "text": "job.type &lt;- 'A'\nif (job.type == 'B') {\n    bonus &lt;- 200 # 직무 유형이 B일 때 실행\n} else {\n    bonus &lt;- 100 # 직무 유형이 B가 아닌 나머지 경우 실행\n}\nprint(bonus)\n## [1] 100\n\n\njob.type &lt;- 'B'\nbonus &lt;- 100\nif (job.type == 'A') {\n    bonus &lt;- 200 # 직무 유형이 A일 때 실행\n}\nprint(bonus)\n## [1] 100\n\n\nscore &lt;- 85\n\nif (score &gt; 90) {\n    grade &lt;- 'A'\n} else if (score &gt; 80) {\n    grade &lt;- 'B'\n} else if (score &gt; 70) {\n    grade &lt;- 'C'\n} else if (score &gt; 60) {\n    grade &lt;- 'D'\n} else {\n    grade &lt;- 'F'\n}\n\nprint(grade)\n## [1] \"B\"\n\n\na &lt;- 10\nb &lt;- 20\nif (a &gt; 5 & b &gt; 5) {    # and 사용\n    print(a + b)\n}\n## [1] 30\n\nif (a &gt; 5 | b &gt; 30) {   # or 사용\n    print(a * b)\n}\n## [1] 200\n\nif (a &gt; 5 & b &gt; 30) {\n    print(a * b)\n}\n\nif (a &gt; 20 | b &gt; 30) {\n    print(a * b)\n}\n\nif (a &gt; 20 & b &gt; 15) {\n    print(a * b)\n}\n\nr_basic &lt;- 70\npython_basic &lt;- 82\n\nif (r_basic &gt; 80 & python_basic &gt; 80) {\n    grade &lt;- \"Excellent\"\n} else {\n    grade &lt;- \"Good\"\n}\ngrade\n## [1] \"Good\"\n\n\na &lt;- 10\nb &lt;- 20\n\nif (a &gt; b) {\n    c &lt;- a\n} else {\n    c &lt;- b\n}\nprint(c)\n## [1] 20\n\na &lt;- 10\nb &lt;- 20\n\nc &lt;- ifelse(a &gt; b, a, b)\nprint(c)\n## [1] 20\n\n\nfor(i in 1:5) {\n    print('*')\n}\n## [1] \"*\"\n## [1] \"*\"\n## [1] \"*\"\n## [1] \"*\"\n## [1] \"*\"\n\nfor (i in 1:5) {\n    print(i)\n}\n## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\n\nfor (i in 1:5) {\n    a &lt;- i * 2\n    print(a)\n}\n## [1] 2\n## [1] 4\n## [1] 6\n## [1] 8\n## [1] 10\n\n# for (i in 1:10000) {\n#     a &lt;- i * 2\n#     print(a)\n# }\n\n# for (i in 1:10000) {\n#     a &lt;- i * 2 / 1521 + 10000\n#     print(a)\n# }\n\n\nfor (i in 6:10) {\n    print(i)\n}\n## [1] 6\n## [1] 7\n## [1] 8\n## [1] 9\n## [1] 10\n\n\nfor(i in 1:9) {\n    cat('2 *', i, '=', 2 * i, '\\n')\n}\n## 2 * 1 = 2 \n## 2 * 2 = 4 \n## 2 * 3 = 6 \n## 2 * 4 = 8 \n## 2 * 5 = 10 \n## 2 * 6 = 12 \n## 2 * 7 = 14 \n## 2 * 8 = 16 \n## 2 * 9 = 18\n\nfor (i in 1:9) {\n    cat('2 *', i, '=', 2 * i)\n}\n## 2 * 1 = 22 * 2 = 42 * 3 = 62 * 4 = 82 * 5 = 102 * 6 = 122 * 7 = 142 * 8 = 162 * 9 = 18\n\nfor (i in 1:9) {\n    j &lt;- i:10\n    print(j)\n}\n##  [1]  1  2  3  4  5  6  7  8  9 10\n## [1]  2  3  4  5  6  7  8  9 10\n## [1]  3  4  5  6  7  8  9 10\n## [1]  4  5  6  7  8  9 10\n## [1]  5  6  7  8  9 10\n## [1]  6  7  8  9 10\n## [1]  7  8  9 10\n## [1]  8  9 10\n## [1]  9 10\n\n\nfor(i in 1:20) {\n    if (i %% 2 == 0) {  # 짝수인지 확인\n        print(i)\n    }\n}\n## [1] 2\n## [1] 4\n## [1] 6\n## [1] 8\n## [1] 10\n## [1] 12\n## [1] 14\n## [1] 16\n## [1] 18\n## [1] 20\n\n\nsum &lt;- 0\nfor (i in 1:100) {\n    sum &lt;- sum + i  # sum에 i 값을 누적\n}\nprint(sum)\n## [1] 5050\n\nsum &lt;- 0\nfor (i in 1:100) {\n    sum &lt;- sum + i\n    print(c(sum, i))\n}\n## [1] 1 1\n## [1] 3 2\n## [1] 6 3\n## [1] 10  4\n## [1] 15  5\n## [1] 21  6\n## [1] 28  7\n## [1] 36  8\n## [1] 45  9\n## [1] 55 10\n## [1] 66 11\n## [1] 78 12\n## [1] 91 13\n## [1] 105  14\n## [1] 120  15\n## [1] 136  16\n## [1] 153  17\n## [1] 171  18\n## [1] 190  19\n## [1] 210  20\n## [1] 231  21\n## [1] 253  22\n## [1] 276  23\n## [1] 300  24\n## [1] 325  25\n## [1] 351  26\n## [1] 378  27\n## [1] 406  28\n## [1] 435  29\n## [1] 465  30\n## [1] 496  31\n## [1] 528  32\n## [1] 561  33\n## [1] 595  34\n## [1] 630  35\n## [1] 666  36\n## [1] 703  37\n## [1] 741  38\n## [1] 780  39\n## [1] 820  40\n## [1] 861  41\n## [1] 903  42\n## [1] 946  43\n## [1] 990  44\n## [1] 1035   45\n## [1] 1081   46\n## [1] 1128   47\n## [1] 1176   48\n## [1] 1225   49\n## [1] 1275   50\n## [1] 1326   51\n## [1] 1378   52\n## [1] 1431   53\n## [1] 1485   54\n## [1] 1540   55\n## [1] 1596   56\n## [1] 1653   57\n## [1] 1711   58\n## [1] 1770   59\n## [1] 1830   60\n## [1] 1891   61\n## [1] 1953   62\n## [1] 2016   63\n## [1] 2080   64\n## [1] 2145   65\n## [1] 2211   66\n## [1] 2278   67\n## [1] 2346   68\n## [1] 2415   69\n## [1] 2485   70\n## [1] 2556   71\n## [1] 2628   72\n## [1] 2701   73\n## [1] 2775   74\n## [1] 2850   75\n## [1] 2926   76\n## [1] 3003   77\n## [1] 3081   78\n## [1] 3160   79\n## [1] 3240   80\n## [1] 3321   81\n## [1] 3403   82\n## [1] 3486   83\n## [1] 3570   84\n## [1] 3655   85\n## [1] 3741   86\n## [1] 3828   87\n## [1] 3916   88\n## [1] 4005   89\n## [1] 4095   90\n## [1] 4186   91\n## [1] 4278   92\n## [1] 4371   93\n## [1] 4465   94\n## [1] 4560   95\n## [1] 4656   96\n## [1] 4753   97\n## [1] 4851   98\n## [1] 4950   99\n## [1] 5050  100\nprint(sum)\n## [1] 5050\n\nsum &lt;- 0\nfor (i in 1:100) {\n    print(c(sum, i))\n    sum &lt;- sum + i\n}\n## [1] 0 1\n## [1] 1 2\n## [1] 3 3\n## [1] 6 4\n## [1] 10  5\n## [1] 15  6\n## [1] 21  7\n## [1] 28  8\n## [1] 36  9\n## [1] 45 10\n## [1] 55 11\n## [1] 66 12\n## [1] 78 13\n## [1] 91 14\n## [1] 105  15\n## [1] 120  16\n## [1] 136  17\n## [1] 153  18\n## [1] 171  19\n## [1] 190  20\n## [1] 210  21\n## [1] 231  22\n## [1] 253  23\n## [1] 276  24\n## [1] 300  25\n## [1] 325  26\n## [1] 351  27\n## [1] 378  28\n## [1] 406  29\n## [1] 435  30\n## [1] 465  31\n## [1] 496  32\n## [1] 528  33\n## [1] 561  34\n## [1] 595  35\n## [1] 630  36\n## [1] 666  37\n## [1] 703  38\n## [1] 741  39\n## [1] 780  40\n## [1] 820  41\n## [1] 861  42\n## [1] 903  43\n## [1] 946  44\n## [1] 990  45\n## [1] 1035   46\n## [1] 1081   47\n## [1] 1128   48\n## [1] 1176   49\n## [1] 1225   50\n## [1] 1275   51\n## [1] 1326   52\n## [1] 1378   53\n## [1] 1431   54\n## [1] 1485   55\n## [1] 1540   56\n## [1] 1596   57\n## [1] 1653   58\n## [1] 1711   59\n## [1] 1770   60\n## [1] 1830   61\n## [1] 1891   62\n## [1] 1953   63\n## [1] 2016   64\n## [1] 2080   65\n## [1] 2145   66\n## [1] 2211   67\n## [1] 2278   68\n## [1] 2346   69\n## [1] 2415   70\n## [1] 2485   71\n## [1] 2556   72\n## [1] 2628   73\n## [1] 2701   74\n## [1] 2775   75\n## [1] 2850   76\n## [1] 2926   77\n## [1] 3003   78\n## [1] 3081   79\n## [1] 3160   80\n## [1] 3240   81\n## [1] 3321   82\n## [1] 3403   83\n## [1] 3486   84\n## [1] 3570   85\n## [1] 3655   86\n## [1] 3741   87\n## [1] 3828   88\n## [1] 3916   89\n## [1] 4005   90\n## [1] 4095   91\n## [1] 4186   92\n## [1] 4278   93\n## [1] 4371   94\n## [1] 4465   95\n## [1] 4560   96\n## [1] 4656   97\n## [1] 4753   98\n## [1] 4851   99\n## [1] 4950  100\nprint(sum)\n## [1] 5050\n\n\nnorow &lt;- nrow(iris)                             # iris의 행의 수\nmylabel &lt;- c()                                  # 비어 있는 벡터 선언\nfor (i in 1:norow) {\n    if (iris$Petal.Length[i] &lt;= 1.6) {          # 꽃잎의 길이에 따라 레이블 결정\n        mylabel[i] &lt;- 'L'\n    } else if (iris$Petal.Length[i] &gt;= 5.1) {\n        mylabel[i] &lt;- 'H'\n    } else {\n        mylabel[i] &lt;- 'M'\n    }\n    print(c(iris$Petal.Length[i], mylabel))\n}\n## [1] \"1.4\" \"L\"  \n## [1] \"1.4\" \"L\"   \"L\"  \n## [1] \"1.3\" \"L\"   \"L\"   \"L\"  \n## [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"  \n## [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [1] \"1.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"  \n## [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"  \n## [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"  \n##  [1] \"1.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"  \n##  [1] \"1.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"  \n##  [1] \"1.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"  \n##  [1] \"1\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\"\n##  [1] \"1.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"  \n##  [1] \"1.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"  \n##  [1] \"4.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"  \n##  [1] \"4.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\"\n##  [1] \"4.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"  \n##  [1] \"3.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"  \n##  [1] \"4.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"  \n##  [1] \"4\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [58] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [1] \"4.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [58] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [1] \"4.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"  \n##  [1] \"4.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"  \n##  [1] \"4.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"5\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [58] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [77] \"M\" \"M\" \"M\"\n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"  \n##  [1] \"4.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [58] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [77] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [1] \"4.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [58] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [77] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [1] \"3.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [97] \"M\"  \n##  [1] \"4.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [97] \"M\"   \"M\"  \n##  [1] \"4.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [97] \"M\"   \"M\"   \"M\"  \n##   [1] \"3\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##   [1] \"4.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##   [1] \"6\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\"\n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"  \n##   [1] \"5.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"6.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n##   [1] \"6.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"  \n##   [1] \"5.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"  \n##   [1] \"6.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"  \n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n## [109] \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"  \n##   [1] \"5.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"  \n##   [1] \"5.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"6.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"6.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n## [109] \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n##   [1] \"5.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"  \n##   [1] \"4.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"  \n##   [1] \"6.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"  \n##   [1] \"4.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"  \n##   [1] \"5.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"  \n##   [1] \"6\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n## [109] \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"M\" \"H\" \"M\" \"H\"\n## [127] \"H\"\n##   [1] \"4.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"  \n##   [1] \"4.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"  \n##   [1] \"5.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"  \n##   [1] \"6.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"6.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"  \n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"6.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"4.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n##   [1] \"5.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"  \n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"  \n##   [1] \"5.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"   \"H\"  \n##   [1] \"5.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"   \"H\"   \"H\"  \n##   [1] \"5\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n## [109] \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"M\" \"H\" \"M\" \"H\"\n## [127] \"H\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\"\n## [145] \"H\" \"H\" \"H\" \"M\"\n##   [1] \"5.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"   \"H\"   \"H\"   \"M\"   \"H\"  \n##   [1] \"5.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"  \n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"\nprint(mylabel)                                  # 레이블 출력\n##   [1] \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\"\n## [109] \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"M\" \"H\" \"M\" \"H\" \"H\"\n## [127] \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\"\n## [145] \"H\" \"H\" \"M\" \"H\" \"H\" \"H\"\nnewds &lt;- data.frame(iris$Petal.Length, mylabel) # 꽃잎의 길이와 레이블 결합\nhead(newds)                                     # 새로운 데이터셋 내용 출력\n##   iris.Petal.Length mylabel\n## 1               1.4       L\n## 2               1.4       L\n## 3               1.3       L\n## 4               1.5       L\n## 5               1.4       L\n## 6               1.7       M\n\n\nsum &lt;- 0\ni &lt;- 1\nwhile (i &lt;= 100) {\n    sum &lt;- sum + i      # sum에 i 값을 누적\n    i &lt;- i + 1          # i 값을 1 증가시킴\n    print(c(sum, i))\n}\n## [1] 1 2\n## [1] 3 3\n## [1] 6 4\n## [1] 10  5\n## [1] 15  6\n## [1] 21  7\n## [1] 28  8\n## [1] 36  9\n## [1] 45 10\n## [1] 55 11\n## [1] 66 12\n## [1] 78 13\n## [1] 91 14\n## [1] 105  15\n## [1] 120  16\n## [1] 136  17\n## [1] 153  18\n## [1] 171  19\n## [1] 190  20\n## [1] 210  21\n## [1] 231  22\n## [1] 253  23\n## [1] 276  24\n## [1] 300  25\n## [1] 325  26\n## [1] 351  27\n## [1] 378  28\n## [1] 406  29\n## [1] 435  30\n## [1] 465  31\n## [1] 496  32\n## [1] 528  33\n## [1] 561  34\n## [1] 595  35\n## [1] 630  36\n## [1] 666  37\n## [1] 703  38\n## [1] 741  39\n## [1] 780  40\n## [1] 820  41\n## [1] 861  42\n## [1] 903  43\n## [1] 946  44\n## [1] 990  45\n## [1] 1035   46\n## [1] 1081   47\n## [1] 1128   48\n## [1] 1176   49\n## [1] 1225   50\n## [1] 1275   51\n## [1] 1326   52\n## [1] 1378   53\n## [1] 1431   54\n## [1] 1485   55\n## [1] 1540   56\n## [1] 1596   57\n## [1] 1653   58\n## [1] 1711   59\n## [1] 1770   60\n## [1] 1830   61\n## [1] 1891   62\n## [1] 1953   63\n## [1] 2016   64\n## [1] 2080   65\n## [1] 2145   66\n## [1] 2211   67\n## [1] 2278   68\n## [1] 2346   69\n## [1] 2415   70\n## [1] 2485   71\n## [1] 2556   72\n## [1] 2628   73\n## [1] 2701   74\n## [1] 2775   75\n## [1] 2850   76\n## [1] 2926   77\n## [1] 3003   78\n## [1] 3081   79\n## [1] 3160   80\n## [1] 3240   81\n## [1] 3321   82\n## [1] 3403   83\n## [1] 3486   84\n## [1] 3570   85\n## [1] 3655   86\n## [1] 3741   87\n## [1] 3828   88\n## [1] 3916   89\n## [1] 4005   90\n## [1] 4095   91\n## [1] 4186   92\n## [1] 4278   93\n## [1] 4371   94\n## [1] 4465   95\n## [1] 4560   96\n## [1] 4656   97\n## [1] 4753   98\n## [1] 4851   99\n## [1] 4950  100\n## [1] 5050  101\nprint(sum)\n## [1] 5050\n\n#---------------------------------------#\n# 오류 없이 계속 실행됨\n# sum &lt;- 0\n# i &lt;- 1\n# while(i &gt;= 1) {\n#   sum &lt;- sum + i # sum에 i 값을 누적\n#   i &lt;- i + 1 # i 값을 1 증가시킴\n#   print(c(sum,i))\n# }\n# print(sum)\n#---------------------------------------#\n\n\nsum &lt;- 0\nfor (i in 1:10) {\n    sum &lt;- sum + i\n    print(c(sum, i))\n    if (i &gt;= 5)\n        break\n}\n## [1] 1 1\n## [1] 3 2\n## [1] 6 3\n## [1] 10  4\n## [1] 15  5\nsum\n## [1] 15\n\n\nsum &lt;- 0\nfor (i in 1:10) {\n    if (i %% 2 == 0)\n        next # %% = 나머지\n    sum &lt;- sum + i\n    print(c(sum, i))\n}\n## [1] 1 1\n## [1] 4 3\n## [1] 9 5\n## [1] 16  7\n## [1] 25  9\nsum\n## [1] 25\n\n\napply(iris[, 1:4], 1, mean) # row 방향으로 함수 적용\n##   [1] 2.550 2.375 2.350 2.350 2.550 2.850 2.425 2.525 2.225 2.400 2.700 2.500\n##  [13] 2.325 2.125 2.800 3.000 2.750 2.575 2.875 2.675 2.675 2.675 2.350 2.650\n##  [25] 2.575 2.450 2.600 2.600 2.550 2.425 2.425 2.675 2.725 2.825 2.425 2.400\n##  [37] 2.625 2.500 2.225 2.550 2.525 2.100 2.275 2.675 2.800 2.375 2.675 2.350\n##  [49] 2.675 2.475 4.075 3.900 4.100 3.275 3.850 3.575 3.975 2.900 3.850 3.300\n##  [61] 2.875 3.650 3.300 3.775 3.350 3.900 3.650 3.400 3.600 3.275 3.925 3.550\n##  [73] 3.800 3.700 3.725 3.850 3.950 4.100 3.725 3.200 3.200 3.150 3.400 3.850\n##  [85] 3.600 3.875 4.000 3.575 3.500 3.325 3.425 3.775 3.400 2.900 3.450 3.525\n##  [97] 3.525 3.675 2.925 3.475 4.525 3.875 4.525 4.150 4.375 4.825 3.400 4.575\n## [109] 4.200 4.850 4.200 4.075 4.350 3.800 4.025 4.300 4.200 5.100 4.875 3.675\n## [121] 4.525 3.825 4.800 3.925 4.450 4.550 3.900 3.950 4.225 4.400 4.550 5.025\n## [133] 4.250 3.925 3.925 4.775 4.425 4.200 3.900 4.375 4.450 4.350 3.875 4.550\n## [145] 4.550 4.300 3.925 4.175 4.325 3.950\napply(iris[, 1:4], 2, mean) # col 방향으로 함수 적용\n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n##     5.843333     3.057333     3.758000     1.199333\n\nresult &lt;- c()\nfor (i in 1:4) {\n    iris_col &lt;- iris[, i]\n    iris_col_mean_temp &lt;- mean(iris_col)\n    result &lt;- c(result, iris_col_mean_temp)\n}\nresult\n## [1] 5.843333 3.057333 3.758000 1.199333\n\n\nmymax &lt;- function(x, y) {\n    num.max &lt;- x\n    if (y &gt; x) {\n        num.max &lt;- y\n    }\n    return(num.max)\n}\n\n\nmymax(10, 15)\n## [1] 15\na &lt;- mymax(20, 15)\nb &lt;- mymax(31, 45)\nprint(a + b)\n## [1] 65\n\n\nmydiv &lt;- function(x, y = 2) {\n    result &lt;- x / y\n    return(result)\n}\n\nmydiv(x = 10, y = 3) # 매개변수 이름과 매개변수값을 쌍으로 입력\n## [1] 3.333333\nmydiv(10, 3) # 매개변수값만 입력\n## [1] 3.333333\nmydiv(10) # x에 대한 값만 입력(y 값이 생략됨)\n## [1] 5\n\n\nmyfunc &lt;- function(x, y) {\n    val.sum &lt;- x + y\n    val.mul &lt;- x * y\n    return(list(sum = val.sum, mul = val.mul))\n}\n\nresult &lt;- myfunc(5, 8)\nresult\n## $sum\n## [1] 13\n## \n## $mul\n## [1] 40\ns &lt;- result$sum # 5, 8의 합\nm &lt;- result$mul # 5, 8의 곱\ncat('5+8=', s, '\\n')\n## 5+8= 13\ncat('5*8=', m, '\\n')\n## 5*8= 40\n\n\ngetwd()\n## [1] \"C:/Users/Hyunsoo Kim/Desktop/senior_grade/blog/my-quarto-website\"\n# source(\"myfunc.R\") # myfunc.R 안에 있는 함수 실행\n\na &lt;- mydiv(20, 4) # 함수 호출\nb &lt;- mydiv(30, 4) # 함수 호출\na + b\n## [1] 12.5\nmydiv(mydiv(20, 2), 5) # 함수 호출\n## [1] 2\n\n\nscore &lt;- c(76, 84, 69, 50, 95, 60, 82, 71, 88, 84)\nwhich(score == 69) # 성적이 69인 학생은 몇 번째에 있나\n## [1] 3\nwhich(score &gt;= 85) # 성적이 85 이상인 학생은 몇 번째에 있나\n## [1] 5 9\n\nmax(score) # 최고 점수는 몇 점인가\n## [1] 95\nwhich.max(score) # 최고 점수는 몇 번째에 있나\n## [1] 5\nscore[which.max(score)] # 최고 점수는 몇 점인가\n## [1] 95\n\nmin(score) # 최저 점수는 몇 점인가\n## [1] 50\nwhich.min(score) # 최저 점수는 몇 번째에 있나\n## [1] 4\nscore[which.min(score)] # 최저 점수는 몇 점인가\n## [1] 50\n\n\nscore &lt;- c(76, 84, 69, 50, 95, 60, 82, 71, 88, 84)\nidx &lt;- which(score &lt;= 60) # 성적이 60 이하인 값들의 인덱스\nidx\n## [1] 4 6\nscore[idx]\n## [1] 50 60\nscore[idx] &lt;- 61 # 성적이 60 이하인 값들은 61점으로 성적 상향 조정\nscore # 상향 조정된 성적 확인\n##  [1] 76 84 69 61 95 61 82 71 88 84\n\nidx &lt;- which(score &gt;= 80) # 성적이 80 이상인 값들의 인덱스\nidx\n## [1]  2  5  7  9 10\nscore[idx]\n## [1] 84 95 82 88 84\nscore.high &lt;- score[idx] # 성적이 80 이상인 값들만 추출하여 저장\nscore.high # score.high의 내용 확인\n## [1] 84 95 82 88 84\n\n\niris\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 1            5.1         3.5          1.4         0.2     setosa\n## 2            4.9         3.0          1.4         0.2     setosa\n## 3            4.7         3.2          1.3         0.2     setosa\n## 4            4.6         3.1          1.5         0.2     setosa\n## 5            5.0         3.6          1.4         0.2     setosa\n## 6            5.4         3.9          1.7         0.4     setosa\n## 7            4.6         3.4          1.4         0.3     setosa\n## 8            5.0         3.4          1.5         0.2     setosa\n## 9            4.4         2.9          1.4         0.2     setosa\n## 10           4.9         3.1          1.5         0.1     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 12           4.8         3.4          1.6         0.2     setosa\n## 13           4.8         3.0          1.4         0.1     setosa\n## 14           4.3         3.0          1.1         0.1     setosa\n## 15           5.8         4.0          1.2         0.2     setosa\n## 16           5.7         4.4          1.5         0.4     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 23           4.6         3.6          1.0         0.2     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 25           4.8         3.4          1.9         0.2     setosa\n## 26           5.0         3.0          1.6         0.2     setosa\n## 27           5.0         3.4          1.6         0.4     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 30           4.7         3.2          1.6         0.2     setosa\n## 31           4.8         3.1          1.6         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 34           5.5         4.2          1.4         0.2     setosa\n## 35           4.9         3.1          1.5         0.2     setosa\n## 36           5.0         3.2          1.2         0.2     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 38           4.9         3.6          1.4         0.1     setosa\n## 39           4.4         3.0          1.3         0.2     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 41           5.0         3.5          1.3         0.3     setosa\n## 42           4.5         2.3          1.3         0.3     setosa\n## 43           4.4         3.2          1.3         0.2     setosa\n## 44           5.0         3.5          1.6         0.6     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\n## 46           4.8         3.0          1.4         0.3     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 48           4.6         3.2          1.4         0.2     setosa\n## 49           5.3         3.7          1.5         0.2     setosa\n## 50           5.0         3.3          1.4         0.2     setosa\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 107          4.9         2.5          4.5         1.7  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 114          5.7         2.5          5.0         2.0  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 120          6.0         2.2          5.0         1.5  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 122          5.6         2.8          4.9         2.0  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 127          6.2         2.8          4.8         1.8  virginica\n## 128          6.1         3.0          4.9         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\niris$Petal.Length\n##   [1] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 1.5 1.6 1.4 1.1 1.2 1.5 1.3 1.4\n##  [19] 1.7 1.5 1.7 1.5 1.0 1.7 1.9 1.6 1.6 1.5 1.4 1.6 1.6 1.5 1.5 1.4 1.5 1.2\n##  [37] 1.3 1.4 1.3 1.5 1.3 1.3 1.3 1.6 1.9 1.4 1.6 1.4 1.5 1.4 4.7 4.5 4.9 4.0\n##  [55] 4.6 4.5 4.7 3.3 4.6 3.9 3.5 4.2 4.0 4.7 3.6 4.4 4.5 4.1 4.5 3.9 4.8 4.0\n##  [73] 4.9 4.7 4.3 4.4 4.8 5.0 4.5 3.5 3.8 3.7 3.9 5.1 4.5 4.5 4.7 4.4 4.1 4.0\n##  [91] 4.4 4.6 4.0 3.3 4.2 4.2 4.2 4.3 3.0 4.1 6.0 5.1 5.9 5.6 5.8 6.6 4.5 6.3\n## [109] 5.8 6.1 5.1 5.3 5.5 5.0 5.1 5.3 5.5 6.7 6.9 5.0 5.7 4.9 6.7 4.9 5.7 6.0\n## [127] 4.8 4.9 5.6 5.8 6.1 6.4 5.6 5.1 5.6 6.1 5.6 5.5 4.8 5.4 5.6 5.1 5.1 5.9\n## [145] 5.7 5.2 5.0 5.2 5.4 5.1\niris$Petal.Length &gt; 5.0\n##   [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n##  [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [97] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n## [109]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n## [121]  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n## [133]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n## [145]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\nwhich(iris$Petal.Length &gt; 5.0)\n##  [1]  84 101 102 103 104 105 106 108 109 110 111 112 113 115 116 117 118 119 121\n## [20] 123 125 126 129 130 131 132 133 134 135 136 137 138 140 141 142 143 144 145\n## [39] 146 148 149 150\n\niris$Petal.Length[iris$Petal.Length &gt; 5.0]\n##  [1] 5.1 6.0 5.1 5.9 5.6 5.8 6.6 6.3 5.8 6.1 5.1 5.3 5.5 5.1 5.3 5.5 6.7 6.9 5.7\n## [20] 6.7 5.7 6.0 5.6 5.8 6.1 6.4 5.6 5.1 5.6 6.1 5.6 5.5 5.4 5.6 5.1 5.1 5.9 5.7\n## [39] 5.2 5.2 5.4 5.1\n\nidx &lt;- which(iris$Petal.Length &gt; 5.0) # 꽃잎의 길이가 5.0 이상인 값들의 인덱스\nidx\n##  [1]  84 101 102 103 104 105 106 108 109 110 111 112 113 115 116 117 118 119 121\n## [20] 123 125 126 129 130 131 132 133 134 135 136 137 138 140 141 142 143 144 145\n## [39] 146 148 149 150\niris.big &lt;- iris[idx, ] # 인덱스에 해당하는 값만 추출하여 저장\niris.big\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\n\n\n# 1~4열의 값 중 5보다 큰 값의 행과 열의 위치\nwhich(iris[, 1:4] &gt; 5.0)\n##   [1]   1   6  11  15  16  17  18  19  20  21  22  24  28  29  32  33  34  37\n##  [19]  40  45  47  49  51  52  53  54  55  56  57  59  60  62  63  64  65  66\n##  [37]  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84\n##  [55]  85  86  87  88  89  90  91  92  93  95  96  97  98  99 100 101 102 103\n##  [73] 104 105 106 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122\n##  [91] 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140\n## [109] 141 142 143 144 145 146 147 148 149 150 384 401 402 403 404 405 406 408\n## [127] 409 410 411 412 413 415 416 417 418 419 421 423 425 426 429 430 431 432\n## [145] 433 434 435 436 437 438 440 441 442 443 444 445 446 448 449 450\nwhich(iris[, 1:4] &gt; 5.0, arr.ind = TRUE) # arr.ind = TRUE : 조건에 맞는 인덱스까지 반환\n##        row col\n##   [1,]   1   1\n##   [2,]   6   1\n##   [3,]  11   1\n##   [4,]  15   1\n##   [5,]  16   1\n##   [6,]  17   1\n##   [7,]  18   1\n##   [8,]  19   1\n##   [9,]  20   1\n##  [10,]  21   1\n##  [11,]  22   1\n##  [12,]  24   1\n##  [13,]  28   1\n##  [14,]  29   1\n##  [15,]  32   1\n##  [16,]  33   1\n##  [17,]  34   1\n##  [18,]  37   1\n##  [19,]  40   1\n##  [20,]  45   1\n##  [21,]  47   1\n##  [22,]  49   1\n##  [23,]  51   1\n##  [24,]  52   1\n##  [25,]  53   1\n##  [26,]  54   1\n##  [27,]  55   1\n##  [28,]  56   1\n##  [29,]  57   1\n##  [30,]  59   1\n##  [31,]  60   1\n##  [32,]  62   1\n##  [33,]  63   1\n##  [34,]  64   1\n##  [35,]  65   1\n##  [36,]  66   1\n##  [37,]  67   1\n##  [38,]  68   1\n##  [39,]  69   1\n##  [40,]  70   1\n##  [41,]  71   1\n##  [42,]  72   1\n##  [43,]  73   1\n##  [44,]  74   1\n##  [45,]  75   1\n##  [46,]  76   1\n##  [47,]  77   1\n##  [48,]  78   1\n##  [49,]  79   1\n##  [50,]  80   1\n##  [51,]  81   1\n##  [52,]  82   1\n##  [53,]  83   1\n##  [54,]  84   1\n##  [55,]  85   1\n##  [56,]  86   1\n##  [57,]  87   1\n##  [58,]  88   1\n##  [59,]  89   1\n##  [60,]  90   1\n##  [61,]  91   1\n##  [62,]  92   1\n##  [63,]  93   1\n##  [64,]  95   1\n##  [65,]  96   1\n##  [66,]  97   1\n##  [67,]  98   1\n##  [68,]  99   1\n##  [69,] 100   1\n##  [70,] 101   1\n##  [71,] 102   1\n##  [72,] 103   1\n##  [73,] 104   1\n##  [74,] 105   1\n##  [75,] 106   1\n##  [76,] 108   1\n##  [77,] 109   1\n##  [78,] 110   1\n##  [79,] 111   1\n##  [80,] 112   1\n##  [81,] 113   1\n##  [82,] 114   1\n##  [83,] 115   1\n##  [84,] 116   1\n##  [85,] 117   1\n##  [86,] 118   1\n##  [87,] 119   1\n##  [88,] 120   1\n##  [89,] 121   1\n##  [90,] 122   1\n##  [91,] 123   1\n##  [92,] 124   1\n##  [93,] 125   1\n##  [94,] 126   1\n##  [95,] 127   1\n##  [96,] 128   1\n##  [97,] 129   1\n##  [98,] 130   1\n##  [99,] 131   1\n## [100,] 132   1\n## [101,] 133   1\n## [102,] 134   1\n## [103,] 135   1\n## [104,] 136   1\n## [105,] 137   1\n## [106,] 138   1\n## [107,] 139   1\n## [108,] 140   1\n## [109,] 141   1\n## [110,] 142   1\n## [111,] 143   1\n## [112,] 144   1\n## [113,] 145   1\n## [114,] 146   1\n## [115,] 147   1\n## [116,] 148   1\n## [117,] 149   1\n## [118,] 150   1\n## [119,]  84   3\n## [120,] 101   3\n## [121,] 102   3\n## [122,] 103   3\n## [123,] 104   3\n## [124,] 105   3\n## [125,] 106   3\n## [126,] 108   3\n## [127,] 109   3\n## [128,] 110   3\n## [129,] 111   3\n## [130,] 112   3\n## [131,] 113   3\n## [132,] 115   3\n## [133,] 116   3\n## [134,] 117   3\n## [135,] 118   3\n## [136,] 119   3\n## [137,] 121   3\n## [138,] 123   3\n## [139,] 125   3\n## [140,] 126   3\n## [141,] 129   3\n## [142,] 130   3\n## [143,] 131   3\n## [144,] 132   3\n## [145,] 133   3\n## [146,] 134   3\n## [147,] 135   3\n## [148,] 136   3\n## [149,] 137   3\n## [150,] 138   3\n## [151,] 140   3\n## [152,] 141   3\n## [153,] 142   3\n## [154,] 143   3\n## [155,] 144   3\n## [156,] 145   3\n## [157,] 146   3\n## [158,] 148   3\n## [159,] 149   3\n## [160,] 150   3\n\nidx &lt;- which(iris[, 1:4] &gt; 5.0, arr.ind = TRUE)\niris[idx[, 1], ]\n##       Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 1              5.1         3.5          1.4         0.2     setosa\n## 6              5.4         3.9          1.7         0.4     setosa\n## 11             5.4         3.7          1.5         0.2     setosa\n## 15             5.8         4.0          1.2         0.2     setosa\n## 16             5.7         4.4          1.5         0.4     setosa\n## 17             5.4         3.9          1.3         0.4     setosa\n## 18             5.1         3.5          1.4         0.3     setosa\n## 19             5.7         3.8          1.7         0.3     setosa\n## 20             5.1         3.8          1.5         0.3     setosa\n## 21             5.4         3.4          1.7         0.2     setosa\n## 22             5.1         3.7          1.5         0.4     setosa\n## 24             5.1         3.3          1.7         0.5     setosa\n## 28             5.2         3.5          1.5         0.2     setosa\n## 29             5.2         3.4          1.4         0.2     setosa\n## 32             5.4         3.4          1.5         0.4     setosa\n## 33             5.2         4.1          1.5         0.1     setosa\n## 34             5.5         4.2          1.4         0.2     setosa\n## 37             5.5         3.5          1.3         0.2     setosa\n## 40             5.1         3.4          1.5         0.2     setosa\n## 45             5.1         3.8          1.9         0.4     setosa\n## 47             5.1         3.8          1.6         0.2     setosa\n## 49             5.3         3.7          1.5         0.2     setosa\n## 51             7.0         3.2          4.7         1.4 versicolor\n## 52             6.4         3.2          4.5         1.5 versicolor\n## 53             6.9         3.1          4.9         1.5 versicolor\n## 54             5.5         2.3          4.0         1.3 versicolor\n## 55             6.5         2.8          4.6         1.5 versicolor\n## 56             5.7         2.8          4.5         1.3 versicolor\n## 57             6.3         3.3          4.7         1.6 versicolor\n## 59             6.6         2.9          4.6         1.3 versicolor\n## 60             5.2         2.7          3.9         1.4 versicolor\n## 62             5.9         3.0          4.2         1.5 versicolor\n## 63             6.0         2.2          4.0         1.0 versicolor\n## 64             6.1         2.9          4.7         1.4 versicolor\n## 65             5.6         2.9          3.6         1.3 versicolor\n## 66             6.7         3.1          4.4         1.4 versicolor\n## 67             5.6         3.0          4.5         1.5 versicolor\n## 68             5.8         2.7          4.1         1.0 versicolor\n## 69             6.2         2.2          4.5         1.5 versicolor\n## 70             5.6         2.5          3.9         1.1 versicolor\n## 71             5.9         3.2          4.8         1.8 versicolor\n## 72             6.1         2.8          4.0         1.3 versicolor\n## 73             6.3         2.5          4.9         1.5 versicolor\n## 74             6.1         2.8          4.7         1.2 versicolor\n## 75             6.4         2.9          4.3         1.3 versicolor\n## 76             6.6         3.0          4.4         1.4 versicolor\n## 77             6.8         2.8          4.8         1.4 versicolor\n## 78             6.7         3.0          5.0         1.7 versicolor\n## 79             6.0         2.9          4.5         1.5 versicolor\n## 80             5.7         2.6          3.5         1.0 versicolor\n## 81             5.5         2.4          3.8         1.1 versicolor\n## 82             5.5         2.4          3.7         1.0 versicolor\n## 83             5.8         2.7          3.9         1.2 versicolor\n## 84             6.0         2.7          5.1         1.6 versicolor\n## 85             5.4         3.0          4.5         1.5 versicolor\n## 86             6.0         3.4          4.5         1.6 versicolor\n## 87             6.7         3.1          4.7         1.5 versicolor\n## 88             6.3         2.3          4.4         1.3 versicolor\n## 89             5.6         3.0          4.1         1.3 versicolor\n## 90             5.5         2.5          4.0         1.3 versicolor\n## 91             5.5         2.6          4.4         1.2 versicolor\n## 92             6.1         3.0          4.6         1.4 versicolor\n## 93             5.8         2.6          4.0         1.2 versicolor\n## 95             5.6         2.7          4.2         1.3 versicolor\n## 96             5.7         3.0          4.2         1.2 versicolor\n## 97             5.7         2.9          4.2         1.3 versicolor\n## 98             6.2         2.9          4.3         1.3 versicolor\n## 99             5.1         2.5          3.0         1.1 versicolor\n## 100            5.7         2.8          4.1         1.3 versicolor\n## 101            6.3         3.3          6.0         2.5  virginica\n## 102            5.8         2.7          5.1         1.9  virginica\n## 103            7.1         3.0          5.9         2.1  virginica\n## 104            6.3         2.9          5.6         1.8  virginica\n## 105            6.5         3.0          5.8         2.2  virginica\n## 106            7.6         3.0          6.6         2.1  virginica\n## 108            7.3         2.9          6.3         1.8  virginica\n## 109            6.7         2.5          5.8         1.8  virginica\n## 110            7.2         3.6          6.1         2.5  virginica\n## 111            6.5         3.2          5.1         2.0  virginica\n## 112            6.4         2.7          5.3         1.9  virginica\n## 113            6.8         3.0          5.5         2.1  virginica\n## 114            5.7         2.5          5.0         2.0  virginica\n## 115            5.8         2.8          5.1         2.4  virginica\n## 116            6.4         3.2          5.3         2.3  virginica\n## 117            6.5         3.0          5.5         1.8  virginica\n## 118            7.7         3.8          6.7         2.2  virginica\n## 119            7.7         2.6          6.9         2.3  virginica\n## 120            6.0         2.2          5.0         1.5  virginica\n## 121            6.9         3.2          5.7         2.3  virginica\n## 122            5.6         2.8          4.9         2.0  virginica\n## 123            7.7         2.8          6.7         2.0  virginica\n## 124            6.3         2.7          4.9         1.8  virginica\n## 125            6.7         3.3          5.7         2.1  virginica\n## 126            7.2         3.2          6.0         1.8  virginica\n## 127            6.2         2.8          4.8         1.8  virginica\n## 128            6.1         3.0          4.9         1.8  virginica\n## 129            6.4         2.8          5.6         2.1  virginica\n## 130            7.2         3.0          5.8         1.6  virginica\n## 131            7.4         2.8          6.1         1.9  virginica\n## 132            7.9         3.8          6.4         2.0  virginica\n## 133            6.4         2.8          5.6         2.2  virginica\n## 134            6.3         2.8          5.1         1.5  virginica\n## 135            6.1         2.6          5.6         1.4  virginica\n## 136            7.7         3.0          6.1         2.3  virginica\n## 137            6.3         3.4          5.6         2.4  virginica\n## 138            6.4         3.1          5.5         1.8  virginica\n## 139            6.0         3.0          4.8         1.8  virginica\n## 140            6.9         3.1          5.4         2.1  virginica\n## 141            6.7         3.1          5.6         2.4  virginica\n## 142            6.9         3.1          5.1         2.3  virginica\n## 143            5.8         2.7          5.1         1.9  virginica\n## 144            6.8         3.2          5.9         2.3  virginica\n## 145            6.7         3.3          5.7         2.5  virginica\n## 146            6.7         3.0          5.2         2.3  virginica\n## 147            6.3         2.5          5.0         1.9  virginica\n## 148            6.5         3.0          5.2         2.0  virginica\n## 149            6.2         3.4          5.4         2.3  virginica\n## 150            5.9         3.0          5.1         1.8  virginica\n## 84.1           6.0         2.7          5.1         1.6 versicolor\n## 101.1          6.3         3.3          6.0         2.5  virginica\n## 102.1          5.8         2.7          5.1         1.9  virginica\n## 103.1          7.1         3.0          5.9         2.1  virginica\n## 104.1          6.3         2.9          5.6         1.8  virginica\n## 105.1          6.5         3.0          5.8         2.2  virginica\n## 106.1          7.6         3.0          6.6         2.1  virginica\n## 108.1          7.3         2.9          6.3         1.8  virginica\n## 109.1          6.7         2.5          5.8         1.8  virginica\n## 110.1          7.2         3.6          6.1         2.5  virginica\n## 111.1          6.5         3.2          5.1         2.0  virginica\n## 112.1          6.4         2.7          5.3         1.9  virginica\n## 113.1          6.8         3.0          5.5         2.1  virginica\n## 115.1          5.8         2.8          5.1         2.4  virginica\n## 116.1          6.4         3.2          5.3         2.3  virginica\n## 117.1          6.5         3.0          5.5         1.8  virginica\n## 118.1          7.7         3.8          6.7         2.2  virginica\n## 119.1          7.7         2.6          6.9         2.3  virginica\n## 121.1          6.9         3.2          5.7         2.3  virginica\n## 123.1          7.7         2.8          6.7         2.0  virginica\n## 125.1          6.7         3.3          5.7         2.1  virginica\n## 126.1          7.2         3.2          6.0         1.8  virginica\n## 129.1          6.4         2.8          5.6         2.1  virginica\n## 130.1          7.2         3.0          5.8         1.6  virginica\n## 131.1          7.4         2.8          6.1         1.9  virginica\n## 132.1          7.9         3.8          6.4         2.0  virginica\n## 133.1          6.4         2.8          5.6         2.2  virginica\n## 134.1          6.3         2.8          5.1         1.5  virginica\n## 135.1          6.1         2.6          5.6         1.4  virginica\n## 136.1          7.7         3.0          6.1         2.3  virginica\n## 137.1          6.3         3.4          5.6         2.4  virginica\n## 138.1          6.4         3.1          5.5         1.8  virginica\n## 140.1          6.9         3.1          5.4         2.1  virginica\n## 141.1          6.7         3.1          5.6         2.4  virginica\n## 142.1          6.9         3.1          5.1         2.3  virginica\n## 143.1          5.8         2.7          5.1         1.9  virginica\n## 144.1          6.8         3.2          5.9         2.3  virginica\n## 145.1          6.7         3.3          5.7         2.5  virginica\n## 146.1          6.7         3.0          5.2         2.3  virginica\n## 148.1          6.5         3.0          5.2         2.0  virginica\n## 149.1          6.2         3.4          5.4         2.3  virginica\n## 150.1          5.9         3.0          5.1         1.8  virginica\n\niris[, 1:4][idx]\n##   [1] 5.1 5.4 5.4 5.8 5.7 5.4 5.1 5.7 5.1 5.4 5.1 5.1 5.2 5.2 5.4 5.2 5.5 5.5\n##  [19] 5.1 5.1 5.1 5.3 7.0 6.4 6.9 5.5 6.5 5.7 6.3 6.6 5.2 5.9 6.0 6.1 5.6 6.7\n##  [37] 5.6 5.8 6.2 5.6 5.9 6.1 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7 5.5 5.5 5.8 6.0\n##  [55] 5.4 6.0 6.7 6.3 5.6 5.5 5.5 6.1 5.8 5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 7.1\n##  [73] 6.3 6.5 7.6 7.3 6.7 7.2 6.5 6.4 6.8 5.7 5.8 6.4 6.5 7.7 7.7 6.0 6.9 5.6\n##  [91] 7.7 6.3 6.7 7.2 6.2 6.1 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 6.3 6.4 6.0 6.9\n## [109] 6.7 6.9 5.8 6.8 6.7 6.7 6.3 6.5 6.2 5.9 5.1 6.0 5.1 5.9 5.6 5.8 6.6 6.3\n## [127] 5.8 6.1 5.1 5.3 5.5 5.1 5.3 5.5 6.7 6.9 5.7 6.7 5.7 6.0 5.6 5.8 6.1 6.4\n## [145] 5.6 5.1 5.6 6.1 5.6 5.5 5.4 5.6 5.1 5.1 5.9 5.7 5.2 5.2 5.4 5.1"
  },
  {
    "objectID": "R_Basic.html#장.-단일변수-자료의-탐색",
    "href": "R_Basic.html#장.-단일변수-자료의-탐색",
    "title": "R Basic",
    "section": "",
    "text": "favorite &lt;- c('WINTER', 'SUMMER', 'SPRING', 'SUMMER', 'SUMMER',\n              'FALL', 'FALL', 'SUMMER', 'SPRING', 'SPRING')\nfavorite # favorite의 내용 출력\n##  [1] \"WINTER\" \"SUMMER\" \"SPRING\" \"SUMMER\" \"SUMMER\" \"FALL\"   \"FALL\"   \"SUMMER\"\n##  [9] \"SPRING\" \"SPRING\"\ntable(favorite) # 도수분포표 계산\n## favorite\n##   FALL SPRING SUMMER WINTER \n##      2      3      4      1\nlength(favorite)\n## [1] 10\ntable(favorite) / length(favorite) # 비율 출력\n## favorite\n##   FALL SPRING SUMMER WINTER \n##    0.2    0.3    0.4    0.1\n\n\nds &lt;- table(favorite)\nds\n## favorite\n##   FALL SPRING SUMMER WINTER \n##      2      3      4      1\nbarplot(ds, main = 'favorite season')\n\n\n\n\n\nds &lt;- table(favorite)\nds\n## favorite\n##   FALL SPRING SUMMER WINTER \n##      2      3      4      1\npie(ds, main = 'favorite season')\n\n\n\n\n\nfavorite.color &lt;- c(2, 3, 2, 1, 1, 2, 2, 1, 3, 2, 1, 3, 2, 1, 2)\nds &lt;- table(favorite.color)\nds\n## favorite.color\n## 1 2 3 \n## 5 7 3\nbarplot(ds, main = 'favorite color')\n\n\n\ncolors &lt;- c('green', 'red', 'blue')\nnames(ds) &lt;- colors # 자료값 1, 2, 3을 green, red, blue로 변경\nds\n## green   red  blue \n##     5     7     3\nbarplot(ds, main = 'favorite color', col = colors) # 색 지정 막대그래프\n\n\n\nbarplot(ds, main = 'favorite color', col = c('green', 'red', 'blue'))\npie(ds, main = 'favorite color', col = colors) # 색 지정 원그래프\n\n\n\n\n\nweight &lt;- c(60, 62, 64, 65, 68, 69)\nweight.heavy &lt;- c(weight, 120)\nweight\n## [1] 60 62 64 65 68 69\nweight.heavy\n## [1]  60  62  64  65  68  69 120\n\nmean(weight) # 평균\n## [1] 64.66667\nmean(weight.heavy) # 평균\n## [1] 72.57143\n\nmedian(weight) # 중앙값\n## [1] 64.5\nmedian(weight.heavy) # 중앙값\n## [1] 65\n\nmean(weight, trim = 0.2) # 절사평균(상하위 20% 제외)\n## [1] 64.75\nmean(weight.heavy, trim = 0.2) # 절사평균(상하위 20% 제외)\n## [1] 65.6\n\n\nmydata &lt;- c(60, 62, 64, 65, 68, 69, 120)\nquantile(mydata)\n##    0%   25%   50%   75%  100% \n##  60.0  63.0  65.0  68.5 120.0\nquantile(mydata, (0:10) / 10) # 10% 단위로 구간을 나누어 계산\n##    0%   10%   20%   30%   40%   50%   60%   70%   80%   90%  100% \n##  60.0  61.2  62.4  63.6  64.4  65.0  66.8  68.2  68.8  89.4 120.0\nsummary(mydata) # 최소값, 중앙값, 평균값, 3분위 값, 최대값\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   60.00   63.00   65.00   72.57   68.50  120.00\n\nmydata &lt;- 0:1000\nquantile(mydata)\n##   0%  25%  50%  75% 100% \n##    0  250  500  750 1000\nquantile(mydata, (0:10) / 10)\n##   0%  10%  20%  30%  40%  50%  60%  70%  80%  90% 100% \n##    0  100  200  300  400  500  600  700  800  900 1000\nsummary(mydata)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##       0     250     500     500     750    1000\n?quantile\n## starting httpd help server ... done\n\n\nmydata &lt;- c(60, 62, 64, 65, 68, 69, 120)\nvar(mydata) # 분산\n## [1] 447.2857\nsd(mydata) # 표준편차\n## [1] 21.14913\nrange(mydata) # 값의 범위\n## [1]  60 120\ndiff(range(mydata)) # 최대값, 최소값의 차이\n## [1] 60\n\n\ndist &lt;- cars[, 2] # 자동차 제동거리\nhist(dist,                            # 자료(data)\n     main = \"Histogram for 제동거리\", # 제목\n     xlab = \"제동거리\",               # x축 레이블\n     ylab = \"빈도수\",                 # y축 레이블\n     border = \"blue\",                 # 막대 테두리색\n     col = rainbow(10),               # 막대 색\n     las = 2,                         # x축 글씨 방향(0~3)\n     breaks = seq(0, 120, 10))        # 막대 개수 조절\n\n\n\n\n\ndist &lt;- cars[,2] # 자동차 제동거리(단위: 피트(ft))\nboxplot(dist, main = \"자동차 제동거리\") # ★★★★★\n\n\n\n\n\nboxplot.stats(dist)\n## $stats\n## [1]  2 26 36 56 93\n## \n## $n\n## [1] 50\n## \n## $conf\n## [1] 29.29663 42.70337\n## \n## $out\n## [1] 120\nboxplot.stats(dist)$stats\n## [1]  2 26 36 56 93\nboxplot.stats(dist)$stats[4]\n## [1] 56\n\n\nboxplot(Petal.Length ~ Species, data = iris, main = \"품종별 꽃잎의 길이\")\n\n\n\n\npar(mfrow = c(1, 3)) # 1*3 가상화면 분할\n\nbarplot(\n    table(mtcars$carb),\n    main = \"Barplot of Carburetors\",\n    xlab = \"#of carburetors\",\n    ylab = \"frequency\",\n    col = \"blue\"\n)\n\nbarplot(\n    table(mtcars$cyl),\n    main = \"Barplot of Cylender\",\n    xlab = \"#of cylender\",\n    ylab = \"frequency\",\n    col = \"red\"\n)\n\nbarplot(\n    table(mtcars$gear),\n    main = \"Barplot of Grar\",\n    xlab = \"#of gears\",\n    ylab = \"frequency\",\n    col = \"green\"\n)\n\n\n\n\npar(mfrow = c(1, 1)) # 가상화면 분할 해제"
  },
  {
    "objectID": "R_Basic.html#장.-다중변수-자료의-탐색",
    "href": "R_Basic.html#장.-다중변수-자료의-탐색",
    "title": "R Basic",
    "section": "",
    "text": "wt &lt;- mtcars$wt                 # 중량 자료\nmpg &lt;- mtcars$mpg               # 연비 자료\nplot(wt, mpg,                   # 2개 변수(x축, y축)\n     main = \"중량-연비 그래프\", # 제목\n     xlab = \"중량\",             # x축 레이블\n     ylab = \"연비(MPG)\",        # y축 레이블\n     col = \"red\",               # point의 color\n     pch = 11)                  # point의 종류\n\n\n\n\n\nvars &lt;- c(\"mpg\", \"disp\", \"drat\", \"wt\") # 대상 변수(연비, 배기량, 후방차측 비율, 중량)\ntarget &lt;- mtcars[, vars]\nhead(target)\n##                    mpg disp drat    wt\n## Mazda RX4         21.0  160 3.90 2.620\n## Mazda RX4 Wag     21.0  160 3.90 2.875\n## Datsun 710        22.8  108 3.85 2.320\n## Hornet 4 Drive    21.4  258 3.08 3.215\n## Hornet Sportabout 18.7  360 3.15 3.440\n## Valiant           18.1  225 2.76 3.460\npairs(target, main = \"Multi Plots\")    # 대상 데이터\n\n\n\n\n\niris.2 &lt;- iris[, 3:4]              # 데이터 준비\npoint &lt;- as.numeric(iris$Species)  # 점의 모양\npoint                              # point 내용 출력\n##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n##  [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n##  [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3\n## [112] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n## [149] 3 3\ncolor &lt;- c(\"red\", \"green\", \"blue\") # 점의 컬러\nplot(iris.2,\n     main = \"Iris plot\",\n     pch = c(point),\n     col = color[point])\n\n\n\n\n\nbeers = c(5, 2, 9, 8, 3, 7, 3, 5, 3, 5) # 자료 입력\nbal &lt;- c(0.1, 0.03, 0.19, 0.12, 0.04, 0.0095, 0.07, 0.06, 0.02, 0.05)\ntbl &lt;- data.frame(beers, bal)           # 데이터프레임 생성\ntbl\n##    beers    bal\n## 1      5 0.1000\n## 2      2 0.0300\n## 3      9 0.1900\n## 4      8 0.1200\n## 5      3 0.0400\n## 6      7 0.0095\n## 7      3 0.0700\n## 8      5 0.0600\n## 9      3 0.0200\n## 10     5 0.0500\nplot(bal ~ beers, data = tbl)           # 산점도 plot(beers, bal)\nres &lt;- lm(bal ~ beers, data = tbl)      # 회귀식 도출\nabline(res)                             # 회귀선 그리기\n\n\n\ncor(beers, bal)                         # 상관계수 계산\n## [1] 0.6797025\n\n\ncor(iris[, 1:4]) # 4개 변수 간 상관성 분석\n##              Sepal.Length Sepal.Width Petal.Length Petal.Width\n## Sepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411\n## Sepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259\n## Petal.Length    0.8717538  -0.4284401    1.0000000   0.9628654\n## Petal.Width     0.8179411  -0.3661259    0.9628654   1.0000000\n\n\nmonth = 1:12 # 자료 입력\nlate = c(5, 8, 7, 9, 4, 6, 12, 13, 8, 6, 6, 4) # 자료 입력\nplot(month,                # x data\n     late,                 # y data\n     main = \"지각생 통계\", # 제목\n     type = \"l\",           # 그래프의 종류 선택(알파벳)\n     lty = 1,              # 선의 종류(line type) 선택\n     lwd = 1,              # 선의 굵기 선택\n     xlab = \"Month\",       # x축 레이블\n     ylab = \"Late cnt\")    # y축 레이블\n\n\n\n\n\nmonth = 1:12\nlate1 = c(5, 8, 7, 9, 4, 6, 12, 13, 8, 6, 6, 4)\nlate2 = c(4, 6, 5, 8, 7, 8, 10, 11, 6, 5, 7, 3)\nplot(month,                  # x data\n     late1,                  # y data\n     main = \"Late Students\",\n     type = \"b\",             # 그래프의 종류 선택(알파벳)\n     lty = 1,                # 선의 종류(line type) 선택\n     col = \"red\",            # 선의 색 선택\n     xlab = \"Month\",         # x축 레이블\n     ylab = \"Late cnt\",      # y축 레이블\n     ylim = c(1, 15))        # y축 값의 (하한, 상한)\n\nlines(month,                 # x data\n      late2,                 # y data\n      type = \"b\",            # 선의 종류(line type) 선택\n      col = \"blue\")          # 선의 색 선택\n\n\n\n\n\n## (1) 분석 대상 데이터셋 준비\n# install.packages(\"mlbench\")\nlibrary(mlbench)\ndata(\"BostonHousing\")\nmyds &lt;- BostonHousing[, c(\"crim\", \"rm\", \"dis\", \"tax\", \"medv\")]\n\n## (2) grp 변수 추가 ★★★★★\ngrp &lt;- c()\nfor (i in 1:nrow(myds)) {\n    # myds$medv 값에 따라 그룹 분류\n    if (myds$medv[i] &gt;= 25.0) {\n        grp[i] &lt;- \"H\"\n    } else if (myds$medv[i] &lt;= 17.0) {\n        grp[i] &lt;- \"L\"\n    } else {\n        grp[i] &lt;- \"M\"\n    }\n}\ngrp &lt;- factor(grp) # 문자 벡터를 팩터 타입으로 변경\ngrp &lt;- factor(grp, levels = c(\"H\", \"M\", \"L\")) # 레벨의 순서를 H, L, M -&gt; H, M, L\n\nmyds &lt;- data.frame(myds, grp) # myds에 grp 열 추가\n\n## (3) 데이터셋의 형태와 기본적인 내용 파악\nstr(myds)\n## 'data.frame':    506 obs. of  6 variables:\n##  $ crim: num  0.00632 0.02731 0.02729 0.03237 0.06905 ...\n##  $ rm  : num  6.58 6.42 7.18 7 7.15 ...\n##  $ dis : num  4.09 4.97 4.97 6.06 6.06 ...\n##  $ tax : num  296 242 242 222 222 222 311 311 311 311 ...\n##  $ medv: num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...\n##  $ grp : Factor w/ 3 levels \"H\",\"M\",\"L\": 2 2 1 1 1 1 2 1 3 2 ...\nhead(myds)\n##      crim    rm    dis tax medv grp\n## 1 0.00632 6.575 4.0900 296 24.0   M\n## 2 0.02731 6.421 4.9671 242 21.6   M\n## 3 0.02729 7.185 4.9671 242 34.7   H\n## 4 0.03237 6.998 6.0622 222 33.4   H\n## 5 0.06905 7.147 6.0622 222 36.2   H\n## 6 0.02985 6.430 6.0622 222 28.7   H\ntable(myds$grp) # 주택 가격 그룹별 분포\n## \n##   H   M   L \n## 132 247 127\n\n## (4) 히스토그램에 의한 관측값의 분포 확인\npar(mfrow = c(2, 3)) # 2*3 가상화면 분할\nfor (i in 1:5) {\n    hist(myds[, i], main = colnames(myds)[i], col = \"yellow\")\n}\npar(mfrow = c(1, 1)) # 2*3 가상화면 분할 해제\n\n\n\n\n## (5) 상자그림에 의한 관측값의 분포 확인\npar(mfrow = c(2, 3)) # 2*3 가상화면 분할\nfor (i in 1:5) {\n    boxplot(myds[, i], main = colnames(myds)[i])\n}\npar(mfrow = c(1, 1)) # 2*3 가상화면 분할 해제\n\n\n\n\n## (6) 그룹별 관측값 분포의 확인\nboxplot(myds$crim ~ myds$grp, main = \"1인당 범죄율\")\n\n\n\nboxplot(myds$rm ~ myds$grp, main = \"방의 개수\")\n\n\n\nboxplot(myds$dis ~ myds$grp, main = \"직업 센터까지의 거리\")\n\n\n\nboxplot(myds$tax ~ myds$grp, main = \"재산세율\")\n\n\n\n\n## (7) 다중 산점도를 통한 변수 간 상관 관계의 확인\npairs(myds[, -6]) # 6번째 열 제거(grp)\npairs(myds[, 1:5])\n\n\n\n\n## (8) 그룹 정보를 포함한 변수 간 상관 관계의 확인\npoint &lt;- as.integer(myds$grp) # 점의 모양 지정\ncolor &lt;- c(\"red\", \"green\", \"blue\") # 점의 색 지정\npairs(myds[, -6], pch = point, col = color[point])\n\n\n\n\n## (9) 변수 간 상관계수의 확인\ncor(myds[, -6])\n##            crim         rm        dis        tax       medv\n## crim  1.0000000 -0.2192467 -0.3796701  0.5827643 -0.3883046\n## rm   -0.2192467  1.0000000  0.2052462 -0.2920478  0.6953599\n## dis  -0.3796701  0.2052462  1.0000000 -0.5344316  0.2499287\n## tax   0.5827643 -0.2920478 -0.5344316  1.0000000 -0.4685359\n## medv -0.3883046  0.6953599  0.2499287 -0.4685359  1.0000000\ncor(myds[1:5])\n##            crim         rm        dis        tax       medv\n## crim  1.0000000 -0.2192467 -0.3796701  0.5827643 -0.3883046\n## rm   -0.2192467  1.0000000  0.2052462 -0.2920478  0.6953599\n## dis  -0.3796701  0.2052462  1.0000000 -0.5344316  0.2499287\n## tax   0.5827643 -0.2920478 -0.5344316  1.0000000 -0.4685359\n## medv -0.3883046  0.6953599  0.2499287 -0.4685359  1.0000000"
  },
  {
    "objectID": "R_Basic.html#장.-데이터-전처리",
    "href": "R_Basic.html#장.-데이터-전처리",
    "title": "R Basic",
    "section": "",
    "text": "z &lt;- c(1, 2, 3, NA, 5, NA, 8)   # 결측값이 포함된 벡터 z\nsum(z)                          # 정상 계산이 안 됨\n## [1] NA\nis.na(z)                        # NA 여부 확인\n## [1] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE\nsum(is.na(z))                   # NA의 개수 확인\n## [1] 2\nsum(z, na.rm = TRUE)            # NA를 제외하고 합계를 계산\n## [1] 19\n\n\nz1 &lt;- c(1, 2, 3, NA, 5, NA, 8)          # 결측값이 포함된 벡터 z1\nz2 &lt;- c(5, 8, 1, NA, 3, NA, 7)          # 결측값이 포함된 벡터 z2\nz1[is.na(z1)] &lt;- 0                      # NA를 0으로 치환\nz1\n## [1] 1 2 3 0 5 0 8\n\nz1[is.na(z1)] &lt;- mean(z1, na.rm = TRUE) # NA를 z1의 평균값으로 치환\nz1\n## [1] 1 2 3 0 5 0 8\n\nz3 &lt;- as.vector(na.omit(z2))            # NA를 제거하고 새로운 벡터 생성\nz3\n## [1] 5 8 1 3 7\n\n\n# NA를 포함하는 test 데이터 생성\nx &lt;- iris\nx[1, 2] &lt;- NA\nx[1, 3] &lt;- NA\nx[2, 3] &lt;- NA\nx[3, 4] &lt;- NA\nhead(x)\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1          NA           NA         0.2  setosa\n## 2          4.9         3.0           NA         0.2  setosa\n## 3          4.7         3.2          1.3          NA  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\n\n\n# for문을 이용한 방법 ★★★★★\nfor (i in 1:ncol(x)) {\n    this.na &lt;- is.na(x[, i])\n    cat(colnames(x)[i], \"\\t\", sum(this.na), \"\\n\")\n}\n## Sepal.Length      0 \n## Sepal.Width   1 \n## Petal.Length      2 \n## Petal.Width   1 \n## Species   0\n\n# apply를 이용한 방법\ncol_na &lt;- function(y) {\n    return(sum(is.na(y)))\n}\n\nna_count &lt;- apply(x, 2, FUN = col_na)\nna_count\n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n##            0            1            2            1            0\n\n\nrowSums(is.na(x))           # 행별 NA의 개수\n##   [1] 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n##  [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n##  [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n## [112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n## [149] 0 0\nsum(rowSums(is.na(x)) &gt; 0)  # NA가 포함된 행의 개수\n## [1] 3\n\nsum(is.na(x))               # 데이터셋 전체에서 NA 개수\n## [1] 4\n\n\nhead(x)\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1          NA           NA         0.2  setosa\n## 2          4.9         3.0           NA         0.2  setosa\n## 3          4.7         3.2          1.3          NA  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\nx[!complete.cases(x), ]     # NA가 포함된 행들 출력\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1          NA           NA         0.2  setosa\n## 2          4.9         3.0           NA         0.2  setosa\n## 3          4.7         3.2          1.3          NA  setosa\ny &lt;- x[complete.cases(x), ] # NA가 포함된 행들 제거\nhead(y)                     # 새로운 데이터셋 y의 내용 확인\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\n## 7          4.6         3.4          1.4         0.3  setosa\n## 8          5.0         3.4          1.5         0.2  setosa\n## 9          4.4         2.9          1.4         0.2  setosa\n\n\nst &lt;- data.frame(state.x77)\nboxplot(st$Income)\n\n\n\nboxplot.stats(st$Income)\n## $stats\n## [1] 3098 3983 4519 4815 5348\n## \n## $n\n## [1] 50\n## \n## $conf\n## [1] 4333.093 4704.907\n## \n## $out\n## [1] 6315\n# stats (각 변수의 최소값, 1사분위수, 2사분위수, 3사분위수, 최대값이 저장되어 있는 행렬)\n# n (각 그룹마다의 관측값 수를 저장한 벡터)\n# conf (중앙값의 95% 신뢰구간, median+-1.58*IQR/(n)^0.5)\n# out (이상치)\nboxplot.stats(st$Income)$out\n## [1] 6315\n\n\nout.val &lt;- boxplot.stats(st$Income)$out     # 특이값 추출\n\nst$Income %in% out.val\n##  [1] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [49] FALSE FALSE\nst$Income == out.val\n##  [1] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [49] FALSE FALSE\n\nst$Income[st$Income %in% out.val] &lt;- NA     # 특이값을 NA로 대체\nst$Income[st$Income == out.val] &lt;- NA\n\nhead(st)\n##            Population Income Illiteracy Life.Exp Murder HS.Grad Frost   Area\n## Alabama          3615   3624        2.1    69.05   15.1    41.3    20  50708\n## Alaska            365     NA        1.5    69.31   11.3    66.7   152 566432\n## Arizona          2212   4530        1.8    70.55    7.8    58.1    15 113417\n## Arkansas         2110   3378        1.9    70.66   10.1    39.9    65  51945\n## California      21198   5114        1.1    71.71   10.3    62.6    20 156361\n## Colorado         2541   4884        0.7    72.06    6.8    63.9   166 103766\nnewdata &lt;- st[complete.cases(st), ]         # NA가 포함된 행 제거 ★★★★★\nhead(newdata)\n##             Population Income Illiteracy Life.Exp Murder HS.Grad Frost   Area\n## Alabama           3615   3624        2.1    69.05   15.1    41.3    20  50708\n## Arizona           2212   4530        1.8    70.55    7.8    58.1    15 113417\n## Arkansas          2110   3378        1.9    70.66   10.1    39.9    65  51945\n## California       21198   5114        1.1    71.71   10.3    62.6    20 156361\n## Colorado          2541   4884        0.7    72.06    6.8    63.9   166 103766\n## Connecticut       3100   5348        1.1    72.48    3.1    56.0   139   4862\n\n\nv1 &lt;- c(1, 7, 6, 8, 4, 2, 3)\norder(v1)\n## [1] 1 6 7 5 3 2 4\n\nv1 &lt;- sort(v1) # 오름차순\nv1\n## [1] 1 2 3 4 6 7 8\nv1[order(v1)]\n## [1] 1 2 3 4 6 7 8\n\nv2 &lt;- sort(v1, decreasing = T) # 내림차순\nv2\n## [1] 8 7 6 4 3 2 1\n\n\nhead(iris)\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\norder(iris$Sepal.Length)\n##   [1]  14   9  39  43  42   4   7  23  48   3  30  12  13  25  31  46   2  10\n##  [19]  35  38  58 107   5   8  26  27  36  41  44  50  61  94   1  18  20  22\n##  [37]  24  40  45  47  99  28  29  33  60  49   6  11  17  21  32  85  34  37\n##  [55]  54  81  82  90  91  65  67  70  89  95 122  16  19  56  80  96  97 100\n##  [73] 114  15  68  83  93 102 115 143  62  71 150  63  79  84  86 120 139  64\n##  [91]  72  74  92 128 135  69  98 127 149  57  73  88 101 104 124 134 137 147\n## [109]  52  75 112 116 129 133 138  55 105 111 117 148  59  76  66  78  87 109\n## [127] 125 141 145 146  77 113 144  53 121 140 142  51 103 110 126 130 108 131\n## [145] 106 118 119 123 136 132\niris[order(iris$Sepal.Length), ]                    # 오름차순으로 정렬\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 14           4.3         3.0          1.1         0.1     setosa\n## 9            4.4         2.9          1.4         0.2     setosa\n## 39           4.4         3.0          1.3         0.2     setosa\n## 43           4.4         3.2          1.3         0.2     setosa\n## 42           4.5         2.3          1.3         0.3     setosa\n## 4            4.6         3.1          1.5         0.2     setosa\n## 7            4.6         3.4          1.4         0.3     setosa\n## 23           4.6         3.6          1.0         0.2     setosa\n## 48           4.6         3.2          1.4         0.2     setosa\n## 3            4.7         3.2          1.3         0.2     setosa\n## 30           4.7         3.2          1.6         0.2     setosa\n## 12           4.8         3.4          1.6         0.2     setosa\n## 13           4.8         3.0          1.4         0.1     setosa\n## 25           4.8         3.4          1.9         0.2     setosa\n## 31           4.8         3.1          1.6         0.2     setosa\n## 46           4.8         3.0          1.4         0.3     setosa\n## 2            4.9         3.0          1.4         0.2     setosa\n## 10           4.9         3.1          1.5         0.1     setosa\n## 35           4.9         3.1          1.5         0.2     setosa\n## 38           4.9         3.6          1.4         0.1     setosa\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 107          4.9         2.5          4.5         1.7  virginica\n## 5            5.0         3.6          1.4         0.2     setosa\n## 8            5.0         3.4          1.5         0.2     setosa\n## 26           5.0         3.0          1.6         0.2     setosa\n## 27           5.0         3.4          1.6         0.4     setosa\n## 36           5.0         3.2          1.2         0.2     setosa\n## 41           5.0         3.5          1.3         0.3     setosa\n## 44           5.0         3.5          1.6         0.6     setosa\n## 50           5.0         3.3          1.4         0.2     setosa\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 1            5.1         3.5          1.4         0.2     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 28           5.2         3.5          1.5         0.2     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 49           5.3         3.7          1.5         0.2     setosa\n## 6            5.4         3.9          1.7         0.4     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 34           5.5         4.2          1.4         0.2     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 122          5.6         2.8          4.9         2.0  virginica\n## 16           5.7         4.4          1.5         0.4     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 114          5.7         2.5          5.0         2.0  virginica\n## 15           5.8         4.0          1.2         0.2     setosa\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 102          5.8         2.7          5.1         1.9  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 150          5.9         3.0          5.1         1.8  virginica\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 120          6.0         2.2          5.0         1.5  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 128          6.1         3.0          4.9         1.8  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 127          6.2         2.8          4.8         1.8  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 112          6.4         2.7          5.3         1.9  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 105          6.5         3.0          5.8         2.2  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 109          6.7         2.5          5.8         1.8  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 113          6.8         3.0          5.5         2.1  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 121          6.9         3.2          5.7         2.3  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 103          7.1         3.0          5.9         2.1  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\niris[order(iris$Sepal.Length, decreasing = T), ]    # 내림차순으로 정렬\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 132          7.9         3.8          6.4         2.0  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 121          6.9         3.2          5.7         2.3  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 113          6.8         3.0          5.5         2.1  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 109          6.7         2.5          5.8         1.8  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 105          6.5         3.0          5.8         2.2  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 112          6.4         2.7          5.3         1.9  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 127          6.2         2.8          4.8         1.8  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 128          6.1         3.0          4.9         1.8  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 120          6.0         2.2          5.0         1.5  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 150          5.9         3.0          5.1         1.8  virginica\n## 15           5.8         4.0          1.2         0.2     setosa\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 102          5.8         2.7          5.1         1.9  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 16           5.7         4.4          1.5         0.4     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 114          5.7         2.5          5.0         2.0  virginica\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 122          5.6         2.8          4.9         2.0  virginica\n## 34           5.5         4.2          1.4         0.2     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 6            5.4         3.9          1.7         0.4     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 49           5.3         3.7          1.5         0.2     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 1            5.1         3.5          1.4         0.2     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 5            5.0         3.6          1.4         0.2     setosa\n## 8            5.0         3.4          1.5         0.2     setosa\n## 26           5.0         3.0          1.6         0.2     setosa\n## 27           5.0         3.4          1.6         0.4     setosa\n## 36           5.0         3.2          1.2         0.2     setosa\n## 41           5.0         3.5          1.3         0.3     setosa\n## 44           5.0         3.5          1.6         0.6     setosa\n## 50           5.0         3.3          1.4         0.2     setosa\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 2            4.9         3.0          1.4         0.2     setosa\n## 10           4.9         3.1          1.5         0.1     setosa\n## 35           4.9         3.1          1.5         0.2     setosa\n## 38           4.9         3.6          1.4         0.1     setosa\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 107          4.9         2.5          4.5         1.7  virginica\n## 12           4.8         3.4          1.6         0.2     setosa\n## 13           4.8         3.0          1.4         0.1     setosa\n## 25           4.8         3.4          1.9         0.2     setosa\n## 31           4.8         3.1          1.6         0.2     setosa\n## 46           4.8         3.0          1.4         0.3     setosa\n## 3            4.7         3.2          1.3         0.2     setosa\n## 30           4.7         3.2          1.6         0.2     setosa\n## 4            4.6         3.1          1.5         0.2     setosa\n## 7            4.6         3.4          1.4         0.3     setosa\n## 23           4.6         3.6          1.0         0.2     setosa\n## 48           4.6         3.2          1.4         0.2     setosa\n## 42           4.5         2.3          1.3         0.3     setosa\n## 9            4.4         2.9          1.4         0.2     setosa\n## 39           4.4         3.0          1.3         0.2     setosa\n## 43           4.4         3.2          1.3         0.2     setosa\n## 14           4.3         3.0          1.1         0.1     setosa\n\niris.new &lt;- iris[order(iris$Sepal.Length), ]        # 정렬된 데이터를 저장\nhead(iris.new)\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 14          4.3         3.0          1.1         0.1  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 39          4.4         3.0          1.3         0.2  setosa\n## 43          4.4         3.2          1.3         0.2  setosa\n## 42          4.5         2.3          1.3         0.3  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\niris[order(iris$Species,-iris$Petal.Length, decreasing = T), ] # 정렬 기준이 2개\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 107          4.9         2.5          4.5         1.7  virginica\n## 127          6.2         2.8          4.8         1.8  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 122          5.6         2.8          4.9         2.0  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 128          6.1         3.0          4.9         1.8  virginica\n## 114          5.7         2.5          5.0         2.0  virginica\n## 120          6.0         2.2          5.0         1.5  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 101          6.3         3.3          6.0         2.5  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 23           4.6         3.6          1.0         0.2     setosa\n## 14           4.3         3.0          1.1         0.1     setosa\n## 15           5.8         4.0          1.2         0.2     setosa\n## 36           5.0         3.2          1.2         0.2     setosa\n## 3            4.7         3.2          1.3         0.2     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 39           4.4         3.0          1.3         0.2     setosa\n## 41           5.0         3.5          1.3         0.3     setosa\n## 42           4.5         2.3          1.3         0.3     setosa\n## 43           4.4         3.2          1.3         0.2     setosa\n## 1            5.1         3.5          1.4         0.2     setosa\n## 2            4.9         3.0          1.4         0.2     setosa\n## 5            5.0         3.6          1.4         0.2     setosa\n## 7            4.6         3.4          1.4         0.3     setosa\n## 9            4.4         2.9          1.4         0.2     setosa\n## 13           4.8         3.0          1.4         0.1     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 34           5.5         4.2          1.4         0.2     setosa\n## 38           4.9         3.6          1.4         0.1     setosa\n## 46           4.8         3.0          1.4         0.3     setosa\n## 48           4.6         3.2          1.4         0.2     setosa\n## 50           5.0         3.3          1.4         0.2     setosa\n## 4            4.6         3.1          1.5         0.2     setosa\n## 8            5.0         3.4          1.5         0.2     setosa\n## 10           4.9         3.1          1.5         0.1     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 16           5.7         4.4          1.5         0.4     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 35           4.9         3.1          1.5         0.2     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 49           5.3         3.7          1.5         0.2     setosa\n## 12           4.8         3.4          1.6         0.2     setosa\n## 26           5.0         3.0          1.6         0.2     setosa\n## 27           5.0         3.4          1.6         0.4     setosa\n## 30           4.7         3.2          1.6         0.2     setosa\n## 31           4.8         3.1          1.6         0.2     setosa\n## 44           5.0         3.5          1.6         0.6     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 6            5.4         3.9          1.7         0.4     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 25           4.8         3.4          1.9         0.2     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\niris[order(iris$Species, decreasing = T, iris$Petal.Length), ]\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 119          7.7         2.6          6.9         2.3  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 101          6.3         3.3          6.0         2.5  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\n## 114          5.7         2.5          5.0         2.0  virginica\n## 120          6.0         2.2          5.0         1.5  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 122          5.6         2.8          4.9         2.0  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 128          6.1         3.0          4.9         1.8  virginica\n## 127          6.2         2.8          4.8         1.8  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 107          4.9         2.5          4.5         1.7  virginica\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 25           4.8         3.4          1.9         0.2     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\n## 6            5.4         3.9          1.7         0.4     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 12           4.8         3.4          1.6         0.2     setosa\n## 26           5.0         3.0          1.6         0.2     setosa\n## 27           5.0         3.4          1.6         0.4     setosa\n## 30           4.7         3.2          1.6         0.2     setosa\n## 31           4.8         3.1          1.6         0.2     setosa\n## 44           5.0         3.5          1.6         0.6     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 4            4.6         3.1          1.5         0.2     setosa\n## 8            5.0         3.4          1.5         0.2     setosa\n## 10           4.9         3.1          1.5         0.1     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 16           5.7         4.4          1.5         0.4     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 35           4.9         3.1          1.5         0.2     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 49           5.3         3.7          1.5         0.2     setosa\n## 1            5.1         3.5          1.4         0.2     setosa\n## 2            4.9         3.0          1.4         0.2     setosa\n## 5            5.0         3.6          1.4         0.2     setosa\n## 7            4.6         3.4          1.4         0.3     setosa\n## 9            4.4         2.9          1.4         0.2     setosa\n## 13           4.8         3.0          1.4         0.1     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 34           5.5         4.2          1.4         0.2     setosa\n## 38           4.9         3.6          1.4         0.1     setosa\n## 46           4.8         3.0          1.4         0.3     setosa\n## 48           4.6         3.2          1.4         0.2     setosa\n## 50           5.0         3.3          1.4         0.2     setosa\n## 3            4.7         3.2          1.3         0.2     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 39           4.4         3.0          1.3         0.2     setosa\n## 41           5.0         3.5          1.3         0.3     setosa\n## 42           4.5         2.3          1.3         0.3     setosa\n## 43           4.4         3.2          1.3         0.2     setosa\n## 15           5.8         4.0          1.2         0.2     setosa\n## 36           5.0         3.2          1.2         0.2     setosa\n## 14           4.3         3.0          1.1         0.1     setosa\n## 23           4.6         3.6          1.0         0.2     setosa\n\n\nsp &lt;- split(iris, iris$Species) # 품종별로 데이터 분리\nsp                              # 분리 결과 확인\n## $setosa\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1           5.1         3.5          1.4         0.2  setosa\n## 2           4.9         3.0          1.4         0.2  setosa\n## 3           4.7         3.2          1.3         0.2  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\n## 5           5.0         3.6          1.4         0.2  setosa\n## 6           5.4         3.9          1.7         0.4  setosa\n## 7           4.6         3.4          1.4         0.3  setosa\n## 8           5.0         3.4          1.5         0.2  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\n## 11          5.4         3.7          1.5         0.2  setosa\n## 12          4.8         3.4          1.6         0.2  setosa\n## 13          4.8         3.0          1.4         0.1  setosa\n## 14          4.3         3.0          1.1         0.1  setosa\n## 15          5.8         4.0          1.2         0.2  setosa\n## 16          5.7         4.4          1.5         0.4  setosa\n## 17          5.4         3.9          1.3         0.4  setosa\n## 18          5.1         3.5          1.4         0.3  setosa\n## 19          5.7         3.8          1.7         0.3  setosa\n## 20          5.1         3.8          1.5         0.3  setosa\n## 21          5.4         3.4          1.7         0.2  setosa\n## 22          5.1         3.7          1.5         0.4  setosa\n## 23          4.6         3.6          1.0         0.2  setosa\n## 24          5.1         3.3          1.7         0.5  setosa\n## 25          4.8         3.4          1.9         0.2  setosa\n## 26          5.0         3.0          1.6         0.2  setosa\n## 27          5.0         3.4          1.6         0.4  setosa\n## 28          5.2         3.5          1.5         0.2  setosa\n## 29          5.2         3.4          1.4         0.2  setosa\n## 30          4.7         3.2          1.6         0.2  setosa\n## 31          4.8         3.1          1.6         0.2  setosa\n## 32          5.4         3.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\n## 35          4.9         3.1          1.5         0.2  setosa\n## 36          5.0         3.2          1.2         0.2  setosa\n## 37          5.5         3.5          1.3         0.2  setosa\n## 38          4.9         3.6          1.4         0.1  setosa\n## 39          4.4         3.0          1.3         0.2  setosa\n## 40          5.1         3.4          1.5         0.2  setosa\n## 41          5.0         3.5          1.3         0.3  setosa\n## 42          4.5         2.3          1.3         0.3  setosa\n## 43          4.4         3.2          1.3         0.2  setosa\n## 44          5.0         3.5          1.6         0.6  setosa\n## 45          5.1         3.8          1.9         0.4  setosa\n## 46          4.8         3.0          1.4         0.3  setosa\n## 47          5.1         3.8          1.6         0.2  setosa\n## 48          4.6         3.2          1.4         0.2  setosa\n## 49          5.3         3.7          1.5         0.2  setosa\n## 50          5.0         3.3          1.4         0.2  setosa\n## \n## $versicolor\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## \n## $virginica\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n## 101          6.3         3.3          6.0         2.5 virginica\n## 102          5.8         2.7          5.1         1.9 virginica\n## 103          7.1         3.0          5.9         2.1 virginica\n## 104          6.3         2.9          5.6         1.8 virginica\n## 105          6.5         3.0          5.8         2.2 virginica\n## 106          7.6         3.0          6.6         2.1 virginica\n## 107          4.9         2.5          4.5         1.7 virginica\n## 108          7.3         2.9          6.3         1.8 virginica\n## 109          6.7         2.5          5.8         1.8 virginica\n## 110          7.2         3.6          6.1         2.5 virginica\n## 111          6.5         3.2          5.1         2.0 virginica\n## 112          6.4         2.7          5.3         1.9 virginica\n## 113          6.8         3.0          5.5         2.1 virginica\n## 114          5.7         2.5          5.0         2.0 virginica\n## 115          5.8         2.8          5.1         2.4 virginica\n## 116          6.4         3.2          5.3         2.3 virginica\n## 117          6.5         3.0          5.5         1.8 virginica\n## 118          7.7         3.8          6.7         2.2 virginica\n## 119          7.7         2.6          6.9         2.3 virginica\n## 120          6.0         2.2          5.0         1.5 virginica\n## 121          6.9         3.2          5.7         2.3 virginica\n## 122          5.6         2.8          4.9         2.0 virginica\n## 123          7.7         2.8          6.7         2.0 virginica\n## 124          6.3         2.7          4.9         1.8 virginica\n## 125          6.7         3.3          5.7         2.1 virginica\n## 126          7.2         3.2          6.0         1.8 virginica\n## 127          6.2         2.8          4.8         1.8 virginica\n## 128          6.1         3.0          4.9         1.8 virginica\n## 129          6.4         2.8          5.6         2.1 virginica\n## 130          7.2         3.0          5.8         1.6 virginica\n## 131          7.4         2.8          6.1         1.9 virginica\n## 132          7.9         3.8          6.4         2.0 virginica\n## 133          6.4         2.8          5.6         2.2 virginica\n## 134          6.3         2.8          5.1         1.5 virginica\n## 135          6.1         2.6          5.6         1.4 virginica\n## 136          7.7         3.0          6.1         2.3 virginica\n## 137          6.3         3.4          5.6         2.4 virginica\n## 138          6.4         3.1          5.5         1.8 virginica\n## 139          6.0         3.0          4.8         1.8 virginica\n## 140          6.9         3.1          5.4         2.1 virginica\n## 141          6.7         3.1          5.6         2.4 virginica\n## 142          6.9         3.1          5.1         2.3 virginica\n## 143          5.8         2.7          5.1         1.9 virginica\n## 144          6.8         3.2          5.9         2.3 virginica\n## 145          6.7         3.3          5.7         2.5 virginica\n## 146          6.7         3.0          5.2         2.3 virginica\n## 147          6.3         2.5          5.0         1.9 virginica\n## 148          6.5         3.0          5.2         2.0 virginica\n## 149          6.2         3.4          5.4         2.3 virginica\n## 150          5.9         3.0          5.1         1.8 virginica\nsummary(sp)                     # 분리 결과 요약\n##            Length Class      Mode\n## setosa     5      data.frame list\n## versicolor 5      data.frame list\n## virginica  5      data.frame list\nsp$setosa                       # setosa 품종의 데이터 확인\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1           5.1         3.5          1.4         0.2  setosa\n## 2           4.9         3.0          1.4         0.2  setosa\n## 3           4.7         3.2          1.3         0.2  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\n## 5           5.0         3.6          1.4         0.2  setosa\n## 6           5.4         3.9          1.7         0.4  setosa\n## 7           4.6         3.4          1.4         0.3  setosa\n## 8           5.0         3.4          1.5         0.2  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\n## 11          5.4         3.7          1.5         0.2  setosa\n## 12          4.8         3.4          1.6         0.2  setosa\n## 13          4.8         3.0          1.4         0.1  setosa\n## 14          4.3         3.0          1.1         0.1  setosa\n## 15          5.8         4.0          1.2         0.2  setosa\n## 16          5.7         4.4          1.5         0.4  setosa\n## 17          5.4         3.9          1.3         0.4  setosa\n## 18          5.1         3.5          1.4         0.3  setosa\n## 19          5.7         3.8          1.7         0.3  setosa\n## 20          5.1         3.8          1.5         0.3  setosa\n## 21          5.4         3.4          1.7         0.2  setosa\n## 22          5.1         3.7          1.5         0.4  setosa\n## 23          4.6         3.6          1.0         0.2  setosa\n## 24          5.1         3.3          1.7         0.5  setosa\n## 25          4.8         3.4          1.9         0.2  setosa\n## 26          5.0         3.0          1.6         0.2  setosa\n## 27          5.0         3.4          1.6         0.4  setosa\n## 28          5.2         3.5          1.5         0.2  setosa\n## 29          5.2         3.4          1.4         0.2  setosa\n## 30          4.7         3.2          1.6         0.2  setosa\n## 31          4.8         3.1          1.6         0.2  setosa\n## 32          5.4         3.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\n## 35          4.9         3.1          1.5         0.2  setosa\n## 36          5.0         3.2          1.2         0.2  setosa\n## 37          5.5         3.5          1.3         0.2  setosa\n## 38          4.9         3.6          1.4         0.1  setosa\n## 39          4.4         3.0          1.3         0.2  setosa\n## 40          5.1         3.4          1.5         0.2  setosa\n## 41          5.0         3.5          1.3         0.3  setosa\n## 42          4.5         2.3          1.3         0.3  setosa\n## 43          4.4         3.2          1.3         0.2  setosa\n## 44          5.0         3.5          1.6         0.6  setosa\n## 45          5.1         3.8          1.9         0.4  setosa\n## 46          4.8         3.0          1.4         0.3  setosa\n## 47          5.1         3.8          1.6         0.2  setosa\n## 48          4.6         3.2          1.4         0.2  setosa\n## 49          5.3         3.7          1.5         0.2  setosa\n## 50          5.0         3.3          1.4         0.2  setosa\nsetosa &lt;- sp$setosa\n\n\nsubset(iris, Species == \"setosa\")\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1           5.1         3.5          1.4         0.2  setosa\n## 2           4.9         3.0          1.4         0.2  setosa\n## 3           4.7         3.2          1.3         0.2  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\n## 5           5.0         3.6          1.4         0.2  setosa\n## 6           5.4         3.9          1.7         0.4  setosa\n## 7           4.6         3.4          1.4         0.3  setosa\n## 8           5.0         3.4          1.5         0.2  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\n## 11          5.4         3.7          1.5         0.2  setosa\n## 12          4.8         3.4          1.6         0.2  setosa\n## 13          4.8         3.0          1.4         0.1  setosa\n## 14          4.3         3.0          1.1         0.1  setosa\n## 15          5.8         4.0          1.2         0.2  setosa\n## 16          5.7         4.4          1.5         0.4  setosa\n## 17          5.4         3.9          1.3         0.4  setosa\n## 18          5.1         3.5          1.4         0.3  setosa\n## 19          5.7         3.8          1.7         0.3  setosa\n## 20          5.1         3.8          1.5         0.3  setosa\n## 21          5.4         3.4          1.7         0.2  setosa\n## 22          5.1         3.7          1.5         0.4  setosa\n## 23          4.6         3.6          1.0         0.2  setosa\n## 24          5.1         3.3          1.7         0.5  setosa\n## 25          4.8         3.4          1.9         0.2  setosa\n## 26          5.0         3.0          1.6         0.2  setosa\n## 27          5.0         3.4          1.6         0.4  setosa\n## 28          5.2         3.5          1.5         0.2  setosa\n## 29          5.2         3.4          1.4         0.2  setosa\n## 30          4.7         3.2          1.6         0.2  setosa\n## 31          4.8         3.1          1.6         0.2  setosa\n## 32          5.4         3.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\n## 35          4.9         3.1          1.5         0.2  setosa\n## 36          5.0         3.2          1.2         0.2  setosa\n## 37          5.5         3.5          1.3         0.2  setosa\n## 38          4.9         3.6          1.4         0.1  setosa\n## 39          4.4         3.0          1.3         0.2  setosa\n## 40          5.1         3.4          1.5         0.2  setosa\n## 41          5.0         3.5          1.3         0.3  setosa\n## 42          4.5         2.3          1.3         0.3  setosa\n## 43          4.4         3.2          1.3         0.2  setosa\n## 44          5.0         3.5          1.6         0.6  setosa\n## 45          5.1         3.8          1.9         0.4  setosa\n## 46          4.8         3.0          1.4         0.3  setosa\n## 47          5.1         3.8          1.6         0.2  setosa\n## 48          4.6         3.2          1.4         0.2  setosa\n## 49          5.3         3.7          1.5         0.2  setosa\n## 50          5.0         3.3          1.4         0.2  setosa\nsubset(iris, Sepal.Length &gt; 7.5)\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n## 106          7.6         3.0          6.6         2.1 virginica\n## 118          7.7         3.8          6.7         2.2 virginica\n## 119          7.7         2.6          6.9         2.3 virginica\n## 123          7.7         2.8          6.7         2.0 virginica\n## 132          7.9         3.8          6.4         2.0 virginica\n## 136          7.7         3.0          6.1         2.3 virginica\nsubset(iris, Sepal.Length &gt; 5.1 &\n           Sepal.Width &gt; 3.9)\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 15          5.8         4.0          1.2         0.2  setosa\n## 16          5.7         4.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\nsubset(iris, Sepal.Length &gt; 5.1 |\n           Sepal.Width &gt; 3.9)\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 6            5.4         3.9          1.7         0.4     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 15           5.8         4.0          1.2         0.2     setosa\n## 16           5.7         4.4          1.5         0.4     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 34           5.5         4.2          1.4         0.2     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 49           5.3         3.7          1.5         0.2     setosa\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 114          5.7         2.5          5.0         2.0  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 120          6.0         2.2          5.0         1.5  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 122          5.6         2.8          4.9         2.0  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 127          6.2         2.8          4.8         1.8  virginica\n## 128          6.1         3.0          4.9         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\nsubset(iris, Sepal.Length &gt; 7.6,\n       select = c(Petal.Length, Petal.Width))\n##     Petal.Length Petal.Width\n## 118          6.7         2.2\n## 119          6.9         2.3\n## 123          6.7         2.0\n## 132          6.4         2.0\n## 136          6.1         2.3\n\n\nx &lt;- 1:10\nsample(x, size = 5, replace = FALSE) # 비복원추출\n## [1] 7 8 6 2 5\nsample(x, size = 5, replace = TRUE)\n## [1] 1 7 8 9 4\n\nx &lt;- 1:45\nsample(x, size = 6, replace = FALSE)\n## [1] 41 40  4  9 15 33\n\n\nidx &lt;- sample(1:nrow(iris), size = 50,\n              replace = FALSE)\niris.50 &lt;- iris[idx, ]  # 50개의 행 추출\ndim(iris.50)            # 행과 열의 개수 확인\n## [1] 50  5\nhead(iris.50)\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 19           5.7         3.8          1.7         0.3     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 109          6.7         2.5          5.8         1.8  virginica\n## 42           4.5         2.3          1.3         0.3     setosa\n## 121          6.9         3.2          5.7         2.3  virginica\n\n\nsample(1:20, size = 5)\n## [1] 17 14  6 13  2\nsample(1:20, size = 5)\n## [1] 13 17  1 20 15\nsample(1:20, size = 5)\n## [1]  2  7 12 16 17\n\n# 같은 값이 추출되도록 고정시키고 싶다면\n# set.seed() 함수를 이용하여 seed값을 지정해주면 된다.\nset.seed(100)\nsample(1:20, size = 5)\n## [1] 10  6 16 14 12\nset.seed(100)\nsample(1:20, size = 5)\n## [1] 10  6 16 14 12\nset.seed(100)\nsample(1:20, size = 5)\n## [1] 10  6 16 14 12\n\n\ncombn(1:5, 3) # 1~5에서 3개를 뽑는 조합\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n## [1,]    1    1    1    1    1    1    2    2    2     3\n## [2,]    2    2    2    3    3    4    3    3    4     4\n## [3,]    3    4    5    4    5    5    4    5    5     5\n\nx = c(\"red\", \"green\", \"blue\", \"black\", \"white\")\ncom &lt;- combn(x, 2) # x의 원소를 2개씩 뽑는 조합\ncom\n##      [,1]    [,2]   [,3]    [,4]    [,5]    [,6]    [,7]    [,8]    [,9]   \n## [1,] \"red\"   \"red\"  \"red\"   \"red\"   \"green\" \"green\" \"green\" \"blue\"  \"blue\" \n## [2,] \"green\" \"blue\" \"black\" \"white\" \"blue\"  \"black\" \"white\" \"black\" \"white\"\n##      [,10]  \n## [1,] \"black\"\n## [2,] \"white\"\n\nfor (i in 1:ncol(com)) {\n    # 조합을 출력\n    cat(com[, i], \"\\n\")\n}\n## red green \n## red blue \n## red black \n## red white \n## green blue \n## green black \n## green white \n## blue black \n## blue white \n## black white\n\n\n# aggregate(data, by = '기준이 되는 컬럼', FUN)\nagg &lt;- aggregate(iris[, -5], by = list(iris$Species), FUN = mean)\nagg\n##      Group.1 Sepal.Length Sepal.Width Petal.Length Petal.Width\n## 1     setosa        5.006       3.428        1.462       0.246\n## 2 versicolor        5.936       2.770        4.260       1.326\n## 3  virginica        6.588       2.974        5.552       2.026\n\n\n# aggregate는 데이터의 특정 컬럼을 기준으로 통계량을 구해주는 함수\nagg &lt;- aggregate(iris[, -5], by = list(표준편차 = iris$Species), FUN = sd)\nagg\n##     표준편차 Sepal.Length Sepal.Width Petal.Length Petal.Width\n## 1     setosa    0.3524897   0.3790644    0.1736640   0.1053856\n## 2 versicolor    0.5161711   0.3137983    0.4699110   0.1977527\n## 3  virginica    0.6358796   0.3224966    0.5518947   0.2746501\n\n\nhead(mtcars)\n##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\nagg &lt;- aggregate(mtcars, by = list(cyl = mtcars$cyl, vs = mtcars$vs), FUN = max)\nagg\n##   cyl vs  mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## 1   4  0 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n## 2   6  0 21.0   6 160.0 175 3.90 2.875 17.02  0  1    5    6\n## 3   8  0 19.2   8 472.0 335 4.22 5.424 18.00  0  1    5    8\n## 4   4  1 33.9   4 146.7 113 4.93 3.190 22.90  1  1    5    2\n## 5   6  1 21.4   6 258.0 123 3.92 3.460 20.22  1  0    4    4\n\nagg &lt;- aggregate(mtcars, by = list(cyl = mtcars$cyl, vs = mtcars$vs), FUN = mean)\nagg\n##   cyl vs      mpg cyl   disp       hp     drat       wt     qsec vs        am\n## 1   4  0 26.00000   4 120.30  91.0000 4.430000 2.140000 16.70000  0 1.0000000\n## 2   6  0 20.56667   6 155.00 131.6667 3.806667 2.755000 16.32667  0 1.0000000\n## 3   8  0 15.10000   8 353.10 209.2143 3.229286 3.999214 16.77214  0 0.1428571\n## 4   4  1 26.73000   4 103.62  81.8000 4.035000 2.300300 19.38100  1 0.7000000\n## 5   6  1 19.12500   6 204.55 115.2500 3.420000 3.388750 19.21500  1 0.0000000\n##       gear     carb\n## 1 5.000000 2.000000\n## 2 4.333333 4.666667\n## 3 3.285714 3.500000\n## 4 4.000000 1.500000\n## 5 3.500000 2.500000\n\n\nx &lt;- data.frame(name = c(\"a\", \"b\", \"c\"), math = c(90, 80, 40))\ny &lt;- data.frame(name = c(\"a\", \"b\", \"d\"), korean = c(75, 60, 90))\nx ; y\n##   name math\n## 1    a   90\n## 2    b   80\n## 3    c   40\n##   name korean\n## 1    a     75\n## 2    b     60\n## 3    d     90\n\n\nz &lt;- merge(x, y, by = c(\"name\"))\nz\n##   name math korean\n## 1    a   90     75\n## 2    b   80     60\n\n\nmerge(x, y, all.x = T)  # 첫 번째 데이터셋의 행들은 모두 표시되도록\n##   name math korean\n## 1    a   90     75\n## 2    b   80     60\n## 3    c   40     NA\nmerge(x, y, all.y = T)  # 두 번째 데이터셋의 행들은 모두 표시되도록\n##   name math korean\n## 1    a   90     75\n## 2    b   80     60\n## 3    d   NA     90\nmerge(x, y, all = T)    # 두 데이터셋의 모든 행들이 표시되도록\n##   name math korean\n## 1    a   90     75\n## 2    b   80     60\n## 3    c   40     NA\n## 4    d   NA     90\n\n\nx &lt;- data.frame(name = c(\"a\", \"b\", \"c\"), math = c(90, 80, 40))\ny &lt;- data.frame(sname = c(\"a\", \"b\", \"d\"), korean = c(75, 60, 90))\nx # 병합 기준 열의 이름이 name\n##   name math\n## 1    a   90\n## 2    b   80\n## 3    c   40\ny # 병합 기준 열의 이름이 sname\n##   sname korean\n## 1     a     75\n## 2     b     60\n## 3     d     90\nmerge(x, y, by.x = c(\"name\"), by.y = c(\"sname\"))\n##   name math korean\n## 1    a   90     75\n## 2    b   80     60"
  }
]