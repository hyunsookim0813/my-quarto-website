[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "BigData_Analysis/실기_1유형.html",
    "href": "BigData_Analysis/실기_1유형.html",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "",
    "text": "실기 1 유형"
  },
  {
    "objectID": "BigData_Analysis/실기_1유형.html#데이터-다루기-유형",
    "href": "BigData_Analysis/실기_1유형.html#데이터-다루기-유형",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "✅ 데이터 다루기 유형",
    "text": "✅ 데이터 다루기 유형\n\n\n데이터 타입(object, int, float, bool 등)\n기초통계량(평균, 중앙값, 사분위수, IQR, 표준편차 등)\n데이터 인덱싱, 필터링, 정렬, 변경 등\n중복값, 결측치, 이상치 처리 (제거 or 대체)\n데이터 Scaling (데이터 표준화(z), 데이터 정규화(min-max))\n데이터 합치기\n날짜/시간 데이터, index 다루기"
  },
  {
    "objectID": "BigData_Analysis/실기_1유형.html#핵심문제-27개-110",
    "href": "BigData_Analysis/실기_1유형.html#핵심문제-27개-110",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "✅ 핵심문제 27개 (1~10)",
    "text": "✅ 핵심문제 27개 (1~10)\n\n# 데이터 불러오기\nimport pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"mtcars.txt\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      Hornet Sportabout\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\n# 문제 1\n# mpg 변수의 제 1사분위수를 구하고 정수값으로 출력하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\nQ1 = df['mpg'].quantile(.25)\nprint(round(Q1))\n\n15\n\n\n\n# 문제 2\n# mpg 값이 19이상 21이하인 데이터의 수를 구하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이 1)\ncond1 = (df[(df['mpg']>=19) & (df['mpg']<=21)])\nprint(len(cond1))\n\n# (풀이 2)\n# cond1 = (df['mpg']>=19)\n# cond2 = (df['mpg']<=21)\n# print(len(df[cond1&cond2]))\n\n5\n\n\n\n# 문제 3\n# hp 변수의 IQR 값을 구하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\nQ1 = df['hp'].quantile(.25)\nQ3 = df['hp'].quantile(.75)\nIQR = Q3 - Q1\nprint(IQR)\n\n83.5\n\n\n\n# 문제 4 \n# wt 변수의 상위 10개 값의 총합을 구하여 소수점을 버리고 정수로 출력하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\ntop10 = df.sort_values('wt', ascending=False)\nsum_top10 = sum(top10['wt'].head(10))\nprint(int(sum_top10))\n\n# top_10 = df.sort_values('wt',ascending=False).head(10)\n# print(int(sum(top_10['wt']))) #주의: 소수점 반올림이 아니라 버리는 문제\n\n42\n\n\n\n# 문제 5\n# 전체 자동차에서 cyl가 6인 비율이 얼마인지 소수점 첫째짜리까지 출력하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\ncond1 = len(df[df['cyl']==6])\ncond2 = len(df['cyl'])\nprint(round(cond1/cond2, 1))\n\n0.2\n\n\n\n# 문제 6\n# 첫번째 행부터 순서대로 10개 뽑은 후 mpg 열의 평균값을 반올림하여 정수로 출력하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\ncond1 = df.head(10) # df[:10], df.loc[0:9]\nmean_mpg = cond1['mpg'].mean()\nprint(round(mean_mpg))\n\n20\n\n\n\n# 문제 7\n# 첫번째 행부터 순서대로 50% 까지 데이터를 뽑아 wt 변수의 중앙값을 구하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\n# 50% 데이터에 해당하는 행의 수 (정수로 구하기)\np50 = int(len(df)*0.5)\ndf50 = df[:p50]\nprint(df50['wt'].median())\n\n# len(df)/2\n# cond1 = df[:17]\n# cond2 = cond1['wt'].median()\n# print(cond2)\n\n3.44\n\n\n\n# 문제 8\n# 결측값이 있는 데이터의 수를 출력하시오\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    '판매수' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      날짜\n      제품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3.0\n      300\n    \n    \n      1\n      20220105\n      B\n      NaN\n      400\n    \n    \n      2\n      None\n      None\n      5.0\n      500\n    \n    \n      3\n      20230127\n      B\n      10.0\n      600\n    \n    \n      4\n      20220203\n      A\n      10.0\n      400\n    \n  \n\n\n\n\n\n# (풀이)\nm_value = df.isnull().sum()\nprint(m_value.sum())\n\n5\n\n\n\n# 문제 9 \n# '판매수' 컬럼의 결측값을 판매수의 중앙값으로 대체하고 판매수의 \n#  평균값을 정수로 출력하시오\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    '판매수' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\n\n\n# (풀이)\ndf[df['판매수'].isnull()]\nmedian = df['판매수'].median()\ndf['판매수'] = df['판매수'].fillna(median)\nmean = df['판매수'].mean()\nprint(int(mean))\n\n15\n\n\n\n# 문제 10\n# 판매수 컬럼에 결측치가 있는 행을 제거하고\n# 첫번째 행부터 순서대로 50%까지 데이터를 추출하여\n# 판매수 변수의 Q1(제1사분위수) 값을 정수로 출력하시오\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    '판매수' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\n\n\n# (풀이)\n# 결측치 삭제(행 기준)\ndf = df['판매수'].dropna()\n# 첫번째 행부터 순서대로 50%까지의 데이터 추출\nper50 = int(len(df)*0.5)\ndf = df[:per50]\n# 값구하기\nprint(round(df.quantile(.25)))\n\n# df[df['판매수'].isnull()]\n# df['판매수'] = df['판매수'].dropna()\n# int(len(df['판매수'])/2)\n# cond1 = df['판매수'].head(6)\n# Q1 = cond1.quantile(.25)\n# print(int(Q1))\n\n5"
  },
  {
    "objectID": "BigData_Analysis/실기_1유형.html#핵심문제-27개-1120",
    "href": "BigData_Analysis/실기_1유형.html#핵심문제-27개-1120",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "✅ 핵심문제 27개 (11~20)",
    "text": "✅ 핵심문제 27개 (11~20)\n\ndf = pd.read_csv('mtcars.txt')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      Hornet Sportabout\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\n# 문제 11\n# cyl가 4인 자동차와 6인 자동차 그룹의 mpg 평균값이 차이를 절대값으로 정수로 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\ncyl_4 = df[df['cyl']==4]\ncyl_4_mean = cyl_4['mpg'].mean()\n\ncyl_6 = df[df['cyl']==6]\ncyl_6_mean = cyl_6['mpg'].mean()\n\nprint(round(abs(cyl_4_mean - cyl_6_mean)))\n\n7\n\n\n\n# 문제 12\n# hp 변수에 대해 데이터표준화(Z-score)를 진행하고 이상치의 수를 구하시오\n# (단, 이상치는 Z값이 1.5를 초과하거나 -1.5미만인 값이다)\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\n# Z = (X-평균) / 표준편차\nstd = df['hp'].std()\nmean_hp = df['hp'].mean()\ndf['zscore'] = (df['hp']-mean_hp)/std\n\ncond1 = (df['zscore']>1.5)\ncond2 = (df['zscore']<-1.5)\nprint(len(df[cond1] + df[cond2]))\n\n2\n\n\n\n# 문제 13\n# mpg 컬럼을 최소최대 Scaling을 진행한 후 0.7보다 큰 값을 가지는 레코드 수를 구하라\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\nfrom sklearn.preprocessing import MinMaxScaler\nmscaler = MinMaxScaler()\ndf['mpg'] = mscaler.fit_transform(df[['mpg']])\nprint(len(df[df['mpg']>0.7]))\n\n# 공식 : (x-min) / (max-min)\n# min_mpg = df['mpg'].min()\n# max_mpg = df['mpg'].max()\n# df['mpg'] = (df['mpg'] - min_mpg)/(max_mpg - min_mpg)\n# cond1 = (df['mpg']>0.7)\n# print(len(df[cond1]))\n\n5\n\n\n\n# 문제 14\n# wt 컬럼에 대해 상자그림 기준으로 이상치의 개수를 구하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\n# 이상치 : Q1, Q3 로부터 1.5*IQR을 넘어가는 값\nQ1 = df['wt'].quantile(.25)\nQ3 = df['wt'].quantile(.75)\nIQR = Q3-Q1\n\nupper = Q3 + (1.5*IQR)\nlower = Q1 - (1.5*IQR)\n\ncond1 = (df['wt'] > upper)\ncond2 = (df['wt'] < lower)\n\nprint(len(df[cond1] + df[cond2]))\n\n3\n\n\n\n# 문제 15\n# 판매수 컬럼의 결측치를 최소값으로 대체하고\n# 결측치가 있을 때와 최소값으로 대체했을 때\n# 평균값의 차이를 절대값으로 정수로 출력하시오\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    '판매수' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf\n\n\n\n\n\n  \n    \n      \n      날짜\n      제품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3.0\n      300\n    \n    \n      1\n      20220105\n      B\n      NaN\n      400\n    \n    \n      2\n      None\n      None\n      5.0\n      500\n    \n    \n      3\n      20230127\n      B\n      10.0\n      600\n    \n    \n      4\n      20220203\n      A\n      10.0\n      400\n    \n    \n      5\n      20220205\n      None\n      10.0\n      500\n    \n    \n      6\n      20230210\n      A\n      15.0\n      500\n    \n    \n      7\n      20230223\n      B\n      15.0\n      600\n    \n    \n      8\n      20230312\n      A\n      20.0\n      600\n    \n    \n      9\n      20230422\n      B\n      NaN\n      700\n    \n    \n      10\n      20220505\n      A\n      30.0\n      600\n    \n    \n      11\n      20230511\n      A\n      40.0\n      600\n    \n  \n\n\n\n\n\n# (풀이)\n# 데이터 복사\ndf2 = df.copy()\n# 최소값으로 결측치 대체\nmin = df['판매수'].min()\ndf2['판매수'] = df2['판매수'].fillna(min)\n\n# 결측치가 있을때, 대체했을때 평균\nm_yes = df['판매수'].mean()\nm_no = df2['판매수'].mean()\n\nprint(round(abs(m_yes - m_no)))\n\n2\n\n\n\n# 문제 16\n# vs변수가 0이 아닌 차량 중에 mpg 값이 가장 큰 차량의 hp 값을 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\ncond1 = df[df['vs']!=0]\n# mpg 값이 가장 큰 차량(내림차순)\ncond1 = cond1.sort_values('mpg',ascending=False)\n# cond1\nprint(cond1['hp'].iloc[0])\n\n65\n\n\n\n# 문제 17\n# gear 변수값이 3, 4인 두 그룹의 hp 표준편차값의 차이를 절대값으로 \n# 소수점 첫째자리까지 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\n# 두개 그룹으로 필터링\ncond1 = df[df['gear']==3]\ncond2 = df[df['gear']==4]\n# std 구하기\nstd3 = cond1['hp'].std()\nstd4 = cond2['hp'].std()\n\nprint(round(abs(std3 - std4),1))\n\n21.8\n\n\n\n# 문제 18\n# gear 변수의 갑별로 그룹화하여 mpg 평균값을 산출하고\n# 평균값이 높은 그룹의 mpg 제3사분위수 값을 구하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\ndf['gear'].unique()\ngear3 = df[df['gear']==3]\ngear4 = df[df['gear']==4]\ngear5 = df[df['gear']==5]\n\nm_3 = gear3['mpg'].mean()\nm_4 = gear4['mpg'].mean()\nm_5 = gear5['mpg'].mean()\n\n(m_3, m_4, m_5) # m_4의 평균값이 가장 큼\nQ3 = gear4['mpg'].quantile(.75)\nprint(Q3)\n\n28.075\n\n\n\n# (풀이_v2)\n# gear, mpg 변수만 필터링\ndf = df.loc[:, ['gear', 'mpg']]\n\n# gear 변수로 그룹화하여 mpg 평균값 보기\n# print(df.groupby('gear').mean()) # gear 4그룹이 제일 높음\n\n# gear=4인 그룹의 mpg Q3 구하기\ngear4 = df[df['gear']==4]\nprint(gear4['mpg'].quantile(.75))\n\n28.075\n\n\n\n# 문제 19\n# hp 항목의 상위 7번째 값으로 상위 7개 값을 변환한 후,\n# hp가 150 이상인 데이터를 추출하여 hp의 평균값을 반올림하여 정수로 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\n# hp 열 기준으로 내림차순 정렬\ndf = df.sort_values('hp', ascending=False)\n\n# 인덱스 초기화 - 내림차순으로 정렬시 최초의 인덱스로 있기에\ndf = df.reset_index(drop=True) # drop=True 는 기존 index 삭제\n# print(df)\n\n# hp 상위 7번째 값 확인\ntop7 = df['hp'].loc[6] # top7 = 205\n\n# np.where 활용\nimport numpy as np\ndf['hp'] = np.where(df['hp'] >= 205, top7, df['hp'])\n# np.where(조건, 조건에 해당할 때 값, 그렇지 않을 때 값)\n\n# hp 150이상인 데이터\ncond1 = (df['hp']>=150)\ndf = df[cond1]\nprint(round(df['hp'].mean()))\n\n187\n\n\n\n# 문제 20\n# car 변수에 Merc 무구가 포함된 자동차의 mpg 평균값을 정수로 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\ndf2 = df[df['car'].str.contains('Merc')] # 문자열 추출 알아두기\nprint(round(df2['mpg'].mean()))\n\n# 시험환경에서 답구하는 방법(reset_index() 사용 후)\n# 시험에서는 car가 index로 되어 있음\n# df.reset_index(inplace=True)\n# df2 = df[df['index'].str.contains(\"Merc\")] \n# print(round(df2['mpg'].mean()))\n\n19"
  },
  {
    "objectID": "BigData_Analysis/실기_1유형.html#핵심문제-27개-2127",
    "href": "BigData_Analysis/실기_1유형.html#핵심문제-27개-2127",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "✅ 핵심문제 27개 (21~27)",
    "text": "✅ 핵심문제 27개 (21~27)\n\n# 문제 21\n# 22년 1분기 A제품의 매출액을 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    '판매수' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      날짜\n      제품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# # (풀이)\n# cond1 = df[df['날짜']<='20220431']\n# cond2 = cond1[cond1['제품']=='A']\n# cond2['매출액'] = (cond2['판매수']*cond2['개당수익'])\n# cond2\n\n# print(cond2['매출액'].sum())\n\n\n# (풀이_v2)\n# 데이터 타입 datetime으로 변경\ndf['날짜'] = pd.to_datetime(df['날짜'])\n\n# 년, 월, 일 변수(열) 추가하기\ndf['year'] = df['날짜'].dt.year\ndf['month'] = df['날짜'].dt.month\ndf['day'] = df['날짜'].dt.day\n\n# 매출액 변수 추가하기\ndf['매출액'] = df['판매수'] * df['개당수익']\n\n# 22년으로 필터링\ndf = df[df['year']==2022]\ndf.head()\n\n# 1,2,3월 매출액 계산\nm1 = df[df['month']==1]['매출액'].sum()\nm2 = df[df['month']==2]['매출액'].sum()\nm3 = df[df['month']==3]['매출액'].sum()\nprint(m1+m2+m3)\n\n11900\n\n\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['날짜'] = pd.to_datetime(df['날짜'])\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['year'] = df['날짜'].dt.year\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['month'] = df['날짜'].dt.month\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['day'] = df['날짜'].dt.day\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['매출액'] = df['판매수'] * df['개당수익']\n\n\n\n# 문제 22\n# 22년과 23년의 총 매출액 차이를 절대값으로 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    '판매수' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      날짜\n      제품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# (풀이)\n# 데이터 타입 datetime으로 변경\ndf['날짜'] = pd.to_datetime(df['날짜'])\n\n# 년 변수(열) 추가하기\ndf['year'] = df['날짜'].dt.year\n\n# 매출액 변수 추가하기\ndf['매출액'] = df['판매수'] * df['개당수익']\n\ncond22 = df[df['year']==2022]\ncond23 = df[df['year']==2023]\n\nprint(abs((cond22['매출액'].sum()) - (cond23['매출액'].sum())))\n\n48600\n\n\n\n# 문제 23\n# 23년 총 매출액이 큰 제품의 23년 판매수를 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    '판매수' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      날짜\n      제품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# 데이터 타입 datetime으로 변경\ndf['날짜'] = pd.to_datetime(df['날짜'])\n\n# 년 변수(열) 추가하기\ndf['year'] = df['날짜'].dt.year\n\n# 매출액 변수 추가하기\ndf['매출액'] = df['판매수'] * df['개당수익']\n\ndf = df[df['year']==2023]\n# df.head()\n\n# 23년 A 매출액과 B 매출액 별도로 구하기\n# A 매출액\ndf_a = df[df['제품']=='A']\na_sales = df_a['매출액'].sum()\n# print(a_sales) # 46000\n\n# B 매출액\ndf_b = df[df['제품']=='B']\nb_sales = df_b['매출액'].sum()\n# print(b_sales) # 32500\n\n# A 제품의 23년 판매수\na_sum = df_a['판매수'].sum()\nprint(a_sum)\n\n80\n\n\n\n# 문제 24\n# 매출액이 4천원 초과, 1만원 미만인 데이터 수를 출력하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    '판매수' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      날짜\n      제품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# (풀이)\ndf['매출액'] = df['판매수'] * df['개당수익']\ncond1 = df['매출액']>4000\ncond2 = df['매출액']<10000\n\ndf = df[cond1&cond2]\nprint(len(df))\n\n4\n\n\n\n# 문제 25\n# 23년 9월 24일 16:00~22:00 사이에 전체 제품의 판매수를 구하시오\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600]})\ntime = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf['time'] = time\ndf = df[['time', '물품', '판매수', '개당수익']]\ndf\n\n\n\n\n\n  \n    \n      \n      time\n      물품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      1\n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2\n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      3\n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      4\n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      5\n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      6\n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# (풀이)\n# 컬럼과 인덱스에 둘다 time 변수를 놓고 풀이\n# 데이터 타입 datetime으로 변경 (항상 df.info()로 데이터 타입 확인할 것)\ndf['time'] = pd.to_datetime(df['time'])\n\n# index 새로 지정\ndf = df.set_index('time', drop=False) # default는 True\n\n# 9월 24일 필터링 // df['변수'].between( , )\ndf_after = df[df['time'].between('2023-09-24', '2023-09-25')] # 25일은 미포함\n\n# 시간 필터링 16:00 ~ 22:00 (주의: 시간이 index에 위치해야 함)\ndf = df_after.between_time(start_time='16:00', end_time='22:00') # 포함 기준\n\n# 전체 판매수 구하기\nprint(df['판매수'].sum())\n\n25\n\n\n\n# (풀이2) // 위에 df 새로 불러와서 실행해보기\n# 데이터 타입 datetime으로 변경\ndf['time'] = pd.to_datetime(df['time'])\n\n# index 새로 지정\ndf = df.set_index('time')\n\n# loc 함수로 필터링\ndf = df.loc[(df.index >= '2023-09-24 16:00:00') & (df.index <= '2023-09-24 22:00:00')]\n\n# 전체 판매수 구하기\nprint(df['판매수'].sum())\n\n25\n\n\n\n# 문제 26\n# 9월 25일 00:10~12:00까지의 B물품의 매출액 총합을 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600]})\ndf['time'] = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf = df[['time', '물품', '판매수', '개당수익']]\ndf = df.set_index('time',drop=True)\ndf\n\n\n\n\n\n  \n    \n      \n      물품\n      판매수\n      개당수익\n    \n    \n      time\n      \n      \n      \n    \n  \n  \n    \n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# (풀이)\n# 매출액 변수 추가\ndf['매출액'] = df['판매수'] * df['개당수익']\n\n# loc 함수로 필터링\ndf = df.loc[(df.index >= '2023-09-25 00:10:00') & (df.index <= '2023-09-25 12:00:00')]\n\n# B 물품의 매출액 총합\ncond1 = df[df['물품']=='B']\nprint(cond1['매출액'].sum())\n\n26500\n\n\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\1645811142.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['매출액'] = df['판매수'] * df['개당수익']\n\n\n\n# 문제 27\n# 9월 24일 12:00~24:00까지의 A물품의 매출액 총합을 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600]})\ndf['time'] = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf = df[['time', '물품', '판매수', '개당수익']]\ndf = df.set_index('time',drop=True)\ndf\n\n\n\n\n\n  \n    \n      \n      물품\n      판매수\n      개당수익\n    \n    \n      time\n      \n      \n      \n    \n  \n  \n    \n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# (풀이)\n# 매출액 변수 추가\ndf['매출액'] = df['판매수'] * df['개당수익']\n\n# loc 함수로 필터링\ndf = df.loc[(df.index >= '2023-09-24 12:00:00') & (df.index < '2023-09-25 00:00:00')]\n\n# A 물품의 매출액 총합\ncond1 = df[df['물품']=='A']\nprint(cond1['매출액'].sum())\n\n10000"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_1.html",
    "href": "BigData_Analysis/실기_2유형_1.html",
    "title": "빅분기 실기 - 2유형 문제풀이(1)",
    "section": "",
    "text": "실기 2 유형(1)\n지도학습만 현재까지 나옴"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_1.html#제2유형_연습하기_와인종류분류",
    "href": "BigData_Analysis/실기_2유형_1.html#제2유형_연습하기_와인종류분류",
    "title": "빅분기 실기 - 2유형 문제풀이(1)",
    "section": "제2유형_연습하기_와인종류분류",
    "text": "제2유형_연습하기_와인종류분류"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_1.html#데이터-분석-분서",
    "href": "BigData_Analysis/실기_2유형_1.html#데이터-분석-분서",
    "title": "빅분기 실기 - 2유형 문제풀이(1)",
    "section": "✅ 데이터 분석 분서",
    "text": "✅ 데이터 분석 분서\n\n1. 라이브러리 및 데이터 확인\n\n\n2. 데이터 탐색(EDA)\n\n\n3. 데이터 전처리 및 분리\n\n\n4. 모델링 및 성능평가\n\n\n5. 예측값 제출"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_1.html#라이브러리-및-데이터-확인-1",
    "href": "BigData_Analysis/실기_2유형_1.html#라이브러리-및-데이터-확인-1",
    "title": "빅분기 실기 - 2유형 문제풀이(1)",
    "section": "✅ 1. 라이브러리 및 데이터 확인",
    "text": "✅ 1. 라이브러리 및 데이터 확인\n\nimport pandas as pd\nimport numpy as np\n\n\n####### 실기환경 복사 영역 #######\nimport pandas as pd\nimport numpy as np\n# 실기 시험 데이터셋으로 세팅하기 (수정금지)\n\nfrom sklearn.datasets import load_wine\n# 와인 데이터셋을 로드합니다\nwine = load_wine()\nx = pd.DataFrame(wine.data, columns=wine.feature_names)\ny = pd.DataFrame(wine.target)\n\n# 실기 시험 데이터셋으로 세팅하기 (수정금지)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2,\n                                                   stratify=y,\n                                                   random_state=2023)\nx_test = pd.DataFrame(x_test)\nx_train = pd.DataFrame(x_train)\ny_train = pd.DataFrame(y_train)\n\nx_test.reset_index()\ny_train.columns = ['target']\n####### 실기환경 복사 영역 #######\n\n🍷 와인의 종류를 분류해보자\n\n데이터의 결측치, 이상치에 대해 처리하고\n분류모델을 사용하여 정확도, F1 score, AUC 값을 산출하시오\n제출은 result 변수에 담아 양식에 맞게 제출하시오\n\n\n# 데이터 설명\n# print(wine.DESCR)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_1.html#데이터-탐색eda-1",
    "href": "BigData_Analysis/실기_2유형_1.html#데이터-탐색eda-1",
    "title": "빅분기 실기 - 2유형 문제풀이(1)",
    "section": "✅ 2. 데이터 탐색(EDA)",
    "text": "✅ 2. 데이터 탐색(EDA)\n\n# 데이터 행/열 확인\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\n\n(142, 13)\n(36, 13)\n(142, 1)\n\n\n\n# 초기 데이터 확인\nprint(x_train.head(3))\nprint(x_test.head(3))\nprint(y_train.head(3))\n\n     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n52     13.82        1.75  2.42               14.0      111.0           3.88   \n146    13.88        5.04  2.23               20.0       80.0           0.98   \n44     13.05        1.77  2.10               17.0      107.0           3.00   \n\n     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n52         3.74                  0.32             1.87             7.05  1.01   \n146        0.34                  0.40             0.68             4.90  0.58   \n44         3.00                  0.28             2.03             5.04  0.88   \n\n     od280/od315_of_diluted_wines  proline  \n52                           3.26   1190.0  \n146                          1.33    415.0  \n44                           3.35    885.0  \n     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n168    13.58        2.58  2.69               24.5      105.0           1.55   \n144    12.25        3.88  2.20               18.5      112.0           1.38   \n151    12.79        2.67  2.48               22.0      112.0           1.48   \n\n     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n168        0.84                  0.39             1.54             8.66  0.74   \n144        0.78                  0.29             1.14             8.21  0.65   \n151        1.36                  0.24             1.26            10.80  0.48   \n\n     od280/od315_of_diluted_wines  proline  \n168                          1.80    750.0  \n144                          2.00    855.0  \n151                          1.47    480.0  \n     target\n52        0\n146       2\n44        0\n\n\n\n# 변수명과 데이터 타입이 매칭이 되는지, 결측치가 있는지 확인\nprint(x_train.info())\nprint(x_test.info())\nprint(y_train.info())\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 142 entries, 52 to 115\nData columns (total 13 columns):\n #   Column                        Non-Null Count  Dtype  \n---  ------                        --------------  -----  \n 0   alcohol                       142 non-null    float64\n 1   malic_acid                    142 non-null    float64\n 2   ash                           142 non-null    float64\n 3   alcalinity_of_ash             142 non-null    float64\n 4   magnesium                     142 non-null    float64\n 5   total_phenols                 142 non-null    float64\n 6   flavanoids                    142 non-null    float64\n 7   nonflavanoid_phenols          142 non-null    float64\n 8   proanthocyanins               142 non-null    float64\n 9   color_intensity               142 non-null    float64\n 10  hue                           142 non-null    float64\n 11  od280/od315_of_diluted_wines  142 non-null    float64\n 12  proline                       142 non-null    float64\ndtypes: float64(13)\nmemory usage: 15.5 KB\nNone\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 36 entries, 168 to 55\nData columns (total 13 columns):\n #   Column                        Non-Null Count  Dtype  \n---  ------                        --------------  -----  \n 0   alcohol                       36 non-null     float64\n 1   malic_acid                    36 non-null     float64\n 2   ash                           36 non-null     float64\n 3   alcalinity_of_ash             36 non-null     float64\n 4   magnesium                     36 non-null     float64\n 5   total_phenols                 36 non-null     float64\n 6   flavanoids                    36 non-null     float64\n 7   nonflavanoid_phenols          36 non-null     float64\n 8   proanthocyanins               36 non-null     float64\n 9   color_intensity               36 non-null     float64\n 10  hue                           36 non-null     float64\n 11  od280/od315_of_diluted_wines  36 non-null     float64\n 12  proline                       36 non-null     float64\ndtypes: float64(13)\nmemory usage: 3.9 KB\nNone\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 142 entries, 52 to 115\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype\n---  ------  --------------  -----\n 0   target  142 non-null    int32\ndtypes: int32(1)\nmemory usage: 1.7 KB\nNone\n\n\n\n# x_train과 x_test 데이터의 기초통계량을 잘 비교해보세요\nprint(x_train.describe())\nprint(x_test.describe())\nprint(y_train.describe())\n\n          alcohol  malic_acid         ash  alcalinity_of_ash   magnesium  \\\ncount  142.000000  142.000000  142.000000         142.000000  142.000000   \nmean    13.025915    2.354296    2.340211          19.354225   98.732394   \nstd      0.812423    1.142722    0.279910           3.476825   13.581859   \nmin     11.030000    0.740000    1.360000          10.600000   70.000000   \n25%     12.370000    1.610000    2.190000          16.800000   88.000000   \n50%     13.050000    1.820000    2.320000          19.300000   97.000000   \n75%     13.685000    3.115000    2.510000          21.500000  106.750000   \nmax     14.830000    5.800000    3.230000          30.000000  151.000000   \n\n       total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\ncount     142.000000  142.000000            142.000000       142.000000   \nmean        2.303592    2.043592              0.361479         1.575070   \nstd         0.633955    1.033597              0.124627         0.576798   \nmin         0.980000    0.340000              0.140000         0.410000   \n25%         1.757500    1.227500              0.270000         1.242500   \n50%         2.335000    2.100000              0.325000         1.555000   \n75%         2.800000    2.917500              0.437500         1.950000   \nmax         3.880000    5.080000              0.630000         3.580000   \n\n       color_intensity         hue  od280/od315_of_diluted_wines      proline  \ncount       142.000000  142.000000                    142.000000   142.000000  \nmean          5.005070    0.950394                      2.603592   742.112676  \nstd           2.150186    0.220736                      0.709751   317.057395  \nmin           1.280000    0.540000                      1.270000   290.000000  \n25%           3.300000    0.782500                      1.922500   496.250000  \n50%           4.850000    0.960000                      2.780000   660.000000  \n75%           6.122500    1.097500                      3.170000   981.250000  \nmax          13.000000    1.710000                      3.920000  1680.000000  \n         alcohol  malic_acid        ash  alcalinity_of_ash   magnesium  \\\ncount  36.000000   36.000000  36.000000           36.00000   36.000000   \nmean   12.900833    2.265556   2.470278           20.05000  103.722222   \nstd     0.813112    1.021943   0.226066            2.70275   16.371772   \nmin    11.640000    0.890000   2.000000           14.60000   84.000000   \n25%    12.230000    1.592500   2.300000           18.00000   91.500000   \n50%    12.835000    1.885000   2.470000           19.50000  101.000000   \n75%    13.635000    2.792500   2.605000           21.70000  112.000000   \nmax    14.390000    4.950000   3.220000           26.50000  162.000000   \n\n       total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\ncount      36.000000   36.000000             36.000000        36.000000   \nmean        2.261667    1.972778              0.363333         1.653333   \nstd         0.600259    0.858882              0.125516         0.558012   \nmin         1.350000    0.660000              0.130000         0.840000   \n25%         1.715000    1.175000              0.267500         1.320000   \n50%         2.420000    2.175000              0.395000         1.550000   \n75%         2.602500    2.682500              0.435000         1.972500   \nmax         3.850000    3.490000              0.660000         3.280000   \n\n       color_intensity        hue  od280/od315_of_diluted_wines     proline  \ncount        36.000000  36.000000                     36.000000    36.00000  \nmean          5.267222   0.985278                      2.643611   765.75000  \nstd           2.915076   0.258694                      0.720100   309.94851  \nmin           2.080000   0.480000                      1.290000   278.00000  \n25%           2.875000   0.787500                      2.037500   542.50000  \n50%           4.325000   0.985000                      2.790000   682.50000  \n75%           6.900000   1.167500                      3.192500   996.25000  \nmax          11.750000   1.450000                      4.000000  1480.00000  \n           target\ncount  142.000000\nmean     0.936620\nstd      0.773816\nmin      0.000000\n25%      0.000000\n50%      1.000000\n75%      2.000000\nmax      2.000000\n\n\n\n# y 데이터도 구체적으로 살펴볼 필요있음\nprint(y_train.head())\n\nprint(y_train.value_counts())\n\n     target\n52        0\n146       2\n44        0\n67        1\n43        0\ntarget\n1         57\n0         47\n2         38\ndtype: int64"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_1.html#데이터-전처리-및-분리-1",
    "href": "BigData_Analysis/실기_2유형_1.html#데이터-전처리-및-분리-1",
    "title": "빅분기 실기 - 2유형 문제풀이(1)",
    "section": "✅ 3. 데이터 전처리 및 분리",
    "text": "✅ 3. 데이터 전처리 및 분리\n\n1) 결측치, 2) 이상치, 3) 변수 처리하기\n\n# 결측치 확인\nprint(x_train.isnull().sum())\nprint(x_test.isnull().sum())\nprint(y_train.isnull().sum())\n\nalcohol                         0\nmalic_acid                      0\nash                             0\nalcalinity_of_ash               0\nmagnesium                       0\ntotal_phenols                   0\nflavanoids                      0\nnonflavanoid_phenols            0\nproanthocyanins                 0\ncolor_intensity                 0\nhue                             0\nod280/od315_of_diluted_wines    0\nproline                         0\ndtype: int64\nalcohol                         0\nmalic_acid                      0\nash                             0\nalcalinity_of_ash               0\nmagnesium                       0\ntotal_phenols                   0\nflavanoids                      0\nnonflavanoid_phenols            0\nproanthocyanins                 0\ncolor_intensity                 0\nhue                             0\nod280/od315_of_diluted_wines    0\nproline                         0\ndtype: int64\ntarget    0\ndtype: int64\n\n\n\n# 결측치 제거\n# df = df.dropna()\n# print(df)\n\n# 참고사항\n# print(df.dropna().shape) # 행기준으로 삭제\n\n\n# 결측치 대체(평균값, 중앙값, 최빈값)\n# 연속형 변수 : 중앙값, 평균값\n#     - df['변수명'].median()\n#     - df['변수명'].mean()\n# 범주형 변수 : 최빈값\n\n# df['변수명'] = df['변수명'].fillna(대체할 값)\n\n\n# 이상치 대체(예시)\n# df['변수명'] = np.where(df['변수명']>=5, 대체할 값, df['변수명'])\n\n\n# 변수처리\n\n# 불필요한 변수 제거\n# df = df.drop(columns = ['변수1', '변수2'])\n# df = df.drop(['변수1', '변수2'], axis=1)\n\n# 필요시 변수 추가(파생변수 생성)\n# df['파생변수명'] = df['A'] * df['B'] (파생변수 생성 수식)\n\n# 원핫인코딩(가변수 처리)\n# x_train = pd.get_dummies(x_train)\n# x_test = pd.get_dummies(x_test)\n# print(x_train.info())\n# print(x_test.info())\n\n\n\n데이터 분리\n\n# 데이터를 훈련 세트와 검증용 세트로 분할 (80% 훈련, 20% 검증용)\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train['target'],\n                                                   test_size = 0.2,\n                                                   stratify = y_train['target'],\n                                                   random_state = 2023)\nprint(x_train.shape)\nprint(x_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)\n\n(113, 13)\n(29, 13)\n(113,)\n(29,)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_1.html#모델링-및-성능평가-1",
    "href": "BigData_Analysis/실기_2유형_1.html#모델링-및-성능평가-1",
    "title": "빅분기 실기 - 2유형 문제풀이(1)",
    "section": "✅ 4. 모델링 및 성능평가",
    "text": "✅ 4. 모델링 및 성능평가\n\n# 랜덤포레스트 모델 사용 (참고: 회귀모델은 RandomForestRegressor)\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nmodel.fit(x_train, y_train)\n\nRandomForestClassifier()\n\n\n\n# 모델을 사용하여 테스트 데이터 예측\ny_pred = model.predict(x_val)\n\n\n# 모델 성늘 평가 (정확도, F1 score, 민감도, 특이도 등)\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score\nacc = accuracy_score(y_val, y_pred)              # (실제값, 예측값)\nf1 = f1_score(y_val, y_pred, average = 'macro')  # (실제값, 예측값)\n# auc = roc_auc_score(y_val, y_pred)             # (실제값, 예측값)  * AUC는 이진분류\n\n\n# 정확도(Accuracy)\nprint(acc)\n\n1.0\n\n\n\n# macro f1 score\nprint(f1)\n\n1.0"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_1.html#예측값-제출-1",
    "href": "BigData_Analysis/실기_2유형_1.html#예측값-제출-1",
    "title": "빅분기 실기 - 2유형 문제풀이(1)",
    "section": "✅ 5. 예측값 제출",
    "text": "✅ 5. 예측값 제출\n\n(주의) test 셋을 모델에 넣어 나온 예측값을 제출해야함\n\n# (실기시험 안내사항)\n# 아래 코드 예측변수와 수험번호를 개인별로 변경하여 활용\n# pd.DataFrame({ 'result':y_result }).to_csv('수험번호_csv', index=False)\n\n# 모델을 사용하여 테스트 데이터 예측\n\n# 1. 특정 클래스로 분류할 경우 (predict)\ny_result = model.predict(x_test)\nprint(y_result[:5])\n\n# 2. 특정 클래스로 분류될 확률을 구할 경우 (predict_proba)\ny_result_prob = model.predict_proba(x_test)\nprint(y_result_prob[:5])\n\n# 이해해보기\nresult_prob = pd.DataFrame({\n    'result' : y_result,\n    'prob_0' : y_result_prob[:, 0],\n    'prob_1' : y_result_prob[:, 1],\n    'prob_2' : y_result_prob[:, 2],\n})\n# Class 0일 확률 : y_result_prob[:, 0]\n# Class 1일 확률 : y_result_prob[:, 1]\n# Class 2일 확률 : y_result_prob[:, 2]\n\nprint(result_prob[:5])\n\n[2 2 2 0 1]\n[[0.01 0.05 0.94]\n [0.13 0.17 0.7 ]\n [0.   0.08 0.92]\n [0.99 0.01 0.  ]\n [0.08 0.81 0.11]]\n   result  prob_0  prob_1  prob_2\n0       2    0.01    0.05    0.94\n1       2    0.13    0.17    0.70\n2       2    0.00    0.08    0.92\n3       0    0.99    0.01    0.00\n4       1    0.08    0.81    0.11"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_2.html",
    "href": "BigData_Analysis/실기_2유형_2.html",
    "title": "빅분기 실기 - 2유형 문제풀이(2)",
    "section": "",
    "text": "실기 2 유형(2)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_2.html#데이터-분석-순서",
    "href": "BigData_Analysis/실기_2유형_2.html#데이터-분석-순서",
    "title": "빅분기 실기 - 2유형 문제풀이(2)",
    "section": "✅ 데이터 분석 순서",
    "text": "✅ 데이터 분석 순서\n\n1. 라이브러리 및 데이터 확인\n\n\n2. 데이터 탐색(EDA)\n\n\n3. 데이터 전처리 및 분리\n\n\n4. 모델링 및 성능평가\n\n\n5. 예측값 제출"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_2.html#라이브러리-및-데이터-확인-1",
    "href": "BigData_Analysis/실기_2유형_2.html#라이브러리-및-데이터-확인-1",
    "title": "빅분기 실기 - 2유형 문제풀이(2)",
    "section": "✅ 1. 라이브러리 및 데이터 확인",
    "text": "✅ 1. 라이브러리 및 데이터 확인\n\nimport pandas as pd\nimport numpy as np\n\n\n###### 실기환경 복사 영역 ######\nimport pandas as pd\nimport numpy as np\n# 실기 시험 데이터셋으로 셋팅하기 (수정금지)\nfrom sklearn.datasets import load_diabetes\n# diabetes 데이터셋 로드\ndiabetes = load_diabetes()\nx = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\ny = pd.DataFrame(diabetes.target)\n\n# 실시 시험 데이터셋으로 셋팅하기 (수정금지)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2,\n                                                   random_state = 2023)\n\nx_test = pd.DataFrame(x_test.reset_index())\nx_train = pd.DataFrame(x_train.reset_index())\ny_train = pd.DataFrame(y_train.reset_index())\n\nx_test.rename(columns = {'index':'cust_id'}, inplace=True)\nx_train.rename(columns = {'index':'cust_id'}, inplace=True)\ny_train.columns = ['cust_id', 'target']\n###### 실기환경 복사 영역 ######\n\n\n# from sklearn.datasets import load_diabetes\n# diabetes = load_diabetes()\n# x = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n# y = pd.DataFrame(diabetes.target)\n\n# from sklearn.model_selection import train_test_split\n# x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,\n#                                                 random_state=2023)\n\n# x_test = pd.DataFrame(x_test.reset_index())\n# x_train = pd.DataFrame(x_train.reset_index())\n# y_train = pd.DataFrame(y_train.reset_index())\n\n# x_test.rename(columns={'index':'cust_id'},inplace=True)\n# x_train.rename(columns={'index':'cust_id'},inplace=True)\n# y_train.columns=['cust']\n\n\n🏥 당뇨병 환자의 질병 진행정도를 예측해보자\n\n\n- 데이터의 결측치, 이상치, 변수들에 대해 전처리하고\n\n\n- 회귀모델을 사용하여 Rsq, MSE 값을 산출하시오\n\n\n- 제출은 cust_id, target 변수를 가진 dataframe 형태로 제출하시오\n\n# 데이터 설명\nprint(diabetes.DESCR)\n\n.. _diabetes_dataset:\n\nDiabetes dataset\n----------------\n\nTen baseline variables, age, sex, body mass index, average blood\npressure, and six blood serum measurements were obtained for each of n =\n442 diabetes patients, as well as the response of interest, a\nquantitative measure of disease progression one year after baseline.\n\n**Data Set Characteristics:**\n\n  :Number of Instances: 442\n\n  :Number of Attributes: First 10 columns are numeric predictive values\n\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n\n  :Attribute Information:\n      - age     age in years\n      - sex\n      - bmi     body mass index\n      - bp      average blood pressure\n      - s1      tc, total serum cholesterol\n      - s2      ldl, low-density lipoproteins\n      - s3      hdl, high-density lipoproteins\n      - s4      tch, total cholesterol / HDL\n      - s5      ltg, possibly log of serum triglycerides level\n      - s6      glu, blood sugar level\n\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n\nSource URL:\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n\nFor more information see:\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_2.html#데이터-탐색eda-1",
    "href": "BigData_Analysis/실기_2유형_2.html#데이터-탐색eda-1",
    "title": "빅분기 실기 - 2유형 문제풀이(2)",
    "section": "✅ 2. 데이터 탐색(EDA)",
    "text": "✅ 2. 데이터 탐색(EDA)\n\n# 데이터의 행/열 확인\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\n\n(353, 11)\n(89, 11)\n(353, 2)\n\n\n\n# 초기 데이터 확인\n\nprint(x_train.head(3))\nprint(x_test.head(3))\nprint(y_train.head(3))\n\n   cust_id       age       sex       bmi        bp        s1        s2  \\\n0        4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596   \n1      318  0.088931 -0.044642  0.006728  0.025315  0.030078  0.008707   \n2      301 -0.001882  0.050680 -0.024529  0.052858  0.027326  0.030001   \n\n         s3        s4        s5        s6  \n0  0.008142 -0.002592 -0.031991 -0.046641  \n1  0.063367 -0.039493  0.009436  0.032059  \n2  0.030232 -0.002592 -0.021394  0.036201  \n   cust_id       age       sex       bmi        bp        s1        s2  \\\n0      280  0.009016  0.050680  0.018584  0.039087  0.017694  0.010586   \n1      412  0.074401 -0.044642  0.085408  0.063187  0.014942  0.013091   \n2       68  0.038076  0.050680 -0.029918 -0.040099 -0.033216 -0.024174   \n\n         s3        s4        s5        s6  \n0  0.019187 -0.002592  0.016305 -0.017646  \n1  0.015505 -0.002592  0.006209  0.085907  \n2 -0.010266 -0.002592 -0.012908  0.003064  \n   cust_id  target\n0        4   135.0\n1      318   109.0\n2      301    65.0\n\n\n\n# 변수명과 데이터 타입이 매칭이 되는지, 결측치가 있는지 확인해보세요\n\nprint(x_train.info())\nprint(x_test.info())\nprint(y_train.info())\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 353 entries, 0 to 352\nData columns (total 11 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   cust_id  353 non-null    int64  \n 1   age      353 non-null    float64\n 2   sex      353 non-null    float64\n 3   bmi      353 non-null    float64\n 4   bp       353 non-null    float64\n 5   s1       353 non-null    float64\n 6   s2       353 non-null    float64\n 7   s3       353 non-null    float64\n 8   s4       353 non-null    float64\n 9   s5       353 non-null    float64\n 10  s6       353 non-null    float64\ndtypes: float64(10), int64(1)\nmemory usage: 30.5 KB\nNone\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 89 entries, 0 to 88\nData columns (total 11 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   cust_id  89 non-null     int64  \n 1   age      89 non-null     float64\n 2   sex      89 non-null     float64\n 3   bmi      89 non-null     float64\n 4   bp       89 non-null     float64\n 5   s1       89 non-null     float64\n 6   s2       89 non-null     float64\n 7   s3       89 non-null     float64\n 8   s4       89 non-null     float64\n 9   s5       89 non-null     float64\n 10  s6       89 non-null     float64\ndtypes: float64(10), int64(1)\nmemory usage: 7.8 KB\nNone\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 353 entries, 0 to 352\nData columns (total 2 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   cust_id  353 non-null    int64  \n 1   target   353 non-null    float64\ndtypes: float64(1), int64(1)\nmemory usage: 5.6 KB\nNone\n\n\n\n# x_train 과 x_test 데이터의 기초통계량을 잘 비교해보세요\n\nprint(x_train.describe())\nprint(x_test.describe())\nprint(y_train.describe())\n\n          cust_id         age         sex         bmi          bp          s1  \\\ncount  353.000000  353.000000  353.000000  353.000000  353.000000  353.000000   \nmean   212.634561    0.000804    0.000724    0.000640   -0.000326    0.001179   \nstd    126.668903    0.047617    0.047673    0.048141    0.046585    0.047891   \nmin      0.000000   -0.107226   -0.044642   -0.084886   -0.112400   -0.126781   \n25%    105.000000   -0.038207   -0.044642   -0.035307   -0.033214   -0.033216   \n50%    210.000000    0.005383   -0.044642   -0.006206   -0.005671   -0.002945   \n75%    322.000000    0.038076    0.050680    0.030440    0.032201    0.027326   \nmax    441.000000    0.110727    0.050680    0.170555    0.125158    0.153914   \n\n               s2          s3          s4          s5          s6  \ncount  353.000000  353.000000  353.000000  353.000000  353.000000  \nmean     0.001110   -0.000452    0.000901    0.001446    0.000589  \nstd      0.048248    0.048600    0.048045    0.047160    0.048122  \nmin     -0.115613   -0.102307   -0.076395   -0.126097   -0.137767  \n25%     -0.029184   -0.039719   -0.039493   -0.033249   -0.034215  \n50%     -0.001314   -0.006584   -0.002592    0.000271    0.003064  \n75%      0.031567    0.030232    0.034309    0.033657    0.032059  \nmax      0.198788    0.181179    0.185234    0.133599    0.135612  \n          cust_id        age        sex        bmi         bp         s1  \\\ncount   89.000000  89.000000  89.000000  89.000000  89.000000  89.000000   \nmean   251.696629  -0.003188  -0.002871  -0.002537   0.001292  -0.004676   \nstd    127.901365   0.047761   0.047563   0.045665   0.051777   0.046493   \nmin      9.000000  -0.099961  -0.044642  -0.090275  -0.108957  -0.091006   \n25%    148.000000  -0.034575  -0.044642  -0.030996  -0.036656  -0.037344   \n50%    280.000000  -0.001882  -0.044642  -0.009439  -0.005671  -0.009825   \n75%    366.000000   0.030811   0.050680   0.034751   0.042530   0.031454   \nmax    436.000000   0.096197   0.050680   0.137143   0.132044   0.119515   \n\n              s2         s3         s4         s5         s6  \ncount  89.000000  89.000000  89.000000  89.000000  89.000000  \nmean   -0.004401   0.001792  -0.003575  -0.005737  -0.002334  \nstd     0.045030   0.043723   0.045980   0.049252   0.045757  \nmin    -0.089935  -0.080217  -0.076395  -0.104365  -0.129483  \n25%    -0.030437  -0.028674  -0.039493  -0.038459  -0.030072  \n50%    -0.014153  -0.002903  -0.002592  -0.014956  -0.005220  \n75%     0.020607   0.022869   0.003312   0.024053   0.019633  \nmax     0.130208   0.122273   0.141322   0.133599   0.135612  \n          cust_id      target\ncount  353.000000  353.000000\nmean   212.634561  152.943343\nstd    126.668903   75.324692\nmin      0.000000   37.000000\n25%    105.000000   90.000000\n50%    210.000000  141.000000\n75%    322.000000  208.000000\nmax    441.000000  346.000000\n\n\n\n# y 데이터도 구체적으로 살펴보세요\nprint(y_train.head())\n\n   cust_id  target\n0        4   135.0\n1      318   109.0\n2      301    65.0\n3      189    79.0\n4      288    80.0\n\n\n\n# y 데이터도 구체적으로 살펴보세요\nprint(y_train.describe())\n\n          cust_id      target\ncount  353.000000  353.000000\nmean   212.634561  152.943343\nstd    126.668903   75.324692\nmin      0.000000   37.000000\n25%    105.000000   90.000000\n50%    210.000000  141.000000\n75%    322.000000  208.000000\nmax    441.000000  346.000000"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_2.html#데이터-전처리-및-분리-1",
    "href": "BigData_Analysis/실기_2유형_2.html#데이터-전처리-및-분리-1",
    "title": "빅분기 실기 - 2유형 문제풀이(2)",
    "section": "✅ 3. 데이터 전처리 및 분리",
    "text": "✅ 3. 데이터 전처리 및 분리\n\n1) 결측치, 2) 이상치, 3) 변수처리하기\n\n# 결측치 확인\n\nprint(x_train.isnull().sum())\nprint(x_test.isnull().sum())\nprint(y_train.isnull().sum())\n\ncust_id    0\nage        0\nsex        0\nbmi        0\nbp         0\ns1         0\ns2         0\ns3         0\ns4         0\ns5         0\ns6         0\ndtype: int64\ncust_id    0\nage        0\nsex        0\nbmi        0\nbp         0\ns1         0\ns2         0\ns3         0\ns4         0\ns5         0\ns6         0\ndtype: int64\ncust_id    0\ntarget     0\ndtype: int64\n\n\n\n# 결측치 제거\n# df = df.dropna()\n# print(df)\n\n# 참고사항\n# print(df.dropna().shape) # 행 기준으로 삭제\n\n\n# 결측치 대체 (평균값, 중앙값, 최빈값)\n# 연속형 변수 :  중앙값, 평균값\n# df['변수명'].median()\n# df['변수명'].mean()\n\n# 범주형 변수 :  최빈값\n# df['변수명'].mode()\n\n\n# df['변수명'] = df['변수명'].fillna(대체할값)\n\n\n# 이상치 대체\n# (참고) df['변수명'] = np.where(df['변수명'] >= 5, 대체할 값, df['변수명'])\n\n\n# 변수처리\n\n# 불필요한 변수 제거\n# df = df.drop(columns = ['변수1', '변수2'])\n# df = df.drop(['변수1', '변수2'], axis=1)\n\n# 필요시 변수 추가(파생변수 생성)\n# df['파생변수명'] = df['A'] * df['B'] (파생변수 생성 수식)\n\n# 원핫인코딩(가변수 처리)\n# x_train = pd.get_dummies(x_train)\n# x_test = pd.get_dummies(x_test)\n\n# print(x_train.info())\n# print(x_test.info())\n\n\n# 변수처리\n\n# 불필요한 변수(columns) 제거\n# cust_id 는 불필요한 변수이므로 제거합니다.\n# 단, test셋의 cust_id가 나중에 제출이 필요하기 떄문에 별도로 저장\n\ncust_id = x_test['cust_id'].copy()\n\n# 각 데이터에서 cust_id 변수 제거\nx_train = x_train.drop(columns = ['cust_id']) # drop(columns = ['변수1', '변수2'])\nx_test = x_test.drop(columns = ['cust_id'])\n\n\n\n데이터 분리\n\n# 데이터를 훈련 세트와 검증용 세트로 분할 (80% 훈련, 20% 검증용)\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train['target'],\n                                                   test_size = 0.2,\n                                                   random_state = 23)\n# 분류 모델에서는 층화(starify)를 할 필요가 없다. 연속형인 경우에만 사용\n\nprint(x_train.shape)\nprint(x_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)\n\n(282, 10)\n(71, 10)\n(282,)\n(71,)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_2.html#모델링-및-성능평가-1",
    "href": "BigData_Analysis/실기_2유형_2.html#모델링-및-성능평가-1",
    "title": "빅분기 실기 - 2유형 문제풀이(2)",
    "section": "✅ 4. 모델링 및 성능평가",
    "text": "✅ 4. 모델링 및 성능평가\n\n# 랜덤포레스 모델 사용 (참고 : 분류모델은 RandomForestClassifier)\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(random_state = 2023)\nmodel.fit(x_train, y_train)\n\nRandomForestRegressor(random_state=2023)\n\n\n\n# 모델을 사용하여 테스트 데이터 예측\ny_pred = model.predict(x_val)\n\n\n# 모델 성능 평가 (평균 제곱 오차 및 R-squared)\nfrom sklearn.metrics import mean_squared_error, r2_score\nmse = mean_squared_error(y_val, y_pred) # (실제값, 예측값)\nr2 = r2_score(y_val, y_pred) # (실제값, 예측값)\n\n\n# MSE(mean_squared_error, 평균 제곱 오차)\nprint(mse)\n\n2571.6276845070424\n\n\n\n# R2 score(R-squared)\nprint(r2)\n\n0.5235611874726152\n\n\n\n# RMSE (root mean squared error)\nrmse = mse ** 0.5\nprint(rmse)\n\n50.71121852713699"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_2.html#예측값-제출-1",
    "href": "BigData_Analysis/실기_2유형_2.html#예측값-제출-1",
    "title": "빅분기 실기 - 2유형 문제풀이(2)",
    "section": "✅ 5. 예측값 제출",
    "text": "✅ 5. 예측값 제출\n\n(주의) x_test 를 모델에 넣어 나온 예측값을 제출해야함\n\n#(실기시험 안내사항)\n# 아래 코드 예측변수와 수험번호를 개인별로 변경하여 활용\n# pd.DataFrame({'cust_id':cust_id, 'target':y_result}).to_csv('0030000.csv', index=False)\n\n# 모델을 사용하여 테스트 데이터 예측\ny_result = model.predict(x_test)\nresult = pd.DataFrame({'cust_id':cust_id, 'target':y_result})\nprint(result[:5])\n\n   cust_id  target\n0      280  186.51\n1      412  255.92\n2       68   77.97\n3      324  184.22\n4      101  111.14\n\n\n\n# ★tip : 데이터를 저장한 다음 불러와서 제대로 제출했는지 확인해보자\n# pd.DataFrame({'result':y_result}).to_csv('수험번호.csv', index=False)\n# df2 = pd.read_csv(\"수험번호.csv\")\n# print(df2.head())"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html",
    "href": "BigData_Analysis/실기_2유형_3.html",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "",
    "text": "실기 2 유형(3)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html#데이터-분석-순서",
    "href": "BigData_Analysis/실기_2유형_3.html#데이터-분석-순서",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "✅ 데이터 분석 순서",
    "text": "✅ 데이터 분석 순서\n\n1. 라이브러리 및 데이터 확인\n\n\n2. 데이터 탐색(EDA)\n\n\n3. 데이터 전처리 및 분리\n\n\n4. 모델링 및 성능평가\n\n\n5. 예측값 제출"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html#라이브러리-및-데이터-확인-1",
    "href": "BigData_Analysis/실기_2유형_3.html#라이브러리-및-데이터-확인-1",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "✅ 1. 라이브러리 및 데이터 확인",
    "text": "✅ 1. 라이브러리 및 데이터 확인\n\nimport pandas as pd\nimport numpy as np\n\n\n###### 실기환경 복사 영역 ######\n# 실기 시험 데이터셋으로 셋팅하기 (수정금지)\n\nimport seaborn as sns\n# tips 데이터셋 로드\ndf = sns.load_dataset('tips')\n\nx = df.drop(['tip'], axis=1)\ny = df['tip']\n\n\n# 실시 시험 데이터셋으로 셋팅하기 (수정금지)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2,\n                                                   random_state = 2023)\n\nx_test = pd.DataFrame(x_test.reset_index())\nx_train = pd.DataFrame(x_train.reset_index())\ny_train = pd.DataFrame(y_train.reset_index())\n\nx_test.rename(columns = {'index':'cust_id'}, inplace=True)\nx_train.rename(columns = {'index':'cust_id'}, inplace=True)\ny_train.columns = ['cust_id', 'target']\n###### 실기환경 복사 영역 ######"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html#레스토랑의-예측-문제",
    "href": "BigData_Analysis/실기_2유형_3.html#레스토랑의-예측-문제",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "레스토랑의 예측 문제",
    "text": "레스토랑의 예측 문제"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html#데이터의-결측치-이상치-변수에-대해-처리하고",
    "href": "BigData_Analysis/실기_2유형_3.html#데이터의-결측치-이상치-변수에-대해-처리하고",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "- 데이터의 결측치, 이상치, 변수에 대해 처리하고",
    "text": "- 데이터의 결측치, 이상치, 변수에 대해 처리하고"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html#회귀모델을-사용하여-rsq-mse-값을-산출하시오",
    "href": "BigData_Analysis/실기_2유형_3.html#회귀모델을-사용하여-rsq-mse-값을-산출하시오",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "- 회귀모델을 사용하여 Rsq, MSE 값을 산출하시오",
    "text": "- 회귀모델을 사용하여 Rsq, MSE 값을 산출하시오"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html#데이터셋-설명",
    "href": "BigData_Analysis/실기_2유형_3.html#데이터셋-설명",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "데이터셋 설명",
    "text": "데이터셋 설명\n\n\ntotal_bill(): 손님의 식사 총 청구액\ntip(팁): 팁의 양\nsex(성별): 손님의 성별\nsmoker(흡연자): 손님의 흡연 여부(“Yes” 또는 “No”)\nday(요일): 식사가 이루어진 요일\ntime(시간): 점심 또는 저녁 중 언제 식사가 이루어졌는지\nsize(인원 수): 식사에 참석한 인원 수"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html#데이터-탐색eda-1",
    "href": "BigData_Analysis/실기_2유형_3.html#데이터-탐색eda-1",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "✅ 2. 데이터 탐색(EDA)",
    "text": "✅ 2. 데이터 탐색(EDA)\n\n# 데이터의 행/열 확인\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\n\n(195, 7)\n(49, 7)\n(195, 2)\n\n\n\n# 초기 데이터 확인\n\nprint(x_train.head(3))\nprint(x_test.head(3))\nprint(y_train.head(3))\n\n   cust_id  total_bill     sex smoker  day    time  size\n0      158       13.39  Female     No  Sun  Dinner     2\n1      186       20.90  Female    Yes  Sun  Dinner     3\n2       21       20.29  Female     No  Sat  Dinner     2\n   cust_id  total_bill     sex smoker  day    time  size\n0      154       19.77    Male     No  Sun  Dinner     4\n1        4       24.59  Female     No  Sun  Dinner     4\n2       30        9.55    Male     No  Sat  Dinner     2\n   cust_id  target\n0      158    2.61\n1      186    3.50\n2       21    2.75\n\n\n\n# 변수명과 데이터 타입이 매칭이 되는지, 결측치가 있는지 확인해보세요\n\nprint(x_train.info())\nprint(x_test.info())\nprint(y_train.info())\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 195 entries, 0 to 194\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   cust_id     195 non-null    int64   \n 1   total_bill  195 non-null    float64 \n 2   sex         195 non-null    category\n 3   smoker      195 non-null    category\n 4   day         195 non-null    category\n 5   time        195 non-null    category\n 6   size        195 non-null    int64   \ndtypes: category(4), float64(1), int64(2)\nmemory usage: 6.0 KB\nNone\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 49 entries, 0 to 48\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   cust_id     49 non-null     int64   \n 1   total_bill  49 non-null     float64 \n 2   sex         49 non-null     category\n 3   smoker      49 non-null     category\n 4   day         49 non-null     category\n 5   time        49 non-null     category\n 6   size        49 non-null     int64   \ndtypes: category(4), float64(1), int64(2)\nmemory usage: 2.0 KB\nNone\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 195 entries, 0 to 194\nData columns (total 2 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   cust_id  195 non-null    int64  \n 1   target   195 non-null    float64\ndtypes: float64(1), int64(1)\nmemory usage: 3.2 KB\nNone\n\n\n\n# x_train 과 x_test 데이터의 기초통계량을 잘 비교해보세요\n\nprint(x_train.describe())\nprint(x_test.describe())\nprint(y_train.describe())\n\n          cust_id  total_bill        size\ncount  195.000000  195.000000  195.000000\nmean   122.056410   20.054667    2.543590\nstd     70.668034    8.961645    0.942631\nmin      0.000000    3.070000    1.000000\n25%     59.500000   13.420000    2.000000\n50%    121.000000   17.920000    2.000000\n75%    182.500000   24.395000    3.000000\nmax    243.000000   50.810000    6.000000\n          cust_id  total_bill       size\ncount   49.000000   49.000000  49.000000\nmean   119.285714   18.716531   2.673469\nstd     70.918674    8.669864   0.987162\nmin      2.000000    5.750000   2.000000\n25%     62.000000   12.740000   2.000000\n50%    123.000000   16.660000   2.000000\n75%    180.000000   21.010000   3.000000\nmax    239.000000   44.300000   6.000000\n          cust_id      target\ncount  195.000000  195.000000\nmean   122.056410    3.021692\nstd     70.668034    1.402690\nmin      0.000000    1.000000\n25%     59.500000    2.000000\n50%    121.000000    2.920000\n75%    182.500000    3.530000\nmax    243.000000   10.000000\n\n\n\n# object, category 데이터도 추가 확인\n# print(x_trian.describe(include='object'))\n# print(x_test.describe(include='object'))\n\nprint(x_train.describe(include='category'))\nprint(x_test.describe(include='category'))\n\n         sex smoker  day    time\ncount    195    195  195     195\nunique     2      2    4       2\ntop     Male     No  Sat  Dinner\nfreq     125    120   71     142\n         sex smoker  day    time\ncount     49     49   49      49\nunique     2      2    4       2\ntop     Male     No  Sat  Dinner\nfreq      32     31   16      34\n\n\n\n# y데이터도 구체적으로 살펴보세요\nprint(y_train.head())\n\n   cust_id  target\n0      158    2.61\n1      186    3.50\n2       21    2.75\n3       74    2.20\n4       43    1.32\n\n\n\n# y데이터도 구체적으로 살펴보세요\nprint(y_train.describe().T)\n\n         count        mean        std  min   25%     50%     75%    max\ncust_id  195.0  122.056410  70.668034  0.0  59.5  121.00  182.50  243.0\ntarget   195.0    3.021692   1.402690  1.0   2.0    2.92    3.53   10.0"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html#데이터-전처리-및-분리-1",
    "href": "BigData_Analysis/실기_2유형_3.html#데이터-전처리-및-분리-1",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "✅ 3. 데이터 전처리 및 분리",
    "text": "✅ 3. 데이터 전처리 및 분리\n\n1) 결측치, 2) 이상치, 3) 변수 처리하기\n\n# 결측치 확인\n\nprint(x_train.isnull().sum())\nprint(x_test.isnull().sum())\nprint(y_train.isnull().sum())\n\ncust_id       0\ntotal_bill    0\nsex           0\nsmoker        0\nday           0\ntime          0\nsize          0\ndtype: int64\ncust_id       0\ntotal_bill    0\nsex           0\nsmoker        0\nday           0\ntime          0\nsize          0\ndtype: int64\ncust_id    0\ntarget     0\ndtype: int64\n\n\n\n# 결측치 제거\n# df = df.dropna()\n# print(df)\n\n# 참고사항\n# print(df.dropna().shape) # 행 기준으로 삭제\n\n\n# 결측치 대체 (평균값, 중앙값, 최빈값)\n# 연속형 변수 :  중앙값, 평균값\n# df['변수명'].median()\n# df['변수명'].mean()\n\n# 범주형 변수 :  최빈값\n# df['변수명'].mode()\n\n\n# df['변수명'] = df['변수명'].fillna(대체할값)\n\n\n# 이상치 대체\n# (참고) df['변수명'] = np.where(df['변수명'] >= 5, 대체할 값, df['변수명'])\n\n\n# 변수처리\n\n# 불필요한 변수 제거\n# df = df.drop(columns = ['변수1', '변수2'])\n# df = df.drop(['변수1', '변수2'], axis=1)\n\n# 필요시 변수 추가(파생변수 생성)\n# df['파생변수명'] = df['A'] * df['B'] (파생변수 생성 수식)\n\n# 원핫인코딩(가변수 처리)\n# x_train = pd.get_dummies(x_train)\n# x_test = pd.get_dummies(x_test)\n\n# print(x_train.info())\n# print(x_test.info())\n\n\n# 변수처리\n\n# 불필요한 변수(columns) 제거\n# cust_id 는 불필요한 변수이므로 제거합니다.\n# 단, test셋의 cust_id가 나중에 제출이 필요하기 떄문에 별도로 저장\n\ncust_id = x_test['cust_id'].copy()\n\n# 각 데이터에서 cust_id 변수 제거\nx_train = x_train.drop(columns = ['cust_id']) # drop(columns = ['변수1', '변수2'])\nx_test = x_test.drop(columns = ['cust_id'])\n\n\n# 변수처리(원핫인코딩)\nprint(x_train.info())\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 195 entries, 0 to 194\nData columns (total 6 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   total_bill  195 non-null    float64 \n 1   sex         195 non-null    category\n 2   smoker      195 non-null    category\n 3   day         195 non-null    category\n 4   time        195 non-null    category\n 5   size        195 non-null    int64   \ndtypes: category(4), float64(1), int64(1)\nmemory usage: 4.5 KB\nNone\n\n\n\n# 변수처리(원핫인코딩)\nx_train = pd.get_dummies(x_train)\nx_test = pd.get_dummies(x_test)\n\nprint(x_train.info())\nprint(x_test.info())\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 195 entries, 0 to 194\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   total_bill   195 non-null    float64\n 1   size         195 non-null    int64  \n 2   sex_Male     195 non-null    uint8  \n 3   sex_Female   195 non-null    uint8  \n 4   smoker_Yes   195 non-null    uint8  \n 5   smoker_No    195 non-null    uint8  \n 6   day_Thur     195 non-null    uint8  \n 7   day_Fri      195 non-null    uint8  \n 8   day_Sat      195 non-null    uint8  \n 9   day_Sun      195 non-null    uint8  \n 10  time_Lunch   195 non-null    uint8  \n 11  time_Dinner  195 non-null    uint8  \ndtypes: float64(1), int64(1), uint8(10)\nmemory usage: 5.1 KB\nNone\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 49 entries, 0 to 48\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   total_bill   49 non-null     float64\n 1   size         49 non-null     int64  \n 2   sex_Male     49 non-null     uint8  \n 3   sex_Female   49 non-null     uint8  \n 4   smoker_Yes   49 non-null     uint8  \n 5   smoker_No    49 non-null     uint8  \n 6   day_Thur     49 non-null     uint8  \n 7   day_Fri      49 non-null     uint8  \n 8   day_Sat      49 non-null     uint8  \n 9   day_Sun      49 non-null     uint8  \n 10  time_Lunch   49 non-null     uint8  \n 11  time_Dinner  49 non-null     uint8  \ndtypes: float64(1), int64(1), uint8(10)\nmemory usage: 1.4 KB\nNone\n\n\n\n\n데이터 분리\n\n# 데이터를 훈련 세트와 검증용 세트로 분할 (80% 훈련, 20% 검증용)\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train['target'],\n                                                   test_size = 0.2,\n                                                   random_state = 23)\n# 분류 모델에서는 층화(starify)를 할 필요가 없다. 연속형인 경우에만 사용\n\nprint(x_train.shape)\nprint(x_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)\n\n(156, 12)\n(39, 12)\n(156,)\n(39,)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html#모델링-밑-성능평가",
    "href": "BigData_Analysis/실기_2유형_3.html#모델링-밑-성능평가",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "✅ 4. 모델링 밑 성능평가",
    "text": "✅ 4. 모델링 밑 성능평가\n\n# 랜덤포레스트 모델 사용 (참고: 분류모델은 RandomForestClassifier)\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(random_state=2023)\nmodel.fit(x_train, y_train)\n\nRandomForestRegressor(random_state=2023)\n\n\n\n# 모델을 사용하여 테스트 데이터 예측\ny_pred = model.predict(x_val)\n\n\n# 모델 성능 평가 (R-squared, MSE 등)\nfrom sklearn.metrics import r2_score, mean_squared_error\nr2 = r2_score(y_val, y_pred)            # (실제값, 예측값)\nmse = mean_squared_error(y_val, y_pred) # (실제값, 예측값)\n\n\n# MSE\nprint(mse)\n\n0.9812277338461534\n\n\n\n# RMSE\nrmse = mse**0.5\nprint(rmse)\n\n0.990569398803614\n\n\n\n# R2 score(R-squared)\nprint(r2)\n\n0.4286497615634072"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html#예측값-제출-1",
    "href": "BigData_Analysis/실기_2유형_3.html#예측값-제출-1",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "✅ 5. 예측값 제출",
    "text": "✅ 5. 예측값 제출\n\n(주의) x_test 를 모델에 넣어 나온 예측값을 제출해야함\n\n#(실기시험 안내사항)\n# 아래 코드 예측변수와 수험번호를 개인별로 변경하여 활용\n# pd.DataFrame({'cust_id':cust_id, 'target':y_result}).to_csv('0030000.csv', index=False)\n\n# 모델을 사용하여 테스트 데이터 예측\ny_result = model.predict(x_test)\nresult = pd.DataFrame({'cust_id':cust_id, 'target':y_result})\nprint(result[:5])\n\n   cust_id  target\n0      154  3.2266\n1        4  4.1160\n2       30  1.8966\n3       75  1.8735\n4       33  3.0267\n\n\n\n# ★tip : 데이터를 저장한 다음 불러와서 제대로 제출했는지 확인해보자\n# pd.DataFrame({'result':y_result}).to_csv('수험번호.csv', index=False)\n# df2 = pd.read_csv(\"수험번호.csv\")\n# print(df2.head())"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html",
    "href": "BigData_Analysis/실기_2유형_4.html",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "",
    "text": "실기 2 유형(4)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html#데이터-분석-순서",
    "href": "BigData_Analysis/실기_2유형_4.html#데이터-분석-순서",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "✅ 데이터 분석 순서",
    "text": "✅ 데이터 분석 순서\n\n1. 라이브러리 및 데이터 확인\n\n\n2. 데이터 탐색(EDA)\n\n\n3. 데이터 전처리 및 분리\n\n\n4. 모델링 및 성능평가\n\n\n5. 예측값 제출"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html#라이브러리-및-데이터-확인-1",
    "href": "BigData_Analysis/실기_2유형_4.html#라이브러리-및-데이터-확인-1",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "✅ 1. 라이브러리 및 데이터 확인",
    "text": "✅ 1. 라이브러리 및 데이터 확인\n\nimport pandas as pd\nimport numpy as np\n\n\n###############  실기환경 복사 영역  ###############\nimport pandas as pd\nimport numpy as np\n# 실기 시험 데이터셋으로 셋팅하기 (수정금지)\nfrom sklearn.datasets import load_iris\n# Iris 데이터셋을 로드\niris = load_iris()\nx = pd.DataFrame(iris.data, columns=['sepal_length', 'sepal_width',\n                                     'petal_length', 'petal_width'])\ny = iris.target   # 'setosa'=0, 'versicolor'=1, 'virginica'=2\ny = np.where(y>0, 1, 0) # setosa 종은 0, 나머지 종은 1로 변경 // 이진분류\n\n# 실기 시험 데이터셋으로 셋팅하기 (수정금지)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, \n                                                    stratify=y,\n                                                    random_state=2023)\n\nx_test = pd.DataFrame(x_test)\nx_train = pd.DataFrame(x_train)\ny_train = pd.DataFrame(y_train)\ny_train.columns = ['species']\n\n# 결측치 삽입\nx_test['sepal_length'].iloc[0] = None  \nx_train['sepal_length'].iloc[0] = None\n# 이상치 삽입\nx_train['sepal_width'].iloc[0] = 150\n###############  실기환경 복사 영역  ###############"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html#붓꽃iris의-종species을-분류해보자",
    "href": "BigData_Analysis/실기_2유형_4.html#붓꽃iris의-종species을-분류해보자",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "붓꽃(iris)의 종(Species)을 분류해보자",
    "text": "붓꽃(iris)의 종(Species)을 분류해보자"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html#데이터의-결측치-이상치에-대해-처리하고",
    "href": "BigData_Analysis/실기_2유형_4.html#데이터의-결측치-이상치에-대해-처리하고",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "- 데이터의 결측치, 이상치에 대해 처리하고",
    "text": "- 데이터의 결측치, 이상치에 대해 처리하고"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html#분류모델을-사용하여-정확도-f1-score-auc-값을-산출하시오",
    "href": "BigData_Analysis/실기_2유형_4.html#분류모델을-사용하여-정확도-f1-score-auc-값을-산출하시오",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "- 분류모델을 사용하여 정확도, F1 score, AUC 값을 산출하시오",
    "text": "- 분류모델을 사용하여 정확도, F1 score, AUC 값을 산출하시오"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html#제출은-result-변수에-담아-양식에-맞게-제출하시오",
    "href": "BigData_Analysis/실기_2유형_4.html#제출은-result-변수에-담아-양식에-맞게-제출하시오",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "- 제출은 result 변수에 담아 양식에 맞게 제출하시오",
    "text": "- 제출은 result 변수에 담아 양식에 맞게 제출하시오\n\n# 데이터 설명\nprint(iris.DESCR)\n\n.. _iris_dataset:\n\nIris plants dataset\n--------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 150 (50 in each of three classes)\n    :Number of Attributes: 4 numeric, predictive attributes and the class\n    :Attribute Information:\n        - sepal length in cm\n        - sepal width in cm\n        - petal length in cm\n        - petal width in cm\n        - class:\n                - Iris-Setosa\n                - Iris-Versicolour\n                - Iris-Virginica\n                \n    :Summary Statistics:\n\n    ============== ==== ==== ======= ===== ====================\n                    Min  Max   Mean    SD   Class Correlation\n    ============== ==== ==== ======= ===== ====================\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n    ============== ==== ==== ======= ===== ====================\n\n    :Missing Attribute Values: None\n    :Class Distribution: 33.3% for each of 3 classes.\n    :Creator: R.A. Fisher\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n    :Date: July, 1988\n\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\nfrom Fisher's paper. Note that it's the same as in R, but not as in the UCI\nMachine Learning Repository, which has two wrong data points.\n\nThis is perhaps the best known database to be found in the\npattern recognition literature.  Fisher's paper is a classic in the field and\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\ndata set contains 3 classes of 50 instances each, where each class refers to a\ntype of iris plant.  One class is linearly separable from the other 2; the\nlatter are NOT linearly separable from each other.\n\n.. topic:: References\n\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n     Mathematical Statistics\" (John Wiley, NY, 1950).\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n     Structure and Classification Rule for Recognition in Partially Exposed\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n     on Information Theory, May 1972, 431-433.\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n     conceptual clustering system finds 3 classes in the data.\n   - Many, many more ..."
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html#데이터-탐색eda-1",
    "href": "BigData_Analysis/실기_2유형_4.html#데이터-탐색eda-1",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "✅ 2. 데이터 탐색(EDA)",
    "text": "✅ 2. 데이터 탐색(EDA)\n\n# 데이터의 행/열 확인\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\n\n(120, 4)\n(30, 4)\n(120, 1)\n\n\n\n# 초기 데이터 확인\n\nprint(x_train.head(3))\nprint(x_test.head(3))\nprint(y_train.head(3))\n\n    sepal_length  sepal_width  petal_length  petal_width\n2            NaN        150.0           1.3          0.2\n49           5.0          3.3           1.4          0.2\n66           5.6          3.0           4.5          1.5\n     sepal_length  sepal_width  petal_length  petal_width\n93            NaN          2.3           3.3          1.0\n69            5.6          2.5           3.9          1.1\n137           6.4          3.1           5.5          1.8\n   species\n0        0\n1        0\n2        1\n\n\n\n# 변수명과 데이터 타입이 매칭이 되는지, 결측치가 있는지 확인\n\nprint(x_train.info())\nprint(x_test.info())\nprint(y_train.info()) # 현재 train, test에 결측치를 하나씩 넣었기에 확인 가능\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 120 entries, 2 to 44\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   sepal_length  119 non-null    float64\n 1   sepal_width   120 non-null    float64\n 2   petal_length  120 non-null    float64\n 3   petal_width   120 non-null    float64\ndtypes: float64(4)\nmemory usage: 4.7 KB\nNone\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 30 entries, 93 to 55\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   sepal_length  29 non-null     float64\n 1   sepal_width   30 non-null     float64\n 2   petal_length  30 non-null     float64\n 3   petal_width   30 non-null     float64\ndtypes: float64(4)\nmemory usage: 1.2 KB\nNone\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 120 entries, 0 to 119\nData columns (total 1 columns):\n #   Column   Non-Null Count  Dtype\n---  ------   --------------  -----\n 0   species  120 non-null    int32\ndtypes: int32(1)\nmemory usage: 608.0 bytes\nNone\n\n\n\n# x_train 과 x_test 데이터의 기초통계량을 잘 비교해보세요\n\nprint(x_train.describe())\nprint(x_test.describe())\nprint(y_train.describe())\n\n       sepal_length  sepal_width  petal_length  petal_width\ncount    119.000000     120.0000    120.000000   120.000000\nmean       5.920168       4.2950      3.816667     1.226667\nstd        0.841667      13.4191      1.798848     0.780512\nmin        4.300000       2.2000      1.100000     0.100000\n25%        5.150000       2.8000      1.575000     0.300000\n50%        6.000000       3.0000      4.400000     1.350000\n75%        6.500000       3.4000      5.225000     1.800000\nmax        7.900000     150.0000      6.900000     2.500000\n       sepal_length  sepal_width  petal_length  petal_width\ncount     29.000000    30.000000     30.000000     30.00000\nmean       5.596552     3.000000      3.523333      1.09000\nstd        0.709367     0.522593      1.631518      0.68549\nmin        4.600000     2.000000      1.000000      0.10000\n25%        5.000000     2.625000      1.600000      0.35000\n50%        5.500000     3.000000      4.050000      1.15000\n75%        5.900000     3.300000      4.925000      1.57500\nmax        7.600000     4.200000      6.600000      2.30000\n          species\ncount  120.000000\nmean     0.666667\nstd      0.473381\nmin      0.000000\n25%      0.000000\n50%      1.000000\n75%      1.000000\nmax      1.000000\n\n\n\n# y 데이터도 구체적으로 살펴보세요\nprint(y_train.head())\n\n   species\n0        0\n1        0\n2        1\n3        1\n4        1\n\n\n\n# y 데이터도 구체적으로 살펴보세요\nprint(y_train.value_counts())\n\nspecies\n1          80\n0          40\ndtype: int64"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html#데이터-전처리-및-분리-1",
    "href": "BigData_Analysis/실기_2유형_4.html#데이터-전처리-및-분리-1",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "✅ 3. 데이터 전처리 및 분리",
    "text": "✅ 3. 데이터 전처리 및 분리\n\n1) 결측치, 2) 이상치, 3) 변수 처리하기\n\n# 결측치 확인\nprint(x_train.isnull().sum())\nprint(x_test.isnull().sum())\nprint(y_train.isnull().sum())\n\nsepal_length    1\nsepal_width     0\npetal_length    0\npetal_width     0\ndtype: int64\nsepal_length    1\nsepal_width     0\npetal_length    0\npetal_width     0\ndtype: int64\nspecies    0\ndtype: int64\n\n\n\n# 결측치 제거 // 데이터의 수가 많은 경우\n# df = df.dropna()\n# print(df)\n\n# 참고사항\n# print(df.dropna().shape) # 행 기준으로 삭제\n\n\n# 결측치 대체(평균값, 중앙값, 최빈값)\n\n# 연속형 변수 : 중앙값, 평균값\n#  - df['변수명'].median()\n#  - df['변수명'].mean()\n\n# 범수형 변수 :  최빈값\n\n# df['변수명'] = df['변수명'].fillna(대체할 값)\n\n\n# 결측치 대체(중앙값)\n# ** 주의사항 : train 데이터의 중앙값으로 test 데이터도 변경해줘야 함 **\n\nmedian = x_train['sepal_length'].median()\nx_train['sepal_length'] = x_train['sepal_length'].fillna(median)\nx_test['sepal_length'] = x_test['sepal_length'].fillna(median)\n\n\n# 이상치 확인\ncond1 = (x_train['sepal_width']>=10)\nprint(len(x_train[cond1]))\n\n1\n\n\n\n# 이상치 대체\n# (참고) df['변수명'] = np.where( df['변수명'] >= 5, 대체할 값, df['변수명'])\n\n# 예를 들어 'sepal_width' 값이 10이 넘으면 이상치라고 가정해본다명\n# 이상치를 제외한 Max 값을 구해서 대체해보자\ncond1 = (x_train['sepal_width'] <= 10)\nmax_sw = x_train[cond1]['sepal_width'].max()\nprint(max_sw)\n\nx_train['sepal_width'] = np.where(x_train['sepal_width'] >=10, max_sw,\n                                 x_train['sepal_width'])\nprint(x_train.describe())\n\n4.4\n       sepal_length  sepal_width  petal_length  petal_width\ncount    120.000000   120.000000    120.000000   120.000000\nmean       5.920833     3.081667      3.816667     1.226667\nstd        0.838155     0.429966      1.798848     0.780512\nmin        4.300000     2.200000      1.100000     0.100000\n25%        5.175000     2.800000      1.575000     0.300000\n50%        6.000000     3.000000      4.400000     1.350000\n75%        6.500000     3.400000      5.225000     1.800000\nmax        7.900000     4.400000      6.900000     2.500000\n\n\n\n# 변수처리\n\n# 불필요한 변수 제거\n# df = df.drop(columns = ['변수1', '변수2'])\n# df = df.drop(['변수1', '변수2'], axis = 1)\n\n# 필요시 변수 추가(파생변수 생성)\n# df['파생변수명'] = df['A'] * df['B'] (파생변수 생성 수식)\n\n# 원핫인코딩(가변수 처리)\n# x_train = pd.get_dummies(x_train)\n# x_test = pd.get_dummies(x_test)\n# print(x_train.info())\n# print(x_test.info())\n\n\n\n데이터 분리\n\n#  데이터를 훈련 세트와 검증용 세트로 분할 (80% 훈련, 20% 검증)\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_train,\n                                                  y_train['species'],\n                                                  test_size=0.2,\n                                                  stratify = y_train['species'],\n                                                  random_state = 2023)\nprint(x_train.shape)\nprint(x_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)\n\n(96, 4)\n(24, 4)\n(96,)\n(24,)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html#모델링-및-성능평가-1",
    "href": "BigData_Analysis/실기_2유형_4.html#모델링-및-성능평가-1",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "✅ 4. 모델링 및 성능평가",
    "text": "✅ 4. 모델링 및 성능평가\n\n# 랜덤포레스트 모델 사용 (참고 : 회귀모델은 RandomForestRegressor)\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nmodel.fit(x_train, y_train)\n\nRandomForestClassifier()\n\n\n\n# 모델을 사용하여 테스트 데이터 예측\ny_pred = model.predict(x_val)\n\n\n# 모델 성능 평가 (accuracy, f1 score, AUC 등)\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score\nacc = accuracy_score(y_val, y_pred)  # (실제값, 예측값)\nf1 = f1_score(y_val, y_pred)         # (실제값, 예측값)\n# 다중분류인 경우 f1  f1_score(y_val, y_pred, average = 'macro')\nauc = roc_auc_score(y_val, y_pred)   # (실제값, 예측값)\n\n\n# 정확도(Accuracy)\nprint(acc)\n\n1.0\n\n\n\n# F1 Score\nprint(f1)\n\n1.0\n\n\n\n# AUC\nprint(auc)\n\n1.0\n\n\n\n# 참고사항\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_val, y_pred)\nprint(cm)\n\n# #####  예측\n# #####  0  1\n# 실제 0 TN FP\n# 실제 1 FN TP\n\n[[ 8  0]\n [ 0 16]]"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html#예측값-제출-1",
    "href": "BigData_Analysis/실기_2유형_4.html#예측값-제출-1",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "✅ 5. 예측값 제출",
    "text": "✅ 5. 예측값 제출\n\n(주의) x_test 를 모델에 넣어 나온 예측값을 제출해야 함\n\n#(실기시험 안내사항)\n# 아래 코드 예측변수와 수험번호를 개인별로 변경하여 활용\n# pd.DataFrame({'cust_id':cust_id, 'target':y_result}).to_csv('0030000.csv', index=False)\n\n# 모델을 사용하여 테스트 데이터 예측\n\n# 1. 특정 클래스로 분류할 경우 (predict)\ny_result = model.predict(x_test)\nprint(y_result[:5])\n\n# 2. 특정 클래스로 분류될 확률을 구할 경우 (predict_proba)\ny_result_prob = model.predict_proba(x_test)\nprint(y_result_prob[:5])\n\n# 이해해보기\nresult_prob = pd.DataFrame({\n    'result' : y_result,\n    'prob_0' : y_result_prob[:,0]\n})\n# setosa 일 확률 : y_result_prob[:, 0]\n# 그 외 종일 확률 : y_result_prob[:, 1]\n\nprint(result_prob[:5])\n\n[1 1 1 0 1]\n[[0.   1.  ]\n [0.   1.  ]\n [0.   1.  ]\n [1.   0.  ]\n [0.04 0.96]]\n   result  prob_0\n0       1    0.00\n1       1    0.00\n2       1    0.00\n3       0    1.00\n4       1    0.04\n\n\n\n# ★tip : 데이터를 저장한 다음 불러와서 제대로 제출했는지 확인해보자\n# pd.DataFrame({'result':y_result}).to_csv('수험번호.csv', index=False)\n# df2 = pd.read_csv(\"수험번호.csv\")\n# print(df2.head())"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_5.html",
    "href": "BigData_Analysis/실기_2유형_5.html",
    "title": "빅분기 실기 - 2유형 문제풀이(5)",
    "section": "",
    "text": "실기 2 유형(5)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_5.html#데이터-분석-순서",
    "href": "BigData_Analysis/실기_2유형_5.html#데이터-분석-순서",
    "title": "빅분기 실기 - 2유형 문제풀이(5)",
    "section": "✅ 데이터 분석 순서",
    "text": "✅ 데이터 분석 순서\n\n1. 라이브러리 및 데이터 확인\n\n\n2. 데이터 탐색(EDA)\n\n\n3. 데이터 전처리 및 분리\n\n\n4. 모델링 및 성능평가\n\n\n5. 예측값 제출"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_5.html#라이브러리-및-데이터-확인-1",
    "href": "BigData_Analysis/실기_2유형_5.html#라이브러리-및-데이터-확인-1",
    "title": "빅분기 실기 - 2유형 문제풀이(5)",
    "section": "✅ 1. 라이브러리 및 데이터 확인",
    "text": "✅ 1. 라이브러리 및 데이터 확인\n\nimport pandas as pd\nimport numpy as np\n\n\n###############  복사 영역  ###############\n# 실기 시험 데이터셋으로 셋팅하기 (수정금지)\n\n# Seaborn의 내장 타이타닉 데이터셋을 불러옵니다.\nimport seaborn as sns\ndf = sns.load_dataset('titanic')\n\nx = df.drop('survived', axis=1)\ny = df['survived']\n\n# 실기 시험 데이터셋으로 셋팅하기 (수정금지)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, \n                                                    stratify=y,\n                                                    random_state=2023)\n\nx_test = pd.DataFrame(x_test)\nx_train = pd.DataFrame(x_train)\ny_train = pd.DataFrame(y_train)\ny_test = pd.DataFrame(y_test) # 평가용\n\nx_test.reset_index()\ny_train.columns = ['target'] \ny_test.columns = ['target'] \n###############  복사 영역  ###############\n\n\n타이타닉 생존자 예측 문제\n\n\n- 데이터의 결측치, 중복 변수값에 대해 처리하고\n\n\n- 분류모델을 사용하여 Accuracy, F1 score, AUC 값을 산출하시오.\n\n\n데이터 설명\n\n\nsurvival : 0 = No, 1 = Yes -pclass : 객실 등급(1,2,3) -sex : 성별 -age : 나이 -sibsp : 타이타닉호에 탑승한 형제/배우자의 수 -parch : 타이타닉호에 탑승한 부모/자녀의 수 -fare : 요금 -embarked : 탑승지 이름(C, Q, S) Cherbourg / Queenstown / Southampton -(중복)class : 객실 등급(First, Second, Third) -who : man, women, child -adult_male : 성인남자인지 여부(True=성인남자, False 그외) -deck : 선실번호 첫 알파벳(A,B,C,D,E,F,G) -(중복) embark_town : 탑승지 이름(Cherbourg, Queenstown, Southampton) -(중복) alive : 생존여부(no:사망, yes:생존) -alone : 혼자 탑승했는지 여부(True=혼자, False=가족과 함께)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_5.html#데이터-탐색eda-1",
    "href": "BigData_Analysis/실기_2유형_5.html#데이터-탐색eda-1",
    "title": "빅분기 실기 - 2유형 문제풀이(5)",
    "section": "✅ 2. 데이터 탐색(EDA)",
    "text": "✅ 2. 데이터 탐색(EDA)\n\n# 데이터의 행/열 확인\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\n\n(712, 14)\n(179, 14)\n(712, 1)\n\n\n\n# 초기 데이터 확인\n\nprint(x_train.head())\nprint(x_test.head())\nprint(y_train.head())\n\n     pclass     sex   age  sibsp  parch   fare embarked   class    who  \\\n3         1  female  35.0      1      0  53.10        S   First  woman   \n517       3    male   NaN      0      0  24.15        Q   Third    man   \n861       2    male  21.0      1      0  11.50        S  Second    man   \n487       1    male  58.0      0      0  29.70        C   First    man   \n58        2  female   5.0      1      2  27.75        S  Second  child   \n\n     adult_male deck  embark_town alive  alone  \n3         False    C  Southampton   yes  False  \n517        True  NaN   Queenstown    no   True  \n861        True  NaN  Southampton    no  False  \n487        True    B    Cherbourg    no   True  \n58        False  NaN  Southampton   yes  False  \n     pclass     sex   age  sibsp  parch      fare embarked   class    who  \\\n800       2    male  34.0      0      0   13.0000        S  Second    man   \n341       1  female  24.0      3      2  263.0000        S   First  woman   \n413       2    male   NaN      0      0    0.0000        S  Second    man   \n575       3    male  19.0      0      0   14.5000        S   Third    man   \n202       3    male  34.0      0      0    6.4958        S   Third    man   \n\n     adult_male deck  embark_town alive  alone  \n800        True  NaN  Southampton    no   True  \n341       False    C  Southampton   yes  False  \n413        True  NaN  Southampton    no   True  \n575        True  NaN  Southampton    no   True  \n202        True  NaN  Southampton    no   True  \n     target\n3         1\n517       0\n861       0\n487       0\n58        1\n\n\n\n# 변수명과 데이터 타입이 매칭이 되는지, 결측치가 있는지 확인해보세요\n\nprint(x_train.info())\nprint(x_test.info())\nprint(y_train.info())\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 712 entries, 3 to 608\nData columns (total 14 columns):\n #   Column       Non-Null Count  Dtype   \n---  ------       --------------  -----   \n 0   pclass       712 non-null    int64   \n 1   sex          712 non-null    object  \n 2   age          579 non-null    float64 \n 3   sibsp        712 non-null    int64   \n 4   parch        712 non-null    int64   \n 5   fare         712 non-null    float64 \n 6   embarked     710 non-null    object  \n 7   class        712 non-null    category\n 8   who          712 non-null    object  \n 9   adult_male   712 non-null    bool    \n 10  deck         164 non-null    category\n 11  embark_town  710 non-null    object  \n 12  alive        712 non-null    object  \n 13  alone        712 non-null    bool    \ndtypes: bool(2), category(2), float64(2), int64(3), object(5)\nmemory usage: 64.4+ KB\nNone\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 179 entries, 800 to 410\nData columns (total 14 columns):\n #   Column       Non-Null Count  Dtype   \n---  ------       --------------  -----   \n 0   pclass       179 non-null    int64   \n 1   sex          179 non-null    object  \n 2   age          135 non-null    float64 \n 3   sibsp        179 non-null    int64   \n 4   parch        179 non-null    int64   \n 5   fare         179 non-null    float64 \n 6   embarked     179 non-null    object  \n 7   class        179 non-null    category\n 8   who          179 non-null    object  \n 9   adult_male   179 non-null    bool    \n 10  deck         39 non-null     category\n 11  embark_town  179 non-null    object  \n 12  alive        179 non-null    object  \n 13  alone        179 non-null    bool    \ndtypes: bool(2), category(2), float64(2), int64(3), object(5)\nmemory usage: 16.6+ KB\nNone\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 712 entries, 3 to 608\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype\n---  ------  --------------  -----\n 0   target  712 non-null    int64\ndtypes: int64(1)\nmemory usage: 11.1 KB\nNone\n\n\n\n# x_train 과 x_test 데이터의 기초통계량을 잘 비교해보세요\n\nprint(x_train.describe())\nprint(x_test.describe())\nprint(y_train.describe())\n\n           pclass         age       sibsp       parch        fare\ncount  712.000000  579.000000  712.000000  712.000000  712.000000\nmean     2.307584   29.479568    0.518258    0.372191   31.741836\nstd      0.834926   14.355304    1.094522    0.792341   45.403910\nmin      1.000000    0.420000    0.000000    0.000000    0.000000\n25%      2.000000   20.000000    0.000000    0.000000    7.895800\n50%      3.000000   28.000000    0.000000    0.000000   14.454200\n75%      3.000000   38.000000    1.000000    0.000000   31.275000\nmax      3.000000   74.000000    8.000000    6.000000  512.329200\n           pclass         age       sibsp       parch        fare\ncount  179.000000  135.000000  179.000000  179.000000  179.000000\nmean     2.312849   30.640741    0.541899    0.418994   34.043364\nstd      0.842950   15.258427    1.137797    0.859760   64.097184\nmin      1.000000    1.000000    0.000000    0.000000    0.000000\n25%      2.000000   22.000000    0.000000    0.000000    7.925000\n50%      3.000000   29.000000    0.000000    0.000000   14.500000\n75%      3.000000   39.000000    1.000000    0.000000   30.285400\nmax      3.000000   80.000000    8.000000    5.000000  512.329200\n           target\ncount  712.000000\nmean     0.383427\nstd      0.486563\nmin      0.000000\n25%      0.000000\n50%      0.000000\n75%      1.000000\nmax      1.000000\n\n\n\n# object, category 데이터도 추가확인\n\nprint(x_train.describe(include=\"object\"))\nprint(x_test.describe(include=\"object\"))\n\nprint(x_train.describe(include=\"category\"))\nprint(x_test.describe(include=\"category\"))\n\n         sex embarked  who  embark_town alive\ncount    712      710  712          710   712\nunique     2        3    3            3     2\ntop     male        S  man  Southampton    no\nfreq     469      518  432          518   439\n         sex embarked  who  embark_town alive\ncount    179      179  179          179   179\nunique     2        3    3            3     2\ntop     male        S  man  Southampton    no\nfreq     108      126  105          126   110\n        class deck\ncount     712  164\nunique      3    7\ntop     Third    C\nfreq      391   47\n        class deck\ncount     179   39\nunique      3    7\ntop     Third    C\nfreq      100   12\n\n\n\n# y 데이터도 구체적으로 살펴보세요\nprint(y_train.head())\n\n     target\n3         1\n517       0\n861       0\n487       0\n58        1\n\n\n\n# y 데이터도 구체적으로 살펴보세요\nprint(y_train.value_counts())\n\ntarget\n0         439\n1         273\ndtype: int64"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_5.html#데이터-전처리-및-분리-1",
    "href": "BigData_Analysis/실기_2유형_5.html#데이터-전처리-및-분리-1",
    "title": "빅분기 실기 - 2유형 문제풀이(5)",
    "section": "✅ 3. 데이터 전처리 및 분리",
    "text": "✅ 3. 데이터 전처리 및 분리\n\n1) 결측치, 2) 이상치, 3) 변수 처리하기\n\n# 결측치 확인\n\nprint(x_train.isnull().sum())\nprint(x_test.isnull().sum())\nprint(y_train.isnull().sum())\n\npclass           0\nsex              0\nage            133\nsibsp            0\nparch            0\nfare             0\nembarked         2\nclass            0\nwho              0\nadult_male       0\ndeck           548\nembark_town      2\nalive            0\nalone            0\ndtype: int64\npclass           0\nsex              0\nage             44\nsibsp            0\nparch            0\nfare             0\nembarked         0\nclass            0\nwho              0\nadult_male       0\ndeck           140\nembark_town      0\nalive            0\nalone            0\ndtype: int64\ntarget    0\ndtype: int64\n\n\n\n# 결측치 제거\n# df = df.dropna()\n# print(df)\n\n# 참고사항\n# print(df.dropna().shape) # 행 기준으로 삭제\n\n\n# 결측치 대체\n# x_train(712, 14) : age(133), embarked(2), deck(548), embark_town(2)\n# x_test(179, 14) : age(44), deck(140)\n\n# 변수 제거\n# (중복)class\n# (중복) embark_town\n# (중복) alive \n# (결측치 다수) deck\n\n\n# 중복변수 제거\nx_train = x_train.drop(['class', 'embark_town', 'alive', 'deck'], axis=1)\nx_test = x_test.drop(['class', 'embark_town', 'alive', 'deck'], axis=1)\n\n\n# 변수 제거 확인\nprint(x_train.info())\nprint(x_test.info())\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 712 entries, 3 to 608\nData columns (total 10 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   pclass      712 non-null    int64  \n 1   sex         712 non-null    object \n 2   age         579 non-null    float64\n 3   sibsp       712 non-null    int64  \n 4   parch       712 non-null    int64  \n 5   fare        712 non-null    float64\n 6   embarked    710 non-null    object \n 7   who         712 non-null    object \n 8   adult_male  712 non-null    bool   \n 9   alone       712 non-null    bool   \ndtypes: bool(2), float64(2), int64(3), object(3)\nmemory usage: 51.5+ KB\nNone\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 179 entries, 800 to 410\nData columns (total 10 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   pclass      179 non-null    int64  \n 1   sex         179 non-null    object \n 2   age         135 non-null    float64\n 3   sibsp       179 non-null    int64  \n 4   parch       179 non-null    int64  \n 5   fare        179 non-null    float64\n 6   embarked    179 non-null    object \n 7   who         179 non-null    object \n 8   adult_male  179 non-null    bool   \n 9   alone       179 non-null    bool   \ndtypes: bool(2), float64(2), int64(3), object(3)\nmemory usage: 12.9+ KB\nNone\n\n\n\n# 결측치 대체\n# x_train(712, 14) : age(133), embarked(2)\n# x_test(179, 14) : age(44)\n\n# age 변수 \nmed_age = x_train['age'].median()\nx_train['age'] = x_train['age'].fillna(med_age)\nx_test['age'] = x_test['age'].fillna(med_age) # train data의 중앙값으로 \n\n# embarked(object 타입은 대체시 주로 최빈값으로)\nmode_et = x_train['embarked'].mode()\nx_train['embarked'] = x_train['embarked'].fillna(mode_et[0]) # 최빈값 [0] 주의\n\n\n# 결측치 대체 여부 확인\nprint(x_train.isnull().sum())\nprint(x_test.isnull().sum())\n\npclass        0\nsex           0\nage           0\nsibsp         0\nparch         0\nfare          0\nembarked      0\nwho           0\nadult_male    0\nalone         0\ndtype: int64\npclass        0\nsex           0\nage           0\nsibsp         0\nparch         0\nfare          0\nembarked      0\nwho           0\nadult_male    0\nalone         0\ndtype: int64\n\n\n\n# 변수처리(원핫인코딩) - object 변수만 적용됨\nx_train = pd.get_dummies(x_train)\nx_test = pd.get_dummies(x_test)\n\nprint(x_train.info())\nprint(x_test.info())\n\n# advanced 버전 사용\nx_train_ad = x_train.copy()\nx_test_ad = x_test.copy()\ny_train_ad = y_train.copy()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 712 entries, 3 to 608\nData columns (total 15 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   pclass      712 non-null    int64  \n 1   age         712 non-null    float64\n 2   sibsp       712 non-null    int64  \n 3   parch       712 non-null    int64  \n 4   fare        712 non-null    float64\n 5   adult_male  712 non-null    bool   \n 6   alone       712 non-null    bool   \n 7   sex_female  712 non-null    uint8  \n 8   sex_male    712 non-null    uint8  \n 9   embarked_C  712 non-null    uint8  \n 10  embarked_Q  712 non-null    uint8  \n 11  embarked_S  712 non-null    uint8  \n 12  who_child   712 non-null    uint8  \n 13  who_man     712 non-null    uint8  \n 14  who_woman   712 non-null    uint8  \ndtypes: bool(2), float64(2), int64(3), uint8(8)\nmemory usage: 40.3 KB\nNone\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 179 entries, 800 to 410\nData columns (total 15 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   pclass      179 non-null    int64  \n 1   age         179 non-null    float64\n 2   sibsp       179 non-null    int64  \n 3   parch       179 non-null    int64  \n 4   fare        179 non-null    float64\n 5   adult_male  179 non-null    bool   \n 6   alone       179 non-null    bool   \n 7   sex_female  179 non-null    uint8  \n 8   sex_male    179 non-null    uint8  \n 9   embarked_C  179 non-null    uint8  \n 10  embarked_Q  179 non-null    uint8  \n 11  embarked_S  179 non-null    uint8  \n 12  who_child   179 non-null    uint8  \n 13  who_man     179 non-null    uint8  \n 14  who_woman   179 non-null    uint8  \ndtypes: bool(2), float64(2), int64(3), uint8(8)\nmemory usage: 10.1 KB\nNone\n\n\n\n# (참고사항)원핫인코딩 후 변수의 수가 다른 경우\n# => x_test의 변수의 수가 x_train 보다 많은 경우 (혹은 그 반대인 경우)\n# 원핫인코딩 후 Feature 수가 다를 경우\n# x_train = pd.get_dummies(x_train)\n# x_test = pd.get_dummies(x_test)\n# x_train.info()\n# x_test.info()\n# 해결방법(x_test의 변수가 수가 더 많은 경우의 코드)\n# x_train = x_train.reindex(columns = x_test.columns, fill_value=0)\n# x_train.info()\n\n\n\n데이터 분리\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_train,\n                                                 y_train['target'],\n                                                 test_size = 0.2,\n                                                 stratify = y_train['target'],\n                                                 random_state = 2023)\nprint(x_train.shape)\nprint(x_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)\n\n(569, 15)\n(143, 15)\n(569,)\n(143,)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_5.html#모델링-및-성능평가-1",
    "href": "BigData_Analysis/실기_2유형_5.html#모델링-및-성능평가-1",
    "title": "빅분기 실기 - 2유형 문제풀이(5)",
    "section": "✅ 4. 모델링 및 성능평가",
    "text": "✅ 4. 모델링 및 성능평가\n\n# 랜덤포레스트 모델 사용 (참고: 회귀모델은 RandomForestRegressor)\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(random_state = 2023)\nmodel.fit(x_train, y_train)\n\nRandomForestClassifier(random_state=2023)\n\n\n\n# 모델을 사용하여 테스트 데이터 예측\ny_pred = model.predict(x_val)\n\n\n# 모델 성능 평가 (accuracy, f1 score, AUC 등)\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score\nacc = accuracy_score(y_val, y_pred)  # (실제값, 예측값)\nf1 = f1_score(y_val, y_pred)         # (실제값, 예측값)\nauc = roc_auc_score(y_val, y_pred)   # (실제값, 예측값)\n\n\nprint(acc) # 정확도(Accuracy)\nprint(f1) # f1 score\nprint(auc) # AUC\n\n0.8531468531468531\n0.8108108108108109\n0.8465909090909092\n\n\n\n# 참고사항\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_val, y_pred)\nprint(cm)\n\n# ####    예측\n# ####   0  1\n# 실제 0 TN FP\n# 실제 1 FN TP\n\n[[77 11]\n [10 45]]\n\n\n\n실제 test 셋으로 성능평가를 한다면?\n\n# 모델을 사용하여 테스트 데이터 예측\ny_pred_f = model.predict(x_test)\n# 모델 성능 평가 (정확도, F1 score, AUC)\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\nacc_f = accuracy_score(y_test, y_pred_f) # (실제값, 예측값)\nf1_f = f1_score(y_test, y_pred_f) # (실제값, 예측값)\nauc_f = roc_auc_score(y_test, y_pred_f) # (실제값,예측값)\n\n\nprint(acc_f) # 정확도(Accuracy)\nprint(f1_f) #f1 score\nprint(auc_f) #AUC\n\n0.7821229050279329\n0.7153284671532847\n0.7687088274044797"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_5.html#예측값-제출-1",
    "href": "BigData_Analysis/실기_2유형_5.html#예측값-제출-1",
    "title": "빅분기 실기 - 2유형 문제풀이(5)",
    "section": "✅ 5. 예측값 제출",
    "text": "✅ 5. 예측값 제출\n\n(주의) x_test 를 모델에 넣어 나온 예측값을 제출해야 함\n\n#(실기시험 안내사항)\n# 아래 코드 예측변수와 수험번호를 개인별로 변경하여 활용\n# pd.DataFrame({'cust_id':cust_id, 'target':y_result}).to_csv('0030000.csv', index=False)\n\n# 모델을 사용하여 테스트 데이터 예측\n\n# 1. 특정 클래스로 분류할 경우 (predict)\ny_result = model.predict(x_test)\nprint(y_result[:5])\n\n# 2. 특정 클래스로 분류될 확률을 구할 경우 (predict_proba)\ny_result_prob = model.predict_proba(x_test)\nprint(y_result_prob[:5])\n\n# 이해해보기\nresult_prob = pd.DataFrame({\n    'result' : y_result,\n    'prob_0' : y_result_prob[:,0]\n})\n# setosa 일 확률 : y_result_prob[:, 0]\n# 그 외 종일 확률 : y_result_prob[:, 1]\n\nprint(result_prob[:5])\n\n[1 1 0 0 0]\n[[0.32 0.68]\n [0.24 0.76]\n [1.   0.  ]\n [0.93 0.07]\n [0.93 0.07]]\n   result  prob_0\n0       1    0.32\n1       1    0.24\n2       0    1.00\n3       0    0.93\n4       0    0.93\n\n\n\n# ★tip : 데이터를 저장한 다음 불러와서 제대로 제출했는지 확인해보자\n# pd.DataFrame({'result':y_result}).to_csv('수험번호.csv', index=False)\n# df2 = pd.read_csv(\"수험번호.csv\")\n# print(df2.head())"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_1.html",
    "href": "BigData_Analysis/실기_3유형_1.html",
    "title": "빅분기 실기 - 3유형 문제풀이(1)",
    "section": "",
    "text": "실기 3 유형(1)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_1.html#검정방법",
    "href": "BigData_Analysis/실기_3유형_1.html#검정방법",
    "title": "빅분기 실기 - 3유형 문제풀이(1)",
    "section": "✅ 검정방법",
    "text": "✅ 검정방법\n\n1) (정규성o) 단일표본 t검정(1sample t-test)\n\n\n2) (정규성x) 윌콕슨 부호순위 검정"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_1.html#가설검정-순서중요",
    "href": "BigData_Analysis/실기_3유형_1.html#가설검정-순서중요",
    "title": "빅분기 실기 - 3유형 문제풀이(1)",
    "section": "✅ 가설검정 순서(중요!!)",
    "text": "✅ 가설검정 순서(중요!!)\n\n\n가설검정\n유의수준 확인\n정규성 검정\n검정실시(통계량, p-value 확인)\n귀무가설 기각여부 결정(채택/기각)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_1.html#데이터-불러오기",
    "href": "BigData_Analysis/실기_3유형_1.html#데이터-불러오기",
    "title": "빅분기 실기 - 3유형 문제풀이(1)",
    "section": "✅ 데이터 불러오기",
    "text": "✅ 데이터 불러오기\n\nimport pandas as pd\nimport numpy as np\n\n\n# 데이터 불러오기 mtcars\ndf = pd.read_csv('mtcars.txt')\ndf.head(3)\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n  \n\n\n\n\n\n예제문제\n\n\n1. mtcars 데이터셋의 mpg열 데이터의 평균이 20과 같다고 할 수 있는지 검정하시오. (유의수준 5%)\n\nimport scipy.stats as stats\nfrom scipy.stats import shapiro\n\n\n# 1. 가설검정\n# H_0 : mpg 열의 평균이 20과 같다\n# H_1 : mpg 열의 평균이 20과 같지 않다\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3. 정규성 검정\n# H_0(귀무가설) : 정규분포를 따른다\n# H_1(대립가설) : 정규분포를 따르지 않는다\nstatistic, pvalue = stats.shapiro(df['mpg'])\nprint(round(statistic, 4), round(pvalue, 4))\n\nresult = stats.shapiro(df['mpg'])\nprint(result)\n\n0.9476 0.1229\nShapiroResult(statistic=0.9475648403167725, pvalue=0.1228824257850647)\n\n\n\np-value 값이 유의수준(0.05) 보다 크다. -> 귀무가설(H_0) 채택\n(만약 정규분포를 따르지 않는다면 비모수 검정방법을 써야 함(윌콕슨의 부호순위 검정)\n\n\n# 4.1 (정규성 만족 o) t-검정 실시\nstatistic, pvalue = stats.ttest_1samp(df['mpg'], popmean=20,\n                                      alternative='two-sided') # default: two-sided\n                                      # H_1 : 왼쪽값이 오른쪽 값과 같지 않다\nprint(round(statistic, 4), round(pvalue, 4))\n# alternative (대립가설: H_1) 옵션 : 'two-sided', 'greater', 'less'\n\n0.0851 0.9328\n\n\n\n# 4.2 (정규성 만족 x) Wilcoxon 부호순위 검정\nstatistic, pvalue = stats.wilcoxon(df['mpg']-20,alternative='two-sided') \nprint(round(statistic, 4), round(pvalue, 4))\n\n249.0 0.7891\n\n\n\n# 5. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값이 0.05보다 크기 때문에(0.9328) 귀무가설을 채택한다\n# 즉, mpg 열의 평균이 20과 같다고 할 수 있다\n\n# 답 : 채택\n\n\n# 실제로 평균을 구해보면\ndf['mpg'].mean()\n\n20.090624999999996\n\n\n\n\n2. mtcars 데이터셋의 mpg 열 데이터의 평균이 17보다 크다고 할 수 있는지 검정하시오. (유의수준 5%)\n\n# 1. 가설검정\n# H_0 : mpg 열의 평균이 17보다 작거나 같다(mpg mean <= 17)\n# H_1 : mpg 열의 평균이 17보다 크다(mpg mean >17)\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3. 정규성 검정\n# H_0(귀무가설) : 정규분포를 따른다\n# H_1(대립가설) : 정규분포를 따르지 않는다\nstatistic, pvalue = stats.shapiro(df['mpg'])\nprint(round(statistic, 4), round(pvalue, 4))\n\n0.9476 0.1229\n\n\n\n# 4.1 (정규성 만족 o) t-검정 실시\nstatistic, pvalue = stats.ttest_1samp(df['mpg'], popmean=17,\n                                      alternative='greater') # alternative(대립가설)\n# H_1: 왼쪽값(df['mpg'].mean)이 오른쪽값(popmean)보다 크다\nprint(round(statistic, 4), round(pvalue, 4))\n\n2.9008 0.0034\n\n\n\n# 4.2 (정규성 만족 x) Wilcoxon 부호순위 검정\nstatistic, pvalue = stats.wilcoxon(df['mpg']-17,alternative='greater') \nprint(round(statistic, 4), round(pvalue, 4))\n\n395.5 0.0066\n\n\n\n# 5. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값이 0.05보다 작기 때문에(0.0034) 귀무가설을 기각한다(대립가설 채택)\n# 즉, mpg 열의 평균이 17보다 크다고 할 수 있다\n\n# 답 : 기각\n\n\n\n3. mtcars 데이터셋의 mpg 열 데이터의 평균이 17보다 작다고 할 수 있는지 검정하시오. (유의수준 5%)\n\n# 1. 가설검정\n# H_0 : mpg 열의 평균이 17보다 크거나 같다(mpg mean >= 17)\n# H_1 : mpg 열의 평균이 17보다 작다(mpg mean <17)\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3. 정규성 검정\nstatistic, pvalue = stats.shapiro(df['mpg'])\nprint(round(statistic, 4), round(pvalue, 4))\n\n0.9476 0.1229\n\n\n\n# 4.1 (정규성 만족 o) t-검정 실시\nstatistic, pvalue = stats.ttest_1samp(df['mpg'], popmean=17,\n                                      alternative='less') # alternative(대립가설)\n# H_1: 왼쪽값(df['mpg'].mean)이 오른쪽값(popmean)보다 작다\nprint(round(statistic, 4), round(pvalue, 4))\n\n2.9008 0.9966\n\n\n\n# 4.2 (정규성 만족 x) Wilcoxon 부호순위 검정\nstatistic, pvalue = stats.wilcoxon(df['mpg']-17,alternative='less') \nprint(round(statistic, 4), round(pvalue, 4))\n\n395.5 0.9938\n\n\n\n# 5. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값이 0.05보다 크기 때문에(0.9966) 귀무가설을 채택한다\n# 즉, mpg 열의 평균이 17보다 크거나 같다고 할 수 있다\n\n# 답 : 채택"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_2.html",
    "href": "BigData_Analysis/실기_3유형_2.html",
    "title": "빅분기 실기 - 3유형 문제풀이(2)",
    "section": "",
    "text": "실기 3 유형(2)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_2.html#검정방법",
    "href": "BigData_Analysis/실기_3유형_2.html#검정방법",
    "title": "빅분기 실기 - 3유형 문제풀이(2)",
    "section": "✅ 검정방법",
    "text": "✅ 검정방법"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_2.html#대응표본쌍체-동일한-객체의-전-vs-후-평균비교",
    "href": "BigData_Analysis/실기_3유형_2.html#대응표본쌍체-동일한-객체의-전-vs-후-평균비교",
    "title": "빅분기 실기 - 3유형 문제풀이(2)",
    "section": "1. 대응표본(쌍체) : 동일한 객체의 전 vs 후 평균비교",
    "text": "1. 대응표본(쌍체) : 동일한 객체의 전 vs 후 평균비교\n\n(정규성o) 대응표본(쌍체) t검정(paired t-test) : 동일한 객체의 전 vs 후 평균비교\n(정규성x) 윌콕슨 부호순위 검정(wilcoxon)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_2.html#독립표본-a-집단의-평균-vs-b-집단의-평균",
    "href": "BigData_Analysis/실기_3유형_2.html#독립표본-a-집단의-평균-vs-b-집단의-평균",
    "title": "빅분기 실기 - 3유형 문제풀이(2)",
    "section": "2. 독립표본 : A 집단의 평균 vs B 집단의 평균",
    "text": "2. 독립표본 : A 집단의 평균 vs B 집단의 평균\n\n(정규성o) 독립표본 t검정(2sample t-test)\n(정규성x) 윌콕슨 순위합 검정(ranksums)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_2.html#가설검정-순서중요",
    "href": "BigData_Analysis/실기_3유형_2.html#가설검정-순서중요",
    "title": "빅분기 실기 - 3유형 문제풀이(2)",
    "section": "✅ 가설검정 순서(중요!!)",
    "text": "✅ 가설검정 순서(중요!!)\n\n1. 대응표본(쌍체) t검정(paired t-test)\n\n\n가설검정\n유의수준 확인\n정규성 검정 (주의) 차이값에 대한 정규성\n검정실시(통계량, p-value 확인)\n귀무가설 기각여부 결정(채택/기각)\n\n\n\n\n2. 독립표본 t검정(2sample t-test)\n\n\n가설검정\n유의수준 확인\n정규성 검정 (주의) 두 집단 모두 정규성을 따를 경우\n등분산 검정\n검정실시(통계량, p-value 확인)\n귀무가설 기각여부 결정(채택/기각)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_2.html#예제문제",
    "href": "BigData_Analysis/실기_3유형_2.html#예제문제",
    "title": "빅분기 실기 - 3유형 문제풀이(2)",
    "section": "✅ 예제문제",
    "text": "✅ 예제문제\n\nCase 1) 대응표본(쌍체) t 검정(paired t-test)\n문제 1-1 다음은 혈압약을 먹기 전, 후의 혈압 데이터이다.\n혈압약을 먹기 전, 후의 차이가 있는지 쌍체 t검정을 실시하시오.\n(유의수준 5%)\n\nbefore : 혈압약을 먹기전 혈압, after : 혈압약을 먹은 후 혈압\nH_0(귀무가설) : after - before = 0\nH_1(대립가설) : after - before != 0\n\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom scipy.stats import shapiro\n\n\n# 데이터 만들기\ndf = pd.DataFrame( {\n    'before': [120, 135, 122, 124, 135, 122, 145, 160, 155, 142, 144, 135, 167],\n    'after' : [110, 132, 123, 119, 123, 115, 140, 162, 142, 138, 135, 142, 160] })\n\n\n# 1. 가설검정\n# H0 : 약을 먹기전과 먹은 후의 혈압 평균은 같다(효과가 없다)\n# H1 : 약을 먹기전과 먹은 후의 혈압 평균은 같지 않다(효과가 있다)\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3. 정규성 검정 (차이값에 대해 정규성 확인)\nstatistic, pvalue = stats.shapiro(df['after'] - df['before'])\nprint(round(statistic,4), round(pvalue,4))\n\n0.9589 0.7363\n\n\n\np-value 값(0.7363)이 유의수준 (0.05)보다 크다.\n귀무가설 (H0) 채택(정규성검정의 H0 : 정규분포를 따른다)\n\n\n# 4.1 (정규성o) 대응표본(쌍체) t검정(paired t-test)\nstatistic, pvalue = stats.ttest_rel(df['after'], df['before']\n                                    , alternative='two-sided')\nprint(round(statistic,4), round(pvalue,4))\n\n-3.1382 0.0086\n\n\n\n# 4.2 (정규성x) wilcoxon 부호순위 검정\nstatistic, pvalue = stats.wilcoxon(df['after']-df['before'],\n                                  alternative='two-sided')\nprint(round(statistic,4), round(pvalue,4))\n\n11.0 0.0134\n\n\n\n# 5. 귀무가설 기각 여부 결정(채택/기각)\n# p-value 값(0.0086)이 0.05보다 작기 떄문에 귀무가설을 기각한다\n# 즉, 약을 먹기 전과 먹은 후의 혈압은 같지 않다(효과가 있다)\n\n# 답 : 기각(H1)\n\n차이가 있는지 없는지 확인은 양측검정 // 증가, 감소를 확인하는 것은 단측검정\n문제 1-2\n다음은 혈압약을 먹기 전, 후의 혈압 데이터이다.\n혈압약을 먹은 후 혈압이 감소했는지 확인하기 위해 쌍체 t 검정을 실시하시오\n(유의수준 5%)\n\nbefore : 혈압약을 먹기전 혈압, after : 혈압약을 먹은 후의 혈압\nH0(귀무가설) : after - before >=0\nH1(대립가설) : after - before < 0\n\n\n# 데이터 만들기\ndf = pd.DataFrame( {\n    'before': [120, 135, 122, 124, 135, 122, 145, 160, 155, 142, 144, 135, 167],\n    'after' : [110, 132, 123, 119, 123, 115, 140, 162, 142, 138, 135, 142, 160] })\nprint(df.head(3))\n\n   before  after\n0     120    110\n1     135    132\n2     122    123\n\n\n\n# 1. 가설검정\n# H0 : 약을 먹은 후 혈압이 같거나 증가했다. (after - before >= 0)\n# H1 : 약을 먹은 후 혈압이 감소했다.        (after - before <  0) \n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\nstatistic, pvalue = stats.shapiro(df['after']-df['before'])\nprint(round(statistic, 4), round(pvalue ,4))\n\n0.9589 0.7363\n\n\np-value가 0.05보다 크다 귀무가설 채택\n\n# 4.1 (정규성o) 대응표본(쌍체) t검정(paired t-test)\nstatistic, pvalue = stats.ttest_rel(df['after'], df['before']\n                                    , alternative='less')\nprint(round(statistic,4), round(pvalue,4))\n\n-3.1382 0.0043\n\n\n\n# 4.2 (정규성x) wilcoxon 부호순위 검정\nstatistic, pvalue = stats.wilcoxon(df['after']-df['before'],\n                                  alternative='less')\nprint(round(statistic,4), round(pvalue,4))\n\n11.0 0.0067\n\n\n\n# 5. 귀무가설 기각 여부 결정(채택/기각)\n# p-value 값(0.0043)이 0.05보다 작기 떄문에 귀무가설을 기각한다\n# 즉, 약을 먹은 후 혈압이 감소했다고 할 수 있다.(효과가 있다)\n\n# 답 : 기각(H1)\n\n\n\nCase 2) 독립표본 t검정(2sample t-test)\n문제 2-1\n다음은 A그룹과 B그룹 인원의 혈압 데이터이다.\n두 그룹의 혈압평균이 다르다고 할 수 있는지 독립표본 t검정을 실시하시오.\n(유의수준 5%)\n\nA : A그룹 인원의 혈압, B : B그룹 인원의 혈압\nH0(귀무가설) : A = B\nH1(대립가설) : A != B\n\n\n# 데이터 만들기\ndf = pd.DataFrame( {\n    'A': [120, 135, 122, 124, 135, 122, 145, 160, 155, 142, 144, 135, 167],\n    'B' : [110, 132, 123, 119, 123, 115, 140, 162, 142, 138, 135, 142, 160] })\nprint(df.head(3))\n\n     A    B\n0  120  110\n1  135  132\n2  122  123\n\n\n\n# 1. 가설검정\n# H0(귀무가설) : A그룹과 B그룹의 혈압 평균은 같다.      (A = B)\n# H1(대립가설) : A그룹과 B그룹의 혈압 평균은 같지 않다. (A != B)\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3. 정규성 검정\n# H0(귀무가설) : 정규분포를 따른다.\n# H1(대립가설) : 정규분포를 따르지 않는다.\n\nstatisticA, pvalueA = stats.shapiro(df['A'])\nstatisticB, pvalueB = stats.shapiro(df['B'])\nprint(round(statisticA, 4), round(pvalueA, 4))\nprint(round(statisticB, 4), round(pvalueB, 4))\n\n0.9314 0.3559\n0.9498 0.5956\n\n\n\np-value 값(0.3559, 0.5956)이 유의수준(0.05)보다 크다.\n귀무가설(H0) 채택\n만약 하나라도 정규분포를 따르지 않는다면 비모수 검정방법을 써야 함\n(윌콕슨의 순위합 검정 ranksums)\n\n\n# 4. 등분산성 검정\n# H0(귀무가설) : 등분산 한다.\n# H1(대립가설) : 등분산 하지 않는다.\nstatistic, pvalue = stats.bartlett(df['A'], df['B'])\nprint(round(statistic, 4), round(pvalue, 4))\n\n0.0279 0.8673\n\n\n\np-value 값이 유의수준(0.05) 보다 크다.\n귀무가설(H0) 채택 => 등분산성을 따른다고 할 수 있다.\n\n\n# 5.1 (정규성o, 등분산성 o/x) t검정\nstatistic, pvalue = stats.ttest_ind(df['A'], df['B'],\n                                   equal_var = True,\n                                   alternative='two-sided')\n# 만약 등분산 하지 않으면 False로 설정\nprint(round(statistic, 4), round(pvalue, 4))\n\n0.8192 0.4207\n\n\n\n# 5.2 (정규성x)윌콕슨의 순위합 검정\nstatistic, pvalue = stats.ranksums(df['A'], df['B'],\n                                   alternative='two-sided')\nprint(round(statistic, 4), round(pvalue, 4))\n\n0.8462 0.3975\n\n\n\n# 6. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값이 0.05보다 크기 떄문에 귀무가설을 채택한다\n# 즉, A그룹과 B그룹의 혈압 평균은 같다고 할 수 있다.\n\n# 답 : 채택(H0)\n\n\n# (참고) 평균데이터 확인\nprint(round(df['A'].mean(), 4))\nprint(round(df['B'].mean(), 4))\n\n138.9231\n133.9231\n\n\n문제 2-2\n다음은 A그룹과 B그룹 인원의 혈압 데이터이다.\nA그룹의 혈압 평균이 B그룹보다 크다고 할 수 있는지 독립표본 t검정을 실시하시오.\n(유의수준 5%)\n\nA : A그룹 인원의 혈압, B : B그룹 인원의 혈압\nH0(귀무가설) : A - B <= 0 (or A <= B)\nH1(대립가설) : A - B > 0 (or A > B)\n\n\n# 데이터 만들기\ndf = pd.DataFrame( {\n    'A': [120, 135, 122, 124, 135, 122, 145, 160, 155, 142, 144, 135, 167],\n    'B' : [110, 132, 123, 119, 123, 115, 140, 162, 142, 138, 135, 142, 160] })\nprint(df.head(3))\n\n     A    B\n0  120  110\n1  135  132\n2  122  123\n\n\n\n# 1. 가설검정\n# H0(귀무가설) : A그룹의 혈압 평균이 B그룹보다 작거나 같다. (A <= B)\n# H1(대립가설) : A그룹의 혈압 평균이 B그룹보다 크다.        (A >  B)\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3. 정규성 검정(차이값에 대해 정규성 확인)\n# H0(귀무가설) : 정규분포를 따른다.\n# H1(대립가설) : 정규분포를 따르지 않는다.\n\nstatisticA, pvalueA = stats.shapiro(df['A'])\nstatisticB, pvalueB = stats.shapiro(df['B'])\nprint(round(statisticA, 4), round(pvalueA, 4))\nprint(round(statisticB, 4), round(pvalueB, 4))\n\n0.9314 0.3559\n0.9498 0.5956\n\n\n\np-value 값(0.3559, 0.5956)이 유의수준(0.05)보다 크다.\n귀무가설(H0) 채택\n만약 하나라도 정규분포를 따르지 않는다면 비모수 검정방법을 써야 함\n(윌콕슨의 순위합 검정 ranksums)\n\n\n# 4. 등분산성 검정\n# H0(귀무가설) : 등분산 한다.\n# H1(대립가설) : 등분산 하지 않는다.\nstatistic, pvalue = stats.bartlett(df['A'], df['B'])\nprint(round(statistic, 4), round(pvalue, 4))\n\n0.0279 0.8673\n\n\n\n# 5.1 (정규성o, 등분산성 o/x) t검정\nstatistic, pvalue = stats.ttest_ind(df['A'], df['B'],\n                                   equal_var = True,\n                                   alternative='greater')\n# 만약 등분산 하지 않으면 False로 설정\nprint(round(statistic, 4), round(pvalue, 4))\n\n0.8192 0.2104\n\n\n\n# 5.2 (정규성x)윌콕슨의 순위합 검정\nstatistic, pvalue = stats.ranksums(df['A'], df['B'],\n                                   alternative='greater')\n# A그룹의 평균값이 B그룹의 평균값보다 클 수 있는가를 대립가설(H1)으로 설정했기에 greater\nprint(round(statistic, 4), round(pvalue, 4))\n\n0.8462 0.1987\n\n\n\n# 6. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값이 0.05보다 크기 떄문에 귀무가설을 채택한다\n# 즉, A그룹의 혈압 평균이 B그룹보다 작거나 같다고 할 수 있다.\n# (A그룹의 혈압 평균이 B그룹보다 크다고 할 수 없다)\n\n# 답 : 채택(H0)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_3.html",
    "href": "BigData_Analysis/실기_3유형_3.html",
    "title": "빅분기 실기 - 3유형 문제풀이(3)",
    "section": "",
    "text": "실기 3 유형(3)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_3.html#검정방법",
    "href": "BigData_Analysis/실기_3유형_3.html#검정방법",
    "title": "빅분기 실기 - 3유형 문제풀이(3)",
    "section": "✅ 검정방법",
    "text": "✅ 검정방법"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_3.html#분산분석anova-a집단-vs-b집단-vs-c집단-vs",
    "href": "BigData_Analysis/실기_3유형_3.html#분산분석anova-a집단-vs-b집단-vs-c집단-vs",
    "title": "빅분기 실기 - 3유형 문제풀이(3)",
    "section": "1. 분산분석(ANOVA) : A집단 vs B집단 vs C집단 vs …",
    "text": "1. 분산분석(ANOVA) : A집단 vs B집단 vs C집단 vs …\n\n(정규성o) ANOVA 분석\n(정규성x) 크루스칼-왈리스 검정(kruskal-wallis test)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_3.html#가설검정-순서중요",
    "href": "BigData_Analysis/실기_3유형_3.html#가설검정-순서중요",
    "title": "빅분기 실기 - 3유형 문제풀이(3)",
    "section": "✅ 가설검정 순서(중요!!)",
    "text": "✅ 가설검정 순서(중요!!)\n\n\n가설검정\n유의수준 확인\n정규성 검정 (주의) 집단 모두 정규성을 따를 경우!\n등분산 검정\n검정실시(통계량, p-value 확인) (주의) 등분산여부 확인\n귀무가설 기각여부 결정(채택/기각)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_3.html#예제문제",
    "href": "BigData_Analysis/실기_3유형_3.html#예제문제",
    "title": "빅분기 실기 - 3유형 문제풀이(3)",
    "section": "✅ 예제문제",
    "text": "✅ 예제문제\n\n문제 1-1\n다음은 A, B, C 그룹 인원 성적 데이터이다.\n세 그룹의 성적 평균이 같다고 할 수 있는지 ANOVA 분석을 실시하시오.\n(유의수준 5%)\n\nA, B, C : 각 그룹 인원의 성적\nH0(귀무가설) : A(평균) = B(평균) = C(평균)\nH1(대립가설) : Not H0 (적어도 하나는 같지 않다)\n\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom scipy.stats import shapiro\n\n\n# 데이터 만들기\ndf = pd.DataFrame( {\n    'A': [120, 135, 122, 124, 135, 122, 145, 160, 155, 142, 144, 135, 167],\n    'B': [110, 132, 123, 119, 123, 115, 140, 162, 142, 138, 135, 142, 160],\n    'C': [130, 120, 115, 122, 133, 144, 122, 120, 110, 134, 125, 122, 122]})\nprint(df.head(3))\n\n     A    B    C\n0  120  110  130\n1  135  132  120\n2  122  123  115\n\n\n\n# 1. 가설검정\n# H0 : 세 그룹 성적의 평균값이 같다. (A(평균) = B(평균) = C(평균))\n# H1 : 세 그룹의 성적 평균값이 적어도 하나는 같지 않다. (not H0)\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3. 정규성 검정\nprint(stats.shapiro(df['A']))\nprint(stats.shapiro(df['B']))\nprint(stats.shapiro(df['C']))\n\n# statistic, pvalue = stats.shapiro(df['A'])\n# print(round(statistic,4), round(pvalue,4))\n\nShapiroResult(statistic=0.9314376711845398, pvalue=0.35585272312164307)\nShapiroResult(statistic=0.9498201012611389, pvalue=0.5955665707588196)\nShapiroResult(statistic=0.9396706223487854, pvalue=0.45265132188796997)\n\n\n\n세 집단 모두 p-value 값이 유의수준(0.05)보다 크다\n귀무가설(H0) 채택 => 정규분포를 따른다고 할 수 있다.\n만약 하나라도 정규분포를 따르지 않는다면 비모수 검정방법을 써야함\n(크루스칼-왈리스 검정)\n\n\n# 4. 등분산성 검정\n# H0(귀무가설) : 등분산 한다\n# H1(대립가설) : 등분산 하지 않는다\nprint(stats.bartlett(df['A'], df['B'], df['C']))\n\nBartlettResult(statistic=4.222248448848066, pvalue=0.12110174433684852)\n\n\n\np-value 값이 유의수준(0.05)보다 크다\n귀무가설(H0) 채택 => 등분산 한다고 할 수 있다.\n\n\n# 5.1 (정규성o, 등분산성 o) 분산분석(F_oneway)\nimport scipy.stats as stats\nstatistic, pvalue = stats.f_oneway(df['A'], df['B'], df['C'])\n# 주의 : 데이터가 각각 들어가야 함(밑에 예제와 비교해볼 것)\n\nprint(round(statistic,4), round(pvalue,4))\n\n3.6971 0.0346\n\n\n\n# 5.2 (정규성o, 등분산성 x) Welch_ANOVA 분석\n# import pingouin as pg     # pingouin 패키지 미지원\n# pg.welch_anova(dv = \"그룹변수명\", between=\"성적데이터\", data=데이터)\n# pg.welch_anova(df['A'], df['B'], df['C']) 형태로 분석불가\n\n\n# 5.3 (정규성x, 등분산성 x) 크루스 왈리스 검정\nimport scipy.stats as stats\nstatistic, pvalue = stats.kruskal(df['A'], df['B'], df['C'])\n\nprint(round(statistic,4), round(pvalue,4))\n\n6.897 0.0318\n\n\n\n# 6. 귀무가설 기각 여부 결정(채택/기각)\n# p-value 값(0.0346)이 0.05보다 작기 떄문에 귀무가설을 기각한다\n# 즉, A, B, C 그룹의 성적 평균이 같다고 할 수 없다.\n\n# 답 : 기각(H1)\n\n\n\n문제 1-2 데이터 형태가 다른 경우\n\n# 데이터 만들기\ndf2 = pd.DataFrame( {\n    '항목': ['A','A','A','A','A','A','A','A','A','A','A','A','A',\n           'B','B','B','B','B','B','B','B','B','B','B','B','B',\n           'C','C','C','C','C','C','C','C','C','C','C','C','C',],\n    'value': [120, 135, 122, 124, 135, 122, 145, 160, 155, 142, 144, 135, 167,\n             110, 132, 123, 119, 123, 115, 140, 162, 142, 138, 135, 142, 160,\n             130, 120, 115, 122, 133, 144, 122, 120, 110, 134, 125, 122, 122]\n    })\nprint(df2.head(3))\n\n  항목  value\n0  A    120\n1  A    135\n2  A    122\n\n\n\n# 각각 필터링해서 변수명에 저장하고 분석 진행\na = df2[ df2['항목']=='A' ]['value']\nb = df2[ df2['항목']=='B' ]['value']\nc = df2[ df2['항목']=='C' ]['value']\n\n\n# 분산분석(F_oneway)\nimport scipy.stats as stats\nstatistic, pvalue = stats.f_oneway(a, b, c)\nprint(round(statistic,4), round(pvalue,4))\n\n3.6971 0.0346"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_4.html",
    "href": "BigData_Analysis/실기_3유형_4.html",
    "title": "빅분기 실기 - 3유형 문제풀이(4)",
    "section": "",
    "text": "실기 3 유형(4)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_4.html#분석-case",
    "href": "BigData_Analysis/실기_3유형_4.html#분석-case",
    "title": "빅분기 실기 - 3유형 문제풀이(4)",
    "section": "✅ 분석 Case",
    "text": "✅ 분석 Case\n\nCase 1. 적합도 검정 - 각 범주에 속할 확률이 같은지?\n\n\nCase 2. 독립성 검정 - 두개의 범주형 변수가 서로 독립인지?"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_4.html#가설검정-순서중요",
    "href": "BigData_Analysis/실기_3유형_4.html#가설검정-순서중요",
    "title": "빅분기 실기 - 3유형 문제풀이(4)",
    "section": "✅ 가설검정 순서(중요!!)",
    "text": "✅ 가설검정 순서(중요!!)\n\n\n가설검정\n유의수준 확인\n검정실시(통계량, p-value 확인)\n귀무가설 기각여부 결정(채택/기각)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_4.html#예제문제",
    "href": "BigData_Analysis/실기_3유형_4.html#예제문제",
    "title": "빅분기 실기 - 3유형 문제풀이(4)",
    "section": "✅ 예제문제",
    "text": "✅ 예제문제\n\nCase 1. 적합도 검정 - 각 범주에 속할 확률이 같은지?\n\n\n문제 1-1\n\n\n랜덤 박스에 상품 A, B, C, D가 들어있다.\n\n\n다음은 랜덤박스에서 100번 상품을 꺼냈을 떄의 상품 데이터라고 할 때\n\n\n상품이 동일한 비율로 들어있다고 할 수 있는지 검정해보시오.\n\nimport pandas as pd\nimport numpy as np\n\n\n# 데이터 생성\nrow1 = [30,20,15,35]\ndf = pd.DataFrame([row1], columns=['A','B','C','D'])\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      30\n      20\n      15\n      35\n    \n  \n\n\n\n\n\n# 1. 가설검정\n# H0 : 랜덤박스에 상품 A,B,C,D가 동일한 비율로 들어있다.\n# H1 : 랜덤박스에 상품 A,B,C,D가 동일한 비율로 들어있지 않다.\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3. 검정실시(통계량, p-value)\nfrom scipy.stats import chisquare\n# chisquare(f_obs=f_obs, f_exp=f_exp)   # 관측값, 기대값\n\n# 관측값과 기대값 구하기\nf_obs = [30,20,15,35]\n# f_obs = df.iloc[0]\nf_exp = [25,25,25,25]\n\nstatistic, pvalue = chisquare(f_obs=f_obs, f_exp=f_exp)\nprint(statistic)\nprint(pvalue)\n\n10.0\n0.01856613546304325\n\n\n\n# 4. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값이 0.05보다 작기 떄문에 귀무가설을 기각한다.\n# 즉, 랜덤박스에 상품 A,B,C,D가 동일한 비율로 들어있지 않다고 할 수 있다.\n\n# 답 : 기각\n\n\n\n문제 1-1\n\n\n랜덤 박스에 상품 A, B, C가 들어있다.\n\n\n다음은 랜덤박스에서 150번 상품을 꺼냈을 떄의 상품 데이터라고 할 때\n\n\n상품별로 A 30%, B 15%, C 55% 비율로 들어있다고 할 수 있는지 검정해보시오.\n\nimport pandas as pd\nimport numpy as np\n\n\n# 데이터 생성\nrow1 = [50,25,75]\ndf = pd.DataFrame([row1], columns=['A','B','C'])\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n    \n  \n  \n    \n      0\n      50\n      25\n      75\n    \n  \n\n\n\n\n\n# 1. 가설검정\n# H0 : 랜덤박스에 상품 A,B,C가 30%, 15%, 55%의 비율로 들어있다.\n# H1 : 랜덤박스에 상품 A,B,C가 30%, 15%, 55%의비율로 들어있지 않다.\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3. 검정실시(통계량, p-value)\nfrom scipy.stats import chisquare\n# chisquare(f_obs=f_obs, f_exp=f_exp)   # 관측값, 기대값\n\n# 관측값과 기대값 구하기\nf_obs = [50,25,75]\n# f_obs = df.iloc[0]\n\na = 150*0.3\nb = 150*0.15\nc = 150*0.55\nf_exp = [a,b,c]\n\nstatistic, pvalue = chisquare(f_obs=f_obs, f_exp=f_exp)\nprint(statistic)\nprint(pvalue)\n\n1.5151515151515151\n0.46880153914023537\n\n\n\n# 4. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값이 0.05보다 크기 떄문에 귀무가설을 채택한다.\n# 즉, 랜덤박스에 상품 A,B,C가 30%, 15%, 55%의 비율로 들어있다고 할 수 있다.\n\n# 답 : 채택\n\n\n\nCase 2. 독립성 검정 - 두개의 범주형 변수가 서로 독립인지?\n\n\n문제 2-1\n\n\n연령대에 따라 먹는 아이스크림의 차이가 있는지 독립성 검정을 실시하시오.\n\nimport pandas as pd\nimport numpy as np\n\n\n# 데이터 생성\nrow1, row2 = [200,190,250],[220,250,300]\ndf = pd.DataFrame([row1, row2], columns=['딸기','초코','바닐라']\n                  ,index=['10대', '20대'])\ndf\n\n\n\n\n\n  \n    \n      \n      딸기\n      초코\n      바닐라\n    \n  \n  \n    \n      10대\n      200\n      190\n      250\n    \n    \n      20대\n      220\n      250\n      300\n    \n  \n\n\n\n\n\n# 1. 가설설정\n# H0 : 연령대와 먹는 아이스크림의 종류는 서로 관련이 없다(두 변수는 서로 독립이다.)\n# H1 : 연령대와 먹는 아이스크림의 종류는 서로 관련이 있다(두 변수는 서로 독립이 아니다.)\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3.검정실시(통계량, p-value, 기대값 확인)\nfrom scipy.stats import chi2_contingency\n\nstatistic, pvalue, dof, expected = chi2_contingency(df)\n# 공식문서상에 : statistic(통계량), pvalue, dof(자유도), expected_freq(기대값)\n\n# 아래와 같이 입력해도 동일한 결과\n# statistic, pvalue, dof, expected = chi2_contingency([row1, row2])\n# statistic, pvalue, dof, expected = chi2_contingency([df.iloc[0], df.iloc[1]])\n\nprint(statistic)\nprint(pvalue)\nprint(dof) # 자유도 = (행-1) * (열*1)\nprint(np.round(expected, 2)) # 반올림하고 싶다면 np.round()\n\n1.708360126075226\n0.4256320394874311\n2\n[[190.64 199.72 249.65]\n [229.36 240.28 300.35]]\n\n\n\n# 4. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값이 0.05보다 크기 떄문에 귀무가설을 채택한다.\n# 즉, 연령대와 먹는 아이스크림의 종류는 서로 관련이 없다고 할 수 있다.\n\n# 답 : 채택\n\n\n\n(추가) 만약 데이터 형태가 다를경우?\n\n# (Case1) 만약 데이터가 아래와 같이 주어진다면?\ndf = pd.DataFrame({\n    '아이스크림' : ['딸기', '초코', '바닐라', '딸기', '초코', '바닐라'],\n    '연령' : ['10대','10대','10대','20대','20대','20대'],\n    '인원' : [200,190,250,220,250,300]\n})\ndf\n\n\n\n\n\n  \n    \n      \n      아이스크림\n      연령\n      인원\n    \n  \n  \n    \n      0\n      딸기\n      10대\n      200\n    \n    \n      1\n      초코\n      10대\n      190\n    \n    \n      2\n      바닐라\n      10대\n      250\n    \n    \n      3\n      딸기\n      20대\n      220\n    \n    \n      4\n      초코\n      20대\n      250\n    \n    \n      5\n      바닐라\n      20대\n      300\n    \n  \n\n\n\n\n\n# pd.crosstab(index= , columns= , values= , aggfunc=sum)\ntable = pd.crosstab(index=df['연령'] , columns=df['아이스크림'] ,\n                    values= df['인원'], aggfunc=sum)\ntable\n# 주의 : index, columns에 순서를 꼭 확인하기\n\n\n\n\n\n  \n    \n      아이스크림\n      딸기\n      바닐라\n      초코\n    \n    \n      연령\n      \n      \n      \n    \n  \n  \n    \n      10대\n      200\n      250\n      190\n    \n    \n      20대\n      220\n      300\n      250\n    \n  \n\n\n\n\n\n# 3.검정실시(통계량, p-value, 기대값 확인)\nfrom scipy.stats import chi2_contingency\n\nstatistic, pvalue, dof, expected = chi2_contingency(table)\n# 공식문서상에 : statistic(통계량), pvalue, dof(자유도), expected_freq(기대값)\n\nprint(statistic)\nprint(pvalue)\nprint(dof) # 자유도 = (행-1) * (열*1)\nprint(np.round(expected, 2)) # 반올림하고 싶다면 np.round()\n\n1.708360126075226\n0.4256320394874311\n2\n[[190.64 249.65 199.72]\n [229.36 300.35 240.28]]\n\n\n\n# (Case2) 만약 데이터가 아래와 같이 주어진다면?\n# (이해를 위한 참고용 입니다. 빈도수 카운팅)\ndf = pd.DataFrame({\n    '아이스크림' : ['딸기', '초코', '바닐라', '딸기', '초코', '바닐라'],\n    '연령' : ['10대','10대','10대','20대','20대','20대']\n})\ndf\n\n\n\n\n\n  \n    \n      \n      아이스크림\n      연령\n    \n  \n  \n    \n      0\n      딸기\n      10대\n    \n    \n      1\n      초코\n      10대\n    \n    \n      2\n      바닐라\n      10대\n    \n    \n      3\n      딸기\n      20대\n    \n    \n      4\n      초코\n      20대\n    \n    \n      5\n      바닐라\n      20대\n    \n  \n\n\n\n\n\n# pd.crosstab(index, columns)\npd.crosstab(df['연령'],df['아이스크림'])\n\n\n\n\n\n  \n    \n      아이스크림\n      딸기\n      바닐라\n      초코\n    \n    \n      연령\n      \n      \n      \n    \n  \n  \n    \n      10대\n      1\n      1\n      1\n    \n    \n      20대\n      1\n      1\n      1\n    \n  \n\n\n\n\n\n\n문제2-2\n\n\n타이타닉에 데이터에서 성별(sex)과 생존여부(survied) 변수간\n\n\n독립성 검정을 실시하시오\n\nimport seaborn as sns\ndf = sns.load_dataset('titanic')\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 15 columns):\n #   Column       Non-Null Count  Dtype   \n---  ------       --------------  -----   \n 0   survived     891 non-null    int64   \n 1   pclass       891 non-null    int64   \n 2   sex          891 non-null    object  \n 3   age          714 non-null    float64 \n 4   sibsp        891 non-null    int64   \n 5   parch        891 non-null    int64   \n 6   fare         891 non-null    float64 \n 7   embarked     889 non-null    object  \n 8   class        891 non-null    category\n 9   who          891 non-null    object  \n 10  adult_male   891 non-null    bool    \n 11  deck         203 non-null    category\n 12  embark_town  889 non-null    object  \n 13  alive        891 non-null    object  \n 14  alone        891 non-null    bool    \ndtypes: bool(2), category(2), float64(2), int64(4), object(5)\nmemory usage: 80.7+ KB\n\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      survived\n      pclass\n      sex\n      age\n      sibsp\n      parch\n      fare\n      embarked\n      class\n      who\n      adult_male\n      deck\n      embark_town\n      alive\n      alone\n    \n  \n  \n    \n      0\n      0\n      3\n      male\n      22.0\n      1\n      0\n      7.2500\n      S\n      Third\n      man\n      True\n      NaN\n      Southampton\n      no\n      False\n    \n    \n      1\n      1\n      1\n      female\n      38.0\n      1\n      0\n      71.2833\n      C\n      First\n      woman\n      False\n      C\n      Cherbourg\n      yes\n      False\n    \n    \n      2\n      1\n      3\n      female\n      26.0\n      0\n      0\n      7.9250\n      S\n      Third\n      woman\n      False\n      NaN\n      Southampton\n      yes\n      True\n    \n    \n      3\n      1\n      1\n      female\n      35.0\n      1\n      0\n      53.1000\n      S\n      First\n      woman\n      False\n      C\n      Southampton\n      yes\n      False\n    \n    \n      4\n      0\n      3\n      male\n      35.0\n      0\n      0\n      8.0500\n      S\n      Third\n      man\n      True\n      NaN\n      Southampton\n      no\n      True\n    \n  \n\n\n\n\n\n# pd.crosstab(index, columns)\ntable = pd.crosstab(df['sex'], df['survived'])\nprint(table)\n\nsurvived    0    1\nsex               \nfemale     81  233\nmale      468  109\n\n\n\n# 1. 가설설정\n# H0 : 성별과 생존 여부는 서로 관련이 없다(두 변수는 서로 독립이다)\n# H1 : 성별과 생존 여부는 서로 관련이 있다(두 변수는 서로 독립이 아니다)\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3.검정실시(통계량, p-value, 기대값 확인)\nfrom scipy.stats import chi2_contingency\n\nstatistic, pvalue, dof, expected = chi2_contingency(table)\n# 공식문서상에 : statistic(통계량), pvalue, dof(자유도), expected_freq(기대값)\n\nprint(statistic)\nprint(pvalue)\nprint(dof) # 자유도 = (행-1) * (열*1)\nprint(np.round(expected, 2)) # 반올림하고 싶다면 np.round()\n\n260.71702016732104\n1.1973570627755645e-58\n1\n[[193.47 120.53]\n [355.53 221.47]]\n\n\n\n# 4. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값(1.1973570627755645e-58)이 0.05보다 작기 떄문에 귀무가설을 기각한다.\n# 즉, 성별과 생존 여부는 서로 관련이 있다고 할 수 있다.\n\n# 답 : 기각\n\n\n\n데이터를 변경해보면서 이해해봅시다.\n\n# 임의 데이터 생성\nsex, survived = [160,160], [250,220]\ntable = pd.DataFrame([sex, survived], columns=['0','1'], index=['female','male'])\nprint(table)\n\n          0    1\nfemale  160  160\nmale    250  220\n\n\n\n# 3.검정실시(통계량, p-value, 기대값 확인)\nfrom scipy.stats import chi2_contingency\n\nstatistic, pvalue, dof, expected = chi2_contingency(table)\n# 공식문서상에 : statistic(통계량), pvalue, dof(자유도), expected_freq(기대값)\n\nprint(statistic)\nprint(pvalue)\nprint(dof) # 자유도 = (행-1) * (열*1)\nprint(np.round(expected, 2)) # 반올림하고 싶다면 np.round()\n\n0.6541895872879862\n0.41861876333789727\n1\n[[166.08 153.92]\n [243.92 226.08]]\n\n\n\n# 4. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값이 0.05보다 크기 떄문에 귀무가설을 채택한다.\n# 즉, 성별과 생존 여부는 서로 관련이 없다고 할 수 있다.\n\n# 답 : 채택"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_5.html",
    "href": "BigData_Analysis/실기_3유형_5.html",
    "title": "빅분기 실기 - 3유형 문제풀이(5)",
    "section": "",
    "text": "실기 3 유형(5)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_5.html#다중회귀분석",
    "href": "BigData_Analysis/실기_3유형_5.html#다중회귀분석",
    "title": "빅분기 실기 - 3유형 문제풀이(5)",
    "section": "✅ 다중회귀분석",
    "text": "✅ 다중회귀분석\n\nimport pandas as pd\nimport numpy as np\n\n\n당뇨병 환자의 질병 진행정도 데이터셋\n\n###############  실기환경 복사 영역  ###############\nimport pandas as pd\nimport numpy as np\n# 실기 시험 데이터셋으로 셋팅하기 (수정금지)\nfrom sklearn.datasets import load_diabetes\n# diabetes 데이터셋 로드\ndiabetes = load_diabetes()\nx = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\ny = pd.DataFrame(diabetes.target)\ny.columns = ['target'] \n###############  실기환경 복사 영역  ###############\n\n\n# 데이터 설명\nprint(diabetes.DESCR)\n\n.. _diabetes_dataset:\n\nDiabetes dataset\n----------------\n\nTen baseline variables, age, sex, body mass index, average blood\npressure, and six blood serum measurements were obtained for each of n =\n442 diabetes patients, as well as the response of interest, a\nquantitative measure of disease progression one year after baseline.\n\n**Data Set Characteristics:**\n\n  :Number of Instances: 442\n\n  :Number of Attributes: First 10 columns are numeric predictive values\n\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n\n  :Attribute Information:\n      - age     age in years\n      - sex\n      - bmi     body mass index\n      - bp      average blood pressure\n      - s1      tc, total serum cholesterol\n      - s2      ldl, low-density lipoproteins\n      - s3      hdl, high-density lipoproteins\n      - s4      tch, total cholesterol / HDL\n      - s5      ltg, possibly log of serum triglycerides level\n      - s6      glu, blood sugar level\n\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n\nSource URL:\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n\nFor more information see:\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n\n\n\n\n1. sklearn 라이브러리 활용\n\n# sklearn 라이브러리 활용\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\n\n# 독립변수와 종속변수 설정\nx = x[['age','sex','bmi']]\nprint(x.head())\nprint(y.head())\n\n        age       sex       bmi\n0  0.038076  0.050680  0.061696\n1 -0.001882 -0.044642 -0.051474\n2  0.085299  0.050680  0.044451\n3 -0.089063 -0.044642 -0.011595\n4  0.005383 -0.044642 -0.036385\n----------------------------------\n   target\n0   151.0\n1    75.0\n2   141.0\n3   206.0\n4   135.0\n\n\n\n회귀식 : y = b0 + b1x1 + b2x2 + b3x3\n(x1=age, x2=sex, x3=bmi)\n\n\n# 모델링 \nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(x, y)\n\nLinearRegression()\n\n\n\n# 회귀분석 관련 지표 출력\n\n# 1. Rsq(결정계수) : model.score(x,y)\nmodel.score(x, y)\nprint(round(model.score(x,y),2))\n\n0.35\n\n\n\n# 2. 회귀계수 출력 : model.coef_\nprint(np.round(model.coef_, 2))        # 전체 회귀계수\nprint(np.round(model.coef_[0,0], 2))   # x1의 회귀계수\nprint(np.round(model.coef_[0,1], 2))   # x2의 회귀계수\nprint(np.round(model.coef_[0,2], 2))   # x3의 회귀계수\n\n[[138.9  -36.14 926.91]]\n138.9\n-36.14\n926.91\n\n\n\n# 3. 회귀계수(절편) : model.intercept_\nprint(np.round(model.intercept_, 2))\n\n[152.13]\n\n\n\n회귀식 : y = b0 + b1x1 + b2x2 + b3x3\n(x1=age, x2=sex, x3=bmi)\n\n\n\n결과 : 152.13 + 138.9age -36.14sex + 926.91bmi"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_5.html#statsmodels-라이브러리-사용",
    "href": "BigData_Analysis/실기_3유형_5.html#statsmodels-라이브러리-사용",
    "title": "빅분기 실기 - 3유형 문제풀이(5)",
    "section": "2. statsmodels 라이브러리 사용",
    "text": "2. statsmodels 라이브러리 사용\n\n###############  실기환경 복사 영역  ###############\nimport pandas as pd\nimport numpy as np\n# 실기 시험 데이터셋으로 셋팅하기 (수정금지)\nfrom sklearn.datasets import load_diabetes\n# diabetes 데이터셋 로드\ndiabetes = load_diabetes()\nx = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\ny = pd.DataFrame(diabetes.target)\ny.columns = ['target'] \n###############  실기환경 복사 영역  ###############\n\n\n# statsmodel.formula 활용\nimport statsmodels.api as sm\n# 독립변수와 종속변수 설정\nx = x[['age','sex','bmi']]\ny = y[['target']]\nprint(x.head())\nprint(y.head())\n\n        age       sex       bmi\n0  0.038076  0.050680  0.061696\n1 -0.001882 -0.044642 -0.051474\n2  0.085299  0.050680  0.044451\n3 -0.089063 -0.044642 -0.011595\n4  0.005383 -0.044642 -0.036385\n   target\n0   151.0\n1    75.0\n2   141.0\n3   206.0\n4   135.0\n\n\n\n# 모델링\nimport statsmodels.api as sm\n\nx = sm.add_constant(x)        # 주의 : 상수항 추가해줘야 함\nmodel = sm.OLS(y, x).fit()\n# y_pred = model.predict(x)\nsummary = model.summary()\nprint(summary)\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 target   R-squared:                       0.351\nModel:                            OLS   Adj. R-squared:                  0.346\nMethod:                 Least Squares   F-statistic:                     78.94\nDate:                Wed, 29 Nov 2023   Prob (F-statistic):           7.77e-41\nTime:                        14:29:14   Log-Likelihood:                -2451.6\nNo. Observations:                 442   AIC:                             4911.\nDf Residuals:                     438   BIC:                             4928.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        152.1335      2.964     51.321      0.000     146.307     157.960\nage          138.9039     64.254      2.162      0.031      12.618     265.189\nsex          -36.1353     63.391     -0.570      0.569    -160.724      88.453\nbmi          926.9120     63.525     14.591      0.000     802.061    1051.763\n==============================================================================\nOmnibus:                       14.687   Durbin-Watson:                   1.851\nProb(Omnibus):                  0.001   Jarque-Bera (JB):                8.290\nSkew:                           0.150   Prob(JB):                       0.0158\nKurtosis:                       2.400   Cond. No.                         23.7\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# 1. Rsq(결정계수)\n# r2 = 0.351\n\n# 2. 회귀계수\n# age = 138.9039\n# sex = -36.1353\n# bmi = 926.9120\n\n# 3. 회귀계수(절편)\n# const = 152.1335\n\n# 4. 회귀식 p-value\n# pvalue = 7.77e-41 # 0에 가까운 작은 값"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_5.html#결과-비교해보기-두-라이브러리-모두-같은-결과값을-출력",
    "href": "BigData_Analysis/실기_3유형_5.html#결과-비교해보기-두-라이브러리-모두-같은-결과값을-출력",
    "title": "빅분기 실기 - 3유형 문제풀이(5)",
    "section": "(결과 비교해보기) 두 라이브러리 모두 같은 결과값을 출력",
    "text": "(결과 비교해보기) 두 라이브러리 모두 같은 결과값을 출력\n\n회귀식 : y = b0 + b1x1 + b2x2 + b3x3\n(x1=age, x2=sex, x3=bmi)\n\n\n1. sklearn : y = 152.13 + 138.9age -36.14sex + 926.91bmi\n\n\n2. statsmodel : y = 152.13 + 138.9age -36.14sex + 926.91bmi"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_5.html#상관분석",
    "href": "BigData_Analysis/실기_3유형_5.html#상관분석",
    "title": "빅분기 실기 - 3유형 문제풀이(5)",
    "section": "✅ 상관분석",
    "text": "✅ 상관분석\n\n###############  실기환경 복사 영역  ###############\nimport pandas as pd\nimport numpy as np\n# 실기 시험 데이터셋으로 셋팅하기 (수정금지)\nfrom sklearn.datasets import load_diabetes\n# diabetes 데이터셋 로드\ndiabetes = load_diabetes()\nx = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\ny = pd.DataFrame(diabetes.target)\ny.columns = ['target'] \n###############  실기환경 복사 영역  ###############\n\n\n# 상관분석을 할 2가지 변수 설정\nx = x['bmi']\ny = y['target']\nprint(x.head())\nprint(y.head())\n\n0    0.061696\n1   -0.051474\n2    0.044451\n3   -0.011595\n4   -0.036385\nName: bmi, dtype: float64\n0    151.0\n1     75.0\n2    141.0\n3    206.0\n4    135.0\nName: target, dtype: float64\n\n\n\n# 라이브러리 불러오기\nfrom scipy.stats import pearsonr\n\n# 상관계수에 대한 검정실시\nr, pvalue = pearsonr(x, y)\n\n# 가설검정\n# H0 : 두 변수간 선형관계가 존재하지 않는다 (p=0)\n# H1 : 두 변수간 선형관계가 존재한다 (p!=0)\n\n# 1. 상관계수\nprint(round(r,2))\n\n# 2. p-value\nprint(round(pvalue,2))\n\n# 3. 검정통계량\n# 통계량은 별로돌 구해야 함 (T = r*root(n-2) / root(1-2r))\n# r = 상관계수\n# n = 데이터의 개수\n\nn = len(x) # 데이터 수\nr2 = r**2  # 상관꼐수의 제곱  \nstatistic = r * ((n-2)**0.5) / ((1-r2)**0.5)\n\nprint(round(statistic,2)) # 통계량값이 크면 p-value 값이 작아진다\n\n# 4. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값이 0.05보다 작기 떄문에 귀무가설을 기각한다.\n# 즉, 두 변수간 선형관계가 존재한다고 할 수 있다.(상관계수가 0이 아니다)\n\n# 답 : 기각\n\n0.59\n0.0\n15.19"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_6.html",
    "href": "BigData_Analysis/실기_3유형_6.html",
    "title": "빅분기 실기 - 3유형 문제풀이(6)",
    "section": "",
    "text": "실기 3 유형(6)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_6.html#타이타닉-데이터-불러오기생존자-예측-데이터",
    "href": "BigData_Analysis/실기_3유형_6.html#타이타닉-데이터-불러오기생존자-예측-데이터",
    "title": "빅분기 실기 - 3유형 문제풀이(6)",
    "section": "타이타닉 데이터 불러오기(생존자 예측 데이터)",
    "text": "타이타닉 데이터 불러오기(생존자 예측 데이터)\n\n# 데이터 불러오기\nimport pandas as pd\nimport numpy as np\n\n# Seaborn의 내장 타이타닉 데이터셋을 불러옵니다.\nimport seaborn as sns\ndf = sns.load_dataset('titanic')\n\n\nprint(df.head())\n\n   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n0         0       3    male  22.0      1      0   7.2500        S  Third   \n1         1       1  female  38.0      1      0  71.2833        C  First   \n2         1       3  female  26.0      0      0   7.9250        S  Third   \n3         1       1  female  35.0      1      0  53.1000        S  First   \n4         0       3    male  35.0      0      0   8.0500        S  Third   \n\n     who  adult_male deck  embark_town alive  alone  \n0    man        True  NaN  Southampton    no  False  \n1  woman       False    C    Cherbourg   yes  False  \n2  woman       False  NaN  Southampton   yes   True  \n3  woman       False    C  Southampton   yes  False  \n4    man        True  NaN  Southampton    no   True  \n\n\n\n# 분석 데이터 설정\ndf = df[['survived', 'sex', 'sibsp', 'fare']] \n# sex:성별, sibsp:탑승한 부모 및 자녀수 fare:요금\n\nprint(df.head())\n\n   survived     sex  sibsp     fare\n0         0    male      1   7.2500\n1         1  female      1  71.2833\n2         1  female      0   7.9250\n3         1  female      1  53.1000\n4         0    male      0   8.0500\n\n\n\n로지스틱 회귀식 : P(1일 확률) = 1 / (1+exp(-f(x))\n\nf(x) = b0 + b1x1 + b2x2 + b3x3\nln(P/1-P) = b0 + b1x1 + b2x2 + b3x3\n(P = 생존할 확률, x1=sex, x2=sibsp, x3=fare)\n\n\n# 데이터 전처리\n# 변수처리\n# 문자형 타입의 데이터의 경우 숫자로 변경해준다.\n# *** 실제 시험에서 지시사항을 따를 것 ***\n\n# 성별을 map 함수를 활용해서 각각 1과 0에 할당한다. (여성을 1, 남성을 0)\n# (실제 시험의 지시 조건에 따를 것)\ndf['sex'] = df['sex'].map({'female':1,\n                          'male':0})\nprint(df.head())\n\n   survived  sex  sibsp     fare\n0         0    0      1   7.2500\n1         1    1      1  71.2833\n2         1    1      0   7.9250\n3         1    1      1  53.1000\n4         0    0      0   8.0500\n\n\n\nprint(df.info())\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 4 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   survived  891 non-null    int64  \n 1   sex       891 non-null    int64  \n 2   sibsp     891 non-null    int64  \n 3   fare      891 non-null    float64\ndtypes: float64(1), int64(3)\nmemory usage: 28.0 KB\nNone"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_6.html#sklearn-라이브러리-활용",
    "href": "BigData_Analysis/실기_3유형_6.html#sklearn-라이브러리-활용",
    "title": "빅분기 실기 - 3유형 문제풀이(6)",
    "section": "1. sklearn 라이브러리 활용",
    "text": "1. sklearn 라이브러리 활용\n\n# 독립변수와 종속변수 설정\nx = df.drop(['survived'], axis=1) # x = df[['sex', 'age', 'fare']]\ny = df['survived']\n\n(주의) LogisticRegrresion() 객체안에 반드시 penalty=None으로 입력해야 함\n\n# 모델링\nfrom sklearn.linear_model import LogisticRegression # 회귀는 LinearRegression\n\n# 반드시 penalty = None으로 입력할 것해야 함.  default='l2'\nmodel1 = LogisticRegression(penalty='none')\nmodel1.fit(x, y)\n\nLogisticRegression(penalty='none')\n\n\n\n# 로지스틱회귀분석 관련 지표 출력\n\n# 1.회귀계수 출력 : model.coef_\nprint(np.round(model1.coef_, 4))        # 전체 회귀계수\nprint(np.round(model1.coef_[0,0], 4))   # x1의 회귀계수\nprint(np.round(model1.coef_[0,1], 4))   # x2의 회귀계수\nprint(np.round(model1.coef_[0,2], 4))   # x3의 회귀계수\n\n# 2. 회귀계수(절편) : model.intercept_\nprint(np.round(model1.intercept_, 4))\n\n[[ 2.5668 -0.4017  0.0138]]\n2.5668\n-0.4017\n0.0138\n[-1.6964]\n\n\n\n로지스틱 회귀식 : P(1일 확률) = 1 / (1+exp(-f(x))\n\nf(x) = b0 + b1x1 + b2x2 + b3x3\nln(P/1-P) = b0 + b1x1 + b2x2 + b3x3\n(P = 생존할 확률, x1=sex, x2=sibsp, x3=fare)\n\n\n\n결과 : ln(P/1-P) = -1.6964 + 2.5668sex - 0.4017sibsp + 0.0138fare\n\n# 3-1. 로지스틱 회귀모형에서 sibsp 변수가 한단위 증가할 때 생존할 오즈가 몇 배 \n#      증가하는지 반올림하여 소수점 셋째 자리까지 구하시오.\n\n# exp(b2) 를 구하면 된다.\nresult = np.exp(model1.coef_[0,1]) #인덱싱 주의하세요.\nprint(round(result,3))\n\n# 해석 : sibsp 변수가 한 단위 증가할 때 생존할 오즈가 0.669배 증가한다.\n#        생존할 오즈가 33% 감소한다.(생존할 확률이 감소한다.)\n\n0.669\n\n\n\n# 3-2. 로지스틱 회귀모형에서 여성일 경우 남성에 비해 오즈가 몇 배 증가하는지\n#      반올림하여 소수점 셋째 자리까지 구하시오\n\n# exp(b2) 를 구하면 된다.\nresult2 = np.exp(model1.coef_[0,0]) #인덱싱 주의하세요.\nprint(round(result2,3))\n\n# 해석 : 여성일 경우 남성에 비해 생존할 오즈가 13.024배 증가한다.\n#        생존할 오즈가 13배 증가한다.(생존할 확률이 증가한다.)\n\n13.024"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_6.html#statsmodels-라이브러리-사용",
    "href": "BigData_Analysis/실기_3유형_6.html#statsmodels-라이브러리-사용",
    "title": "빅분기 실기 - 3유형 문제풀이(6)",
    "section": "2. statsmodels 라이브러리 사용",
    "text": "2. statsmodels 라이브러리 사용\n(주의) 실제 오즈가 몇 배 증가했는지 계산하는 문제가 나온다면\nsklearn 라이브러리를 사용하여 회귀계수를 직접구해서 계산할 것(소수점이 결과값에 영향을 줄 수 있음)\n\n# 모델링\nimport statsmodels.api as sm\n\nx = sm.add_constant(x)       # 주의 : 상수항 추가해줘야 함\nmodel2 = sm.Logit(y,x).fit() # 주의할 것 : y, x 순으로 입력해야 함\nsummary = model2.summary()\nprint(summary)\n\nOptimization terminated successfully.\n         Current function value: 0.483846\n         Iterations 6\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:               survived   No. Observations:                  891\nModel:                          Logit   Df Residuals:                      887\nMethod:                           MLE   Df Model:                            3\nDate:                Wed, 29 Nov 2023   Pseudo R-squ.:                  0.2734\nTime:                        19:06:26   Log-Likelihood:                -431.11\nconverged:                       True   LL-Null:                       -593.33\nCovariance Type:            nonrobust   LLR p-value:                 5.094e-70\n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -1.6964      0.129    -13.134      0.000      -1.950      -1.443\nsex            2.5668      0.179     14.321      0.000       2.216       2.918\nsibsp         -0.4017      0.095     -4.222      0.000      -0.588      -0.215\nfare           0.0138      0.003      5.367      0.000       0.009       0.019\n==============================================================================\n\n\n\n(결과 비교해보기) 두 라이브러리 모두 같은 결과값을 출력\n\n회귀식 : P(1일 확률) = 1 / (1+exp(-f(x))\nf(x) = b0 + b1x1 + b2x2 + b3x3\nln(P/1-P) = b0 + b1x1 + b2x2 + b3x3\n(P=생존할 확률, x1=sex, x2=sibsp, x3=fare)\n\n1. sklearn : ln(P/1-P) = -1.6964 + 2.5668sex - 0.4017sibsp + 0.0138fare\n2. statsmodel : ln(P/1-P) = -1.6964 + 2.5668sex - 0.4017sibsp + 0.0138fare"
  },
  {
    "objectID": "BigData_Analysis/실기_파이썬기초.html",
    "href": "BigData_Analysis/실기_파이썬기초.html",
    "title": "빅분기 실기 - 파이썬 기초",
    "section": "",
    "text": "파이썬 기초"
  },
  {
    "objectID": "BigData_Analysis/실기_파이썬기초.html#데이터-타입object-int-float-bool-등",
    "href": "BigData_Analysis/실기_파이썬기초.html#데이터-타입object-int-float-bool-등",
    "title": "빅분기 실기 - 파이썬 기초",
    "section": "1.데이터 타입(object, int, float, bool 등)",
    "text": "1.데이터 타입(object, int, float, bool 등)\n\n# 데이터 타입 확인\ndf.dtypes\n\ncar      object\nmpg     float64\ncyl       int64\ndisp    float64\nhp        int64\ndrat    float64\nwt      float64\nqsec    float64\nvs        int64\nam        int64\ngear      int64\ncarb      int64\ndtype: object\n\n\n\n# 데이터 타입 변경 (1개)\ndf1 = df.copy()\ndf1 = df1.astype({'cyl':'object'})\nprint(df1.dtypes)\n\ncar      object\nmpg     float64\ncyl      object\ndisp    float64\nhp        int64\ndrat    float64\nwt      float64\nqsec    float64\nvs        int64\nam        int64\ngear      int64\ncarb      int64\ndtype: object\n\n\n\n# 데이터 타입 변경 (2개 이상)\ndf1 = df.copy()\ndf1 = df1.astype({'cyl':'int', 'gear':'object'})\nprint(df1.dtypes)\n\ncar      object\nmpg     float64\ncyl       int32\ndisp    float64\nhp        int64\ndrat    float64\nwt      float64\nqsec    float64\nvs        int64\nam        int64\ngear     object\ncarb      int64\ndtype: object\n\n\n\n#df1['cyl']\n\n\ndf1['cyl'].value_counts()\n\n8    14\n4    11\n6     7\nName: cyl, dtype: int64"
  },
  {
    "objectID": "BigData_Analysis/실기_파이썬기초.html#기초통계량평균-중앙값-사분위수-iqr-표준편차-등",
    "href": "BigData_Analysis/실기_파이썬기초.html#기초통계량평균-중앙값-사분위수-iqr-표준편차-등",
    "title": "빅분기 실기 - 파이썬 기초",
    "section": "2. 기초통계량(평균, 중앙값, 사분위수, IQR, 표준편차 등)",
    "text": "2. 기초통계량(평균, 중앙값, 사분위수, IQR, 표준편차 등)\n\n# Import CSV mtcars\ndf = pd.read_csv('mtcars.txt')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      Hornet Sportabout\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\ndf.shape # (행, 열)\n\n(32, 12)\n\n\n\n# 평균값 구하기\nmpg_mean = df['mpg'].mean()\nprint(mpg_mean)\n\n20.090624999999996\n\n\n\n# 중앙값 구하기\nmpg_median = df['mpg'].median()\nprint(mpg_median)\n\n19.2\n\n\n\n# 최빈값 구하기\ncyl_mode = df['cyl'].mode()\nprint(cyl_mode)\n\n0    8\nName: cyl, dtype: int64\n\n\n\ndf['cyl'].value_counts()\n\n8    14\n4    11\n6     7\nName: cyl, dtype: int64\n\n\n\n# 분산\nmpg_var = df['mpg'].var()\nprint(mpg_var)\n\n36.32410282258065\n\n\n\n# 표준편차\nmpg_std = df['mpg'].std()\nprint(mpg_std)\n\n6.026948052089105\n\n\n\n# IQR (Q3 - Q1)\nQ1 = df['mpg'].quantile(.25)\nprint(Q1)\n\n15.425\n\n\n\nQ3 = df['mpg'].quantile(.75)\nprint(Q3)\n\n22.8\n\n\n\nIQR = Q3 - Q1\nprint(IQR)\n\n7.375\n\n\n\nQ2 = df['mpg'].quantile(.5)\nprint(Q2)\nprint(df['mpg'].median()) # 2사분위수와 중앙값은 동일한 값을 출력 \n\n19.2\n19.2\n\n\n\n# 범위(Range) = 최대값 - 최소값\nmpg_max = df['mpg'].max()\nprint(mpg_max)\n\n33.9\n\n\n\nmpg_min = df['mpg'].min()\nprint(mpg_min)\n\n10.4\n\n\n\nmpg_range = mpg_max - mpg_min\nprint(mpg_range)\n\n23.5\n\n\n\n1) 분포의 비대칭도\n\n# 왜도\nmpg_skew = df['mpg'].skew()\nprint(mpg_skew)\n\n0.6723771376290805\n\n\n\n# 첨도\nmpg_kurt = df['mpg'].kurt()\nprint(mpg_kurt)\n\n-0.0220062914240855\n\n\n\n\n2) 기타(합계, 절대값, 데이터 수 등)\n\n# 합계\nmpg_sum = df['mpg'].sum()\nprint(mpg_sum)\n\n642.9000000000001\n\n\n\n# 절대값\nIQR2 = Q1 - Q3\nprint(IQR2)\nprint(abs(IQR2))\n\n-7.375\n7.375\n\n\n\n# 데이터 수\nprint(len(df['mpg']))\n\n32\n\n\n\n\n3) 그룹화하여 계산하기 (groupby 활용)\n\nimport seaborn as sns\ndf = sns.load_dataset('iris')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      sepal_length\n      sepal_width\n      petal_length\n      petal_width\n      species\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      setosa\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      setosa\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      setosa\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      setosa\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      setosa\n    \n  \n\n\n\n\n\n# species 별로 각 변수의 평균 구해보기\ndf.groupby('species').mean()\n\n\n\n\n\n  \n    \n      \n      sepal_length\n      sepal_width\n      petal_length\n      petal_width\n    \n    \n      species\n      \n      \n      \n      \n    \n  \n  \n    \n      setosa\n      5.006\n      3.428\n      1.462\n      0.246\n    \n    \n      versicolor\n      5.936\n      2.770\n      4.260\n      1.326\n    \n    \n      virginica\n      6.588\n      2.974\n      5.552\n      2.026\n    \n  \n\n\n\n\n\ndf.groupby('species').median()\n\n\n\n\n\n  \n    \n      \n      sepal_length\n      sepal_width\n      petal_length\n      petal_width\n    \n    \n      species\n      \n      \n      \n      \n    \n  \n  \n    \n      setosa\n      5.0\n      3.4\n      1.50\n      0.2\n    \n    \n      versicolor\n      5.9\n      2.8\n      4.35\n      1.3\n    \n    \n      virginica\n      6.5\n      3.0\n      5.55\n      2.0"
  },
  {
    "objectID": "BigData_Analysis/실기_파이썬기초.html#데이터-인덱싱-필터링-정렬-변경-등",
    "href": "BigData_Analysis/실기_파이썬기초.html#데이터-인덱싱-필터링-정렬-변경-등",
    "title": "빅분기 실기 - 파이썬 기초",
    "section": "3. 데이터 인덱싱, 필터링, 정렬, 변경 등",
    "text": "3. 데이터 인덱싱, 필터링, 정렬, 변경 등\n\ndf = pd.read_csv('mtcars.txt')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      Hornet Sportabout\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\n1) 데이터 인덱싱\n인덱싱 - 행, 열을 기준으로 데이터값을 기준으로 뽑는 것\n\n# 행/열 인덱싱 : df.loc['행', '열']\ndf.loc[3, 'mpg'] # 인덱싱은 0부터 시작임 \n\n21.4\n\n\n\n# 열만 인덱싱\ndf.loc[:, 'mpg'].head() # ':' 는 전체를 가지고 올 때(빈칸으로 두면 에러)\n\n0    21.0\n1    21.0\n2    22.8\n3    21.4\n4    18.7\nName: mpg, dtype: float64\n\n\n\ndf.loc[0:3, ['mpg', 'cyl', 'disp']] # 행에서 0~3번 사이의 인덱스를 가지고 와라\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n    \n  \n  \n    \n      0\n      21.0\n      6\n      160.0\n    \n    \n      1\n      21.0\n      6\n      160.0\n    \n    \n      2\n      22.8\n      4\n      108.0\n    \n    \n      3\n      21.4\n      6\n      258.0\n    \n  \n\n\n\n\n\ndf.loc[0:3, 'mpg':'disp']  # 열에서 mpg와 disp 사이의 변수를 가지고 와라\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n    \n  \n  \n    \n      0\n      21.0\n      6\n      160.0\n    \n    \n      1\n      21.0\n      6\n      160.0\n    \n    \n      2\n      22.8\n      4\n      108.0\n    \n    \n      3\n      21.4\n      6\n      258.0\n    \n  \n\n\n\n\n\n# 앞에서 n행 인덱싱\ndf.head(2)\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.9\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.9\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n  \n\n\n\n\n\n# 뒤에서 n행 인덱싱\ndf.tail(3)\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      29\n      Ferrari Dino\n      19.7\n      6\n      145.0\n      175\n      3.62\n      2.77\n      15.5\n      0\n      1\n      5\n      6\n    \n    \n      30\n      Maserati Bora\n      15.0\n      8\n      301.0\n      335\n      3.54\n      3.57\n      14.6\n      0\n      1\n      5\n      8\n    \n    \n      31\n      Volvo 142E\n      21.4\n      4\n      121.0\n      109\n      4.11\n      2.78\n      18.6\n      1\n      1\n      4\n      2\n    \n  \n\n\n\n\n\n\n2) 열(Columns) 추가/제거\n\n# 열 선택\ndf_cyl = df['cyl']\ndf_cyl.head(3) # df.cyl.head(3) 같은 결과 비추\n\n0    6\n1    6\n2    4\nName: cyl, dtype: int64\n\n\n\ndf_new = df[['cyl','mpg']]\ndf_new.head(3)\n\n\n\n\n\n  \n    \n      \n      cyl\n      mpg\n    \n  \n  \n    \n      0\n      6\n      21.0\n    \n    \n      1\n      6\n      21.0\n    \n    \n      2\n      4\n      22.8\n    \n  \n\n\n\n\n\n# 열 제거\ndf.drop(columns = ['car','mpg','cyl']).head(3)\n\n\n\n\n\n  \n    \n      \n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n  \n\n\n\n\n\n# 열 추가\ndf2 = df.copy()\ndf2['new'] = df['mpg'] + 10 # 새로운 컬럼으로 생김\ndf2.head(3)\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n      new\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n      31.0\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n      31.0\n    \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n      32.8\n    \n  \n\n\n\n\n\n\n3) 데이터 필터링\n\n# 1개 조건 필터링\n# cyl = 4 인 데이터의 수 \ncond1 = (df['cyl'] == 4)\nlen(df[cond1])\n\n# cyl_4 = df[df['cyl']==4]\n# print(len(cyl_4))\n\n11\n\n\n\n# mpg 가 22 이상인 데이터 수\ncond2 = (df['mpg'] >= 22)\nlen(df[cond2])\n\n9\n\n\n\n# 2개 조건 필터링 \ndf[cond1 & cond2]\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      7\n      Merc 240D\n      24.4\n      4\n      146.7\n      62\n      3.69\n      3.190\n      20.00\n      1\n      0\n      4\n      2\n    \n    \n      8\n      Merc 230\n      22.8\n      4\n      140.8\n      95\n      3.92\n      3.150\n      22.90\n      1\n      0\n      4\n      2\n    \n    \n      17\n      Fiat 128\n      32.4\n      4\n      78.7\n      66\n      4.08\n      2.200\n      19.47\n      1\n      1\n      4\n      1\n    \n    \n      18\n      Honda Civic\n      30.4\n      4\n      75.7\n      52\n      4.93\n      1.615\n      18.52\n      1\n      1\n      4\n      2\n    \n    \n      19\n      Toyota Corolla\n      33.9\n      4\n      71.1\n      65\n      4.22\n      1.835\n      19.90\n      1\n      1\n      4\n      1\n    \n    \n      25\n      Fiat X1-9\n      27.3\n      4\n      79.0\n      66\n      4.08\n      1.935\n      18.90\n      1\n      1\n      4\n      1\n    \n    \n      26\n      Porsche 914-2\n      26.0\n      4\n      120.3\n      91\n      4.43\n      2.140\n      16.70\n      0\n      1\n      5\n      2\n    \n    \n      27\n      Lotus Europa\n      30.4\n      4\n      95.1\n      113\n      3.77\n      1.513\n      16.90\n      1\n      1\n      5\n      2\n    \n  \n\n\n\n\n\n# 2개 조건 필터링 후 데이터 개수 (and)\nprint(len(df[cond1 & cond2])) # print() 함수를 이용해서 개수를 출력해야 인정됨 \n\n9\n\n\n\n# 2개 조건 필터링 후 데이터 개수 (or)\ndf[cond1 | cond2]\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      7\n      Merc 240D\n      24.4\n      4\n      146.7\n      62\n      3.69\n      3.190\n      20.00\n      1\n      0\n      4\n      2\n    \n    \n      8\n      Merc 230\n      22.8\n      4\n      140.8\n      95\n      3.92\n      3.150\n      22.90\n      1\n      0\n      4\n      2\n    \n    \n      17\n      Fiat 128\n      32.4\n      4\n      78.7\n      66\n      4.08\n      2.200\n      19.47\n      1\n      1\n      4\n      1\n    \n    \n      18\n      Honda Civic\n      30.4\n      4\n      75.7\n      52\n      4.93\n      1.615\n      18.52\n      1\n      1\n      4\n      2\n    \n    \n      19\n      Toyota Corolla\n      33.9\n      4\n      71.1\n      65\n      4.22\n      1.835\n      19.90\n      1\n      1\n      4\n      1\n    \n    \n      20\n      Toyota Corona\n      21.5\n      4\n      120.1\n      97\n      3.70\n      2.465\n      20.01\n      1\n      0\n      3\n      1\n    \n    \n      25\n      Fiat X1-9\n      27.3\n      4\n      79.0\n      66\n      4.08\n      1.935\n      18.90\n      1\n      1\n      4\n      1\n    \n    \n      26\n      Porsche 914-2\n      26.0\n      4\n      120.3\n      91\n      4.43\n      2.140\n      16.70\n      0\n      1\n      5\n      2\n    \n    \n      27\n      Lotus Europa\n      30.4\n      4\n      95.1\n      113\n      3.77\n      1.513\n      16.90\n      1\n      1\n      5\n      2\n    \n    \n      31\n      Volvo 142E\n      21.4\n      4\n      121.0\n      109\n      4.11\n      2.780\n      18.60\n      1\n      1\n      4\n      2\n    \n  \n\n\n\n\n\nprint(len(df[cond1 | cond2]))\n\n11\n\n\n\n# 한번에 코딩할 경우\nprint(len(df[(df['cyl'] == 4) & (df['mpg'] >= 22)]))\nprint(len(df[(df['cyl'] == 4) | (df['mpg'] >= 22)])) # 이렇게 한번에 하면 실수할 가능성 존재 cond1,2를 만들어서 하는 것 추천\n\n9\n11\n\n\n\n\n4) 데이터 정렬\n\n# 내림차순 정렬 (위에서부터 내려간다)\ndf.sort_values('mpg', ascending=False).head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      19\n      Toyota Corolla\n      33.9\n      4\n      71.1\n      65\n      4.22\n      1.835\n      19.90\n      1\n      1\n      4\n      1\n    \n    \n      17\n      Fiat 128\n      32.4\n      4\n      78.7\n      66\n      4.08\n      2.200\n      19.47\n      1\n      1\n      4\n      1\n    \n    \n      27\n      Lotus Europa\n      30.4\n      4\n      95.1\n      113\n      3.77\n      1.513\n      16.90\n      1\n      1\n      5\n      2\n    \n    \n      18\n      Honda Civic\n      30.4\n      4\n      75.7\n      52\n      4.93\n      1.615\n      18.52\n      1\n      1\n      4\n      2\n    \n    \n      25\n      Fiat X1-9\n      27.3\n      4\n      79.0\n      66\n      4.08\n      1.935\n      18.90\n      1\n      1\n      4\n      1\n    \n  \n\n\n\n\n\n# 오름차순 정렬 (아래에서부터 올라간다)\ndf.sort_values('mpg', ascending=True).head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      15\n      Lincoln Continental\n      10.4\n      8\n      460.0\n      215\n      3.00\n      5.424\n      17.82\n      0\n      0\n      3\n      4\n    \n    \n      14\n      Cadillac Fleetwood\n      10.4\n      8\n      472.0\n      205\n      2.93\n      5.250\n      17.98\n      0\n      0\n      3\n      4\n    \n    \n      23\n      Camaro Z28\n      13.3\n      8\n      350.0\n      245\n      3.73\n      3.840\n      15.41\n      0\n      0\n      3\n      4\n    \n    \n      6\n      Duster 360\n      14.3\n      8\n      360.0\n      245\n      3.21\n      3.570\n      15.84\n      0\n      0\n      3\n      4\n    \n    \n      16\n      Chrysler Imperial\n      14.7\n      8\n      440.0\n      230\n      3.23\n      5.345\n      17.42\n      0\n      0\n      3\n      4\n    \n  \n\n\n\n\n\n\n5) 데이터 변경 (조건문)\n\nimport numpy as np \ndf = pd.read_csv(\"mtcars.txt\")\n# np.where 활용\n# hp 변수 값 중에서 205가 넘는 값은 205로 처리하고, 나머지는 그대로 유지\ndf['hp'] = np.where(df['hp'] > 205, 205, df['hp'])\n\n# 내림차순 정렬 (위에서부터 내려간다)\ndf.sort_values('hp', ascending=False).head(10)\n\n# 활용 : 이상치를 Max 값이나 Min 값으로 대체할 경우 조건문 활용\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      16\n      Chrysler Imperial\n      14.7\n      8\n      440.0\n      205\n      3.23\n      5.345\n      17.42\n      0\n      0\n      3\n      4\n    \n    \n      30\n      Maserati Bora\n      15.0\n      8\n      301.0\n      205\n      3.54\n      3.570\n      14.60\n      0\n      1\n      5\n      8\n    \n    \n      28\n      Ford Pantera L\n      15.8\n      8\n      351.0\n      205\n      4.22\n      3.170\n      14.50\n      0\n      1\n      5\n      4\n    \n    \n      6\n      Duster 360\n      14.3\n      8\n      360.0\n      205\n      3.21\n      3.570\n      15.84\n      0\n      0\n      3\n      4\n    \n    \n      23\n      Camaro Z28\n      13.3\n      8\n      350.0\n      205\n      3.73\n      3.840\n      15.41\n      0\n      0\n      3\n      4\n    \n    \n      15\n      Lincoln Continental\n      10.4\n      8\n      460.0\n      205\n      3.00\n      5.424\n      17.82\n      0\n      0\n      3\n      4\n    \n    \n      14\n      Cadillac Fleetwood\n      10.4\n      8\n      472.0\n      205\n      2.93\n      5.250\n      17.98\n      0\n      0\n      3\n      4\n    \n    \n      13\n      Merc 450SLC\n      15.2\n      8\n      275.8\n      180\n      3.07\n      3.780\n      18.00\n      0\n      0\n      3\n      3\n    \n    \n      11\n      Merc 450SE\n      16.4\n      8\n      275.8\n      180\n      3.07\n      4.070\n      17.40\n      0\n      0\n      3\n      3\n    \n    \n      12\n      Merc 450SL\n      17.3\n      8\n      275.8\n      180\n      3.07\n      3.730\n      17.60\n      0\n      0\n      3\n      3"
  },
  {
    "objectID": "BigData_Analysis/실기_파이썬기초.html#결측치-이상치-중복값-처리제거-or-대체",
    "href": "BigData_Analysis/실기_파이썬기초.html#결측치-이상치-중복값-처리제거-or-대체",
    "title": "빅분기 실기 - 파이썬 기초",
    "section": "4. 결측치, 이상치, 중복값 처리(제거 or 대체)",
    "text": "4. 결측치, 이상치, 중복값 처리(제거 or 대체)\n\n🚢 데이터 불러오기 (타이타닉 데이터셋)\n\n종속변수(y) : 생존 여부 (0 사망, 1 생존)\n\n\n독립변수(x) : pclass, sex, age 등의 탑승자 정보(변수)\n\n\nimport seaborn as sns\n# 데이터셋 목록 : sns.get_dataset_names()\n# 타이타닉 데이터 불러오기\ndf = sns.load_dataset('titanic')\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      survived\n      pclass\n      sex\n      age\n      sibsp\n      parch\n      fare\n      embarked\n      class\n      who\n      adult_male\n      deck\n      embark_town\n      alive\n      alone\n    \n  \n  \n    \n      0\n      0\n      3\n      male\n      22.0\n      1\n      0\n      7.2500\n      S\n      Third\n      man\n      True\n      NaN\n      Southampton\n      no\n      False\n    \n    \n      1\n      1\n      1\n      female\n      38.0\n      1\n      0\n      71.2833\n      C\n      First\n      woman\n      False\n      C\n      Cherbourg\n      yes\n      False\n    \n    \n      2\n      1\n      3\n      female\n      26.0\n      0\n      0\n      7.9250\n      S\n      Third\n      woman\n      False\n      NaN\n      Southampton\n      yes\n      True\n    \n    \n      3\n      1\n      1\n      female\n      35.0\n      1\n      0\n      53.1000\n      S\n      First\n      woman\n      False\n      C\n      Southampton\n      yes\n      False\n    \n    \n      4\n      0\n      3\n      male\n      35.0\n      0\n      0\n      8.0500\n      S\n      Third\n      man\n      True\n      NaN\n      Southampton\n      no\n      True\n    \n  \n\n\n\n\n\ndf.shape\n\n(891, 15)\n\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 15 columns):\n #   Column       Non-Null Count  Dtype   \n---  ------       --------------  -----   \n 0   survived     891 non-null    int64   \n 1   pclass       891 non-null    int64   \n 2   sex          891 non-null    object  \n 3   age          714 non-null    float64 \n 4   sibsp        891 non-null    int64   \n 5   parch        891 non-null    int64   \n 6   fare         891 non-null    float64 \n 7   embarked     889 non-null    object  \n 8   class        891 non-null    category\n 9   who          891 non-null    object  \n 10  adult_male   891 non-null    bool    \n 11  deck         203 non-null    category\n 12  embark_town  889 non-null    object  \n 13  alive        891 non-null    object  \n 14  alone        891 non-null    bool    \ndtypes: bool(2), category(2), float64(2), int64(4), object(5)\nmemory usage: 80.7+ KB\n\n\n\n\n1) 결측치 확인 및 처리\n\n# 결측치 확인 \ndf.isnull().sum()\n\nsurvived         0\npclass           0\nsex              0\nage            177\nsibsp            0\nparch            0\nfare             0\nembarked         2\nclass            0\nwho              0\nadult_male       0\ndeck           688\nembark_town      2\nalive            0\nalone            0\ndtype: int64\n\n\n\n# 결측치 제거\nprint(df.dropna(axis=0).shape) # 행 기준, default는 axis = 0 따로 설정 안해도 됨\nprint(df.dropna(axis=1).shape) # 열 기준\n\n(182, 15)\n(891, 11)\n\n\n\n# 결측치 대체\n# 데이터 복사 \ndf2 = df.copy()\ndf2 = pd.DataFrame(df) # df를 데이터 프레임 형태로 변환\n\n\n# 1. 중앙값/평균값 등으로 대체\n\n# 먼저 중앙값을 구합니다\nmedian_age = df2['age'].median()\nprint(median_age)\n\n# 평균으로 대체할 경우 \n# mean_age = df2['age'].mean()\n\n28.0\n\n\n\n# 구한 중앙값으로 결측치를 대체합니다.\ndf2['age'] = df['age'].fillna(median_age)\n\n\n# 결측치가 잘 대체되었는지 확인합니다\ndf2.isnull().sum()\n\nsurvived         0\npclass           0\nsex              0\nage              0\nsibsp            0\nparch            0\nfare             0\nembarked         2\nclass            0\nwho              0\nadult_male       0\ndeck           688\nembark_town      2\nalive            0\nalone            0\ndtype: int64\n\n\n\nprint(df['age'].mean()) # 원본 데이터\nprint(df2['age'].mean()) # 중앙값으로 대체한 데이터 \n\n29.69911764705882\n29.36158249158249\n\n\n\n# 중복값 확인\ndf.drop_duplicates().shape\n\n(784, 15)\n\n\n\n\n2) 이상치 확인 및 처리\n\n\n✔ 상자그림 활용 (이상치: Q1, Q3로부터 1.5*IQR을 초과하는 값)\n\n# 타이타닉 데이터 불러오기\ndf = sns.load_dataset('titanic')\n\n# (참고) 상자그림\nsns.boxplot(df['age'])\n\nC:\\Users\\Hyunsoo Kim\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n  warnings.warn(\n\n\n<AxesSubplot:xlabel='age'>\n\n\n\n\n\n\n# Q1, Q3, IQR 구하기\nQ1 = df['age'].quantile(.25)\nQ3 = df['age'].quantile(.75)\nIQR = Q3 - Q1\nprint(Q1, Q3, IQR)\n\n20.125 38.0 17.875\n\n\n\nupper = Q3 + 1.5*IQR\nlower = Q1 - 1.5*IQR\nprint(upper, lower)\n\n64.8125 -6.6875\n\n\n\n# 문제 : age 변수의 이상치를 제외한 데이터 수는? (상자그림 기준)\ncond1 = (df['age'] <= upper)\ncond2 = (df['age'] >= lower)\nprint(len(df[cond1 & cond2]))\nprint(len(df[cond1]))\nprint(len(df))\n\n703\n703\n891\n\n\n\n# 문제 : age 변수의 이상치를 제외한 데이터셋 확인(상자그림 기준)\ndf_new = df[cond1 & cond2]\ndf_new\n\n\n\n\n\n  \n    \n      \n      survived\n      pclass\n      sex\n      age\n      sibsp\n      parch\n      fare\n      embarked\n      class\n      who\n      adult_male\n      deck\n      embark_town\n      alive\n      alone\n    \n  \n  \n    \n      0\n      0\n      3\n      male\n      22.0\n      1\n      0\n      7.2500\n      S\n      Third\n      man\n      True\n      NaN\n      Southampton\n      no\n      False\n    \n    \n      1\n      1\n      1\n      female\n      38.0\n      1\n      0\n      71.2833\n      C\n      First\n      woman\n      False\n      C\n      Cherbourg\n      yes\n      False\n    \n    \n      2\n      1\n      3\n      female\n      26.0\n      0\n      0\n      7.9250\n      S\n      Third\n      woman\n      False\n      NaN\n      Southampton\n      yes\n      True\n    \n    \n      3\n      1\n      1\n      female\n      35.0\n      1\n      0\n      53.1000\n      S\n      First\n      woman\n      False\n      C\n      Southampton\n      yes\n      False\n    \n    \n      4\n      0\n      3\n      male\n      35.0\n      0\n      0\n      8.0500\n      S\n      Third\n      man\n      True\n      NaN\n      Southampton\n      no\n      True\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      885\n      0\n      3\n      female\n      39.0\n      0\n      5\n      29.1250\n      Q\n      Third\n      woman\n      False\n      NaN\n      Queenstown\n      no\n      False\n    \n    \n      886\n      0\n      2\n      male\n      27.0\n      0\n      0\n      13.0000\n      S\n      Second\n      man\n      True\n      NaN\n      Southampton\n      no\n      True\n    \n    \n      887\n      1\n      1\n      female\n      19.0\n      0\n      0\n      30.0000\n      S\n      First\n      woman\n      False\n      B\n      Southampton\n      yes\n      True\n    \n    \n      889\n      1\n      1\n      male\n      26.0\n      0\n      0\n      30.0000\n      C\n      First\n      man\n      True\n      C\n      Cherbourg\n      yes\n      True\n    \n    \n      890\n      0\n      3\n      male\n      32.0\n      0\n      0\n      7.7500\n      Q\n      Third\n      man\n      True\n      NaN\n      Queenstown\n      no\n      True\n    \n  \n\n703 rows × 15 columns\n\n\n\n\n\n✔ 표준정규분포 활용(이상치 : \\(\\pm\\) 3Z 값을 넘어가는 값)\n\n# 데이터 표준화, Z = (개별값 -  평균) / 표준편차\n\n\nmean_age = df['age'].mean()\nstd_age = df['age'].std()\nprint(mean_age)\nprint(std_age)\n\n29.69911764705882\n14.526497332334044\n\n\n\nznorm = (df['age']-mean_age) / std_age\nznorm\n\n0     -0.530005\n1      0.571430\n2     -0.254646\n3      0.364911\n4      0.364911\n         ...   \n886   -0.185807\n887   -0.736524\n888         NaN\n889   -0.254646\n890    0.158392\nName: age, Length: 891, dtype: float64\n\n\n\n# 문제 : 이상치의 개수는 몇개인가? (: ±3Z 기준)\n\n\ncond1 = (znorm > 3)\nlen(df[cond1])\n\n2\n\n\n\ncond2 = (znorm < -3)\nlen(df[cond2])\n\n0\n\n\n\nprint(len(df[cond1]) + len(df[cond2]))\n\n2\n\n\n\n\n3) 중복값 제거\n\n# 데이터 불러오기\ndf = sns.load_dataset('titanic')\n\n\ndf.shape\n\n(891, 15)\n\n\n\ndf1 = df.copy()\ndf1 = df1.drop_duplicates()\nprint(df1.shape)\n# (주의) 예제에서는 중복값이 있어서 제거했지만,\n# 중복값이 나올 수 있는 상황이변 제거할 필요없음\n\n(784, 15)"
  },
  {
    "objectID": "BigData_Analysis/실기_파이썬기초.html#데이터-scaling데이터-표준화-정규화",
    "href": "BigData_Analysis/실기_파이썬기초.html#데이터-scaling데이터-표준화-정규화",
    "title": "빅분기 실기 - 파이썬 기초",
    "section": "✅ 5. 데이터 scaling(데이터 표준화, 정규화)",
    "text": "✅ 5. 데이터 scaling(데이터 표준화, 정규화)\n\n1) 데이터 표준화(Z-score normalization)\n\ndf = pd.read_csv(\"mtcars.txt\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      Hornet Sportabout\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\nzscaler = StandardScaler() # 변수명은 사용하기 편한 변수명으로 사용\ndf['mpg'] = zscaler.fit_transform(df[['mpg']])\ndf.head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      0.153299\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      0.153299\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      0.456737\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      Hornet 4 Drive\n      0.220730\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      Hornet Sportabout\n      -0.234427\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\n# 확인\nprint(df['mpg'].mean(), df['mpg'].std())\n\n-5.48172618408671e-16 1.016001016001524\n\n\n\n\n2) 데이터 정규화(min-max normalization)\n\ndf = pd.read_csv(\"mtcars.txt\")\ndf.head\n\nfrom sklearn.preprocessing import MinMaxScaler\nmscaler = MinMaxScaler()\ndf['mpg'] = mscaler.fit_transform(df[['mpg']])\ndf.head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      0.451064\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      0.451064\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      0.527660\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      Hornet 4 Drive\n      0.468085\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      Hornet Sportabout\n      0.353191\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\n# 확인\nprint(df['mpg'].min(), df['mpg'].max())\n\n0.0 1.0"
  },
  {
    "objectID": "BigData_Analysis/실기_파이썬기초.html#데이터-합치기",
    "href": "BigData_Analysis/실기_파이썬기초.html#데이터-합치기",
    "title": "빅분기 실기 - 파이썬 기초",
    "section": "6. 데이터 합치기",
    "text": "6. 데이터 합치기\n\n# 행, 열 방향으로 데이터 합치기\ndf = sns.load_dataset('iris')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      sepal_length\n      sepal_width\n      petal_length\n      petal_width\n      species\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      setosa\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      setosa\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      setosa\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      setosa\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      setosa\n    \n  \n\n\n\n\n\n# 데이터 2개로 분리\ndf1 = df.loc[0:30, ] # 0~30행 데이터\ndf2 = df.loc[31:60, ] # 31~60행 데이터 \n\n\ndf1.head()\n\n\n\n\n\n  \n    \n      \n      sepal_length\n      sepal_width\n      petal_length\n      petal_width\n      species\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      setosa\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      setosa\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      setosa\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      setosa\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      setosa\n    \n  \n\n\n\n\n\ndf2.head()\n\n\n\n\n\n  \n    \n      \n      sepal_length\n      sepal_width\n      petal_length\n      petal_width\n      species\n    \n  \n  \n    \n      31\n      5.4\n      3.4\n      1.5\n      0.4\n      setosa\n    \n    \n      32\n      5.2\n      4.1\n      1.5\n      0.1\n      setosa\n    \n    \n      33\n      5.5\n      4.2\n      1.4\n      0.2\n      setosa\n    \n    \n      34\n      4.9\n      3.1\n      1.5\n      0.2\n      setosa\n    \n    \n      35\n      5.0\n      3.2\n      1.2\n      0.2\n      setosa\n    \n  \n\n\n\n\n\ndf_sum = pd.concat([df1, df2], axis=0) # 행 방향으로 결합 (위, 아래)\nprint(df_sum.head())\nprint(df_sum.shape)\n\n   sepal_length  sepal_width  petal_length  petal_width species\n0           5.1          3.5           1.4          0.2  setosa\n1           4.9          3.0           1.4          0.2  setosa\n2           4.7          3.2           1.3          0.2  setosa\n3           4.6          3.1           1.5          0.2  setosa\n4           5.0          3.6           1.4          0.2  setosa\n(61, 5)\n\n\n\n# 데이터 2개로 나누기\ndf1 = df.loc[:, 'sepal_length':'petal_length'] # 1~3열 추출 데이터\ndf2 = df.loc[:, ['petal_width','species']] # 4~5열 추출 데이터\n\n\ndf_sum = pd.concat([df1, df2], axis=1) # 열 방향으로 결합 (좌, 우)\ndf_sum.head()\n\n\n\n\n\n  \n    \n      \n      sepal_length\n      sepal_width\n      petal_length\n      petal_width\n      species\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      setosa\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      setosa\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      setosa\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      setosa\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      setosa"
  },
  {
    "objectID": "BigData_Analysis/실기_파이썬기초.html#날짜시간-데이터-index-다루기",
    "href": "BigData_Analysis/실기_파이썬기초.html#날짜시간-데이터-index-다루기",
    "title": "빅분기 실기 - 파이썬 기초",
    "section": "7. 날짜/시간 데이터, index 다루기",
    "text": "7. 날짜/시간 데이터, index 다루기\n\n1) 날짜 다루기\n\n# 데이터 만들기\ndf = pd.DataFrame({\n    '날짜' : ['20230105','20230105','20230223','20230223',\n            '20230312','20230422','20230511'],\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600] })\ndf\n\n\n\n\n\n  \n    \n      \n      날짜\n      물품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      20230105\n      A\n      5\n      500\n    \n    \n      1\n      20230105\n      B\n      10\n      600\n    \n    \n      2\n      20230223\n      A\n      15\n      500\n    \n    \n      3\n      20230223\n      B\n      15\n      600\n    \n    \n      4\n      20230312\n      A\n      20\n      600\n    \n    \n      5\n      20230422\n      B\n      25\n      700\n    \n    \n      6\n      20230511\n      A\n      40\n      600\n    \n  \n\n\n\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7 entries, 0 to 6\nData columns (total 4 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   날짜      7 non-null      object\n 1   물품      7 non-null      object\n 2   판매수     7 non-null      int64 \n 3   개당수익    7 non-null      int64 \ndtypes: int64(2), object(2)\nmemory usage: 352.0+ bytes\n\n\n\n# 데이터 타입 datetime으로 변경\ndf['날짜'] = pd.to_datetime(df['날짜'])\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7 entries, 0 to 6\nData columns (total 4 columns):\n #   Column  Non-Null Count  Dtype         \n---  ------  --------------  -----         \n 0   날짜      7 non-null      datetime64[ns]\n 1   물품      7 non-null      object        \n 2   판매수     7 non-null      int64         \n 3   개당수익    7 non-null      int64         \ndtypes: datetime64[ns](1), int64(2), object(1)\nmemory usage: 352.0+ bytes\n\n\n\n# 년, 월, 일 변수(열) 추가하기\ndf['year'] = df['날짜'].dt.year\ndf['month'] = df['날짜'].dt.month\ndf['day'] = df['날짜'].dt.day\ndf\n\n\n\n\n\n  \n    \n      \n      날짜\n      물품\n      판매수\n      개당수익\n      year\n      month\n      day\n    \n  \n  \n    \n      0\n      2023-01-05\n      A\n      5\n      500\n      2023\n      1\n      5\n    \n    \n      1\n      2023-01-05\n      B\n      10\n      600\n      2023\n      1\n      5\n    \n    \n      2\n      2023-02-23\n      A\n      15\n      500\n      2023\n      2\n      23\n    \n    \n      3\n      2023-02-23\n      B\n      15\n      600\n      2023\n      2\n      23\n    \n    \n      4\n      2023-03-12\n      A\n      20\n      600\n      2023\n      3\n      12\n    \n    \n      5\n      2023-04-22\n      B\n      25\n      700\n      2023\n      4\n      22\n    \n    \n      6\n      2023-05-11\n      A\n      40\n      600\n      2023\n      5\n      11\n    \n  \n\n\n\n\n\n# 날짜 구간 필터링\ndf[df['날짜'].between('2023-01-01', '2023-01-31')] # 1월 31일은 미포함\n\n\n\n\n\n  \n    \n      \n      날짜\n      물품\n      판매수\n      개당수익\n      year\n      month\n      day\n    \n  \n  \n    \n      0\n      2023-01-05\n      A\n      5\n      500\n      2023\n      1\n      5\n    \n    \n      1\n      2023-01-05\n      B\n      10\n      600\n      2023\n      1\n      5\n    \n  \n\n\n\n\n\n# 날짜를 인덱스로 설정후 loc 함수 사용\n# 데이터 만들기 \ndf = pd.DataFrame({\n    '날짜' : ['20230105','20230105','20230223','20230223',\n            '20230312','20230422','20230511'],\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600] })\n\n# 데이터 타입 datetime으로 변경(필수)\ndf['날짜'] = pd.to_datetime(df['날짜'])\n\ndf = df.set_index('날짜', drop=True) # drop=True(디폴트) or False\ndf.head(3)\n\n\n\n\n\n  \n    \n      \n      물품\n      판매수\n      개당수익\n    \n    \n      날짜\n      \n      \n      \n    \n  \n  \n    \n      2023-01-05\n      A\n      5\n      500\n    \n    \n      2023-01-05\n      B\n      10\n      600\n    \n    \n      2023-02-23\n      A\n      15\n      500\n    \n  \n\n\n\n\n\nprint(df.loc['2023-01-05':'2023-02-23']) # 둘다 기간 포함\nprint(df.loc[ (df.index>='2023-01-05') & (df.index<='2023-02-23') ])\n\n           물품  판매수  개당수익\n날짜                      \n2023-01-05  A    5   500\n2023-01-05  B   10   600\n2023-02-23  A   15   500\n2023-02-23  B   15   600\n           물품  판매수  개당수익\n날짜                      \n2023-01-05  A    5   500\n2023-01-05  B   10   600\n2023-02-23  A   15   500\n2023-02-23  B   15   600\n\n\n\n\n2) 시간 다루기\n\n# 시간 데이터 만들기 \ndf = pd.DataFrame({\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600] })\ntime = pd.date_range('2023-09-24 12:25:00', '2023-09-25 14:45:30', periods=7)\ndf['time'] = time\ndf = df[['time','물품','판매수','개당수익']]\ndf\n\n\n\n\n\n  \n    \n      \n      time\n      물품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      1\n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2\n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      3\n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      4\n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      5\n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      6\n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# index 초기화 (인덱스를 컬럼으로)\n# df = df.reset_index()\n# df\n\n\n# index 새로 지정\ndf = df.set_index('time')\ndf\n\n\n\n\n\n  \n    \n      \n      물품\n      판매수\n      개당수익\n    \n    \n      time\n      \n      \n      \n    \n  \n  \n    \n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# 시간 데이터 다루기(주의: 시간이 index에 위치해야 함)\ndf.between_time(start_time='12:25', end_time='21:00') #시간 시작, 끝 모두 포함\n# include_start=False, include_end=False 옵션을 시작, 끝 시간 제외 가능\n\n\n\n\n\n  \n    \n      \n      물품\n      판매수\n      개당수익\n    \n    \n      time\n      \n      \n      \n    \n  \n  \n    \n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# 날짜를 인덱스로 설정후 loc 함수 사용\nprint(df.loc['2023-09-24 12:25:00':'2023-09-24 21:11:50']) # 둘다 포함\nprint(df.loc[ (df.index>='2023-09-24 12:25:00') & (df.index<='2023-09-24 21:11:50') ])\n\n                    물품  판매수  개당수익\ntime                             \n2023-09-24 12:25:00  A    5   500\n2023-09-24 16:48:25  B   10   600\n2023-09-24 21:11:50  A   15   500\n                    물품  판매수  개당수익\ntime                             \n2023-09-24 12:25:00  A    5   500\n2023-09-24 16:48:25  B   10   600\n2023-09-24 21:11:50  A   15   500"
  },
  {
    "objectID": "BigData_Analysis.html",
    "href": "BigData_Analysis.html",
    "title": "Big Data Analysis Engineer",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nsklearn\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nsklearn\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nscipy\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nscipy\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nscipy\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nscipy\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nsklearn\n\n\n\n\n\n\n\n\n\n\n\nOct 23, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nsklearn\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nsklearn\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nsklearn\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nsklearn\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html",
    "href": "Data_Mining/2022-03-04-numpy.html",
    "title": "Numpy 기본",
    "section": "",
    "text": "numpy 기본 코드 실습\n도구 - 넘파이(NumPy)\n*넘파이(NumPy)는 파이썬의 과학 컴퓨팅을 위한 기본 라이브러리입니다. 넘파이의 핵심은 강력한 N-차원 배열 객체입니다. 또한 선형 대수, 푸리에(Fourier) 변환, 유사 난수 생성과 같은 유용한 함수들도 제공합니다.”"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.zeros",
    "href": "Data_Mining/2022-03-04-numpy.html#np.zeros",
    "title": "Numpy 기본",
    "section": "np.zeros",
    "text": "np.zeros\nzeros 함수는 0으로 채워진 배열을 만듭니다:\n\nnp.zeros(5)\n\narray([0., 0., 0., 0., 0.])\n\n\n2D 배열(즉, 행렬)을 만들려면 원하는 행과 열의 크기를 튜플로 전달합니다. 예를 들어 다음은 \\(3 \\times 4\\) 크기의 행렬입니다:\n\nnp.zeros((3,4))\n\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#용어",
    "href": "Data_Mining/2022-03-04-numpy.html#용어",
    "title": "Numpy 기본",
    "section": "용어",
    "text": "용어\n\n넘파이에서 각 차원을 축(axis) 이라고 합니다\n축의 개수를 랭크(rank) 라고 합니다.\n\n예를 들어, 위의 \\(3 \\times 4\\) 행렬은 랭크 2인 배열입니다(즉 2차원입니다).\n첫 번째 축의 길이는 3이고 두 번째 축의 길이는 4입니다.\n\n배열의 축 길이를 배열의 크기(shape)라고 합니다.\n\n예를 들어, 위 행렬의 크기는 (3, 4)입니다.\n랭크는 크기의 길이와 같습니다.\n\n배열의 사이즈(size)는 전체 원소의 개수입니다. 축의 길이를 모두 곱해서 구할 수 있습니다(가령, \\(3 \\times 4=12\\)).\n\n\na = np.zeros((3,4))\na\n\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])\n\n\n\na.shape\n\n(3, 4)\n\n\n\na.ndim  # len(a.shape)와 같습니다\n\n2\n\n\n\na.size\n\n12"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#n-차원-배열",
    "href": "Data_Mining/2022-03-04-numpy.html#n-차원-배열",
    "title": "Numpy 기본",
    "section": "N-차원 배열",
    "text": "N-차원 배열\n임의의 랭크 수를 가진 N-차원 배열을 만들 수 있습니다. 예를 들어, 다음은 크기가 (2,3,4)인 3D 배열(랭크=3)입니다:\n\nnp.zeros((2,2,5))\n\narray([[[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]],\n\n       [[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#배열-타입",
    "href": "Data_Mining/2022-03-04-numpy.html#배열-타입",
    "title": "Numpy 기본",
    "section": "배열 타입",
    "text": "배열 타입\n넘파이 배열의 타입은 ndarray입니다:\n\ntype(np.zeros((3,4)))\n\nnumpy.ndarray"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.ones",
    "href": "Data_Mining/2022-03-04-numpy.html#np.ones",
    "title": "Numpy 기본",
    "section": "np.ones",
    "text": "np.ones\nndarray를 만들 수 있는 넘파이 함수가 많습니다.\n다음은 1로 채워진 \\(3 \\times 4\\) 크기의 행렬입니다:\n\nnp.ones((3,4))\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.full",
    "href": "Data_Mining/2022-03-04-numpy.html#np.full",
    "title": "Numpy 기본",
    "section": "np.full",
    "text": "np.full\n주어진 값으로 지정된 크기의 배열을 초기화합니다. 다음은 π로 채워진 \\(3 \\times 4\\) 크기의 행렬입니다.\n\nnp.full((3,4), np.pi)\n\narray([[3.14159265, 3.14159265, 3.14159265, 3.14159265],\n       [3.14159265, 3.14159265, 3.14159265, 3.14159265],\n       [3.14159265, 3.14159265, 3.14159265, 3.14159265]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.empty",
    "href": "Data_Mining/2022-03-04-numpy.html#np.empty",
    "title": "Numpy 기본",
    "section": "np.empty",
    "text": "np.empty\n초기화되지 않은 \\(2 \\times 3\\) 크기의 배열을 만듭니다(배열의 내용은 예측이 불가능하며 메모리 상황에 따라 달라집니다):\n\nnp.empty((2,3))\n\narray([[9.6677106e-317, 0.0000000e+000, 0.0000000e+000],\n       [0.0000000e+000, 0.0000000e+000, 0.0000000e+000]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.array",
    "href": "Data_Mining/2022-03-04-numpy.html#np.array",
    "title": "Numpy 기본",
    "section": "np.array",
    "text": "np.array\narray 함수는 파이썬 리스트를 사용하여 ndarray를 초기화합니다:\n\nnp.array([[1,2,3,4], [10, 20, 30, 40]])\n\narray([[ 1,  2,  3,  4],\n       [10, 20, 30, 40]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.arange",
    "href": "Data_Mining/2022-03-04-numpy.html#np.arange",
    "title": "Numpy 기본",
    "section": "np.arange",
    "text": "np.arange\n파이썬의 기본 range 함수와 비슷한 넘파이 arange 함수를 사용하여 ndarray를 만들 수 있습니다:\n\nnp.arange(1, 5)\n\narray([1, 2, 3, 4])\n\n\n부동 소수도 가능합니다:\n\nnp.arange(1.0, 5.0)\n\narray([1., 2., 3., 4.])\n\n\n파이썬의 기본 range 함수처럼 건너 뛰는 정도를 지정할 수 있습니다:\n\nnp.arange(1, 5, 0.5)\n\narray([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5])\n\n\n부동 소수를 사용하면 원소의 개수가 일정하지 않을 수 있습니다. 예를 들면 다음과 같습니다:\n\nprint(np.arange(0, 5/3, 1/3)) # 부동 소수 오차 때문에, 최댓값은 4/3 또는 5/3이 됩니다.\nprint(np.arange(0, 5/3, 0.333333333))\nprint(np.arange(0, 5/3, 0.333333334))\n\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]\n[0.         0.33333333 0.66666667 1.         1.33333334]"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.linspace",
    "href": "Data_Mining/2022-03-04-numpy.html#np.linspace",
    "title": "Numpy 기본",
    "section": "np.linspace",
    "text": "np.linspace\n이런 이유로 부동 소수를 사용할 땐 arange 대신에 linspace 함수를 사용하는 것이 좋습니다. linspace 함수는 지정된 개수만큼 두 값 사이를 나눈 배열을 반환합니다(arange와는 다르게 최댓값이 포함됩니다):\n\nprint(np.linspace(0, 5/3, 6))\n\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.rand와-np.randn",
    "href": "Data_Mining/2022-03-04-numpy.html#np.rand와-np.randn",
    "title": "Numpy 기본",
    "section": "np.rand와 np.randn",
    "text": "np.rand와 np.randn\n넘파이의 random 모듈에는 ndarray를 랜덤한 값으로 초기화할 수 있는 함수들이 많이 있습니다. 예를 들어, 다음은 (균등 분포인) 0과 1사이의 랜덤한 부동 소수로 \\(3 \\times 4\\) 행렬을 초기화합니다:\n\nnp.random.rand(3,4)\n\narray([[0.37892456, 0.17966937, 0.38206837, 0.34922123],\n       [0.80462136, 0.9845914 , 0.9416127 , 0.28305275],\n       [0.21201033, 0.54891417, 0.03781613, 0.4369229 ]])\n\n\n다음은 평균이 0이고 분산이 1인 일변량 정규 분포(가우시안 분포)에서 샘플링한 랜덤한 부동 소수를 담은 \\(3 \\times 4\\) 행렬입니다:\n\nnp.random.randn(3,4)\n\narray([[ 0.83811287, -0.57131751, -0.4381827 ,  1.1485899 ],\n       [ 1.45316084, -0.47259181, -1.23426057, -0.0669813 ],\n       [ 1.01003549,  1.04381736, -0.93060038,  2.39043293]])\n\n\n이 분포의 모양을 알려면 맷플롯립을 사용해 그려보는 것이 좋습니다(더 자세한 것은 맷플롯립 튜토리얼을 참고하세요):\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\n\nplt.hist(np.random.rand(100000), density=True, bins=100, histtype=\"step\", color=\"blue\", label=\"rand\")\nplt.hist(np.random.randn(100000), density=True, bins=100, histtype=\"step\", color=\"red\", label=\"randn\")\nplt.axis([-2.5, 2.5, 0, 1.1])\nplt.legend(loc = \"upper left\")\nplt.title(\"Random distributions\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Density\")\nplt.show()"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.fromfunction",
    "href": "Data_Mining/2022-03-04-numpy.html#np.fromfunction",
    "title": "Numpy 기본",
    "section": "np.fromfunction",
    "text": "np.fromfunction\n함수를 사용하여 ndarray를 초기화할 수도 있습니다:\n\ndef my_function(z, y, x):\n    return x + 10 * y + 100 * z\n\nnp.fromfunction(my_function, (3, 2, 10))\n\narray([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.],\n        [ 10.,  11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.]],\n\n       [[100., 101., 102., 103., 104., 105., 106., 107., 108., 109.],\n        [110., 111., 112., 113., 114., 115., 116., 117., 118., 119.]],\n\n       [[200., 201., 202., 203., 204., 205., 206., 207., 208., 209.],\n        [210., 211., 212., 213., 214., 215., 216., 217., 218., 219.]]])\n\n\n넘파이는 먼저 크기가 (3, 2, 10)인 세 개의 ndarray(차원마다 하나씩)를 만듭니다. 각 배열은 축을 따라 좌표 값과 같은 값을 가집니다. 예를 들어, z 축에 있는 배열의 모든 원소는 z-축의 값과 같습니다:\n[[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n\n [[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]]\n\n [[ 2.  2.  2.  2.  2.  2.  2.  2.  2.  2.]\n  [ 2.  2.  2.  2.  2.  2.  2.  2.  2.  2.]]]\n위의 식 x + 10 * y + 100 * z에서 x, y, z는 사실 ndarray입니다(배열의 산술 연산에 대해서는 아래에서 설명합니다). 중요한 점은 함수 my_function이 원소마다 호출되는 것이 아니고 딱 한 번 호출된다는 점입니다. 그래서 매우 효율적으로 초기화할 수 있습니다."
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#dtype",
    "href": "Data_Mining/2022-03-04-numpy.html#dtype",
    "title": "Numpy 기본",
    "section": "dtype",
    "text": "dtype\n넘파이의 ndarray는 모든 원소가 동일한 타입(보통 숫자)을 가지기 때문에 효율적입니다. dtype 속성으로 쉽게 데이터 타입을 확인할 수 있습니다:\n\nc = np.arange(1, 5)\nprint(c.dtype, c)\n\nint64 [1 2 3 4]\n\n\n\nc = np.arange(1.0, 5.0)\nprint(c.dtype, c)\n\nfloat64 [1. 2. 3. 4.]\n\n\n넘파이가 데이터 타입을 결정하도록 내버려 두는 대신 dtype 매개변수를 사용해서 배열을 만들 때 명시적으로 지정할 수 있습니다:\n\nd = np.arange(1, 5, dtype=np.complex64)\nprint(d.dtype, d)\n\ncomplex64 [1.+0.j 2.+0.j 3.+0.j 4.+0.j]\n\n\n가능한 데이터 타입은 int8, int16, int32, int64, uint8|16|32|64, float16|32|64, complex64|128가 있습니다. 전체 리스트는 온라인 문서를 참고하세요."
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#itemsize",
    "href": "Data_Mining/2022-03-04-numpy.html#itemsize",
    "title": "Numpy 기본",
    "section": "itemsize",
    "text": "itemsize\nitemsize 속성은 각 아이템의 크기(바이트)를 반환합니다:\n\ne = np.arange(1, 5, dtype=np.complex64)\ne.itemsize\n\n8"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#data-버퍼",
    "href": "Data_Mining/2022-03-04-numpy.html#data-버퍼",
    "title": "Numpy 기본",
    "section": "data 버퍼",
    "text": "data 버퍼\n배열의 데이터는 1차원 바이트 버퍼로 메모리에 저장됩니다. data 속성을 사용해 참조할 수 있습니다(사용할 일은 거의 없겠지만요).\n\nf = np.array([[1,2],[1000, 2000]], dtype=np.int32)\nf.data\n\n<memory at 0x7f97929dd790>\n\n\n파이썬 2에서는 f.data가 버퍼이고 파이썬 3에서는 memoryview입니다.\n\nif (hasattr(f.data, \"tobytes\")):\n    data_bytes = f.data.tobytes() # python 3\nelse:\n    data_bytes = memoryview(f.data).tobytes() # python 2\n\ndata_bytes\n\nb'\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\xe8\\x03\\x00\\x00\\xd0\\x07\\x00\\x00'\n\n\n여러 개의 ndarray가 데이터 버퍼를 공유할 수 있습니다. 하나를 수정하면 다른 것도 바뀝니다. 잠시 후에 예를 살펴 보겠습니다."
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#자신을-변경",
    "href": "Data_Mining/2022-03-04-numpy.html#자신을-변경",
    "title": "Numpy 기본",
    "section": "자신을 변경",
    "text": "자신을 변경\nndarray의 shape 속성을 지정하면 간단히 크기를 바꿀 수 있습니다. 배열의 원소 개수는 동일하게 유지됩니다.\n\ng = np.arange(24)\nprint(g)\nprint(\"랭크:\", g.ndim)\n\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n랭크: 1\n\n\n\ng.shape = (6, 4)\nprint(g)\nprint(\"랭크:\", g.ndim)\n\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]\n [12 13 14 15]\n [16 17 18 19]\n [20 21 22 23]]\n랭크: 2\n\n\n\ng.shape = (2, 3, 4)\nprint(g)\nprint(\"랭크:\", g.ndim)\n\n[[[ 0  1  2  3]\n  [ 4  5  6  7]\n  [ 8  9 10 11]]\n\n [[12 13 14 15]\n  [16 17 18 19]\n  [20 21 22 23]]]\n랭크: 3"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#reshape",
    "href": "Data_Mining/2022-03-04-numpy.html#reshape",
    "title": "Numpy 기본",
    "section": "reshape",
    "text": "reshape\nreshape 함수는 동일한 데이터를 가리키는 새로운 ndarray 객체를 반환합니다. 한 배열을 수정하면 다른 것도 함께 바뀝니다.\n\ng2 = g.reshape(4,6)\nprint(g2)\nprint(\"랭크:\", g2.ndim)\n\n[[ 0  1  2  3  4  5]\n [ 6  7  8  9 10 11]\n [12 13 14 15 16 17]\n [18 19 20 21 22 23]]\n랭크: 2\n\n\n행 1, 열 2의 원소를 999로 설정합니다(인덱싱 방식은 아래를 참고하세요).\n\ng2[1, 2] = 999\ng2\n\narray([[  0,   1,   2,   3,   4,   5],\n       [  6,   7, 999,   9,  10,  11],\n       [ 12,  13,  14,  15,  16,  17],\n       [ 18,  19,  20,  21,  22,  23]])\n\n\n이에 상응하는 g의 원소도 수정됩니다.\n\ng\n\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [999,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#ravel",
    "href": "Data_Mining/2022-03-04-numpy.html#ravel",
    "title": "Numpy 기본",
    "section": "ravel",
    "text": "ravel\n마지막으로 ravel 함수는 동일한 데이터를 가리키는 새로운 1차원 ndarray를 반환합니다:\n\ng.ravel()\n\narray([  0,   1,   2,   3,   4,   5,   6,   7, 999,   9,  10,  11,  12,\n        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#규칙-1",
    "href": "Data_Mining/2022-03-04-numpy.html#규칙-1",
    "title": "Numpy 기본",
    "section": "규칙 1",
    "text": "규칙 1\n배열의 랭크가 동일하지 않으면 랭크가 맞을 때까지 랭크가 작은 배열 앞에 1을 추가합니다.\n\nh = np.arange(5).reshape(1, 1, 5)\nh\n\narray([[[0, 1, 2, 3, 4]]])\n\n\n여기에 (1,1,5) 크기의 3D 배열에 (5,) 크기의 1D 배열을 더해 보죠. 브로드캐스팅의 규칙 1이 적용됩니다!\n\nh + [10, 20, 30, 40, 50]  # 다음과 동일합니다: h + [[[10, 20, 30, 40, 50]]]\n\narray([[[10, 21, 32, 43, 54]]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#규칙-2",
    "href": "Data_Mining/2022-03-04-numpy.html#규칙-2",
    "title": "Numpy 기본",
    "section": "규칙 2",
    "text": "규칙 2\n특정 차원이 1인 배열은 그 차원에서 크기가 가장 큰 배열의 크기에 맞춰 동작합니다. 배열의 원소가 차원을 따라 반복됩니다.\n\nk = np.arange(6).reshape(2, 3)\nk\n\narray([[0, 1, 2],\n       [3, 4, 5]])\n\n\n(2,3) 크기의 2D ndarray에 (2,1) 크기의 2D 배열을 더해 보죠. 넘파이는 브로드캐스팅 규칙 2를 적용합니다:\n\nk + [[100], [200]]  # 다음과 같습니다: k + [[100, 100, 100], [200, 200, 200]]\n\narray([[100, 101, 102],\n       [203, 204, 205]])\n\n\n규칙 1과 2를 합치면 다음과 같이 동작합니다:\n\nk + [100, 200, 300]  # 규칙 1 적용: [[100, 200, 300]], 규칙 2 적용: [[100, 200, 300], [100, 200, 300]]\n\narray([[100, 201, 302],\n       [103, 204, 305]])\n\n\n또 매우 간단히 다음 처럼 해도 됩니다:\n\nk + 1000  # 다음과 같습니다: k + [[1000, 1000, 1000], [1000, 1000, 1000]]\n\narray([[1000, 1001, 1002],\n       [1003, 1004, 1005]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#규칙-3",
    "href": "Data_Mining/2022-03-04-numpy.html#규칙-3",
    "title": "Numpy 기본",
    "section": "규칙 3",
    "text": "규칙 3\n규칙 1 & 2을 적용했을 때 모든 배열의 크기가 맞아야 합니다.\n\ntry:\n    k + [33, 44]\nexcept ValueError as e:\n    print(e)\n\noperands could not be broadcast together with shapes (2,3) (2,) \n\n\n브로드캐스팅 규칙은 산술 연산 뿐만 아니라 넘파이 연산에서 많이 사용됩니다. 아래에서 더 보도록 하죠. 브로드캐스팅에 관한 더 자세한 정보는 온라인 문서를 참고하세요."
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#업캐스팅",
    "href": "Data_Mining/2022-03-04-numpy.html#업캐스팅",
    "title": "Numpy 기본",
    "section": "업캐스팅",
    "text": "업캐스팅\ndtype이 다른 배열을 합칠 때 넘파이는 (실제 값에 상관없이) 모든 값을 다룰 수 있는 타입으로 업캐스팅합니다.\n\nk1 = np.arange(0, 5, dtype=np.uint8)\nprint(k1.dtype, k1)\n\nuint8 [0 1 2 3 4]\n\n\n\nk2 = k1 + np.array([5, 6, 7, 8, 9], dtype=np.int8)\nprint(k2.dtype, k2)\n\nint16 [ 5  7  9 11 13]\n\n\n모든 int8과 uint8 값(-128에서 255까지)을 표현하기 위해 int16이 필요합니다. 이 코드에서는 uint8이면 충분하지만 업캐스팅되었습니다.\n\nk3 = k1 + 1.5\nprint(k3.dtype, k3)\n\nfloat64 [1.5 2.5 3.5 4.5 5.5]"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#ndarray-메서드",
    "href": "Data_Mining/2022-03-04-numpy.html#ndarray-메서드",
    "title": "Numpy 기본",
    "section": "ndarray 메서드",
    "text": "ndarray 메서드\n일부 함수는 ndarray 메서드로 제공됩니다. 예를 들면:\n\na = np.array([[-2.5, 3.1, 7], [10, 11, 12]])\nprint(a)\nprint(\"평균 =\", a.mean())\n\n[[-2.5  3.1  7. ]\n [10.  11.  12. ]]\n평균 = 6.766666666666667\n\n\n이 명령은 크기에 상관없이 ndarray에 있는 모든 원소의 평균을 계산합니다.\n다음은 유용한 ndarray 메서드입니다:\n\nfor func in (a.min, a.max, a.sum, a.prod, a.std, a.var):\n    print(func.__name__, \"=\", func())\n\nmin = -2.5\nmax = 12.0\nsum = 40.6\nprod = -71610.0\nstd = 5.084835843520964\nvar = 25.855555555555554\n\n\n이 함수들은 선택적으로 매개변수 axis를 사용합니다. 지정된 축을 따라 원소에 연산을 적용하는데 사용합니다. 예를 들면:\n\nc=np.arange(24).reshape(2,3,4)\nc\n\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])\n\n\n\nc.sum(axis=0)  # 첫 번째 축을 따라 더함, 결과는 3x4 배열\n\narray([[12, 14, 16, 18],\n       [20, 22, 24, 26],\n       [28, 30, 32, 34]])\n\n\n\nc.sum(axis=1)  # 두 번째 축을 따라 더함, 결과는 2x4 배열\n\narray([[12, 15, 18, 21],\n       [48, 51, 54, 57]])\n\n\n여러 축에 대해서 더할 수도 있습니다:\n\nc.sum(axis=(0,2))  # 첫 번째 축과 세 번째 축을 따라 더함, 결과는 (3,) 배열\n\narray([ 60,  92, 124])\n\n\n\n0+1+2+3 + 12+13+14+15, 4+5+6+7 + 16+17+18+19, 8+9+10+11 + 20+21+22+23\n\n(60, 92, 124)"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#일반-함수",
    "href": "Data_Mining/2022-03-04-numpy.html#일반-함수",
    "title": "Numpy 기본",
    "section": "일반 함수",
    "text": "일반 함수\n넘파이는 일반 함수(universal function) 또는 ufunc라고 부르는 원소별 함수를 제공합니다. 예를 들면 square 함수는 원본 ndarray를 복사하여 각 원소를 제곱한 새로운 ndarray 객체를 반환합니다:\n\na = np.array([[-2.5, 3.1, 7], [10, 11, 12]])\nnp.square(a)\n\narray([[  6.25,   9.61,  49.  ],\n       [100.  , 121.  , 144.  ]])\n\n\n다음은 유용한 단항 일반 함수들입니다:\n\nprint(\"원본 ndarray\")\nprint(a)\nfor func in (np.abs, np.sqrt, np.exp, np.log, np.sign, np.ceil, np.modf, np.isnan, np.cos):\n    print(\"\\n\", func.__name__)\n    print(func(a))\n\n원본 ndarray\n[[-2.5  3.1  7. ]\n [10.  11.  12. ]]\n\n absolute\n[[ 2.5  3.1  7. ]\n [10.  11.  12. ]]\n\n sqrt\n[[       nan 1.76068169 2.64575131]\n [3.16227766 3.31662479 3.46410162]]\n\n exp\n[[8.20849986e-02 2.21979513e+01 1.09663316e+03]\n [2.20264658e+04 5.98741417e+04 1.62754791e+05]]\n\n log\n[[       nan 1.13140211 1.94591015]\n [2.30258509 2.39789527 2.48490665]]\n\n sign\n[[-1.  1.  1.]\n [ 1.  1.  1.]]\n\n ceil\n[[-2.  4.  7.]\n [10. 11. 12.]]\n\n modf\n(array([[-0.5,  0.1,  0. ],\n       [ 0. ,  0. ,  0. ]]), array([[-2.,  3.,  7.],\n       [10., 11., 12.]]))\n\n isnan\n[[False False False]\n [False False False]]\n\n cos\n[[-0.80114362 -0.99913515  0.75390225]\n [-0.83907153  0.0044257   0.84385396]]\n\n\nRuntimeWarning: invalid value encountered in sqrt\n  print(func(a))\n<ipython-input-59-d791c8e37e6f>:5: RuntimeWarning: invalid value encountered in log\n  print(func(a))"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#이항-일반-함수",
    "href": "Data_Mining/2022-03-04-numpy.html#이항-일반-함수",
    "title": "Numpy 기본",
    "section": "이항 일반 함수",
    "text": "이항 일반 함수\n두 개의 ndarray에 원소별로 적용되는 이항 함수도 많습니다. 두 배열이 동일한 크기가 아니면 브로드캐스팅 규칙이 적용됩니다:\n\na = np.array([1, -2, 3, 4])\nb = np.array([2, 8, -1, 7])\nnp.add(a, b)  # a + b 와 동일\n\narray([ 3,  6,  2, 11])\n\n\n\nnp.greater(a, b)  # a > b 와 동일\n\narray([False, False,  True, False])\n\n\n\nnp.maximum(a, b)\n\narray([2, 8, 3, 7])\n\n\n\nnp.copysign(a, b)\n\narray([ 1.,  2., -3.,  4.])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#차원-배열",
    "href": "Data_Mining/2022-03-04-numpy.html#차원-배열",
    "title": "Numpy 기본",
    "section": "1차원 배열",
    "text": "1차원 배열\n1차원 넘파이 배열은 보통의 파이썬 배열과 비슷하게 사용할 수 있습니다:\n\na = np.array([1, 5, 3, 19, 13, 7, 3])\na[3]\n\n19\n\n\n\na[2:5]\n\narray([ 3, 19, 13])\n\n\n\na[2:-1]\n\narray([ 3, 19, 13,  7])\n\n\n\na[:2]\n\narray([1, 5])\n\n\n\na[2::2]\n\narray([ 3, 13,  3])\n\n\n\na[::-1]\n\narray([ 3,  7, 13, 19,  3,  5,  1])\n\n\n물론 원소를 수정할 수 있죠:\n\na[3]=999\na\n\narray([  1,   5,   3, 999,  13,   7,   3])\n\n\n슬라이싱을 사용해 ndarray를 수정할 수 있습니다:\n\na[2:5] = [997, 998, 999]\na\n\narray([  1,   5, 997, 998, 999,   7,   3])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#보통의-파이썬-배열과-차이점",
    "href": "Data_Mining/2022-03-04-numpy.html#보통의-파이썬-배열과-차이점",
    "title": "Numpy 기본",
    "section": "보통의 파이썬 배열과 차이점",
    "text": "보통의 파이썬 배열과 차이점\n보통의 파이썬 배열과 대조적으로 ndarray 슬라이싱에 하나의 값을 할당하면 슬라이싱 전체에 복사됩니다. 위에서 언급한 브로드캐스팅 덕택입니다.\n\na[2:5] = -1\na\n\narray([ 1,  5, -1, -1, -1,  7,  3])\n\n\n또한 이런 식으로 ndarray 크기를 늘리거나 줄일 수 없습니다:\n\ntry:\n    a[2:5] = [1,2,3,4,5,6]  # 너무 길어요\nexcept ValueError as e:\n    print(e)\n\ncannot copy sequence with size 6 to array axis with dimension 3\n\n\n원소를 삭제할 수도 없습니다:\n\ntry:\n    del a[2:5]\nexcept ValueError as e:\n    print(e)\n\ncannot delete array elements\n\n\n중요한 점은 ndarray의 슬라이싱은 같은 데이터 버퍼를 바라보는 뷰(view)입니다. 슬라이싱된 객체를 수정하면 실제 원본 ndarray가 수정됩니다!\n\na_slice = a[2:6]\na_slice[1] = 1000\na  # 원본 배열이 수정됩니다!\n\narray([   1,    5,   -1, 1000,   -1,    7,    3])\n\n\n\na[3] = 2000\na_slice  # 비슷하게 원본 배열을 수정하면 슬라이싱 객체에도 반영됩니다!\n\narray([  -1, 2000,   -1,    7])\n\n\n데이터를 복사하려면 copy 메서드를 사용해야 합니다:\n\nanother_slice = a[2:6].copy()\nanother_slice[1] = 3000\na  # 원본 배열이 수정되지 않습니다\n\narray([   1,    5,   -1, 2000,   -1,    7,    3])\n\n\n\na[3] = 4000\nanother_slice  # 마찬가지로 원본 배열을 수정해도 복사된 배열은 바뀌지 않습니다\n\narray([  -1, 3000,   -1,    7])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#다차원-배열",
    "href": "Data_Mining/2022-03-04-numpy.html#다차원-배열",
    "title": "Numpy 기본",
    "section": "다차원 배열",
    "text": "다차원 배열\n다차원 배열은 비슷한 방식으로 각 축을 따라 인덱싱 또는 슬라이싱해서 사용합니다. 콤마로 구분합니다:\n\nb = np.arange(48).reshape(4, 12)\nb\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\n\n\n\nb[1, 2]  # 행 1, 열 2\n\n14\n\n\n\nb[1, :]  # 행 1, 모든 열\n\narray([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\n\n\n\nb[:, 1]  # 모든 행, 열 1\n\narray([ 1, 13, 25, 37])\n\n\n주의: 다음 두 표현에는 미묘한 차이가 있습니다:\n\nb[1, :]\n\narray([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\n\n\n\nb[1:2, :]\n\narray([[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]])\n\n\n첫 번째 표현식은 (12,) 크기인 1D 배열로 행이 하나입니다. 두 번째는 (1, 12) 크기인 2D 배열로 같은 행을 반환합니다."
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#팬시-인덱싱fancy-indexing",
    "href": "Data_Mining/2022-03-04-numpy.html#팬시-인덱싱fancy-indexing",
    "title": "Numpy 기본",
    "section": "팬시 인덱싱(Fancy indexing)",
    "text": "팬시 인덱싱(Fancy indexing)\n관심 대상의 인덱스 리스트를 지정할 수도 있습니다. 이를 팬시 인덱싱이라고 부릅니다.\n\nb[(0,2), 2:5]  # 행 0과 2, 열 2에서 4(5-1)까지\n\narray([[ 2,  3,  4],\n       [26, 27, 28]])\n\n\n\nb[:, (-1, 2, -1)]  # 모든 행, 열 -1 (마지막), 2와 -1 (다시 반대 방향으로)\n\narray([[11,  2, 11],\n       [23, 14, 23],\n       [35, 26, 35],\n       [47, 38, 47]])\n\n\n여러 개의 인덱스 리스트를 지정하면 인덱스에 맞는 값이 포함된 1D ndarray를 반환됩니다.\n\nb[(-1, 2, -1, 2), (5, 9, 1, 9)]  # returns a 1D array with b[-1, 5], b[2, 9], b[-1, 1] and b[2, 9] (again)\n\narray([41, 33, 37, 33])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#고차원",
    "href": "Data_Mining/2022-03-04-numpy.html#고차원",
    "title": "Numpy 기본",
    "section": "고차원",
    "text": "고차원\n고차원에서도 동일한 방식이 적용됩니다. 몇 가지 예를 살펴 보겠습니다:\n\nc = b.reshape(4,2,6)\nc\n\narray([[[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11]],\n\n       [[12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23]],\n\n       [[24, 25, 26, 27, 28, 29],\n        [30, 31, 32, 33, 34, 35]],\n\n       [[36, 37, 38, 39, 40, 41],\n        [42, 43, 44, 45, 46, 47]]])\n\n\n\nc[2, 1, 4]  # 행렬 2, 행 1, 열 4\n\n34\n\n\n\nc[2, :, 3]  # 행렬 2, 모든 행, 열 3\n\narray([27, 33])\n\n\n어떤 축에 대한 인덱스를 지정하지 않으면 이 축의 모든 원소가 반환됩니다:\n\nc[2, 1]  # 행렬 2, 행 1, 모든 열이 반환됩니다. c[2, 1, :]와 동일합니다.\n\narray([30, 31, 32, 33, 34, 35])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#생략-부호-...",
    "href": "Data_Mining/2022-03-04-numpy.html#생략-부호-...",
    "title": "Numpy 기본",
    "section": "생략 부호 (...)",
    "text": "생략 부호 (...)\n생략 부호(...)를 쓰면 모든 지정하지 않은 축의 원소를 포함합니다.\n\nc[2, ...]  #  행렬 2, 모든 행, 모든 열. c[2, :, :]와 동일\n\narray([[24, 25, 26, 27, 28, 29],\n       [30, 31, 32, 33, 34, 35]])\n\n\n\nc[2, 1, ...]  # 행렬 2, 행 1, 모든 열. c[2, 1, :]와 동일\n\narray([30, 31, 32, 33, 34, 35])\n\n\n\nc[2, ..., 3]  # 행렬 2, 모든 행, 열 3. c[2, :, 3]와 동일\n\narray([27, 33])\n\n\n\nc[..., 3]  # 모든 행렬, 모든 행, 열 3. c[:, :, 3]와 동일\n\narray([[ 3,  9],\n       [15, 21],\n       [27, 33],\n       [39, 45]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#불리언-인덱싱",
    "href": "Data_Mining/2022-03-04-numpy.html#불리언-인덱싱",
    "title": "Numpy 기본",
    "section": "불리언 인덱싱",
    "text": "불리언 인덱싱\n불리언 값을 가진 ndarray를 사용해 축의 인덱스를 지정할 수 있습니다.\n\nb = np.arange(48).reshape(4, 12)\nb\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\n\n\n\nrows_on = np.array([True, False, True, False])\nb[rows_on, :]  # 행 0과 2, 모든 열. b[(0, 2), :]와 동일\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]])\n\n\n\ncols_on = np.array([False, True, False] * 4)\nb[:, cols_on]  # 모든 행, 열 1, 4, 7, 10\n\narray([[ 1,  4,  7, 10],\n       [13, 16, 19, 22],\n       [25, 28, 31, 34],\n       [37, 40, 43, 46]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.ix_",
    "href": "Data_Mining/2022-03-04-numpy.html#np.ix_",
    "title": "Numpy 기본",
    "section": "np.ix_",
    "text": "np.ix_\n여러 축에 걸쳐서는 불리언 인덱싱을 사용할 수 없고 ix_ 함수를 사용합니다:\n\nb[np.ix_(rows_on, cols_on)]\n\narray([[ 1,  4,  7, 10],\n       [25, 28, 31, 34]])\n\n\n\nnp.ix_(rows_on, cols_on)\n\n(array([[0],\n        [2]]),\n array([[ 1,  4,  7, 10]]))\n\n\nndarray와 같은 크기의 불리언 배열을 사용하면 해당 위치가 True인 모든 원소를 담은 1D 배열이 반환됩니다. 일반적으로 조건 연산자와 함께 사용합니다:\n\nb[b % 3 == 1]\n\narray([ 1,  4,  7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#vstack",
    "href": "Data_Mining/2022-03-04-numpy.html#vstack",
    "title": "Numpy 기본",
    "section": "vstack",
    "text": "vstack\nvstack 함수를 사용하여 수직으로 쌓아보죠:\n\nq4 = np.vstack((q1, q2, q3))\nq4\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.]])\n\n\n\nq4.shape\n\n(10, 4)\n\n\nq1, q2, q3가 모두 같은 크기이므로 가능합니다(수직으로 쌓기 때문에 수직 축은 크기가 달라도 됩니다)."
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#hstack",
    "href": "Data_Mining/2022-03-04-numpy.html#hstack",
    "title": "Numpy 기본",
    "section": "hstack",
    "text": "hstack\nhstack을 사용해 수평으로도 쌓을 수 있습니다:\n\nq5 = np.hstack((q1, q3))\nq5\n\narray([[1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.]])\n\n\n\nq5.shape\n\n(3, 8)\n\n\nq1과 q3가 모두 3개의 행을 가지고 있기 때문에 가능합니다. q2는 4개의 행을 가지고 있기 때문에 q1, q3와 수평으로 쌓을 수 없습니다:\n\ntry:\n    q5 = np.hstack((q1, q2, q3))\nexcept ValueError as e:\n    print(e)\n\nall the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 3 and the array at index 1 has size 4"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#concatenate",
    "href": "Data_Mining/2022-03-04-numpy.html#concatenate",
    "title": "Numpy 기본",
    "section": "concatenate",
    "text": "concatenate\nconcatenate 함수는 지정한 축으로도 배열을 쌓습니다.\n\nq7 = np.concatenate((q1, q2, q3), axis=0)  # vstack과 동일\nq7\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.]])\n\n\n\nq7.shape\n\n(10, 4)\n\n\n예상했겠지만 hstack은 axis=1으로 concatenate를 호출하는 것과 같습니다."
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#stack",
    "href": "Data_Mining/2022-03-04-numpy.html#stack",
    "title": "Numpy 기본",
    "section": "stack",
    "text": "stack\nstack 함수는 새로운 축을 따라 배열을 쌓습니다. 모든 배열은 같은 크기를 가져야 합니다.\n\nq8 = np.stack((q1, q3))\nq8\n\narray([[[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]],\n\n       [[3., 3., 3., 3.],\n        [3., 3., 3., 3.],\n        [3., 3., 3., 3.]]])\n\n\n\nq8.shape\n\n(2, 3, 4)"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#행렬-전치",
    "href": "Data_Mining/2022-03-04-numpy.html#행렬-전치",
    "title": "Numpy 기본",
    "section": "행렬 전치",
    "text": "행렬 전치\nT 속성은 랭크가 2보다 크거나 같을 때 transpose()를 호출하는 것과 같습니다:\n\nm1 = np.arange(10).reshape(2,5)\nm1\n\narray([[0, 1, 2, 3, 4],\n       [5, 6, 7, 8, 9]])\n\n\n\nm1.T\n\narray([[0, 5],\n       [1, 6],\n       [2, 7],\n       [3, 8],\n       [4, 9]])\n\n\nT 속성은 랭크가 0이거나 1인 배열에는 아무런 영향을 미치지 않습니다:\n\nm2 = np.arange(5)\nm2\n\narray([0, 1, 2, 3, 4])\n\n\n\nm2.T\n\narray([0, 1, 2, 3, 4])\n\n\n먼저 1D 배열을 하나의 행이 있는 행렬(2D)로 바꾼다음 전치를 수행할 수 있습니다:\n\nm2r = m2.reshape(1,5)\nm2r\n\narray([[0, 1, 2, 3, 4]])\n\n\n\nm2r.T\n\narray([[0],\n       [1],\n       [2],\n       [3],\n       [4]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#행렬-곱셈",
    "href": "Data_Mining/2022-03-04-numpy.html#행렬-곱셈",
    "title": "Numpy 기본",
    "section": "행렬 곱셈",
    "text": "행렬 곱셈\n두 개의 행렬을 만들어 dot 메서드로 행렬 곱셈을 실행해 보죠.\n\nn1 = np.arange(10).reshape(2, 5)\nn1\n\narray([[0, 1, 2, 3, 4],\n       [5, 6, 7, 8, 9]])\n\n\n\nn2 = np.arange(15).reshape(5,3)\nn2\n\narray([[ 0,  1,  2],\n       [ 3,  4,  5],\n       [ 6,  7,  8],\n       [ 9, 10, 11],\n       [12, 13, 14]])\n\n\n\nn1.dot(n2)\n\narray([[ 90, 100, 110],\n       [240, 275, 310]])\n\n\n주의: 앞서 언급한 것처럼 n1*n2는 행렬 곱셈이 아니라 원소별 곱셈(또는 아다마르 곱이라 부릅니다)입니다."
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#역행렬과-유사-역행렬",
    "href": "Data_Mining/2022-03-04-numpy.html#역행렬과-유사-역행렬",
    "title": "Numpy 기본",
    "section": "역행렬과 유사 역행렬",
    "text": "역행렬과 유사 역행렬\nnumpy.linalg 모듈 안에 많은 선형 대수 함수들이 있습니다. 특히 inv 함수는 정방 행렬의 역행렬을 계산합니다:\n\nimport numpy.linalg as linalg\n\nm3 = np.array([[1,2,3],[5,7,11],[21,29,31]])\nm3\n\narray([[ 1,  2,  3],\n       [ 5,  7, 11],\n       [21, 29, 31]])\n\n\n\nlinalg.inv(m3)\n\narray([[-2.31818182,  0.56818182,  0.02272727],\n       [ 1.72727273, -0.72727273,  0.09090909],\n       [-0.04545455,  0.29545455, -0.06818182]])\n\n\npinv 함수를 사용하여 유사 역행렬을 계산할 수도 있습니다:\n\nlinalg.pinv(m3)\n\narray([[-2.31818182,  0.56818182,  0.02272727],\n       [ 1.72727273, -0.72727273,  0.09090909],\n       [-0.04545455,  0.29545455, -0.06818182]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#단위-행렬",
    "href": "Data_Mining/2022-03-04-numpy.html#단위-행렬",
    "title": "Numpy 기본",
    "section": "단위 행렬",
    "text": "단위 행렬\n행렬과 그 행렬의 역행렬을 곱하면 단위 행렬이 됩니다(작은 소숫점 오차가 있습니다):\n\nm3.dot(linalg.inv(m3))\n\narray([[ 1.00000000e+00, -1.66533454e-16,  0.00000000e+00],\n       [ 6.31439345e-16,  1.00000000e+00, -1.38777878e-16],\n       [ 5.21110932e-15, -2.38697950e-15,  1.00000000e+00]])\n\n\neye 함수는 NxN 크기의 단위 행렬을 만듭니다:\n\nnp.eye(3)\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#qr-분해",
    "href": "Data_Mining/2022-03-04-numpy.html#qr-분해",
    "title": "Numpy 기본",
    "section": "QR 분해",
    "text": "QR 분해\nqr 함수는 행렬을 QR 분해합니다:\n\nq, r = linalg.qr(m3)\nq\n\narray([[-0.04627448,  0.98786672,  0.14824986],\n       [-0.23137241,  0.13377362, -0.96362411],\n       [-0.97176411, -0.07889213,  0.22237479]])\n\n\n\nr\n\narray([[-21.61018278, -29.89331494, -32.80860727],\n       [  0.        ,   0.62427688,   1.9894538 ],\n       [  0.        ,   0.        ,  -3.26149699]])\n\n\n\nq.dot(r)  # q.r는 m3와 같습니다\n\narray([[ 1.,  2.,  3.],\n       [ 5.,  7., 11.],\n       [21., 29., 31.]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#행렬식",
    "href": "Data_Mining/2022-03-04-numpy.html#행렬식",
    "title": "Numpy 기본",
    "section": "행렬식",
    "text": "행렬식\ndet 함수는 행렬식을 계산합니다:\n\nlinalg.det(m3)  # 행렬식 계산\n\n43.99999999999997"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#고윳값과-고유벡터",
    "href": "Data_Mining/2022-03-04-numpy.html#고윳값과-고유벡터",
    "title": "Numpy 기본",
    "section": "고윳값과 고유벡터",
    "text": "고윳값과 고유벡터\neig 함수는 정방 행렬의 고윳값과 고유벡터를 계산합니다:\n\neigenvalues, eigenvectors = linalg.eig(m3)\neigenvalues # λ\n\narray([42.26600592, -0.35798416, -2.90802176])\n\n\n\neigenvectors # v\n\narray([[-0.08381182, -0.76283526, -0.18913107],\n       [-0.3075286 ,  0.64133975, -0.6853186 ],\n       [-0.94784057, -0.08225377,  0.70325518]])\n\n\n\nm3.dot(eigenvectors) - eigenvalues * eigenvectors  # m3.v - λ*v = 0\n\narray([[ 8.88178420e-15,  2.22044605e-16, -3.10862447e-15],\n       [ 3.55271368e-15,  2.02615702e-15, -1.11022302e-15],\n       [ 3.55271368e-14,  3.33413852e-15, -8.43769499e-15]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#특잇값-분해",
    "href": "Data_Mining/2022-03-04-numpy.html#특잇값-분해",
    "title": "Numpy 기본",
    "section": "특잇값 분해",
    "text": "특잇값 분해\nsvd 함수는 행렬을 입력으로 받아 그 행렬의 특잇값 분해를 반환합니다:\n\nm4 = np.array([[1,0,0,0,2], [0,0,3,0,0], [0,0,0,0,0], [0,2,0,0,0]])\nm4\n\narray([[1, 0, 0, 0, 2],\n       [0, 0, 3, 0, 0],\n       [0, 0, 0, 0, 0],\n       [0, 2, 0, 0, 0]])\n\n\n\nU, S_diag, V = linalg.svd(m4)\nU\n\narray([[ 0.,  1.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0., -1.],\n       [ 0.,  0.,  1.,  0.]])\n\n\n\nS_diag\n\narray([3.        , 2.23606798, 2.        , 0.        ])\n\n\nsvd 함수는 Σ의 대각 원소 값만 반환합니다. 전체 Σ 행렬은 다음과 같이 만듭니다:\n\nS = np.zeros((4, 5))\nS[np.diag_indices(4)] = S_diag\nS  # Σ\n\narray([[3.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 2.23606798, 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 2.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ]])\n\n\n\nV\n\narray([[-0.        ,  0.        ,  1.        , -0.        ,  0.        ],\n       [ 0.4472136 ,  0.        ,  0.        ,  0.        ,  0.89442719],\n       [-0.        ,  1.        ,  0.        , -0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ],\n       [-0.89442719,  0.        ,  0.        ,  0.        ,  0.4472136 ]])\n\n\n\nU.dot(S).dot(V) # U.Σ.V == m4\n\narray([[1., 0., 0., 0., 2.],\n       [0., 0., 3., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 2., 0., 0., 0.]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#대각원소와-대각합",
    "href": "Data_Mining/2022-03-04-numpy.html#대각원소와-대각합",
    "title": "Numpy 기본",
    "section": "대각원소와 대각합",
    "text": "대각원소와 대각합\n\nnp.diag(m3)  # m3의 대각 원소입니다(왼쪽 위에서 오른쪽 아래)\n\narray([ 1,  7, 31])\n\n\n\nnp.trace(m3)  # np.diag(m3).sum()와 같습니다\n\n39"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#선형-방정식-풀기",
    "href": "Data_Mining/2022-03-04-numpy.html#선형-방정식-풀기",
    "title": "Numpy 기본",
    "section": "선형 방정식 풀기",
    "text": "선형 방정식 풀기\nsolve 함수는 다음과 같은 선형 방정식을 풉니다:\n\n\\(2x + 6y = 6\\)\n\\(5x + 3y = -9\\)\n\n\ncoeffs  = np.array([[2, 6], [5, 3]])\ndepvars = np.array([6, -9])\nsolution = linalg.solve(coeffs, depvars)\nsolution\n\narray([-3.,  2.])\n\n\nsolution을 확인해 보죠:\n\ncoeffs.dot(solution), depvars  # 네 같네요\n\n(array([ 6., -9.]), array([ 6, -9]))\n\n\n좋습니다! 다른 방식으로도 solution을 확인해 보죠:\n\nnp.allclose(coeffs.dot(solution), depvars)\n\nTrue"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#바이너리-.npy-포맷",
    "href": "Data_Mining/2022-03-04-numpy.html#바이너리-.npy-포맷",
    "title": "Numpy 기본",
    "section": "바이너리 .npy 포맷",
    "text": "바이너리 .npy 포맷\n랜덤 배열을 만들고 저장해 보죠.\n\na = np.random.rand(2,3)\na\n\narray([[0.5435938 , 0.92886307, 0.01535158],\n       [0.4157283 , 0.9102127 , 0.55129708]])\n\n\n\nnp.save(\"my_array\", a)\n\n끝입니다! 파일 이름의 확장자를 지정하지 않았기 때문에 넘파이는 자동으로 .npy를 붙입니다. 파일 내용을 확인해 보겠습니다:\n\nwith open(\"my_array.npy\", \"rb\") as f:\n    content = f.read()\n\ncontent\n\nb\"\\x93NUMPY\\x01\\x00v\\x00{'descr': '<f8', 'fortran_order': False, 'shape': (2, 3), }                                                          \\nY\\xc1\\xfc\\xd0\\x1ee\\xe1?\\xde{3\\t?\\xb9\\xed?\\x80V\\x08\\xef\\xa5p\\x8f?\\x96I}\\xe0J\\x9b\\xda?\\xe0U\\xfaav \\xed?\\xd8\\xe50\\xc59\\xa4\\xe1?\"\n\n\n이 파일을 넘파이 배열로 로드하려면 load 함수를 사용합니다:\n\na_loaded = np.load(\"my_array.npy\")\na_loaded\n\narray([[0.5435938 , 0.92886307, 0.01535158],\n       [0.4157283 , 0.9102127 , 0.55129708]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#텍스트-포맷",
    "href": "Data_Mining/2022-03-04-numpy.html#텍스트-포맷",
    "title": "Numpy 기본",
    "section": "텍스트 포맷",
    "text": "텍스트 포맷\n배열을 텍스트 포맷으로 저장해 보죠:\n\nnp.savetxt(\"my_array.csv\", a)\n\n파일 내용을 확인해 보겠습니다:\n\nwith open(\"my_array.csv\", \"rt\") as f:\n    print(f.read())\n\n5.435937959464737235e-01 9.288630656918674955e-01 1.535157809943688001e-02\n4.157283012656532994e-01 9.102126992826775620e-01 5.512970782648904944e-01\n\n\n\n이 파일은 탭으로 구분된 CSV 파일입니다. 다른 구분자를 지정할 수도 있습니다:\n\nnp.savetxt(\"my_array.csv\", a, delimiter=\",\")\n\n이 파일을 로드하려면 loadtxt 함수를 사용합니다:\n\na_loaded = np.loadtxt(\"my_array.csv\", delimiter=\",\")\na_loaded\n\narray([[0.5435938 , 0.92886307, 0.01535158],\n       [0.4157283 , 0.9102127 , 0.55129708]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#압축된-.npz-포맷",
    "href": "Data_Mining/2022-03-04-numpy.html#압축된-.npz-포맷",
    "title": "Numpy 기본",
    "section": "압축된 .npz 포맷",
    "text": "압축된 .npz 포맷\n여러 개의 배열을 압축된 한 파일로 저장하는 것도 가능합니다:\n\nb = np.arange(24, dtype=np.uint8).reshape(2, 3, 4)\nb\n\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]], dtype=uint8)\n\n\n\nnp.savez(\"my_arrays\", my_a=a, my_b=b)\n\n파일 내용을 확인해 보죠. .npz 파일 확장자가 자동으로 추가되었습니다.\n\nwith open(\"my_arrays.npz\", \"rb\") as f:\n    content = f.read()\n\nrepr(content)[:180] + \"[...]\"\n\n'b\"PK\\\\x03\\\\x04\\\\x14\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00!\\\\x00\\\\x063\\\\xcf\\\\xb9\\\\xb0\\\\x00\\\\x00\\\\x00\\\\xb0\\\\x00\\\\x00\\\\x00\\\\x08\\\\x00\\\\x14\\\\x00my_a.npy\\\\x01\\\\x00\\\\x10\\\\x00\\\\xb0\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\xb0\\\\x00\\\\x00\\\\x[...]'\n\n\n다음과 같이 이 파일을 로드할 수 있습니다:\n\nmy_arrays = np.load(\"my_arrays.npz\")\nmy_arrays\n\n<numpy.lib.npyio.NpzFile at 0x7f9791c73d60>\n\n\n게으른 로딩을 수행하는 딕셔너리와 유사한 객체입니다:\n\nmy_arrays.keys()\n\nKeysView(<numpy.lib.npyio.NpzFile object at 0x7f9791c73d60>)\n\n\n\nmy_arrays[\"my_a\"]\n\narray([[0.5435938 , 0.92886307, 0.01535158],\n       [0.4157283 , 0.9102127 , 0.55129708]])"
  },
  {
    "objectID": "Data_Mining/2022-03-18-Python Language Basics.html",
    "href": "Data_Mining/2022-03-18-Python Language Basics.html",
    "title": "Python 기초",
    "section": "",
    "text": "Python language bascis\n\nlanguage semantics\n\nIdentation, not braces // 들여쓰기\nfor x in array: if x < pivot : less.append(x) else: greater.append(x)\na=5; b=6; c=7\n\na=5; b=6; c=7\n\n\nc\n\n7\n\n\n\n\nEverything is an object // 오브젝트를 기본 할당하고 동작함\n\n\nComments\nresult =[] for line in file_handle: # keep the empty line for now # if len(line) == 0: # continue result.append(line.replace(‘foo’,‘bar’)) print(“Reach this line”) #simple statis report\n\n\nFunction and object method calls\nresult = f(x,y,z) g()\nobj.some_method(x,y,z) result = f(a,b,c,d=5,e=‘foo’)\n\n\nVariables and argument passing\n\na=[1,2,3]\n\n\nb=a\n\n\na.append(4) #뒤에 붙여줌 \nb\n\n[1, 2, 3, 4]\n\n\n\na=5\ntype(a) #int - integer - 수치형 \n\nint\n\n\n\na='foo'\ntype(a)\n\nstr\n\n\n\n'5'+5\n\nTypeError: can only concatenate str (not \"int\") to str\n\n\n\na=4.5\nb=2\n# String formatting, to be visted later\nprint('a is {0}, b is {1}'.format(type(a),type(b)))\na/b\n\na is <class 'float'>, b is <class 'int'>\n\n\n2.25\n\n\n\na=5\nisinstance(a,int)\n\nTrue\n\n\n\na=5; b=4.5\nisinstance(a,(int,float))\nisinstance(b,(int,float))\n\nTrue\n\n\n\na='foo' #Tab 키를 누르면 다양한 옵션이 나오면서 대문자화 카운트등 여러옵션 존재\n\n\na.<Press Tab>\n\nSyntaxError: invalid syntax (3084391244.py, line 1)\n\n\n\na.upper()\n\n'FOO'\n\n\n\ngetattr(a,'split') #object에 속한 속성을 가지고 온다 \n\n<function str.split(sep=None, maxsplit=-1)>\n\n\n\n\n\nDuck typing\n\ndef isiterable(obj):\n    try:\n        iter(obj)\n        return True\n    except TypeError: # not literable\n        return False\n\n\nisiterable('a string') \n\nTrue\n\n\n\nisiterable([1,2,3])\n\nTrue\n\n\n\nisiterable(5)\n\nFalse\n\n\nif not isinstance(x,list) and isiterable(x): x=list(x)\n\n\nImports\nIn Python a module is simply a file with the .py extension containing Python code. Suppose that we had the following module:\nIf you wanted to access the variables and functions defined in some_module.py, from another file in the same directory we could do:\n\nimport some_module #같은 디렉토리의 .py파일을 불러와서 사용가능 \n\nresult = some_module.f(5)\nresult\n\n7\n\n\n\npi = some_module.PI\npi\n\n3.14159\n\n\nOr equivalently:\n\nfrom some_module import f,g,PI\nresult = g(5,PI)\nresult\n\n8.14159\n\n\nBy using the as keyword you can give imports different variable names:\n\nimport some_module as sm\nfrom some_module import PI as pi, g as gf\n\nr1 = sm.f(pi)\nr2 = gf(6,pi)\n\n\nr1\n\n5.14159\n\n\n\nr2\n\n9.14159\n\n\n\n\nBinary operators and comparisons\nMost of the binary math operations and comparisons are as you might expect:\n\n5-7\n12+21.5\n5 <= 2\n\nFalse\n\n\n\na = [1,2,3]\nb=a\nc=list()\na is b\na is not c \n\nTrue\n\n\n\na == c\n\nFalse\n\n\n\na = None\na is None\n\nTrue\n\n\n\nMutable and immutable objects\nMost object in Python such as list, dict, NumPy arrays, and most user-defined types(classes), are mutable.\nThis means that the object or values that they contain can be modified\n\na_list = ['foo',2,[4,5]]\na_list[2] = (3,4)\na_list\n\n['foo', 2, (3, 4)]\n\n\nOthers, like strings and tuples, are immutable:\n\na_tuple = (3,5,(4,5))\na_tuple[1] = 'four' #tuple은 변경이 불가능 하다 / list는 가능  \n\nTypeError: 'tuple' object does not support item assignment\n\n\n\n\n\nScalar Types\n\nNumeric types\nThe primary Python types for numbers are int and float. An int can store arbitrarily large numbers:\n\nival = 17239871\nival ** 6\n\n26254519291092456596965462913230729701102721\n\n\nFloating-point numbers are represented with the Python float type. Under the hood each one is a double-precision(64-bit) value. They can also be expressed with scientific notation:\n\nfval = 7.2343\nfval2 = 6.78e-5\n\n\n3/2\n\n1.5\n\n\n\ntype(3/2)\n\nfloat\n\n\n\n3//2 #몫\n\n1\n\n\n\ntype(3//2)\n\nint\n\n\n\n\n\nStrings\nMany people use Python for its powerful and flexible built-in string processing capabilities.\nYou can write string literals using either single quotes or double quotes:\n\na = 'one way of writing a string'\nb = 'another way'\n\nFor multiline strings with line breaks, you can use triple qutoes, either ’’’ or ““”\n\nc = \"\"\" \nThis is a longer string that\nspans multiple lines\n\"\"\"\n\n\nc\n\n' \\nThis is a longer string that\\nspans multiple lines\\n'\n\n\nIt may surprise you that this string c actually contains for lines of text;\nthe line breaks after ““” and after lines are include in the string.\nWe can count the new line characters with the count method on c:\n\nc.count('\\n')\n\n3\n\n\nPython strings are immutable; you cannot modify a string:\n\na = 'this is a string'\na[10] = 'f' #변형불가 \n\nTypeError: 'str' object does not support item assignment\n\n\n\nb = a.replace('string','longer string')\nb\n\n'this is a longer string'\n\n\n\na #변형은 안되나 대체는 가능\n\n'this is a string'\n\n\nMany Python object can be converted to a string using the str function\n\na = 5.6\ns = str(a)\nprint(s)\n\n5.6\n\n\nStrings are a sequnce of Unicode characters and therefore can be treated like other sequences, such as lists and tuples(which we will explore in more detail in the next chapter):\n\ns='python'\nlist(s)\n\n['p', 'y', 't', 'h', 'o', 'n']\n\n\n\ns[:3]\n\n'pyt'\n\n\nThe syntax s[:3] is called slicing and is implemented for many kinds of Python sequences. This will be explained in moire detail later on, as it it used extensively in the book.\nThe backslash character  is an escape character, meaning that it is used to specify special characters like newline or Unicode characters. To write a string literal with backslashes, you need to escape them:\n\nprint('12\\n34') #\\n is Enter \n\n12\n34\n\n\n\ns = '12\\\\34' #백슬래쉬를 문자열로 바꾼다 \nprint(s)\n\n12\\34\n\n\nAdding two strings together concatenates them and produces a new string:\n\na='this is the first half '\nb='and this is the second half'\na+b\n\n'this is the first half and this is the second half'\n\n\nString templating or formatting is another important topic.\nThe number of ways tod do so has expanded with the advent of Python 3,\nand here I will briefly describe the mechanics of one of the main interfaces.\nString objects have a format method that can be used to substitute formatted arguments into the string, producting a new string:\n\ntemplate = '{0:.2f} {1:s} are worth US${2:d}'\ntemplate\n\n'{0:.2f} {1:s} are worth US${2:d}'\n\n\n\n{0:.2f} means to format the first argument as a floating-point number with two decimal places.\n{1:s} means to format the second argument as a string.\n{2:d} means to format the third argument as an exact integer\n\n\ntemplate.format(4.560, 'Argentine Pesos',1)\n\n'4.56 Argentine Pesos are worth US$1'\n\n\n\ntemplate.format(1263.23,'won',1)\n\n'1263.23 won are worth US$1'\n\n\n\n\nBooleans\n\nThe two boolean value in python are written as True as False.\n\n\nComprasions and other conditional expressions evaluate to either True or False.\n\n\nBoolean values are combined with the and or keywords:\n\nTrue and True\n\nTrue\n\n\n\nFalse or True\n\nTrue\n\n\n\n\nType casting\nThe str, bool, int, and float types are also functions that can be used to cast values\n\ns='3.14159'\nfval=float(s)\ntype(fval)\n\nfloat\n\n\n\nint(fval)\n\n3\n\n\n\nbool(fval)\n\nTrue\n\n\n\nbool(0)\n\nFalse\n\n\n\n\n\nNone\nNone is the Python null value type. If a function does not explicitly return a value, it implicitly returns None:\n\na = None\na is None\n\nTrue\n\n\n\nb = 5\nb is not None\n\nTrue\n\n\nNone is also a common default vlaue for function arguments:\n\ndef add_and_maybe_multiply(a,b,c=None):\n    result = a+b\n\n    if c is not None:\n        result = result * c\n    \n    return result\n\n\nadd_and_maybe_multiply(5,3)\n\n8\n\n\n\nadd_and_maybe_multiply(5,3,10)\n\n80\n\n\nWhile a technical point, it’s worth bearing in mind that None is not only a reserved keyword but also a unique instance of NoneType:\n\ntype(None)\n\nNoneType\n\n\n\n\nDates and times\nThe built-in Python datetime module provides datetime, date, and time types.\nThe datetime type, as you may imagine, combines the information stored in date and time and is the most commonly used:\n\nfrom datetime import datetime, date, time\ndt = datetime(2011,10,29,20,30,21) #year,month,day,hour,minute,second\ndt\n\ndatetime.datetime(2011, 10, 29, 20, 30, 21)\n\n\n\ndt.day\n\n29\n\n\n\ndt.minute\n\n30\n\n\nGiven a datetime instance, you can extract the equivalent date and time objects by calling methods on the datetime of the same name:\n\ndt.date()\n\ndatetime.date(2011, 10, 29)\n\n\n\ndt.time()\n\ndatetime.time(20, 30, 21)\n\n\nThe strfime method formats a datetime as a string:\n\ndt.strftime('%m/%d/%Y %H:%M')\n\n'10/29/2011 20:30'\n\n\n\ndt.strftime('%Y/%m/%d %H:%M')\n\n'2011/10/29 20:30'\n\n\nString can be converted (parsed) into datetime objects with the strptime function:\n\ndatetime.strptime('20091031',\"%Y%m%d\")\n\ndatetime.datetime(2009, 10, 31, 0, 0)\n\n\nWhen you are aggregating or otherwise grouping time series data, it will occasionally be useful to replace time fields of a series of datetimes-for example,replacing the minute and second fields with zero:\n\ndt.replace(minute=0,second=0)\n\ndatetime.datetime(2011, 10, 29, 20, 0)\n\n\n\ndt2 = datetime(2011,11,15,22,30) \ndelta =dt2 - dt #dt = datetime(2011,10,29,20,30,21)\ndelta\n\ndatetime.timedelta(days=17, seconds=7179)\n\n\n\ntype(delta)\n\ndatetime.timedelta\n\n\n\ndt\ndt + delta\n\ndatetime.datetime(2011, 11, 15, 22, 30)\n\n\n\n\n\nControl Flow\nPython has several built-in keywords for conditonal logic, loops, and other standard control flow concepts found in other porgramming languages.\n\nif, elif, and else\nThe if statement is one of the most well-known control flow statement types.\nIt checks a conditon that, if True, evaluates the code in the block that follows:\n\nx = -5\nif x < 0:\n    print('It is negative')\n\nIt is negative\n\n\nAn if statement can be optionally followed by one or mor elif blocks and a catch all else block if all of the conditions are False\n\nx = -5\n\nif x < 0 :\n    print('It is negative')\nelif x == 0 :\n    print('Equal to zero')\nelif 0 < x < 5 :\n    print('Postive but smaller than 5')\nelse :\n    print('Postive and larger than or equal to 5')\n\nIt is negative\n\n\nIf any of the conditions is True, no futher elif or else blocks will be reached.\nWith a compound condition using and or or, conditions are evaluated left to right and will short-circuit:\n\na = 5; b = 7\nc = 8; d = 4\nif a < b or c > d :\n    print('Made it')\n\nMade it\n\n\nIn this examplme, the comparison c > d never get evaluated because the first compar- ison was True. It is also possible to chain comparisons:\n\n4 > 3 > 2 > 1\n\nTrue\n\n\n\n3 > 5 or 2 > 1\n\nTrue\n\n\n\n3>5>2>1\n\nFalse\n\n\n\n\nfor loops\nfor loops are fir iterating over a collection (like a list or tuple) or an iterater. They standard syntax for a for loop is:\nfor value in collection: # do something with value\nYou can advance a for loop to the next iteration, skipping the remainder of the block, using the continue keyword.\nConsider this code, which sums up integers in a list and skips None values\n\nsequnce = [1,2,None,4,None,5]\ntotal = 0\n\nfor value in sequnce:\n    total += value\n\nTypeError: unsupported operand type(s) for +=: 'int' and 'NoneType'\n\n\n\nsequnce = [1,2,None,4,None,5]\ntotal = 0\n\nfor value in sequnce:\n    if value is None:\n        continue\n    total += value\n\n\ntotal\n\n12\n\n\nA for loop cna be exited altogether with the break keyword. This code sums ele- ments of the list until a 5 is reached:\n\nsequnce = [1,2,0,4,6,5,2,1]\ntotal_until_5 = 0\n\nfor value in sequnce:\n    if value == 5:\n        break\n    total_until_5 += value\n\n\ntotal_until_5 #값이 5가 되면 멈추는 조건으로 1+2+0+4+6까지 계산후 5가 오기에 for문이 종료된다 \n\n13\n\n\nThe break keyword only terminates the innermost for loop; any outer for loops will continue to run:\n\nfor i in range(4):\n    for j in range(4):\n        if j>i:\n            break\n        print((i,j))\n\n(0, 0)\n(1, 0)\n(1, 1)\n(2, 0)\n(2, 1)\n(2, 2)\n(3, 0)\n(3, 1)\n(3, 2)\n(3, 3)\n\n\nAs we will see in more detail, if the elements in the collection or iterator are sequences (tuples or list, say), they can be conveniently unpacked into variables in the for loop statement:\nfor a,b,c in iterator: # do something\n\nfor a,b,c in [[1,2,3],[4,5,6],[7,8,9]]:\n    print(a,b,c)\n\n1 2 3\n4 5 6\n7 8 9\n\n\n\nWhile loops\nA while looops specifies a conditon and a block o f code that is to be excused until the condition evaluates to False or the loops is explicitly ended with break:\n\nx = 256\ntotal = 0\nwhile x > 0:\n    if total > 500 :\n        break\n    total += x\n    x = x //2\n\n\ntotal #504\n\n504\n\n\n\nx\n\n4\n\n\n\n256+128+64+32+16+8\n\n504\n\n\n\n\npass\npass is the “no-op”(No Operation) statement in Python. It can be used in block where no action is to be taken (or as a placeholder for code not yer implemented); it is only required becauese Python uese whitespace to delimit blocks:\n\nx = -1\n\nif x < 0:\n    print('negative')\nelif x == 0:\n    # TODO: put something smart here\n    pass\nelse:\n    print('Postive!')\n\nnegative\n\n\n\n\nrange\nThe range function returns an iterator that yields a sequnce of evenly spaced intergers:\n\nrange(10)\n\nrange(0, 10)\n\n\n\nlist(range(10))\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\nBoth a start,end,and step(Which may be negative) can be given:\n\nlist(range(0,20,2)) #등차수열\n\n[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n\n\n\nlist(range(5,0,-1)) #리버스 인덱스\n\n[5, 4, 3, 2, 1]\n\n\nAs you can see, range prodices integers up to but not including the endpoint.\nA common use of range is for iterating through sequcnes by index:\n\nseq = [1,2,3,4]\nfor i in range(len(seq)):\n    val = seq[i]\n\n\nval\n\n4\n\n\nWhile you can use fuctions loke list to store all the integers generated by range in some other data structure, often the default iterator form will be what you want. This snippet sums all numbers form 0 to 99,999 that are multiples of 3 or 5:\n\nsum = 0\nfor i in range(10000) :\n    # % is the modulo operator\n    if i % 3 == 0 or i % 5 == 0:\n        sum += 1\n\n\n\n\nTernary expressions\nvalue = true - expr if conditon else false - expr\nHere, true-expr and false-expr cna be any Python expressions. It has the identical effect as the more verbose:\nif conditon: value = true-expr else: value = false-expr\n\nx = 5\n'Non-negative' if x >= 0 else 'Negative'\n\n'Non-negative'\n\n\n\nx = 5\n\na = 100 if x>= 0 else -100\na\n\n100"
  },
  {
    "objectID": "Data_Mining/2022-05-07-matplotlib and seaborn.html",
    "href": "Data_Mining/2022-05-07-matplotlib and seaborn.html",
    "title": "Visualization_with_seaborn_and_matplotlib",
    "section": "",
    "text": "seaborn과 matplotlib의 시각화\n코드 출처 - 이제현님 블로그"
  },
  {
    "objectID": "Data_Mining/2022-05-07-matplotlib and seaborn.html#visualization-with-seaborn",
    "href": "Data_Mining/2022-05-07-matplotlib and seaborn.html#visualization-with-seaborn",
    "title": "Visualization_with_seaborn_and_matplotlib",
    "section": "Visualization with seaborn",
    "text": "Visualization with seaborn\nseaborn은 python의 시각화 라이브러리인 matplolib를 기반으로 제작된 라이브러리입니다.\n\nimport seaborn as sns\nsns.set()\nsns.set(style=\"darkgrid\")\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nplt.rcParams['figure.figsize']=(5,5) #figure의 사이즈를 설정이 가능하다 \n\n\nLoding dataset\n\n# 사용할 데이터 불러오기 \ndata_BM = pd.read_csv('bigmart_data.csv')\ndata_BM = data_BM.dropna(how=\"any\") #NA값 제거\ndata_BM[\"Visibility_Scaled\"] = data_BM[\"Item_Visibility\"] * 100 #Visibility_Scaled 컬럼의 값들에 100 곱해줌\ndata_BM.head()\n\n\n\n\n\n  \n    \n      \n      Item_Identifier\n      Item_Weight\n      Item_Fat_Content\n      Item_Visibility\n      Item_Type\n      Item_MRP\n      Outlet_Identifier\n      Outlet_Establishment_Year\n      Outlet_Size\n      Outlet_Location_Type\n      Outlet_Type\n      Item_Outlet_Sales\n      Visibility_Scaled\n    \n  \n  \n    \n      0\n      FDA15\n      9.300\n      Low Fat\n      0.016047\n      Dairy\n      249.8092\n      OUT049\n      1999\n      Medium\n      Tier 1\n      Supermarket Type1\n      3735.1380\n      1.604730\n    \n    \n      1\n      DRC01\n      5.920\n      Regular\n      0.019278\n      Soft Drinks\n      48.2692\n      OUT018\n      2009\n      Medium\n      Tier 3\n      Supermarket Type2\n      443.4228\n      1.927822\n    \n    \n      2\n      FDN15\n      17.500\n      Low Fat\n      0.016760\n      Meat\n      141.6180\n      OUT049\n      1999\n      Medium\n      Tier 1\n      Supermarket Type1\n      2097.2700\n      1.676007\n    \n    \n      4\n      NCD19\n      8.930\n      Low Fat\n      0.000000\n      Household\n      53.8614\n      OUT013\n      1987\n      High\n      Tier 3\n      Supermarket Type1\n      994.7052\n      0.000000\n    \n    \n      5\n      FDP36\n      10.395\n      Regular\n      0.000000\n      Baking Goods\n      51.4008\n      OUT018\n      2009\n      Medium\n      Tier 3\n      Supermarket Type2\n      556.6088\n      0.000000\n    \n  \n\n\n\n\n\ndata_BM.describe() #이상치가 있는지 확인\n\n\n\n\n\n  \n    \n      \n      Item_Weight\n      Item_Visibility\n      Item_MRP\n      Outlet_Establishment_Year\n      Item_Outlet_Sales\n      Visibility_Scaled\n    \n  \n  \n    \n      count\n      4650.000000\n      4650.000000\n      4650.000000\n      4650.000000\n      4650.000000\n      4650.000000\n    \n    \n      mean\n      12.898675\n      0.060700\n      141.716328\n      1999.190538\n      2272.037489\n      6.070048\n    \n    \n      std\n      4.670973\n      0.044607\n      62.420534\n      7.388800\n      1497.964740\n      4.460652\n    \n    \n      min\n      4.555000\n      0.000000\n      31.490000\n      1987.000000\n      69.243200\n      0.000000\n    \n    \n      25%\n      8.770000\n      0.025968\n      94.409400\n      1997.000000\n      1125.202000\n      2.596789\n    \n    \n      50%\n      12.650000\n      0.049655\n      142.979900\n      1999.000000\n      1939.808300\n      4.965549\n    \n    \n      75%\n      17.000000\n      0.088736\n      186.614150\n      2004.000000\n      3111.616300\n      8.873565\n    \n    \n      max\n      21.350000\n      0.188323\n      266.888400\n      2009.000000\n      10256.649000\n      18.832266"
  },
  {
    "objectID": "Data_Mining/2022-05-07-matplotlib and seaborn.html#creating-basic-plots",
    "href": "Data_Mining/2022-05-07-matplotlib and seaborn.html#creating-basic-plots",
    "title": "Visualization_with_seaborn_and_matplotlib",
    "section": "1. Creating basic plots",
    "text": "1. Creating basic plots\nmatplotlib에서 여러 줄이 필요한 한 줄로 seaborn에서 몇 가지 기본 플롯을 만드는 방법을 살펴보겠습니다.\n\nLine Chart\n\n일부 데이터 세트의 경우 한 변수의 변화를 시간의 함수로 이해하거나 이와 유사한 연속 변수를 이해하고자 할 수 있습니다.\nseaborn에서 이는 lineplot() 함수로 직접 또는 kind=\"line\":을 설정하여 relplot()으로 수행할 수 있습니다.\n\n\nsns.lineplot(x=\"Item_Weight\", y=\"Item_MRP\",data=data_BM[:50]); #처음부터 50번째 까지의 데이터만 사용한다\n\n\n\n\n\n\nBar Chart\n\nSeaborn에서는 barplot 기능을 사용하여 간단하게 막대 차트를 생성할 수 있습니다.\nmatplotlib에서 동일한 결과를 얻으려면 데이터 범주를 현명하게 그룹화하기 위해 추가 코드를 작성해야 했습니다.\n그리고 나서 플롯이 올바르게 나오도록 훨씬 더 많은 코드를 작성해야 했습니다.\n\n\nsns.barplot(x=\"Item_Type\", y=\"Item_MRP\", data=data_BM[:5])\n\n<AxesSubplot:xlabel='Item_Type', ylabel='Item_MRP'>\n\n\n\n\n\n\n\nHistogram\n\ndistplot()을 사용하여 seaborn에서 히스토그램을 만들 수 있습니다. 사용할 수 있는 여러 옵션이 있으며 노트북에서 더 자세히 살펴보겠습니다.\n\n\nsns.distplot(data_BM['Item_MRP'])\n\n<AxesSubplot:xlabel='Item_MRP', ylabel='Density'>\n\n\n\n\n\n\n\nBox plots\n\nSeaborn에서 boxplot을 생성하기 위해 boxplot()을 사용할 수 있습니다.\n\n\nsns.boxplot(data_BM['Item_Outlet_Sales'], orient='vertical') \n\n<AxesSubplot:xlabel='Item_Outlet_Sales'>\n\n\n\n\n\n\n\nViolin plot\n\n바이올린 플롯은 상자 및 수염 플롯과 유사한 역할을 합니다.\n이는 하나(또는 그 이상) 범주형 변수의 여러 수준에 걸친 정량적 데이터의 분포를 보여줌으로써 해당 분포를 비교할 수 있습니다.\n모든 플롯 구성 요소가 실제 데이터 포인트에 해당하는 상자 플롯과 달리 바이올린 플롯은 기본 분포의 커널 밀도 추정을 특징으로 합니다.\nseaborn에서 violinplot()을 사용하여 바이올린 플롯을 만들 수 있습니다.\n\n\nsns.violinplot(data_BM['Item_Outlet_Sales'], orient='vertical', color='skyblue')\n\n<AxesSubplot:xlabel='Item_Outlet_Sales'>\n\n\n\n\n\n\n\nScatter plot\n\n각 포인트는 데이터 세트의 관찰을 나타내는 포인트 클라우드를 사용하여 두 변수의 분포를 나타냅니다.\n이 묘사를 통해 눈은 그들 사이에 의미 있는 관계가 있는지 여부에 대한 상당한 양의 정보를 추론할 수 있습니다.\nrelplot()을 kind=scatter 옵션과 함께 사용하여 seaborn에서 산점도를 그릴 수 있습니다.\n\n\nsns.relplot(x=\"Item_MRP\", y=\"Item_Outlet_Sales\", data=data_BM[:200], kind=\"scatter\"); \n#sns.relplot(x=\"Item_MRP\", y=\"Item_Outlet_Sales\", data=data_BM[:200], kind=\"line\"); #kind의 설정으로 표현 설정을 바꿈\n\n\n\n\n\n\nHue semantic\n세 번째 변수에 따라 점을 색칠하여 플롯에 다른 차원을 추가할 수도 있습니다. Seaborn에서는 이것을 “hue semantic” 사용이라고 합니다.\n\nsns.relplot(x=\"Item_MRP\", y=\"Item_Outlet_Sales\", hue=\"Item_Type\",data=data_BM[:200]); #hue로 item_type에 대한 플롯을 그려줌 \n\n\n\n\n\nsns.lineplot(x=\"Item_Weight\", y=\"Item_MRP\",hue='Outlet_Size',data=data_BM[:150]);\n\n\n\n\n\n\nBubble plot\n\nhue semantic 활용하여 Item_Visibility별로 거품을 색칠함과 동시에 개별 거품의 크기로 사용합니다.\n\n\n# Bubble plot\nsns.relplot(x=\"Item_MRP\", y=\"Item_Outlet_Sales\", data=data_BM[:200], kind=\"scatter\", size=\"Visibility_Scaled\", hue=\"Visibility_Scaled\");\n\n\n\n\n\n\nCategory wise sub plot\n\nSeaborn에서 카테고리를 기반으로 플롯을 만들 수도 있습니다.\n각 Outlet_Size에 대한 산점도를 만들었습니다.\n\n\nsns.relplot(x=\"Item_Weight\", y=\"Item_Visibility\",\n            hue='Outlet_Size',style='Outlet_Size',\n            col='Outlet_Size',data=data_BM[:100]);"
  },
  {
    "objectID": "Data_Mining/2022-05-07-matplotlib and seaborn.html#advance-categorical-plots-in-seaborn",
    "href": "Data_Mining/2022-05-07-matplotlib and seaborn.html#advance-categorical-plots-in-seaborn",
    "title": "Visualization_with_seaborn_and_matplotlib",
    "section": "2. Advance categorical plots in seaborn",
    "text": "2. Advance categorical plots in seaborn\n범주형 변수의 경우 seaborn에 세 가지 다른 패밀리가 있습니다.\ncatplot()에서 데이터의 기본 표현은 산점도를 사용합니다.\n\na. Categorical scatterplots\n\n\nStrip plot\n\n하나의 변수가 범주형인 산점도를 그립니다.\ncatplot()에서 kind=strip을 전달하여 생성할 수 있습니다.\n\n\nsns.catplot(x=\"Outlet_Size\", y=\"Item_Outlet_Sales\", kind='strip',data=data_BM[:250]);\n\n\n\n\n\n\nSwarm plot\n\n이 함수는 stripplot()과 유사하지만 점이 겹치지 않도록 조정됩니다(범주형 축을 따라만).\n이렇게 하면 값 분포를 더 잘 표현할 수 있지만 많은 수의 관측치에 대해서는 잘 확장되지 않습니다. 이러한 스타일의 플롯은 때때로 “beeswarm”이라고 불립니다.\ncatplot()에서 kind=swarm을 전달하여 이를 생성할 수 있습니다.\n\n\nsns.catplot(x=\"Outlet_Size\", y=\"Item_Outlet_Sales\", kind='swarm',data=data_BM[:250]);\n\n\n\n\n\n\nb. Categorical distribution plots\n\n\nBox Plots\n\n상자 그림은 극단값과 함께 분포의 3사분위수 값을 보여줍니다.\n“whiskers”은 하위 및 상위 사분위수의 1.5 IQR 내에 있는 점으로 확장되고 이 범위를 벗어나는 관찰은 독립적으로 표시됩니다.\n즉, 상자 그림의 각 값은 데이터의 실제 관측값에 해당합니다.\n\n\nsns.catplot(x=\"Outlet_Size\", y=\"Item_Outlet_Sales\",kind=\"box\",data=data_BM);\n\n\n\n\n\n\nViolin Plots\n\nsns.catplot(x=\"Outlet_Size\", y=\"Item_Outlet_Sales\",kind=\"violin\",data=data_BM);\n\n\n\n\n\n\nPoint Plot\n\nsns.catplot(x=\"Outlet_Size\", y=\"Item_Outlet_Sales\",kind=\"point\",data=data_BM); #y축은 연속형 플랏 x축은 범주형 \n\n\n\n\n\n\nBar plots\n\nsns.catplot(x=\"Outlet_Size\", y=\"Item_Outlet_Sales\",kind=\"bar\",data=data_BM);"
  },
  {
    "objectID": "Data_Mining/2022-05-07-matplotlib and seaborn.html#density-plots",
    "href": "Data_Mining/2022-05-07-matplotlib and seaborn.html#density-plots",
    "title": "Visualization_with_seaborn_and_matplotlib",
    "section": "3. Density Plots",
    "text": "3. Density Plots\n히스토그램 대신 Seaborn이 sn.kdeplot으로 수행하는 커널 밀도 추정을 사용하여 분포의 부드러운 추정치를 얻을 수 있습니다.:\n\n# distribution of Item Visibility\nplt.figure(figsize=(5,5))\nsns.kdeplot(data_BM['Item_Visibility'], shade=True);\n\n\n\n\n\nHistogram and Density Plot\ndistplot을 사용하여 히스토그램과 KDE를 결합할 수 있습니다.:\n\nplt.figure(figsize=(10,10))\nsns.distplot(data_BM['Item_Outlet_Sales']);"
  },
  {
    "objectID": "Data_Mining/2022-05-07-matplotlib and seaborn.html#pair-plots",
    "href": "Data_Mining/2022-05-07-matplotlib and seaborn.html#pair-plots",
    "title": "Visualization_with_seaborn_and_matplotlib",
    "section": "4. Pair plots",
    "text": "4. Pair plots\n\n조인트 플롯을 더 큰 차원의 데이터세트로 일반화하면 쌍 플롯으로 끝납니다. 이것은 모든 값 쌍을 서로에 대해 플롯하려는 경우 다차원 데이터 간의 상관 관계를 탐색하는 데 매우 유용합니다.\n세 가지 붓꽃 종의 꽃잎과 꽃받침 측정값을 나열하는 잘 알려진 Iris 데이터 세트를 사용하여 이것을 시연할 것입니다.\n\n\niris = sns.load_dataset(\"iris\")\nsns.pairplot(iris, hue='species', height=2.5); #둘다 연속형이면 scatter플랏으로 보여줌"
  },
  {
    "objectID": "Data_Mining/2022-05-07-matplotlib and seaborn.html#seaborn-and-matplotlib",
    "href": "Data_Mining/2022-05-07-matplotlib and seaborn.html#seaborn-and-matplotlib",
    "title": "Visualization_with_seaborn_and_matplotlib",
    "section": "Seaborn and Matplotlib",
    "text": "Seaborn and Matplotlib\n\n1.1 Load data\n\n예제로 사용할 펭귄 데이터를 불러옵니다.\nseaborn에 내장되어 있습니다.\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npenguins = sns.load_dataset(\"penguins\")\npenguins.head()\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      Male\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      Female\n    \n    \n      2\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      Female\n    \n    \n      3\n      Adelie\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      Female\n    \n  \n\n\n\n\n\n\n1.2 Figure and Axes\n\nmatplotlib으로 도화지figure를 깔고 축공간axes를 만듭니다.\n1 x 2 축공간을 구성합니다.\n\n\nfig, axes = plt.subplots(ncols=2, figsize=(8,4))\n\nfig.tight_layout()\n\n\n\n\n\n\n1.3 plot with matplotlib\n\nmatplotlib 기능을 이용해서 산점도를 그립니다.\n\nx축은 부리 길이 bill length\ny축은 부리 위 아래 두께 bill depth\n색상은 종species로 합니다.\nAdelie, Chinstrap, Gentoo이 있습니다.\n\n두 축공간 중 왼쪽에만 그립니다.\n\n\nfig, axes = plt.subplots(ncols=2,figsize=(8,4))\n\nspecies_u = penguins[\"species\"].unique()\n\nfor i, s in enumerate(species_u):\n    axes[0].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s],\n                    penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s],\n                    c=f\"C{i}\", label=s, alpha=0.3)\n    \naxes[0].legend(species_u, title=\"species\")\naxes[0].set_xlabel(\"Bill Length (mm)\")\naxes[0].set_ylabel(\"Bill Depth (mm)\")\n\n# plt.show()\nfig.tight_layout()\n\n\n\n\n\n\n1.4 Plot with seaborn\n\n단 세 줄로 거의 동일한 그림이 나왔습니다.\n\nscatter plot의 점 크기만 살짝 작습니다.\nlabel의 투명도만 살짝 다릅니다.\n\nseaborn 명령 scatterplot()을 그대로 사용했습니다.\nx축과 y축 label도 바꾸었습니다.\n\nax=axes[1] 인자에서 볼 수 있듯, 존재하는 axes에 그림만 얹었습니다.\nmatplotlib 틀 + seaborn 그림 이므로, matplotlib 명령이 모두 통합니다.\n\n\n\nfig, axes = plt.subplots(ncols=2,figsize=(8,4))\n\nspecies_u = penguins[\"species\"].unique()\n\n# plot 0 : matplotlib\n\nfor i, s in enumerate(species_u):\n    axes[0].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s],\n                    penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s],\n                    c=f\"C{i}\", label=s, alpha=0.3)\n    \naxes[0].legend(species_u, title=\"species\")\naxes[0].set_xlabel(\"Bill Length (mm)\")\naxes[0].set_ylabel(\"Bill Depth (mm)\")\n\n\n# plot 1 : seaborn\nsns.scatterplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", data=penguins, alpha=0.3, ax=axes[1])\naxes[1].set_xlabel(\"Bill Length (mm)\")\naxes[1].set_ylabel(\"Bill Depth (mm)\")\n\nfig.tight_layout()\n\n\n\n\n\n\n1.5 matplotlib + seaborn & seaborn + matplotlib\n\nmatplotlib과 seaborn이 자유롭게 섞일 수 있습니다.\n\nmatplotlib 산점도 위에 seaborn 추세선을 얹을 수 있고,\nseaborn 산점도 위에 matplotlib 중심점을 얹을 수 있습니다.\n\n\n\nfig, axes = plt.subplots(ncols=2, figsize=(8, 4))\n\nspecies_u = penguins[\"species\"].unique()\n\n# plot 0 : matplotlib + seaborn\nfor i, s in enumerate(species_u):\n    # matplotlib 산점도\n    axes[0].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s],\n                   penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s],\n                   c=f\"C{i}\", label=s, alpha=0.3\n                  )\n                  \n    # seaborn 추세선\n    sns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=penguins.loc[penguins[\"species\"]==s], \n                scatter=False, ax=axes[0])\n    \naxes[0].legend(species_u, title=\"species\")\naxes[0].set_xlabel(\"Bill Length (mm)\")\naxes[0].set_ylabel(\"Bill Depth (mm)\")\n\n# plot 1 : seaborn + matplotlib\n# seaborn 산점도\nsns.scatterplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", data=penguins, alpha=0.3, ax=axes[1])\naxes[1].set_xlabel(\"Bill Length (mm)\")\naxes[1].set_ylabel(\"Bill Depth (mm)\")\n\nfor i, s in enumerate(species_u):\n    # matplotlib 중심점\n    axes[1].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s].mean(),\n                   penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s].mean(),\n                   c=f\"C{i}\", alpha=1, marker=\"x\", s=100\n                  )\n\nfig.tight_layout()\n\n\n\n\n\n\n1.6 seaborn + seaborn + matplotlib\n\nseaborn scatterplot + seaborn kdeplot + matplotlib text입니다\n\n\nfig, ax = plt.subplots(figsize=(6,5))\n\n# plot 0: scatter plot\nsns.scatterplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", color=\"k\", data=penguins, alpha=0.3, ax=ax, legend=False)\n\n# plot 1: kde plot\nsns.kdeplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", data=penguins, alpha=0.5, ax=ax, legend=False)\n\n# text:\nspecies_u = penguins[\"species\"].unique()\nfor i, s in enumerate(species_u):\n    ax.text(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s].mean(),\n            penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s].mean(),\n            s = s, fontdict={\"fontsize\":14, \"fontweight\":\"bold\",\"color\":\"k\"}\n            )\n\nax.set_xlabel(\"Bill Length (mm)\")\nax.set_ylabel(\"Bill Depth (mm)\")\n\nfig.tight_layout()"
  },
  {
    "objectID": "Data_Mining/2022-05-24-exercise-coordinate-reference-systems.html",
    "href": "Data_Mining/2022-05-24-exercise-coordinate-reference-systems.html",
    "title": "Coordinate Refrence Systems",
    "section": "",
    "text": "This notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link.\n\n\nimport pandas as pd\nimport geopandas as gpd\n\nfrom shapely.geometry import LineString\n\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.geospatial.ex2 import *\n\n\nExercises\n\n1) Load the data.\nRun the next code cell (without changes) to load the GPS data into a pandas DataFrame birds_df.\n다음 코드 셀(변경 없이)을 실행하여 GPS 데이터를 pandas DataFrame birds_df에 로드합니다.\n\n# Load the data and print the first 5 rows\nbirds_df = pd.read_csv(\"../input/geospatial-learn-course-data/purple_martin.csv\", parse_dates=['timestamp'])\nprint(\"There are {} different birds in the dataset.\".format(birds_df[\"tag-local-identifier\"].nunique()))\nbirds_df.head()\n\nThere are 11 birds in the dataset, where each bird is identified by a unique value in the “tag-local-identifier” column. Each bird has several measurements, collected at different times of the year.\nUse the next code cell to create a GeoDataFrame birds.\n- birds should have all of the columns from birds_df, along with a “geometry” column that contains Point objects with (longitude, latitude) locations.\n-birds에는 birds_df의 모든 열과 함께 (경도, 위도) 위치가 있는 Point 개체가 포함된 “geometry” 열이 있어야 합니다. - Set the CRS of birds to {'init': 'epsg:4326'}.\n\n# Your code here: Create the GeoDataFrame\nbirds = gpd.GeoDataFrame(birds_df, geometry=gpd.points_from_xy(birds_df[\"location-long\"], birds_df[\"location-lat\"]))\n\n# Your code here: Set the CRS to {'init': 'epsg:4326'}\nbirds.crs = {'init' :'epsg:4326'}\n\n# Check your answer\nq_1.check()\n\n\n# Lines below will give you a hint or solution code\n#q_1.hint()\n#q_1.solution()\n\n\n\n2) Plot the data.\nNext, we load in the 'naturalearth_lowres' dataset from GeoPandas, and set americas to a GeoDataFrame containing the boundaries of all countries in the Americas (both North and South America). Run the next code cell without changes.\n다음으로 GeoPandas에서 naturalearth_lowres 데이터 세트를 로드하고 americas를 미주(북미와 남미 모두)의 모든 국가 경계를 포함하는 GeoDataFrame으로 설정합니다. 변경 없이 다음 코드 셀을 실행합니다.\n\n# Load a GeoDataFrame with country boundaries in North/South America, print the first 5 rows\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\namericas = world.loc[world['continent'].isin(['North America', 'South America'])]\namericas.head()\n\nUse the next code cell to create a single plot that shows both: (1) the country boundaries in the americas GeoDataFrame, and (2) all of the points in the birds_gdf GeoDataFrame.\n다음 코드 셀을 사용하여 (1) americas GeoDataFrame의 국가 경계와 (2) birds_gdf GeoDataFrame의 모든 점을 모두 표시하는 단일 플롯을 만듭니다.\nDon’t worry about any special styling here; just create a preliminary plot, as a quick sanity check that all of the data was loaded properly. In particular, you don’t have to worry about color-coding the points to differentiate between birds, and you don’t have to differentiate starting points from ending points. We’ll do that in the next part of the exercise.\n\n# Your code here\nax = americas.plot(figsize=(8,8), color='red', linestyle=':', edgecolor='black')\namericas.plot(markersize=1, ax=ax)\n# Uncomment to see a hint\n#q_2.hint()\n\n\n# Get credit for your work after you have created a map\nq_2.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_2.solution()\n\n\n\n3) Where does each bird start and end its journey? (Part 1)\nNow, we’re ready to look more closely at each bird’s path. Run the next code cell to create two GeoDataFrames: - path_gdf contains LineString objects that show the path of each bird. It uses the LineString() method to create a LineString object from a list of Point objects. - path_gdf에는 각 새의 경로를 표시하는 LineString 개체가 포함되어 있습니다. LineString() 메서드를 사용하여 Point 개체 목록에서 LineString 개체를 만듭니다. - start_gdf contains the starting points for each bird. - start_gdf는 각 새의 시작 지점을 포함합니다.\n\n# GeoDataFrame showing path for each bird\npath_df = birds.groupby(\"tag-local-identifier\")['geometry'].apply(list).apply(lambda x: LineString(x)).reset_index()\npath_gdf = gpd.GeoDataFrame(path_df, geometry=path_df.geometry)\npath_gdf.crs = {'init' :'epsg:4326'}\n\n# GeoDataFrame showing starting point for each bird\nstart_df = birds.groupby(\"tag-local-identifier\")['geometry'].apply(list).apply(lambda x: x[0]).reset_index()\nstart_gdf = gpd.GeoDataFrame(start_df, geometry=start_df.geometry)\nstart_gdf.crs = {'init' :'epsg:4326'}\n\n# Show first five rows of GeoDataFrame\nstart_gdf.head()\n\nUse the next code cell to create a GeoDataFrame end_gdf containing the final location of each bird.\n- The format should be identical to that of start_gdf, with two columns (“tag-local-identifier” and “geometry”), where the “geometry” column contains Point objects. - 형식은 두 개의 열(“tag-local-identifier” 및 “geometry”)이 있는 start_gdf'의 형식과 동일해야 합니다. 여기서 \"geometry\" 열은 Point 개체를 포함합니다. - Set the CRS ofend_gdfto{‘init’: ‘epsg:4326’}. -end_gdf의 CRS를{‘init’: ‘epsg:4326’}`으로 설정합니다.\n\n# Your code here\nend_df = birds.groupby(\"tag-local-identifier\")['geometry'].apply(list).apply(lambda x: x[-1]).reset_index()\nend_gdf = gpd.GeoDataFrame(end_df, geometry=end_df.geometry)\nend_gdf.crs = {'init' :'epsg:4326'}\n\n# Check your answer\nq_3.check()\n\n\n# Lines below will give you a hint or solution code\n#q_3.hint()\n#q_3.solution()\n\n\n\n4) Where does each bird start and end its journey? (Part 2)\nUse the GeoDataFrames from the question above (path_gdf, start_gdf, and end_gdf) to visualize the paths of all birds on a single map. You may also want to use the americas GeoDataFrame.\n위 질문의 GeoDataFrames(path_gdf, start_gdf, end_gdf)를 사용하여 단일 지도에서 모든 새의 경로를 시각화하세요. americas GeoDataFrame을 사용할 수도 있습니다.\n\n# Your code here\nax = americas.plot(figsize=(10,10), color='none', edgecolor='gainsboro', zorder=3)\n\n# Add wild lands, campsites, and foot trails to the base map\nstart_gdf.plot(color='lightgreen', ax=ax)\npath_gdf.plot(color='maroon', markersize=2, ax=ax)\nend_gdf.plot(color='black', markersize=1, ax=ax) \n\n# Uncomment to see a hint\n#q_4.hint()\n\n\n# Get credit for your work after you have created a map\nq_4.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_4.solution()\n\n\n\n5) Where are the protected areas in South America? (Part 1)\nIt looks like all of the birds end up somewhere in South America. But are they going to protected areas?\nIn the next code cell, you’ll create a GeoDataFrame protected_areas containing the locations of all of the protected areas in South America. The corresponding shapefile is located at filepath protected_filepath.\n다음 코드 셀에서는 남미의 모든 보호 지역 위치를 포함하는 GeoDataFrame protected_areas를 생성합니다. 해당 shapefile은 파일 경로 protected_filepath에 있습니다.\n\n# Path of the shapefile to load\nprotected_filepath = \"../input/geospatial-learn-course-data/SAPA_Aug2019-shapefile/SAPA_Aug2019-shapefile/SAPA_Aug2019-shapefile-polygons.shp\"\n\n# Your code here\nprotected_areas = gpd.read_file(protected_filepath)\n\n# Check your answer\nq_5.check()\n\n\n# Lines below will give you a hint or solution code\n#q_5.hint()\n#q_5.solution()\n\n\n\n6) Where are the protected areas in South America? (Part 2)\nCreate a plot that uses the protected_areas GeoDataFrame to show the locations of the protected areas in South America. (You’ll notice that some protected areas are on land, while others are in marine waters.)\n‘protected_areas’ GeoDataFrame을 사용하여 남아메리카의 보호 지역 위치를 표시하는 플롯을 만듭니다. (일부 보호 구역은 육지에 있고 다른 보호 구역은 바다에 있음을 알 수 있습니다.)\n\n# Country boundaries in South America\nsouth_america = americas.loc[americas['continent']=='South America']\n\n# Your code here: plot protected areas in South America\nax = south_america.plot(figsize=(8,8), color='whitesmoke', edgecolor='black')\nprotected_areas.plot(markersize=1, ax=ax,alpha=0.4)\n\n# Uncomment to see a hint\n#q_6.hint()\n\n\n# Get credit for your work after you have created a map\nq_6.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_6.solution()\n\n\n\n7) What percentage of South America is protected?\nYou’re interested in determining what percentage of South America is protected, so that you know how much of South America is suitable for the birds.\nAs a first step, you calculate the total area of all protected lands in South America (not including marine area). To do this, you use the “REP_AREA” and “REP_M_AREA” columns, which contain the total area and total marine area, respectively, in square kilometers.\nRun the code cell below without changes.\n\nP_Area = sum(protected_areas['REP_AREA']-protected_areas['REP_M_AREA'])\nprint(\"South America has {} square kilometers of protected areas.\".format(P_Area))\n\nThen, to finish the calculation, you’ll use the south_america GeoDataFrame.\n\nsouth_america.head()\n\nCalculate the total area of South America by following these steps: - Calculate the area of each country using the area attribute of each polygon (with EPSG 3035 as the CRS), and add up the results. The calculated area will be in units of square meters. - 각 폴리곤(CRS로 EPSG 3035 사용)의 ‘area’ 속성을 사용하여 각 국가의 면적을 계산하고 결과를 합산합니다. 계산된 면적은 평방 미터 단위입니다. - Convert your answer to have units of square kilometeters. - 평방 킬로미터 단위가 되도록 답을 변환하십시오.\n\n# Your code here: Calculate the total area of South America (in square kilometers)\ntotalArea = sum(south_america.geometry.to_crs(epsg=3035).area)/10**6\ntotalArea \n# Check your answer\nq_7.check()\n\n\n# Lines below will give you a hint or solution code\n#q_7.hint()\n#q_7.solution()\n\nRun the code cell below to calculate the percentage of South America that is protected.\n\n# What percentage of South America is protected?\npercentage_protected = P_Area/totalArea\nprint('Approximately {}% of South America is protected.'.format(round(percentage_protected*100, 2)))\n\n\n\n8) Where are the birds in South America?\nSo, are the birds in protected areas?\n그렇다면 새들은 보호 구역에 있습니까?\nCreate a plot that shows for all birds, all of the locations where they were discovered in South America. Also plot the locations of all protected areas in South America.\n모든 새, 남미에서 발견된 모든 위치를 보여주는 플롯을 만듭니다. 또한 남아메리카의 모든 보호 지역의 위치를 ​​표시합니다.\nTo exclude protected areas that are purely marine areas (with no land component), you can use the “MARINE” column (and plot only the rows in protected_areas[protected_areas['MARINE']!='2'], instead of every row in the protected_areas GeoDataFrame).\n순수한 해양 지역(토지 구성요소 없음)인 보호 지역을 제외하려면 “MARINE” 열을 사용할 수 있습니다(그리고 protected_areas[protected_areas['MARINE']!='2'] protected_areas GeoDataFrame의 모든 행).\n\n# Your code here\nax = south_america.plot(figsize=(8,8), color='whitesmoke', edgecolor='black')\nprotected_areas[protected_areas['MARINE']!='2'].plot(ax=ax, alpha=0.4, zorder=1)\nbirds[birds.geometry.y < 0].plot(ax=ax, color='red', alpha=0.6, markersize=10, zorder=2)\n\n# Uncomment to see a hint\n#q_8.hint()\n\n\n# Get credit for your work after you have created a map\nq_8.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_8.solution()\n\n\n\n\nKeep going\nCreate stunning interactive maps with your geospatial data.\n\nHave questions or comments? Visit the course discussion forum to chat with other learners."
  },
  {
    "objectID": "Data_Mining/2022-05-24-exercise-interactive-maps.html",
    "href": "Data_Mining/2022-05-24-exercise-interactive-maps.html",
    "title": "Interactive Maps",
    "section": "",
    "text": "This notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link.\n\n\nimport pandas as pd\nimport geopandas as gpd\n\nimport folium\nfrom folium import Choropleth\nfrom folium.plugins import HeatMap\n\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.geospatial.ex3 import *\n\nWe define a function embed_map() for displaying interactive maps. It accepts two arguments: the variable containing the map, and the name of the HTML file where the map will be saved.\nThis function ensures that the maps are visible in all web browsers.\n\ndef embed_map(m, file_name):\n    from IPython.display import IFrame\n    m.save(file_name)\n    return IFrame(file_name, width='100%', height='500px')\n\n\nExercises\n\n1) Do earthquakes coincide with plate boundaries?\nRun the code cell below to create a DataFrame plate_boundaries that shows global plate boundaries. The “coordinates” column is a list of (latitude, longitude) locations along the boundaries.\n아래 코드 셀을 실행하여 전역 플레이트 경계를 표시하는 DataFrame plate_boundaries를 만듭니다. “좌표” 열은 경계를 따라 (위도, 경도) 위치의 목록입니다.\n\nplate_boundaries = gpd.read_file(\"../input/geospatial-learn-course-data/Plate_Boundaries/Plate_Boundaries/Plate_Boundaries.shp\")\nplate_boundaries['coordinates'] = plate_boundaries.apply(lambda x: [(b,a) for (a,b) in list(x.geometry.coords)], axis='columns')\nplate_boundaries.drop('geometry', axis=1, inplace=True)\n\nplate_boundaries.head()\n\nNext, run the code cell below without changes to load the historical earthquake data into a DataFrame earthquakes.\n\n# Load the data and print the first 5 rows\nearthquakes = pd.read_csv(\"../input/geospatial-learn-course-data/earthquakes1970-2014.csv\", parse_dates=[\"DateTime\"])\nearthquakes.head()\n\nThe code cell below visualizes the plate boundaries on a map. Use all of the earthquake data to add a heatmap to the same map, to determine whether earthquakes coincide with plate boundaries.\n아래 코드 셀은 지도에서 판 경계를 시각화합니다. 모든 지진 데이터를 사용하여 동일한 지도에 히트맵을 추가하여 지진이 판 경계와 일치하는지 확인합니다.\n\n# Create a base map with plate boundaries\nm_1 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)\nfor i in range(len(plate_boundaries)):\n    folium.PolyLine(locations=plate_boundaries.coordinates.iloc[i], weight=2, color='black').add_to(m_1)\n\n# Your code here: Add a heatmap to the map\nHeatMap(data=earthquakes[['Latitude', 'Longitude']], radius=10).add_to(m_1)\n\n# Uncomment to see a hint\n#q_1.a.hint()\n\n# Show the map\nembed_map(m_1, 'q_1.html')\n\n\n# Get credit for your work after you have created a map\nq_1.a.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_1.a.solution()\n\nSo, given the map above, do earthquakes coincide with plate boundaries?\n\n# View the solution (Run this code cell to receive credit!)\nq_1.b.solution()\n\n\n\n2) Is there a relationship between earthquake depth and proximity to a plate boundary in Japan?\nYou recently read that the depth of earthquakes tells us important information about the structure of the earth. You’re interested to see if there are any intereresting global patterns, and you’d also like to understand how depth varies in Japan.\n최근에 지진의 깊이가 중요 정보를 알려준다는 내용을 읽었습니다. -news_science_products) 지구의 구조에 대해. 흥미로운 글로벌 패턴이 있는지 확인하고 싶고 일본의 깊이가 어떻게 다른지 이해하고 싶습니다.\n\n# Create a base map with plate boundaries\nm_2 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)\nfor i in range(len(plate_boundaries)):\n    folium.PolyLine(locations=plate_boundaries.coordinates.iloc[i], weight=2, color='black').add_to(m_2)\n    \n# Your code here: Add a map to visualize earthquake depth\ndef color_producer(val):\n    if val < 50:\n        return 'forestgreen'\n    else:\n        return 'darkred'\n\n\nfor i in range(0,len(earthquakes)):\n    folium.Circle(\n        location=[earthquakes.iloc[i]['Latitude'], earthquakes.iloc[i]['Longitude']],\n        radius=2000,\n        color=color_producer(earthquakes.iloc[i]['Depth'])).add_to(m_2)\n    \n# Uncomment to see a hint\nq_2.a.hint()\n\n# View the map\nembed_map(m_2, 'q_2.html')\n\n\n# Get credit for your work after you have created a map\nq_2.a.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_2.a.solution()\n\nCan you detect a relationship between proximity to a plate boundary and earthquake depth? Does this pattern hold globally? In Japan?\n\n# View the solution (Run this code cell to receive credit!)\nq_2.b.solution()\n\n\n\n3) Which prefectures have high population density?\nRun the next code cell (without changes) to create a GeoDataFrame prefectures that contains the geographical boundaries of Japanese prefectures.\n다음 코드 셀을 변경 없이 실행하여 일본 현의 지리적 경계를 포함하는 GeoDataFrame ’현’을 만듭니다.\n\n# GeoDataFrame with prefecture boundaries\nprefectures = gpd.read_file(\"../input/geospatial-learn-course-data/japan-prefecture-boundaries/japan-prefecture-boundaries/japan-prefecture-boundaries.shp\")\nprefectures.set_index('prefecture', inplace=True)\nprefectures.head()\n\nThe next code cell creates a DataFrame stats containing the population, area (in square kilometers), and population density (per square kilometer) for each Japanese prefecture. Run the code cell without changes.\n다음 코드 셀은 각 일본 현에 대한 인구, 면적(제곱 킬로미터 단위) 및 인구 밀도(제곱 킬로미터당)를 포함하는 DataFrame ’통계’를 생성합니다. 변경 없이 코드 셀을 실행합니다.\n\n# DataFrame containing population of each prefecture\npopulation = pd.read_csv(\"../input/geospatial-learn-course-data/japan-prefecture-population.csv\")\npopulation.set_index('prefecture', inplace=True)\n\n# Calculate area (in square kilometers) of each prefecture\narea_sqkm = pd.Series(prefectures.geometry.to_crs(epsg=32654).area / 10**6, name='area_sqkm')\nstats = population.join(area_sqkm)\n\n# Add density (per square kilometer) of each prefecture\nstats['density'] = stats[\"population\"] / stats[\"area_sqkm\"]\nstats.head()\n\nUse the next code cell to create a choropleth map to visualize population density.\n다음 코드 셀을 사용하여 등치 지도를 만들어 인구 밀도를 시각화합니다.\n\n# Create a base map\nm_3 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)\n\n# Your code here: create a choropleth map to visualize population density\nChoropleth(geo_data=prefectures['geometry'].__geo_interface__, \n           data=stats['density'], \n           key_on=\"feature.id\", \n           fill_color='YlGnBu', \n           legend_name='population density'\n          ).add_to(m_3)\n\n# Uncomment to see a hint\n#q_3.a.hint()\n\n# View the map\nembed_map(m_3, 'q_3.html')\n\n\n# Get credit for your work after you have created a map\nq_3.a.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_3.a.solution()\n\nWhich three prefectures have relatively higher density than the others? Are they spread throughout the country, or all located in roughly the same geographical region? (If you’re unfamiliar with Japanese geography, you might find this map useful to answer the questions.)\n\n# View the solution (Run this code cell to receive credit!)\nq_3.b.solution()\n\n\n\n4) Which high-density prefecture is prone to high-magnitude earthquakes?\nCreate a map to suggest one prefecture that might benefit from earthquake reinforcement. Your map should visualize both density and earthquake magnitude.\n지진 보강의 혜택을 받을 수 있는 한 현을 제안하는 지도를 만듭니다. 지도는 밀도와 지진 규모를 모두 시각화해야 합니다.\n\n# Create a base map\nm_4 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)\n\n# Your code here: create a map\ndef color_producer(magnitude):\n    if magnitude > 6.5:\n        return 'red'\n    else:\n        return 'green'\n\nChoropleth(\n    geo_data=prefectures['geometry'].__geo_interface__,\n    data=stats['density'],\n    key_on=\"feature.id\",\n    fill_color='BuPu',\n    legend_name='Population density (per square kilometer)').add_to(m_4)\n\nfor i in range(0,len(earthquakes)):\n    folium.Circle(\n        location=[earthquakes.iloc[i]['Latitude'], earthquakes.iloc[i]['Longitude']],\n        popup=(\"{} ({})\").format(\n            earthquakes.iloc[i]['Magnitude'],\n            earthquakes.iloc[i]['DateTime'].year),\n        radius=earthquakes.iloc[i]['Magnitude']**5.5,\n        color=color_producer(earthquakes.iloc[i]['Magnitude'])).add_to(m_4)\n# Uncomment to see a hint\n#q_4.a.hint()\n\n# View the map\nembed_map(m_4, 'q_4.html')\n\n\n# Get credit for your work after you have created a map\nq_4.a.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_4.a.solution()\n\nWhich prefecture do you recommend for extra earthquake reinforcement?\n\n# View the solution (Run this code cell to receive credit!)\nq_4.b.solution()\n\n\n\n\nKeep going\nLearn how to convert names of places to geographic coordinates with geocoding. You’ll also explore special ways to join information from multiple GeoDataFrames.\n\nHave questions or comments? Visit the course discussion forum to chat with other learners."
  },
  {
    "objectID": "Data_Mining/2022-05-24-exercise-your-first-map.html",
    "href": "Data_Mining/2022-05-24-exercise-your-first-map.html",
    "title": "Your First Map",
    "section": "",
    "text": "This notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link.\n\n\nimport geopandas as gpd\n\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.geospatial.ex1 import *\n\n\n1) Get the data.\nUse the next cell to load the shapefile located at loans_filepath to create a GeoDataFrame world_loans.\n다음 셀을 사용하여 loans_filepath에 있는 shapefile을 로드하여 GeoDataFrame world_loans를 생성합니다.\n\nloans_filepath = \"../input/geospatial-learn-course-data/kiva_loans/kiva_loans/kiva_loans.shp\"\n\n# Your code here: Load the data\nworld_loans = gpd.read_file(loans_filepath)\n\n# Check your answer\nq_1.check()\n\n# Uncomment to view the first five rows of the data\n#world_loans.head()\n\n\n\n2) Plot the data.\nRun the next code cell without changes to load a GeoDataFrame world containing country boundaries.\n변경 없이 다음 코드 셀을 실행하여 국가 경계가 포함된 GeoDataFrame ’world’를 로드합니다.\n\n# Lines below will give you a hint or solution code\n#q_1.hint()\n#q_1.solution()\n\n\n# This dataset is provided in GeoPandas\nworld_filepath = gpd.datasets.get_path('naturalearth_lowres')\nworld = gpd.read_file(world_filepath)\nworld.head()\n\nUse the world and world_loans GeoDataFrames to visualize Kiva loan locations across the world.\nworld 및 world_loans GeoDataFrames를 사용하여 전 세계의 Kiva loan locations를 시각화합니다.\n\n# Your code here\nax = world.plot(figsize=(20,20), color='none', edgecolor='gainsboro',zorder=3)\nworld_loans.plot(color='skyblue', markersize=2,ax=ax)\n# Uncomment to see a hint\n#q_2.hint()\n\n\n# Get credit for your work after you have created a map\nq_2.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_2.solution()\n\n\n\n3) Select loans based in the Philippines.\nNext, you’ll focus on loans that are based in the Philippines. Use the next code cell to create a GeoDataFrame PHL_loans which contains all rows from world_loans with loans that are based in the Philippines.\n다음으로 필리핀에 기반을 둔 대출에 중점을 둘 것입니다. 다음 코드 셀을 사용하여 필리핀에 기반을 둔 대출이 있는 world_loans의 모든 행을 포함하는 GeoDataFrame PHL_loans를 만듭니다.\n\n# Your code here\nPHL_loans = world_loans.loc[world_loans.country==\"Philippines\"].copy()\n\n\n# Check your answer\nq_3.check()\n\n\n# Lines below will give you a hint or solution code\n#q_3.hint()\n#q_3.solution()\n\n\n\n4) Understand loans in the Philippines.\nRun the next code cell without changes to load a GeoDataFrame PHL containing boundaries for all islands in the Philippines.\n필리핀의 모든 섬에 대한 경계를 포함하는 GeoDataFrame PHL을 로드하려면 변경 없이 다음 코드 셀을 실행하십시오.\n\n# Load a KML file containing island boundaries\ngpd.io.file.fiona.drvsupport.supported_drivers['KML'] = 'rw'\nPHL = gpd.read_file(\"../input/geospatial-learn-course-data/Philippines_AL258.kml\", driver='KML')\nPHL.head()\n\nUse the PHL and PHL_loans GeoDataFrames to visualize loans in the Philippines.\n‘PHL’ 및 ‘PHL_loans’ GeoDataFrames를 사용하여 필리핀의 대출을 시각화합니다.\n\n# Your code here\nax = PHL.plot(figsize=(20,20), color='none', edgecolor='gainsboro', zorder=3)\nPHL_loans.plot(color='skyblue', markersize=2, ax=ax)\n\n# Uncomment to see a hint\n#q_4.a.hint()\n\n\n# Get credit for your work after you have created a map\nq_4.a.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_4.a.solution()\n\nCan you identify any islands where it might be useful to recruit new Field Partners? Do any islands currently look outside of Kiva’s reach?\nYou might find this map useful to answer the question.\n\n# View the solution (Run this code cell to receive credit!)\nq_4.b.solution()\n\n\n\nKeep going\nContinue to learn about coordinate reference systems.\n\nHave questions or comments? Visit the course discussion forum to chat with other learners."
  },
  {
    "objectID": "Data_Mining/2022-05-25-newyorktaxi.html",
    "href": "Data_Mining/2022-05-25-newyorktaxi.html",
    "title": "NYC taxi",
    "section": "",
    "text": "NYC taxi"
  },
  {
    "objectID": "Data_Mining/2022-05-25-newyorktaxi.html#visual-analytics-with-python",
    "href": "Data_Mining/2022-05-25-newyorktaxi.html#visual-analytics-with-python",
    "title": "NYC taxi",
    "section": "Visual Analytics with Python",
    "text": "Visual Analytics with Python\n강의자료 출처 - kaggle.com\nIn this script we will explore the spatial and temporal behavior of the people of New York as can be inferred by examining their cab usage.\nThe main fields of this dataset are taxi pickup time and location, as well as dropoff location and trip duration. There is a total of around 1.4 Million trips in the dataset that took place during the first half of 2016.\nWe will study how the patterns of cab usage change throughout the year, throughout the week and throughout the day, and we will focus on difference between weekdays and weekends.\n\n%matplotlib inline\n\nfrom sklearn import decomposition\nfrom scipy import stats\nfrom sklearn import cluster\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nmatplotlib.style.use('fivethirtyeight')\nmatplotlib.rcParams['font.size'] = 12\nmatplotlib.rcParams['figure.figsize'] = (10,10)\n\n\nLoad data and preprocess measurements to sensible units\n\ndata_frame = pd.read_csv('train.csv')\n\n\ndata_frame.describe()\n\n\n\n\n\n  \n    \n      \n      vendor_id\n      passenger_count\n      pickup_longitude\n      pickup_latitude\n      dropoff_longitude\n      dropoff_latitude\n      trip_duration\n    \n  \n  \n    \n      count\n      1.458644e+06\n      1.458644e+06\n      1.458644e+06\n      1.458644e+06\n      1.458644e+06\n      1.458644e+06\n      1.458644e+06\n    \n    \n      mean\n      1.534950e+00\n      1.664530e+00\n      -7.397349e+01\n      4.075092e+01\n      -7.397342e+01\n      4.075180e+01\n      9.594923e+02\n    \n    \n      std\n      4.987772e-01\n      1.314242e+00\n      7.090186e-02\n      3.288119e-02\n      7.064327e-02\n      3.589056e-02\n      5.237432e+03\n    \n    \n      min\n      1.000000e+00\n      0.000000e+00\n      -1.219333e+02\n      3.435970e+01\n      -1.219333e+02\n      3.218114e+01\n      1.000000e+00\n    \n    \n      25%\n      1.000000e+00\n      1.000000e+00\n      -7.399187e+01\n      4.073735e+01\n      -7.399133e+01\n      4.073588e+01\n      3.970000e+02\n    \n    \n      50%\n      2.000000e+00\n      1.000000e+00\n      -7.398174e+01\n      4.075410e+01\n      -7.397975e+01\n      4.075452e+01\n      6.620000e+02\n    \n    \n      75%\n      2.000000e+00\n      2.000000e+00\n      -7.396733e+01\n      4.076836e+01\n      -7.396301e+01\n      4.076981e+01\n      1.075000e+03\n    \n    \n      max\n      2.000000e+00\n      9.000000e+00\n      -6.133553e+01\n      5.188108e+01\n      -6.133553e+01\n      4.392103e+01\n      3.526282e+06\n    \n  \n\n\n\n\n\ndata_frame.shape\n\n(1458644, 11)\n\n\n\nnp.max(data_frame['trip_duration']) #이렇게 보면서 이상치가 있는 것을 눈으로 봄 3526282(전처리전) --> 4764(전처리후)\n\n3526282\n\n\n\n# remove obvious outliers / 위경도 좌표가 튀는 것을 제거 \nallLat  = np.array(list(data_frame['pickup_latitude'])  + list(data_frame['dropoff_latitude'])) #두개의 컬럼을 리스트로 합쳐줌 \nallLong = np.array(list(data_frame['pickup_longitude']) + list(data_frame['dropoff_longitude']))\n\nlongLimits = [np.percentile(allLong, 0.3), np.percentile(allLong, 99.7)]\nlatLimits  = [np.percentile(allLat , 0.3), np.percentile(allLat , 99.7)]\ndurLimits  = [np.percentile(data_frame['trip_duration'], 0.4), np.percentile(data_frame['trip_duration'], 99.7)]\n\ndata_frame = data_frame[(data_frame['pickup_latitude']   >= latLimits[0] ) & (data_frame['pickup_latitude']   <= latLimits[1]) ] #이상치 제거\ndata_frame = data_frame[(data_frame['dropoff_latitude']  >= latLimits[0] ) & (data_frame['dropoff_latitude']  <= latLimits[1]) ]\ndata_frame = data_frame[(data_frame['pickup_longitude']  >= longLimits[0]) & (data_frame['pickup_longitude']  <= longLimits[1])]\ndata_frame = data_frame[(data_frame['dropoff_longitude'] >= longLimits[0]) & (data_frame['dropoff_longitude'] <= longLimits[1])]\ndata_frame = data_frame[(data_frame['trip_duration']     >= durLimits[0] ) & (data_frame['trip_duration']     <= durLimits[1]) ]\ndata_frame = data_frame.reset_index(drop=True)\n\n\n# convert fields to sensible units\nmedianLat  = np.percentile(allLat,50)\nmedianLong = np.percentile(allLong,50)\n\nlatMultiplier  = 111.32\nlongMultiplier = np.cos(medianLat*(np.pi/180.0)) * 111.32\n\ndata_frame['duration [min]'] = data_frame['trip_duration']/60.0\ndata_frame['src lat [km]']   = latMultiplier  * (data_frame['pickup_latitude']   - medianLat)\ndata_frame['src long [km]']  = longMultiplier * (data_frame['pickup_longitude']  - medianLong)\ndata_frame['dst lat [km]']   = latMultiplier  * (data_frame['dropoff_latitude']  - medianLat)\ndata_frame['dst long [km]']  = longMultiplier * (data_frame['dropoff_longitude'] - medianLong)\n\nallLat  = np.array(list(data_frame['src lat [km]'])  + list(data_frame['dst lat [km]']))\nallLong = np.array(list(data_frame['src long [km]']) + list(data_frame['dst long [km]']))\n\n\n\nPlot the histograms of trip duration, latitude and longitude\n\nfig, axArray = plt.subplots(nrows=1,ncols=3,figsize=(13,4))\naxArray[0].hist(data_frame['duration [min]'],80);\naxArray[0].set_xlabel('trip duration [min]'); axArray[0].set_ylabel('counts')\naxArray[1].hist(allLat ,80); axArray[1].set_xlabel('latitude [km]')\naxArray[2].hist(allLong,80); axArray[2].set_xlabel('longitude [km]')\n#위경도는 큰의미는 없지만 플랏으로 이상치가 있는지 시각적으로 확인 \n\nText(0.5, 0, 'longitude [km]')\n\n\n\n\n\n\n\nPlot the trip Duration vs. the Aerial Distance between pickup and dropoff\n\ndata_frame['log duration']       = np.log1p(data_frame['duration [min]'])\ndata_frame['euclidian distance'] = np.sqrt((data_frame['src lat [km]']  - data_frame['dst lat [km]'] )**2 +\n                                       (data_frame['src long [km]'] - data_frame['dst long [km]'])**2)\n\nfig, axArray = plt.subplots(nrows=1,ncols=2,figsize=(13,6))\naxArray[0].scatter(data_frame['euclidian distance'], data_frame['duration [min]'],c='b',s=5,alpha=0.01);\naxArray[0].set_xlabel('Aerial Euclidian Distance [km]'); axArray[0].set_ylabel('Duration [min]')\naxArray[0].set_xlim(data_frame['euclidian distance'].min(),data_frame['euclidian distance'].max())\naxArray[0].set_ylim(data_frame['duration [min]'].min(),data_frame['duration [min]'].max())\naxArray[0].set_title('trip Duration vs Aerial trip Distance')\n\naxArray[1].scatter(data_frame['euclidian distance'], data_frame['log duration'],c='b',s=5,alpha=0.01);\naxArray[1].set_xlabel('Aerial Euclidian Distance [km]'); axArray[1].set_ylabel('log(1+Duration) [log(min)]')\naxArray[1].set_xlim(data_frame['euclidian distance'].min(),data_frame['euclidian distance'].max())\naxArray[1].set_ylim(data_frame['log duration'].min(),data_frame['log duration'].max())\naxArray[1].set_title('log of trip Duration vs Aerial trip Distance')\n\nText(0.5, 1.0, 'log of trip Duration vs Aerial trip Distance')\n\n\n\n\n\n\n\nExercise 1\n위의 Scatter plot은 Point가 너무 많이 존재하여 좋은 시각화가 아닐 수 있습니다. 보다 효율적인 시각화 방안을 제시해 보세요.\nmatplotlib, seaborn, plotly 등 다양한 패키지를 활용할 수 있습니다\n\nimport seaborn as sns\n\nsns.jointplot(x=data_frame['euclidian distance'], \n              y=data_frame['duration [min]'], \n              kind=\"hex\", \n              color=\"#4CB391\",\n              xlim = (0,30),\n              ylim = (0,70))\n\n<seaborn.axisgrid.JointGrid at 0x1d6899db160>\n\n\n\n\n\n\np = sns.jointplot(x=np.log1p(data_frame['euclidian distance']),\n              y=data_frame['log duration'], \n              kind=\"hex\", \n              color=\"#4CB391\") #log를 취해 주어서 히트맵을 변환시켜 보여줌 \n\np.ax_joint.set_xlabel('log(1+Aerial Euclidian Distance) [log(km)]')\np.ax_joint.set_ylabel('log(1+Duration) [log(min)]')\n\nText(8.060000000000002, 0.5, 'log(1+Duration) [log(min)]')\n\n\n\n\n\nWe can see that the trip distance defines the lower bound on trip duration, as one would expect.\n\n\nPlot spatial density plot of the pickup and dropoff locations\n\nimageSize = (700,700)\nlongRange = [-5,19]\nlatRange = [-13,11]\n\nallLatInds  = imageSize[0] - (imageSize[0] * (allLat  - latRange[0])  / (latRange[1]  - latRange[0]) ).astype(int)\nallLongInds =                (imageSize[1] * (allLong - longRange[0]) / (longRange[1] - longRange[0])).astype(int)\n\nlocationDensityImage = np.zeros(imageSize)\nfor latInd, longInd in zip(allLatInds,allLongInds):\n    locationDensityImage[latInd,longInd] += 1\n\nfig, ax = plt.subplots(nrows=1,ncols=1,figsize=(12,12))\nax.imshow(np.log(locationDensityImage+1),cmap='inferno')\nax.set_axis_off()\n\n\n\n\n\n\nExercise 2\nfolium이나 pydeck을 사용하여 택시의 승차지점, 하차지점을 시각화 해봅시다. 디서 승객이 많이 타고 내리는지를 확인할 수 있습니까?\n\ndata_frame.head()\n\n\n\n\n\n  \n    \n      \n      id\n      vendor_id\n      pickup_datetime\n      dropoff_datetime\n      passenger_count\n      pickup_longitude\n      pickup_latitude\n      dropoff_longitude\n      dropoff_latitude\n      store_and_fwd_flag\n      trip_duration\n      duration [min]\n      src lat [km]\n      src long [km]\n      dst lat [km]\n      dst long [km]\n      log duration\n      euclidian distance\n    \n  \n  \n    \n      0\n      id2875421\n      2\n      2016-03-14 17:24:55\n      2016-03-14 17:32:30\n      1\n      -73.982155\n      40.767937\n      -73.964630\n      40.765602\n      N\n      455\n      7.583333\n      1.516008\n      -0.110015\n      1.256121\n      1.367786\n      2.149822\n      1.500479\n    \n    \n      1\n      id2377394\n      1\n      2016-06-12 00:43:35\n      2016-06-12 00:54:38\n      1\n      -73.980415\n      40.738564\n      -73.999481\n      40.731152\n      N\n      663\n      11.050000\n      -1.753813\n      0.036672\n      -2.578912\n      -1.571088\n      2.489065\n      1.807119\n    \n    \n      2\n      id3858529\n      2\n      2016-01-19 11:35:24\n      2016-01-19 12:10:48\n      1\n      -73.979027\n      40.763939\n      -74.005333\n      40.710087\n      N\n      2124\n      35.400000\n      1.070973\n      0.153763\n      -4.923841\n      -2.064547\n      3.594569\n      6.392080\n    \n    \n      3\n      id3504673\n      2\n      2016-04-06 19:32:31\n      2016-04-06 19:39:40\n      1\n      -74.010040\n      40.719971\n      -74.012268\n      40.706718\n      N\n      429\n      7.150000\n      -3.823568\n      -2.461500\n      -5.298809\n      -2.649362\n      2.098018\n      1.487155\n    \n    \n      4\n      id2181028\n      2\n      2016-03-26 13:30:55\n      2016-03-26 13:38:10\n      1\n      -73.973053\n      40.793209\n      -73.972923\n      40.782520\n      N\n      435\n      7.250000\n      4.329328\n      0.657515\n      3.139453\n      0.668452\n      2.110213\n      1.189925\n    \n  \n\n\n\n\n\ndata_frame.columns\n\nIndex(['id', 'vendor_id', 'pickup_datetime', 'dropoff_datetime',\n       'passenger_count', 'pickup_longitude', 'pickup_latitude',\n       'dropoff_longitude', 'dropoff_latitude', 'store_and_fwd_flag',\n       'trip_duration', 'duration [min]', 'src lat [km]', 'src long [km]',\n       'dst lat [km]', 'dst long [km]', 'log duration', 'euclidian distance'],\n      dtype='object')\n\n\n\nimport pydeck as pdk\n\n\n# Define a layer to display on a map\nlayer = pdk.Layer(\n    \"HexagonLayer\",\n    data_frame[:1000],\n    get_position=[\"pickup_longitude\", \"pickup_latitude\"],\n    auto_highlight=True,\n    elevation_scale=50,\n    pickable=True,\n    elevation_range=[0, 50],\n    extruded=True,\n    coverage=1,\n)\n\n# Set the viewport location\nview_lon = np.mean(data_frame[\"pickup_longitude\"])\nview_lat = np.mean(data_frame[\"pickup_latitude\"])\n\nview_state = pdk.ViewState(\n    longitude=view_lon, latitude=view_lat, zoom=10, min_zoom=5, max_zoom=15, pitch=40.5, bearing=-27.36,\n)\n\n# Render\nr = pdk.Deck(layers=[layer], initial_view_state=view_state)\nr.show()\n#r.to_html(\"hexagon_ny_taxi.html\")\n\n\n\n\n\nlayer = pdk.Layer(\n    \"HeatmapLayer\",\n    data_frame[:1000],\n    get_position=[\"pickup_longitude\", \"pickup_latitude\"],\n    auto_highlight=True,\n    elevation_scale=50,\n    pickable=True,\n    elevation_range=[0, 50],\n    extruded=True,\n    coverage=1,\n)\n\n# Set the viewport location\nview_lon = np.mean(data_frame[\"pickup_longitude\"])\nview_lat = np.mean(data_frame[\"pickup_latitude\"])\n\nview_state = pdk.ViewState(\n    longitude=view_lon, latitude=view_lat, zoom=10, min_zoom=5, max_zoom=15, pitch=40.5, bearing=-27.36,\n)\n\n# Render\nr = pdk.Deck(layers=[layer], initial_view_state=view_state)\nr.show()\n\n\n\n\n\n#folium 을 이용해서도 예제로 해보기 \n\n\n\nClosing in on Manhattan\n\nimageSizeMan = (720,480)\nlatRangeMan = [-8,10]\nlongRangeMan = [-5,7]\n\nindToKeep  = np.logical_and(allLat > latRangeMan[0], allLat < latRangeMan[1])\nindToKeep  = np.logical_and(indToKeep, np.logical_and(allLong > longRangeMan[0], allLong < longRangeMan[1]))\nallLatMan  = allLat[indToKeep]\nallLongMan = allLong[indToKeep]\n\nallLatIndsMan  = (imageSizeMan[0]-1) - (imageSizeMan[0] * (allLatMan  - latRangeMan[0])\n                                                        / (latRangeMan[1] - latRangeMan[0])).astype(int)\nallLongIndsMan =                       (imageSizeMan[1] * (allLongMan - longRangeMan[0])\n                                                        / (longRangeMan[1] - longRangeMan[0])).astype(int)\n\nlocationDensityImageMan = np.zeros(imageSizeMan)\nfor latInd, longInd in zip(allLatIndsMan,allLongIndsMan):\n    locationDensityImageMan[latInd,longInd] += 1\n\nfig, ax = plt.subplots(nrows=1,ncols=1,figsize=(12,18))\nax.imshow(np.log(locationDensityImageMan+1),cmap='inferno')\nax.set_axis_off()\n\n\n\n\n\n\nCluster the Trips and Look at their distribution\n\npickupTime = pd.to_datetime(data_frame['pickup_datetime'])\n\ndata_frame['src hourOfDay'] = (pickupTime.dt.hour*60.0 + pickupTime.dt.minute)   / 60.0\ndata_frame['dst hourOfDay'] = data_frame['src hourOfDay'] + data_frame['duration [min]'] / 60.0\n\ndata_frame['dayOfWeek']     = pickupTime.dt.weekday\ndata_frame['hourOfWeek']    = data_frame['dayOfWeek']*24.0 + data_frame['src hourOfDay']\n\ndata_frame['monthOfYear']   = pickupTime.dt.month\ndata_frame['dayOfYear']     = pickupTime.dt.dayofyear\ndata_frame['weekOfYear']    = pickupTime.dt.weekofyear\ndata_frame['hourOfYear']    = data_frame['dayOfYear']*24.0 + data_frame['src hourOfDay']\n\nFutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n  data_frame['weekOfYear']    = pickupTime.dt.weekofyear\n\n\n\ntripAttributes = np.array(data_frame.loc[:,['src lat [km]','src long [km]','dst lat [km]','dst long [km]','duration [min]']])\nmeanTripAttr = tripAttributes.mean(axis=0)\nstdTripAttr  = tripAttributes.std(axis=0)\ntripAttributes = stats.zscore(tripAttributes, axis=0)\n\nnumClusters = 40\nTripKmeansModel = cluster.MiniBatchKMeans(n_clusters=numClusters, batch_size=120000, n_init=100, random_state=1)\nclusterInds = TripKmeansModel.fit_predict(tripAttributes)\n\nclusterTotalCounts, _ = np.histogram(clusterInds, bins=numClusters)\nsortedClusterInds = np.flipud(np.argsort(clusterTotalCounts))\n\nplt.figure(figsize=(12,4)); plt.title('Cluster Histogram of all trip')\nplt.bar(range(1,numClusters+1),clusterTotalCounts[sortedClusterInds])\nplt.ylabel('Frequency [counts]'); plt.xlabel('Cluster index (sorted by cluster frequency)')\nplt.xlim(0,numClusters+1)\n\n(0.0, 41.0)\n\n\n\n\n\n\n\nPlot typical Trips on the Map\n\ndef ConvertToImageCoords(latCoord, longCoord, latRange, longRange, imageSize):\n    latInds  = imageSize[0] - (imageSize[0] * (latCoord  - latRange[0])  / (latRange[1]  - latRange[0]) ).astype(int)\n    longInds =                (imageSize[1] * (longCoord - longRange[0]) / (longRange[1] - longRange[0])).astype(int)\n\n    return latInds, longInds\n\ntemplateTrips = TripKmeansModel.cluster_centers_ * np.tile(stdTripAttr,(numClusters,1)) + np.tile(meanTripAttr,(numClusters,1))\n\nsrcCoords = templateTrips[:,:2]\ndstCoords = templateTrips[:,2:4]\n\nsrcImCoords = ConvertToImageCoords(srcCoords[:,0],srcCoords[:,1], latRange, longRange, imageSize)\ndstImCoords = ConvertToImageCoords(dstCoords[:,0],dstCoords[:,1], latRange, longRange, imageSize)\n\nplt.figure(figsize=(12,12))\nplt.imshow(np.log(locationDensityImage+1),cmap='inferno'); plt.grid('off')\nplt.scatter(srcImCoords[1],srcImCoords[0],c='m',s=200,alpha=0.8)\nplt.scatter(dstImCoords[1],dstImCoords[0],c='g',s=200,alpha=0.8)\n\nfor i in range(len(srcImCoords[0])):\n    plt.arrow(srcImCoords[1][i],srcImCoords[0][i], dstImCoords[1][i]-srcImCoords[1][i], dstImCoords[0][i]-srcImCoords[0][i],\n              edgecolor='c', facecolor='c', width=0.8,alpha=0.4,head_width=10.0,head_length=10.0,length\n\nSyntaxError: unexpected EOF while parsing (<ipython-input-22-39197d099a2f>, line 22)\n\n\n\n\nCalculate the trip distribution for different hours of the weekday\n\nhoursOfDay = np.sort(data_frame['src hourOfDay'].astype(int).unique())\nclusterDistributionHourOfDay_weekday = np.zeros((len(hoursOfDay),numClusters))\nfor k, hour in enumerate(hoursOfDay):\n    slectedInds = (data_frame['src hourOfDay'].astype(int) == hour) & (data_frame['dayOfWeek'] <= 4)\n    currDistribution, _ = np.histogram(clusterInds[slectedInds], bins=numClusters)\n    clusterDistributionHourOfDay_weekday[k,:] = currDistribution[sortedClusterInds]\n\nfig, ax = plt.subplots(nrows=1,ncols=1,figsize=(12,6))\nax.set_title('Trip Distribution during Weekdays', fontsize=12)\nax.imshow(clusterDistributionHourOfDay_weekday); ax.grid('off')\nax.set_xlabel('Trip Cluster'); ax.set_ylabel('Hour of Day')\n\n\n\nCalculate the trip distribution for different hours of the weekend\n\nhoursOfDay = np.sort(data_frame['src hourOfDay'].astype(int).unique())\nclusterDistributionHourOfDay_weekend = np.zeros((len(hoursOfDay),numClusters))\nfor k, hour in enumerate(hoursOfDay):\n    slectedInds = (data_frame['src hourOfDay'].astype(int) == hour) & (data_frame['dayOfWeek'] >= 5)\n    currDistribution, _ = np.histogram(clusterInds[slectedInds], bins=numClusters)\n    clusterDistributionHourOfDay_weekend[k,:] = currDistribution[sortedClusterInds]\n\nfig, ax = plt.subplots(nrows=1,ncols=1,figsize=(12,6))\nax.set_title('Trip Distribution during Weekends', fontsize=12)\nax.imshow(clusterDistributionHourOfDay_weekend); ax.grid('off')\nax.set_xlabel('Trip Cluster'); ax.set_ylabel('Hour of Day')\n\n\n\nCalculate the trip distribution for day of week\n\ndaysOfWeek = np.sort(data_frame['dayOfWeek'].unique())\nclusterDistributionDayOfWeek = np.zeros((len(daysOfWeek),numClusters))\nfor k, day in enumerate(daysOfWeek):\n    slectedInds = data_frame['dayOfWeek'] == day\n    currDistribution, _ = np.histogram(clusterInds[slectedInds], bins=numClusters)\n    clusterDistributionDayOfWeek[k,:] = currDistribution[sortedClusterInds]\n\nplt.figure(figsize=(12,5)); plt.title('Trip Distribution throughout the Week')\nplt.imshow(clusterDistributionDayOfWeek); plt.grid('off')\nplt.xlabel('Trip Cluster'); plt.ylabel('Day of Week')\n\n\n\nCalculate the trip distribution for day of year\n\ndaysOfYear = data_frame['dayOfYear'].unique()\ndaysOfYear = np.sort(daysOfYear)\nclusterDistributionDayOfYear = np.zeros((len(daysOfYear),numClusters))\nfor k, day in enumerate(daysOfYear):\n    slectedInds = data_frame['dayOfYear'] == day\n    currDistribution, _ = np.histogram(clusterInds[slectedInds], bins=numClusters)\n    clusterDistributionDayOfYear[k,:] = currDistribution[sortedClusterInds]\n\nfig, ax = plt.subplots(nrows=1,ncols=1,figsize=(10,16))\nax.set_title('Trip Distribution throughout the Year', fontsize=12)\nax.imshow(clusterDistributionDayOfYear); ax.grid('off')\nax.set_xlabel('Trip Cluster'); ax.set_ylabel('Day of Year')\nax.annotate('Large Snowstorm', color='r', fontsize=15 ,xy=(5, 21), xytext=(20, 17),\n            arrowprops=dict(facecolor='red', shrink=0.03))\nax.annotate('Memorial Day', color='r', fontsize=15, xy=(5, 151), xytext=(20, 157),\n            arrowprops=dict(facecolor='red', shrink=0.03))\n\n\n\nComputing PCA coefficients\n\nhoursOfYear = np.sort(data_frame['hourOfYear'].astype(int).unique())\nclusterDistributionHourOfYear = np.zeros((len(range(hoursOfYear[0],hoursOfYear[-1])),numClusters))\ndayOfYearVec  = np.zeros(clusterDistributionHourOfYear.shape[0])\nweekdayVec    = np.zeros(clusterDistributionHourOfYear.shape[0])\nweekOfYearVec = np.zeros(clusterDistributionHourOfYear.shape[0])\nfor k, hour in enumerate(hoursOfYear):\n    slectedInds = data_frame['hourOfYear'].astype(int) == hour\n    currDistribution, _ = np.histogram(clusterInds[slectedInds], bins=numClusters)\n    clusterDistributionHourOfYear[k,:] = currDistribution[sortedClusterInds]\n\n    dayOfYearVec[k]  = data_frame[slectedInds]['dayOfYear'].mean()\n    weekdayVec[k]    = data_frame[slectedInds]['dayOfWeek'].mean()\n    weekOfYearVec[k] = data_frame[slectedInds]['weekOfYear'].mean()\n\nnumComponents = 3\nTripDistributionPCAModel = decomposition.PCA(n_components=numComponents,whiten=True, random_state=1)\ncompactClusterDistributionHourOfYear = TripDistributionPCAModel.fit_transform(clusterDistributionHourOfYear)\n\n\n\nCollect traces for all weeks of year\n\nlistOfFullWeeks = []\nfor uniqueVal in np.unique(weekOfYearVec):\n    if (weekOfYearVec == uniqueVal).sum() == 24*7:\n        listOfFullWeeks.append(uniqueVal)\n\nweeklyTraces = np.zeros((24*7,numComponents,len(listOfFullWeeks)))\nfor k, weekInd in enumerate(listOfFullWeeks):\n    weeklyTraces[:,:,k] = compactClusterDistributionHourOfYear[weekOfYearVec == weekInd,:]\n\nfig, axArray = plt.subplots(nrows=numComponents,ncols=1,sharex=True, figsize=(10,10))\nfig.suptitle('PCA coefficients during the Week', fontsize=25)\nfor PC_coeff in range(numComponents):\n    meanTrace = weeklyTraces[:,PC_coeff,:].mean(axis=1)\n    axArray[PC_coeff].plot(weeklyTraces[:,PC_coeff,:],'red',linewidth=1.5)\n    axArray[PC_coeff].plot(meanTrace,'k',linewidth=2.5)\n    axArray[PC_coeff].set_ylabel('PC %d coeff' %(PC_coeff+1))\n    axArray[PC_coeff].vlines([0,23,47,71,95,119,143,167], weeklyTraces[:,PC_coeff,:].min(), weeklyTraces[:,PC_coeff,:].max(), colors='black', lw=2)\n\naxArray[PC_coeff].set_xlabel('hours since start of week')\naxArray[PC_coeff].set_xlim(-0.9,24*7-0.1)\n\n\n\nExamine what different PC coefficients mean by looking at their trip template distributions\n\nfig, axArray = plt.subplots(nrows=numComponents,ncols=1,sharex=True, figsize=(12,11))\nfig.suptitle('Trip Distribution PCA Components', fontsize=25)\nfor PC_coeff in range(numComponents):\n    tripTemplateDistributionDifference = TripDistributionPCAModel.components_[PC_coeff,:] * \\\n                                         TripDistributionPCAModel.explained_variance_[PC_coeff]\n    axArray[PC_coeff].bar(range(1,numClusters+1),tripTemplateDistributionDifference)\n    axArray[PC_coeff].set_title('PCA %d component' %(PC_coeff+1))\n    axArray[PC_coeff].set_ylabel('delta frequency [counts]')\n    \naxArray[PC_coeff].set_xlabel('cluster index (sorted by cluster frequency)')\naxArray[PC_coeff].set_xlim(0,numClusters+0.5)\n\naxArray[1].hlines([-25,25], 0, numClusters+0.5, colors='r', lw=0.7)\naxArray[2].hlines([-11,11], 0, numClusters+0.5, colors='r', lw=0.7)\n\nWe can see that the first PCA component looks very similar to the overall trip distribution, suggesting that it’s mainly a “gain” component that controls just the number of total trips in that period of time."
  },
  {
    "objectID": "Data_Mining/2022-05-26-exercise-manipulating-geospatial-data.html",
    "href": "Data_Mining/2022-05-26-exercise-manipulating-geospatial-data.html",
    "title": "Manipulating Geospatial Data",
    "section": "",
    "text": "This notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link.\n\n\nimport math\nimport pandas as pd\nimport geopandas as gpd\n#from geopy.geocoders import Nominatim            # What you'd normally run\nfrom learntools.geospatial.tools import Nominatim # Just for this exercise\n\nimport folium \nfrom folium import Marker\nfrom folium.plugins import MarkerCluster\n\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.geospatial.ex4 import *\n\nYou’ll use the embed_map() function from the previous exercise to visualize your maps.\n\ndef embed_map(m, file_name):\n    from IPython.display import IFrame\n    m.save(file_name)\n    return IFrame(file_name, width='100%', height='500px')\n\n\nExercises\n\n1) Geocode the missing locations.\nRun the next code cell to create a DataFrame starbucks containing Starbucks locations in the state of California.\n\n# Load and preview Starbucks locations in California\nstarbucks = pd.read_csv(\"../input/geospatial-learn-course-data/starbucks_locations.csv\")\nstarbucks.head()\n\nMost of the stores have known (latitude, longitude) locations. But, all of the locations in the city of Berkeley are missing.\n\n# How many rows in each column have missing values?\nprint(starbucks.isnull().sum())\n\n# View rows with missing locations\nrows_with_missing = starbucks[starbucks[\"City\"]==\"Berkeley\"]\nrows_with_missing\n\nUse the code cell below to fill in these values with the Nominatim geocoder.\nNote that in the tutorial, we used Nominatim() (from geopy.geocoders) to geocode values, and this is what you can use in your own projects outside of this course.\n튜토리얼에서 우리는 값을 지오코딩하기 위해 Nominatim()(geopy.geocoders에서)을 사용했으며 이것은 이 과정 이외의 자신의 프로젝트에서 사용할 수 있는 것입니다.\nIn this exercise, you will use a slightly different function Nominatim() (from learntools.geospatial.tools). This function was imported at the top of the notebook and works identically to the function from GeoPandas.\n이 연습에서는 약간 다른 함수 Nominatim()(learntools.geospatial.tools에서)을 사용합니다. 이 기능은 노트북 상단에서 가져온 것으로 GeoPandas의 기능과 동일하게 작동합니다.\nSo, in other words, as long as: - you don’t change the import statements at the top of the notebook, and - you call the geocoding function as geocode() in the code cell below,\nyour code will work as intended!\n\n# Create the geocoder\ngeolocator = Nominatim(user_agent=\"kaggle_learn\")\n\n# Your code here\n\ndef my_geocoder(row):\n    point = geolocator.geocode(row).point\n    return pd.Series({'Latitude': point.latitude, 'Longitude': point.longitude})\n\nberkeley_locations = rows_with_missing.apply(lambda x: my_geocoder(x['Address']), axis=1)\nstarbucks.update(berkeley_locations)\n\n# Check your answer\nq_1.check()\n\n\n# Line below will give you solution code\n#q_1.solution()\n\n\n\n2) View Berkeley locations.\nLet’s take a look at the locations you just found. Visualize the (latitude, longitude) locations in Berkeley in the OpenStreetMap style.\n방금 찾은 위치를 살펴보겠습니다. OpenStreetMap 스타일로 버클리의 (위도, 경도) 위치를 시각화합니다.\n\n# Create a base map\nm_2 = folium.Map(location=[37.88,-122.26], zoom_start=13)\n\n# Your code here: Add a marker for each Berkeley location\nfor idx, row in starbucks[starbucks[\"City\"]=='Berkeley'].iterrows():\n    Marker([row['Latitude'], row['Longitude']]).add_to(m_2)\n\n# Uncomment to see a hint\n#q_2.a.hint()\n\n# Show the map\nembed_map(m_2, 'q_2.html')\n\n\n# Get credit for your work after you have created a map\nq_2.a.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_2.a.solution()\n\nConsidering only the five locations in Berkeley, how many of the (latitude, longitude) locations seem potentially correct (are located in the correct city)?\n\n# View the solution (Run this code cell to receive credit!)\nq_2.b.solution()\n\n\n\n3) Consolidate your data.\nRun the code below to load a GeoDataFrame CA_counties containing the name, area (in square kilometers), and a unique id (in the “GEOID” column) for each county in the state of California. The “geometry” column contains a polygon with county boundaries.\n\nCA_counties = gpd.read_file(\"../input/geospatial-learn-course-data/CA_county_boundaries/CA_county_boundaries/CA_county_boundaries.shp\")\nCA_counties.head()\n\nNext, we create three DataFrames: - CA_pop contains an estimate of the population of each county. - CA_high_earners contains the number of households with an income of at least $150,000 per year. - CA_median_age contains the median age for each county.\n\nCA_pop = pd.read_csv(\"../input/geospatial-learn-course-data/CA_county_population.csv\", index_col=\"GEOID\")\nCA_high_earners = pd.read_csv(\"../input/geospatial-learn-course-data/CA_county_high_earners.csv\", index_col=\"GEOID\")\nCA_median_age = pd.read_csv(\"../input/geospatial-learn-course-data/CA_county_median_age.csv\", index_col=\"GEOID\")\n\nUse the next code cell to join the CA_counties GeoDataFrame with CA_pop, CA_high_earners, and CA_median_age.\nName the resultant GeoDataFrame CA_stats, and make sure it has 8 columns: “GEOID”, “name”, “area_sqkm”, “geometry”, “population”, “high_earners”, and “median_age”. Also, make sure the CRS is set to {'init': 'epsg:4326'}.\n결과 GeoDataFrame의 이름을 CA_stats로 지정하고 “GEOID”, “name”, “area_sqkm”, “geometry”, “population”, “high_earners” 및 “median_age”의 8개 열이 있는지 확인합니다. 또한 CRS가 {'init': 'epsg:4326'}으로 설정되어 있는지 확인하십시오.\n\n# Your code here\ncols_to_add = CA_pop.join([CA_high_earners, CA_median_age]).reset_index()\nCA_stats = CA_counties.merge(cols_to_add, on=\"GEOID\")\n\n# Check your answer\nq_3.check()\n\n\n# Lines below will give you a hint or solution code\n#q_3.hint()\n#q_3.solution()\n\nNow that we have all of the data in one place, it’s much easier to calculate statistics that use a combination of columns. Run the next code cell to create a “density” column with the population density.\n\nCA_stats[\"density\"] = CA_stats[\"population\"] / CA_stats[\"area_sqkm\"]\n\n\n\n4) Which counties look promising?\nCollapsing all of the information into a single GeoDataFrame also makes it much easier to select counties that meet specific criteria.\nUse the next code cell to create a GeoDataFrame sel_counties that contains a subset of the rows (and all of the columns) from the CA_stats GeoDataFrame. In particular, you should select counties where:\n다음 코드 셀을 사용하여 CA_stats GeoDataFrame에서 행(및 모든 열)의 하위 집합을 포함하는 GeoDataFrame sel_counties를 만듭니다. 특히 다음과 같은 카운티를 선택해야 합니다.\n\nthere are at least 100,000 households making $150,000 per year,\nthe median age is less than 38.5, and\nthe density of inhabitants is at least 285 (per square kilometer).\n\nAdditionally, selected counties should satisfy at least one of the following criteria: - there are at least 500,000 households making $150,000 per year, - the median age is less than 35.5, or - the density of inhabitants is at least 1400 (per square kilometer).\n\n# Your code here\nsel_counties = CA_stats[((CA_stats.high_earners > 100000) &\n                         (CA_stats.median_age < 38.5) &\n                         (CA_stats.density > 285) &\n                         ((CA_stats.median_age < 35.5) |\n                         (CA_stats.density > 1400) |\n                         (CA_stats.high_earners > 500000)))]\n\n# Check your answer\nq_4.check()\n\n\n# Lines below will give you a hint or solution code\n#q_4.hint()\n#q_4.solution()\n\n\n\n5) How many stores did you identify?\nWhen looking for the next Starbucks Reserve Roastery location, you’d like to consider all of the stores within the counties that you selected. So, how many stores are within the selected counties?\nTo prepare to answer this question, run the next code cell to create a GeoDataFrame starbucks_gdf with all of the starbucks locations.\n\nstarbucks_gdf = gpd.GeoDataFrame(starbucks, geometry=gpd.points_from_xy(starbucks.Longitude, starbucks.Latitude))\nstarbucks_gdf.crs = {'init': 'epsg:4326'}\n\nSo, how many stores are in the counties you selected?\n그렇다면 선택한 카운티에는 몇 개의 매장이 있습니까?\n\n# Fill in your answer\nlocations_of_interest = gpd.sjoin(starbucks_gdf, sel_counties)\nnum_stores = len(locations_of_interest)\n\n# Check your answer\nq_5.check()\n\n\n# Lines below will give you a hint or solution code\n#q_5.hint()\n#q_5.solution()\n\n\n\n6) Visualize the store locations.\nCreate a map that shows the locations of the stores that you identified in the previous question.\n이전 질문에서 식별한 상점의 위치를 ​​보여주는 지도를 만드십시오.\n\n# Create a base map\nm_6 = folium.Map(location=[37,-120], zoom_start=6)\n\n# Your code here: show selected store locations\nmc = MarkerCluster()\n\nlocations_of_interest = gpd.sjoin(starbucks_gdf, sel_counties)\nfor idx, row in locations_of_interest.iterrows():\n    if not math.isnan(row['Longitude']) and not math.isnan(row['Latitude']):\n        mc.add_child(folium.Marker([row['Latitude'], row['Longitude']]))\n\nm_6.add_child(mc)\n\n# Uncomment to see a hint\n#q_6.hint()\n\n# Show the map\nembed_map(m_6, 'q_6.html')\n\n\n# Get credit for your work after you have created a map\n#q_6.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_6.solution()\n\n\n\n\nKeep going\nLearn about how proximity analysis can help you to understand the relationships between points on a map.\n\nHave questions or comments? Visit the course discussion forum to chat with other learners."
  },
  {
    "objectID": "Data_Mining/2022-05-26-exercise-proximity-analysis.html",
    "href": "Data_Mining/2022-05-26-exercise-proximity-analysis.html",
    "title": "Proximity Analysis",
    "section": "",
    "text": "This notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link.\n\n\nimport math\nimport geopandas as gpd\nimport pandas as pd\nfrom shapely.geometry import MultiPolygon\n\nimport folium\nfrom folium import Choropleth, Marker\nfrom folium.plugins import HeatMap, MarkerCluster\n\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.geospatial.ex5 import *\n\nYou’ll use the embed_map() function to visualize your maps.\n\ndef embed_map(m, file_name):\n    from IPython.display import IFrame\n    m.save(file_name)\n    return IFrame(file_name, width='100%', height='500px')\n\n\nExercises\n\n1) Visualize the collision data.\nRun the code cell below to load a GeoDataFrame collisions tracking major motor vehicle collisions in 2013-2018.\n\ncollisions = gpd.read_file(\"../input/geospatial-learn-course-data/NYPD_Motor_Vehicle_Collisions/NYPD_Motor_Vehicle_Collisions/NYPD_Motor_Vehicle_Collisions.shp\")\ncollisions.head()\n\nUse the “LATITUDE” and “LONGITUDE” columns to create an interactive map to visualize the collision data. What type of map do you think is most effective?\n“LATITUDE” 및 “LONGITUDE” 열을 사용하여 충돌 데이터를 시각화하는 대화형 맵을 만듭니다. 어떤 유형의 지도가 가장 효과적이라고 생각하십니까?\n\nm_1 = folium.Map(location=[40.7, -74], zoom_start=11) \n\n# Your code here: Visualize the collision data\nHeatMap(data=collisions[['LATITUDE', 'LONGITUDE']], radius=9).add_to(m_1)\n\n# Uncomment to see a hint\n#q_1.hint()\n\n# Show the map\nembed_map(m_1, \"q_1.html\")\n\n\n# Get credit for your work after you have created a map\n#q_1.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_1.solution()\n\n\n\n2) Understand hospital coverage.\nRun the next code cell to load the hospital data.\n\nhospitals = gpd.read_file(\"../input/geospatial-learn-course-data/nyu_2451_34494/nyu_2451_34494/nyu_2451_34494.shp\")\nhospitals.head()\n\nUse the “latitude” and “longitude” columns to visualize the hospital locations.\n“위도” 및 “경도” 열을 사용하여 병원 위치를 시각화합니다.\n\nm_2 = folium.Map(location=[40.7, -74], zoom_start=11) \n\n# Your code here: Visualize the hospital locations\nfor idx, row in hospitals.iterrows():\n    Marker([row['latitude'], row['longitude']], popup=row['name']).add_to(m_2)\n    \n# Uncomment to see a hint\n#q_2.hint()\n        \n# Show the map\nembed_map(m_2, \"q_2.html\")\n\n\n# Get credit for your work after you have created a map\nq_2.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_2.solution()\n\n\n\n3) When was the closest hospital more than 10 kilometers away?\nCreate a DataFrame outside_range containing all rows from collisions with crashes that occurred more than 10 kilometers from the closest hospital.\n가장 가까운 병원에서 10km 이상 떨어진 곳에서 발생한 충돌이 있는 ’충돌’의 모든 행을 포함하는 DataFrame ’outside_range’를 만듭니다.\nNote that both hospitals and collisions have EPSG 2263 as the coordinate reference system, and EPSG 2263 has units of meters.\n‘병원’과 ’충돌’ 모두 좌표 참조 시스템으로 EPSG 2263을 사용하고 EPSG 2263은 미터 단위를 사용합니다.\n\n# Your code here\ncoverage = gpd.GeoDataFrame(geometry=hospitals.geometry).buffer(10000)\nmy_union = coverage.geometry.unary_union\noutside_range = collisions.loc[~collisions[\"geometry\"].apply(lambda x: my_union.contains(x))]\n\n# Check your answer\nq_3.check()\n\n\n# Lines below will give you a hint or solution code\n#q_3.hint()\n#q_3.solution()\n\nThe next code cell calculates the percentage of collisions that occurred more than 10 kilometers away from the closest hospital.\n\npercentage = round(100*len(outside_range)/len(collisions), 2)\nprint(\"Percentage of collisions more than 10 km away from the closest hospital: {}%\".format(percentage))\n\n\n\n4) Make a recommender.\nWhen collisions occur in distant locations, it becomes even more vital that injured persons are transported to the nearest available hospital.\n멀리 떨어진 곳에서 충돌이 발생하면 부상자를 가장 가까운 병원으로 이송하는 것이 더욱 중요해집니다.\nWith this in mind, you decide to create a recommender that: - takes the location of the crash (in EPSG 2263) as input, - finds the closest hospital (where distance calculations are done in EPSG 2263), and - returns the name of the closest hospital.\n\ndef best_hospital(collision_location):\n    idx_min = hospitals.geometry.distance(collision_location).idxmin()\n    my_hospital = hospitals.iloc[idx_min]\n    # Your code here\n    name = my_hospital[\"name\"]\n    return name\n\n# Test your function: this should suggest CALVARY HOSPITAL INC\nprint(best_hospital(outside_range.geometry.iloc[0]))\n\n# Check your answer\nq_4.check()\n\n\n# Lines below will give you a hint or solution code\n#q_4.hint()\n#q_4.solution()\n\n\n\n5) Which hospital is under the highest demand?\nConsidering only collisions in the outside_range DataFrame, which hospital is most recommended?\noutside_range DataFrame에서 충돌만 고려한다면 어느 병원을 가장 추천하는가?\nYour answer should be a Python string that exactly matches the name of the hospital returned by the function you created in 4).\n\n# Your code here\nhighest_demand = outside_range.geometry.apply(best_hospital).value_counts().idxmax()\n\n# Check your answer\nq_5.check()\n\n\n# Lines below will give you a hint or solution code\n#q_5.hint()\n#q_5.solution()\n\n\n\n6) Where should the city construct new hospitals?\nRun the next code cell (without changes) to visualize hospital locations, in addition to collisions that occurred more than 10 kilometers away from the closest hospital.\n다음 코드 셀(변경 없이)을 실행하여 가장 가까운 병원에서 10km 이상 떨어진 곳에서 발생한 충돌 외에도 병원 위치를 시각화합니다.\n\nm_6 = folium.Map(location=[40.7, -74], zoom_start=11) \n\ncoverage = gpd.GeoDataFrame(geometry=hospitals.geometry).buffer(10000)\nfolium.GeoJson(coverage.geometry.to_crs(epsg=4326)).add_to(m_6)\nHeatMap(data=outside_range[['LATITUDE', 'LONGITUDE']], radius=9).add_to(m_6)\nfolium.LatLngPopup().add_to(m_6)\n\nembed_map(m_6, 'm_6.html')\n\nClick anywhere on the map to see a pop-up with the corresponding location in latitude and longitude.\nThe city of New York reaches out to you for help with deciding locations for two brand new hospitals. They specifically want your help with identifying locations to bring the calculated percentage from step 3) to less than ten percent. Using the map (and without worrying about zoning laws or what potential buildings would have to be removed in order to build the hospitals), can you identify two locations that would help the city accomplish this goal?\nPut the proposed latitude and longitude for hospital 1 in lat_1 and long_1, respectively. (Likewise for hospital 2.)\n병원 1에 대해 제안된 위도와 경도를 각각 lat_1과 long_1에 넣습니다. (병원2도 마찬가지)\nThen, run the rest of the cell as-is to see the effect of the new hospitals. Your answer will be marked correct, if the two new hospitals bring the percentage to less than ten percent.\n그런 다음 나머지 셀을 그대로 실행하여 새 병원의 효과를 확인하십시오. 두 개의 새로운 병원에서 백분율을 10% 미만으로 낮추면 답이 정답으로 표시됩니다.\n\n# Your answer here: proposed location of hospital 1\nlat_1 = 40.6714\nlong_1 = -73.8492\n\n# Your answer here: proposed location of hospital 2\nlat_2 = 40.6702\nlong_2 = -73.7612\n\n# Do not modify the code below this line\ntry:\n    new_df = pd.DataFrame(\n        {'Latitude': [lat_1, lat_2],\n         'Longitude': [long_1, long_2]})\n    new_gdf = gpd.GeoDataFrame(new_df, geometry=gpd.points_from_xy(new_df.Longitude, new_df.Latitude))\n    new_gdf.crs = {'init' :'epsg:4326'}\n    new_gdf = new_gdf.to_crs(epsg=2263)\n    # get new percentage\n    new_coverage = gpd.GeoDataFrame(geometry=new_gdf.geometry).buffer(10000)\n    new_my_union = new_coverage.geometry.unary_union\n    new_outside_range = outside_range.loc[~outside_range[\"geometry\"].apply(lambda x: new_my_union.contains(x))]\n    new_percentage = round(100*len(new_outside_range)/len(collisions), 2)\n    print(\"(NEW) Percentage of collisions more than 10 km away from the closest hospital: {}%\".format(new_percentage))\n    # Did you help the city to meet its goal?\n    q_6.check()\n    # make the map\n    m = folium.Map(location=[40.7, -74], zoom_start=11) \n    folium.GeoJson(coverage.geometry.to_crs(epsg=4326)).add_to(m)\n    folium.GeoJson(new_coverage.geometry.to_crs(epsg=4326)).add_to(m)\n    for idx, row in new_gdf.iterrows():\n        Marker([row['Latitude'], row['Longitude']]).add_to(m)\n    HeatMap(data=new_outside_range[['LATITUDE', 'LONGITUDE']], radius=9).add_to(m)\n    folium.LatLngPopup().add_to(m)\n    display(embed_map(m, 'q_6.html'))\nexcept:\n    q_6.hint()\n\n\n# Uncomment to see one potential answer \n#q_6.solution()\n\n\n\n\nCongratulations!\nYou have just completed the Geospatial Analysis micro-course! Great job!\n\nHave questions or comments? Visit the course discussion forum to chat with other learners."
  },
  {
    "objectID": "Data_Mining/2022-06-07-last.html",
    "href": "Data_Mining/2022-06-07-last.html",
    "title": "COVID-19 Analysis & Visualization",
    "section": "",
    "text": "서론 \n\n분석 배경 및 목적 \n\n분석 배경 \n분석 배경 - 2022년 6월 현재 코로나의 상황은 매일 10000명의 확진자가 나오고 있는 상황이지만 코로나가 처음 발병하고 나서와는 조금은 다른 반응이다. 최근 정부에서는 집단 면역이 90% 이상 형성이 되어있으며 확진자의 추세 또한 감소세를 보이고 있는 상황에서 2020년 01월부터 2020년 06월까지 수집된 해당 데이터를 기반으로 과연 과거와 현재의 차이는 얼마나 있고 당시 정부와 뉴스에서 주장하던 코로나에 대한 정보는 과연 타당하였고 올바른 정보였는지 궁금하여 해당 주제를 선정하여 분석을 진행하게 되었습니다.\n\n\n분석 목적 \n분석 목적 - 자료를 제공한 데이콘에서는 해당 자료들은 이용해서 다음과 같은 인공지능 AI를 활용 코로나 확산 방지와 예방을 위한 인사이트 / 시각화 발굴. 이라는 목적을 가지고 진행을 하였습니다. 해서 저는 당시 기간동안 가장 많이 확진된 연령층과 주된 감염 원인과 그 이유에 대해서 알아보고, 어떤 연령층에게 가장 치명적인 질병이었느지와 당시 정부의 방역 대책은 타당하였는지 에 대해서 목적을 가지고 해당 분석을 진행하였습니다.\n\n\n\n데이터 소개 \n\n\n데이터 카테고리 \n\nCase Data\n\n\nCase: 한국의 COVID-19 감염 사례 데이터\n\n\nPatient Data\n\n\nPatientInfo: 한국의 코로나19 환자 역학 데이터\nPatientRoute: 국내 코로나19 환자 경로 데이터\n\n\nTime Series Data\n\n\nTime: 한국의 코로나19 상태의 시계열 데이터\nTimeAge: 한국의 연령별 코로나19 현황 시계열 데이터\nTimeGender: 한국의 성별에 따른 코로나19 현황의 시계열 데이터\nTimeProvince: 한국의 지역별 코로나19 현황 시계열 데이터\n\n\nAdditional Data\n\n\nRegion: 대한민국 내 지역의 위치 및 통계 자료\nWeather: 한국 지역의 날씨 데이터\nSearchTrend: 국내 최대 포털사이트 네이버에서 검색된 키워드의 트렌드 데이터\nSeoulFloating: 대한민국 서울 유동인구 데이터(SK텔레콤 빅데이터 허브에서)\nPolicy: 한국의 코로나19에 대한 정부 정책 데이터\n\n\n\n데이터 형태 \n\n색상이 의미하는 것은 비슷한 속성을 가지고 있다는 것입니다.\n열 사이에 선이 연결되어 있다는 것은 열의 값이 부분적으로 공유됨을 의미합니다.\n점선은 약한 관련성을 의미합니다.\n\nhttps://user-images.githubusercontent.com/50820635/86225695-8dca0580-bbc5-11ea-9e9b-b0ca33414d8a.PNG\n\n\n데이터 세부 설명 \n\nimport numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('../notebook/coronavirusdataset'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n../notebook/coronavirusdataset\\Case.csv\n../notebook/coronavirusdataset\\PatientInfo.csv\n../notebook/coronavirusdataset\\PatientRoute.csv\n../notebook/coronavirusdataset\\Policy.csv\n../notebook/coronavirusdataset\\Region.csv\n../notebook/coronavirusdataset\\SearchTrend.csv\n../notebook/coronavirusdataset\\SeoulFloating.csv\n../notebook/coronavirusdataset\\Time.csv\n../notebook/coronavirusdataset\\TimeAge.csv\n../notebook/coronavirusdataset\\TimeGender.csv\n../notebook/coronavirusdataset\\TimeProvince.csv\n../notebook/coronavirusdataset\\Weather.csv\n\n\n\npath = '../notebook/coronavirusdataset/'\n\ncase = p_info = pd.read_csv(path+'Case.csv')\npatientinfo = pd.read_csv(path+'PatientInfo.csv')\npatientroute = pd.read_csv(path+'PatientRoute.csv')\ntime = pd.read_csv(path+'Time.csv')\ntimeage = pd.read_csv(path+'TimeAge.csv')\ntimegender = pd.read_csv(path+'TimeGender.csv')\ntimeprovince = pd.read_csv(path+'TimeProvince.csv')\nregion = pd.read_csv(path+'Region.csv')\nweather = pd.read_csv(path+'Weather.csv')\nsearchtrend = pd.read_csv(path+'SearchTrend.csv')\nseoulfloating = pd.read_csv(path+'SeoulFloating.csv')\npolicy = pd.read_csv(path+'Policy.csv')\n\n\nCase\n\n한국의 COVID-19 감염 사례 데이터\n\ncase_id: the ID of the infection case\n\n\ncase_id(7) = region_code(5) + case_number(2)\nYou can check the region_code in ‘Region.csv’\nprovince: Special City / Metropolitan City / Province(-do)\ncity: City(-si) / Country (-gun) / District (-gu)\n\nThe value ‘from other city’ means that where the group infection started is other city.\n\ngroup: TRUE: group infection / FALSE: not group\n\nIf the value is ‘TRUE’ in this column, the value of ‘infection_cases’ means the name of group.\nThe values named ‘contact with patient’, ‘overseas inflow’ and ‘etc’ are not group infection.\n\ninfection_case: the infection case (the name of group or other cases)\n\nThe value ‘overseas inflow’ means that the infection is from other country.\nThe value ‘etc’ includes individual cases, cases where relevance classification is ongoing after investigation, and cases under investigation.\n\nconfirmed: the accumulated number of the confirmed\nlatitude: the latitude of the group (WGS84)\nlongitude: the longitude of the group (WGS84)\n\n\ncase.head()\n\n\n\n\n\n  \n    \n      \n      case_id\n      province\n      city\n      group\n      infection_case\n      confirmed\n      latitude\n      longitude\n    \n  \n  \n    \n      0\n      1000001\n      Seoul\n      Yongsan-gu\n      True\n      Itaewon Clubs\n      139\n      37.538621\n      126.992652\n    \n    \n      1\n      1000002\n      Seoul\n      Gwanak-gu\n      True\n      Richway\n      119\n      37.48208\n      126.901384\n    \n    \n      2\n      1000003\n      Seoul\n      Guro-gu\n      True\n      Guro-gu Call Center\n      95\n      37.508163\n      126.884387\n    \n    \n      3\n      1000004\n      Seoul\n      Yangcheon-gu\n      True\n      Yangcheon Table Tennis Club\n      43\n      37.546061\n      126.874209\n    \n    \n      4\n      1000005\n      Seoul\n      Dobong-gu\n      True\n      Day Care Center\n      43\n      37.679422\n      127.044374\n    \n  \n\n\n\n\n\nPatientInfo\n\n한국의 코로나19 환자 역학 데이터\n\npatient_id: the ID of the patient\n\n\npatient_id(10) = region_code(5) + patient_number(5)\nYou can check the region_code in ‘Region.csv’\nThere are two types of the patient_number\n\nlocal_num: The number given by the local government.\nglobal_num: The number given by the KCDC\n\nsex: the sex of the patient\nage: the age of the patient\n\n0s: 0 ~ 9\n10s: 10 ~ 19 …\n90s: 90 ~ 99\n100s: 100 ~ 109\n\ncountry: the country of the patient\nprovince: the province of the patient\ncity: the city of the patient\ninfection_case: the case of infection\ninfected_by: the ID of who infected the patient\n\nThis column refers to the ‘patient_id’ column.\n\ncontact_number: the number of contacts with people\nsymptom_onset_date: the date of symptom onset\nconfirmed_date: the date of being confirmed\nreleased_date: the date of being released\ndeceased_date: the date of being deceased\nstate: isolated / released / deceased\n\nisolated: being isolated in the hospital\nreleased: being released from the hospital\ndeceased: being deceased\n\n\n\npatientinfo.head()\n\n\n\n\n\n  \n    \n      \n      patient_id\n      sex\n      age\n      country\n      province\n      city\n      infection_case\n      infected_by\n      contact_number\n      symptom_onset_date\n      confirmed_date\n      released_date\n      deceased_date\n      state\n    \n  \n  \n    \n      0\n      1000000001\n      male\n      50s\n      Korea\n      Seoul\n      Gangseo-gu\n      overseas inflow\n      NaN\n      75\n      2020-01-22\n      2020-01-23\n      2020-02-05\n      NaN\n      released\n    \n    \n      1\n      1000000002\n      male\n      30s\n      Korea\n      Seoul\n      Jungnang-gu\n      overseas inflow\n      NaN\n      31\n      NaN\n      2020-01-30\n      2020-03-02\n      NaN\n      released\n    \n    \n      2\n      1000000003\n      male\n      50s\n      Korea\n      Seoul\n      Jongno-gu\n      contact with patient\n      2002000001\n      17\n      NaN\n      2020-01-30\n      2020-02-19\n      NaN\n      released\n    \n    \n      3\n      1000000004\n      male\n      20s\n      Korea\n      Seoul\n      Mapo-gu\n      overseas inflow\n      NaN\n      9\n      2020-01-26\n      2020-01-30\n      2020-02-15\n      NaN\n      released\n    \n    \n      4\n      1000000005\n      female\n      20s\n      Korea\n      Seoul\n      Seongbuk-gu\n      contact with patient\n      1000000002\n      2\n      NaN\n      2020-01-31\n      2020-02-24\n      NaN\n      released\n    \n  \n\n\n\n\n\nPatientRoute\n\n한국의 코로나19 환자 경로 데이터\n\npatient_id: the ID of the patient\ndate: YYYY-MM-DD\nprovince: Special City / Metropolitan City / Province(-do)\ncity: City(-si) / Country (-gun) / District (-gu)\nlatitude: the latitude of the visit (WGS84)\nlongitude: the longitude of the visit (WGS84)\n\n\npatientroute.head()\n\n\n\n\n\n  \n    \n      \n      patient_id\n      global_num\n      date\n      province\n      city\n      type\n      latitude\n      longitude\n    \n  \n  \n    \n      0\n      1000000001\n      2.0\n      2020-01-22\n      Gyeonggi-do\n      Gimpo-si\n      airport\n      37.615246\n      126.715632\n    \n    \n      1\n      1000000001\n      2.0\n      2020-01-24\n      Seoul\n      Jung-gu\n      hospital\n      37.567241\n      127.005659\n    \n    \n      2\n      1000000002\n      5.0\n      2020-01-25\n      Seoul\n      Seongbuk-gu\n      etc\n      37.592560\n      127.017048\n    \n    \n      3\n      1000000002\n      5.0\n      2020-01-26\n      Seoul\n      Seongbuk-gu\n      store\n      37.591810\n      127.016822\n    \n    \n      4\n      1000000002\n      5.0\n      2020-01-26\n      Seoul\n      Seongdong-gu\n      public_transportation\n      37.563992\n      127.029534\n    \n  \n\n\n\n\n\nTime\n\n한국의 COVID-19 상태의 시계열 데이터\n\ndate: YYYY-MM-DD\ntime: Time (0 = AM 12:00 / 16 = PM 04:00)\n\nThe time for KCDC to open the information has been changed from PM 04:00 to AM 12:00 since March 2nd.\n\ntest: the accumulated number of tests\n\nA test is a diagnosis of an infection.\n\nnegative: the accumulated number of negative results\nconfirmed: the accumulated number of positive results\nreleased: the accumulated number of releases\ndeceased: the accumulated number of deceases\n\n\ntime.head()\n\n\n\n\n\n  \n    \n      \n      date\n      time\n      test\n      negative\n      confirmed\n      released\n      deceased\n    \n  \n  \n    \n      0\n      2020-01-20\n      16\n      1\n      0\n      1\n      0\n      0\n    \n    \n      1\n      2020-01-21\n      16\n      1\n      0\n      1\n      0\n      0\n    \n    \n      2\n      2020-01-22\n      16\n      4\n      3\n      1\n      0\n      0\n    \n    \n      3\n      2020-01-23\n      16\n      22\n      21\n      1\n      0\n      0\n    \n    \n      4\n      2020-01-24\n      16\n      27\n      25\n      2\n      0\n      0\n    \n  \n\n\n\n\n\nTimeAge\n\n한국의 연령별 코로나19 현황 시계열 데이터\n\ndate: YYYY-MM-DD\n\nThe status in terms of the age has been presented since March 2nd.\n\ntime: Time\nage: the age of patients\nconfirmed: the accumulated number of the confirmed\ndeceased: the accumulated number of the deceased\n\n\ntimeage.head()\n\n\n\n\n\n  \n    \n      \n      date\n      time\n      age\n      confirmed\n      deceased\n    \n  \n  \n    \n      0\n      2020-03-02\n      0\n      0s\n      32\n      0\n    \n    \n      1\n      2020-03-02\n      0\n      10s\n      169\n      0\n    \n    \n      2\n      2020-03-02\n      0\n      20s\n      1235\n      0\n    \n    \n      3\n      2020-03-02\n      0\n      30s\n      506\n      1\n    \n    \n      4\n      2020-03-02\n      0\n      40s\n      633\n      1\n    \n  \n\n\n\n\n\nTimeGender\n\n한국의 성별에 따른 COVID-19 현황의 시계열 데이터\n\ndate: YYYY-MM-DD\n\nThe status in terms of the gender has been presented since March 2nd.\n\ntime: Time\nsex: the gender of patients\nconfirmed: the accumulated number of the confirmed\ndeceased: the accumulated number of the deceased\n\n\ntimegender.head()\n\n\n\n\n\n  \n    \n      \n      date\n      time\n      sex\n      confirmed\n      deceased\n    \n  \n  \n    \n      0\n      2020-03-02\n      0\n      male\n      1591\n      13\n    \n    \n      1\n      2020-03-02\n      0\n      female\n      2621\n      9\n    \n    \n      2\n      2020-03-03\n      0\n      male\n      1810\n      16\n    \n    \n      3\n      2020-03-03\n      0\n      female\n      3002\n      12\n    \n    \n      4\n      2020-03-04\n      0\n      male\n      1996\n      20\n    \n  \n\n\n\n\n\nTimeProvince\n\n한국의 지역별 코로나19 현황 시계열 데이터\n\ndate: YYYY-MM-DD\ntime: Time\nprovince: the province of South Korea\nconfirmed: the accumulated number of the confirmed in the province\n\nThe confirmed status in terms of the provinces has been presented since Feburary 21th.\nThe value before Feburary 21th can be different.\n\nreleased: the accumulated number of the released in the province\n\nThe confirmed status in terms of the provinces has been presented since March 5th. -The value before March 5th can be different.\n\ndeceased: the accumulated number of the deceased in the province\n\nThe confirmed status in terms of the provinces has been presented since March 5th.\nThe value before March 5th can be different.\n\n\n\ntimeprovince.head()\n\n\n\n\n\n  \n    \n      \n      date\n      time\n      province\n      confirmed\n      released\n      deceased\n    \n  \n  \n    \n      0\n      2020-01-20\n      16\n      Seoul\n      0\n      0\n      0\n    \n    \n      1\n      2020-01-20\n      16\n      Busan\n      0\n      0\n      0\n    \n    \n      2\n      2020-01-20\n      16\n      Daegu\n      0\n      0\n      0\n    \n    \n      3\n      2020-01-20\n      16\n      Incheon\n      1\n      0\n      0\n    \n    \n      4\n      2020-01-20\n      16\n      Gwangju\n      0\n      0\n      0\n    \n  \n\n\n\n\n\nRegion\n\n대한민국 내 지역의 위치 및 통계 자료\n\ncode: the code of the region\nprovince: Special City / Metropolitan City / Province(-do)\ncity: City(-si) / Country (-gun) / District (-gu)\nlatitude: the latitude of the visit (WGS84)\nlongitude: the longitude of the visit (WGS84)\nelementary_school_count: the number of elementary schools\nkindergarten_count: the number of kindergartens\nuniversity_count: the number of universities\nacademy_ratio: the ratio of academies\nelderly_population_ratio: the ratio of the elderly population\nelderly_alone_ratio: the ratio of elderly households living alone\nnursing_home_count: the number of nursing homes\n\nSource of the statistic: KOSTAT (Statistics Korea)\n\nregion.head()\n\n\n\n\n\n  \n    \n      \n      code\n      province\n      city\n      latitude\n      longitude\n      elementary_school_count\n      kindergarten_count\n      university_count\n      academy_ratio\n      elderly_population_ratio\n      elderly_alone_ratio\n      nursing_home_count\n    \n  \n  \n    \n      0\n      10000\n      Seoul\n      Seoul\n      37.566953\n      126.977977\n      607\n      830\n      48\n      1.44\n      15.38\n      5.8\n      22739\n    \n    \n      1\n      10010\n      Seoul\n      Gangnam-gu\n      37.518421\n      127.047222\n      33\n      38\n      0\n      4.18\n      13.17\n      4.3\n      3088\n    \n    \n      2\n      10020\n      Seoul\n      Gangdong-gu\n      37.530492\n      127.123837\n      27\n      32\n      0\n      1.54\n      14.55\n      5.4\n      1023\n    \n    \n      3\n      10030\n      Seoul\n      Gangbuk-gu\n      37.639938\n      127.025508\n      14\n      21\n      0\n      0.67\n      19.49\n      8.5\n      628\n    \n    \n      4\n      10040\n      Seoul\n      Gangseo-gu\n      37.551166\n      126.849506\n      36\n      56\n      1\n      1.17\n      14.39\n      5.7\n      1080\n    \n  \n\n\n\n\n\nWeather\n\n한국 지역의 날씨 데이터\n\ncode: the code of the region\nprovince: Special City / Metropolitan City / Province(-do)\ndate: YYYY-MM-DD\navg_temp: the average temperature\nmin_temp: the lowest temperature\nmax_temp: the highest temperature\nprecipitation: the daily precipitation\nmax_wind_speed: the maximum wind speed\nmost_wind_direction: the most frequent wind direction\navg_relative_humidity: the average relative humidity\n\nSource of the weather data: KMA (Korea Meteorological Administration)\n\nweather.head()\n\n\n\n\n\n  \n    \n      \n      code\n      province\n      date\n      avg_temp\n      min_temp\n      max_temp\n      precipitation\n      max_wind_speed\n      most_wind_direction\n      avg_relative_humidity\n    \n  \n  \n    \n      0\n      10000\n      Seoul\n      2016-01-01\n      1.2\n      -3.3\n      4.0\n      0.0\n      3.5\n      90.0\n      73.0\n    \n    \n      1\n      11000\n      Busan\n      2016-01-01\n      5.3\n      1.1\n      10.9\n      0.0\n      7.4\n      340.0\n      52.1\n    \n    \n      2\n      12000\n      Daegu\n      2016-01-01\n      1.7\n      -4.0\n      8.0\n      0.0\n      3.7\n      270.0\n      70.5\n    \n    \n      3\n      13000\n      Gwangju\n      2016-01-01\n      3.2\n      -1.5\n      8.1\n      0.0\n      2.7\n      230.0\n      73.1\n    \n    \n      4\n      14000\n      Incheon\n      2016-01-01\n      3.1\n      -0.4\n      5.7\n      0.0\n      5.3\n      180.0\n      83.9\n    \n  \n\n\n\n\n\nSearchTrend\n\n국내 최대 포털인 네이버에서 검색된 키워드의 트렌드 데이터\n\ndate: YYYY-MM-DD\ncold: the search volume of ‘cold’ in Korean language\n\nThe unit means relative value by setting the highest search volume in the period to 100.\n\nflu: the search volume of ‘flu’ in Korean language\n\nSame as above.\n\npneumonia: the search volume of ‘pneumonia’ in Korean language -Same as above.\ncoronavirus: the search volume of ‘coronavirus’ in Korean language -Same as above.\n\nSource of the data: NAVER DataLab\n\nsearchtrend.head()\n\n\n\n\n\n  \n    \n      \n      date\n      cold\n      flu\n      pneumonia\n      coronavirus\n    \n  \n  \n    \n      0\n      2016-01-01\n      0.11663\n      0.05590\n      0.15726\n      0.00736\n    \n    \n      1\n      2016-01-02\n      0.13372\n      0.17135\n      0.20826\n      0.00890\n    \n    \n      2\n      2016-01-03\n      0.14917\n      0.22317\n      0.19326\n      0.00845\n    \n    \n      3\n      2016-01-04\n      0.17463\n      0.18626\n      0.29008\n      0.01145\n    \n    \n      4\n      2016-01-05\n      0.17226\n      0.15072\n      0.24562\n      0.01381\n    \n  \n\n\n\n\n\nSeoulFloating\n\n대한민국 서울 유동인구 데이터(SK텔레콤 빅데이터 허브에서)\n\ndate: YYYY-MM-DD\nhour: Hour\nbirth_year: the birth year of the floating population\nsext: he sex of the floating population\nprovince: Special City / Metropolitan City / Province(-do)\ncity: City(-si) / Country (-gun) / District (-gu)\nfp_num: the number of floating population\n\nSource of the data: SKT Big Data Hub\n\nseoulfloating.head()\n\n\n\n\n\n  \n    \n      \n      date\n      hour\n      birth_year\n      sex\n      province\n      city\n      fp_num\n    \n  \n  \n    \n      0\n      2020-01-01\n      0\n      20\n      female\n      Seoul\n      Dobong-gu\n      19140\n    \n    \n      1\n      2020-01-01\n      0\n      20\n      male\n      Seoul\n      Dobong-gu\n      19950\n    \n    \n      2\n      2020-01-01\n      0\n      20\n      female\n      Seoul\n      Dongdaemun-gu\n      25450\n    \n    \n      3\n      2020-01-01\n      0\n      20\n      male\n      Seoul\n      Dongdaemun-gu\n      27050\n    \n    \n      4\n      2020-01-01\n      0\n      20\n      female\n      Seoul\n      Dongjag-gu\n      28880\n    \n  \n\n\n\n\n\nPolicy\n\n한국의 COVID-19에 대한 정부 정책 데이터\n\npolicy_id: the ID of the policy\ncountry: the country that implemented the policy\ntype: the type of the policy\ngov_policy: the policy of the government\ndetail: the detail of the policy\nstart_date: the start date of the policy\nend_date: the end date of the policy\n\n\npolicy.head()\n\n\n\n\n\n  \n    \n      \n      policy_id\n      country\n      type\n      gov_policy\n      detail\n      start_date\n      end_date\n    \n  \n  \n    \n      0\n      1\n      Korea\n      Alert\n      Infectious Disease Alert Level\n      Level 1 (Blue)\n      2020-01-03\n      2020-01-19\n    \n    \n      1\n      2\n      Korea\n      Alert\n      Infectious Disease Alert Level\n      Level 2 (Yellow)\n      2020-01-20\n      2020-01-27\n    \n    \n      2\n      3\n      Korea\n      Alert\n      Infectious Disease Alert Level\n      Level 3 (Orange)\n      2020-01-28\n      2020-02-22\n    \n    \n      3\n      4\n      Korea\n      Alert\n      Infectious Disease Alert Level\n      Level 4 (Red)\n      2020-02-23\n      NaN\n    \n    \n      4\n      5\n      Korea\n      Immigration\n      Special Immigration Procedure\n      from China\n      2020-02-04\n      NaN\n    \n  \n\n\n\n\n\n\n\n본론 \n\n주제1 - 어떤 연령층이 가장 많이 확진되었는가? \n\n주제1 - EDA \n\n# import packages - 사용할 패키지 불러오기\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom matplotlib import pyplot as plt\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\n\ntime.head() #time 데이터의 상위 5개를 확인\n\n\n\n\n\n  \n    \n      \n      date\n      time\n      test\n      negative\n      confirmed\n      released\n      deceased\n    \n  \n  \n    \n      0\n      2020-01-20\n      16\n      1\n      0\n      1\n      0\n      0\n    \n    \n      1\n      2020-01-21\n      16\n      1\n      0\n      1\n      0\n      0\n    \n    \n      2\n      2020-01-22\n      16\n      4\n      3\n      1\n      0\n      0\n    \n    \n      3\n      2020-01-23\n      16\n      22\n      21\n      1\n      0\n      0\n    \n    \n      4\n      2020-01-24\n      16\n      27\n      25\n      2\n      0\n      0\n    \n  \n\n\n\n\n\n#시간의 흐름에 따른 확진자 추이\nfig = go.Figure() #빈 도화지를 만든다는 개념\n\nfig.add_trace(go.Scatter(x=time['date'], y=time['confirmed'],\n                    mode='lines', \n                    name='확진(confirmed)')) \n                    #Scatter 형태의 플랏으로 x축은 time데이터의 date컬럼을 사용하고 y축은 time데이터의 confirmed 컬럼을 사용하고 표현 방법은 line이며 선의 이름은 확진으로 지정\n\nfig.update_layout(title='시간의 흐름에 따른 확진자 추이',\n                   xaxis_title='Date',\n                   yaxis_title='Number') #그래프의 제목과 x축 y축의 이름을 지정\n\nfig.show() #그래프를 출력해서 보이도록\n\n\n                                                \n\n\n해당 그래프를 보면 시간의 흐름에 따른 누적 확진자의 수를 나타낸것으로 20년 3월부터 가파른 경사를 보이면서 우상향해서 증가하는 추세를 보이고 있습니다\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=time['date'], y=time['confirmed'],\n                    mode='lines', \n                    name='확진(confirmed)'))\n\nfig.add_trace(go.Scatter(x=time['date'], y=time['released'],\n                    mode='lines', \n                    name='해제(released)'))\n                    \nfig.add_trace(go.Scatter(x=time['date'], y=time['deceased'],\n                    mode='lines', \n                    name='사망(deceased)'))\n\nfig.update_layout(title='시간의 흐름에 따른 코로나의 추이',\n                   xaxis_title='Date',\n                   yaxis_title='Number')\n\nfig.show()\n\n\n                                                \n\n\n다음 그래프는 시간의 흐름에 따른 코로나의 추이로 확진자와 격리해제자의 추이와 사망자의 추이에 대해서 보여주고 있으며 확진자와 격리해제자의 추이는 유사하게 우상향하는 모습을 보여주고 있으며 사망자는 확진자와 격리자의 수에 비해서는 적어서 눈에 보이는 변화는 없습니다.\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=time['date'], y=time['confirmed'],\n                    mode='lines', \n                    name='확진(confirmed)'))\n\nfig.add_trace(go.Scatter(x=time['date'],y=time['negative'],\n             mode='lines', name='음성(Negative)'))\n\nfig.add_trace(go.Scatter(x=time['date'],y=time['test'],\n             mode='markers', name='검사(Test)'))\n\nfig.update_layout(title='시간의 흐름에 따른 코로나의 검사 추이',\n                   xaxis_title='Date',\n                   yaxis_title='Number')\n\nfig.show()\n\n\n                                                \n\n\n해당 그래프는 시간의 흐름에 따른 코로나의 검사 추이로 검사수와 음성의 수가 거의 붙어서 우상향하는 모습이고 확진자는 이에 비해 변동이 없어 보이는 모습입니다. 이를 통해 검사를 많이 했지만 이에 비해서 확진이 된 정도는 상당히 적음을 알 수 있습니다.\n\nfig = go.Figure()\nfig.add_trace(go.Bar(x=time['date'],y=time['confirmed'].diff(), \n                     name='confirmed', marker_color='rgba(152, 0, 0, .8)'))\n\nfig.update_layout(title='일단위 확진자 수',\n                   xaxis_title='Date',\n                   yaxis_title='Number')\n\nfig.show()\n\n\n                                                \n\n\n다음은 일단위 확진자의 수를 보여주고 있습니다. 20년 3월 인근에서 800여명 까지 일일 확진되는 모습을 보이고 점차 감소하는 모습을 보이는 형태입니다\n\n\n주제1 - 연령대별 확진 비율 \n\ndisplay(timeage.head()) #timeage 데이터셋의 기본적인 형태를 파악하기 위해 상위 5개의 행만 추출\ndisplay(timeage.age.unique()) #timeage 데이터셋에서 age 컬럼에서 어떤 연령층이 있는지 unique 함수를 통해서 추출\n\n\n\n\n\n  \n    \n      \n      date\n      time\n      age\n      confirmed\n      deceased\n    \n  \n  \n    \n      0\n      2020-03-02\n      0\n      0s\n      32\n      0\n    \n    \n      1\n      2020-03-02\n      0\n      10s\n      169\n      0\n    \n    \n      2\n      2020-03-02\n      0\n      20s\n      1235\n      0\n    \n    \n      3\n      2020-03-02\n      0\n      30s\n      506\n      1\n    \n    \n      4\n      2020-03-02\n      0\n      40s\n      633\n      1\n    \n  \n\n\n\n\narray(['0s', '10s', '20s', '30s', '40s', '50s', '60s', '70s', '80s'],\n      dtype=object)\n\n\n\nage_list = timeage.age.unique()\nage_list #앞에서 설명한 연령대를 따로 추출하여 age_list라는 곳에 할당을 시킴 \n\narray(['0s', '10s', '20s', '30s', '40s', '50s', '60s', '70s', '80s'],\n      dtype=object)\n\n\n\nfig, ax = plt.subplots(figsize = (13,7)) #도화지(Figure : fig)를 깔고 그래프를 그릴 구역(Axes : ax)을 정의합니다. figsize를 통해서 도화지의 크기를 지정해준다\n#이는 objection oriented API으로 그래프의 각 부분을 객체로 지정하고 그리는 유형이다\nsns.barplot(age_list,timeage.confirmed[-9:])\nax.set_xlabel('age',size=13) #연령\nax.set_ylabel('number of case',size=13) #케이스의 횟수\nplt.title('Confirmed Cases by Age')\nplt.show()\n\nc:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning:\n\nPass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n\n\n\n\n\n\n연령대별로 분석을 해본결과 20대가 압도적으로 많은 수를 차지하고 있는 형태의 플랏을 볼 수가 있다\n2020년 연령대별 인구\n\nhttps://kosis.kr/visual/populationKorea/experienceYard/populationPyramid.do?mb=N&menuId=M_3_2\n\n해당 자료를 이용해서 인구수와 확진 비율을 확인하여 과연 20대가 인구수가 많아서 이렇게 많이 확진이 되었는가에 대해서 알아본다\n\nage_order = pd.DataFrame() #빈 데이터 프레임을 생성\nage_order['age']  = age_list #앞서 생성한 age_list를 새로 만드는 데이터 프레임에 age라는 이름의 컬럼으로 할당\nage_order['population'] = [4054901, 4769187, 7037893, 7174782, 8257903, 8575336, 6476602, 3598811, 1657942] #population이라는 컬럼에 통계청 홈페이지에서 확인한 값을 입력\nage_order['proportion'] = round(age_order['population']/sum(age_order['population'])*100,2) \n#인구 비율을 구하기 위해 모든 인구수를 더하고 각 연령별로 나누고 소수점으로 나오는것을 방지하기 위해 100을 곱하고 소수점 2번째 자리까지 표현이 되도록 설정\nage_order = age_order.sort_values('age') #age를 기준으로 재정렬\nage_order.set_index(np.arange(1,10),inplace=True) #인덱스의 설정을 1~10순으로 들어가도록 설정하고 본래 있던것은 대체해서 사용하도록\nage_order\n\n\n\n\n\n  \n    \n      \n      age\n      population\n      proportion\n    \n  \n  \n    \n      1\n      0s\n      4054901\n      7.86\n    \n    \n      2\n      10s\n      4769187\n      9.24\n    \n    \n      3\n      20s\n      7037893\n      13.64\n    \n    \n      4\n      30s\n      7174782\n      13.90\n    \n    \n      5\n      40s\n      8257903\n      16.00\n    \n    \n      6\n      50s\n      8575336\n      16.62\n    \n    \n      7\n      60s\n      6476602\n      12.55\n    \n    \n      8\n      70s\n      3598811\n      6.97\n    \n    \n      9\n      80s\n      1657942\n      3.21\n    \n  \n\n\n\n\n\nfig, ax = plt.subplots(figsize=(13, 7)) \nplt.title('Korea Age Proportion', fontsize=17)\nsns.barplot(age_list, age_order.proportion[-9:])\nax.set_xlabel('Age', size=13)\nax.set_ylabel('Rate (%)', size=13)\nplt.show() \n#한국의 2020년 연령별 인구의 수를 나타낸 표이다 \n\nc:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning:\n\nPass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n\n\n\n\n\n\n예상과는 다르게 20대가 가장 많은 인구수를 가지고 있는 연령대가 아닌 40,50대가 가장 인구수가 많은 연령대임을 알 수 있다. 이로 20대 인구수가 다른 연령층에 비해 많기 때문에 확진이 많이 된것은 아니다.\n\nconfirmed_by_population = age_order.sort_values('age') #'age'라는 컬럼으로 정렬\nconfirmed_by_population['confirmed'] = list(timeage[-9:].confirmed) #confirmed라는 컬럼을 만들고 timeage의 해당 리스트를 할당 시킴\n\n\nconfirmed_by_population['confirmed_ratio'] = confirmed_by_population['confirmed']/confirmed_by_population['population'] *100 #인구비율에 따른 확진 비율 컬럼 추가\ndisplay(confirmed_by_population)\n\n\n\n\n\n  \n    \n      \n      age\n      population\n      proportion\n      confirmed\n      confirmed_ratio\n    \n  \n  \n    \n      1\n      0s\n      4054901\n      7.86\n      193\n      0.004760\n    \n    \n      2\n      10s\n      4769187\n      9.24\n      708\n      0.014845\n    \n    \n      3\n      20s\n      7037893\n      13.64\n      3362\n      0.047770\n    \n    \n      4\n      30s\n      7174782\n      13.90\n      1496\n      0.020851\n    \n    \n      5\n      40s\n      8257903\n      16.00\n      1681\n      0.020356\n    \n    \n      6\n      50s\n      8575336\n      16.62\n      2286\n      0.026658\n    \n    \n      7\n      60s\n      6476602\n      12.55\n      1668\n      0.025754\n    \n    \n      8\n      70s\n      3598811\n      6.97\n      850\n      0.023619\n    \n    \n      9\n      80s\n      1657942\n      3.21\n      556\n      0.033536\n    \n  \n\n\n\n\n\n## 3. Plot confirmed rate by age\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Population-adjusted Confirmed Rate by Age', fontsize=17)\nsns.barplot(age_list, confirmed_by_population.confirmed_ratio[-9:])\nax.set_xlabel('Age', size=13)\nax.set_ylabel('Confirmed rate (%)', size=13)\nplt.show() #인구 비율에 따른 확진 확률\n\nc:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning:\n\nPass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n\n\n\n\n\n\n인구 비율에 따른 확진자의 수를 보아도 20대가 인구수가 많은 연령대인 40,50대 보다도 확연하게 많은 것을 알 수 있으며 오히려 80대 이상의 연령대가 차지하는 비율이 증가하였다.\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(23, 7)) #1행 2열의 도화지를 생성\n\n## 1. Confirmed Cases by Age\nax[0].set_title('Confirmed Cases by Age', fontsize=15)\nax[0].bar(age_list, confirmed_by_population.confirmed)\n\n## 2. Population-adjusted Confirmed Rate\nax[1].set_title('Population-adjusted Confirmed Rate', fontsize=15)\nax[1].bar(age_list, confirmed_by_population.confirmed_ratio)\n\nplt.show() \n\n\n\n\n다음 두개의 플랏을 보면 앞선 그래프에서는 20대의 확진 확률이 다른 연령대에 비해서 앞도적으로 높았지만 인구의 비율에 따른 확진의 비율을 나타내는 두번째 플랏을 보면 아직도 20대가 다른 연령대에 비해서 높기는 하지만 첫번째 그래프에 비해서는 조금은 낮아진 모습과 60~80대까지 연령층의 비중이 조금은 증가 했다는 사실을 두개의 플랏을 비교하면서 알 수있습니다 따라서 저는 다른 연령에 비해 압도적으로 많은 확진 비율을 가지고 있는 20대에 대해서 집중적으로 분석을 해보도록 하겠습니다.\n\n\n주제1 - 연령대별 확진 경로 \n\npatientinfo.head()\n\n\n\n\n\n  \n    \n      \n      patient_id\n      sex\n      age\n      country\n      province\n      city\n      infection_case\n      infected_by\n      contact_number\n      symptom_onset_date\n      confirmed_date\n      released_date\n      deceased_date\n      state\n    \n  \n  \n    \n      0\n      1000000001\n      male\n      50s\n      Korea\n      Seoul\n      Gangseo-gu\n      overseas inflow\n      NaN\n      75\n      2020-01-22\n      2020-01-23\n      2020-02-05\n      NaN\n      released\n    \n    \n      1\n      1000000002\n      male\n      30s\n      Korea\n      Seoul\n      Jungnang-gu\n      overseas inflow\n      NaN\n      31\n      NaN\n      2020-01-30\n      2020-03-02\n      NaN\n      released\n    \n    \n      2\n      1000000003\n      male\n      50s\n      Korea\n      Seoul\n      Jongno-gu\n      contact with patient\n      2002000001\n      17\n      NaN\n      2020-01-30\n      2020-02-19\n      NaN\n      released\n    \n    \n      3\n      1000000004\n      male\n      20s\n      Korea\n      Seoul\n      Mapo-gu\n      overseas inflow\n      NaN\n      9\n      2020-01-26\n      2020-01-30\n      2020-02-15\n      NaN\n      released\n    \n    \n      4\n      1000000005\n      female\n      20s\n      Korea\n      Seoul\n      Seongbuk-gu\n      contact with patient\n      1000000002\n      2\n      NaN\n      2020-01-31\n      2020-02-24\n      NaN\n      released\n    \n  \n\n\n\n\n\npatientinfo['infection_case'] = patientinfo['infection_case'].astype(str).apply(lambda x:x.split()[0])\n#PatientInfo['infection_case']\ninfectionCase = patientinfo.pivot_table(index='infection_case',columns='age',\nvalues='patient_id',aggfunc=\"count\")\n#infectionCase\n#전체 감염 케이스\npatientTotal = infectionCase.fillna(0).sum(axis=1)\npatientTotal = patientTotal.sort_values(ascending = False)[:5]\n# 20대 감염 케이스\npatient20s = infectionCase['20s'].dropna()\npatient20sTop = patient20s.sort_values(ascending=False)[:5]\n\n\ndisplay(patientTotal)\ndisplay(patient20sTop)\n\ninfection_case\ncontact     1112.0\nnan          827.0\noverseas     653.0\netc          638.0\nGuro-gu      112.0\ndtype: float64\n\n\ninfection_case\noverseas       269.0\nnan            221.0\ncontact        172.0\netc            127.0\nShincheonji     41.0\nName: 20s, dtype: float64\n\n\n\nfig = go.Figure()\nfig.add_trace(go.Pie(labels=patientTotal.index, values=patientTotal.values,\n                     textinfo='label+percent'))\n\nfig.update_layout(title='Confirmed infection case Total AGe')\n\nfig.show()\n\n\n                                                \n\n\n전체 연령측의 감염원인에 대해서 본다면 접촉에 의한 확진이 33% 해외 입국이 19% 그외 nan과 etc가 각각 24,19%의 비율을 차지하고 있다\n\nfig = go.Figure()\nfig.add_trace(go.Pie(labels=patient20sTop.index, values=patient20sTop.values,\n                     textinfo='label+percent'))\n\nfig.update_layout(title='Confirmed infection case 20s AGe')\n\nfig.show()\n\n\n                                                \n\n\n20대 연령의 그룹은 전체연령에 비해 해외입국과 nan이 각각 32 26%를 차지 하고 있다. 그러나 해당 데이터에는 원인을 알수 없는 nan데이가 전체의 1/4가량을 차지 하기 때문에 정확한 분석을 하기 어렵다\n그렇다면 확진된 20대가 많이 돌아다닌 장소에 대해서 patientinfo 데이터를 이용해서 찾아보겠습니다\n\npatientroute = pd.read_csv(path+'PatientRoute.csv')\n\n\npatientroute.head()\n\n\n\n\n\n  \n    \n      \n      patient_id\n      global_num\n      date\n      province\n      city\n      type\n      latitude\n      longitude\n    \n  \n  \n    \n      0\n      1000000001\n      2.0\n      2020-01-22\n      Gyeonggi-do\n      Gimpo-si\n      airport\n      37.615246\n      126.715632\n    \n    \n      1\n      1000000001\n      2.0\n      2020-01-24\n      Seoul\n      Jung-gu\n      hospital\n      37.567241\n      127.005659\n    \n    \n      2\n      1000000002\n      5.0\n      2020-01-25\n      Seoul\n      Seongbuk-gu\n      etc\n      37.592560\n      127.017048\n    \n    \n      3\n      1000000002\n      5.0\n      2020-01-26\n      Seoul\n      Seongbuk-gu\n      store\n      37.591810\n      127.016822\n    \n    \n      4\n      1000000002\n      5.0\n      2020-01-26\n      Seoul\n      Seongdong-gu\n      public_transportation\n      37.563992\n      127.029534\n    \n  \n\n\n\n\n\npatientroute[['patient_id','date','type']] #필요한 컬럼만 선택\n\n\n\n\n\n  \n    \n      \n      patient_id\n      date\n      type\n    \n  \n  \n    \n      0\n      1000000001\n      2020-01-22\n      airport\n    \n    \n      1\n      1000000001\n      2020-01-24\n      hospital\n    \n    \n      2\n      1000000002\n      2020-01-25\n      etc\n    \n    \n      3\n      1000000002\n      2020-01-26\n      store\n    \n    \n      4\n      1000000002\n      2020-01-26\n      public_transportation\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      6709\n      6100000090\n      2020-03-24\n      airport\n    \n    \n      6710\n      6100000090\n      2020-03-24\n      airport\n    \n    \n      6711\n      6100000090\n      2020-03-25\n      store\n    \n    \n      6712\n      6100000090\n      2020-03-25\n      hospital\n    \n    \n      6713\n      6100000090\n      2020-03-25\n      store\n    \n  \n\n6714 rows × 3 columns\n\n\n\n\nplaces = patientroute.type.unique()\nsimproute = patientroute[['patient_id','date','type']]\nagedf = patientinfo[['patient_id','age']]\nsimproutewage = pd.merge(simproute,agedf,how='left')\nfiplot = simproutewage.set_index('type')\nfiplot_count = fiplot.groupby('type').count().patient_id.sort_values()\n\n\nfig = fiplot_count.iplot(asFigure = True, kind='bar')\nfig.show()\n\n\n                                                \n\n\n해당 그래프는 전체 연령의 확진 원인에 대한 것을 카운트 시킨 결과로 etc와 hospital이 가장 많은 것을 보이나 병원은 확진자가 이상증세를 느끼고 찾아가는 당연한 경로이므로 제외를 하고 etc 또한 어느 곳에 다녀왔는지 정확하게 알 수 없어서 제외를 하고 다시 진행을 해보겠습니다.\n\ntwtfi = fiplot[fiplot.age == '20s']\nuntwtfi = fiplot[fiplot.age != '20s']\ntwt = twtfi.groupby('type').count().patient_id\nuntwt = untwtfi.groupby('type').count().patient_id\ntwt = twt[~twt.index.isin( ['etc','hospital'])]\nuntwt = untwt[~untwt.index.isin( ['etc','hospital'])]\nfig = go.Figure()\nfig.add_trace(go.Bar(x = twt.index, \n                     y = twt,\n                     name = '20s',\n                     marker_color='indianred'))\nfig.add_trace(go.Bar(x = untwt.index, \n                     y = untwt,\n                     name = 'except 20s',\n                     marker_color='lightsalmon'))\nfig.update_layout(barmode='group', xaxis_tickangle=-45)\nfig.show()\n\n\n                                                \n\n\n다음 그래프는 20대와 그외의 연령층이 확진된 원인에 대한것으로 앞서 말한것 처럼 etc와 hospital은 가장 많은 비율을 차지 하지만 분석을 하는데 크게 도움이 되지 않는다고 판단을 하여 제외를 하고 진행을 한 결과 이다.\n공통적으로 많이 방문하는 store과 church는 비슷한 양상을 보여주고 있습니다. 하지만, 20대는 restaurant, pc방, cafe, bar 등에서 훨씬 많은 방문비율을 확인할 수 있었습니다.\n\n\n\n주제2 - 코로나는 누구에게 가장 치명적인가 \n\n주제2 - 연령별로 확진자의 치명률 \n\ntime.head()\n\n\n\n\n\n  \n    \n      \n      date\n      time\n      test\n      negative\n      confirmed\n      released\n      deceased\n    \n  \n  \n    \n      0\n      2020-01-20\n      16\n      1\n      0\n      1\n      0\n      0\n    \n    \n      1\n      2020-01-21\n      16\n      1\n      0\n      1\n      0\n      0\n    \n    \n      2\n      2020-01-22\n      16\n      4\n      3\n      1\n      0\n      0\n    \n    \n      3\n      2020-01-23\n      16\n      22\n      21\n      1\n      0\n      0\n    \n    \n      4\n      2020-01-24\n      16\n      27\n      25\n      2\n      0\n      0\n    \n  \n\n\n\n\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=time['date'], y=time['deceased'],\n                    mode='lines', # Line plot만 그리기\n                    name='사망(deceased)'))\n\nfig.update_layout(title='시간의 흐름에 따른 확진자 사망 추이',\n                   xaxis_title='Date',\n                   yaxis_title='Number')\n\nfig.show()\n\n\n                                                \n\n\n해당 그래프는 시간의 흐름에 따른 확진자의 사망 추이로서 2020년 3월 부터 계속해서 우상향하는 모습을 볼 수 있다\n\nfig, ax = plt.subplots(figsize = (13,7)) #도화지(Figure : fig)를 깔고 그래프를 그릴 구역(Axes : ax)을 정의합니다. figsize를 통해서 도화지의 크기를 지정해준다\n#이는 objection oriented API으로 그래프의 각 부분을 객체로 지정하고 그리는 유형이다\nsns.barplot(age_list,timeage.deceased[-9:])\nax.set_xlabel('age',size=13) #연령\nax.set_ylabel('number of case',size=13) #케이스의 횟수\nplt.title('Deceased Cases by Age')\nplt.show()\n\nc:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning:\n\nPass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n\n\n\n\n\n\n나이가 많아질수록 사망자의 비율이 높다 과연 나이가 많아질수록 인구수가 많아져서 이러한 현상이 나오는 지 인구 비율에 따른 사망자에 대해서 다시 한번 살펴보자\n\nconfirmed_by_population = age_order.sort_values('age')\nconfirmed_by_population['deceased'] = list(timeage[-9:].deceased)\n\n# 2. Get confirmed ratio regarding population\nconfirmed_by_population['deceased_ratio'] = confirmed_by_population['deceased']/confirmed_by_population['population'] *100\ndisplay(confirmed_by_population)\n\n\n\n\n\n  \n    \n      \n      age\n      population\n      proportion\n      deceased\n      deceased_ratio\n    \n  \n  \n    \n      1\n      0s\n      4054901\n      7.86\n      0\n      0.000000\n    \n    \n      2\n      10s\n      4769187\n      9.24\n      0\n      0.000000\n    \n    \n      3\n      20s\n      7037893\n      13.64\n      0\n      0.000000\n    \n    \n      4\n      30s\n      7174782\n      13.90\n      2\n      0.000028\n    \n    \n      5\n      40s\n      8257903\n      16.00\n      3\n      0.000036\n    \n    \n      6\n      50s\n      8575336\n      16.62\n      15\n      0.000175\n    \n    \n      7\n      60s\n      6476602\n      12.55\n      41\n      0.000633\n    \n    \n      8\n      70s\n      3598811\n      6.97\n      82\n      0.002279\n    \n    \n      9\n      80s\n      1657942\n      3.21\n      139\n      0.008384\n    \n  \n\n\n\n\n\n## 3. Plot confirmed rate by age\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Population-adjusted Deceased Rate by Age', fontsize=17)\nsns.barplot(age_list, confirmed_by_population.deceased_ratio[-9:])\nax.set_xlabel('Age', size=13)\nax.set_ylabel('Deceased rate (%)', size=13)\nplt.show() #인구 비율에 따른 확진 확률\n\nc:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning:\n\nPass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n\n\n\n\n\n\n인구 비율에 따른 사망자의 비율을 살펴본 결과이다 0~20대 까지는 사망자는 없으며 30대부터 확진으로 인한 사망자가 존재한다. 그러나 80대 이상의 연령층의 경우는 가장 많은 인구수를 가진 연령층도 아니지만 사망자의 비중이 가장 높은 것을 볼 수 있다. 이로 코로나 바이러스는 고연령층 일수록 가장 치명적인 질병임을 예측할 수 있다.\n그렇다면 고연령층의 확진 원인에 대해서 알아보자\n\n\n주제2 - 연령대의 확진 원인 \n\naged_pat = patientinfo[(patientinfo['age'] == '60s')|(patientinfo['age'] == '70s')|\n                (patientinfo['age'] == '80s')][['province','age','infection_case']]\n                \naged_inf = pd.DataFrame(aged_pat['infection_case'].value_counts())\n#고연령측의 확진 원인\n\n\npatientinfo['infection_case'] = patientinfo['infection_case'].astype(str).apply(lambda x:x.split()[0])\n#PatientInfo['infection_case']\ninfectionCase = patientinfo.pivot_table(index='infection_case',columns='age',\nvalues='patient_id',aggfunc=\"count\")\n#infectionCase\n#전체 감염 케이스\npatientTotal = infectionCase.fillna(0).sum(axis=1)\npatientTotal = patientTotal.sort_values(ascending = False)[:5]\n# 60대 감염 케이스\npatient60s = infectionCase['60s'].dropna()\npatient60sTop = patient60s.sort_values(ascending=False)[:5]\n# 70대 감염 케이스\npatient70s = infectionCase['70s'].dropna()\npatient70sTop = patient70s.sort_values(ascending=False)[:5]\n# 80대 감염 케이스\npatient80s = infectionCase['80s'].dropna()\npatient80sTop = patient80s.sort_values(ascending=False)[:5]\n\n\nfig = go.Figure()\nfig.add_trace(go.Pie(labels=patient60sTop.index, values=patient60sTop.values,\n                     textinfo='label+percent'))\n\nfig.update_layout(title='Confirmed infection case 60s AGe')\n\nfig.show()\n\n\n                                                \n\n\n\nfig = go.Figure()\nfig.add_trace(go.Pie(labels=patient70sTop.index, values=patient70sTop.values,\n                     textinfo='label+percent'))\n\nfig.update_layout(title='Confirmed infection case 70s AGe')\n\nfig.show()\n\n\n                                                \n\n\n\nfig = go.Figure()\nfig.add_trace(go.Pie(labels=patient80sTop.index, values=patient80sTop.values,\n                     textinfo='label+percent'))\n\nfig.update_layout(title='Confirmed infection case 80s AGe')\n\nfig.show()\n\n\n                                                \n\n\n60,70,80대의 확진 원인을 보니 nan과 etc가 많기는 하지만 다른 연령대에 비해서 contact가 많다는 사실을 알수 있다. 그래도 nan과 etc가 많아 확진의 주요 원인을 접촉에 의해서 라고 단정할 수는 없다\n그렇다면 나이가 많은 사람들은 완치 기간이 길어서 치명률이 높은 것이지 이에 대한 관계에 대해 알아보았습니다\n\n\n주제2 - 연령대별 회복기간 \n\npatientinfo\n\n\n\n\n\n  \n    \n      \n      patient_id\n      sex\n      age\n      country\n      province\n      city\n      infection_case\n      infected_by\n      contact_number\n      symptom_onset_date\n      confirmed_date\n      released_date\n      deceased_date\n      state\n    \n  \n  \n    \n      0\n      1000000001\n      male\n      50s\n      Korea\n      Seoul\n      Gangseo-gu\n      overseas\n      NaN\n      75\n      2020-01-22\n      2020-01-23\n      2020-02-05\n      NaN\n      released\n    \n    \n      1\n      1000000002\n      male\n      30s\n      Korea\n      Seoul\n      Jungnang-gu\n      overseas\n      NaN\n      31\n      NaN\n      2020-01-30\n      2020-03-02\n      NaN\n      released\n    \n    \n      2\n      1000000003\n      male\n      50s\n      Korea\n      Seoul\n      Jongno-gu\n      contact\n      2002000001\n      17\n      NaN\n      2020-01-30\n      2020-02-19\n      NaN\n      released\n    \n    \n      3\n      1000000004\n      male\n      20s\n      Korea\n      Seoul\n      Mapo-gu\n      overseas\n      NaN\n      9\n      2020-01-26\n      2020-01-30\n      2020-02-15\n      NaN\n      released\n    \n    \n      4\n      1000000005\n      female\n      20s\n      Korea\n      Seoul\n      Seongbuk-gu\n      contact\n      1000000002\n      2\n      NaN\n      2020-01-31\n      2020-02-24\n      NaN\n      released\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      5160\n      7000000015\n      female\n      30s\n      Korea\n      Jeju-do\n      Jeju-do\n      overseas\n      NaN\n      25\n      NaN\n      2020-05-30\n      2020-06-13\n      NaN\n      released\n    \n    \n      5161\n      7000000016\n      NaN\n      NaN\n      Korea\n      Jeju-do\n      Jeju-do\n      overseas\n      NaN\n      NaN\n      NaN\n      2020-06-16\n      2020-06-24\n      NaN\n      released\n    \n    \n      5162\n      7000000017\n      NaN\n      NaN\n      Bangladesh\n      Jeju-do\n      Jeju-do\n      overseas\n      NaN\n      72\n      NaN\n      2020-06-18\n      NaN\n      NaN\n      isolated\n    \n    \n      5163\n      7000000018\n      NaN\n      NaN\n      Bangladesh\n      Jeju-do\n      Jeju-do\n      overseas\n      NaN\n      NaN\n      NaN\n      2020-06-18\n      NaN\n      NaN\n      isolated\n    \n    \n      5164\n      7000000019\n      NaN\n      NaN\n      Bangladesh\n      Jeju-do\n      Jeju-do\n      overseas\n      NaN\n      NaN\n      NaN\n      2020-06-18\n      NaN\n      NaN\n      isolated\n    \n  \n\n5165 rows × 14 columns\n\n\n\n\nfrom datetime import datetime\n\n\npat_rel = patientinfo[['age','confirmed_date','released_date']]\n#pat_rel['diff'] = pat_rel.released_date - pat_rel.confirmed_date\nstr(pat_rel.confirmed_date)\npat_rel.released_date = pd.to_datetime(pat_rel['released_date'], format='%Y %m %d')\npat_rel.confirmed_date = pd.to_datetime(pat_rel['confirmed_date'], format='%Y %m %d')\npat_rel['diff'] = pat_rel.released_date - pat_rel.confirmed_date\npat_rel = pat_rel[:5161] #격리 날짜 없는 것 삭제\ndisplay(pat_rel)\n\n\n\n\n\n  \n    \n      \n      age\n      confirmed_date\n      released_date\n      diff\n    \n  \n  \n    \n      0\n      50s\n      2020-01-23\n      2020-02-05\n      13 days\n    \n    \n      1\n      30s\n      2020-01-30\n      2020-03-02\n      32 days\n    \n    \n      2\n      50s\n      2020-01-30\n      2020-02-19\n      20 days\n    \n    \n      3\n      20s\n      2020-01-30\n      2020-02-15\n      16 days\n    \n    \n      4\n      20s\n      2020-01-31\n      2020-02-24\n      24 days\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      5156\n      30s\n      2020-04-03\n      2020-05-19\n      46 days\n    \n    \n      5157\n      20s\n      2020-04-03\n      2020-05-05\n      32 days\n    \n    \n      5158\n      10s\n      2020-04-14\n      2020-04-26\n      12 days\n    \n    \n      5159\n      30s\n      2020-05-09\n      2020-06-12\n      34 days\n    \n    \n      5160\n      30s\n      2020-05-30\n      2020-06-13\n      14 days\n    \n  \n\n5161 rows × 4 columns\n\n\n\n\ndisplay(pat_rel['diff'].mean()) \ndisplay(pat_rel['diff'].min()) \ndisplay(pat_rel['diff'].max()) \n\nTimedelta('24 days 17:48:39.041614123')\n\n\nTimedelta('0 days 00:00:00')\n\n\nTimedelta('114 days 00:00:00')\n\n\n평균 완치일은 24일\n최대 완치일은 114일 입니다.\n\npat_rel['over_avg'] = np.where(pat_rel['diff']>'24 days 17:48:39.041614123',1,0)\nover_av_released = pat_rel[pat_rel['over_avg']==1]\nunder_av_released = pat_rel[pat_rel['over_avg']==0]\n\nover_av=pd.DataFrame(over_av_released['age'].value_counts().sort_index()).reset_index()\nunder_av=pd.DataFrame(under_av_released['age'].value_counts().sort_index()).reset_index()\n\n#연령대층별로 감염자수가 확연히 다르기때문에 각 연령층별의 비율로 계산\nunder_av['per']=under_av['age']/(under_av['age']+over_av['age']) \nover_av['per']=over_av['age']/(under_av['age']+over_av['age'])\n\n#컬럼 재정리\nunder_av.columns=['age', 'count', 'under_per']\nover_av.columns=['age', 'count', 'over_per']\n\n\nover_av = pd.DataFrame({'age':['0s','10s','20s','30s','40s','50s','60s','70s','80s','90s'],\n                             'count':[7,20,156,90,86,119,90,46,39,8],\n                             'over_per':[0.106061,0.006289,0.026212,0.264856,0.172414,0.135647,0.232877,0.326087,0.2598887,0.487500]})\n\n\nfig = go.Figure()\n\nfig.add_trace(go.Bar(x=under_av.under_per, y=under_av.age, name='빠른 완치 기간',\n                     orientation='h'))\n\nfig.add_trace(go.Bar(x=over_av.over_per, y=over_av.age, name='오랜 완치 기간',\n                     text=over_av.over_per, texttemplate='%{x:.1%}', textposition='inside',\n                    textfont=dict(color='white'),\n                    orientation='h'))\nfig.update_layout(barmode='stack',\n                  paper_bgcolor='rgb(248, 248, 255)',\n                  plot_bgcolor='rgb(248, 248, 255)',\n                 )\nfig.update_layout(title='연령대에 따른 회복 기간')\n\nfig.show()\n\n\n                                                \n\n\n그래프를 본다면 0~20대의 연령츠은 10% 미만으로 평균보다 빠른 완치 기간을 가지고 있음을 알 수 있습니다. 고연령층인 70,80,90대의 경우 32,26,48%로 다른 연령층에 비해서 높기는 하지만 과반을 넘지 않기에 고연령층이라고 모두가 장기간의 회복 기간을 가진다고 판단하기 어렵습니다.\n\n\n\n주제3 - 정부의 정책은 타당했는가? \n\n주제3 - 감염병 경보 단계 공표 시점 \n\npolicy.head()\n\n\n\n\n\n  \n    \n      \n      policy_id\n      country\n      type\n      gov_policy\n      detail\n      start_date\n      end_date\n    \n  \n  \n    \n      0\n      1\n      Korea\n      Alert\n      Infectious Disease Alert Level\n      Level 1 (Blue)\n      2020-01-03\n      2020-01-19\n    \n    \n      1\n      2\n      Korea\n      Alert\n      Infectious Disease Alert Level\n      Level 2 (Yellow)\n      2020-01-20\n      2020-01-27\n    \n    \n      2\n      3\n      Korea\n      Alert\n      Infectious Disease Alert Level\n      Level 3 (Orange)\n      2020-01-28\n      2020-02-22\n    \n    \n      3\n      4\n      Korea\n      Alert\n      Infectious Disease Alert Level\n      Level 4 (Red)\n      2020-02-23\n      NaN\n    \n    \n      4\n      5\n      Korea\n      Immigration\n      Special Immigration Procedure\n      from China\n      2020-02-04\n      NaN\n    \n  \n\n\n\n\n\npolicy.isna().sum() #여기서 end_date의 NA값이 너무 많은 것을 알 수 있다. 따라서 기점을 start_date로 지정하고 사용을 하겠다.\n\npolicy_id      0\ncountry        0\ntype           0\ngov_policy     0\ndetail         2\nstart_date     0\nend_date      37\ndtype: int64\n\n\n\npolicy_alerts = policy[policy.type == 'Alert']\ndisplay(policy_alerts)\n\n\n\n\n\n  \n    \n      \n      policy_id\n      country\n      type\n      gov_policy\n      detail\n      start_date\n      end_date\n    \n  \n  \n    \n      0\n      1\n      Korea\n      Alert\n      Infectious Disease Alert Level\n      Level 1 (Blue)\n      2020-01-03\n      2020-01-19\n    \n    \n      1\n      2\n      Korea\n      Alert\n      Infectious Disease Alert Level\n      Level 2 (Yellow)\n      2020-01-20\n      2020-01-27\n    \n    \n      2\n      3\n      Korea\n      Alert\n      Infectious Disease Alert Level\n      Level 3 (Orange)\n      2020-01-28\n      2020-02-22\n    \n    \n      3\n      4\n      Korea\n      Alert\n      Infectious Disease Alert Level\n      Level 4 (Red)\n      2020-02-23\n      NaN\n    \n  \n\n\n\n\n\nfig, ax = plt.subplots(figsize=(13,7))\nplt.title(\"Infection Disease Alert Level\",size=17)\nplt.plot(time.date.unique(), time.confirmed.diff(),\n         color = 'gray', lw = 3)\nax.set_xticks(ax.get_xticks()[::int(len(time.date.unique())/8)])\n\nfor day, color in zip(policy_alerts.start_date.values[1:], ['yellow','orange','red']):\n    ax.axvline(day, ls=':', color=color, lw=4)\nax.legend(['confirmed','alert level 2','alert level 3','alert level 4'])\nplt.xlabel('date')\nplt.ylabel('Number of Confirmed')\nplt.show()\n\n\n\n\n다음은 감염병의 경보 단계 별로 해당 시점과 확진자의 일일 추이를 나타낸 것으로 2,3단계는 발생을 하고 국내에 들어온 시점에 공표가 되었고 가장 강력한 단계인 4단계는 일일 확진자가 정점에 이르기 전에 공표가 된 사실을 알수 있습니다.\n그렇다면 정부의 거리두기는 어떠하였는지 알아보겠습니다.\n\n\n주제3 - 정부의 거리두기 공표 시점 \n\npolicy_social = policy[policy.type == 'Social'][:-1]\ndisplay(policy_social)\n\n\n\n\n\n  \n    \n      \n      policy_id\n      country\n      type\n      gov_policy\n      detail\n      start_date\n      end_date\n    \n  \n  \n    \n      28\n      29\n      Korea\n      Social\n      Social Distancing Campaign\n      Strong\n      2020-02-29\n      2020-03-21\n    \n    \n      29\n      30\n      Korea\n      Social\n      Social Distancing Campaign\n      Strong\n      2020-03-22\n      2020-04-19\n    \n    \n      30\n      31\n      Korea\n      Social\n      Social Distancing Campaign\n      Weak\n      2020-04-20\n      2020-05-05\n    \n    \n      31\n      32\n      Korea\n      Social\n      Social Distancing Campaign\n      Weak(1st)\n      2020-05-06\n      NaN\n    \n  \n\n\n\n\n\nfig, ax = plt.subplots(figsize=(13,7))\nplt.title(\"Social Distancing Campaign\",size=17)\nplt.plot(time.date.unique(), time.confirmed.diff(),\n         color = 'gray', lw = 3)\nax.set_xticks(ax.get_xticks()[::int(len(time.date.unique())/8)])\n\nfor day, color in zip(policy_social.start_date.values[:], ['red','red','orange']):\n    ax.axvline(day, ls=':', color=color, lw=4)\nax.legend(['confirmed','strong','strong','weak'])\nplt.xlabel('date')\nplt.ylabel('Number of Confirmed')\nplt.show()\n\n\n\n\n3월 23일에 감염병 경고가 4단계 발표 되고 나서 3월 29일부터 사회적 거리두기를 진행 하였습니다. 두번의 강한 거리두기를 유지하고 4월 20일부터 거리두기의 단계를 낮췄습니다. 거리두기를 확진자의 정점에 다다르는 시점에서 발표하고 그 이후 감소하는 일일 확진자의 수를 확인할 수 있다 따라서 분석을 하고 있는 해당 기간 동안은 정부의 방역 정책은 성공을 했다고 할 수 있다\n\n\n\n\n결론 \n\n20대가 분석 기간동안 가장 많이 확진이 된 연령층이었으며, 20대의 인구수가 다른 연령층에 비해서 많아서가 아닌 다른 연령층에 비해서 활동 반경이 넓고 활발하며 불필요한 방문 지역과 장소에 자주 방문을 하였기 때문에 20대가 가장 많이 확진된 연령층이라는 결론을 내렸습니다.\n코로나 바이러스는 고연령층이 될수록 더욱 치명적이라는 사실을 얻었습니다. 치명률이 인구 대비 비율로 살펴보아도 80대 이상이 가장 압도적으로 많았고 60,70대 또한 적지 않은 치명률을 가지고 있음을 알게 되었으며, 완치까지 걸리는 기간은 고연령층일수록 저연령층에 비해서 평균 이상으로 오래 걸리기는 하였으나 과반이상이거나 다른 연령층에 비해서 조금은 많치만 크게 상관성이 있어보지는 않았습니다.\n정부의 거리두기는 당시 신천지로 인해서 일일 확진자가 800명 가량 나오던 시점에 공표되고 그 이후로 분석기간 동안에는 감소세를 보였습니다. 이로 정부의 거리두기는 적어도 당시에는 타당했다라는 결론을 내리게 되었습니다.\n\n코로나 종식 및 예방을 위해서는 해외 유입에 의한 확진자를 차단해야합니다. 현재 입국자에 대한 검사 및 2주 자가격리 등 많은 노력이 진행되고 있습니다. 하지만, 그럼에도 유의사항을 잘 따르지 않는 일부 인원에 의해서 신천지와 같은 큰 집단 감염이 발생될 수 있다는 사실을 잊지 말아야합니다. 따라서, 코로나에 대한 경각심과 인식을 잘 심어주어야하며, 특히나 가장 안일하게 생각하는 20대의 인식 변화를 이끌어야 할 것입니다. 또한, 20대의 행동 패턴 및 방문 경로를 바탕으로 감염 위험이 있는 업종은 특히나 더욱 신경써서 사회적 거리두기, 마스크 착용, 손세정제와 손씻기 등을 더욱 권장하도록 해야합니다.\n분석을 하면서 느낀 한계점은 자료에 etc나 NaN으로 표시된 자료의 형태가 많아서 정확한 원인들을 찾기에 어려움이 있었습니다.\n자료 출처 및 참고 출처 - https://dacon.io/competitions/official/235590/overview/description (데이콘 - 코로나 데이터 시각화 AI 경진대회) - https://www.kaggle.com/datasets/kimjihoo/coronavirusdataset (kaggle - Data Science for COVID-19 in South Korea) - https://chancoding.tistory.com/119 (plotly line plot) - https://plotly.com/python/pie-charts/(about pie plot)"
  },
  {
    "objectID": "Data_Mining.html",
    "href": "Data_Mining.html",
    "title": "Data Mining",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\n\n\n\n\n\n\n\n\n\nJun 7, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nFolium\n\n\nGeopandas\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nFolium\n\n\nGeopandas\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nGeopandas\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nGeopandas\n\n\nFolium\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nGeopandas\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nScipy\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPython\n\n\nSeaborn\n\n\nMatplotlib\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nMar 18, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPython\n\n\nNumpy\n\n\n\n\n\n\n\n\n\n\n\nMar 4, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Data_Visualization/실기_1유형.html",
    "href": "Data_Visualization/실기_1유형.html",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "",
    "text": "실기 1 유형"
  },
  {
    "objectID": "Data_Visualization/실기_1유형.html#데이터-다루기-유형",
    "href": "Data_Visualization/실기_1유형.html#데이터-다루기-유형",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "✅ 데이터 다루기 유형",
    "text": "✅ 데이터 다루기 유형\n\n\n데이터 타입(object, int, float, bool 등)\n기초통계량(평균, 중앙값, 사분위수, IQR, 표준편차 등)\n데이터 인덱싱, 필터링, 정렬, 변경 등\n중복값, 결측치, 이상치 처리 (제거 or 대체)\n데이터 Scaling (데이터 표준화(z), 데이터 정규화(min-max))\n데이터 합치기\n날짜/시간 데이터, index 다루기"
  },
  {
    "objectID": "Data_Visualization/실기_1유형.html#핵심문제-27개-110",
    "href": "Data_Visualization/실기_1유형.html#핵심문제-27개-110",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "✅ 핵심문제 27개 (1~10)",
    "text": "✅ 핵심문제 27개 (1~10)\n\n# 데이터 불러오기\nimport pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"mtcars.txt\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      Hornet Sportabout\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\n# 문제 1\n# mpg 변수의 제 1사분위수를 구하고 정수값으로 출력하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\nQ1 = df['mpg'].quantile(.25)\nprint(round(Q1))\n\n15\n\n\n\n# 문제 2\n# mpg 값이 19이상 21이하인 데이터의 수를 구하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이 1)\ncond1 = (df[(df['mpg']>=19) & (df['mpg']<=21)])\nprint(len(cond1))\n\n# (풀이 2)\n# cond1 = (df['mpg']>=19)\n# cond2 = (df['mpg']<=21)\n# print(len(df[cond1&cond2]))\n\n5\n\n\n\n# 문제 3\n# hp 변수의 IQR 값을 구하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\nQ1 = df['hp'].quantile(.25)\nQ3 = df['hp'].quantile(.75)\nIQR = Q3 - Q1\nprint(IQR)\n\n83.5\n\n\n\n# 문제 4 \n# wt 변수의 상위 10개 값의 총합을 구하여 소수점을 버리고 정수로 출력하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\ntop10 = df.sort_values('wt', ascending=False)\nsum_top10 = sum(top10['wt'].head(10))\nprint(int(sum_top10))\n\n# top_10 = df.sort_values('wt',ascending=False).head(10)\n# print(int(sum(top_10['wt']))) #주의: 소수점 반올림이 아니라 버리는 문제\n\n42\n\n\n\n# 문제 5\n# 전체 자동차에서 cyl가 6인 비율이 얼마인지 소수점 첫째짜리까지 출력하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\ncond1 = len(df[df['cyl']==6])\ncond2 = len(df['cyl'])\nprint(round(cond1/cond2, 1))\n\n0.2\n\n\n\n# 문제 6\n# 첫번째 행부터 순서대로 10개 뽑은 후 mpg 열의 평균값을 반올림하여 정수로 출력하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\ncond1 = df.head(10) # df[:10], df.loc[0:9]\nmean_mpg = cond1['mpg'].mean()\nprint(round(mean_mpg))\n\n20\n\n\n\n# 문제 7\n# 첫번째 행부터 순서대로 50% 까지 데이터를 뽑아 wt 변수의 중앙값을 구하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\n# 50% 데이터에 해당하는 행의 수 (정수로 구하기)\np50 = int(len(df)*0.5)\ndf50 = df[:p50]\nprint(df50['wt'].median())\n\n# len(df)/2\n# cond1 = df[:17]\n# cond2 = cond1['wt'].median()\n# print(cond2)\n\n3.44\n\n\n\n# 문제 8\n# 결측값이 있는 데이터의 수를 출력하시오\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    '판매수' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      날짜\n      제품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3.0\n      300\n    \n    \n      1\n      20220105\n      B\n      NaN\n      400\n    \n    \n      2\n      None\n      None\n      5.0\n      500\n    \n    \n      3\n      20230127\n      B\n      10.0\n      600\n    \n    \n      4\n      20220203\n      A\n      10.0\n      400\n    \n  \n\n\n\n\n\n# (풀이)\nm_value = df.isnull().sum()\nprint(m_value.sum())\n\n5\n\n\n\n# 문제 9 \n# '판매수' 컬럼의 결측값을 판매수의 중앙값으로 대체하고 판매수의 \n#  평균값을 정수로 출력하시오\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    '판매수' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\n\n\n# (풀이)\ndf[df['판매수'].isnull()]\nmedian = df['판매수'].median()\ndf['판매수'] = df['판매수'].fillna(median)\nmean = df['판매수'].mean()\nprint(int(mean))\n\n15\n\n\n\n# 문제 10\n# 판매수 컬럼에 결측치가 있는 행을 제거하고\n# 첫번째 행부터 순서대로 50%까지 데이터를 추출하여\n# 판매수 변수의 Q1(제1사분위수) 값을 정수로 출력하시오\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    '판매수' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\n\n\n# (풀이)\n# 결측치 삭제(행 기준)\ndf = df['판매수'].dropna()\n# 첫번째 행부터 순서대로 50%까지의 데이터 추출\nper50 = int(len(df)*0.5)\ndf = df[:per50]\n# 값구하기\nprint(round(df.quantile(.25)))\n\n# df[df['판매수'].isnull()]\n# df['판매수'] = df['판매수'].dropna()\n# int(len(df['판매수'])/2)\n# cond1 = df['판매수'].head(6)\n# Q1 = cond1.quantile(.25)\n# print(int(Q1))\n\n5"
  },
  {
    "objectID": "Data_Visualization/실기_1유형.html#핵심문제-27개-1120",
    "href": "Data_Visualization/실기_1유형.html#핵심문제-27개-1120",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "✅ 핵심문제 27개 (11~20)",
    "text": "✅ 핵심문제 27개 (11~20)\n\ndf = pd.read_csv('mtcars.txt')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      Hornet Sportabout\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\n# 문제 11\n# cyl가 4인 자동차와 6인 자동차 그룹의 mpg 평균값이 차이를 절대값으로 정수로 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\ncyl_4 = df[df['cyl']==4]\ncyl_4_mean = cyl_4['mpg'].mean()\n\ncyl_6 = df[df['cyl']==6]\ncyl_6_mean = cyl_6['mpg'].mean()\n\nprint(round(abs(cyl_4_mean - cyl_6_mean)))\n\n7\n\n\n\n# 문제 12\n# hp 변수에 대해 데이터표준화(Z-score)를 진행하고 이상치의 수를 구하시오\n# (단, 이상치는 Z값이 1.5를 초과하거나 -1.5미만인 값이다)\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\n# Z = (X-평균) / 표준편차\nstd = df['hp'].std()\nmean_hp = df['hp'].mean()\ndf['zscore'] = (df['hp']-mean_hp)/std\n\ncond1 = (df['zscore']>1.5)\ncond2 = (df['zscore']<-1.5)\nprint(len(df[cond1] + df[cond2]))\n\n2\n\n\n\n# 문제 13\n# mpg 컬럼을 최소최대 Scaling을 진행한 후 0.7보다 큰 값을 가지는 레코드 수를 구하라\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\nfrom sklearn.preprocessing import MinMaxScaler\nmscaler = MinMaxScaler()\ndf['mpg'] = mscaler.fit_transform(df[['mpg']])\nprint(len(df[df['mpg']>0.7]))\n\n# 공식 : (x-min) / (max-min)\n# min_mpg = df['mpg'].min()\n# max_mpg = df['mpg'].max()\n# df['mpg'] = (df['mpg'] - min_mpg)/(max_mpg - min_mpg)\n# cond1 = (df['mpg']>0.7)\n# print(len(df[cond1]))\n\n5\n\n\n\n# 문제 14\n# wt 컬럼에 대해 상자그림 기준으로 이상치의 개수를 구하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\n# 이상치 : Q1, Q3 로부터 1.5*IQR을 넘어가는 값\nQ1 = df['wt'].quantile(.25)\nQ3 = df['wt'].quantile(.75)\nIQR = Q3-Q1\n\nupper = Q3 + (1.5*IQR)\nlower = Q1 - (1.5*IQR)\n\ncond1 = (df['wt'] > upper)\ncond2 = (df['wt'] < lower)\n\nprint(len(df[cond1] + df[cond2]))\n\n3\n\n\n\n# 문제 15\n# 판매수 컬럼의 결측치를 최소값으로 대체하고\n# 결측치가 있을 때와 최소값으로 대체했을 때\n# 평균값의 차이를 절대값으로 정수로 출력하시오\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    '판매수' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf\n\n\n\n\n\n  \n    \n      \n      날짜\n      제품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3.0\n      300\n    \n    \n      1\n      20220105\n      B\n      NaN\n      400\n    \n    \n      2\n      None\n      None\n      5.0\n      500\n    \n    \n      3\n      20230127\n      B\n      10.0\n      600\n    \n    \n      4\n      20220203\n      A\n      10.0\n      400\n    \n    \n      5\n      20220205\n      None\n      10.0\n      500\n    \n    \n      6\n      20230210\n      A\n      15.0\n      500\n    \n    \n      7\n      20230223\n      B\n      15.0\n      600\n    \n    \n      8\n      20230312\n      A\n      20.0\n      600\n    \n    \n      9\n      20230422\n      B\n      NaN\n      700\n    \n    \n      10\n      20220505\n      A\n      30.0\n      600\n    \n    \n      11\n      20230511\n      A\n      40.0\n      600\n    \n  \n\n\n\n\n\n# (풀이)\n# 데이터 복사\ndf2 = df.copy()\n# 최소값으로 결측치 대체\nmin = df['판매수'].min()\ndf2['판매수'] = df2['판매수'].fillna(min)\n\n# 결측치가 있을때, 대체했을때 평균\nm_yes = df['판매수'].mean()\nm_no = df2['판매수'].mean()\n\nprint(round(abs(m_yes - m_no)))\n\n2\n\n\n\n# 문제 16\n# vs변수가 0이 아닌 차량 중에 mpg 값이 가장 큰 차량의 hp 값을 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\ncond1 = df[df['vs']!=0]\n# mpg 값이 가장 큰 차량(내림차순)\ncond1 = cond1.sort_values('mpg',ascending=False)\n# cond1\nprint(cond1['hp'].iloc[0])\n\n65\n\n\n\n# 문제 17\n# gear 변수값이 3, 4인 두 그룹의 hp 표준편차값의 차이를 절대값으로 \n# 소수점 첫째자리까지 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\n# 두개 그룹으로 필터링\ncond1 = df[df['gear']==3]\ncond2 = df[df['gear']==4]\n# std 구하기\nstd3 = cond1['hp'].std()\nstd4 = cond2['hp'].std()\n\nprint(round(abs(std3 - std4),1))\n\n21.8\n\n\n\n# 문제 18\n# gear 변수의 갑별로 그룹화하여 mpg 평균값을 산출하고\n# 평균값이 높은 그룹의 mpg 제3사분위수 값을 구하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\ndf['gear'].unique()\ngear3 = df[df['gear']==3]\ngear4 = df[df['gear']==4]\ngear5 = df[df['gear']==5]\n\nm_3 = gear3['mpg'].mean()\nm_4 = gear4['mpg'].mean()\nm_5 = gear5['mpg'].mean()\n\n(m_3, m_4, m_5) # m_4의 평균값이 가장 큼\nQ3 = gear4['mpg'].quantile(.75)\nprint(Q3)\n\n28.075\n\n\n\n# (풀이_v2)\n# gear, mpg 변수만 필터링\ndf = df.loc[:, ['gear', 'mpg']]\n\n# gear 변수로 그룹화하여 mpg 평균값 보기\n# print(df.groupby('gear').mean()) # gear 4그룹이 제일 높음\n\n# gear=4인 그룹의 mpg Q3 구하기\ngear4 = df[df['gear']==4]\nprint(gear4['mpg'].quantile(.75))\n\n28.075\n\n\n\n# 문제 19\n# hp 항목의 상위 7번째 값으로 상위 7개 값을 변환한 후,\n# hp가 150 이상인 데이터를 추출하여 hp의 평균값을 반올림하여 정수로 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\n# hp 열 기준으로 내림차순 정렬\ndf = df.sort_values('hp', ascending=False)\n\n# 인덱스 초기화 - 내림차순으로 정렬시 최초의 인덱스로 있기에\ndf = df.reset_index(drop=True) # drop=True 는 기존 index 삭제\n# print(df)\n\n# hp 상위 7번째 값 확인\ntop7 = df['hp'].loc[6] # top7 = 205\n\n# np.where 활용\nimport numpy as np\ndf['hp'] = np.where(df['hp'] >= 205, top7, df['hp'])\n# np.where(조건, 조건에 해당할 때 값, 그렇지 않을 때 값)\n\n# hp 150이상인 데이터\ncond1 = (df['hp']>=150)\ndf = df[cond1]\nprint(round(df['hp'].mean()))\n\n187\n\n\n\n# 문제 20\n# car 변수에 Merc 무구가 포함된 자동차의 mpg 평균값을 정수로 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\ndf2 = df[df['car'].str.contains('Merc')] # 문자열 추출 알아두기\nprint(round(df2['mpg'].mean()))\n\n# 시험환경에서 답구하는 방법(reset_index() 사용 후)\n# 시험에서는 car가 index로 되어 있음\n# df.reset_index(inplace=True)\n# df2 = df[df['index'].str.contains(\"Merc\")] \n# print(round(df2['mpg'].mean()))\n\n19"
  },
  {
    "objectID": "Data_Visualization/실기_1유형.html#핵심문제-27개-2127",
    "href": "Data_Visualization/실기_1유형.html#핵심문제-27개-2127",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "✅ 핵심문제 27개 (21~27)",
    "text": "✅ 핵심문제 27개 (21~27)\n\n# 문제 21\n# 22년 1분기 A제품의 매출액을 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    '판매수' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      날짜\n      제품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# # (풀이)\n# cond1 = df[df['날짜']<='20220431']\n# cond2 = cond1[cond1['제품']=='A']\n# cond2['매출액'] = (cond2['판매수']*cond2['개당수익'])\n# cond2\n\n# print(cond2['매출액'].sum())\n\n\n# (풀이_v2)\n# 데이터 타입 datetime으로 변경\ndf['날짜'] = pd.to_datetime(df['날짜'])\n\n# 년, 월, 일 변수(열) 추가하기\ndf['year'] = df['날짜'].dt.year\ndf['month'] = df['날짜'].dt.month\ndf['day'] = df['날짜'].dt.day\n\n# 매출액 변수 추가하기\ndf['매출액'] = df['판매수'] * df['개당수익']\n\n# 22년으로 필터링\ndf = df[df['year']==2022]\ndf.head()\n\n# 1,2,3월 매출액 계산\nm1 = df[df['month']==1]['매출액'].sum()\nm2 = df[df['month']==2]['매출액'].sum()\nm3 = df[df['month']==3]['매출액'].sum()\nprint(m1+m2+m3)\n\n11900\n\n\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['날짜'] = pd.to_datetime(df['날짜'])\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['year'] = df['날짜'].dt.year\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['month'] = df['날짜'].dt.month\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['day'] = df['날짜'].dt.day\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['매출액'] = df['판매수'] * df['개당수익']\n\n\n\n# 문제 22\n# 22년과 23년의 총 매출액 차이를 절대값으로 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    '판매수' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      날짜\n      제품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# (풀이)\n# 데이터 타입 datetime으로 변경\ndf['날짜'] = pd.to_datetime(df['날짜'])\n\n# 년 변수(열) 추가하기\ndf['year'] = df['날짜'].dt.year\n\n# 매출액 변수 추가하기\ndf['매출액'] = df['판매수'] * df['개당수익']\n\ncond22 = df[df['year']==2022]\ncond23 = df[df['year']==2023]\n\nprint(abs((cond22['매출액'].sum()) - (cond23['매출액'].sum())))\n\n48600\n\n\n\n# 문제 23\n# 23년 총 매출액이 큰 제품의 23년 판매수를 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    '판매수' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      날짜\n      제품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# 데이터 타입 datetime으로 변경\ndf['날짜'] = pd.to_datetime(df['날짜'])\n\n# 년 변수(열) 추가하기\ndf['year'] = df['날짜'].dt.year\n\n# 매출액 변수 추가하기\ndf['매출액'] = df['판매수'] * df['개당수익']\n\ndf = df[df['year']==2023]\n# df.head()\n\n# 23년 A 매출액과 B 매출액 별도로 구하기\n# A 매출액\ndf_a = df[df['제품']=='A']\na_sales = df_a['매출액'].sum()\n# print(a_sales) # 46000\n\n# B 매출액\ndf_b = df[df['제품']=='B']\nb_sales = df_b['매출액'].sum()\n# print(b_sales) # 32500\n\n# A 제품의 23년 판매수\na_sum = df_a['판매수'].sum()\nprint(a_sum)\n\n80\n\n\n\n# 문제 24\n# 매출액이 4천원 초과, 1만원 미만인 데이터 수를 출력하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    '판매수' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      날짜\n      제품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# (풀이)\ndf['매출액'] = df['판매수'] * df['개당수익']\ncond1 = df['매출액']>4000\ncond2 = df['매출액']<10000\n\ndf = df[cond1&cond2]\nprint(len(df))\n\n4\n\n\n\n# 문제 25\n# 23년 9월 24일 16:00~22:00 사이에 전체 제품의 판매수를 구하시오\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600]})\ntime = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf['time'] = time\ndf = df[['time', '물품', '판매수', '개당수익']]\ndf\n\n\n\n\n\n  \n    \n      \n      time\n      물품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      1\n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2\n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      3\n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      4\n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      5\n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      6\n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# (풀이)\n# 컬럼과 인덱스에 둘다 time 변수를 놓고 풀이\n# 데이터 타입 datetime으로 변경 (항상 df.info()로 데이터 타입 확인할 것)\ndf['time'] = pd.to_datetime(df['time'])\n\n# index 새로 지정\ndf = df.set_index('time', drop=False) # default는 True\n\n# 9월 24일 필터링 // df['변수'].between( , )\ndf_after = df[df['time'].between('2023-09-24', '2023-09-25')] # 25일은 미포함\n\n# 시간 필터링 16:00 ~ 22:00 (주의: 시간이 index에 위치해야 함)\ndf = df_after.between_time(start_time='16:00', end_time='22:00') # 포함 기준\n\n# 전체 판매수 구하기\nprint(df['판매수'].sum())\n\n25\n\n\n\n# (풀이2) // 위에 df 새로 불러와서 실행해보기\n# 데이터 타입 datetime으로 변경\ndf['time'] = pd.to_datetime(df['time'])\n\n# index 새로 지정\ndf = df.set_index('time')\n\n# loc 함수로 필터링\ndf = df.loc[(df.index >= '2023-09-24 16:00:00') & (df.index <= '2023-09-24 22:00:00')]\n\n# 전체 판매수 구하기\nprint(df['판매수'].sum())\n\n25\n\n\n\n# 문제 26\n# 9월 25일 00:10~12:00까지의 B물품의 매출액 총합을 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600]})\ndf['time'] = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf = df[['time', '물품', '판매수', '개당수익']]\ndf = df.set_index('time',drop=True)\ndf\n\n\n\n\n\n  \n    \n      \n      물품\n      판매수\n      개당수익\n    \n    \n      time\n      \n      \n      \n    \n  \n  \n    \n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# (풀이)\n# 매출액 변수 추가\ndf['매출액'] = df['판매수'] * df['개당수익']\n\n# loc 함수로 필터링\ndf = df.loc[(df.index >= '2023-09-25 00:10:00') & (df.index <= '2023-09-25 12:00:00')]\n\n# B 물품의 매출액 총합\ncond1 = df[df['물품']=='B']\nprint(cond1['매출액'].sum())\n\n26500\n\n\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\1645811142.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['매출액'] = df['판매수'] * df['개당수익']\n\n\n\n# 문제 27\n# 9월 24일 12:00~24:00까지의 A물품의 매출액 총합을 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600]})\ndf['time'] = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf = df[['time', '물품', '판매수', '개당수익']]\ndf = df.set_index('time',drop=True)\ndf\n\n\n\n\n\n  \n    \n      \n      물품\n      판매수\n      개당수익\n    \n    \n      time\n      \n      \n      \n    \n  \n  \n    \n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# (풀이)\n# 매출액 변수 추가\ndf['매출액'] = df['판매수'] * df['개당수익']\n\n# loc 함수로 필터링\ndf = df.loc[(df.index >= '2023-09-24 12:00:00') & (df.index < '2023-09-25 00:00:00')]\n\n# A 물품의 매출액 총합\ncond1 = df[df['물품']=='A']\nprint(cond1['매출액'].sum())\n\n10000"
  },
  {
    "objectID": "Data_Visualization.html",
    "href": "Data_Visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Profile",
    "section": "",
    "text": "한남대학교 비즈니스통계학과 | 2018.03 ~ 2021.02\n한남대학교 빅데이터응용학과 | 2021.03 ~ 2024.02 \n\n\n\n\n\n과학기술정보통신부 장관상 | 2022.09\n한국수자원공사 사장상 | 2022.10 \n\n\n\n\n\n데이터 청년 캠퍼스 | 2022.08 ~ 2022.09\n빅리더 AI 산학협력 | 2022.08 ~ 2022.10\n멋쟁이사자처럼 11기 | 2023.03 ~ 2023.12\n태블로 신병훈련소 21기 | 2023.10 ~ 2023.11\n\n\n\n\n\n데이터 분석 준전문가 (ADsP) | 2023.06\nSQL 개발자 (SQLD) | 2023.10\n빅데이터분석기사_필기 (BAE) | 2023.10"
  },
  {
    "objectID": "OpenData_Analysis/실기_1유형.html",
    "href": "OpenData_Analysis/실기_1유형.html",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "",
    "text": "실기 1 유형"
  },
  {
    "objectID": "OpenData_Analysis/실기_1유형.html#데이터-다루기-유형",
    "href": "OpenData_Analysis/실기_1유형.html#데이터-다루기-유형",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "✅ 데이터 다루기 유형",
    "text": "✅ 데이터 다루기 유형\n\n\n데이터 타입(object, int, float, bool 등)\n기초통계량(평균, 중앙값, 사분위수, IQR, 표준편차 등)\n데이터 인덱싱, 필터링, 정렬, 변경 등\n중복값, 결측치, 이상치 처리 (제거 or 대체)\n데이터 Scaling (데이터 표준화(z), 데이터 정규화(min-max))\n데이터 합치기\n날짜/시간 데이터, index 다루기"
  },
  {
    "objectID": "OpenData_Analysis/실기_1유형.html#핵심문제-27개-110",
    "href": "OpenData_Analysis/실기_1유형.html#핵심문제-27개-110",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "✅ 핵심문제 27개 (1~10)",
    "text": "✅ 핵심문제 27개 (1~10)\n\n# 데이터 불러오기\nimport pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"mtcars.txt\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      Hornet Sportabout\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\n# 문제 1\n# mpg 변수의 제 1사분위수를 구하고 정수값으로 출력하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\nQ1 = df['mpg'].quantile(.25)\nprint(round(Q1))\n\n15\n\n\n\n# 문제 2\n# mpg 값이 19이상 21이하인 데이터의 수를 구하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이 1)\ncond1 = (df[(df['mpg']>=19) & (df['mpg']<=21)])\nprint(len(cond1))\n\n# (풀이 2)\n# cond1 = (df['mpg']>=19)\n# cond2 = (df['mpg']<=21)\n# print(len(df[cond1&cond2]))\n\n5\n\n\n\n# 문제 3\n# hp 변수의 IQR 값을 구하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\nQ1 = df['hp'].quantile(.25)\nQ3 = df['hp'].quantile(.75)\nIQR = Q3 - Q1\nprint(IQR)\n\n83.5\n\n\n\n# 문제 4 \n# wt 변수의 상위 10개 값의 총합을 구하여 소수점을 버리고 정수로 출력하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\ntop10 = df.sort_values('wt', ascending=False)\nsum_top10 = sum(top10['wt'].head(10))\nprint(int(sum_top10))\n\n# top_10 = df.sort_values('wt',ascending=False).head(10)\n# print(int(sum(top_10['wt']))) #주의: 소수점 반올림이 아니라 버리는 문제\n\n42\n\n\n\n# 문제 5\n# 전체 자동차에서 cyl가 6인 비율이 얼마인지 소수점 첫째짜리까지 출력하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\ncond1 = len(df[df['cyl']==6])\ncond2 = len(df['cyl'])\nprint(round(cond1/cond2, 1))\n\n0.2\n\n\n\n# 문제 6\n# 첫번째 행부터 순서대로 10개 뽑은 후 mpg 열의 평균값을 반올림하여 정수로 출력하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\ncond1 = df.head(10) # df[:10], df.loc[0:9]\nmean_mpg = cond1['mpg'].mean()\nprint(round(mean_mpg))\n\n20\n\n\n\n# 문제 7\n# 첫번째 행부터 순서대로 50% 까지 데이터를 뽑아 wt 변수의 중앙값을 구하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\n# 50% 데이터에 해당하는 행의 수 (정수로 구하기)\np50 = int(len(df)*0.5)\ndf50 = df[:p50]\nprint(df50['wt'].median())\n\n# len(df)/2\n# cond1 = df[:17]\n# cond2 = cond1['wt'].median()\n# print(cond2)\n\n3.44\n\n\n\n# 문제 8\n# 결측값이 있는 데이터의 수를 출력하시오\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    '판매수' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      날짜\n      제품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3.0\n      300\n    \n    \n      1\n      20220105\n      B\n      NaN\n      400\n    \n    \n      2\n      None\n      None\n      5.0\n      500\n    \n    \n      3\n      20230127\n      B\n      10.0\n      600\n    \n    \n      4\n      20220203\n      A\n      10.0\n      400\n    \n  \n\n\n\n\n\n# (풀이)\nm_value = df.isnull().sum()\nprint(m_value.sum())\n\n5\n\n\n\n# 문제 9 \n# '판매수' 컬럼의 결측값을 판매수의 중앙값으로 대체하고 판매수의 \n#  평균값을 정수로 출력하시오\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    '판매수' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\n\n\n# (풀이)\ndf[df['판매수'].isnull()]\nmedian = df['판매수'].median()\ndf['판매수'] = df['판매수'].fillna(median)\nmean = df['판매수'].mean()\nprint(int(mean))\n\n15\n\n\n\n# 문제 10\n# 판매수 컬럼에 결측치가 있는 행을 제거하고\n# 첫번째 행부터 순서대로 50%까지 데이터를 추출하여\n# 판매수 변수의 Q1(제1사분위수) 값을 정수로 출력하시오\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    '판매수' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\n\n\n# (풀이)\n# 결측치 삭제(행 기준)\ndf = df['판매수'].dropna()\n# 첫번째 행부터 순서대로 50%까지의 데이터 추출\nper50 = int(len(df)*0.5)\ndf = df[:per50]\n# 값구하기\nprint(round(df.quantile(.25)))\n\n# df[df['판매수'].isnull()]\n# df['판매수'] = df['판매수'].dropna()\n# int(len(df['판매수'])/2)\n# cond1 = df['판매수'].head(6)\n# Q1 = cond1.quantile(.25)\n# print(int(Q1))\n\n5"
  },
  {
    "objectID": "OpenData_Analysis/실기_1유형.html#핵심문제-27개-1120",
    "href": "OpenData_Analysis/실기_1유형.html#핵심문제-27개-1120",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "✅ 핵심문제 27개 (11~20)",
    "text": "✅ 핵심문제 27개 (11~20)\n\ndf = pd.read_csv('mtcars.txt')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      Hornet Sportabout\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\n# 문제 11\n# cyl가 4인 자동차와 6인 자동차 그룹의 mpg 평균값이 차이를 절대값으로 정수로 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\ncyl_4 = df[df['cyl']==4]\ncyl_4_mean = cyl_4['mpg'].mean()\n\ncyl_6 = df[df['cyl']==6]\ncyl_6_mean = cyl_6['mpg'].mean()\n\nprint(round(abs(cyl_4_mean - cyl_6_mean)))\n\n7\n\n\n\n# 문제 12\n# hp 변수에 대해 데이터표준화(Z-score)를 진행하고 이상치의 수를 구하시오\n# (단, 이상치는 Z값이 1.5를 초과하거나 -1.5미만인 값이다)\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\n# Z = (X-평균) / 표준편차\nstd = df['hp'].std()\nmean_hp = df['hp'].mean()\ndf['zscore'] = (df['hp']-mean_hp)/std\n\ncond1 = (df['zscore']>1.5)\ncond2 = (df['zscore']<-1.5)\nprint(len(df[cond1] + df[cond2]))\n\n2\n\n\n\n# 문제 13\n# mpg 컬럼을 최소최대 Scaling을 진행한 후 0.7보다 큰 값을 가지는 레코드 수를 구하라\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\nfrom sklearn.preprocessing import MinMaxScaler\nmscaler = MinMaxScaler()\ndf['mpg'] = mscaler.fit_transform(df[['mpg']])\nprint(len(df[df['mpg']>0.7]))\n\n# 공식 : (x-min) / (max-min)\n# min_mpg = df['mpg'].min()\n# max_mpg = df['mpg'].max()\n# df['mpg'] = (df['mpg'] - min_mpg)/(max_mpg - min_mpg)\n# cond1 = (df['mpg']>0.7)\n# print(len(df[cond1]))\n\n5\n\n\n\n# 문제 14\n# wt 컬럼에 대해 상자그림 기준으로 이상치의 개수를 구하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\n# 이상치 : Q1, Q3 로부터 1.5*IQR을 넘어가는 값\nQ1 = df['wt'].quantile(.25)\nQ3 = df['wt'].quantile(.75)\nIQR = Q3-Q1\n\nupper = Q3 + (1.5*IQR)\nlower = Q1 - (1.5*IQR)\n\ncond1 = (df['wt'] > upper)\ncond2 = (df['wt'] < lower)\n\nprint(len(df[cond1] + df[cond2]))\n\n3\n\n\n\n# 문제 15\n# 판매수 컬럼의 결측치를 최소값으로 대체하고\n# 결측치가 있을 때와 최소값으로 대체했을 때\n# 평균값의 차이를 절대값으로 정수로 출력하시오\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    '판매수' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf\n\n\n\n\n\n  \n    \n      \n      날짜\n      제품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3.0\n      300\n    \n    \n      1\n      20220105\n      B\n      NaN\n      400\n    \n    \n      2\n      None\n      None\n      5.0\n      500\n    \n    \n      3\n      20230127\n      B\n      10.0\n      600\n    \n    \n      4\n      20220203\n      A\n      10.0\n      400\n    \n    \n      5\n      20220205\n      None\n      10.0\n      500\n    \n    \n      6\n      20230210\n      A\n      15.0\n      500\n    \n    \n      7\n      20230223\n      B\n      15.0\n      600\n    \n    \n      8\n      20230312\n      A\n      20.0\n      600\n    \n    \n      9\n      20230422\n      B\n      NaN\n      700\n    \n    \n      10\n      20220505\n      A\n      30.0\n      600\n    \n    \n      11\n      20230511\n      A\n      40.0\n      600\n    \n  \n\n\n\n\n\n# (풀이)\n# 데이터 복사\ndf2 = df.copy()\n# 최소값으로 결측치 대체\nmin = df['판매수'].min()\ndf2['판매수'] = df2['판매수'].fillna(min)\n\n# 결측치가 있을때, 대체했을때 평균\nm_yes = df['판매수'].mean()\nm_no = df2['판매수'].mean()\n\nprint(round(abs(m_yes - m_no)))\n\n2\n\n\n\n# 문제 16\n# vs변수가 0이 아닌 차량 중에 mpg 값이 가장 큰 차량의 hp 값을 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\ncond1 = df[df['vs']!=0]\n# mpg 값이 가장 큰 차량(내림차순)\ncond1 = cond1.sort_values('mpg',ascending=False)\n# cond1\nprint(cond1['hp'].iloc[0])\n\n65\n\n\n\n# 문제 17\n# gear 변수값이 3, 4인 두 그룹의 hp 표준편차값의 차이를 절대값으로 \n# 소수점 첫째자리까지 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\n# 두개 그룹으로 필터링\ncond1 = df[df['gear']==3]\ncond2 = df[df['gear']==4]\n# std 구하기\nstd3 = cond1['hp'].std()\nstd4 = cond2['hp'].std()\n\nprint(round(abs(std3 - std4),1))\n\n21.8\n\n\n\n# 문제 18\n# gear 변수의 갑별로 그룹화하여 mpg 평균값을 산출하고\n# 평균값이 높은 그룹의 mpg 제3사분위수 값을 구하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\ndf['gear'].unique()\ngear3 = df[df['gear']==3]\ngear4 = df[df['gear']==4]\ngear5 = df[df['gear']==5]\n\nm_3 = gear3['mpg'].mean()\nm_4 = gear4['mpg'].mean()\nm_5 = gear5['mpg'].mean()\n\n(m_3, m_4, m_5) # m_4의 평균값이 가장 큼\nQ3 = gear4['mpg'].quantile(.75)\nprint(Q3)\n\n28.075\n\n\n\n# (풀이_v2)\n# gear, mpg 변수만 필터링\ndf = df.loc[:, ['gear', 'mpg']]\n\n# gear 변수로 그룹화하여 mpg 평균값 보기\n# print(df.groupby('gear').mean()) # gear 4그룹이 제일 높음\n\n# gear=4인 그룹의 mpg Q3 구하기\ngear4 = df[df['gear']==4]\nprint(gear4['mpg'].quantile(.75))\n\n28.075\n\n\n\n# 문제 19\n# hp 항목의 상위 7번째 값으로 상위 7개 값을 변환한 후,\n# hp가 150 이상인 데이터를 추출하여 hp의 평균값을 반올림하여 정수로 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\n# hp 열 기준으로 내림차순 정렬\ndf = df.sort_values('hp', ascending=False)\n\n# 인덱스 초기화 - 내림차순으로 정렬시 최초의 인덱스로 있기에\ndf = df.reset_index(drop=True) # drop=True 는 기존 index 삭제\n# print(df)\n\n# hp 상위 7번째 값 확인\ntop7 = df['hp'].loc[6] # top7 = 205\n\n# np.where 활용\nimport numpy as np\ndf['hp'] = np.where(df['hp'] >= 205, top7, df['hp'])\n# np.where(조건, 조건에 해당할 때 값, 그렇지 않을 때 값)\n\n# hp 150이상인 데이터\ncond1 = (df['hp']>=150)\ndf = df[cond1]\nprint(round(df['hp'].mean()))\n\n187\n\n\n\n# 문제 20\n# car 변수에 Merc 무구가 포함된 자동차의 mpg 평균값을 정수로 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\ndf2 = df[df['car'].str.contains('Merc')] # 문자열 추출 알아두기\nprint(round(df2['mpg'].mean()))\n\n# 시험환경에서 답구하는 방법(reset_index() 사용 후)\n# 시험에서는 car가 index로 되어 있음\n# df.reset_index(inplace=True)\n# df2 = df[df['index'].str.contains(\"Merc\")] \n# print(round(df2['mpg'].mean()))\n\n19"
  },
  {
    "objectID": "OpenData_Analysis/실기_1유형.html#핵심문제-27개-2127",
    "href": "OpenData_Analysis/실기_1유형.html#핵심문제-27개-2127",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "✅ 핵심문제 27개 (21~27)",
    "text": "✅ 핵심문제 27개 (21~27)\n\n# 문제 21\n# 22년 1분기 A제품의 매출액을 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    '판매수' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      날짜\n      제품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# # (풀이)\n# cond1 = df[df['날짜']<='20220431']\n# cond2 = cond1[cond1['제품']=='A']\n# cond2['매출액'] = (cond2['판매수']*cond2['개당수익'])\n# cond2\n\n# print(cond2['매출액'].sum())\n\n\n# (풀이_v2)\n# 데이터 타입 datetime으로 변경\ndf['날짜'] = pd.to_datetime(df['날짜'])\n\n# 년, 월, 일 변수(열) 추가하기\ndf['year'] = df['날짜'].dt.year\ndf['month'] = df['날짜'].dt.month\ndf['day'] = df['날짜'].dt.day\n\n# 매출액 변수 추가하기\ndf['매출액'] = df['판매수'] * df['개당수익']\n\n# 22년으로 필터링\ndf = df[df['year']==2022]\ndf.head()\n\n# 1,2,3월 매출액 계산\nm1 = df[df['month']==1]['매출액'].sum()\nm2 = df[df['month']==2]['매출액'].sum()\nm3 = df[df['month']==3]['매출액'].sum()\nprint(m1+m2+m3)\n\n11900\n\n\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['날짜'] = pd.to_datetime(df['날짜'])\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['year'] = df['날짜'].dt.year\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['month'] = df['날짜'].dt.month\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['day'] = df['날짜'].dt.day\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['매출액'] = df['판매수'] * df['개당수익']\n\n\n\n# 문제 22\n# 22년과 23년의 총 매출액 차이를 절대값으로 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    '판매수' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      날짜\n      제품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# (풀이)\n# 데이터 타입 datetime으로 변경\ndf['날짜'] = pd.to_datetime(df['날짜'])\n\n# 년 변수(열) 추가하기\ndf['year'] = df['날짜'].dt.year\n\n# 매출액 변수 추가하기\ndf['매출액'] = df['판매수'] * df['개당수익']\n\ncond22 = df[df['year']==2022]\ncond23 = df[df['year']==2023]\n\nprint(abs((cond22['매출액'].sum()) - (cond23['매출액'].sum())))\n\n48600\n\n\n\n# 문제 23\n# 23년 총 매출액이 큰 제품의 23년 판매수를 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    '판매수' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      날짜\n      제품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# 데이터 타입 datetime으로 변경\ndf['날짜'] = pd.to_datetime(df['날짜'])\n\n# 년 변수(열) 추가하기\ndf['year'] = df['날짜'].dt.year\n\n# 매출액 변수 추가하기\ndf['매출액'] = df['판매수'] * df['개당수익']\n\ndf = df[df['year']==2023]\n# df.head()\n\n# 23년 A 매출액과 B 매출액 별도로 구하기\n# A 매출액\ndf_a = df[df['제품']=='A']\na_sales = df_a['매출액'].sum()\n# print(a_sales) # 46000\n\n# B 매출액\ndf_b = df[df['제품']=='B']\nb_sales = df_b['매출액'].sum()\n# print(b_sales) # 32500\n\n# A 제품의 23년 판매수\na_sum = df_a['판매수'].sum()\nprint(a_sum)\n\n80\n\n\n\n# 문제 24\n# 매출액이 4천원 초과, 1만원 미만인 데이터 수를 출력하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    '판매수' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      날짜\n      제품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# (풀이)\ndf['매출액'] = df['판매수'] * df['개당수익']\ncond1 = df['매출액']>4000\ncond2 = df['매출액']<10000\n\ndf = df[cond1&cond2]\nprint(len(df))\n\n4\n\n\n\n# 문제 25\n# 23년 9월 24일 16:00~22:00 사이에 전체 제품의 판매수를 구하시오\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600]})\ntime = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf['time'] = time\ndf = df[['time', '물품', '판매수', '개당수익']]\ndf\n\n\n\n\n\n  \n    \n      \n      time\n      물품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      1\n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2\n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      3\n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      4\n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      5\n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      6\n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# (풀이)\n# 컬럼과 인덱스에 둘다 time 변수를 놓고 풀이\n# 데이터 타입 datetime으로 변경 (항상 df.info()로 데이터 타입 확인할 것)\ndf['time'] = pd.to_datetime(df['time'])\n\n# index 새로 지정\ndf = df.set_index('time', drop=False) # default는 True\n\n# 9월 24일 필터링 // df['변수'].between( , )\ndf_after = df[df['time'].between('2023-09-24', '2023-09-25')] # 25일은 미포함\n\n# 시간 필터링 16:00 ~ 22:00 (주의: 시간이 index에 위치해야 함)\ndf = df_after.between_time(start_time='16:00', end_time='22:00') # 포함 기준\n\n# 전체 판매수 구하기\nprint(df['판매수'].sum())\n\n25\n\n\n\n# (풀이2) // 위에 df 새로 불러와서 실행해보기\n# 데이터 타입 datetime으로 변경\ndf['time'] = pd.to_datetime(df['time'])\n\n# index 새로 지정\ndf = df.set_index('time')\n\n# loc 함수로 필터링\ndf = df.loc[(df.index >= '2023-09-24 16:00:00') & (df.index <= '2023-09-24 22:00:00')]\n\n# 전체 판매수 구하기\nprint(df['판매수'].sum())\n\n25\n\n\n\n# 문제 26\n# 9월 25일 00:10~12:00까지의 B물품의 매출액 총합을 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600]})\ndf['time'] = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf = df[['time', '물품', '판매수', '개당수익']]\ndf = df.set_index('time',drop=True)\ndf\n\n\n\n\n\n  \n    \n      \n      물품\n      판매수\n      개당수익\n    \n    \n      time\n      \n      \n      \n    \n  \n  \n    \n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# (풀이)\n# 매출액 변수 추가\ndf['매출액'] = df['판매수'] * df['개당수익']\n\n# loc 함수로 필터링\ndf = df.loc[(df.index >= '2023-09-25 00:10:00') & (df.index <= '2023-09-25 12:00:00')]\n\n# B 물품의 매출액 총합\ncond1 = df[df['물품']=='B']\nprint(cond1['매출액'].sum())\n\n26500\n\n\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\1645811142.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['매출액'] = df['판매수'] * df['개당수익']\n\n\n\n# 문제 27\n# 9월 24일 12:00~24:00까지의 A물품의 매출액 총합을 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600]})\ndf['time'] = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf = df[['time', '물품', '판매수', '개당수익']]\ndf = df.set_index('time',drop=True)\ndf\n\n\n\n\n\n  \n    \n      \n      물품\n      판매수\n      개당수익\n    \n    \n      time\n      \n      \n      \n    \n  \n  \n    \n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# (풀이)\n# 매출액 변수 추가\ndf['매출액'] = df['판매수'] * df['개당수익']\n\n# loc 함수로 필터링\ndf = df.loc[(df.index >= '2023-09-24 12:00:00') & (df.index < '2023-09-25 00:00:00')]\n\n# A 물품의 매출액 총합\ncond1 = df[df['물품']=='A']\nprint(cond1['매출액'].sum())\n\n10000"
  },
  {
    "objectID": "OpenData_Analysis.html",
    "href": "OpenData_Analysis.html",
    "title": "OpenData Analysis",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Regression_Analysis/실기_1유형.html",
    "href": "Regression_Analysis/실기_1유형.html",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "",
    "text": "실기 1 유형"
  },
  {
    "objectID": "Regression_Analysis/실기_1유형.html#데이터-다루기-유형",
    "href": "Regression_Analysis/실기_1유형.html#데이터-다루기-유형",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "✅ 데이터 다루기 유형",
    "text": "✅ 데이터 다루기 유형\n\n\n데이터 타입(object, int, float, bool 등)\n기초통계량(평균, 중앙값, 사분위수, IQR, 표준편차 등)\n데이터 인덱싱, 필터링, 정렬, 변경 등\n중복값, 결측치, 이상치 처리 (제거 or 대체)\n데이터 Scaling (데이터 표준화(z), 데이터 정규화(min-max))\n데이터 합치기\n날짜/시간 데이터, index 다루기"
  },
  {
    "objectID": "Regression_Analysis/실기_1유형.html#핵심문제-27개-110",
    "href": "Regression_Analysis/실기_1유형.html#핵심문제-27개-110",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "✅ 핵심문제 27개 (1~10)",
    "text": "✅ 핵심문제 27개 (1~10)\n\n# 데이터 불러오기\nimport pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"mtcars.txt\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      Hornet Sportabout\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\n# 문제 1\n# mpg 변수의 제 1사분위수를 구하고 정수값으로 출력하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\nQ1 = df['mpg'].quantile(.25)\nprint(round(Q1))\n\n15\n\n\n\n# 문제 2\n# mpg 값이 19이상 21이하인 데이터의 수를 구하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이 1)\ncond1 = (df[(df['mpg']>=19) & (df['mpg']<=21)])\nprint(len(cond1))\n\n# (풀이 2)\n# cond1 = (df['mpg']>=19)\n# cond2 = (df['mpg']<=21)\n# print(len(df[cond1&cond2]))\n\n5\n\n\n\n# 문제 3\n# hp 변수의 IQR 값을 구하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\nQ1 = df['hp'].quantile(.25)\nQ3 = df['hp'].quantile(.75)\nIQR = Q3 - Q1\nprint(IQR)\n\n83.5\n\n\n\n# 문제 4 \n# wt 변수의 상위 10개 값의 총합을 구하여 소수점을 버리고 정수로 출력하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\ntop10 = df.sort_values('wt', ascending=False)\nsum_top10 = sum(top10['wt'].head(10))\nprint(int(sum_top10))\n\n# top_10 = df.sort_values('wt',ascending=False).head(10)\n# print(int(sum(top_10['wt']))) #주의: 소수점 반올림이 아니라 버리는 문제\n\n42\n\n\n\n# 문제 5\n# 전체 자동차에서 cyl가 6인 비율이 얼마인지 소수점 첫째짜리까지 출력하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\ncond1 = len(df[df['cyl']==6])\ncond2 = len(df['cyl'])\nprint(round(cond1/cond2, 1))\n\n0.2\n\n\n\n# 문제 6\n# 첫번째 행부터 순서대로 10개 뽑은 후 mpg 열의 평균값을 반올림하여 정수로 출력하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\ncond1 = df.head(10) # df[:10], df.loc[0:9]\nmean_mpg = cond1['mpg'].mean()\nprint(round(mean_mpg))\n\n20\n\n\n\n# 문제 7\n# 첫번째 행부터 순서대로 50% 까지 데이터를 뽑아 wt 변수의 중앙값을 구하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\n# 50% 데이터에 해당하는 행의 수 (정수로 구하기)\np50 = int(len(df)*0.5)\ndf50 = df[:p50]\nprint(df50['wt'].median())\n\n# len(df)/2\n# cond1 = df[:17]\n# cond2 = cond1['wt'].median()\n# print(cond2)\n\n3.44\n\n\n\n# 문제 8\n# 결측값이 있는 데이터의 수를 출력하시오\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    '판매수' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      날짜\n      제품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3.0\n      300\n    \n    \n      1\n      20220105\n      B\n      NaN\n      400\n    \n    \n      2\n      None\n      None\n      5.0\n      500\n    \n    \n      3\n      20230127\n      B\n      10.0\n      600\n    \n    \n      4\n      20220203\n      A\n      10.0\n      400\n    \n  \n\n\n\n\n\n# (풀이)\nm_value = df.isnull().sum()\nprint(m_value.sum())\n\n5\n\n\n\n# 문제 9 \n# '판매수' 컬럼의 결측값을 판매수의 중앙값으로 대체하고 판매수의 \n#  평균값을 정수로 출력하시오\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    '판매수' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\n\n\n# (풀이)\ndf[df['판매수'].isnull()]\nmedian = df['판매수'].median()\ndf['판매수'] = df['판매수'].fillna(median)\nmean = df['판매수'].mean()\nprint(int(mean))\n\n15\n\n\n\n# 문제 10\n# 판매수 컬럼에 결측치가 있는 행을 제거하고\n# 첫번째 행부터 순서대로 50%까지 데이터를 추출하여\n# 판매수 변수의 Q1(제1사분위수) 값을 정수로 출력하시오\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    '판매수' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\n\n\n# (풀이)\n# 결측치 삭제(행 기준)\ndf = df['판매수'].dropna()\n# 첫번째 행부터 순서대로 50%까지의 데이터 추출\nper50 = int(len(df)*0.5)\ndf = df[:per50]\n# 값구하기\nprint(round(df.quantile(.25)))\n\n# df[df['판매수'].isnull()]\n# df['판매수'] = df['판매수'].dropna()\n# int(len(df['판매수'])/2)\n# cond1 = df['판매수'].head(6)\n# Q1 = cond1.quantile(.25)\n# print(int(Q1))\n\n5"
  },
  {
    "objectID": "Regression_Analysis/실기_1유형.html#핵심문제-27개-1120",
    "href": "Regression_Analysis/실기_1유형.html#핵심문제-27개-1120",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "✅ 핵심문제 27개 (11~20)",
    "text": "✅ 핵심문제 27개 (11~20)\n\ndf = pd.read_csv('mtcars.txt')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      Hornet Sportabout\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\n# 문제 11\n# cyl가 4인 자동차와 6인 자동차 그룹의 mpg 평균값이 차이를 절대값으로 정수로 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\ncyl_4 = df[df['cyl']==4]\ncyl_4_mean = cyl_4['mpg'].mean()\n\ncyl_6 = df[df['cyl']==6]\ncyl_6_mean = cyl_6['mpg'].mean()\n\nprint(round(abs(cyl_4_mean - cyl_6_mean)))\n\n7\n\n\n\n# 문제 12\n# hp 변수에 대해 데이터표준화(Z-score)를 진행하고 이상치의 수를 구하시오\n# (단, 이상치는 Z값이 1.5를 초과하거나 -1.5미만인 값이다)\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\n# Z = (X-평균) / 표준편차\nstd = df['hp'].std()\nmean_hp = df['hp'].mean()\ndf['zscore'] = (df['hp']-mean_hp)/std\n\ncond1 = (df['zscore']>1.5)\ncond2 = (df['zscore']<-1.5)\nprint(len(df[cond1] + df[cond2]))\n\n2\n\n\n\n# 문제 13\n# mpg 컬럼을 최소최대 Scaling을 진행한 후 0.7보다 큰 값을 가지는 레코드 수를 구하라\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\nfrom sklearn.preprocessing import MinMaxScaler\nmscaler = MinMaxScaler()\ndf['mpg'] = mscaler.fit_transform(df[['mpg']])\nprint(len(df[df['mpg']>0.7]))\n\n# 공식 : (x-min) / (max-min)\n# min_mpg = df['mpg'].min()\n# max_mpg = df['mpg'].max()\n# df['mpg'] = (df['mpg'] - min_mpg)/(max_mpg - min_mpg)\n# cond1 = (df['mpg']>0.7)\n# print(len(df[cond1]))\n\n5\n\n\n\n# 문제 14\n# wt 컬럼에 대해 상자그림 기준으로 이상치의 개수를 구하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\n# 이상치 : Q1, Q3 로부터 1.5*IQR을 넘어가는 값\nQ1 = df['wt'].quantile(.25)\nQ3 = df['wt'].quantile(.75)\nIQR = Q3-Q1\n\nupper = Q3 + (1.5*IQR)\nlower = Q1 - (1.5*IQR)\n\ncond1 = (df['wt'] > upper)\ncond2 = (df['wt'] < lower)\n\nprint(len(df[cond1] + df[cond2]))\n\n3\n\n\n\n# 문제 15\n# 판매수 컬럼의 결측치를 최소값으로 대체하고\n# 결측치가 있을 때와 최소값으로 대체했을 때\n# 평균값의 차이를 절대값으로 정수로 출력하시오\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    '판매수' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf\n\n\n\n\n\n  \n    \n      \n      날짜\n      제품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3.0\n      300\n    \n    \n      1\n      20220105\n      B\n      NaN\n      400\n    \n    \n      2\n      None\n      None\n      5.0\n      500\n    \n    \n      3\n      20230127\n      B\n      10.0\n      600\n    \n    \n      4\n      20220203\n      A\n      10.0\n      400\n    \n    \n      5\n      20220205\n      None\n      10.0\n      500\n    \n    \n      6\n      20230210\n      A\n      15.0\n      500\n    \n    \n      7\n      20230223\n      B\n      15.0\n      600\n    \n    \n      8\n      20230312\n      A\n      20.0\n      600\n    \n    \n      9\n      20230422\n      B\n      NaN\n      700\n    \n    \n      10\n      20220505\n      A\n      30.0\n      600\n    \n    \n      11\n      20230511\n      A\n      40.0\n      600\n    \n  \n\n\n\n\n\n# (풀이)\n# 데이터 복사\ndf2 = df.copy()\n# 최소값으로 결측치 대체\nmin = df['판매수'].min()\ndf2['판매수'] = df2['판매수'].fillna(min)\n\n# 결측치가 있을때, 대체했을때 평균\nm_yes = df['판매수'].mean()\nm_no = df2['판매수'].mean()\n\nprint(round(abs(m_yes - m_no)))\n\n2\n\n\n\n# 문제 16\n# vs변수가 0이 아닌 차량 중에 mpg 값이 가장 큰 차량의 hp 값을 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\ncond1 = df[df['vs']!=0]\n# mpg 값이 가장 큰 차량(내림차순)\ncond1 = cond1.sort_values('mpg',ascending=False)\n# cond1\nprint(cond1['hp'].iloc[0])\n\n65\n\n\n\n# 문제 17\n# gear 변수값이 3, 4인 두 그룹의 hp 표준편차값의 차이를 절대값으로 \n# 소수점 첫째자리까지 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\n# 두개 그룹으로 필터링\ncond1 = df[df['gear']==3]\ncond2 = df[df['gear']==4]\n# std 구하기\nstd3 = cond1['hp'].std()\nstd4 = cond2['hp'].std()\n\nprint(round(abs(std3 - std4),1))\n\n21.8\n\n\n\n# 문제 18\n# gear 변수의 갑별로 그룹화하여 mpg 평균값을 산출하고\n# 평균값이 높은 그룹의 mpg 제3사분위수 값을 구하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\ndf['gear'].unique()\ngear3 = df[df['gear']==3]\ngear4 = df[df['gear']==4]\ngear5 = df[df['gear']==5]\n\nm_3 = gear3['mpg'].mean()\nm_4 = gear4['mpg'].mean()\nm_5 = gear5['mpg'].mean()\n\n(m_3, m_4, m_5) # m_4의 평균값이 가장 큼\nQ3 = gear4['mpg'].quantile(.75)\nprint(Q3)\n\n28.075\n\n\n\n# (풀이_v2)\n# gear, mpg 변수만 필터링\ndf = df.loc[:, ['gear', 'mpg']]\n\n# gear 변수로 그룹화하여 mpg 평균값 보기\n# print(df.groupby('gear').mean()) # gear 4그룹이 제일 높음\n\n# gear=4인 그룹의 mpg Q3 구하기\ngear4 = df[df['gear']==4]\nprint(gear4['mpg'].quantile(.75))\n\n28.075\n\n\n\n# 문제 19\n# hp 항목의 상위 7번째 값으로 상위 7개 값을 변환한 후,\n# hp가 150 이상인 데이터를 추출하여 hp의 평균값을 반올림하여 정수로 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\n# hp 열 기준으로 내림차순 정렬\ndf = df.sort_values('hp', ascending=False)\n\n# 인덱스 초기화 - 내림차순으로 정렬시 최초의 인덱스로 있기에\ndf = df.reset_index(drop=True) # drop=True 는 기존 index 삭제\n# print(df)\n\n# hp 상위 7번째 값 확인\ntop7 = df['hp'].loc[6] # top7 = 205\n\n# np.where 활용\nimport numpy as np\ndf['hp'] = np.where(df['hp'] >= 205, top7, df['hp'])\n# np.where(조건, 조건에 해당할 때 값, 그렇지 않을 때 값)\n\n# hp 150이상인 데이터\ncond1 = (df['hp']>=150)\ndf = df[cond1]\nprint(round(df['hp'].mean()))\n\n187\n\n\n\n# 문제 20\n# car 변수에 Merc 무구가 포함된 자동차의 mpg 평균값을 정수로 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\ndf2 = df[df['car'].str.contains('Merc')] # 문자열 추출 알아두기\nprint(round(df2['mpg'].mean()))\n\n# 시험환경에서 답구하는 방법(reset_index() 사용 후)\n# 시험에서는 car가 index로 되어 있음\n# df.reset_index(inplace=True)\n# df2 = df[df['index'].str.contains(\"Merc\")] \n# print(round(df2['mpg'].mean()))\n\n19"
  },
  {
    "objectID": "Regression_Analysis/실기_1유형.html#핵심문제-27개-2127",
    "href": "Regression_Analysis/실기_1유형.html#핵심문제-27개-2127",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "✅ 핵심문제 27개 (21~27)",
    "text": "✅ 핵심문제 27개 (21~27)\n\n# 문제 21\n# 22년 1분기 A제품의 매출액을 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    '판매수' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      날짜\n      제품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# # (풀이)\n# cond1 = df[df['날짜']<='20220431']\n# cond2 = cond1[cond1['제품']=='A']\n# cond2['매출액'] = (cond2['판매수']*cond2['개당수익'])\n# cond2\n\n# print(cond2['매출액'].sum())\n\n\n# (풀이_v2)\n# 데이터 타입 datetime으로 변경\ndf['날짜'] = pd.to_datetime(df['날짜'])\n\n# 년, 월, 일 변수(열) 추가하기\ndf['year'] = df['날짜'].dt.year\ndf['month'] = df['날짜'].dt.month\ndf['day'] = df['날짜'].dt.day\n\n# 매출액 변수 추가하기\ndf['매출액'] = df['판매수'] * df['개당수익']\n\n# 22년으로 필터링\ndf = df[df['year']==2022]\ndf.head()\n\n# 1,2,3월 매출액 계산\nm1 = df[df['month']==1]['매출액'].sum()\nm2 = df[df['month']==2]['매출액'].sum()\nm3 = df[df['month']==3]['매출액'].sum()\nprint(m1+m2+m3)\n\n11900\n\n\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['날짜'] = pd.to_datetime(df['날짜'])\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['year'] = df['날짜'].dt.year\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['month'] = df['날짜'].dt.month\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['day'] = df['날짜'].dt.day\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['매출액'] = df['판매수'] * df['개당수익']\n\n\n\n# 문제 22\n# 22년과 23년의 총 매출액 차이를 절대값으로 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    '판매수' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      날짜\n      제품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# (풀이)\n# 데이터 타입 datetime으로 변경\ndf['날짜'] = pd.to_datetime(df['날짜'])\n\n# 년 변수(열) 추가하기\ndf['year'] = df['날짜'].dt.year\n\n# 매출액 변수 추가하기\ndf['매출액'] = df['판매수'] * df['개당수익']\n\ncond22 = df[df['year']==2022]\ncond23 = df[df['year']==2023]\n\nprint(abs((cond22['매출액'].sum()) - (cond23['매출액'].sum())))\n\n48600\n\n\n\n# 문제 23\n# 23년 총 매출액이 큰 제품의 23년 판매수를 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    '판매수' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      날짜\n      제품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# 데이터 타입 datetime으로 변경\ndf['날짜'] = pd.to_datetime(df['날짜'])\n\n# 년 변수(열) 추가하기\ndf['year'] = df['날짜'].dt.year\n\n# 매출액 변수 추가하기\ndf['매출액'] = df['판매수'] * df['개당수익']\n\ndf = df[df['year']==2023]\n# df.head()\n\n# 23년 A 매출액과 B 매출액 별도로 구하기\n# A 매출액\ndf_a = df[df['제품']=='A']\na_sales = df_a['매출액'].sum()\n# print(a_sales) # 46000\n\n# B 매출액\ndf_b = df[df['제품']=='B']\nb_sales = df_b['매출액'].sum()\n# print(b_sales) # 32500\n\n# A 제품의 23년 판매수\na_sum = df_a['판매수'].sum()\nprint(a_sum)\n\n80\n\n\n\n# 문제 24\n# 매출액이 4천원 초과, 1만원 미만인 데이터 수를 출력하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    '판매수' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      날짜\n      제품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# (풀이)\ndf['매출액'] = df['판매수'] * df['개당수익']\ncond1 = df['매출액']>4000\ncond2 = df['매출액']<10000\n\ndf = df[cond1&cond2]\nprint(len(df))\n\n4\n\n\n\n# 문제 25\n# 23년 9월 24일 16:00~22:00 사이에 전체 제품의 판매수를 구하시오\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600]})\ntime = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf['time'] = time\ndf = df[['time', '물품', '판매수', '개당수익']]\ndf\n\n\n\n\n\n  \n    \n      \n      time\n      물품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      1\n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2\n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      3\n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      4\n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      5\n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      6\n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# (풀이)\n# 컬럼과 인덱스에 둘다 time 변수를 놓고 풀이\n# 데이터 타입 datetime으로 변경 (항상 df.info()로 데이터 타입 확인할 것)\ndf['time'] = pd.to_datetime(df['time'])\n\n# index 새로 지정\ndf = df.set_index('time', drop=False) # default는 True\n\n# 9월 24일 필터링 // df['변수'].between( , )\ndf_after = df[df['time'].between('2023-09-24', '2023-09-25')] # 25일은 미포함\n\n# 시간 필터링 16:00 ~ 22:00 (주의: 시간이 index에 위치해야 함)\ndf = df_after.between_time(start_time='16:00', end_time='22:00') # 포함 기준\n\n# 전체 판매수 구하기\nprint(df['판매수'].sum())\n\n25\n\n\n\n# (풀이2) // 위에 df 새로 불러와서 실행해보기\n# 데이터 타입 datetime으로 변경\ndf['time'] = pd.to_datetime(df['time'])\n\n# index 새로 지정\ndf = df.set_index('time')\n\n# loc 함수로 필터링\ndf = df.loc[(df.index >= '2023-09-24 16:00:00') & (df.index <= '2023-09-24 22:00:00')]\n\n# 전체 판매수 구하기\nprint(df['판매수'].sum())\n\n25\n\n\n\n# 문제 26\n# 9월 25일 00:10~12:00까지의 B물품의 매출액 총합을 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600]})\ndf['time'] = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf = df[['time', '물품', '판매수', '개당수익']]\ndf = df.set_index('time',drop=True)\ndf\n\n\n\n\n\n  \n    \n      \n      물품\n      판매수\n      개당수익\n    \n    \n      time\n      \n      \n      \n    \n  \n  \n    \n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# (풀이)\n# 매출액 변수 추가\ndf['매출액'] = df['판매수'] * df['개당수익']\n\n# loc 함수로 필터링\ndf = df.loc[(df.index >= '2023-09-25 00:10:00') & (df.index <= '2023-09-25 12:00:00')]\n\n# B 물품의 매출액 총합\ncond1 = df[df['물품']=='B']\nprint(cond1['매출액'].sum())\n\n26500\n\n\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\1645811142.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['매출액'] = df['판매수'] * df['개당수익']\n\n\n\n# 문제 27\n# 9월 24일 12:00~24:00까지의 A물품의 매출액 총합을 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600]})\ndf['time'] = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf = df[['time', '물품', '판매수', '개당수익']]\ndf = df.set_index('time',drop=True)\ndf\n\n\n\n\n\n  \n    \n      \n      물품\n      판매수\n      개당수익\n    \n    \n      time\n      \n      \n      \n    \n  \n  \n    \n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# (풀이)\n# 매출액 변수 추가\ndf['매출액'] = df['판매수'] * df['개당수익']\n\n# loc 함수로 필터링\ndf = df.loc[(df.index >= '2023-09-24 12:00:00') & (df.index < '2023-09-25 00:00:00')]\n\n# A 물품의 매출액 총합\ncond1 = df[df['물품']=='A']\nprint(cond1['매출액'].sum())\n\n10000"
  },
  {
    "objectID": "Regression_Analysis.html",
    "href": "Regression_Analysis.html",
    "title": "Regression Analysis",
    "section": "",
    "text": "예제를 통한 회귀분석\n\n\ngetwd() #\"C:/Users/Hyunsoo Kim/Documents/lecture/regression_analysis\"\n\n[1] \"C:/Users/Hyunsoo Kim/Desktop/senior_grade/blog/my-quarto-website\"\n\n\n\n\n\n\n\ndata_2.5<-read.table(\"All_Data/p031.txt\",header=TRUE,sep=\"\\t\")\n\ndim(data_2.5) #14 2\n\n[1] 14  2\n\nhead(data_2.5)\n\n  Minutes Units\n1      23     1\n2      29     2\n3      49     3\n4      64     4\n5      74     4\n6      87     5\n\nX<-data_2.5$Units\n\nY<-data_2.5$Minutes\n\n\n\n\n\ndf<-data.frame(\n\n  #1:length(X),\n\n  Y,\n\n  X,\n\n  Y-mean(Y),\n\n  X-mean(X),\n\n  (Y-mean(Y))^2,\n\n  (X-mean(X))^2,\n\n  (Y-mean(Y))*(X-mean(X))\n\n)\n\ndf\n\n     Y  X Y...mean.Y. X...mean.X. X.Y...mean.Y...2 X.X...mean.X...2\n1   23  1 -74.2142857          -5     5.507760e+03               25\n2   29  2 -68.2142857          -4     4.653189e+03               16\n3   49  3 -48.2142857          -3     2.324617e+03                9\n4   64  4 -33.2142857          -2     1.103189e+03                4\n5   74  4 -23.2142857          -2     5.389031e+02                4\n6   87  5 -10.2142857          -1     1.043316e+02                1\n7   96  6  -1.2142857           0     1.474490e+00                0\n8   97  6  -0.2142857           0     4.591837e-02                0\n9  109  7  11.7857143           1     1.389031e+02                1\n10 119  8  21.7857143           2     4.746173e+02                4\n11 149  9  51.7857143           3     2.681760e+03                9\n12 145  9  47.7857143           3     2.283474e+03                9\n13 154 10  56.7857143           4     3.224617e+03               16\n14 166 10  68.7857143           4     4.731474e+03               16\n   X.Y...mean.Y......X...mean.X..\n1                       371.07143\n2                       272.85714\n3                       144.64286\n4                        66.42857\n5                        46.42857\n6                        10.21429\n7                         0.00000\n8                         0.00000\n9                        11.78571\n10                       43.57143\n11                      155.35714\n12                      143.35714\n13                      227.14286\n14                      275.14286\n\n\n\n\n\n\nCOV_XY<-sum((Y-mean(Y))*(X-mean(X))) / (length(X)-1) #136\n\n### cov() 함수\n\ncov(X,Y) #136\n\n[1] 136\n\n### 상관계수(correalationship)\n\n### cor() 함수\n\ncor(X,Y) #0.9936987 \n\n[1] 0.9936987\n\n\n\n\n\n\n\ndata_2.5<-read.table(\"All_Data/p031.txt\",header=TRUE,sep=\"\\t\")\n\nx<-data_2.5$Units\n\ny<-data_2.5$Minutes\n\n\n\n\ncor_xy<- COV_XY / (sd(x)*sd(y))\n\ncor_xy\n\n[1] 0.9936987\n\n### cor() 함수\n\ncor(x,y)\n\n[1] 0.9936987\n\ncor(y,x)\n\n[1] 0.9936987\n\ndata_2.5\n\n   Minutes Units\n1       23     1\n2       29     2\n3       49     3\n4       64     4\n5       74     4\n6       87     5\n7       96     6\n8       97     6\n9      109     7\n10     119     8\n11     149     9\n12     145     9\n13     154    10\n14     166    10\n\ncor(data_2.5)\n\n          Minutes     Units\nMinutes 1.0000000 0.9936987\nUnits   0.9936987 1.0000000\n\n\n\n\n\n\nclass(X)\n\n[1] \"numeric\"\n\nclass(Y) #both numeric\n\n[1] \"numeric\"\n\nplot(X,Y, pch=19,xlab=\"Units\",ylab=\"Minutes\") \n\n\n\n\n\n\n\n\ndata_2.3<-read.table(\"All_Data/p029a.txt\",header=TRUE,sep=\"\\t\")\n\ndata_2.3\n\n    Y  X\n1   1 -7\n2  14 -6\n3  25 -5\n4  34 -4\n5  41 -3\n6  46 -2\n7  49 -1\n8  50  0\n9  49  1\n10 46  2\n11 41  3\n12 34  4\n13 25  5\n14 14  6\n15  1  7\n\nX<-data_2.3$X\n\nY<-data_2.3$Y\n\n\n\n\n\nplot(X,Y)\n\n\n\ncor(X,Y) # 0 완벽하게 2차함수의 형태도 0이 나옴(직선의 형태가 아닌것만)\n\n[1] 0\n\n\n\n\n\n\ndata_2.4<-read.table(\"All_Data/p029b.txt\",header=TRUE,sep=\"\\t\")\n\n\n\n\n\nplot(data_2.4$X1,data_2.4$Y1, pch=19); abline(3,0.5) #기울기 3 절편0.5인 선을 추가해라 \n\n\n\nplot(data_2.4$X2,data_2.4$Y2, pch=19); abline(3,0.5)\n\n\n\nplot(data_2.4$X3,data_2.4$Y3, pch=19); abline(3,0.5)\n\n\n\nplot(data_2.4$X4,data_2.4$Y4, pch=19); abline(3,0.5)\n\n\n\nm<-matrix(1:4,ncol=2,byrow=TRUE) #2행의 매트릭스 생성 \n\nm\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nlayout(m)\n\nplot(Y1~X1, data=data_2.4,pch=19); abline(3,0.5)\n\n#y~x  -> y=ax+b 이러한 형태를 가지는 모형식이라는 의미 \n\nplot(Y2~X2, data=data_2.4,pch=19); abline(3,0.5)\n\nplot(Y3~X3, data=data_2.4,pch=19); abline(3,0.5)\n\nplot(Y4~X4, data=data_2.4,pch=19); abline(3,0.5)\n\n\n\nlayout(1) #변환을 다시 하지 않으면 설정한 매트릭스의 비율로 그래프가 그려짐 해제 필요 \n\n# cor()\n\ncor(data_2.4$X1,data_2.4$Y1) #0.8164205\n\n[1] 0.8164205\n\ncor(data_2.4$X2,data_2.4$Y2) #0.8162365\n\n[1] 0.8162365\n\ncor(data_2.4$X3,data_2.4$Y3) #0.8162867\n\n[1] 0.8162867\n\ncor(data_2.4$X4,data_2.4$Y4) #0.8165214\n\n[1] 0.8165214\n\ncor(data_2.4) #이렇게 한번에 할 수 있으나 가독성 떨어짐 \n\n           Y1         X1         Y2         X2         Y3         X3         Y4\nY1  1.0000000  0.8164205  0.7500054  0.8164205  0.4687167  0.8164205 -0.4891162\nX1  0.8164205  1.0000000  0.8162365  1.0000000  0.8162867  1.0000000 -0.3140467\nY2  0.7500054  0.8162365  1.0000000  0.8162365  0.5879193  0.8162365 -0.4780949\nX2  0.8164205  1.0000000  0.8162365  1.0000000  0.8162867  1.0000000 -0.3140467\nY3  0.4687167  0.8162867  0.5879193  0.8162867  1.0000000  0.8162867 -0.1554718\nX3  0.8164205  1.0000000  0.8162365  1.0000000  0.8162867  1.0000000 -0.3140467\nY4 -0.4891162 -0.3140467 -0.4780949 -0.3140467 -0.1554718 -0.3140467  1.0000000\nX4 -0.5290927 -0.5000000 -0.7184365 -0.5000000 -0.3446610 -0.5000000  0.8165214\n           X4\nY1 -0.5290927\nX1 -0.5000000\nY2 -0.7184365\nX2 -0.5000000\nY3 -0.3446610\nX3 -0.5000000\nY4  0.8165214\nX4  1.0000000\n\n\n\n\n\n\n\n\n\ndata_2.5<-read.table(\"All_Data/p031.txt\",header=TRUE,sep=\"\\t\")\n\nx<-data_2.5$Units\n\ny<-data_2.5$Minutes\n\n\n\n\n\nsum((y-mean(y))*(x-mean(x))) #1768\n\n[1] 1768\n\nsum((x-mean(x))^2) #114\n\n[1] 114\n\nbeta1_hat<-sum((y-mean(y))*(x-mean(x))) / sum((x-mean(x))^2)\n\nbeta1_hat #15.50877\n\n[1] 15.50877\n\nbeta0_hat <- mean(y) - (beta1_hat*mean(x))\n\nbeta0_hat #4.161654\n\n[1] 4.161654\n\n### 최소제곱회귀 방정식\n\n# Minutes = 4.161654 + 15.50877 * Units\n\nplot(beta0_hat+beta1_hat*x,pch=19);\n\n\n\n# 4개의 고장 난 부품을 수리하는데 걸리는 에측시간\n\n4.161654 + 15.50877 * 4 #66.19673\n\n[1] 66.19673\n\nunits<-4\n\nbeta0_hat + beta1_hat*units\n\n[1] 66.19674\n\n### 적합값(Fitted value)\n\ny_hat<-beta0_hat + beta1_hat*(x)\n\n### 최소 제곱 잔차(residual)\n\ne<-y-y_hat\n\ne #합이 0이라는 특징이 존재\n\n [1]  3.3295739 -6.1791980 -1.6879699 -2.1967419  7.8032581  5.2944862\n [7] -1.2142857 -0.2142857 -3.7230576 -9.2318296  5.2593985  1.2593985\n[13] -5.2493734  6.7506266\n\nsum(e) #1.278977e-13 0에 근사한 추지가 나옴\n\n[1] 1.278977e-13\n\n\n\n\n\n\ndf_2.7<-data.frame(\n\n  x=x,\n\n  y=y,\n\n  y_hat,\n\n  e\n\n)\n\ndf_2.7\n\n    x   y     y_hat          e\n1   1  23  19.67043  3.3295739\n2   2  29  35.17920 -6.1791980\n3   3  49  50.68797 -1.6879699\n4   4  64  66.19674 -2.1967419\n5   4  74  66.19674  7.8032581\n6   5  87  81.70551  5.2944862\n7   6  96  97.21429 -1.2142857\n8   6  97  97.21429 -0.2142857\n9   7 109 112.72306 -3.7230576\n10  8 119 128.23183 -9.2318296\n11  9 149 143.74060  5.2593985\n12  9 145 143.74060  1.2593985\n13 10 154 159.24937 -5.2493734\n14 10 166 159.24937  6.7506266\n\n### lm() 함수 (linear model)\n\n# Minutes = beta0 + beta1 * Units + epsilon\n\n# 모형식 : y~x\n\nlm(y~x)\n\n\nCall:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)            x  \n      4.162       15.509  \n\nres_lm<-lm(Minutes~Units,data=data_2.5)\n\nres_lm\n\n\nCall:\nlm(formula = Minutes ~ Units, data = data_2.5)\n\nCoefficients:\n(Intercept)        Units  \n      4.162       15.509  \n\n# 리스트의 이름 \n\nnames(res_lm)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n# 회귀계수\n\nres_lm$coefficients\n\n(Intercept)       Units \n   4.161654   15.508772 \n\ncoef(res_lm)\n\n(Intercept)       Units \n   4.161654   15.508772 \n\n# 적합값\n\nres_lm$fitted.values\n\n        1         2         3         4         5         6         7         8 \n 19.67043  35.17920  50.68797  66.19674  66.19674  81.70551  97.21429  97.21429 \n        9        10        11        12        13        14 \n112.72306 128.23183 143.74060 143.74060 159.24937 159.24937 \n\nfitted(res_lm)\n\n        1         2         3         4         5         6         7         8 \n 19.67043  35.17920  50.68797  66.19674  66.19674  81.70551  97.21429  97.21429 \n        9        10        11        12        13        14 \n112.72306 128.23183 143.74060 143.74060 159.24937 159.24937 \n\n# 최소제곱잔차\n\nres_lm$residuals\n\n         1          2          3          4          5          6          7 \n 3.3295739 -6.1791980 -1.6879699 -2.1967419  7.8032581  5.2944862 -1.2142857 \n         8          9         10         11         12         13         14 \n-0.2142857 -3.7230576 -9.2318296  5.2593985  1.2593985 -5.2493734  6.7506266 \n\nresid(res_lm)\n\n         1          2          3          4          5          6          7 \n 3.3295739 -6.1791980 -1.6879699 -2.1967419  7.8032581  5.2944862 -1.2142857 \n         8          9         10         11         12         13         14 \n-0.2142857 -3.7230576 -9.2318296  5.2593985  1.2593985 -5.2493734  6.7506266 \n\nresiduals(res_lm)\n\n         1          2          3          4          5          6          7 \n 3.3295739 -6.1791980 -1.6879699 -2.1967419  7.8032581  5.2944862 -1.2142857 \n         8          9         10         11         12         13         14 \n-0.2142857 -3.7230576 -9.2318296  5.2593985  1.2593985 -5.2493734  6.7506266 \n\n\n\n\n\n\nplot(beta0_hat+beta1_hat*x,pch=19);\n\n#abline(beta0_hat,beta1_hat)\n\nabline(res_lm)\n\n\n\n\n\n\n\n\n\ndata_2.5<-read.table(\"All_Data/p031.txt\",header=TRUE,sep=\"\\t\")\n\nres_lm <- lm(Minutes ~ Units, data = data_2.5)\n\nres_lm\n\n\nCall:\nlm(formula = Minutes ~ Units, data = data_2.5)\n\nCoefficients:\n(Intercept)        Units  \n      4.162       15.509  \n\nres_lm_summ<-summary(res_lm)\n\nres_lm_summ #Pr(>|t|) - p-value\n\n\nCall:\nlm(formula = Minutes ~ Units, data = data_2.5)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.2318 -3.3415 -0.7143  4.7769  7.8033 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    4.162      3.355    1.24    0.239    \nUnits         15.509      0.505   30.71 8.92e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.392 on 12 degrees of freedom\nMultiple R-squared:  0.9874,    Adjusted R-squared:  0.9864 \nF-statistic: 943.2 on 1 and 12 DF,  p-value: 8.916e-13\n\n# unit은 시간에 영향을준다 약15.5분 만큼씩 \n\n# coefficient에서 p-value에 대해서 알 수 있음 \n\n# beta_0는 0이라고 보면되느냐? p-value가 0.05보다 크기에 \n\n\n\n\n\n\n\nconfint(res_lm) # beta_0,1의 95% 신뢰구간을 뽑아줌 \n\n                2.5 %   97.5 %\n(Intercept) -3.148482 11.47179\nUnits       14.408512 16.60903\n\n?confint #level = 1-alpha\n\nstarting httpd help server ... done\n\nconfint(res_lm, level=0.90) # 90%의 신뢰구간\n\n                 5 %     95 %\n(Intercept) -1.81810 10.14141\nUnits       14.60875 16.40879\n\n\n\n\n\n\n# 4개의 고장난 부품을 수리하는 데에 걸리는 시간 예측\n\nx<-4\n\n4.161654 + 15.508772 *4\n\n[1] 66.19674\n\nres_lm$coefficients[1]+(res_lm$coefficients[2]*x)\n\n(Intercept) \n   66.19674 \n\n### predict()\n\ndf<-data.frame(Units=4) \n\npredict(res_lm,newdata=df) # res_lm을 만들때 사용한 데이터형식으로 만들어주어야함\n\n       1 \n66.19674 \n\nres_lm_pred<-predict(res_lm,newdata=df,se.fit=TRUE)\n\n### 예측값\n\nres_lm_pred$fit\n\n       1 \n66.19674 \n\n### 표준오차\n\nres_lm_pred$se.fit # 평균반응에 대한 표준오차 \n\n[1] 1.759688\n\n### 예측한계\n\ndf<-data.frame(Units=4) #예제서는 4대기준\n\ndf<-data.frame(Units=1:10) #다른것도 보고 싶은경우 \n\nres_lm_pred_int_p<-predict(res_lm,newdata=df,interval=\"prediction\")\n\n### 신뢰한계\n\ndf<-data.frame(Units=1:10) #다른것도 보고 싶은경우 \n\nres_lm_pred_int_c<-predict(res_lm,newdata=df,interval=\"confidence\") #둘의 차이를 보면 예측한계의 범위가 더큼 \n\n### 예측한계 & 신뢰한계\n\n# 신뢰한계는 평균에서 멀어지만 오차의범위가 커지고 평균에 다가갈수록 오차가 줄어듬\n\nplot(Minutes~Units,data=data_2.5,pch=19)\n\nabline(res_lm,col=\"red\",lwd=2)\n\nlines(1:10,res_lm_pred_int_p[,\"lwr\"],col=\"darkgreen\")\n\nlines(1:10,res_lm_pred_int_p[,\"upr\"],col=\"darkgreen\")\n\nlines(1:10,res_lm_pred_int_c[,\"lwr\"],col=\"blue\")\n\nlines(1:10,res_lm_pred_int_c[,\"upr\"],col=\"blue\")\n\n\n\n\n\n\n\n\nres_lm_summ<-summary(res_lm)\n\nres_lm_summ #Multiple R-squared:0.9874 -> 반응변수의 전체변이중 98.94%가 예측변수에 의해 설명된다\n\n\nCall:\nlm(formula = Minutes ~ Units, data = data_2.5)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.2318 -3.3415 -0.7143  4.7769  7.8033 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    4.162      3.355    1.24    0.239    \nUnits         15.509      0.505   30.71 8.92e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.392 on 12 degrees of freedom\nMultiple R-squared:  0.9874,    Adjusted R-squared:  0.9864 \nF-statistic: 943.2 on 1 and 12 DF,  p-value: 8.916e-13\n\n# 만약 R-squared가 1이면 완벽한 선형의 관계 100%라는 것을 의미한다.\n\n# R-squared는 변수가 들어갈수록 커지기에 adjust R-squared를 사용 추후 설명 \n\n\n\n\n\n# Minutes = beta1 + Units + epsilon\n\nres_lm_no<-lm(Minutes~Units-1,data=data_2.5)\n\nres_lm_no\n\n\nCall:\nlm(formula = Minutes ~ Units - 1, data = data_2.5)\n\nCoefficients:\nUnits  \n16.07  \n\nsummary(res_lm_no)\n\n\nCall:\nlm(formula = Minutes ~ Units - 1, data = data_2.5)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5955 -2.4733  0.4417  5.0243  9.7023 \n\nCoefficients:\n      Estimate Std. Error t value Pr(>|t|)    \nUnits  16.0744     0.2213   72.62   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.502 on 13 degrees of freedom\nMultiple R-squared:  0.9975,    Adjusted R-squared:  0.9974 \nF-statistic:  5274 on 1 and 13 DF,  p-value: < 2.2e-16\n\ncoef(summary(res_lm_no)) #rsquared=0.9975\n\n      Estimate Std. Error  t value     Pr(>|t|)\nUnits 16.07443  0.2213341 72.62519 2.380325e-18\n\n\n\n\n\n\ny<-rnorm(30)\n\nt.test(y,mu=0)\n\n\n    One Sample t-test\n\ndata:  y\nt = 0.37112, df = 29, p-value = 0.7132\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.2786540  0.4022005\nsample estimates:\n mean of x \n0.06177325 \n\nsummary(lm(y~1))\n\n\nCall:\nlm(formula = y ~ 1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.02445 -0.52714 -0.08273  0.63884  1.65052 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  0.06177    0.16645   0.371    0.713\n\nResidual standard error: 0.9117 on 29 degrees of freedom\n\n\n\n\n\n\n\n\n\ndata_3.3<-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\ndim(data_3.3)\n\n[1] 30  7\n\nclass(data_3.3)\n\n[1] \"data.frame\"\n\nsapply(data_3.3,class) #all numeric\n\n        Y        X1        X2        X3        X4        X5        X6 \n\"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \n\nsummary(data_3.3) #모든변수가 numeric이면 분위수도 보여준다 \n\n       Y               X1             X2              X3              X4       \n Min.   :40.00   Min.   :37.0   Min.   :30.00   Min.   :34.00   Min.   :43.00  \n 1st Qu.:58.75   1st Qu.:58.5   1st Qu.:45.00   1st Qu.:47.00   1st Qu.:58.25  \n Median :65.50   Median :65.0   Median :51.50   Median :56.50   Median :63.50  \n Mean   :64.63   Mean   :66.6   Mean   :53.13   Mean   :56.37   Mean   :64.63  \n 3rd Qu.:71.75   3rd Qu.:77.0   3rd Qu.:62.50   3rd Qu.:66.75   3rd Qu.:71.00  \n Max.   :85.00   Max.   :90.0   Max.   :83.00   Max.   :75.00   Max.   :88.00  \n       X5              X6       \n Min.   :49.00   Min.   :25.00  \n 1st Qu.:69.25   1st Qu.:35.00  \n Median :77.50   Median :41.00  \n Mean   :74.77   Mean   :42.93  \n 3rd Qu.:80.00   3rd Qu.:47.75  \n Max.   :92.00   Max.   :72.00  \n\n### 산점도 행렬\n\nplot(data_3.3)\n\n\n\n\n\n\n\n\nlm(Y~X1+X2+X3+X4+X5+X6,data=data_3.3)\n\n\nCall:\nlm(formula = Y ~ X1 + X2 + X3 + X4 + X5 + X6, data = data_3.3)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n   10.78708      0.61319     -0.07305      0.32033      0.08173      0.03838  \n         X6  \n   -0.21706  \n\nlm(Y~.,data=data_3.3) # X1+X2+X3+X4+X5+X6쓰는 것이 아니라 .을 써서 모든 변수를 다써줌 \n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n   10.78708      0.61319     -0.07305      0.32033      0.08173      0.03838  \n         X6  \n   -0.21706  \n\n\n\n\n\n\ndata_3.3<-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\nlm(Y~X1+X2,data=data_3.3)\n\n\nCall:\nlm(formula = Y ~ X1 + X2, data = data_3.3)\n\nCoefficients:\n(Intercept)           X1           X2  \n   15.32762      0.78034     -0.05016  \n\n# (Intercept)         X1           X2  \n\n#  15.32762      0.78034     -0.05016\n\n# 1) Y에서 X1 효과 제거\n\nm1<-lm(Y~X1,data=data_3.3) # y prime\n\nm1$residuals # x1이 설명하지 못한값 / x1의 효과를 제거한 값\n\n           1            2            3            4            5            6 \n -9.86142016   0.32865220   3.80099328  -0.91673799   7.76411473 -12.87985944 \n           7            8            9           10           11           12 \n -6.93517726   0.02794419  -4.25432454   6.59248165   9.62936020   7.34709147 \n          13           14           15           16           17           18 \n  7.83787183  -9.00893436   4.51872455  -1.29120309  -4.51815400   5.34709147 \n          19           20           21           22           23           24 \n -2.19900672  -8.14368889   5.43928784   3.59248165 -11.18056744  -2.29688270 \n          25           26           27           28           29           30 \n  7.87475038  -6.48127545   7.02794419  -9.38907907   6.48184600   5.74567546 \n\n# 2) X2에서 X1 효과 제거\n\nm2<-lm(X2~X1,data=data_3.3) # x2 prime \n\nm2$residuals \n\n          1           2           3           4           5           6 \n-15.1300345  -0.7994502  13.1223579  -6.2864182  -2.9818979   1.8178376 \n          7           8           9          10          11          12 \n-11.3385461  -7.4428019  10.9659742  -5.2603543   6.8439016  -2.7473223 \n         13          14          15          16          17          18 \n  6.2266138  21.4529422  -4.4688659 -15.1382816   1.4268783  15.2526777 \n         19          20          21          22          23          24 \n -8.8776421  19.2787417  -6.4866827   1.7396457  -0.8255141   4.0524132 \n         25          26          27          28          29          30 \n -4.6691304   7.5311341   0.5571981  -4.2082264   8.4268783 -22.0340258 \n\n# 3) X1의 효과가 제거된 Y와 X2의 적합 - 원점을 지나는 회귀선\n\nlm(m1$residuals~m2$residuals-1) # 원점을 지나면 -1를 하고 진행 // -3.25e-17\n\n\nCall:\nlm(formula = m1$residuals ~ m2$residuals - 1)\n\nCoefficients:\nm2$residuals  \n    -0.05016  \n\n# 다른 효과 없이(다른값이 고정) Y에 영향을 주는 순수한 X2의 값\n\n# m2$residuals  : -0.05016  ==  X2 : -0.05016  \n\n### 단위길이 척도화 - 잘사용하지않음\n\nfn_scaling_len<-function(x){\n\n  x0<-x-mean(x)\n\n  x0/sqrt(sum(x0^2))\n\n}\n\ndata_3.3_len<-sapply(data_3.3, fn_scaling_len)\n\ndata_3.3_len<-data.frame(data_3.3_len)\n\nsummary(data_3.3_len)\n\n       Y                  X1                 X2                 X3           \n Min.   :-0.37579   Min.   :-0.41282   Min.   :-0.35109   Min.   :-0.353871  \n 1st Qu.:-0.08975   1st Qu.:-0.11297   1st Qu.:-0.12344   1st Qu.:-0.148193  \n Median : 0.01322   Median :-0.02231   Median :-0.02479   Median : 0.002109  \n Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.000000  \n 3rd Qu.: 0.10857   3rd Qu.: 0.14504   3rd Qu.: 0.14216   3rd Qu.: 0.164278  \n Max.   : 0.31070   Max.   : 0.32635   Max.   : 0.45328   Max.   : 0.294804  \n       X4                 X5                 X6          \n Min.   :-0.38637   Min.   :-0.48356   Min.   :-0.32367  \n 1st Qu.:-0.11401   1st Qu.:-0.10353   1st Qu.:-0.14318  \n Median :-0.02024   Median : 0.05130   Median :-0.03489  \n Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.00000  \n 3rd Qu.: 0.11371   3rd Qu.: 0.09821   3rd Qu.: 0.08693  \n Max.   : 0.41733   Max.   : 0.32341   Max.   : 0.52461  \n\nlm(Y~.,data=data_3.3_len)\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3_len)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n -1.259e-16    6.707e-01   -7.343e-02    3.089e-01    6.981e-02    3.120e-02  \n         X6  \n -1.835e-01  \n\n### 표준화\n\n# scale()\n\ndata_3.3_std<-scale(data_3.3)\n\n#summary(data_3.3_std)\n\n#sapply(data_3.3_std, sd, na.rm=T)\n\n#class(data_3.3_std) #\"matrix\"\n\ndata_3.3_std<-data.frame(data_3.3_std)\n\n#class(data_3.3_std) #\"data.frame\"\n\n#sapply(data_3.3_std, sd, na.rm=T)\n\nlm(Y~.,data=data_3.3_std) # beta게수 구하기 \n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3_std)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n -7.717e-16    6.707e-01   -7.343e-02    3.089e-01    6.981e-02    3.120e-02  \n         X6  \n -1.835e-01  \n\n\n\n\n\n\nres_lm<-lm(Y~.,data=data_3.3)\n\nres_lm\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n   10.78708      0.61319     -0.07305      0.32033      0.08173      0.03838  \n         X6  \n   -0.21706  \n\nsummary(res_lm)\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.9418  -4.3555   0.3158   5.5425  11.5990 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 10.78708   11.58926   0.931 0.361634    \nX1           0.61319    0.16098   3.809 0.000903 ***\nX2          -0.07305    0.13572  -0.538 0.595594    \nX3           0.32033    0.16852   1.901 0.069925 .  \nX4           0.08173    0.22148   0.369 0.715480    \nX5           0.03838    0.14700   0.261 0.796334    \nX6          -0.21706    0.17821  -1.218 0.235577    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.068 on 23 degrees of freedom\nMultiple R-squared:  0.7326,    Adjusted R-squared:  0.6628 \nF-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05\n\nm1<-summary(lm(Y~X1+X2+X3+X4+X5+X6,data=data_3.3)) #Adjusted R-squared:  0.6628 \n\nm2<-summary(lm(Y~X1+X2+X3+X4+X5,data=data_3.3)) #Adjusted R-squared:  0.6561 \n\n# X6가 들어가는 것이 더 좋은 모델 \n\nm1$adj.r.squared\n\n[1] 0.662846\n\nm2$adj.r.squared #summary에서 보다 더 정확하게 수치가 나옴 \n\n[1] 0.6560539\n\n\n\n\n\n\nres_lm_summ<-summary(res_lm)\n\nres_lm_summ #p-value의 존재는 무언가를 검정했다라는 반증\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.9418  -4.3555   0.3158   5.5425  11.5990 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 10.78708   11.58926   0.931 0.361634    \nX1           0.61319    0.16098   3.809 0.000903 ***\nX2          -0.07305    0.13572  -0.538 0.595594    \nX3           0.32033    0.16852   1.901 0.069925 .  \nX4           0.08173    0.22148   0.369 0.715480    \nX5           0.03838    0.14700   0.261 0.796334    \nX6          -0.21706    0.17821  -1.218 0.235577    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.068 on 23 degrees of freedom\nMultiple R-squared:  0.7326,    Adjusted R-squared:  0.6628 \nF-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05\n\n# p-value<0.05 H_1 귀무가설 채택 \n\n# p-value>0.05 H_0 영가설 채택 // X1을 제외하고는 영가설 유의한 의미가 없음(Y에영향주는)\n\n# 모두다 0이라는 가설을 가지고 분모 분자의 오차가 카이제곱을 따르고 거기서 나온 통계량\n\n# F-분포 자유도는 분자 분모 두개를 가짐 //모아서 계산을 하기에 각각 계산하는것과 결과다름 \n\n# 영가설-모든 회귀계수가 0이다.\n\n# 대립가설-적어도 하나는 0이 아니다. p-value: 1.24e-05 <0.05 대립가설 채택 \n\n# p-value가 0.05보다 작으면 대립가설 채택!!!!!! 기억해 \n\n# 회귀계수에 대한 신뢰구간 - 95% 신뢰한계\n\nconfint(res_lm) #-13.18712881 ~ 34.7612816\n\n                   2.5 %     97.5 %\n(Intercept) -13.18712881 34.7612816\nX1            0.28016866  0.9462066\nX2           -0.35381806  0.2077178\nX3           -0.02827872  0.6689430\nX4           -0.37642935  0.5398936\nX5           -0.26570179  0.3424647\nX6           -0.58571106  0.1515977\n\n#X1  0.28016866  0.9462066  사이에 0이 들어가있으면 영향을 준다라느걸 의미\n\n#X2 -0.35381806  0.2077178  p-value없이도 알 수 있음 \n\n#X5가 가장 영향이 적음 p-value가 가장 크기에(영향 효과의 크기를 비교할때)\n\n#p-value가 작을 수록 영향을 많이 준다 beta값을 보는 것이 아닌 p-value를 보는 것 중요\n\n#가장 의미있는 변수?->p-value가장 작은거 // 대립가설채택 Y에 영향을 가장\n\n\n\n\n\n\n\n\n# H_0: beta_1:beta_6=0\n\nmodel_reduce<-lm(Y~1,data=data_3.3)\n\nmodel_full<-lm(Y~.,data=data_3.3)\n\nanova(model_reduce,model_full)\n\nAnalysis of Variance Table\n\nModel 1: Y ~ 1\nModel 2: Y ~ X1 + X2 + X3 + X4 + X5 + X6\n  Res.Df  RSS Df Sum of Sq      F   Pr(>F)    \n1     29 4297                                 \n2     23 1149  6      3148 10.502 1.24e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#대립가설 = 완전모형이 적절하다 / 1.24e-05 *** < 0.05 \n\n#의미 있는 예측 변수가 한개 이상 존재한다 \n\nsummary(model_full) #summary에서 beta_1~beta_6까지 모두가 0이라는 가설로 진행을 이미함\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.9418  -4.3555   0.3158   5.5425  11.5990 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 10.78708   11.58926   0.931 0.361634    \nX1           0.61319    0.16098   3.809 0.000903 ***\nX2          -0.07305    0.13572  -0.538 0.595594    \nX3           0.32033    0.16852   1.901 0.069925 .  \nX4           0.08173    0.22148   0.369 0.715480    \nX5           0.03838    0.14700   0.261 0.796334    \nX6          -0.21706    0.17821  -1.218 0.235577    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.068 on 23 degrees of freedom\nMultiple R-squared:  0.7326,    Adjusted R-squared:  0.6628 \nF-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05\n\n#F-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05\n\n#예상// 가장의미있는변수? -> X1 이유?-> p-value 0.000903 로 가장 작기에 영향많이 줄것으로 예측 \n\n\n\n\n\nmodel_reduce<-lm(Y~X1+X3,data=data_3.3)\n\nmodel_full<-lm(Y~.,data=data_3.3)\n\nanova(model_reduce,model_full) #0.7158 > 0.05\n\nAnalysis of Variance Table\n\nModel 1: Y ~ X1 + X3\nModel 2: Y ~ X1 + X2 + X3 + X4 + X5 + X6\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1     27 1254.7                           \n2     23 1149.0  4    105.65 0.5287 0.7158\n\n#영가설은 H_0: b_2=b_4=b_5=b_6=0 이라는 사실을 알 수 있다 \n\n#b_1&b_3는 반응변수에 유의한 반응을 준다라는 것도 연계하여 알 수 있다 \n\n\n\n\n\n#해당 조건이 주어지고 만족할 때 beta_1=beta_3은 맞는가?\n\nmodel_reduce<-lm(Y~I(X1-X3),data=data_3.3) #I를 씌우면 새로운 변수를 만든것과 동일\n\n# X1-X3를 한 그자체를 분석하라는 의미//본래는 X1-X3 해서 새로운 변수를 만들어서 해야함 \n\nmodel_full<-lm(Y~X1+X3,data=data_3.3)\n\nanova(model_reduce,model_full) \n\nAnalysis of Variance Table\n\nModel 1: Y ~ I(X1 - X3)\nModel 2: Y ~ X1 + X3\n  Res.Df    RSS Df Sum of Sq     F    Pr(>F)    \n1     28 3846.7                                 \n2     27 1254.6  1      2592 55.78 4.925e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#install.packages(\"car\")\n\nlibrary(car)\n\nWarning: package 'car' was built under R version 4.3.2\n\n\nLoading required package: carData\n\n\nWarning: package 'carData' was built under R version 4.3.2\n\nmodel_full<-lm(Y~X1+X3,data=data_3.3)\n\ncar::linearHypothesis(model_full,c(\"X1=X3\"))\n\nLinear hypothesis test\n\nHypothesis:\nX1 - X3 = 0\n\nModel 1: restricted model\nModel 2: Y ~ X1 + X3\n\n  Res.Df    RSS Df Sum of Sq      F  Pr(>F)  \n1     28 1424.6                              \n2     27 1254.7  1    169.95 3.6572 0.06649 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n# H_0: beta_1+beta_3=1 | beta_2=beta_3:beta_6=0\n\nmodel_full<-lm(Y~X1+X3,data=data_3.3)\n\ncar::linearHypothesis(model_full,c(\"X1 + X3 = 1\"))\n\nLinear hypothesis test\n\nHypothesis:\nX1  + X3 = 1\n\nModel 1: restricted model\nModel 2: Y ~ X1 + X3\n\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1     28 1329.5                           \n2     27 1254.7  1    74.898 1.6118 0.2151\n\n# x1의 효과가 증가하면 x3의 효과는 감소한다 상대적인 관계 (반대로도 가능)\n\n\n\n\n\nmodel_full<-lm(Y~.,data_3.3)\n\n# 예측값 - 적합값\n\nmodel_full$fitted.values\n\n       1        2        3        4        5        6        7        8 \n51.11030 61.35277 69.93944 61.22684 74.45380 53.94185 67.14841 70.09701 \n       9       10       11       12       13       14       15       16 \n79.53099 59.19846 57.92572 55.40103 59.58168 70.21401 76.54933 84.54785 \n      17       18       19       20       21       22       23       24 \n76.15013 61.39736 68.01656 55.62014 42.60324 63.81902 63.66400 44.62475 \n      25       26       27       28       29       30 \n57.31710 67.84347 75.14036 56.04535 77.66053 76.87850 \n\n# 예측한계(Prediction Limits)\n\npredict(model_full,newdata = data_3.3,interval = \"prediction\")\n\n        fit      lwr       upr\n1  51.11030 34.16999  68.05060\n2  61.35277 46.34536  76.36018\n3  69.93944 53.94267  85.93622\n4  61.22684 45.44586  77.00783\n5  74.45380 59.17630  89.73129\n6  53.94185 37.21813  70.66557\n7  67.14841 51.64493  82.65189\n8  70.09701 54.54384  85.65017\n9  79.53099 62.71383  96.34814\n10 59.19846 44.03506  74.36185\n11 57.92572 42.00674  73.84470\n12 55.40103 39.79333  71.00873\n13 59.58168 43.39853  75.76483\n14 70.21401 52.06636  88.36167\n15 76.54933 60.79444  92.30422\n16 84.54785 66.41374 102.68197\n17 76.15013 59.99991  92.30036\n18 61.39736 43.23384  79.56088\n19 68.01656 52.44673  83.58639\n20 55.62014 39.63744  71.60284\n21 42.60324 26.35046  58.85603\n22 63.81902 48.40145  79.23659\n23 63.66400 48.56222  78.76578\n24 44.62475 27.25435  61.99514\n25 57.31710 41.29380  73.34041\n26 67.84347 49.98605  85.70089\n27 75.14036 59.31975  90.96097\n28 56.04535 40.18723  71.90348\n29 77.66053 61.97564  93.34541\n30 76.87850 60.27441  93.48258\n\n# 신뢰한계(Confidence limits)\n\npredict(model_full,newdata = data_3.3,interval = \"confidence\")\n\n        fit      lwr      upr\n1  51.11030 42.55502 59.66557\n2  61.35277 57.97029 64.73524\n3  69.93944 63.44979 76.42909\n4  61.22684 55.28897 67.16471\n5  74.45380 70.02428 78.88332\n6  53.94185 45.82386 62.05984\n7  67.14841 61.99316 72.30367\n8  70.09701 64.79421 75.39980\n9  79.53099 71.22222 87.83975\n10 59.19846 55.18008 63.21683\n11 57.92572 51.63028 64.22116\n12 55.40103 49.94035 60.86171\n13 59.58168 52.64531 66.51805\n14 70.21401 59.46431 80.96372\n15 76.54933 70.68118 82.41748\n16 84.54785 73.82102 95.27468\n17 76.15013 69.29093 83.00933\n18 61.39736 50.62090 72.17383\n19 68.01656 62.66507 73.36805\n20 55.62014 49.16527 62.07502\n21 42.60324 35.50593 49.70055\n22 63.81902 58.92819 68.70985\n23 63.66400 59.88479 67.44321\n24 44.62475 35.24662 54.00288\n25 57.31710 50.76233 63.87187\n26 67.84347 57.59134 78.09561\n27 75.14036 69.09798 81.18275\n28 56.04535 49.90540 62.18531\n29 77.66053 71.98300 83.33806\n30 76.87850 69.00992 84.74707\n\n\n\n\n\n\n\ndata_3.3<-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\nY<-data_3.3$Y\n\nX<-data_3.3[,-1]\n\nX<-cbind(1,X)\n\nX<-as.matrix(X)\n\n#beta_hat<-solve(t(X) %*% X) %*% t(X) %*% Y # %*%행렬 계산 \n\nP<-solve(t(X) %*% X) %*% t(X)\n\nbeta_hat<- P %*% Y\n\nlm(Y~.,data_3.3)\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n   10.78708      0.61319     -0.07305      0.32033      0.08173      0.03838  \n         X6  \n   -0.21706  \n\n\n\n\n\n\n\n\n# 표준화 잔차\n\ndata_3.3<-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\nres_lm<-lm(Y~.,data=data_3.3)\n\nclass(res_lm)\n\n[1] \"lm\"\n\nmode(res_lm)\n\n[1] \"list\"\n\nnames(res_lm)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\nres_lm$fitted.values\n\n       1        2        3        4        5        6        7        8 \n51.11030 61.35277 69.93944 61.22684 74.45380 53.94185 67.14841 70.09701 \n       9       10       11       12       13       14       15       16 \n79.53099 59.19846 57.92572 55.40103 59.58168 70.21401 76.54933 84.54785 \n      17       18       19       20       21       22       23       24 \n76.15013 61.39736 68.01656 55.62014 42.60324 63.81902 63.66400 44.62475 \n      25       26       27       28       29       30 \n57.31710 67.84347 75.14036 56.04535 77.66053 76.87850 \n\nstr(res_lm)\n\nList of 12\n $ coefficients : Named num [1:7] 10.7871 0.6132 -0.0731 0.3203 0.0817 ...\n  ..- attr(*, \"names\")= chr [1:7] \"(Intercept)\" \"X1\" \"X2\" \"X3\" ...\n $ residuals    : Named num [1:30] -8.11 1.647 1.061 -0.227 6.546 ...\n  ..- attr(*, \"names\")= chr [1:30] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:30] -354.011 54.107 2.742 11.715 -0.971 ...\n  ..- attr(*, \"names\")= chr [1:30] \"(Intercept)\" \"X1\" \"X2\" \"X3\" ...\n $ rank         : int 7\n $ fitted.values: Named num [1:30] 51.1 61.4 69.9 61.2 74.5 ...\n  ..- attr(*, \"names\")= chr [1:30] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:7] 0 1 2 3 4 5 6\n $ qr           :List of 5\n  ..$ qr   : num [1:30, 1:7] -5.477 0.183 0.183 0.183 0.183 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:30] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ : chr [1:7] \"(Intercept)\" \"X1\" \"X2\" \"X3\" ...\n  .. ..- attr(*, \"assign\")= int [1:7] 0 1 2 3 4 5 6\n  ..$ qraux: num [1:7] 1.18 1 1.29 1.1 1.07 ...\n  ..$ pivot: int [1:7] 1 2 3 4 5 6 7\n  ..$ tol  : num 1e-07\n  ..$ rank : int 7\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 23\n $ xlevels      : Named list()\n $ call         : language lm(formula = Y ~ ., data = data_3.3)\n $ terms        :Classes 'terms', 'formula'  language Y ~ X1 + X2 + X3 + X4 + X5 + X6\n  .. ..- attr(*, \"variables\")= language list(Y, X1, X2, X3, X4, X5, X6)\n  .. ..- attr(*, \"factors\")= int [1:7, 1:6] 0 1 0 0 0 0 0 0 0 1 ...\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:7] \"Y\" \"X1\" \"X2\" \"X3\" ...\n  .. .. .. ..$ : chr [1:6] \"X1\" \"X2\" \"X3\" \"X4\" ...\n  .. ..- attr(*, \"term.labels\")= chr [1:6] \"X1\" \"X2\" \"X3\" \"X4\" ...\n  .. ..- attr(*, \"order\")= int [1:6] 1 1 1 1 1 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n  .. ..- attr(*, \"predvars\")= language list(Y, X1, X2, X3, X4, X5, X6)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:7] \"numeric\" \"numeric\" \"numeric\" \"numeric\" ...\n  .. .. ..- attr(*, \"names\")= chr [1:7] \"Y\" \"X1\" \"X2\" \"X3\" ...\n $ model        :'data.frame':  30 obs. of  7 variables:\n  ..$ Y : num [1:30] 43 63 71 61 81 43 58 71 72 67 ...\n  ..$ X1: num [1:30] 51 64 70 63 78 55 67 75 82 61 ...\n  ..$ X2: num [1:30] 30 51 68 45 56 49 42 50 72 45 ...\n  ..$ X3: num [1:30] 39 54 69 47 66 44 56 55 67 47 ...\n  ..$ X4: num [1:30] 61 63 76 54 71 54 66 70 71 62 ...\n  ..$ X5: num [1:30] 92 73 86 84 83 49 68 66 83 80 ...\n  ..$ X6: num [1:30] 45 47 48 35 47 34 35 41 31 41 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language Y ~ X1 + X2 + X3 + X4 + X5 + X6\n  .. .. ..- attr(*, \"variables\")= language list(Y, X1, X2, X3, X4, X5, X6)\n  .. .. ..- attr(*, \"factors\")= int [1:7, 1:6] 0 1 0 0 0 0 0 0 0 1 ...\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:7] \"Y\" \"X1\" \"X2\" \"X3\" ...\n  .. .. .. .. ..$ : chr [1:6] \"X1\" \"X2\" \"X3\" \"X4\" ...\n  .. .. ..- attr(*, \"term.labels\")= chr [1:6] \"X1\" \"X2\" \"X3\" \"X4\" ...\n  .. .. ..- attr(*, \"order\")= int [1:6] 1 1 1 1 1 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n  .. .. ..- attr(*, \"predvars\")= language list(Y, X1, X2, X3, X4, X5, X6)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:7] \"numeric\" \"numeric\" \"numeric\" \"numeric\" ...\n  .. .. .. ..- attr(*, \"names\")= chr [1:7] \"Y\" \"X1\" \"X2\" \"X3\" ...\n - attr(*, \"class\")= chr \"lm\"\n\n# 잔차 \n\nres_lm$residuals\n\n          1           2           3           4           5           6 \n -8.1102953   1.6472337   1.0605589  -0.2268416   6.5462010 -10.9418499 \n          7           8           9          10          11          12 \n -9.1484140   0.9029929  -7.5309862   7.8015424   6.0742817  11.5989723 \n         13          14          15          16          17          18 \n  9.4183197  -2.2140147   0.4506705  -3.5478519  -2.1501319   3.6026355 \n         19          20          21          22          23          24 \n -3.0165587  -5.6201442   7.3967582   0.1809831 -10.6639999  -4.6247464 \n         25          26          27          28          29          30 \n  5.6828983  -1.8434727   2.8596385  -8.0453540   7.3394730   5.1215016 \n\nresid(res_lm) #실제값에서 예측된 값을 뺸값\n\n          1           2           3           4           5           6 \n -8.1102953   1.6472337   1.0605589  -0.2268416   6.5462010 -10.9418499 \n          7           8           9          10          11          12 \n -9.1484140   0.9029929  -7.5309862   7.8015424   6.0742817  11.5989723 \n         13          14          15          16          17          18 \n  9.4183197  -2.2140147   0.4506705  -3.5478519  -2.1501319   3.6026355 \n         19          20          21          22          23          24 \n -3.0165587  -5.6201442   7.3967582   0.1809831 -10.6639999  -4.6247464 \n         25          26          27          28          29          30 \n  5.6828983  -1.8434727   2.8596385  -8.0453540   7.3394730   5.1215016 \n\n### 내적 표준화잔차\n\nrstandard(res_lm)\n\n          1           2           3           4           5           6 \n-1.41498026  0.23955370  0.16744867 -0.03512080  0.97184596 -1.86133876 \n          7           8           9          10          11          12 \n-1.38317210  0.13709194 -1.29490454  1.14799070  0.95218982  1.76906521 \n         13          14          15          16          17          18 \n 1.51371017 -0.46212316  0.06961486 -0.73868563 -0.34446368  0.75418016 \n         19          20          21          22          23          24 \n-0.45861365 -0.88618779  1.19699287  0.02717120 -1.56184734 -0.85286680 \n         25          26          27          28          29          30 \n 0.89948517 -0.36581416  0.44430497 -1.25422677  1.12683185  0.85971512 \n\n### 외적 표준화잔차\n\nMASS::studres(res_lm)\n\n          1           2           3           4           5           6 \n-1.44835328  0.23458097  0.16386794 -0.03434974  0.97062209 -1.97526518 \n          7           8           9          10          11          12 \n-1.41280382  0.13413337 -1.31529351  1.15637546  0.95017640  1.86145176 \n         13          14          15          16          17          18 \n 1.56019127 -0.45407837  0.06809185 -0.73117411 -0.33776450  0.74689589 \n         19          20          21          22          23          24 \n-0.45059801 -0.88189556  1.20894332  0.02657438 -1.61559196 -0.84763116 \n         25          26          27          28          29          30 \n 0.89560731 -0.35881868  0.43641573 -1.27088888  1.13380428  0.85466249 \n\nredsid_df<-data.frame(\n\n  Y=data_3.3$Y,\n\n  Y_hat=res_lm$fitted.values,\n\n  resid=resid(res_lm),\n\n  rstandard=rstandard(res_lm),\n\n  studres=MASS::studres(res_lm)\n\n)\n\nredsid_df\n\n    Y    Y_hat       resid   rstandard     studres\n1  43 51.11030  -8.1102953 -1.41498026 -1.44835328\n2  63 61.35277   1.6472337  0.23955370  0.23458097\n3  71 69.93944   1.0605589  0.16744867  0.16386794\n4  61 61.22684  -0.2268416 -0.03512080 -0.03434974\n5  81 74.45380   6.5462010  0.97184596  0.97062209\n6  43 53.94185 -10.9418499 -1.86133876 -1.97526518\n7  58 67.14841  -9.1484140 -1.38317210 -1.41280382\n8  71 70.09701   0.9029929  0.13709194  0.13413337\n9  72 79.53099  -7.5309862 -1.29490454 -1.31529351\n10 67 59.19846   7.8015424  1.14799070  1.15637546\n11 64 57.92572   6.0742817  0.95218982  0.95017640\n12 67 55.40103  11.5989723  1.76906521  1.86145176\n13 69 59.58168   9.4183197  1.51371017  1.56019127\n14 68 70.21401  -2.2140147 -0.46212316 -0.45407837\n15 77 76.54933   0.4506705  0.06961486  0.06809185\n16 81 84.54785  -3.5478519 -0.73868563 -0.73117411\n17 74 76.15013  -2.1501319 -0.34446368 -0.33776450\n18 65 61.39736   3.6026355  0.75418016  0.74689589\n19 65 68.01656  -3.0165587 -0.45861365 -0.45059801\n20 50 55.62014  -5.6201442 -0.88618779 -0.88189556\n21 50 42.60324   7.3967582  1.19699287  1.20894332\n22 64 63.81902   0.1809831  0.02717120  0.02657438\n23 53 63.66400 -10.6639999 -1.56184734 -1.61559196\n24 40 44.62475  -4.6247464 -0.85286680 -0.84763116\n25 63 57.31710   5.6828983  0.89948517  0.89560731\n26 66 67.84347  -1.8434727 -0.36581416 -0.35881868\n27 78 75.14036   2.8596385  0.44430497  0.43641573\n28 48 56.04535  -8.0453540 -1.25422677 -1.27088888\n29 85 77.66053   7.3394730  1.12683185  1.13380428\n30 82 76.87850   5.1215016  0.85971512  0.85466249\n\n\n\n\n\n\n\n\n\na<-rnorm(100,70,10) #연속형 데이터\n\n# 히스토그램 \n\nhist(a)\n\n\n\nhist(a,breaks=5) #범위를 조절 막대의 5번 자름 \n\n\n\n# 줄기 잎 그림 \n\nstem(a)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  4 | 7\n  5 | 003\n  5 | 567779999\n  6 | 111112222234444444\n  6 | 5555566677888888999\n  7 | 011112222233334\n  7 | 5556666666677788888999\n  8 | 01222334\n  8 | 679\n  9 | 00\n\nstem(round(a)) #줄기잎을 그릴때는 반올림을 하고 항상 진행 \n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  4 | 7\n  5 | 003\n  5 | 567779999\n  6 | 111112222234444444\n  6 | 5555566677888888999\n  7 | 011112222233334\n  7 | 5556666666677788888999\n  8 | 01222334\n  8 | 679\n  9 | 00\n\nstem(round(a),scale=2) #scale을 2배로 늘려라 5기준으로 반으로 잘라서 \n\n\n  The decimal point is at the |\n\n  46 | 0\n  48 | \n  50 | 00\n  52 | 0\n  54 | 0\n  56 | 0000\n  58 | 0000\n  60 | 00000\n  62 | 000000\n  64 | 000000000000\n  66 | 00000\n  68 | 000000000\n  70 | 00000\n  72 | 000000000\n  74 | 0000\n  76 | 00000000000\n  78 | 00000000\n  80 | 00\n  82 | 00000\n  84 | 0\n  86 | 00\n  88 | 0\n  90 | 00\n\n# 모든데이터를 볼 수있는 장점 데이터가 많으면 구림 \n\n# 점플롯\n\nidx<-rep(1,length(a)) #a의 갯수에 맞춰서 1를 반복 \n\nplot(idx,a)\n\n\n\nplot(jitter(idx),a,xlim=c(0.5,1.5))\n\n\n\n# 상자그림\n\nboxplot(a) #사분위수에 대해서 알 수 있음 \n\n# 상자그림 + 점플롯\n\nboxplot(a)\n\npoints(jitter(idx),a)\n\n\n\n\n\n\n\n\ndata_4.1<-read.table(\"All_Data/p103.txt\",header=T,sep=\"\\t\")\n\ndata_4.1\n\n       Y   X1   X2\n1  12.37 2.23 9.66\n2  12.66 2.57 8.94\n3  12.00 3.87 4.40\n4  11.93 3.10 6.64\n5  11.06 3.39 4.91\n6  13.03 2.83 8.52\n7  13.13 3.02 8.04\n8  11.44 2.14 9.05\n9  12.86 3.04 7.71\n10 10.84 3.26 5.11\n11 11.20 3.39 5.05\n12 11.56 2.35 8.51\n13 10.83 2.76 6.59\n14 12.63 3.90 4.90\n15 12.46 3.16 6.96\n\nclass(data_4.1) #data.frame\n\n[1] \"data.frame\"\n\n# 산점도 행렬\n\nplot(data_4.1)\n\ncor(data_4.1) #상관계수\n\n             Y           X1         X2\nY  1.000000000  0.002497966  0.4340688\nX1 0.002497966  1.000000000 -0.8997765\nX2 0.434068758 -0.899776481  1.0000000\n\npairs(data_4.1)\n\n\n\n# Correlation panel\n\npanel.cor<-function(x,y){\n\n  par(usr=c(0,1,0,1))\n\n  r<-round(cor(x,y),digits = 3)\n\n  text(0.5,0.5,r,cex=1.5)\n\n}\n\npairs(data_4.1,lower.panel = panel.cor)\n\n\n\n# 회전도표, 동적 그래프(3차원)\n\n#install.packages(\"rgl\")\n\nlibrary(rgl)\n\nWarning: package 'rgl' was built under R version 4.3.2\n\nplot3d(x=data_4.1$X1,y=data_4.1$X2,z=data_4.1$Y) \n\n\n\n\n\ndata_3.3<-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\nres_lm<-lm(Y~X1+X3,data=data_3.3)\n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(res_lm) #회귀진단 플랏이 나옴 \n\n\n\nlayout(1)\n\n# 1. 표준화잔차의 정규확률분포\n\nplot(res_lm,2)\n\n\n\n# 2. 표준화잔차 대 각 예측변수들의 산점도\n\nplot(res_lm,3) #이런경우에는 데이터를 추가한다 좌측하단이 없어서 \n\n\n\n# 3. 표준화잔차 대 적합값의 플롯\n\n# 4. 표준화잔차의 인덱스 플롯"
  },
  {
    "objectID": "R_Basic.html",
    "href": "R_Basic.html",
    "title": "R Basic",
    "section": "",
    "text": "모두를 위한 R 데이터 분석 입문\n\n\n\n\n2 + 3  # 2 더하기 3\n## [1] 5\n(3 + 6) * 8\n## [1] 72\n2 ^ 3  # 2의 세제곱\n## [1] 8\n8 %% 3\n## [1] 2\n\n\n7 + 4\n## [1] 11\n\n\nlog(10) + 5 # 로그함수\n## [1] 7.302585\nsqrt(25) # 제곱근\n## [1] 5\nmax(5, 3, 2) # 가장 큰 값\n## [1] 5\n\n\na <- 10\nb <- 20\nc <- a+b\nprint(c)\n## [1] 30\n\n\na <- 125\na\n## [1] 125\nprint(a)\n## [1] 125\n\n\na <- 10 # a에 숫자 저장\nb <- 20\na + b # a+b의 결과 출력\n## [1] 30\na <- \"A\" # a에 문자 저장\na + b # a+b의 결과 출력. 에러 발생\n## Error in a + b: non-numeric argument to binary operator\n\n\nx <- c(1, 2, 3) # 숫자형 벡터\ny <- c(\"a\", \"b\", \"c\") # 문자형 벡터\nz <- c(TRUE, TRUE, FALSE, TRUE) # 논리형 벡터\nx ; y ;z\n## [1] 1 2 3\n## [1] \"a\" \"b\" \"c\"\n## [1]  TRUE  TRUE FALSE  TRUE\n\n\nw <- c(1, 2, 3, \"a\", \"b\", \"c\")\nw\n## [1] \"1\" \"2\" \"3\" \"a\" \"b\" \"c\"\n\n\nv1 <- 50:90\nv1\n##  [1] 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74\n## [26] 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90\nv2 <- c(1, 2, 5, 50:90)\nv2\n##  [1]  1  2  5 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n## [26] 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90\n\n\nv3 <- seq(1, 101, 3)\nv3\n##  [1]   1   4   7  10  13  16  19  22  25  28  31  34  37  40  43  46  49  52  55\n## [20]  58  61  64  67  70  73  76  79  82  85  88  91  94  97 100\nv4 <- seq(0.1, 1.0, 0.1)\nv4\n##  [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n\n\nv5 <- rep(1, times = 5) # 1을 5번 반복\nv5\n## [1] 1 1 1 1 1\nv6 <- rep(1:5, times = 3) # 1에서 5까지 3번 반복\nv6\n##  [1] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5\nv7 <- rep(c(1, 5, 9), times = 3) # 1, 5, 9를 3번 반복\nv7\n## [1] 1 5 9 1 5 9 1 5 9\nv8 <- rep(1:5, each = 3) # 1에서 5를 각각 3번 반복\nv8\n##  [1] 1 1 1 2 2 2 3 3 3 4 4 4 5 5 5\n\nrep(1:3, each = 3, times = 3)\n##  [1] 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3\nrep(1:3, times = 3, each = 3)\n##  [1] 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3\n\n\nscore <- c(90, 85, 70) # 성적\nscore\n## [1] 90 85 70\nnames(score) # score에 저장된 값들의 이름을 보이시오\n## NULL\nnames(score) <- c(\"John\", \"Tom\", \"Jane\") # 값들에 이름을 부여\nnames(score) # score에 저장된 값들의 이름을 보이시오\n## [1] \"John\" \"Tom\"  \"Jane\"\nscore # 이름과 함께 값이 출력\n## John  Tom Jane \n##   90   85   70\n\n\nd <- c(1, 4, 3, 7, 8)\nd[1]\n## [1] 1\nd[2]\n## [1] 4\nd[3]\n## [1] 3\nd[4]\n## [1] 7\nd[5]\n## [1] 8\nd[6]\n## [1] NA\nd[c(2, 4)]\n## [1] 4 7\n\n\nd <- c(1, 4, 3, 7, 8)\nd[c(1, 3, 5)] # 1, 3, 5번째 값 출력\n## [1] 1 3 8\nd[1:3] # 처음 세 개의 값 출력\n## [1] 1 4 3\nd[seq(1, 5, 2)] # 홀수 번째 값 출력\n## [1] 1 3 8\nd[-2] # 2번째 값 제외하고 출력\n## [1] 1 3 7 8\nd[-c(3:5)] # 3~5번째 값은 제외하고 출력\n## [1] 1 4\n\n\nGNP <- c(2000, 2450, 960)\nGNP\n## [1] 2000 2450  960\nnames(GNP) <- c(\"Korea\", \"Japan\", \"Nepal\")\nGNP\n## Korea Japan Nepal \n##  2000  2450   960\nGNP[1]\n## Korea \n##  2000\nGNP[\"Korea\"]\n## Korea \n##  2000\nGNP_NEW <- GNP[c(\"Korea\", \"Nepal\")]\nGNP_NEW\n## Korea Nepal \n##  2000   960\n\n\nv1 <- c(1, 5, 7, 8, 9)\nv1\n## [1] 1 5 7 8 9\nv1[2] <- 3 # v1의 2번째 값을 3으로 변경\nv1\n## [1] 1 3 7 8 9\nv1[c(1, 5)] <- c(10, 20) # v1의 1, 5번째 값을 각각 10, 20으로 변경\nv1\n## [1] 10  3  7  8 20\n\n\nd <- c(1, 4, 3, 7, 8)\n2 * d\n## [1]  2  8  6 14 16\nd - 5\n## [1] -4 -1 -2  2  3\n3 * d + 4\n## [1]  7 16 13 25 28\n\n\nx <- c(1, 2, 3)\ny <- c(4, 5, 6)\nx + y # 대응하는 원소끼리 더하여 출력\n## [1] 5 7 9\nx * y # 대응하는 원소끼리 곱하여 출력\n## [1]  4 10 18\nz <- x + y # x, y를 더하여 z에 저장\nz\n## [1] 5 7 9\n\n\nd <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\nsum(d) # d에 포함된 값들의 합\n## [1] 55\nsum(2 * d) # d에 포함된 값들에 2를 곱한 후 합한 값\n## [1] 110\nlength(d) # d에 포함된 값들의 개수\n## [1] 10\nmean(d[1:5]) # 1~5번째 값들의 평균\n## [1] 3\nmax(d) # d에 포함된 값들의 최댓값\n## [1] 10\nmin(d) # d에 포함된 값들의 최솟값\n## [1] 1\nsort(d) # 오름차순 정렬\n##  [1]  1  2  3  4  5  6  7  8  9 10\nsort(d, decreasing = FALSE) # 오름차순 정렬\n##  [1]  1  2  3  4  5  6  7  8  9 10\nsort(d, decreasing = TRUE) # 내림차순 정렬\n##  [1] 10  9  8  7  6  5  4  3  2  1\nsort(d, TRUE) # 내림차순 정렬\n##  [1] 10  9  8  7  6  5  4  3  2  1\n\nv1 <- median(d)\nv1\n## [1] 5.5\nv2 <- sum(d) / length(d)\nv2\n## [1] 5.5\nmean(d)\n## [1] 5.5\n\n\nd <- c(1, 2, 3, 4, 5, 6, 7, 8, 9)\nd >= 5\n## [1] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\nd[d > 5] # 5보다 큰 값\n## [1] 6 7 8 9\nsum(d > 5) # 5보다 큰 값의 개수를 출력\n## [1] 4\nsum(d[d > 5]) # 5보다 큰 값의 합계를 출력\n## [1] 30\nd == 5\n## [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n\ncondi <- d > 5 & d < 8 # 조건을 변수에 저장\nd[condi] # 조건에 맞는 값들을 선택\n## [1] 6 7\nd[d > 5 & d < 8]\n## [1] 6 7\n\n\nds <- c(90, 85, 70, 84)\nmy.info <- list(name = 'Tom', age = 60, status = TRUE, score = ds)\nmy.info # 리스트에 저장된 내용을 모두 출력\n## $name\n## [1] \"Tom\"\n## \n## $age\n## [1] 60\n## \n## $status\n## [1] TRUE\n## \n## $score\n## [1] 90 85 70 84\nmy.info[1] # 이름이랑 내용 다 출력\n## $name\n## [1] \"Tom\"\nmy.info[[1]] # 리스트의 첫 번째 값을 출력\n## [1] \"Tom\"\nmy.info$name # 리스트에서 값의 이름이 name인 값을 출력\n## [1] \"Tom\"\nmy.info[[4]] # 리스트의 네 번째 값을 출력\n## [1] 90 85 70 84\n\n\nbt <- c('A', 'B', 'B', 'O', 'AB', 'A') # 문자형 벡터 bt 정의\nbt.new <- factor(bt) # 팩터 bt.new 정의\nbt # 벡터 bt의 내용 출력\n## [1] \"A\"  \"B\"  \"B\"  \"O\"  \"AB\" \"A\"\nbt.new # 팩터 bt.new의 내용 출력\n## [1] A  B  B  O  AB A \n## Levels: A AB B O\nbt[5] # 벡터 bt의 5번째 값 출력\n## [1] \"AB\"\nbt.new[5] # 팩터 bt.new의 5번째 값 출력\n## [1] AB\n## Levels: A AB B O\nlevels(bt.new) # 팩터에 저장된 값의 종류를 출력\n## [1] \"A\"  \"AB\" \"B\"  \"O\"\nas.integer(bt.new) # 팩터의 문자값을 숫자로 바꾸어 출력\n## [1] 1 3 3 4 2 1\nbt.new[7] <- 'B' # 팩터 bt.new의 7번째에 'B' 저장\nbt.new[8] <- 'C' # 팩터 bt.new의 8번째에 'C' 저장\n## Warning in `[<-.factor`(`*tmp*`, 8, value = \"C\"): invalid factor level, NA\n## generated\nbt.new # 팩터 bt.new의 내용 출력\n## [1] A    B    B    O    AB   A    B    <NA>\n## Levels: A AB B O\n\n\n\n\n\nz <- matrix(1:20, nrow = 4, ncol = 5)\nz # 매트릭스 z의 내용을 출력\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\n\n\nz2 <- matrix(1:20, nrow = 4, ncol = 5, byrow = T)\nz2 # 매트릭스 z2의 내용을 출력\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    2    3    4    5\n## [2,]    6    7    8    9   10\n## [3,]   11   12   13   14   15\n## [4,]   16   17   18   19   20\n\nz <- matrix(1:16, nrow = 4, ncol = 5)\n## Warning in matrix(1:16, nrow = 4, ncol = 5): data length [16] is not a\n## sub-multiple or multiple of the number of columns [5]\nz\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13    1\n## [2,]    2    6   10   14    2\n## [3,]    3    7   11   15    3\n## [4,]    4    8   12   16    4\n\n\nx <- 1:4 # 벡터 x 생성\ny <- 5:8 # 벡터 y 생성\nz <- matrix(1:20, nrow = 4, ncol = 5) # 매트릭스 z 생성\n\nm1 <- cbind(x, y) # x와 y를 열 방향으로 결합하여 매트릭스 생성\nm1 # 매트릭스 m1의 내용을 출력\n##      x y\n## [1,] 1 5\n## [2,] 2 6\n## [3,] 3 7\n## [4,] 4 8\nm2 <- rbind(x, y) # x와 y를 행 방향으로 결합하여 매트릭스 생성\nm2 # 매트릭스 m2의 내용을 출력\n##   [,1] [,2] [,3] [,4]\n## x    1    2    3    4\n## y    5    6    7    8\nm3 <- rbind(m2, x) # m2와 벡터 x를 행 방향으로 결합\nm3 # 매트릭스 m3의 내용을 출력\n##   [,1] [,2] [,3] [,4]\n## x    1    2    3    4\n## y    5    6    7    8\n## x    1    2    3    4\nm4 <- cbind(z, x) # 매트릭스 z와 벡터 x를 열 방향으로 결합\nm4 # 매트릭스 m4의 내용을 출력\n##                   x\n## [1,] 1 5  9 13 17 1\n## [2,] 2 6 10 14 18 2\n## [3,] 3 7 11 15 19 3\n## [4,] 4 8 12 16 20 4\n\nx <- 1:5\nm5 <- cbind(z, x)\n## Warning in cbind(z, x): number of rows of result is not a multiple of vector\n## length (arg 2)\nm5\n##                   x\n## [1,] 1 5  9 13 17 1\n## [2,] 2 6 10 14 18 2\n## [3,] 3 7 11 15 19 3\n## [4,] 4 8 12 16 20 4\n\n\nz <- matrix(1:20, nrow = 4, ncol = 5) # 매트릭스 z 생성\nz # 매트릭스 z의 내용 출력\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\n\nz[2, 3] # 2행 3열에 있는 값\n## [1] 10\nz[1, 4] # 1행 4열에 있는 값\n## [1] 13\nz[2, ] # 2행에 있는 모든 값\n## [1]  2  6 10 14 18\nz[, 4] # 4열에 있는 모든 값\n## [1] 13 14 15 16\nz[, ]\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\nz\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\n\n\nz <- matrix(1:20, nrow = 4, ncol = 5) # 매트릭스 z 생성\nz # 매트릭스 z의 내용 출력\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\n\nz[2, 1:3] # 2행의 값 중 1~3열에 있는 값\n## [1]  2  6 10\nz[1, c(1, 2, 4)] # 1행의 값 중 1, 2, 4열에 있는 값\n## [1]  1  5 13\nz[1:2, ] # 1, 2행에 있는 모든 값\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\nz[, c(1, 4)] # 1, 4열에 있는 모든 값\n##      [,1] [,2]\n## [1,]    1   13\n## [2,]    2   14\n## [3,]    3   15\n## [4,]    4   16\n\n\nscore <- matrix(c(90, 85, 69, 78,\n                  85, 96, 49, 95,\n                  90, 80, 70, 60),\n                nrow = 4,\n                ncol = 3)\nscore\n##      [,1] [,2] [,3]\n## [1,]   90   85   90\n## [2,]   85   96   80\n## [3,]   69   49   70\n## [4,]   78   95   60\nrownames(score) <- c('John', 'Tom', 'Mark', 'Jane')\ncolnames(score) <- c('English', 'Math', 'Science')\nscore\n##      English Math Science\n## John      90   85      90\n## Tom       85   96      80\n## Mark      69   49      70\n## Jane      78   95      60\n\n\nscore['John', 'Math'] # John의 수학 성적\n## [1] 85\nscore['Tom', c('Math', 'Science')] # Tom의 수학, 과학 성적\n##    Math Science \n##      96      80\nscore['Mark', ] # Mark의 모든 과목 성적\n## English    Math Science \n##      69      49      70\nscore[, 'English'] # 모든 학생의 영어 성적\n## John  Tom Mark Jane \n##   90   85   69   78\nrownames(score) # score의 행의 이름\n## [1] \"John\" \"Tom\"  \"Mark\" \"Jane\"\ncolnames(score) # score의 열의 이름\n## [1] \"English\" \"Math\"    \"Science\"\ncolnames(score)[2] # score의 열의 이름 중 두 번째 값\n## [1] \"Math\"\n\n\ncity <- c(\"Seoul\", \"Tokyo\", \"Washington\") # 문자로 이루어진 벡터\nrank <- c(1, 3, 2) # 숫자로 이루어진 벡터\ncity.info <- data.frame(city, rank) # 데이터프레임 생성\ncity.info # city.info의 내용 출력\n##         city rank\n## 1      Seoul    1\n## 2      Tokyo    3\n## 3 Washington    2\n\n\n# iris\niris[, c(1:2)] # 1, 2열의 모든 데이터\n##     Sepal.Length Sepal.Width\n## 1            5.1         3.5\n## 2            4.9         3.0\n## 3            4.7         3.2\n## 4            4.6         3.1\n## 5            5.0         3.6\n## 6            5.4         3.9\n## 7            4.6         3.4\n## 8            5.0         3.4\n## 9            4.4         2.9\n## 10           4.9         3.1\n## 11           5.4         3.7\n## 12           4.8         3.4\n## 13           4.8         3.0\n## 14           4.3         3.0\n## 15           5.8         4.0\n## 16           5.7         4.4\n## 17           5.4         3.9\n## 18           5.1         3.5\n## 19           5.7         3.8\n## 20           5.1         3.8\n## 21           5.4         3.4\n## 22           5.1         3.7\n## 23           4.6         3.6\n## 24           5.1         3.3\n## 25           4.8         3.4\n## 26           5.0         3.0\n## 27           5.0         3.4\n## 28           5.2         3.5\n## 29           5.2         3.4\n## 30           4.7         3.2\n## 31           4.8         3.1\n## 32           5.4         3.4\n## 33           5.2         4.1\n## 34           5.5         4.2\n## 35           4.9         3.1\n## 36           5.0         3.2\n## 37           5.5         3.5\n## 38           4.9         3.6\n## 39           4.4         3.0\n## 40           5.1         3.4\n## 41           5.0         3.5\n## 42           4.5         2.3\n## 43           4.4         3.2\n## 44           5.0         3.5\n## 45           5.1         3.8\n## 46           4.8         3.0\n## 47           5.1         3.8\n## 48           4.6         3.2\n## 49           5.3         3.7\n## 50           5.0         3.3\n## 51           7.0         3.2\n## 52           6.4         3.2\n## 53           6.9         3.1\n## 54           5.5         2.3\n## 55           6.5         2.8\n## 56           5.7         2.8\n## 57           6.3         3.3\n## 58           4.9         2.4\n## 59           6.6         2.9\n## 60           5.2         2.7\n## 61           5.0         2.0\n## 62           5.9         3.0\n## 63           6.0         2.2\n## 64           6.1         2.9\n## 65           5.6         2.9\n## 66           6.7         3.1\n## 67           5.6         3.0\n## 68           5.8         2.7\n## 69           6.2         2.2\n## 70           5.6         2.5\n## 71           5.9         3.2\n## 72           6.1         2.8\n## 73           6.3         2.5\n## 74           6.1         2.8\n## 75           6.4         2.9\n## 76           6.6         3.0\n## 77           6.8         2.8\n## 78           6.7         3.0\n## 79           6.0         2.9\n## 80           5.7         2.6\n## 81           5.5         2.4\n## 82           5.5         2.4\n## 83           5.8         2.7\n## 84           6.0         2.7\n## 85           5.4         3.0\n## 86           6.0         3.4\n## 87           6.7         3.1\n## 88           6.3         2.3\n## 89           5.6         3.0\n## 90           5.5         2.5\n## 91           5.5         2.6\n## 92           6.1         3.0\n## 93           5.8         2.6\n## 94           5.0         2.3\n## 95           5.6         2.7\n## 96           5.7         3.0\n## 97           5.7         2.9\n## 98           6.2         2.9\n## 99           5.1         2.5\n## 100          5.7         2.8\n## 101          6.3         3.3\n## 102          5.8         2.7\n## 103          7.1         3.0\n## 104          6.3         2.9\n## 105          6.5         3.0\n## 106          7.6         3.0\n## 107          4.9         2.5\n## 108          7.3         2.9\n## 109          6.7         2.5\n## 110          7.2         3.6\n## 111          6.5         3.2\n## 112          6.4         2.7\n## 113          6.8         3.0\n## 114          5.7         2.5\n## 115          5.8         2.8\n## 116          6.4         3.2\n## 117          6.5         3.0\n## 118          7.7         3.8\n## 119          7.7         2.6\n## 120          6.0         2.2\n## 121          6.9         3.2\n## 122          5.6         2.8\n## 123          7.7         2.8\n## 124          6.3         2.7\n## 125          6.7         3.3\n## 126          7.2         3.2\n## 127          6.2         2.8\n## 128          6.1         3.0\n## 129          6.4         2.8\n## 130          7.2         3.0\n## 131          7.4         2.8\n## 132          7.9         3.8\n## 133          6.4         2.8\n## 134          6.3         2.8\n## 135          6.1         2.6\n## 136          7.7         3.0\n## 137          6.3         3.4\n## 138          6.4         3.1\n## 139          6.0         3.0\n## 140          6.9         3.1\n## 141          6.7         3.1\n## 142          6.9         3.1\n## 143          5.8         2.7\n## 144          6.8         3.2\n## 145          6.7         3.3\n## 146          6.7         3.0\n## 147          6.3         2.5\n## 148          6.5         3.0\n## 149          6.2         3.4\n## 150          5.9         3.0\niris[, c(1, 3, 5)] # 1, 3, 5열의 모든 데이터\n##     Sepal.Length Petal.Length    Species\n## 1            5.1          1.4     setosa\n## 2            4.9          1.4     setosa\n## 3            4.7          1.3     setosa\n## 4            4.6          1.5     setosa\n## 5            5.0          1.4     setosa\n## 6            5.4          1.7     setosa\n## 7            4.6          1.4     setosa\n## 8            5.0          1.5     setosa\n## 9            4.4          1.4     setosa\n## 10           4.9          1.5     setosa\n## 11           5.4          1.5     setosa\n## 12           4.8          1.6     setosa\n## 13           4.8          1.4     setosa\n## 14           4.3          1.1     setosa\n## 15           5.8          1.2     setosa\n## 16           5.7          1.5     setosa\n## 17           5.4          1.3     setosa\n## 18           5.1          1.4     setosa\n## 19           5.7          1.7     setosa\n## 20           5.1          1.5     setosa\n## 21           5.4          1.7     setosa\n## 22           5.1          1.5     setosa\n## 23           4.6          1.0     setosa\n## 24           5.1          1.7     setosa\n## 25           4.8          1.9     setosa\n## 26           5.0          1.6     setosa\n## 27           5.0          1.6     setosa\n## 28           5.2          1.5     setosa\n## 29           5.2          1.4     setosa\n## 30           4.7          1.6     setosa\n## 31           4.8          1.6     setosa\n## 32           5.4          1.5     setosa\n## 33           5.2          1.5     setosa\n## 34           5.5          1.4     setosa\n## 35           4.9          1.5     setosa\n## 36           5.0          1.2     setosa\n## 37           5.5          1.3     setosa\n## 38           4.9          1.4     setosa\n## 39           4.4          1.3     setosa\n## 40           5.1          1.5     setosa\n## 41           5.0          1.3     setosa\n## 42           4.5          1.3     setosa\n## 43           4.4          1.3     setosa\n## 44           5.0          1.6     setosa\n## 45           5.1          1.9     setosa\n## 46           4.8          1.4     setosa\n## 47           5.1          1.6     setosa\n## 48           4.6          1.4     setosa\n## 49           5.3          1.5     setosa\n## 50           5.0          1.4     setosa\n## 51           7.0          4.7 versicolor\n## 52           6.4          4.5 versicolor\n## 53           6.9          4.9 versicolor\n## 54           5.5          4.0 versicolor\n## 55           6.5          4.6 versicolor\n## 56           5.7          4.5 versicolor\n## 57           6.3          4.7 versicolor\n## 58           4.9          3.3 versicolor\n## 59           6.6          4.6 versicolor\n## 60           5.2          3.9 versicolor\n## 61           5.0          3.5 versicolor\n## 62           5.9          4.2 versicolor\n## 63           6.0          4.0 versicolor\n## 64           6.1          4.7 versicolor\n## 65           5.6          3.6 versicolor\n## 66           6.7          4.4 versicolor\n## 67           5.6          4.5 versicolor\n## 68           5.8          4.1 versicolor\n## 69           6.2          4.5 versicolor\n## 70           5.6          3.9 versicolor\n## 71           5.9          4.8 versicolor\n## 72           6.1          4.0 versicolor\n## 73           6.3          4.9 versicolor\n## 74           6.1          4.7 versicolor\n## 75           6.4          4.3 versicolor\n## 76           6.6          4.4 versicolor\n## 77           6.8          4.8 versicolor\n## 78           6.7          5.0 versicolor\n## 79           6.0          4.5 versicolor\n## 80           5.7          3.5 versicolor\n## 81           5.5          3.8 versicolor\n## 82           5.5          3.7 versicolor\n## 83           5.8          3.9 versicolor\n## 84           6.0          5.1 versicolor\n## 85           5.4          4.5 versicolor\n## 86           6.0          4.5 versicolor\n## 87           6.7          4.7 versicolor\n## 88           6.3          4.4 versicolor\n## 89           5.6          4.1 versicolor\n## 90           5.5          4.0 versicolor\n## 91           5.5          4.4 versicolor\n## 92           6.1          4.6 versicolor\n## 93           5.8          4.0 versicolor\n## 94           5.0          3.3 versicolor\n## 95           5.6          4.2 versicolor\n## 96           5.7          4.2 versicolor\n## 97           5.7          4.2 versicolor\n## 98           6.2          4.3 versicolor\n## 99           5.1          3.0 versicolor\n## 100          5.7          4.1 versicolor\n## 101          6.3          6.0  virginica\n## 102          5.8          5.1  virginica\n## 103          7.1          5.9  virginica\n## 104          6.3          5.6  virginica\n## 105          6.5          5.8  virginica\n## 106          7.6          6.6  virginica\n## 107          4.9          4.5  virginica\n## 108          7.3          6.3  virginica\n## 109          6.7          5.8  virginica\n## 110          7.2          6.1  virginica\n## 111          6.5          5.1  virginica\n## 112          6.4          5.3  virginica\n## 113          6.8          5.5  virginica\n## 114          5.7          5.0  virginica\n## 115          5.8          5.1  virginica\n## 116          6.4          5.3  virginica\n## 117          6.5          5.5  virginica\n## 118          7.7          6.7  virginica\n## 119          7.7          6.9  virginica\n## 120          6.0          5.0  virginica\n## 121          6.9          5.7  virginica\n## 122          5.6          4.9  virginica\n## 123          7.7          6.7  virginica\n## 124          6.3          4.9  virginica\n## 125          6.7          5.7  virginica\n## 126          7.2          6.0  virginica\n## 127          6.2          4.8  virginica\n## 128          6.1          4.9  virginica\n## 129          6.4          5.6  virginica\n## 130          7.2          5.8  virginica\n## 131          7.4          6.1  virginica\n## 132          7.9          6.4  virginica\n## 133          6.4          5.6  virginica\n## 134          6.3          5.1  virginica\n## 135          6.1          5.6  virginica\n## 136          7.7          6.1  virginica\n## 137          6.3          5.6  virginica\n## 138          6.4          5.5  virginica\n## 139          6.0          4.8  virginica\n## 140          6.9          5.4  virginica\n## 141          6.7          5.6  virginica\n## 142          6.9          5.1  virginica\n## 143          5.8          5.1  virginica\n## 144          6.8          5.9  virginica\n## 145          6.7          5.7  virginica\n## 146          6.7          5.2  virginica\n## 147          6.3          5.0  virginica\n## 148          6.5          5.2  virginica\n## 149          6.2          5.4  virginica\n## 150          5.9          5.1  virginica\niris[, c(\"Sepal.Length\", \"Species\")] # 1, 5열의 모든 데이터\n##     Sepal.Length    Species\n## 1            5.1     setosa\n## 2            4.9     setosa\n## 3            4.7     setosa\n## 4            4.6     setosa\n## 5            5.0     setosa\n## 6            5.4     setosa\n## 7            4.6     setosa\n## 8            5.0     setosa\n## 9            4.4     setosa\n## 10           4.9     setosa\n## 11           5.4     setosa\n## 12           4.8     setosa\n## 13           4.8     setosa\n## 14           4.3     setosa\n## 15           5.8     setosa\n## 16           5.7     setosa\n## 17           5.4     setosa\n## 18           5.1     setosa\n## 19           5.7     setosa\n## 20           5.1     setosa\n## 21           5.4     setosa\n## 22           5.1     setosa\n## 23           4.6     setosa\n## 24           5.1     setosa\n## 25           4.8     setosa\n## 26           5.0     setosa\n## 27           5.0     setosa\n## 28           5.2     setosa\n## 29           5.2     setosa\n## 30           4.7     setosa\n## 31           4.8     setosa\n## 32           5.4     setosa\n## 33           5.2     setosa\n## 34           5.5     setosa\n## 35           4.9     setosa\n## 36           5.0     setosa\n## 37           5.5     setosa\n## 38           4.9     setosa\n## 39           4.4     setosa\n## 40           5.1     setosa\n## 41           5.0     setosa\n## 42           4.5     setosa\n## 43           4.4     setosa\n## 44           5.0     setosa\n## 45           5.1     setosa\n## 46           4.8     setosa\n## 47           5.1     setosa\n## 48           4.6     setosa\n## 49           5.3     setosa\n## 50           5.0     setosa\n## 51           7.0 versicolor\n## 52           6.4 versicolor\n## 53           6.9 versicolor\n## 54           5.5 versicolor\n## 55           6.5 versicolor\n## 56           5.7 versicolor\n## 57           6.3 versicolor\n## 58           4.9 versicolor\n## 59           6.6 versicolor\n## 60           5.2 versicolor\n## 61           5.0 versicolor\n## 62           5.9 versicolor\n## 63           6.0 versicolor\n## 64           6.1 versicolor\n## 65           5.6 versicolor\n## 66           6.7 versicolor\n## 67           5.6 versicolor\n## 68           5.8 versicolor\n## 69           6.2 versicolor\n## 70           5.6 versicolor\n## 71           5.9 versicolor\n## 72           6.1 versicolor\n## 73           6.3 versicolor\n## 74           6.1 versicolor\n## 75           6.4 versicolor\n## 76           6.6 versicolor\n## 77           6.8 versicolor\n## 78           6.7 versicolor\n## 79           6.0 versicolor\n## 80           5.7 versicolor\n## 81           5.5 versicolor\n## 82           5.5 versicolor\n## 83           5.8 versicolor\n## 84           6.0 versicolor\n## 85           5.4 versicolor\n## 86           6.0 versicolor\n## 87           6.7 versicolor\n## 88           6.3 versicolor\n## 89           5.6 versicolor\n## 90           5.5 versicolor\n## 91           5.5 versicolor\n## 92           6.1 versicolor\n## 93           5.8 versicolor\n## 94           5.0 versicolor\n## 95           5.6 versicolor\n## 96           5.7 versicolor\n## 97           5.7 versicolor\n## 98           6.2 versicolor\n## 99           5.1 versicolor\n## 100          5.7 versicolor\n## 101          6.3  virginica\n## 102          5.8  virginica\n## 103          7.1  virginica\n## 104          6.3  virginica\n## 105          6.5  virginica\n## 106          7.6  virginica\n## 107          4.9  virginica\n## 108          7.3  virginica\n## 109          6.7  virginica\n## 110          7.2  virginica\n## 111          6.5  virginica\n## 112          6.4  virginica\n## 113          6.8  virginica\n## 114          5.7  virginica\n## 115          5.8  virginica\n## 116          6.4  virginica\n## 117          6.5  virginica\n## 118          7.7  virginica\n## 119          7.7  virginica\n## 120          6.0  virginica\n## 121          6.9  virginica\n## 122          5.6  virginica\n## 123          7.7  virginica\n## 124          6.3  virginica\n## 125          6.7  virginica\n## 126          7.2  virginica\n## 127          6.2  virginica\n## 128          6.1  virginica\n## 129          6.4  virginica\n## 130          7.2  virginica\n## 131          7.4  virginica\n## 132          7.9  virginica\n## 133          6.4  virginica\n## 134          6.3  virginica\n## 135          6.1  virginica\n## 136          7.7  virginica\n## 137          6.3  virginica\n## 138          6.4  virginica\n## 139          6.0  virginica\n## 140          6.9  virginica\n## 141          6.7  virginica\n## 142          6.9  virginica\n## 143          5.8  virginica\n## 144          6.8  virginica\n## 145          6.7  virginica\n## 146          6.7  virginica\n## 147          6.3  virginica\n## 148          6.5  virginica\n## 149          6.2  virginica\n## 150          5.9  virginica\niris[1:5, ] # 1~5행의 모든 데이터\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\niris[1:5, c(1, 3)] # 1~5행의 데이터 중 1, 3열의 데이터\n##   Sepal.Length Petal.Length\n## 1          5.1          1.4\n## 2          4.9          1.4\n## 3          4.7          1.3\n## 4          4.6          1.5\n## 5          5.0          1.4\n\n\ndim(iris) # 행과 열의 개수 출력\n## [1] 150   5\nnrow(iris) # 행의 개수 출력\n## [1] 150\nncol(iris) # 열의 개수 출력\n## [1] 5\ncolnames(iris) # 열 이름 출력, names()와 결과 동일\n## [1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"\nhead(iris) # 데이터셋의 앞부분 일부 출력\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\ntail(iris) # 데이터셋의 뒷부분 일부 출력\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n## 145          6.7         3.3          5.7         2.5 virginica\n## 146          6.7         3.0          5.2         2.3 virginica\n## 147          6.3         2.5          5.0         1.9 virginica\n## 148          6.5         3.0          5.2         2.0 virginica\n## 149          6.2         3.4          5.4         2.3 virginica\n## 150          5.9         3.0          5.1         1.8 virginica\n\nhead(iris, 10)\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1           5.1         3.5          1.4         0.2  setosa\n## 2           4.9         3.0          1.4         0.2  setosa\n## 3           4.7         3.2          1.3         0.2  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\n## 5           5.0         3.6          1.4         0.2  setosa\n## 6           5.4         3.9          1.7         0.4  setosa\n## 7           4.6         3.4          1.4         0.3  setosa\n## 8           5.0         3.4          1.5         0.2  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\ntail(iris, 20)\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n## 131          7.4         2.8          6.1         1.9 virginica\n## 132          7.9         3.8          6.4         2.0 virginica\n## 133          6.4         2.8          5.6         2.2 virginica\n## 134          6.3         2.8          5.1         1.5 virginica\n## 135          6.1         2.6          5.6         1.4 virginica\n## 136          7.7         3.0          6.1         2.3 virginica\n## 137          6.3         3.4          5.6         2.4 virginica\n## 138          6.4         3.1          5.5         1.8 virginica\n## 139          6.0         3.0          4.8         1.8 virginica\n## 140          6.9         3.1          5.4         2.1 virginica\n## 141          6.7         3.1          5.6         2.4 virginica\n## 142          6.9         3.1          5.1         2.3 virginica\n## 143          5.8         2.7          5.1         1.9 virginica\n## 144          6.8         3.2          5.9         2.3 virginica\n## 145          6.7         3.3          5.7         2.5 virginica\n## 146          6.7         3.0          5.2         2.3 virginica\n## 147          6.3         2.5          5.0         1.9 virginica\n## 148          6.5         3.0          5.2         2.0 virginica\n## 149          6.2         3.4          5.4         2.3 virginica\n## 150          5.9         3.0          5.1         1.8 virginica\n\n\nstr(iris) # 데이터셋 요약 정보 보기\n## 'data.frame':    150 obs. of  5 variables:\n##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n##  $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\niris[, 5] # 품종 데이터 보기\n##   [1] setosa     setosa     setosa     setosa     setosa     setosa    \n##   [7] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [13] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [19] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [25] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [31] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [37] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [43] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [49] setosa     setosa     versicolor versicolor versicolor versicolor\n##  [55] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [61] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [67] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [73] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [79] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [85] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [91] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [97] versicolor versicolor versicolor versicolor virginica  virginica \n## [103] virginica  virginica  virginica  virginica  virginica  virginica \n## [109] virginica  virginica  virginica  virginica  virginica  virginica \n## [115] virginica  virginica  virginica  virginica  virginica  virginica \n## [121] virginica  virginica  virginica  virginica  virginica  virginica \n## [127] virginica  virginica  virginica  virginica  virginica  virginica \n## [133] virginica  virginica  virginica  virginica  virginica  virginica \n## [139] virginica  virginica  virginica  virginica  virginica  virginica \n## [145] virginica  virginica  virginica  virginica  virginica  virginica \n## Levels: setosa versicolor virginica\nunique(iris[, 5]) # 품종의 종류 보기(중복 제거)\n## [1] setosa     versicolor virginica \n## Levels: setosa versicolor virginica\ntable(iris[, \"Species\"]) # 품종의 종류별 행의 개수 세기\n## \n##     setosa versicolor  virginica \n##         50         50         50\n\n\niris[, -5]\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width\n## 1            5.1         3.5          1.4         0.2\n## 2            4.9         3.0          1.4         0.2\n## 3            4.7         3.2          1.3         0.2\n## 4            4.6         3.1          1.5         0.2\n## 5            5.0         3.6          1.4         0.2\n## 6            5.4         3.9          1.7         0.4\n## 7            4.6         3.4          1.4         0.3\n## 8            5.0         3.4          1.5         0.2\n## 9            4.4         2.9          1.4         0.2\n## 10           4.9         3.1          1.5         0.1\n## 11           5.4         3.7          1.5         0.2\n## 12           4.8         3.4          1.6         0.2\n## 13           4.8         3.0          1.4         0.1\n## 14           4.3         3.0          1.1         0.1\n## 15           5.8         4.0          1.2         0.2\n## 16           5.7         4.4          1.5         0.4\n## 17           5.4         3.9          1.3         0.4\n## 18           5.1         3.5          1.4         0.3\n## 19           5.7         3.8          1.7         0.3\n## 20           5.1         3.8          1.5         0.3\n## 21           5.4         3.4          1.7         0.2\n## 22           5.1         3.7          1.5         0.4\n## 23           4.6         3.6          1.0         0.2\n## 24           5.1         3.3          1.7         0.5\n## 25           4.8         3.4          1.9         0.2\n## 26           5.0         3.0          1.6         0.2\n## 27           5.0         3.4          1.6         0.4\n## 28           5.2         3.5          1.5         0.2\n## 29           5.2         3.4          1.4         0.2\n## 30           4.7         3.2          1.6         0.2\n## 31           4.8         3.1          1.6         0.2\n## 32           5.4         3.4          1.5         0.4\n## 33           5.2         4.1          1.5         0.1\n## 34           5.5         4.2          1.4         0.2\n## 35           4.9         3.1          1.5         0.2\n## 36           5.0         3.2          1.2         0.2\n## 37           5.5         3.5          1.3         0.2\n## 38           4.9         3.6          1.4         0.1\n## 39           4.4         3.0          1.3         0.2\n## 40           5.1         3.4          1.5         0.2\n## 41           5.0         3.5          1.3         0.3\n## 42           4.5         2.3          1.3         0.3\n## 43           4.4         3.2          1.3         0.2\n## 44           5.0         3.5          1.6         0.6\n## 45           5.1         3.8          1.9         0.4\n## 46           4.8         3.0          1.4         0.3\n## 47           5.1         3.8          1.6         0.2\n## 48           4.6         3.2          1.4         0.2\n## 49           5.3         3.7          1.5         0.2\n## 50           5.0         3.3          1.4         0.2\n## 51           7.0         3.2          4.7         1.4\n## 52           6.4         3.2          4.5         1.5\n## 53           6.9         3.1          4.9         1.5\n## 54           5.5         2.3          4.0         1.3\n## 55           6.5         2.8          4.6         1.5\n## 56           5.7         2.8          4.5         1.3\n## 57           6.3         3.3          4.7         1.6\n## 58           4.9         2.4          3.3         1.0\n## 59           6.6         2.9          4.6         1.3\n## 60           5.2         2.7          3.9         1.4\n## 61           5.0         2.0          3.5         1.0\n## 62           5.9         3.0          4.2         1.5\n## 63           6.0         2.2          4.0         1.0\n## 64           6.1         2.9          4.7         1.4\n## 65           5.6         2.9          3.6         1.3\n## 66           6.7         3.1          4.4         1.4\n## 67           5.6         3.0          4.5         1.5\n## 68           5.8         2.7          4.1         1.0\n## 69           6.2         2.2          4.5         1.5\n## 70           5.6         2.5          3.9         1.1\n## 71           5.9         3.2          4.8         1.8\n## 72           6.1         2.8          4.0         1.3\n## 73           6.3         2.5          4.9         1.5\n## 74           6.1         2.8          4.7         1.2\n## 75           6.4         2.9          4.3         1.3\n## 76           6.6         3.0          4.4         1.4\n## 77           6.8         2.8          4.8         1.4\n## 78           6.7         3.0          5.0         1.7\n## 79           6.0         2.9          4.5         1.5\n## 80           5.7         2.6          3.5         1.0\n## 81           5.5         2.4          3.8         1.1\n## 82           5.5         2.4          3.7         1.0\n## 83           5.8         2.7          3.9         1.2\n## 84           6.0         2.7          5.1         1.6\n## 85           5.4         3.0          4.5         1.5\n## 86           6.0         3.4          4.5         1.6\n## 87           6.7         3.1          4.7         1.5\n## 88           6.3         2.3          4.4         1.3\n## 89           5.6         3.0          4.1         1.3\n## 90           5.5         2.5          4.0         1.3\n## 91           5.5         2.6          4.4         1.2\n## 92           6.1         3.0          4.6         1.4\n## 93           5.8         2.6          4.0         1.2\n## 94           5.0         2.3          3.3         1.0\n## 95           5.6         2.7          4.2         1.3\n## 96           5.7         3.0          4.2         1.2\n## 97           5.7         2.9          4.2         1.3\n## 98           6.2         2.9          4.3         1.3\n## 99           5.1         2.5          3.0         1.1\n## 100          5.7         2.8          4.1         1.3\n## 101          6.3         3.3          6.0         2.5\n## 102          5.8         2.7          5.1         1.9\n## 103          7.1         3.0          5.9         2.1\n## 104          6.3         2.9          5.6         1.8\n## 105          6.5         3.0          5.8         2.2\n## 106          7.6         3.0          6.6         2.1\n## 107          4.9         2.5          4.5         1.7\n## 108          7.3         2.9          6.3         1.8\n## 109          6.7         2.5          5.8         1.8\n## 110          7.2         3.6          6.1         2.5\n## 111          6.5         3.2          5.1         2.0\n## 112          6.4         2.7          5.3         1.9\n## 113          6.8         3.0          5.5         2.1\n## 114          5.7         2.5          5.0         2.0\n## 115          5.8         2.8          5.1         2.4\n## 116          6.4         3.2          5.3         2.3\n## 117          6.5         3.0          5.5         1.8\n## 118          7.7         3.8          6.7         2.2\n## 119          7.7         2.6          6.9         2.3\n## 120          6.0         2.2          5.0         1.5\n## 121          6.9         3.2          5.7         2.3\n## 122          5.6         2.8          4.9         2.0\n## 123          7.7         2.8          6.7         2.0\n## 124          6.3         2.7          4.9         1.8\n## 125          6.7         3.3          5.7         2.1\n## 126          7.2         3.2          6.0         1.8\n## 127          6.2         2.8          4.8         1.8\n## 128          6.1         3.0          4.9         1.8\n## 129          6.4         2.8          5.6         2.1\n## 130          7.2         3.0          5.8         1.6\n## 131          7.4         2.8          6.1         1.9\n## 132          7.9         3.8          6.4         2.0\n## 133          6.4         2.8          5.6         2.2\n## 134          6.3         2.8          5.1         1.5\n## 135          6.1         2.6          5.6         1.4\n## 136          7.7         3.0          6.1         2.3\n## 137          6.3         3.4          5.6         2.4\n## 138          6.4         3.1          5.5         1.8\n## 139          6.0         3.0          4.8         1.8\n## 140          6.9         3.1          5.4         2.1\n## 141          6.7         3.1          5.6         2.4\n## 142          6.9         3.1          5.1         2.3\n## 143          5.8         2.7          5.1         1.9\n## 144          6.8         3.2          5.9         2.3\n## 145          6.7         3.3          5.7         2.5\n## 146          6.7         3.0          5.2         2.3\n## 147          6.3         2.5          5.0         1.9\n## 148          6.5         3.0          5.2         2.0\n## 149          6.2         3.4          5.4         2.3\n## 150          5.9         3.0          5.1         1.8\ncolSums(iris[, -5]) # 열별 합계\n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n##        876.5        458.6        563.7        179.9\ncolMeans(iris[, -5]) # 열별 평균\n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n##     5.843333     3.057333     3.758000     1.199333\nrowSums(iris[, -5]) # 행별 합계\n##   [1] 10.2  9.5  9.4  9.4 10.2 11.4  9.7 10.1  8.9  9.6 10.8 10.0  9.3  8.5 11.2\n##  [16] 12.0 11.0 10.3 11.5 10.7 10.7 10.7  9.4 10.6 10.3  9.8 10.4 10.4 10.2  9.7\n##  [31]  9.7 10.7 10.9 11.3  9.7  9.6 10.5 10.0  8.9 10.2 10.1  8.4  9.1 10.7 11.2\n##  [46]  9.5 10.7  9.4 10.7  9.9 16.3 15.6 16.4 13.1 15.4 14.3 15.9 11.6 15.4 13.2\n##  [61] 11.5 14.6 13.2 15.1 13.4 15.6 14.6 13.6 14.4 13.1 15.7 14.2 15.2 14.8 14.9\n##  [76] 15.4 15.8 16.4 14.9 12.8 12.8 12.6 13.6 15.4 14.4 15.5 16.0 14.3 14.0 13.3\n##  [91] 13.7 15.1 13.6 11.6 13.8 14.1 14.1 14.7 11.7 13.9 18.1 15.5 18.1 16.6 17.5\n## [106] 19.3 13.6 18.3 16.8 19.4 16.8 16.3 17.4 15.2 16.1 17.2 16.8 20.4 19.5 14.7\n## [121] 18.1 15.3 19.2 15.7 17.8 18.2 15.6 15.8 16.9 17.6 18.2 20.1 17.0 15.7 15.7\n## [136] 19.1 17.7 16.8 15.6 17.5 17.8 17.4 15.5 18.2 18.2 17.2 15.7 16.7 17.3 15.8\nrowMeans(iris[, -5]) # 행별 평균\n##   [1] 2.550 2.375 2.350 2.350 2.550 2.850 2.425 2.525 2.225 2.400 2.700 2.500\n##  [13] 2.325 2.125 2.800 3.000 2.750 2.575 2.875 2.675 2.675 2.675 2.350 2.650\n##  [25] 2.575 2.450 2.600 2.600 2.550 2.425 2.425 2.675 2.725 2.825 2.425 2.400\n##  [37] 2.625 2.500 2.225 2.550 2.525 2.100 2.275 2.675 2.800 2.375 2.675 2.350\n##  [49] 2.675 2.475 4.075 3.900 4.100 3.275 3.850 3.575 3.975 2.900 3.850 3.300\n##  [61] 2.875 3.650 3.300 3.775 3.350 3.900 3.650 3.400 3.600 3.275 3.925 3.550\n##  [73] 3.800 3.700 3.725 3.850 3.950 4.100 3.725 3.200 3.200 3.150 3.400 3.850\n##  [85] 3.600 3.875 4.000 3.575 3.500 3.325 3.425 3.775 3.400 2.900 3.450 3.525\n##  [97] 3.525 3.675 2.925 3.475 4.525 3.875 4.525 4.150 4.375 4.825 3.400 4.575\n## [109] 4.200 4.850 4.200 4.075 4.350 3.800 4.025 4.300 4.200 5.100 4.875 3.675\n## [121] 4.525 3.825 4.800 3.925 4.450 4.550 3.900 3.950 4.225 4.400 4.550 5.025\n## [133] 4.250 3.925 3.925 4.775 4.425 4.200 3.900 4.375 4.450 4.350 3.875 4.550\n## [145] 4.550 4.300 3.925 4.175 4.325 3.950\n\n\nz <- matrix(1:20, nrow = 4, ncol = 5)\nz\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\nt(z) # 행과열 방향 전환\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    2    3    4\n## [2,]    5    6    7    8\n## [3,]    9   10   11   12\n## [4,]   13   14   15   16\n## [5,]   17   18   19   20\n\n\nIR.1 <- subset(iris, Species == \"setosa\")\nIR.1\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1           5.1         3.5          1.4         0.2  setosa\n## 2           4.9         3.0          1.4         0.2  setosa\n## 3           4.7         3.2          1.3         0.2  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\n## 5           5.0         3.6          1.4         0.2  setosa\n## 6           5.4         3.9          1.7         0.4  setosa\n## 7           4.6         3.4          1.4         0.3  setosa\n## 8           5.0         3.4          1.5         0.2  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\n## 11          5.4         3.7          1.5         0.2  setosa\n## 12          4.8         3.4          1.6         0.2  setosa\n## 13          4.8         3.0          1.4         0.1  setosa\n## 14          4.3         3.0          1.1         0.1  setosa\n## 15          5.8         4.0          1.2         0.2  setosa\n## 16          5.7         4.4          1.5         0.4  setosa\n## 17          5.4         3.9          1.3         0.4  setosa\n## 18          5.1         3.5          1.4         0.3  setosa\n## 19          5.7         3.8          1.7         0.3  setosa\n## 20          5.1         3.8          1.5         0.3  setosa\n## 21          5.4         3.4          1.7         0.2  setosa\n## 22          5.1         3.7          1.5         0.4  setosa\n## 23          4.6         3.6          1.0         0.2  setosa\n## 24          5.1         3.3          1.7         0.5  setosa\n## 25          4.8         3.4          1.9         0.2  setosa\n## 26          5.0         3.0          1.6         0.2  setosa\n## 27          5.0         3.4          1.6         0.4  setosa\n## 28          5.2         3.5          1.5         0.2  setosa\n## 29          5.2         3.4          1.4         0.2  setosa\n## 30          4.7         3.2          1.6         0.2  setosa\n## 31          4.8         3.1          1.6         0.2  setosa\n## 32          5.4         3.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\n## 35          4.9         3.1          1.5         0.2  setosa\n## 36          5.0         3.2          1.2         0.2  setosa\n## 37          5.5         3.5          1.3         0.2  setosa\n## 38          4.9         3.6          1.4         0.1  setosa\n## 39          4.4         3.0          1.3         0.2  setosa\n## 40          5.1         3.4          1.5         0.2  setosa\n## 41          5.0         3.5          1.3         0.3  setosa\n## 42          4.5         2.3          1.3         0.3  setosa\n## 43          4.4         3.2          1.3         0.2  setosa\n## 44          5.0         3.5          1.6         0.6  setosa\n## 45          5.1         3.8          1.9         0.4  setosa\n## 46          4.8         3.0          1.4         0.3  setosa\n## 47          5.1         3.8          1.6         0.2  setosa\n## 48          4.6         3.2          1.4         0.2  setosa\n## 49          5.3         3.7          1.5         0.2  setosa\n## 50          5.0         3.3          1.4         0.2  setosa\n\nIR.2 <- subset(iris, Sepal.Length > 5.0 & Sepal.Width > 4.0)\nIR.2\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 16          5.7         4.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\nIR.2[, c(2, 4)] # 2, 4열의 값만 추출\n##    Sepal.Width Petal.Width\n## 16         4.4         0.4\n## 33         4.1         0.1\n## 34         4.2         0.2\n\nIR.3 <- subset(iris, Sepal.Length > 5.0 | Sepal.Width > 4.0)\nIR.3\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 1            5.1         3.5          1.4         0.2     setosa\n## 6            5.4         3.9          1.7         0.4     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 15           5.8         4.0          1.2         0.2     setosa\n## 16           5.7         4.4          1.5         0.4     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 34           5.5         4.2          1.4         0.2     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 49           5.3         3.7          1.5         0.2     setosa\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 114          5.7         2.5          5.0         2.0  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 120          6.0         2.2          5.0         1.5  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 122          5.6         2.8          4.9         2.0  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 127          6.2         2.8          4.8         1.8  virginica\n## 128          6.1         3.0          4.9         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\n\n\na <- matrix(1:20, 4, 5)\nb <- matrix(21:40, 4, 5)\na ; b\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   21   25   29   33   37\n## [2,]   22   26   30   34   38\n## [3,]   23   27   31   35   39\n## [4,]   24   28   32   36   40\n\n2 * a # 매트릭스 a에 저장된 값들에 2를 곱하기\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    2   10   18   26   34\n## [2,]    4   12   20   28   36\n## [3,]    6   14   22   30   38\n## [4,]    8   16   24   32   40\nb - 5\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   16   20   24   28   32\n## [2,]   17   21   25   29   33\n## [3,]   18   22   26   30   34\n## [4,]   19   23   27   31   35\n2 * a + 3 * b\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   65   85  105  125  145\n## [2,]   70   90  110  130  150\n## [3,]   75   95  115  135  155\n## [4,]   80  100  120  140  160\n\na + b\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   22   30   38   46   54\n## [2,]   24   32   40   48   56\n## [3,]   26   34   42   50   58\n## [4,]   28   36   44   52   60\nb - a\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   20   20   20   20   20\n## [2,]   20   20   20   20   20\n## [3,]   20   20   20   20   20\n## [4,]   20   20   20   20   20\nb / a\n##           [,1]     [,2]     [,3]     [,4]     [,5]\n## [1,] 21.000000 5.000000 3.222222 2.538462 2.176471\n## [2,] 11.000000 4.333333 3.000000 2.428571 2.111111\n## [3,]  7.666667 3.857143 2.818182 2.333333 2.052632\n## [4,]  6.000000 3.500000 2.666667 2.250000 2.000000\na * b\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   21  125  261  429  629\n## [2,]   44  156  300  476  684\n## [3,]   69  189  341  525  741\n## [4,]   96  224  384  576  800\n\na <- a * 3\na\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    3   15   27   39   51\n## [2,]    6   18   30   42   54\n## [3,]    9   21   33   45   57\n## [4,]   12   24   36   48   60\nb <- b - 5\nb\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   16   20   24   28   32\n## [2,]   17   21   25   29   33\n## [3,]   18   22   26   30   34\n## [4,]   19   23   27   31   35\n\n\nclass(iris) # iris 데이터셋의 자료구조 확인\n## [1] \"data.frame\"\nclass(state.x77) # state.x77 데이터셋의 자료구조 확인\n## [1] \"matrix\" \"array\"\nis.matrix(iris) # 데이터셋이 매트릭스인지를 확인하는 함수\n## [1] FALSE\nis.data.frame(iris) # 데이터셋이 데이터프레임인지를 확인하는 함수\n## [1] TRUE\nis.matrix(state.x77)\n## [1] TRUE\nis.data.frame(state.x77)\n## [1] FALSE\n\n\n# 매트릭스를 데이터프레임으로 변환\nst <- data.frame(state.x77)\nhead(st)\n##            Population Income Illiteracy Life.Exp Murder HS.Grad Frost   Area\n## Alabama          3615   3624        2.1    69.05   15.1    41.3    20  50708\n## Alaska            365   6315        1.5    69.31   11.3    66.7   152 566432\n## Arizona          2212   4530        1.8    70.55    7.8    58.1    15 113417\n## Arkansas         2110   3378        1.9    70.66   10.1    39.9    65  51945\n## California      21198   5114        1.1    71.71   10.3    62.6    20 156361\n## Colorado         2541   4884        0.7    72.06    6.8    63.9   166 103766\nclass(st)\n## [1] \"data.frame\"\n\n\niris[, \"Species\"] # 결과=벡터. 매트릭스와 데이터프레임 모두 가능\n##   [1] setosa     setosa     setosa     setosa     setosa     setosa    \n##   [7] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [13] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [19] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [25] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [31] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [37] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [43] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [49] setosa     setosa     versicolor versicolor versicolor versicolor\n##  [55] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [61] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [67] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [73] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [79] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [85] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [91] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [97] versicolor versicolor versicolor versicolor virginica  virginica \n## [103] virginica  virginica  virginica  virginica  virginica  virginica \n## [109] virginica  virginica  virginica  virginica  virginica  virginica \n## [115] virginica  virginica  virginica  virginica  virginica  virginica \n## [121] virginica  virginica  virginica  virginica  virginica  virginica \n## [127] virginica  virginica  virginica  virginica  virginica  virginica \n## [133] virginica  virginica  virginica  virginica  virginica  virginica \n## [139] virginica  virginica  virginica  virginica  virginica  virginica \n## [145] virginica  virginica  virginica  virginica  virginica  virginica \n## Levels: setosa versicolor virginica\niris[, 5] # 결과=벡터. 매트릭스와 데이터프레임 모두 가능\n##   [1] setosa     setosa     setosa     setosa     setosa     setosa    \n##   [7] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [13] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [19] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [25] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [31] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [37] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [43] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [49] setosa     setosa     versicolor versicolor versicolor versicolor\n##  [55] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [61] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [67] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [73] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [79] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [85] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [91] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [97] versicolor versicolor versicolor versicolor virginica  virginica \n## [103] virginica  virginica  virginica  virginica  virginica  virginica \n## [109] virginica  virginica  virginica  virginica  virginica  virginica \n## [115] virginica  virginica  virginica  virginica  virginica  virginica \n## [121] virginica  virginica  virginica  virginica  virginica  virginica \n## [127] virginica  virginica  virginica  virginica  virginica  virginica \n## [133] virginica  virginica  virginica  virginica  virginica  virginica \n## [139] virginica  virginica  virginica  virginica  virginica  virginica \n## [145] virginica  virginica  virginica  virginica  virginica  virginica \n## Levels: setosa versicolor virginica\niris[\"Species\"] # 결과=데이터프레임. 데이터프레임만 가능\n##        Species\n## 1       setosa\n## 2       setosa\n## 3       setosa\n## 4       setosa\n## 5       setosa\n## 6       setosa\n## 7       setosa\n## 8       setosa\n## 9       setosa\n## 10      setosa\n## 11      setosa\n## 12      setosa\n## 13      setosa\n## 14      setosa\n## 15      setosa\n## 16      setosa\n## 17      setosa\n## 18      setosa\n## 19      setosa\n## 20      setosa\n## 21      setosa\n## 22      setosa\n## 23      setosa\n## 24      setosa\n## 25      setosa\n## 26      setosa\n## 27      setosa\n## 28      setosa\n## 29      setosa\n## 30      setosa\n## 31      setosa\n## 32      setosa\n## 33      setosa\n## 34      setosa\n## 35      setosa\n## 36      setosa\n## 37      setosa\n## 38      setosa\n## 39      setosa\n## 40      setosa\n## 41      setosa\n## 42      setosa\n## 43      setosa\n## 44      setosa\n## 45      setosa\n## 46      setosa\n## 47      setosa\n## 48      setosa\n## 49      setosa\n## 50      setosa\n## 51  versicolor\n## 52  versicolor\n## 53  versicolor\n## 54  versicolor\n## 55  versicolor\n## 56  versicolor\n## 57  versicolor\n## 58  versicolor\n## 59  versicolor\n## 60  versicolor\n## 61  versicolor\n## 62  versicolor\n## 63  versicolor\n## 64  versicolor\n## 65  versicolor\n## 66  versicolor\n## 67  versicolor\n## 68  versicolor\n## 69  versicolor\n## 70  versicolor\n## 71  versicolor\n## 72  versicolor\n## 73  versicolor\n## 74  versicolor\n## 75  versicolor\n## 76  versicolor\n## 77  versicolor\n## 78  versicolor\n## 79  versicolor\n## 80  versicolor\n## 81  versicolor\n## 82  versicolor\n## 83  versicolor\n## 84  versicolor\n## 85  versicolor\n## 86  versicolor\n## 87  versicolor\n## 88  versicolor\n## 89  versicolor\n## 90  versicolor\n## 91  versicolor\n## 92  versicolor\n## 93  versicolor\n## 94  versicolor\n## 95  versicolor\n## 96  versicolor\n## 97  versicolor\n## 98  versicolor\n## 99  versicolor\n## 100 versicolor\n## 101  virginica\n## 102  virginica\n## 103  virginica\n## 104  virginica\n## 105  virginica\n## 106  virginica\n## 107  virginica\n## 108  virginica\n## 109  virginica\n## 110  virginica\n## 111  virginica\n## 112  virginica\n## 113  virginica\n## 114  virginica\n## 115  virginica\n## 116  virginica\n## 117  virginica\n## 118  virginica\n## 119  virginica\n## 120  virginica\n## 121  virginica\n## 122  virginica\n## 123  virginica\n## 124  virginica\n## 125  virginica\n## 126  virginica\n## 127  virginica\n## 128  virginica\n## 129  virginica\n## 130  virginica\n## 131  virginica\n## 132  virginica\n## 133  virginica\n## 134  virginica\n## 135  virginica\n## 136  virginica\n## 137  virginica\n## 138  virginica\n## 139  virginica\n## 140  virginica\n## 141  virginica\n## 142  virginica\n## 143  virginica\n## 144  virginica\n## 145  virginica\n## 146  virginica\n## 147  virginica\n## 148  virginica\n## 149  virginica\n## 150  virginica\niris[5] # 결과=데이터프레임. 데이터프레임만 가능\n##        Species\n## 1       setosa\n## 2       setosa\n## 3       setosa\n## 4       setosa\n## 5       setosa\n## 6       setosa\n## 7       setosa\n## 8       setosa\n## 9       setosa\n## 10      setosa\n## 11      setosa\n## 12      setosa\n## 13      setosa\n## 14      setosa\n## 15      setosa\n## 16      setosa\n## 17      setosa\n## 18      setosa\n## 19      setosa\n## 20      setosa\n## 21      setosa\n## 22      setosa\n## 23      setosa\n## 24      setosa\n## 25      setosa\n## 26      setosa\n## 27      setosa\n## 28      setosa\n## 29      setosa\n## 30      setosa\n## 31      setosa\n## 32      setosa\n## 33      setosa\n## 34      setosa\n## 35      setosa\n## 36      setosa\n## 37      setosa\n## 38      setosa\n## 39      setosa\n## 40      setosa\n## 41      setosa\n## 42      setosa\n## 43      setosa\n## 44      setosa\n## 45      setosa\n## 46      setosa\n## 47      setosa\n## 48      setosa\n## 49      setosa\n## 50      setosa\n## 51  versicolor\n## 52  versicolor\n## 53  versicolor\n## 54  versicolor\n## 55  versicolor\n## 56  versicolor\n## 57  versicolor\n## 58  versicolor\n## 59  versicolor\n## 60  versicolor\n## 61  versicolor\n## 62  versicolor\n## 63  versicolor\n## 64  versicolor\n## 65  versicolor\n## 66  versicolor\n## 67  versicolor\n## 68  versicolor\n## 69  versicolor\n## 70  versicolor\n## 71  versicolor\n## 72  versicolor\n## 73  versicolor\n## 74  versicolor\n## 75  versicolor\n## 76  versicolor\n## 77  versicolor\n## 78  versicolor\n## 79  versicolor\n## 80  versicolor\n## 81  versicolor\n## 82  versicolor\n## 83  versicolor\n## 84  versicolor\n## 85  versicolor\n## 86  versicolor\n## 87  versicolor\n## 88  versicolor\n## 89  versicolor\n## 90  versicolor\n## 91  versicolor\n## 92  versicolor\n## 93  versicolor\n## 94  versicolor\n## 95  versicolor\n## 96  versicolor\n## 97  versicolor\n## 98  versicolor\n## 99  versicolor\n## 100 versicolor\n## 101  virginica\n## 102  virginica\n## 103  virginica\n## 104  virginica\n## 105  virginica\n## 106  virginica\n## 107  virginica\n## 108  virginica\n## 109  virginica\n## 110  virginica\n## 111  virginica\n## 112  virginica\n## 113  virginica\n## 114  virginica\n## 115  virginica\n## 116  virginica\n## 117  virginica\n## 118  virginica\n## 119  virginica\n## 120  virginica\n## 121  virginica\n## 122  virginica\n## 123  virginica\n## 124  virginica\n## 125  virginica\n## 126  virginica\n## 127  virginica\n## 128  virginica\n## 129  virginica\n## 130  virginica\n## 131  virginica\n## 132  virginica\n## 133  virginica\n## 134  virginica\n## 135  virginica\n## 136  virginica\n## 137  virginica\n## 138  virginica\n## 139  virginica\n## 140  virginica\n## 141  virginica\n## 142  virginica\n## 143  virginica\n## 144  virginica\n## 145  virginica\n## 146  virginica\n## 147  virginica\n## 148  virginica\n## 149  virginica\n## 150  virginica\niris$Species # 결과=벡터. 데이터프레임만 가능\n##   [1] setosa     setosa     setosa     setosa     setosa     setosa    \n##   [7] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [13] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [19] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [25] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [31] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [37] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [43] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [49] setosa     setosa     versicolor versicolor versicolor versicolor\n##  [55] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [61] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [67] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [73] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [79] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [85] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [91] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [97] versicolor versicolor versicolor versicolor virginica  virginica \n## [103] virginica  virginica  virginica  virginica  virginica  virginica \n## [109] virginica  virginica  virginica  virginica  virginica  virginica \n## [115] virginica  virginica  virginica  virginica  virginica  virginica \n## [121] virginica  virginica  virginica  virginica  virginica  virginica \n## [127] virginica  virginica  virginica  virginica  virginica  virginica \n## [133] virginica  virginica  virginica  virginica  virginica  virginica \n## [139] virginica  virginica  virginica  virginica  virginica  virginica \n## [145] virginica  virginica  virginica  virginica  virginica  virginica \n## Levels: setosa versicolor virginica\n\n\ngetwd()\n## [1] \"C:/Users/Hyunsoo Kim/Desktop/senior_grade/blog/my-quarto-website\"\n# setwd(\"G:/내 드라이브/202202/R_Basic/data\") # 작업 폴더 지정\nair <- read.csv(\"./R_Basic/data/airquality.csv\", header = T) # .csv 파일 읽기\nhead(air)\n##                                    version.https...git.lfs.github.com.spec.v1\n## 1 oid sha256:6fdc84af524856a54abe063336bfea6511e9fb5dfcd2ec6e1dfa9e1e4d8c7357\n## 2                                                                   size 3044\n\n\nmy.iris <- subset(iris, Species = 'Setosa') # Setosa 품종 데이터만 추출\n## Warning: In subset.data.frame(iris, Species = \"Setosa\") :\n##  extra argument 'Species' will be disregarded\nwrite.csv(my.iris, \"./R_Basic/data/my_iris_1.csv\") # .csv 파일에 저장하기\n\n\n\n\n\njob.type <- 'A'\nif (job.type == 'B') {\n    bonus <- 200 # 직무 유형이 B일 때 실행\n} else {\n    bonus <- 100 # 직무 유형이 B가 아닌 나머지 경우 실행\n}\nprint(bonus)\n## [1] 100\n\n\njob.type <- 'B'\nbonus <- 100\nif (job.type == 'A') {\n    bonus <- 200 # 직무 유형이 A일 때 실행\n}\nprint(bonus)\n## [1] 100\n\n\nscore <- 85\n\nif (score > 90) {\n    grade <- 'A'\n} else if (score > 80) {\n    grade <- 'B'\n} else if (score > 70) {\n    grade <- 'C'\n} else if (score > 60) {\n    grade <- 'D'\n} else {\n    grade <- 'F'\n}\n\nprint(grade)\n## [1] \"B\"\n\n\na <- 10\nb <- 20\nif (a > 5 & b > 5) {    # and 사용\n    print(a + b)\n}\n## [1] 30\n\nif (a > 5 | b > 30) {   # or 사용\n    print(a * b)\n}\n## [1] 200\n\nif (a > 5 & b > 30) {\n    print(a * b)\n}\n\nif (a > 20 | b > 30) {\n    print(a * b)\n}\n\nif (a > 20 & b > 15) {\n    print(a * b)\n}\n\nr_basic <- 70\npython_basic <- 82\n\nif (r_basic > 80 & python_basic > 80) {\n    grade <- \"Excellent\"\n} else {\n    grade <- \"Good\"\n}\ngrade\n## [1] \"Good\"\n\n\na <- 10\nb <- 20\n\nif (a > b) {\n    c <- a\n} else {\n    c <- b\n}\nprint(c)\n## [1] 20\n\na <- 10\nb <- 20\n\nc <- ifelse(a > b, a, b)\nprint(c)\n## [1] 20\n\n\nfor(i in 1:5) {\n    print('*')\n}\n## [1] \"*\"\n## [1] \"*\"\n## [1] \"*\"\n## [1] \"*\"\n## [1] \"*\"\n\nfor (i in 1:5) {\n    print(i)\n}\n## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\n\nfor (i in 1:5) {\n    a <- i * 2\n    print(a)\n}\n## [1] 2\n## [1] 4\n## [1] 6\n## [1] 8\n## [1] 10\n\n# for (i in 1:10000) {\n#     a <- i * 2\n#     print(a)\n# }\n\n# for (i in 1:10000) {\n#     a <- i * 2 / 1521 + 10000\n#     print(a)\n# }\n\n\nfor (i in 6:10) {\n    print(i)\n}\n## [1] 6\n## [1] 7\n## [1] 8\n## [1] 9\n## [1] 10\n\n\nfor(i in 1:9) {\n    cat('2 *', i, '=', 2 * i, '\\n')\n}\n## 2 * 1 = 2 \n## 2 * 2 = 4 \n## 2 * 3 = 6 \n## 2 * 4 = 8 \n## 2 * 5 = 10 \n## 2 * 6 = 12 \n## 2 * 7 = 14 \n## 2 * 8 = 16 \n## 2 * 9 = 18\n\nfor (i in 1:9) {\n    cat('2 *', i, '=', 2 * i)\n}\n## 2 * 1 = 22 * 2 = 42 * 3 = 62 * 4 = 82 * 5 = 102 * 6 = 122 * 7 = 142 * 8 = 162 * 9 = 18\n\nfor (i in 1:9) {\n    j <- i:10\n    print(j)\n}\n##  [1]  1  2  3  4  5  6  7  8  9 10\n## [1]  2  3  4  5  6  7  8  9 10\n## [1]  3  4  5  6  7  8  9 10\n## [1]  4  5  6  7  8  9 10\n## [1]  5  6  7  8  9 10\n## [1]  6  7  8  9 10\n## [1]  7  8  9 10\n## [1]  8  9 10\n## [1]  9 10\n\n\nfor(i in 1:20) {\n    if (i %% 2 == 0) {  # 짝수인지 확인\n        print(i)\n    }\n}\n## [1] 2\n## [1] 4\n## [1] 6\n## [1] 8\n## [1] 10\n## [1] 12\n## [1] 14\n## [1] 16\n## [1] 18\n## [1] 20\n\n\nsum <- 0\nfor (i in 1:100) {\n    sum <- sum + i  # sum에 i 값을 누적\n}\nprint(sum)\n## [1] 5050\n\nsum <- 0\nfor (i in 1:100) {\n    sum <- sum + i\n    print(c(sum, i))\n}\n## [1] 1 1\n## [1] 3 2\n## [1] 6 3\n## [1] 10  4\n## [1] 15  5\n## [1] 21  6\n## [1] 28  7\n## [1] 36  8\n## [1] 45  9\n## [1] 55 10\n## [1] 66 11\n## [1] 78 12\n## [1] 91 13\n## [1] 105  14\n## [1] 120  15\n## [1] 136  16\n## [1] 153  17\n## [1] 171  18\n## [1] 190  19\n## [1] 210  20\n## [1] 231  21\n## [1] 253  22\n## [1] 276  23\n## [1] 300  24\n## [1] 325  25\n## [1] 351  26\n## [1] 378  27\n## [1] 406  28\n## [1] 435  29\n## [1] 465  30\n## [1] 496  31\n## [1] 528  32\n## [1] 561  33\n## [1] 595  34\n## [1] 630  35\n## [1] 666  36\n## [1] 703  37\n## [1] 741  38\n## [1] 780  39\n## [1] 820  40\n## [1] 861  41\n## [1] 903  42\n## [1] 946  43\n## [1] 990  44\n## [1] 1035   45\n## [1] 1081   46\n## [1] 1128   47\n## [1] 1176   48\n## [1] 1225   49\n## [1] 1275   50\n## [1] 1326   51\n## [1] 1378   52\n## [1] 1431   53\n## [1] 1485   54\n## [1] 1540   55\n## [1] 1596   56\n## [1] 1653   57\n## [1] 1711   58\n## [1] 1770   59\n## [1] 1830   60\n## [1] 1891   61\n## [1] 1953   62\n## [1] 2016   63\n## [1] 2080   64\n## [1] 2145   65\n## [1] 2211   66\n## [1] 2278   67\n## [1] 2346   68\n## [1] 2415   69\n## [1] 2485   70\n## [1] 2556   71\n## [1] 2628   72\n## [1] 2701   73\n## [1] 2775   74\n## [1] 2850   75\n## [1] 2926   76\n## [1] 3003   77\n## [1] 3081   78\n## [1] 3160   79\n## [1] 3240   80\n## [1] 3321   81\n## [1] 3403   82\n## [1] 3486   83\n## [1] 3570   84\n## [1] 3655   85\n## [1] 3741   86\n## [1] 3828   87\n## [1] 3916   88\n## [1] 4005   89\n## [1] 4095   90\n## [1] 4186   91\n## [1] 4278   92\n## [1] 4371   93\n## [1] 4465   94\n## [1] 4560   95\n## [1] 4656   96\n## [1] 4753   97\n## [1] 4851   98\n## [1] 4950   99\n## [1] 5050  100\nprint(sum)\n## [1] 5050\n\nsum <- 0\nfor (i in 1:100) {\n    print(c(sum, i))\n    sum <- sum + i\n}\n## [1] 0 1\n## [1] 1 2\n## [1] 3 3\n## [1] 6 4\n## [1] 10  5\n## [1] 15  6\n## [1] 21  7\n## [1] 28  8\n## [1] 36  9\n## [1] 45 10\n## [1] 55 11\n## [1] 66 12\n## [1] 78 13\n## [1] 91 14\n## [1] 105  15\n## [1] 120  16\n## [1] 136  17\n## [1] 153  18\n## [1] 171  19\n## [1] 190  20\n## [1] 210  21\n## [1] 231  22\n## [1] 253  23\n## [1] 276  24\n## [1] 300  25\n## [1] 325  26\n## [1] 351  27\n## [1] 378  28\n## [1] 406  29\n## [1] 435  30\n## [1] 465  31\n## [1] 496  32\n## [1] 528  33\n## [1] 561  34\n## [1] 595  35\n## [1] 630  36\n## [1] 666  37\n## [1] 703  38\n## [1] 741  39\n## [1] 780  40\n## [1] 820  41\n## [1] 861  42\n## [1] 903  43\n## [1] 946  44\n## [1] 990  45\n## [1] 1035   46\n## [1] 1081   47\n## [1] 1128   48\n## [1] 1176   49\n## [1] 1225   50\n## [1] 1275   51\n## [1] 1326   52\n## [1] 1378   53\n## [1] 1431   54\n## [1] 1485   55\n## [1] 1540   56\n## [1] 1596   57\n## [1] 1653   58\n## [1] 1711   59\n## [1] 1770   60\n## [1] 1830   61\n## [1] 1891   62\n## [1] 1953   63\n## [1] 2016   64\n## [1] 2080   65\n## [1] 2145   66\n## [1] 2211   67\n## [1] 2278   68\n## [1] 2346   69\n## [1] 2415   70\n## [1] 2485   71\n## [1] 2556   72\n## [1] 2628   73\n## [1] 2701   74\n## [1] 2775   75\n## [1] 2850   76\n## [1] 2926   77\n## [1] 3003   78\n## [1] 3081   79\n## [1] 3160   80\n## [1] 3240   81\n## [1] 3321   82\n## [1] 3403   83\n## [1] 3486   84\n## [1] 3570   85\n## [1] 3655   86\n## [1] 3741   87\n## [1] 3828   88\n## [1] 3916   89\n## [1] 4005   90\n## [1] 4095   91\n## [1] 4186   92\n## [1] 4278   93\n## [1] 4371   94\n## [1] 4465   95\n## [1] 4560   96\n## [1] 4656   97\n## [1] 4753   98\n## [1] 4851   99\n## [1] 4950  100\nprint(sum)\n## [1] 5050\n\n\nnorow <- nrow(iris)                             # iris의 행의 수\nmylabel <- c()                                  # 비어 있는 벡터 선언\nfor (i in 1:norow) {\n    if (iris$Petal.Length[i] <= 1.6) {          # 꽃잎의 길이에 따라 레이블 결정\n        mylabel[i] <- 'L'\n    } else if (iris$Petal.Length[i] >= 5.1) {\n        mylabel[i] <- 'H'\n    } else {\n        mylabel[i] <- 'M'\n    }\n    print(c(iris$Petal.Length[i], mylabel))\n}\n## [1] \"1.4\" \"L\"  \n## [1] \"1.4\" \"L\"   \"L\"  \n## [1] \"1.3\" \"L\"   \"L\"   \"L\"  \n## [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"  \n## [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [1] \"1.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"  \n## [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"  \n## [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"  \n##  [1] \"1.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"  \n##  [1] \"1.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"  \n##  [1] \"1.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"  \n##  [1] \"1\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\"\n##  [1] \"1.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"  \n##  [1] \"1.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"  \n##  [1] \"4.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"  \n##  [1] \"4.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\"\n##  [1] \"4.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"  \n##  [1] \"3.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"  \n##  [1] \"4.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"  \n##  [1] \"4\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [58] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [1] \"4.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [58] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [1] \"4.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"  \n##  [1] \"4.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"  \n##  [1] \"4.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"5\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [58] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [77] \"M\" \"M\" \"M\"\n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"  \n##  [1] \"4.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [58] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [77] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [1] \"4.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [58] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [77] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [1] \"3.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [97] \"M\"  \n##  [1] \"4.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [97] \"M\"   \"M\"  \n##  [1] \"4.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [97] \"M\"   \"M\"   \"M\"  \n##   [1] \"3\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##   [1] \"4.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##   [1] \"6\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\"\n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"  \n##   [1] \"5.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"6.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n##   [1] \"6.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"  \n##   [1] \"5.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"  \n##   [1] \"6.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"  \n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n## [109] \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"  \n##   [1] \"5.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"  \n##   [1] \"5.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"6.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"6.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n## [109] \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n##   [1] \"5.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"  \n##   [1] \"4.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"  \n##   [1] \"6.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"  \n##   [1] \"4.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"  \n##   [1] \"5.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"  \n##   [1] \"6\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n## [109] \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"M\" \"H\" \"M\" \"H\"\n## [127] \"H\"\n##   [1] \"4.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"  \n##   [1] \"4.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"  \n##   [1] \"5.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"  \n##   [1] \"6.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"6.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"  \n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"6.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"4.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n##   [1] \"5.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"  \n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"  \n##   [1] \"5.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"   \"H\"  \n##   [1] \"5.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"   \"H\"   \"H\"  \n##   [1] \"5\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n## [109] \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"M\" \"H\" \"M\" \"H\"\n## [127] \"H\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\"\n## [145] \"H\" \"H\" \"H\" \"M\"\n##   [1] \"5.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"   \"H\"   \"H\"   \"M\"   \"H\"  \n##   [1] \"5.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"  \n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"\nprint(mylabel)                                  # 레이블 출력\n##   [1] \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\"\n## [109] \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"M\" \"H\" \"M\" \"H\" \"H\"\n## [127] \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\"\n## [145] \"H\" \"H\" \"M\" \"H\" \"H\" \"H\"\nnewds <- data.frame(iris$Petal.Length, mylabel) # 꽃잎의 길이와 레이블 결합\nhead(newds)                                     # 새로운 데이터셋 내용 출력\n##   iris.Petal.Length mylabel\n## 1               1.4       L\n## 2               1.4       L\n## 3               1.3       L\n## 4               1.5       L\n## 5               1.4       L\n## 6               1.7       M\n\n\nsum <- 0\ni <- 1\nwhile (i <= 100) {\n    sum <- sum + i      # sum에 i 값을 누적\n    i <- i + 1          # i 값을 1 증가시킴\n    print(c(sum, i))\n}\n## [1] 1 2\n## [1] 3 3\n## [1] 6 4\n## [1] 10  5\n## [1] 15  6\n## [1] 21  7\n## [1] 28  8\n## [1] 36  9\n## [1] 45 10\n## [1] 55 11\n## [1] 66 12\n## [1] 78 13\n## [1] 91 14\n## [1] 105  15\n## [1] 120  16\n## [1] 136  17\n## [1] 153  18\n## [1] 171  19\n## [1] 190  20\n## [1] 210  21\n## [1] 231  22\n## [1] 253  23\n## [1] 276  24\n## [1] 300  25\n## [1] 325  26\n## [1] 351  27\n## [1] 378  28\n## [1] 406  29\n## [1] 435  30\n## [1] 465  31\n## [1] 496  32\n## [1] 528  33\n## [1] 561  34\n## [1] 595  35\n## [1] 630  36\n## [1] 666  37\n## [1] 703  38\n## [1] 741  39\n## [1] 780  40\n## [1] 820  41\n## [1] 861  42\n## [1] 903  43\n## [1] 946  44\n## [1] 990  45\n## [1] 1035   46\n## [1] 1081   47\n## [1] 1128   48\n## [1] 1176   49\n## [1] 1225   50\n## [1] 1275   51\n## [1] 1326   52\n## [1] 1378   53\n## [1] 1431   54\n## [1] 1485   55\n## [1] 1540   56\n## [1] 1596   57\n## [1] 1653   58\n## [1] 1711   59\n## [1] 1770   60\n## [1] 1830   61\n## [1] 1891   62\n## [1] 1953   63\n## [1] 2016   64\n## [1] 2080   65\n## [1] 2145   66\n## [1] 2211   67\n## [1] 2278   68\n## [1] 2346   69\n## [1] 2415   70\n## [1] 2485   71\n## [1] 2556   72\n## [1] 2628   73\n## [1] 2701   74\n## [1] 2775   75\n## [1] 2850   76\n## [1] 2926   77\n## [1] 3003   78\n## [1] 3081   79\n## [1] 3160   80\n## [1] 3240   81\n## [1] 3321   82\n## [1] 3403   83\n## [1] 3486   84\n## [1] 3570   85\n## [1] 3655   86\n## [1] 3741   87\n## [1] 3828   88\n## [1] 3916   89\n## [1] 4005   90\n## [1] 4095   91\n## [1] 4186   92\n## [1] 4278   93\n## [1] 4371   94\n## [1] 4465   95\n## [1] 4560   96\n## [1] 4656   97\n## [1] 4753   98\n## [1] 4851   99\n## [1] 4950  100\n## [1] 5050  101\nprint(sum)\n## [1] 5050\n\n#---------------------------------------#\n# 오류 없이 계속 실행됨\n# sum <- 0\n# i <- 1\n# while(i >= 1) {\n#   sum <- sum + i # sum에 i 값을 누적\n#   i <- i + 1 # i 값을 1 증가시킴\n#   print(c(sum,i))\n# }\n# print(sum)\n#---------------------------------------#\n\n\nsum <- 0\nfor (i in 1:10) {\n    sum <- sum + i\n    print(c(sum, i))\n    if (i >= 5)\n        break\n}\n## [1] 1 1\n## [1] 3 2\n## [1] 6 3\n## [1] 10  4\n## [1] 15  5\nsum\n## [1] 15\n\n\nsum <- 0\nfor (i in 1:10) {\n    if (i %% 2 == 0)\n        next # %% = 나머지\n    sum <- sum + i\n    print(c(sum, i))\n}\n## [1] 1 1\n## [1] 4 3\n## [1] 9 5\n## [1] 16  7\n## [1] 25  9\nsum\n## [1] 25\n\n\napply(iris[, 1:4], 1, mean) # row 방향으로 함수 적용\n##   [1] 2.550 2.375 2.350 2.350 2.550 2.850 2.425 2.525 2.225 2.400 2.700 2.500\n##  [13] 2.325 2.125 2.800 3.000 2.750 2.575 2.875 2.675 2.675 2.675 2.350 2.650\n##  [25] 2.575 2.450 2.600 2.600 2.550 2.425 2.425 2.675 2.725 2.825 2.425 2.400\n##  [37] 2.625 2.500 2.225 2.550 2.525 2.100 2.275 2.675 2.800 2.375 2.675 2.350\n##  [49] 2.675 2.475 4.075 3.900 4.100 3.275 3.850 3.575 3.975 2.900 3.850 3.300\n##  [61] 2.875 3.650 3.300 3.775 3.350 3.900 3.650 3.400 3.600 3.275 3.925 3.550\n##  [73] 3.800 3.700 3.725 3.850 3.950 4.100 3.725 3.200 3.200 3.150 3.400 3.850\n##  [85] 3.600 3.875 4.000 3.575 3.500 3.325 3.425 3.775 3.400 2.900 3.450 3.525\n##  [97] 3.525 3.675 2.925 3.475 4.525 3.875 4.525 4.150 4.375 4.825 3.400 4.575\n## [109] 4.200 4.850 4.200 4.075 4.350 3.800 4.025 4.300 4.200 5.100 4.875 3.675\n## [121] 4.525 3.825 4.800 3.925 4.450 4.550 3.900 3.950 4.225 4.400 4.550 5.025\n## [133] 4.250 3.925 3.925 4.775 4.425 4.200 3.900 4.375 4.450 4.350 3.875 4.550\n## [145] 4.550 4.300 3.925 4.175 4.325 3.950\napply(iris[, 1:4], 2, mean) # col 방향으로 함수 적용\n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n##     5.843333     3.057333     3.758000     1.199333\n\nresult <- c()\nfor (i in 1:4) {\n    iris_col <- iris[, i]\n    iris_col_mean_temp <- mean(iris_col)\n    result <- c(result, iris_col_mean_temp)\n}\nresult\n## [1] 5.843333 3.057333 3.758000 1.199333\n\n\nmymax <- function(x, y) {\n    num.max <- x\n    if (y > x) {\n        num.max <- y\n    }\n    return(num.max)\n}\n\n\nmymax(10, 15)\n## [1] 15\na <- mymax(20, 15)\nb <- mymax(31, 45)\nprint(a + b)\n## [1] 65\n\n\nmydiv <- function(x, y = 2) {\n    result <- x / y\n    return(result)\n}\n\nmydiv(x = 10, y = 3) # 매개변수 이름과 매개변수값을 쌍으로 입력\n## [1] 3.333333\nmydiv(10, 3) # 매개변수값만 입력\n## [1] 3.333333\nmydiv(10) # x에 대한 값만 입력(y 값이 생략됨)\n## [1] 5\n\n\nmyfunc <- function(x, y) {\n    val.sum <- x + y\n    val.mul <- x * y\n    return(list(sum = val.sum, mul = val.mul))\n}\n\nresult <- myfunc(5, 8)\nresult\n## $sum\n## [1] 13\n## \n## $mul\n## [1] 40\ns <- result$sum # 5, 8의 합\nm <- result$mul # 5, 8의 곱\ncat('5+8=', s, '\\n')\n## 5+8= 13\ncat('5*8=', m, '\\n')\n## 5*8= 40\n\n\ngetwd()\n## [1] \"C:/Users/Hyunsoo Kim/Desktop/senior_grade/blog/my-quarto-website\"\n# source(\"myfunc.R\") # myfunc.R 안에 있는 함수 실행\n\na <- mydiv(20, 4) # 함수 호출\nb <- mydiv(30, 4) # 함수 호출\na + b\n## [1] 12.5\nmydiv(mydiv(20, 2), 5) # 함수 호출\n## [1] 2\n\n\nscore <- c(76, 84, 69, 50, 95, 60, 82, 71, 88, 84)\nwhich(score == 69) # 성적이 69인 학생은 몇 번째에 있나\n## [1] 3\nwhich(score >= 85) # 성적이 85 이상인 학생은 몇 번째에 있나\n## [1] 5 9\n\nmax(score) # 최고 점수는 몇 점인가\n## [1] 95\nwhich.max(score) # 최고 점수는 몇 번째에 있나\n## [1] 5\nscore[which.max(score)] # 최고 점수는 몇 점인가\n## [1] 95\n\nmin(score) # 최저 점수는 몇 점인가\n## [1] 50\nwhich.min(score) # 최저 점수는 몇 번째에 있나\n## [1] 4\nscore[which.min(score)] # 최저 점수는 몇 점인가\n## [1] 50\n\n\nscore <- c(76, 84, 69, 50, 95, 60, 82, 71, 88, 84)\nidx <- which(score <= 60) # 성적이 60 이하인 값들의 인덱스\nidx\n## [1] 4 6\nscore[idx]\n## [1] 50 60\nscore[idx] <- 61 # 성적이 60 이하인 값들은 61점으로 성적 상향 조정\nscore # 상향 조정된 성적 확인\n##  [1] 76 84 69 61 95 61 82 71 88 84\n\nidx <- which(score >= 80) # 성적이 80 이상인 값들의 인덱스\nidx\n## [1]  2  5  7  9 10\nscore[idx]\n## [1] 84 95 82 88 84\nscore.high <- score[idx] # 성적이 80 이상인 값들만 추출하여 저장\nscore.high # score.high의 내용 확인\n## [1] 84 95 82 88 84\n\n\niris\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 1            5.1         3.5          1.4         0.2     setosa\n## 2            4.9         3.0          1.4         0.2     setosa\n## 3            4.7         3.2          1.3         0.2     setosa\n## 4            4.6         3.1          1.5         0.2     setosa\n## 5            5.0         3.6          1.4         0.2     setosa\n## 6            5.4         3.9          1.7         0.4     setosa\n## 7            4.6         3.4          1.4         0.3     setosa\n## 8            5.0         3.4          1.5         0.2     setosa\n## 9            4.4         2.9          1.4         0.2     setosa\n## 10           4.9         3.1          1.5         0.1     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 12           4.8         3.4          1.6         0.2     setosa\n## 13           4.8         3.0          1.4         0.1     setosa\n## 14           4.3         3.0          1.1         0.1     setosa\n## 15           5.8         4.0          1.2         0.2     setosa\n## 16           5.7         4.4          1.5         0.4     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 23           4.6         3.6          1.0         0.2     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 25           4.8         3.4          1.9         0.2     setosa\n## 26           5.0         3.0          1.6         0.2     setosa\n## 27           5.0         3.4          1.6         0.4     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 30           4.7         3.2          1.6         0.2     setosa\n## 31           4.8         3.1          1.6         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 34           5.5         4.2          1.4         0.2     setosa\n## 35           4.9         3.1          1.5         0.2     setosa\n## 36           5.0         3.2          1.2         0.2     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 38           4.9         3.6          1.4         0.1     setosa\n## 39           4.4         3.0          1.3         0.2     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 41           5.0         3.5          1.3         0.3     setosa\n## 42           4.5         2.3          1.3         0.3     setosa\n## 43           4.4         3.2          1.3         0.2     setosa\n## 44           5.0         3.5          1.6         0.6     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\n## 46           4.8         3.0          1.4         0.3     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 48           4.6         3.2          1.4         0.2     setosa\n## 49           5.3         3.7          1.5         0.2     setosa\n## 50           5.0         3.3          1.4         0.2     setosa\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 107          4.9         2.5          4.5         1.7  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 114          5.7         2.5          5.0         2.0  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 120          6.0         2.2          5.0         1.5  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 122          5.6         2.8          4.9         2.0  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 127          6.2         2.8          4.8         1.8  virginica\n## 128          6.1         3.0          4.9         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\niris$Petal.Length\n##   [1] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 1.5 1.6 1.4 1.1 1.2 1.5 1.3 1.4\n##  [19] 1.7 1.5 1.7 1.5 1.0 1.7 1.9 1.6 1.6 1.5 1.4 1.6 1.6 1.5 1.5 1.4 1.5 1.2\n##  [37] 1.3 1.4 1.3 1.5 1.3 1.3 1.3 1.6 1.9 1.4 1.6 1.4 1.5 1.4 4.7 4.5 4.9 4.0\n##  [55] 4.6 4.5 4.7 3.3 4.6 3.9 3.5 4.2 4.0 4.7 3.6 4.4 4.5 4.1 4.5 3.9 4.8 4.0\n##  [73] 4.9 4.7 4.3 4.4 4.8 5.0 4.5 3.5 3.8 3.7 3.9 5.1 4.5 4.5 4.7 4.4 4.1 4.0\n##  [91] 4.4 4.6 4.0 3.3 4.2 4.2 4.2 4.3 3.0 4.1 6.0 5.1 5.9 5.6 5.8 6.6 4.5 6.3\n## [109] 5.8 6.1 5.1 5.3 5.5 5.0 5.1 5.3 5.5 6.7 6.9 5.0 5.7 4.9 6.7 4.9 5.7 6.0\n## [127] 4.8 4.9 5.6 5.8 6.1 6.4 5.6 5.1 5.6 6.1 5.6 5.5 4.8 5.4 5.6 5.1 5.1 5.9\n## [145] 5.7 5.2 5.0 5.2 5.4 5.1\niris$Petal.Length > 5.0\n##   [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n##  [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [97] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n## [109]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n## [121]  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n## [133]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n## [145]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\nwhich(iris$Petal.Length > 5.0)\n##  [1]  84 101 102 103 104 105 106 108 109 110 111 112 113 115 116 117 118 119 121\n## [20] 123 125 126 129 130 131 132 133 134 135 136 137 138 140 141 142 143 144 145\n## [39] 146 148 149 150\n\niris$Petal.Length[iris$Petal.Length > 5.0]\n##  [1] 5.1 6.0 5.1 5.9 5.6 5.8 6.6 6.3 5.8 6.1 5.1 5.3 5.5 5.1 5.3 5.5 6.7 6.9 5.7\n## [20] 6.7 5.7 6.0 5.6 5.8 6.1 6.4 5.6 5.1 5.6 6.1 5.6 5.5 5.4 5.6 5.1 5.1 5.9 5.7\n## [39] 5.2 5.2 5.4 5.1\n\nidx <- which(iris$Petal.Length > 5.0) # 꽃잎의 길이가 5.0 이상인 값들의 인덱스\nidx\n##  [1]  84 101 102 103 104 105 106 108 109 110 111 112 113 115 116 117 118 119 121\n## [20] 123 125 126 129 130 131 132 133 134 135 136 137 138 140 141 142 143 144 145\n## [39] 146 148 149 150\niris.big <- iris[idx, ] # 인덱스에 해당하는 값만 추출하여 저장\niris.big\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\n\n\n# 1~4열의 값 중 5보다 큰 값의 행과 열의 위치\nwhich(iris[, 1:4] > 5.0)\n##   [1]   1   6  11  15  16  17  18  19  20  21  22  24  28  29  32  33  34  37\n##  [19]  40  45  47  49  51  52  53  54  55  56  57  59  60  62  63  64  65  66\n##  [37]  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84\n##  [55]  85  86  87  88  89  90  91  92  93  95  96  97  98  99 100 101 102 103\n##  [73] 104 105 106 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122\n##  [91] 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140\n## [109] 141 142 143 144 145 146 147 148 149 150 384 401 402 403 404 405 406 408\n## [127] 409 410 411 412 413 415 416 417 418 419 421 423 425 426 429 430 431 432\n## [145] 433 434 435 436 437 438 440 441 442 443 444 445 446 448 449 450\nwhich(iris[, 1:4] > 5.0, arr.ind = TRUE) # arr.ind = TRUE : 조건에 맞는 인덱스까지 반환\n##        row col\n##   [1,]   1   1\n##   [2,]   6   1\n##   [3,]  11   1\n##   [4,]  15   1\n##   [5,]  16   1\n##   [6,]  17   1\n##   [7,]  18   1\n##   [8,]  19   1\n##   [9,]  20   1\n##  [10,]  21   1\n##  [11,]  22   1\n##  [12,]  24   1\n##  [13,]  28   1\n##  [14,]  29   1\n##  [15,]  32   1\n##  [16,]  33   1\n##  [17,]  34   1\n##  [18,]  37   1\n##  [19,]  40   1\n##  [20,]  45   1\n##  [21,]  47   1\n##  [22,]  49   1\n##  [23,]  51   1\n##  [24,]  52   1\n##  [25,]  53   1\n##  [26,]  54   1\n##  [27,]  55   1\n##  [28,]  56   1\n##  [29,]  57   1\n##  [30,]  59   1\n##  [31,]  60   1\n##  [32,]  62   1\n##  [33,]  63   1\n##  [34,]  64   1\n##  [35,]  65   1\n##  [36,]  66   1\n##  [37,]  67   1\n##  [38,]  68   1\n##  [39,]  69   1\n##  [40,]  70   1\n##  [41,]  71   1\n##  [42,]  72   1\n##  [43,]  73   1\n##  [44,]  74   1\n##  [45,]  75   1\n##  [46,]  76   1\n##  [47,]  77   1\n##  [48,]  78   1\n##  [49,]  79   1\n##  [50,]  80   1\n##  [51,]  81   1\n##  [52,]  82   1\n##  [53,]  83   1\n##  [54,]  84   1\n##  [55,]  85   1\n##  [56,]  86   1\n##  [57,]  87   1\n##  [58,]  88   1\n##  [59,]  89   1\n##  [60,]  90   1\n##  [61,]  91   1\n##  [62,]  92   1\n##  [63,]  93   1\n##  [64,]  95   1\n##  [65,]  96   1\n##  [66,]  97   1\n##  [67,]  98   1\n##  [68,]  99   1\n##  [69,] 100   1\n##  [70,] 101   1\n##  [71,] 102   1\n##  [72,] 103   1\n##  [73,] 104   1\n##  [74,] 105   1\n##  [75,] 106   1\n##  [76,] 108   1\n##  [77,] 109   1\n##  [78,] 110   1\n##  [79,] 111   1\n##  [80,] 112   1\n##  [81,] 113   1\n##  [82,] 114   1\n##  [83,] 115   1\n##  [84,] 116   1\n##  [85,] 117   1\n##  [86,] 118   1\n##  [87,] 119   1\n##  [88,] 120   1\n##  [89,] 121   1\n##  [90,] 122   1\n##  [91,] 123   1\n##  [92,] 124   1\n##  [93,] 125   1\n##  [94,] 126   1\n##  [95,] 127   1\n##  [96,] 128   1\n##  [97,] 129   1\n##  [98,] 130   1\n##  [99,] 131   1\n## [100,] 132   1\n## [101,] 133   1\n## [102,] 134   1\n## [103,] 135   1\n## [104,] 136   1\n## [105,] 137   1\n## [106,] 138   1\n## [107,] 139   1\n## [108,] 140   1\n## [109,] 141   1\n## [110,] 142   1\n## [111,] 143   1\n## [112,] 144   1\n## [113,] 145   1\n## [114,] 146   1\n## [115,] 147   1\n## [116,] 148   1\n## [117,] 149   1\n## [118,] 150   1\n## [119,]  84   3\n## [120,] 101   3\n## [121,] 102   3\n## [122,] 103   3\n## [123,] 104   3\n## [124,] 105   3\n## [125,] 106   3\n## [126,] 108   3\n## [127,] 109   3\n## [128,] 110   3\n## [129,] 111   3\n## [130,] 112   3\n## [131,] 113   3\n## [132,] 115   3\n## [133,] 116   3\n## [134,] 117   3\n## [135,] 118   3\n## [136,] 119   3\n## [137,] 121   3\n## [138,] 123   3\n## [139,] 125   3\n## [140,] 126   3\n## [141,] 129   3\n## [142,] 130   3\n## [143,] 131   3\n## [144,] 132   3\n## [145,] 133   3\n## [146,] 134   3\n## [147,] 135   3\n## [148,] 136   3\n## [149,] 137   3\n## [150,] 138   3\n## [151,] 140   3\n## [152,] 141   3\n## [153,] 142   3\n## [154,] 143   3\n## [155,] 144   3\n## [156,] 145   3\n## [157,] 146   3\n## [158,] 148   3\n## [159,] 149   3\n## [160,] 150   3\n\nidx <- which(iris[, 1:4] > 5.0, arr.ind = TRUE)\niris[idx[, 1], ]\n##       Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 1              5.1         3.5          1.4         0.2     setosa\n## 6              5.4         3.9          1.7         0.4     setosa\n## 11             5.4         3.7          1.5         0.2     setosa\n## 15             5.8         4.0          1.2         0.2     setosa\n## 16             5.7         4.4          1.5         0.4     setosa\n## 17             5.4         3.9          1.3         0.4     setosa\n## 18             5.1         3.5          1.4         0.3     setosa\n## 19             5.7         3.8          1.7         0.3     setosa\n## 20             5.1         3.8          1.5         0.3     setosa\n## 21             5.4         3.4          1.7         0.2     setosa\n## 22             5.1         3.7          1.5         0.4     setosa\n## 24             5.1         3.3          1.7         0.5     setosa\n## 28             5.2         3.5          1.5         0.2     setosa\n## 29             5.2         3.4          1.4         0.2     setosa\n## 32             5.4         3.4          1.5         0.4     setosa\n## 33             5.2         4.1          1.5         0.1     setosa\n## 34             5.5         4.2          1.4         0.2     setosa\n## 37             5.5         3.5          1.3         0.2     setosa\n## 40             5.1         3.4          1.5         0.2     setosa\n## 45             5.1         3.8          1.9         0.4     setosa\n## 47             5.1         3.8          1.6         0.2     setosa\n## 49             5.3         3.7          1.5         0.2     setosa\n## 51             7.0         3.2          4.7         1.4 versicolor\n## 52             6.4         3.2          4.5         1.5 versicolor\n## 53             6.9         3.1          4.9         1.5 versicolor\n## 54             5.5         2.3          4.0         1.3 versicolor\n## 55             6.5         2.8          4.6         1.5 versicolor\n## 56             5.7         2.8          4.5         1.3 versicolor\n## 57             6.3         3.3          4.7         1.6 versicolor\n## 59             6.6         2.9          4.6         1.3 versicolor\n## 60             5.2         2.7          3.9         1.4 versicolor\n## 62             5.9         3.0          4.2         1.5 versicolor\n## 63             6.0         2.2          4.0         1.0 versicolor\n## 64             6.1         2.9          4.7         1.4 versicolor\n## 65             5.6         2.9          3.6         1.3 versicolor\n## 66             6.7         3.1          4.4         1.4 versicolor\n## 67             5.6         3.0          4.5         1.5 versicolor\n## 68             5.8         2.7          4.1         1.0 versicolor\n## 69             6.2         2.2          4.5         1.5 versicolor\n## 70             5.6         2.5          3.9         1.1 versicolor\n## 71             5.9         3.2          4.8         1.8 versicolor\n## 72             6.1         2.8          4.0         1.3 versicolor\n## 73             6.3         2.5          4.9         1.5 versicolor\n## 74             6.1         2.8          4.7         1.2 versicolor\n## 75             6.4         2.9          4.3         1.3 versicolor\n## 76             6.6         3.0          4.4         1.4 versicolor\n## 77             6.8         2.8          4.8         1.4 versicolor\n## 78             6.7         3.0          5.0         1.7 versicolor\n## 79             6.0         2.9          4.5         1.5 versicolor\n## 80             5.7         2.6          3.5         1.0 versicolor\n## 81             5.5         2.4          3.8         1.1 versicolor\n## 82             5.5         2.4          3.7         1.0 versicolor\n## 83             5.8         2.7          3.9         1.2 versicolor\n## 84             6.0         2.7          5.1         1.6 versicolor\n## 85             5.4         3.0          4.5         1.5 versicolor\n## 86             6.0         3.4          4.5         1.6 versicolor\n## 87             6.7         3.1          4.7         1.5 versicolor\n## 88             6.3         2.3          4.4         1.3 versicolor\n## 89             5.6         3.0          4.1         1.3 versicolor\n## 90             5.5         2.5          4.0         1.3 versicolor\n## 91             5.5         2.6          4.4         1.2 versicolor\n## 92             6.1         3.0          4.6         1.4 versicolor\n## 93             5.8         2.6          4.0         1.2 versicolor\n## 95             5.6         2.7          4.2         1.3 versicolor\n## 96             5.7         3.0          4.2         1.2 versicolor\n## 97             5.7         2.9          4.2         1.3 versicolor\n## 98             6.2         2.9          4.3         1.3 versicolor\n## 99             5.1         2.5          3.0         1.1 versicolor\n## 100            5.7         2.8          4.1         1.3 versicolor\n## 101            6.3         3.3          6.0         2.5  virginica\n## 102            5.8         2.7          5.1         1.9  virginica\n## 103            7.1         3.0          5.9         2.1  virginica\n## 104            6.3         2.9          5.6         1.8  virginica\n## 105            6.5         3.0          5.8         2.2  virginica\n## 106            7.6         3.0          6.6         2.1  virginica\n## 108            7.3         2.9          6.3         1.8  virginica\n## 109            6.7         2.5          5.8         1.8  virginica\n## 110            7.2         3.6          6.1         2.5  virginica\n## 111            6.5         3.2          5.1         2.0  virginica\n## 112            6.4         2.7          5.3         1.9  virginica\n## 113            6.8         3.0          5.5         2.1  virginica\n## 114            5.7         2.5          5.0         2.0  virginica\n## 115            5.8         2.8          5.1         2.4  virginica\n## 116            6.4         3.2          5.3         2.3  virginica\n## 117            6.5         3.0          5.5         1.8  virginica\n## 118            7.7         3.8          6.7         2.2  virginica\n## 119            7.7         2.6          6.9         2.3  virginica\n## 120            6.0         2.2          5.0         1.5  virginica\n## 121            6.9         3.2          5.7         2.3  virginica\n## 122            5.6         2.8          4.9         2.0  virginica\n## 123            7.7         2.8          6.7         2.0  virginica\n## 124            6.3         2.7          4.9         1.8  virginica\n## 125            6.7         3.3          5.7         2.1  virginica\n## 126            7.2         3.2          6.0         1.8  virginica\n## 127            6.2         2.8          4.8         1.8  virginica\n## 128            6.1         3.0          4.9         1.8  virginica\n## 129            6.4         2.8          5.6         2.1  virginica\n## 130            7.2         3.0          5.8         1.6  virginica\n## 131            7.4         2.8          6.1         1.9  virginica\n## 132            7.9         3.8          6.4         2.0  virginica\n## 133            6.4         2.8          5.6         2.2  virginica\n## 134            6.3         2.8          5.1         1.5  virginica\n## 135            6.1         2.6          5.6         1.4  virginica\n## 136            7.7         3.0          6.1         2.3  virginica\n## 137            6.3         3.4          5.6         2.4  virginica\n## 138            6.4         3.1          5.5         1.8  virginica\n## 139            6.0         3.0          4.8         1.8  virginica\n## 140            6.9         3.1          5.4         2.1  virginica\n## 141            6.7         3.1          5.6         2.4  virginica\n## 142            6.9         3.1          5.1         2.3  virginica\n## 143            5.8         2.7          5.1         1.9  virginica\n## 144            6.8         3.2          5.9         2.3  virginica\n## 145            6.7         3.3          5.7         2.5  virginica\n## 146            6.7         3.0          5.2         2.3  virginica\n## 147            6.3         2.5          5.0         1.9  virginica\n## 148            6.5         3.0          5.2         2.0  virginica\n## 149            6.2         3.4          5.4         2.3  virginica\n## 150            5.9         3.0          5.1         1.8  virginica\n## 84.1           6.0         2.7          5.1         1.6 versicolor\n## 101.1          6.3         3.3          6.0         2.5  virginica\n## 102.1          5.8         2.7          5.1         1.9  virginica\n## 103.1          7.1         3.0          5.9         2.1  virginica\n## 104.1          6.3         2.9          5.6         1.8  virginica\n## 105.1          6.5         3.0          5.8         2.2  virginica\n## 106.1          7.6         3.0          6.6         2.1  virginica\n## 108.1          7.3         2.9          6.3         1.8  virginica\n## 109.1          6.7         2.5          5.8         1.8  virginica\n## 110.1          7.2         3.6          6.1         2.5  virginica\n## 111.1          6.5         3.2          5.1         2.0  virginica\n## 112.1          6.4         2.7          5.3         1.9  virginica\n## 113.1          6.8         3.0          5.5         2.1  virginica\n## 115.1          5.8         2.8          5.1         2.4  virginica\n## 116.1          6.4         3.2          5.3         2.3  virginica\n## 117.1          6.5         3.0          5.5         1.8  virginica\n## 118.1          7.7         3.8          6.7         2.2  virginica\n## 119.1          7.7         2.6          6.9         2.3  virginica\n## 121.1          6.9         3.2          5.7         2.3  virginica\n## 123.1          7.7         2.8          6.7         2.0  virginica\n## 125.1          6.7         3.3          5.7         2.1  virginica\n## 126.1          7.2         3.2          6.0         1.8  virginica\n## 129.1          6.4         2.8          5.6         2.1  virginica\n## 130.1          7.2         3.0          5.8         1.6  virginica\n## 131.1          7.4         2.8          6.1         1.9  virginica\n## 132.1          7.9         3.8          6.4         2.0  virginica\n## 133.1          6.4         2.8          5.6         2.2  virginica\n## 134.1          6.3         2.8          5.1         1.5  virginica\n## 135.1          6.1         2.6          5.6         1.4  virginica\n## 136.1          7.7         3.0          6.1         2.3  virginica\n## 137.1          6.3         3.4          5.6         2.4  virginica\n## 138.1          6.4         3.1          5.5         1.8  virginica\n## 140.1          6.9         3.1          5.4         2.1  virginica\n## 141.1          6.7         3.1          5.6         2.4  virginica\n## 142.1          6.9         3.1          5.1         2.3  virginica\n## 143.1          5.8         2.7          5.1         1.9  virginica\n## 144.1          6.8         3.2          5.9         2.3  virginica\n## 145.1          6.7         3.3          5.7         2.5  virginica\n## 146.1          6.7         3.0          5.2         2.3  virginica\n## 148.1          6.5         3.0          5.2         2.0  virginica\n## 149.1          6.2         3.4          5.4         2.3  virginica\n## 150.1          5.9         3.0          5.1         1.8  virginica\n\niris[, 1:4][idx]\n##   [1] 5.1 5.4 5.4 5.8 5.7 5.4 5.1 5.7 5.1 5.4 5.1 5.1 5.2 5.2 5.4 5.2 5.5 5.5\n##  [19] 5.1 5.1 5.1 5.3 7.0 6.4 6.9 5.5 6.5 5.7 6.3 6.6 5.2 5.9 6.0 6.1 5.6 6.7\n##  [37] 5.6 5.8 6.2 5.6 5.9 6.1 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7 5.5 5.5 5.8 6.0\n##  [55] 5.4 6.0 6.7 6.3 5.6 5.5 5.5 6.1 5.8 5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 7.1\n##  [73] 6.3 6.5 7.6 7.3 6.7 7.2 6.5 6.4 6.8 5.7 5.8 6.4 6.5 7.7 7.7 6.0 6.9 5.6\n##  [91] 7.7 6.3 6.7 7.2 6.2 6.1 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 6.3 6.4 6.0 6.9\n## [109] 6.7 6.9 5.8 6.8 6.7 6.7 6.3 6.5 6.2 5.9 5.1 6.0 5.1 5.9 5.6 5.8 6.6 6.3\n## [127] 5.8 6.1 5.1 5.3 5.5 5.1 5.3 5.5 6.7 6.9 5.7 6.7 5.7 6.0 5.6 5.8 6.1 6.4\n## [145] 5.6 5.1 5.6 6.1 5.6 5.5 5.4 5.6 5.1 5.1 5.9 5.7 5.2 5.2 5.4 5.1\n\n\n\n\n\nfavorite <- c('WINTER', 'SUMMER', 'SPRING', 'SUMMER', 'SUMMER',\n              'FALL', 'FALL', 'SUMMER', 'SPRING', 'SPRING')\nfavorite # favorite의 내용 출력\n##  [1] \"WINTER\" \"SUMMER\" \"SPRING\" \"SUMMER\" \"SUMMER\" \"FALL\"   \"FALL\"   \"SUMMER\"\n##  [9] \"SPRING\" \"SPRING\"\ntable(favorite) # 도수분포표 계산\n## favorite\n##   FALL SPRING SUMMER WINTER \n##      2      3      4      1\nlength(favorite)\n## [1] 10\ntable(favorite) / length(favorite) # 비율 출력\n## favorite\n##   FALL SPRING SUMMER WINTER \n##    0.2    0.3    0.4    0.1\n\n\nds <- table(favorite)\nds\n## favorite\n##   FALL SPRING SUMMER WINTER \n##      2      3      4      1\nbarplot(ds, main = 'favorite season')\n\n\n\n\n\nds <- table(favorite)\nds\n## favorite\n##   FALL SPRING SUMMER WINTER \n##      2      3      4      1\npie(ds, main = 'favorite season')\n\n\n\n\n\nfavorite.color <- c(2, 3, 2, 1, 1, 2, 2, 1, 3, 2, 1, 3, 2, 1, 2)\nds <- table(favorite.color)\nds\n## favorite.color\n## 1 2 3 \n## 5 7 3\nbarplot(ds, main = 'favorite color')\n\n\n\ncolors <- c('green', 'red', 'blue')\nnames(ds) <- colors # 자료값 1, 2, 3을 green, red, blue로 변경\nds\n## green   red  blue \n##     5     7     3\nbarplot(ds, main = 'favorite color', col = colors) # 색 지정 막대그래프\n\n\n\nbarplot(ds, main = 'favorite color', col = c('green', 'red', 'blue'))\npie(ds, main = 'favorite color', col = colors) # 색 지정 원그래프\n\n\n\n\n\nweight <- c(60, 62, 64, 65, 68, 69)\nweight.heavy <- c(weight, 120)\nweight\n## [1] 60 62 64 65 68 69\nweight.heavy\n## [1]  60  62  64  65  68  69 120\n\nmean(weight) # 평균\n## [1] 64.66667\nmean(weight.heavy) # 평균\n## [1] 72.57143\n\nmedian(weight) # 중앙값\n## [1] 64.5\nmedian(weight.heavy) # 중앙값\n## [1] 65\n\nmean(weight, trim = 0.2) # 절사평균(상하위 20% 제외)\n## [1] 64.75\nmean(weight.heavy, trim = 0.2) # 절사평균(상하위 20% 제외)\n## [1] 65.6\n\n\nmydata <- c(60, 62, 64, 65, 68, 69, 120)\nquantile(mydata)\n##    0%   25%   50%   75%  100% \n##  60.0  63.0  65.0  68.5 120.0\nquantile(mydata, (0:10) / 10) # 10% 단위로 구간을 나누어 계산\n##    0%   10%   20%   30%   40%   50%   60%   70%   80%   90%  100% \n##  60.0  61.2  62.4  63.6  64.4  65.0  66.8  68.2  68.8  89.4 120.0\nsummary(mydata) # 최소값, 중앙값, 평균값, 3분위 값, 최대값\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   60.00   63.00   65.00   72.57   68.50  120.00\n\nmydata <- 0:1000\nquantile(mydata)\n##   0%  25%  50%  75% 100% \n##    0  250  500  750 1000\nquantile(mydata, (0:10) / 10)\n##   0%  10%  20%  30%  40%  50%  60%  70%  80%  90% 100% \n##    0  100  200  300  400  500  600  700  800  900 1000\nsummary(mydata)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##       0     250     500     500     750    1000\n?quantile\n## starting httpd help server ... done\n\n\nmydata <- c(60, 62, 64, 65, 68, 69, 120)\nvar(mydata) # 분산\n## [1] 447.2857\nsd(mydata) # 표준편차\n## [1] 21.14913\nrange(mydata) # 값의 범위\n## [1]  60 120\ndiff(range(mydata)) # 최대값, 최소값의 차이\n## [1] 60\n\n\ndist <- cars[, 2] # 자동차 제동거리\nhist(dist,                            # 자료(data)\n     main = \"Histogram for 제동거리\", # 제목\n     xlab = \"제동거리\",               # x축 레이블\n     ylab = \"빈도수\",                 # y축 레이블\n     border = \"blue\",                 # 막대 테두리색\n     col = rainbow(10),               # 막대 색\n     las = 2,                         # x축 글씨 방향(0~3)\n     breaks = seq(0, 120, 10))        # 막대 개수 조절\n\n\n\n\n\ndist <- cars[,2] # 자동차 제동거리(단위: 피트(ft))\nboxplot(dist, main = \"자동차 제동거리\") # ★★★★★\n\n\n\n\n\nboxplot.stats(dist)\n## $stats\n## [1]  2 26 36 56 93\n## \n## $n\n## [1] 50\n## \n## $conf\n## [1] 29.29663 42.70337\n## \n## $out\n## [1] 120\nboxplot.stats(dist)$stats\n## [1]  2 26 36 56 93\nboxplot.stats(dist)$stats[4]\n## [1] 56\n\n\nboxplot(Petal.Length ~ Species, data = iris, main = \"품종별 꽃잎의 길이\")\n\n\n\n\npar(mfrow = c(1, 3)) # 1*3 가상화면 분할\n\nbarplot(\n    table(mtcars$carb),\n    main = \"Barplot of Carburetors\",\n    xlab = \"#of carburetors\",\n    ylab = \"frequency\",\n    col = \"blue\"\n)\n\nbarplot(\n    table(mtcars$cyl),\n    main = \"Barplot of Cylender\",\n    xlab = \"#of cylender\",\n    ylab = \"frequency\",\n    col = \"red\"\n)\n\nbarplot(\n    table(mtcars$gear),\n    main = \"Barplot of Grar\",\n    xlab = \"#of gears\",\n    ylab = \"frequency\",\n    col = \"green\"\n)\n\n\n\n\npar(mfrow = c(1, 1)) # 가상화면 분할 해제\n\n\n\n\n\nwt <- mtcars$wt                 # 중량 자료\nmpg <- mtcars$mpg               # 연비 자료\nplot(wt, mpg,                   # 2개 변수(x축, y축)\n     main = \"중량-연비 그래프\", # 제목\n     xlab = \"중량\",             # x축 레이블\n     ylab = \"연비(MPG)\",        # y축 레이블\n     col = \"red\",               # point의 color\n     pch = 11)                  # point의 종류\n\n\n\n\n\nvars <- c(\"mpg\", \"disp\", \"drat\", \"wt\") # 대상 변수(연비, 배기량, 후방차측 비율, 중량)\ntarget <- mtcars[, vars]\nhead(target)\n##                    mpg disp drat    wt\n## Mazda RX4         21.0  160 3.90 2.620\n## Mazda RX4 Wag     21.0  160 3.90 2.875\n## Datsun 710        22.8  108 3.85 2.320\n## Hornet 4 Drive    21.4  258 3.08 3.215\n## Hornet Sportabout 18.7  360 3.15 3.440\n## Valiant           18.1  225 2.76 3.460\npairs(target, main = \"Multi Plots\")    # 대상 데이터\n\n\n\n\n\niris.2 <- iris[, 3:4]              # 데이터 준비\npoint <- as.numeric(iris$Species)  # 점의 모양\npoint                              # point 내용 출력\n##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n##  [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n##  [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3\n## [112] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n## [149] 3 3\ncolor <- c(\"red\", \"green\", \"blue\") # 점의 컬러\nplot(iris.2,\n     main = \"Iris plot\",\n     pch = c(point),\n     col = color[point])\n\n\n\n\n\nbeers = c(5, 2, 9, 8, 3, 7, 3, 5, 3, 5) # 자료 입력\nbal <- c(0.1, 0.03, 0.19, 0.12, 0.04, 0.0095, 0.07, 0.06, 0.02, 0.05)\ntbl <- data.frame(beers, bal)           # 데이터프레임 생성\ntbl\n##    beers    bal\n## 1      5 0.1000\n## 2      2 0.0300\n## 3      9 0.1900\n## 4      8 0.1200\n## 5      3 0.0400\n## 6      7 0.0095\n## 7      3 0.0700\n## 8      5 0.0600\n## 9      3 0.0200\n## 10     5 0.0500\nplot(bal ~ beers, data = tbl)           # 산점도 plot(beers, bal)\nres <- lm(bal ~ beers, data = tbl)      # 회귀식 도출\nabline(res)                             # 회귀선 그리기\n\n\n\ncor(beers, bal)                         # 상관계수 계산\n## [1] 0.6797025\n\n\ncor(iris[, 1:4]) # 4개 변수 간 상관성 분석\n##              Sepal.Length Sepal.Width Petal.Length Petal.Width\n## Sepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411\n## Sepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259\n## Petal.Length    0.8717538  -0.4284401    1.0000000   0.9628654\n## Petal.Width     0.8179411  -0.3661259    0.9628654   1.0000000\n\n\nmonth = 1:12 # 자료 입력\nlate = c(5, 8, 7, 9, 4, 6, 12, 13, 8, 6, 6, 4) # 자료 입력\nplot(month,                # x data\n     late,                 # y data\n     main = \"지각생 통계\", # 제목\n     type = \"l\",           # 그래프의 종류 선택(알파벳)\n     lty = 1,              # 선의 종류(line type) 선택\n     lwd = 1,              # 선의 굵기 선택\n     xlab = \"Month\",       # x축 레이블\n     ylab = \"Late cnt\")    # y축 레이블\n\n\n\n\n\nmonth = 1:12\nlate1 = c(5, 8, 7, 9, 4, 6, 12, 13, 8, 6, 6, 4)\nlate2 = c(4, 6, 5, 8, 7, 8, 10, 11, 6, 5, 7, 3)\nplot(month,                  # x data\n     late1,                  # y data\n     main = \"Late Students\",\n     type = \"b\",             # 그래프의 종류 선택(알파벳)\n     lty = 1,                # 선의 종류(line type) 선택\n     col = \"red\",            # 선의 색 선택\n     xlab = \"Month\",         # x축 레이블\n     ylab = \"Late cnt\",      # y축 레이블\n     ylim = c(1, 15))        # y축 값의 (하한, 상한)\n\nlines(month,                 # x data\n      late2,                 # y data\n      type = \"b\",            # 선의 종류(line type) 선택\n      col = \"blue\")          # 선의 색 선택\n\n\n\n\n\n## (1) 분석 대상 데이터셋 준비\n# install.packages(\"mlbench\")\nlibrary(mlbench)\ndata(\"BostonHousing\")\nmyds <- BostonHousing[, c(\"crim\", \"rm\", \"dis\", \"tax\", \"medv\")]\n\n## (2) grp 변수 추가 ★★★★★\ngrp <- c()\nfor (i in 1:nrow(myds)) {\n    # myds$medv 값에 따라 그룹 분류\n    if (myds$medv[i] >= 25.0) {\n        grp[i] <- \"H\"\n    } else if (myds$medv[i] <= 17.0) {\n        grp[i] <- \"L\"\n    } else {\n        grp[i] <- \"M\"\n    }\n}\ngrp <- factor(grp) # 문자 벡터를 팩터 타입으로 변경\ngrp <- factor(grp, levels = c(\"H\", \"M\", \"L\")) # 레벨의 순서를 H, L, M -> H, M, L\n\nmyds <- data.frame(myds, grp) # myds에 grp 열 추가\n\n## (3) 데이터셋의 형태와 기본적인 내용 파악\nstr(myds)\n## 'data.frame':    506 obs. of  6 variables:\n##  $ crim: num  0.00632 0.02731 0.02729 0.03237 0.06905 ...\n##  $ rm  : num  6.58 6.42 7.18 7 7.15 ...\n##  $ dis : num  4.09 4.97 4.97 6.06 6.06 ...\n##  $ tax : num  296 242 242 222 222 222 311 311 311 311 ...\n##  $ medv: num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...\n##  $ grp : Factor w/ 3 levels \"H\",\"M\",\"L\": 2 2 1 1 1 1 2 1 3 2 ...\nhead(myds)\n##      crim    rm    dis tax medv grp\n## 1 0.00632 6.575 4.0900 296 24.0   M\n## 2 0.02731 6.421 4.9671 242 21.6   M\n## 3 0.02729 7.185 4.9671 242 34.7   H\n## 4 0.03237 6.998 6.0622 222 33.4   H\n## 5 0.06905 7.147 6.0622 222 36.2   H\n## 6 0.02985 6.430 6.0622 222 28.7   H\ntable(myds$grp) # 주택 가격 그룹별 분포\n## \n##   H   M   L \n## 132 247 127\n\n## (4) 히스토그램에 의한 관측값의 분포 확인\npar(mfrow = c(2, 3)) # 2*3 가상화면 분할\nfor (i in 1:5) {\n    hist(myds[, i], main = colnames(myds)[i], col = \"yellow\")\n}\npar(mfrow = c(1, 1)) # 2*3 가상화면 분할 해제\n\n\n\n\n## (5) 상자그림에 의한 관측값의 분포 확인\npar(mfrow = c(2, 3)) # 2*3 가상화면 분할\nfor (i in 1:5) {\n    boxplot(myds[, i], main = colnames(myds)[i])\n}\npar(mfrow = c(1, 1)) # 2*3 가상화면 분할 해제\n\n\n\n\n## (6) 그룹별 관측값 분포의 확인\nboxplot(myds$crim ~ myds$grp, main = \"1인당 범죄율\")\n\n\n\nboxplot(myds$rm ~ myds$grp, main = \"방의 개수\")\n\n\n\nboxplot(myds$dis ~ myds$grp, main = \"직업 센터까지의 거리\")\n\n\n\nboxplot(myds$tax ~ myds$grp, main = \"재산세율\")\n\n\n\n\n## (7) 다중 산점도를 통한 변수 간 상관 관계의 확인\npairs(myds[, -6]) # 6번째 열 제거(grp)\npairs(myds[, 1:5])\n\n\n\n\n## (8) 그룹 정보를 포함한 변수 간 상관 관계의 확인\npoint <- as.integer(myds$grp) # 점의 모양 지정\ncolor <- c(\"red\", \"green\", \"blue\") # 점의 색 지정\npairs(myds[, -6], pch = point, col = color[point])\n\n\n\n\n## (9) 변수 간 상관계수의 확인\ncor(myds[, -6])\n##            crim         rm        dis        tax       medv\n## crim  1.0000000 -0.2192467 -0.3796701  0.5827643 -0.3883046\n## rm   -0.2192467  1.0000000  0.2052462 -0.2920478  0.6953599\n## dis  -0.3796701  0.2052462  1.0000000 -0.5344316  0.2499287\n## tax   0.5827643 -0.2920478 -0.5344316  1.0000000 -0.4685359\n## medv -0.3883046  0.6953599  0.2499287 -0.4685359  1.0000000\ncor(myds[1:5])\n##            crim         rm        dis        tax       medv\n## crim  1.0000000 -0.2192467 -0.3796701  0.5827643 -0.3883046\n## rm   -0.2192467  1.0000000  0.2052462 -0.2920478  0.6953599\n## dis  -0.3796701  0.2052462  1.0000000 -0.5344316  0.2499287\n## tax   0.5827643 -0.2920478 -0.5344316  1.0000000 -0.4685359\n## medv -0.3883046  0.6953599  0.2499287 -0.4685359  1.0000000\n\n\n\n\n\nz <- c(1, 2, 3, NA, 5, NA, 8)   # 결측값이 포함된 벡터 z\nsum(z)                          # 정상 계산이 안 됨\n## [1] NA\nis.na(z)                        # NA 여부 확인\n## [1] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE\nsum(is.na(z))                   # NA의 개수 확인\n## [1] 2\nsum(z, na.rm = TRUE)            # NA를 제외하고 합계를 계산\n## [1] 19\n\n\nz1 <- c(1, 2, 3, NA, 5, NA, 8)          # 결측값이 포함된 벡터 z1\nz2 <- c(5, 8, 1, NA, 3, NA, 7)          # 결측값이 포함된 벡터 z2\nz1[is.na(z1)] <- 0                      # NA를 0으로 치환\nz1\n## [1] 1 2 3 0 5 0 8\n\nz1[is.na(z1)] <- mean(z1, na.rm = TRUE) # NA를 z1의 평균값으로 치환\nz1\n## [1] 1 2 3 0 5 0 8\n\nz3 <- as.vector(na.omit(z2))            # NA를 제거하고 새로운 벡터 생성\nz3\n## [1] 5 8 1 3 7\n\n\n# NA를 포함하는 test 데이터 생성\nx <- iris\nx[1, 2] <- NA\nx[1, 3] <- NA\nx[2, 3] <- NA\nx[3, 4] <- NA\nhead(x)\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1          NA           NA         0.2  setosa\n## 2          4.9         3.0           NA         0.2  setosa\n## 3          4.7         3.2          1.3          NA  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\n\n\n# for문을 이용한 방법 ★★★★★\nfor (i in 1:ncol(x)) {\n    this.na <- is.na(x[, i])\n    cat(colnames(x)[i], \"\\t\", sum(this.na), \"\\n\")\n}\n## Sepal.Length      0 \n## Sepal.Width   1 \n## Petal.Length      2 \n## Petal.Width   1 \n## Species   0\n\n# apply를 이용한 방법\ncol_na <- function(y) {\n    return(sum(is.na(y)))\n}\n\nna_count <- apply(x, 2, FUN = col_na)\nna_count\n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n##            0            1            2            1            0\n\n\nrowSums(is.na(x))           # 행별 NA의 개수\n##   [1] 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n##  [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n##  [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n## [112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n## [149] 0 0\nsum(rowSums(is.na(x)) > 0)  # NA가 포함된 행의 개수\n## [1] 3\n\nsum(is.na(x))               # 데이터셋 전체에서 NA 개수\n## [1] 4\n\n\nhead(x)\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1          NA           NA         0.2  setosa\n## 2          4.9         3.0           NA         0.2  setosa\n## 3          4.7         3.2          1.3          NA  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\nx[!complete.cases(x), ]     # NA가 포함된 행들 출력\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1          NA           NA         0.2  setosa\n## 2          4.9         3.0           NA         0.2  setosa\n## 3          4.7         3.2          1.3          NA  setosa\ny <- x[complete.cases(x), ] # NA가 포함된 행들 제거\nhead(y)                     # 새로운 데이터셋 y의 내용 확인\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\n## 7          4.6         3.4          1.4         0.3  setosa\n## 8          5.0         3.4          1.5         0.2  setosa\n## 9          4.4         2.9          1.4         0.2  setosa\n\n\nst <- data.frame(state.x77)\nboxplot(st$Income)\n\n\n\nboxplot.stats(st$Income)\n## $stats\n## [1] 3098 3983 4519 4815 5348\n## \n## $n\n## [1] 50\n## \n## $conf\n## [1] 4333.093 4704.907\n## \n## $out\n## [1] 6315\n# stats (각 변수의 최소값, 1사분위수, 2사분위수, 3사분위수, 최대값이 저장되어 있는 행렬)\n# n (각 그룹마다의 관측값 수를 저장한 벡터)\n# conf (중앙값의 95% 신뢰구간, median+-1.58*IQR/(n)^0.5)\n# out (이상치)\nboxplot.stats(st$Income)$out\n## [1] 6315\n\n\nout.val <- boxplot.stats(st$Income)$out     # 특이값 추출\n\nst$Income %in% out.val\n##  [1] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [49] FALSE FALSE\nst$Income == out.val\n##  [1] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [49] FALSE FALSE\n\nst$Income[st$Income %in% out.val] <- NA     # 특이값을 NA로 대체\nst$Income[st$Income == out.val] <- NA\n\nhead(st)\n##            Population Income Illiteracy Life.Exp Murder HS.Grad Frost   Area\n## Alabama          3615   3624        2.1    69.05   15.1    41.3    20  50708\n## Alaska            365     NA        1.5    69.31   11.3    66.7   152 566432\n## Arizona          2212   4530        1.8    70.55    7.8    58.1    15 113417\n## Arkansas         2110   3378        1.9    70.66   10.1    39.9    65  51945\n## California      21198   5114        1.1    71.71   10.3    62.6    20 156361\n## Colorado         2541   4884        0.7    72.06    6.8    63.9   166 103766\nnewdata <- st[complete.cases(st), ]         # NA가 포함된 행 제거 ★★★★★\nhead(newdata)\n##             Population Income Illiteracy Life.Exp Murder HS.Grad Frost   Area\n## Alabama           3615   3624        2.1    69.05   15.1    41.3    20  50708\n## Arizona           2212   4530        1.8    70.55    7.8    58.1    15 113417\n## Arkansas          2110   3378        1.9    70.66   10.1    39.9    65  51945\n## California       21198   5114        1.1    71.71   10.3    62.6    20 156361\n## Colorado          2541   4884        0.7    72.06    6.8    63.9   166 103766\n## Connecticut       3100   5348        1.1    72.48    3.1    56.0   139   4862\n\n\nv1 <- c(1, 7, 6, 8, 4, 2, 3)\norder(v1)\n## [1] 1 6 7 5 3 2 4\n\nv1 <- sort(v1) # 오름차순\nv1\n## [1] 1 2 3 4 6 7 8\nv1[order(v1)]\n## [1] 1 2 3 4 6 7 8\n\nv2 <- sort(v1, decreasing = T) # 내림차순\nv2\n## [1] 8 7 6 4 3 2 1\n\n\nhead(iris)\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\norder(iris$Sepal.Length)\n##   [1]  14   9  39  43  42   4   7  23  48   3  30  12  13  25  31  46   2  10\n##  [19]  35  38  58 107   5   8  26  27  36  41  44  50  61  94   1  18  20  22\n##  [37]  24  40  45  47  99  28  29  33  60  49   6  11  17  21  32  85  34  37\n##  [55]  54  81  82  90  91  65  67  70  89  95 122  16  19  56  80  96  97 100\n##  [73] 114  15  68  83  93 102 115 143  62  71 150  63  79  84  86 120 139  64\n##  [91]  72  74  92 128 135  69  98 127 149  57  73  88 101 104 124 134 137 147\n## [109]  52  75 112 116 129 133 138  55 105 111 117 148  59  76  66  78  87 109\n## [127] 125 141 145 146  77 113 144  53 121 140 142  51 103 110 126 130 108 131\n## [145] 106 118 119 123 136 132\niris[order(iris$Sepal.Length), ]                    # 오름차순으로 정렬\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 14           4.3         3.0          1.1         0.1     setosa\n## 9            4.4         2.9          1.4         0.2     setosa\n## 39           4.4         3.0          1.3         0.2     setosa\n## 43           4.4         3.2          1.3         0.2     setosa\n## 42           4.5         2.3          1.3         0.3     setosa\n## 4            4.6         3.1          1.5         0.2     setosa\n## 7            4.6         3.4          1.4         0.3     setosa\n## 23           4.6         3.6          1.0         0.2     setosa\n## 48           4.6         3.2          1.4         0.2     setosa\n## 3            4.7         3.2          1.3         0.2     setosa\n## 30           4.7         3.2          1.6         0.2     setosa\n## 12           4.8         3.4          1.6         0.2     setosa\n## 13           4.8         3.0          1.4         0.1     setosa\n## 25           4.8         3.4          1.9         0.2     setosa\n## 31           4.8         3.1          1.6         0.2     setosa\n## 46           4.8         3.0          1.4         0.3     setosa\n## 2            4.9         3.0          1.4         0.2     setosa\n## 10           4.9         3.1          1.5         0.1     setosa\n## 35           4.9         3.1          1.5         0.2     setosa\n## 38           4.9         3.6          1.4         0.1     setosa\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 107          4.9         2.5          4.5         1.7  virginica\n## 5            5.0         3.6          1.4         0.2     setosa\n## 8            5.0         3.4          1.5         0.2     setosa\n## 26           5.0         3.0          1.6         0.2     setosa\n## 27           5.0         3.4          1.6         0.4     setosa\n## 36           5.0         3.2          1.2         0.2     setosa\n## 41           5.0         3.5          1.3         0.3     setosa\n## 44           5.0         3.5          1.6         0.6     setosa\n## 50           5.0         3.3          1.4         0.2     setosa\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 1            5.1         3.5          1.4         0.2     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 28           5.2         3.5          1.5         0.2     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 49           5.3         3.7          1.5         0.2     setosa\n## 6            5.4         3.9          1.7         0.4     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 34           5.5         4.2          1.4         0.2     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 122          5.6         2.8          4.9         2.0  virginica\n## 16           5.7         4.4          1.5         0.4     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 114          5.7         2.5          5.0         2.0  virginica\n## 15           5.8         4.0          1.2         0.2     setosa\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 102          5.8         2.7          5.1         1.9  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 150          5.9         3.0          5.1         1.8  virginica\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 120          6.0         2.2          5.0         1.5  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 128          6.1         3.0          4.9         1.8  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 127          6.2         2.8          4.8         1.8  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 112          6.4         2.7          5.3         1.9  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 105          6.5         3.0          5.8         2.2  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 109          6.7         2.5          5.8         1.8  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 113          6.8         3.0          5.5         2.1  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 121          6.9         3.2          5.7         2.3  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 103          7.1         3.0          5.9         2.1  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\niris[order(iris$Sepal.Length, decreasing = T), ]    # 내림차순으로 정렬\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 132          7.9         3.8          6.4         2.0  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 121          6.9         3.2          5.7         2.3  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 113          6.8         3.0          5.5         2.1  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 109          6.7         2.5          5.8         1.8  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 105          6.5         3.0          5.8         2.2  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 112          6.4         2.7          5.3         1.9  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 127          6.2         2.8          4.8         1.8  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 128          6.1         3.0          4.9         1.8  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 120          6.0         2.2          5.0         1.5  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 150          5.9         3.0          5.1         1.8  virginica\n## 15           5.8         4.0          1.2         0.2     setosa\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 102          5.8         2.7          5.1         1.9  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 16           5.7         4.4          1.5         0.4     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 114          5.7         2.5          5.0         2.0  virginica\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 122          5.6         2.8          4.9         2.0  virginica\n## 34           5.5         4.2          1.4         0.2     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 6            5.4         3.9          1.7         0.4     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 49           5.3         3.7          1.5         0.2     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 1            5.1         3.5          1.4         0.2     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 5            5.0         3.6          1.4         0.2     setosa\n## 8            5.0         3.4          1.5         0.2     setosa\n## 26           5.0         3.0          1.6         0.2     setosa\n## 27           5.0         3.4          1.6         0.4     setosa\n## 36           5.0         3.2          1.2         0.2     setosa\n## 41           5.0         3.5          1.3         0.3     setosa\n## 44           5.0         3.5          1.6         0.6     setosa\n## 50           5.0         3.3          1.4         0.2     setosa\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 2            4.9         3.0          1.4         0.2     setosa\n## 10           4.9         3.1          1.5         0.1     setosa\n## 35           4.9         3.1          1.5         0.2     setosa\n## 38           4.9         3.6          1.4         0.1     setosa\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 107          4.9         2.5          4.5         1.7  virginica\n## 12           4.8         3.4          1.6         0.2     setosa\n## 13           4.8         3.0          1.4         0.1     setosa\n## 25           4.8         3.4          1.9         0.2     setosa\n## 31           4.8         3.1          1.6         0.2     setosa\n## 46           4.8         3.0          1.4         0.3     setosa\n## 3            4.7         3.2          1.3         0.2     setosa\n## 30           4.7         3.2          1.6         0.2     setosa\n## 4            4.6         3.1          1.5         0.2     setosa\n## 7            4.6         3.4          1.4         0.3     setosa\n## 23           4.6         3.6          1.0         0.2     setosa\n## 48           4.6         3.2          1.4         0.2     setosa\n## 42           4.5         2.3          1.3         0.3     setosa\n## 9            4.4         2.9          1.4         0.2     setosa\n## 39           4.4         3.0          1.3         0.2     setosa\n## 43           4.4         3.2          1.3         0.2     setosa\n## 14           4.3         3.0          1.1         0.1     setosa\n\niris.new <- iris[order(iris$Sepal.Length), ]        # 정렬된 데이터를 저장\nhead(iris.new)\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 14          4.3         3.0          1.1         0.1  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 39          4.4         3.0          1.3         0.2  setosa\n## 43          4.4         3.2          1.3         0.2  setosa\n## 42          4.5         2.3          1.3         0.3  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\niris[order(iris$Species,-iris$Petal.Length, decreasing = T), ] # 정렬 기준이 2개\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 107          4.9         2.5          4.5         1.7  virginica\n## 127          6.2         2.8          4.8         1.8  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 122          5.6         2.8          4.9         2.0  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 128          6.1         3.0          4.9         1.8  virginica\n## 114          5.7         2.5          5.0         2.0  virginica\n## 120          6.0         2.2          5.0         1.5  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 101          6.3         3.3          6.0         2.5  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 23           4.6         3.6          1.0         0.2     setosa\n## 14           4.3         3.0          1.1         0.1     setosa\n## 15           5.8         4.0          1.2         0.2     setosa\n## 36           5.0         3.2          1.2         0.2     setosa\n## 3            4.7         3.2          1.3         0.2     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 39           4.4         3.0          1.3         0.2     setosa\n## 41           5.0         3.5          1.3         0.3     setosa\n## 42           4.5         2.3          1.3         0.3     setosa\n## 43           4.4         3.2          1.3         0.2     setosa\n## 1            5.1         3.5          1.4         0.2     setosa\n## 2            4.9         3.0          1.4         0.2     setosa\n## 5            5.0         3.6          1.4         0.2     setosa\n## 7            4.6         3.4          1.4         0.3     setosa\n## 9            4.4         2.9          1.4         0.2     setosa\n## 13           4.8         3.0          1.4         0.1     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 34           5.5         4.2          1.4         0.2     setosa\n## 38           4.9         3.6          1.4         0.1     setosa\n## 46           4.8         3.0          1.4         0.3     setosa\n## 48           4.6         3.2          1.4         0.2     setosa\n## 50           5.0         3.3          1.4         0.2     setosa\n## 4            4.6         3.1          1.5         0.2     setosa\n## 8            5.0         3.4          1.5         0.2     setosa\n## 10           4.9         3.1          1.5         0.1     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 16           5.7         4.4          1.5         0.4     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 35           4.9         3.1          1.5         0.2     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 49           5.3         3.7          1.5         0.2     setosa\n## 12           4.8         3.4          1.6         0.2     setosa\n## 26           5.0         3.0          1.6         0.2     setosa\n## 27           5.0         3.4          1.6         0.4     setosa\n## 30           4.7         3.2          1.6         0.2     setosa\n## 31           4.8         3.1          1.6         0.2     setosa\n## 44           5.0         3.5          1.6         0.6     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 6            5.4         3.9          1.7         0.4     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 25           4.8         3.4          1.9         0.2     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\niris[order(iris$Species, decreasing = T, iris$Petal.Length), ]\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 119          7.7         2.6          6.9         2.3  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 101          6.3         3.3          6.0         2.5  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\n## 114          5.7         2.5          5.0         2.0  virginica\n## 120          6.0         2.2          5.0         1.5  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 122          5.6         2.8          4.9         2.0  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 128          6.1         3.0          4.9         1.8  virginica\n## 127          6.2         2.8          4.8         1.8  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 107          4.9         2.5          4.5         1.7  virginica\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 25           4.8         3.4          1.9         0.2     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\n## 6            5.4         3.9          1.7         0.4     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 12           4.8         3.4          1.6         0.2     setosa\n## 26           5.0         3.0          1.6         0.2     setosa\n## 27           5.0         3.4          1.6         0.4     setosa\n## 30           4.7         3.2          1.6         0.2     setosa\n## 31           4.8         3.1          1.6         0.2     setosa\n## 44           5.0         3.5          1.6         0.6     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 4            4.6         3.1          1.5         0.2     setosa\n## 8            5.0         3.4          1.5         0.2     setosa\n## 10           4.9         3.1          1.5         0.1     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 16           5.7         4.4          1.5         0.4     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 35           4.9         3.1          1.5         0.2     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 49           5.3         3.7          1.5         0.2     setosa\n## 1            5.1         3.5          1.4         0.2     setosa\n## 2            4.9         3.0          1.4         0.2     setosa\n## 5            5.0         3.6          1.4         0.2     setosa\n## 7            4.6         3.4          1.4         0.3     setosa\n## 9            4.4         2.9          1.4         0.2     setosa\n## 13           4.8         3.0          1.4         0.1     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 34           5.5         4.2          1.4         0.2     setosa\n## 38           4.9         3.6          1.4         0.1     setosa\n## 46           4.8         3.0          1.4         0.3     setosa\n## 48           4.6         3.2          1.4         0.2     setosa\n## 50           5.0         3.3          1.4         0.2     setosa\n## 3            4.7         3.2          1.3         0.2     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 39           4.4         3.0          1.3         0.2     setosa\n## 41           5.0         3.5          1.3         0.3     setosa\n## 42           4.5         2.3          1.3         0.3     setosa\n## 43           4.4         3.2          1.3         0.2     setosa\n## 15           5.8         4.0          1.2         0.2     setosa\n## 36           5.0         3.2          1.2         0.2     setosa\n## 14           4.3         3.0          1.1         0.1     setosa\n## 23           4.6         3.6          1.0         0.2     setosa\n\n\nsp <- split(iris, iris$Species) # 품종별로 데이터 분리\nsp                              # 분리 결과 확인\n## $setosa\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1           5.1         3.5          1.4         0.2  setosa\n## 2           4.9         3.0          1.4         0.2  setosa\n## 3           4.7         3.2          1.3         0.2  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\n## 5           5.0         3.6          1.4         0.2  setosa\n## 6           5.4         3.9          1.7         0.4  setosa\n## 7           4.6         3.4          1.4         0.3  setosa\n## 8           5.0         3.4          1.5         0.2  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\n## 11          5.4         3.7          1.5         0.2  setosa\n## 12          4.8         3.4          1.6         0.2  setosa\n## 13          4.8         3.0          1.4         0.1  setosa\n## 14          4.3         3.0          1.1         0.1  setosa\n## 15          5.8         4.0          1.2         0.2  setosa\n## 16          5.7         4.4          1.5         0.4  setosa\n## 17          5.4         3.9          1.3         0.4  setosa\n## 18          5.1         3.5          1.4         0.3  setosa\n## 19          5.7         3.8          1.7         0.3  setosa\n## 20          5.1         3.8          1.5         0.3  setosa\n## 21          5.4         3.4          1.7         0.2  setosa\n## 22          5.1         3.7          1.5         0.4  setosa\n## 23          4.6         3.6          1.0         0.2  setosa\n## 24          5.1         3.3          1.7         0.5  setosa\n## 25          4.8         3.4          1.9         0.2  setosa\n## 26          5.0         3.0          1.6         0.2  setosa\n## 27          5.0         3.4          1.6         0.4  setosa\n## 28          5.2         3.5          1.5         0.2  setosa\n## 29          5.2         3.4          1.4         0.2  setosa\n## 30          4.7         3.2          1.6         0.2  setosa\n## 31          4.8         3.1          1.6         0.2  setosa\n## 32          5.4         3.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\n## 35          4.9         3.1          1.5         0.2  setosa\n## 36          5.0         3.2          1.2         0.2  setosa\n## 37          5.5         3.5          1.3         0.2  setosa\n## 38          4.9         3.6          1.4         0.1  setosa\n## 39          4.4         3.0          1.3         0.2  setosa\n## 40          5.1         3.4          1.5         0.2  setosa\n## 41          5.0         3.5          1.3         0.3  setosa\n## 42          4.5         2.3          1.3         0.3  setosa\n## 43          4.4         3.2          1.3         0.2  setosa\n## 44          5.0         3.5          1.6         0.6  setosa\n## 45          5.1         3.8          1.9         0.4  setosa\n## 46          4.8         3.0          1.4         0.3  setosa\n## 47          5.1         3.8          1.6         0.2  setosa\n## 48          4.6         3.2          1.4         0.2  setosa\n## 49          5.3         3.7          1.5         0.2  setosa\n## 50          5.0         3.3          1.4         0.2  setosa\n## \n## $versicolor\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## \n## $virginica\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n## 101          6.3         3.3          6.0         2.5 virginica\n## 102          5.8         2.7          5.1         1.9 virginica\n## 103          7.1         3.0          5.9         2.1 virginica\n## 104          6.3         2.9          5.6         1.8 virginica\n## 105          6.5         3.0          5.8         2.2 virginica\n## 106          7.6         3.0          6.6         2.1 virginica\n## 107          4.9         2.5          4.5         1.7 virginica\n## 108          7.3         2.9          6.3         1.8 virginica\n## 109          6.7         2.5          5.8         1.8 virginica\n## 110          7.2         3.6          6.1         2.5 virginica\n## 111          6.5         3.2          5.1         2.0 virginica\n## 112          6.4         2.7          5.3         1.9 virginica\n## 113          6.8         3.0          5.5         2.1 virginica\n## 114          5.7         2.5          5.0         2.0 virginica\n## 115          5.8         2.8          5.1         2.4 virginica\n## 116          6.4         3.2          5.3         2.3 virginica\n## 117          6.5         3.0          5.5         1.8 virginica\n## 118          7.7         3.8          6.7         2.2 virginica\n## 119          7.7         2.6          6.9         2.3 virginica\n## 120          6.0         2.2          5.0         1.5 virginica\n## 121          6.9         3.2          5.7         2.3 virginica\n## 122          5.6         2.8          4.9         2.0 virginica\n## 123          7.7         2.8          6.7         2.0 virginica\n## 124          6.3         2.7          4.9         1.8 virginica\n## 125          6.7         3.3          5.7         2.1 virginica\n## 126          7.2         3.2          6.0         1.8 virginica\n## 127          6.2         2.8          4.8         1.8 virginica\n## 128          6.1         3.0          4.9         1.8 virginica\n## 129          6.4         2.8          5.6         2.1 virginica\n## 130          7.2         3.0          5.8         1.6 virginica\n## 131          7.4         2.8          6.1         1.9 virginica\n## 132          7.9         3.8          6.4         2.0 virginica\n## 133          6.4         2.8          5.6         2.2 virginica\n## 134          6.3         2.8          5.1         1.5 virginica\n## 135          6.1         2.6          5.6         1.4 virginica\n## 136          7.7         3.0          6.1         2.3 virginica\n## 137          6.3         3.4          5.6         2.4 virginica\n## 138          6.4         3.1          5.5         1.8 virginica\n## 139          6.0         3.0          4.8         1.8 virginica\n## 140          6.9         3.1          5.4         2.1 virginica\n## 141          6.7         3.1          5.6         2.4 virginica\n## 142          6.9         3.1          5.1         2.3 virginica\n## 143          5.8         2.7          5.1         1.9 virginica\n## 144          6.8         3.2          5.9         2.3 virginica\n## 145          6.7         3.3          5.7         2.5 virginica\n## 146          6.7         3.0          5.2         2.3 virginica\n## 147          6.3         2.5          5.0         1.9 virginica\n## 148          6.5         3.0          5.2         2.0 virginica\n## 149          6.2         3.4          5.4         2.3 virginica\n## 150          5.9         3.0          5.1         1.8 virginica\nsummary(sp)                     # 분리 결과 요약\n##            Length Class      Mode\n## setosa     5      data.frame list\n## versicolor 5      data.frame list\n## virginica  5      data.frame list\nsp$setosa                       # setosa 품종의 데이터 확인\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1           5.1         3.5          1.4         0.2  setosa\n## 2           4.9         3.0          1.4         0.2  setosa\n## 3           4.7         3.2          1.3         0.2  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\n## 5           5.0         3.6          1.4         0.2  setosa\n## 6           5.4         3.9          1.7         0.4  setosa\n## 7           4.6         3.4          1.4         0.3  setosa\n## 8           5.0         3.4          1.5         0.2  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\n## 11          5.4         3.7          1.5         0.2  setosa\n## 12          4.8         3.4          1.6         0.2  setosa\n## 13          4.8         3.0          1.4         0.1  setosa\n## 14          4.3         3.0          1.1         0.1  setosa\n## 15          5.8         4.0          1.2         0.2  setosa\n## 16          5.7         4.4          1.5         0.4  setosa\n## 17          5.4         3.9          1.3         0.4  setosa\n## 18          5.1         3.5          1.4         0.3  setosa\n## 19          5.7         3.8          1.7         0.3  setosa\n## 20          5.1         3.8          1.5         0.3  setosa\n## 21          5.4         3.4          1.7         0.2  setosa\n## 22          5.1         3.7          1.5         0.4  setosa\n## 23          4.6         3.6          1.0         0.2  setosa\n## 24          5.1         3.3          1.7         0.5  setosa\n## 25          4.8         3.4          1.9         0.2  setosa\n## 26          5.0         3.0          1.6         0.2  setosa\n## 27          5.0         3.4          1.6         0.4  setosa\n## 28          5.2         3.5          1.5         0.2  setosa\n## 29          5.2         3.4          1.4         0.2  setosa\n## 30          4.7         3.2          1.6         0.2  setosa\n## 31          4.8         3.1          1.6         0.2  setosa\n## 32          5.4         3.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\n## 35          4.9         3.1          1.5         0.2  setosa\n## 36          5.0         3.2          1.2         0.2  setosa\n## 37          5.5         3.5          1.3         0.2  setosa\n## 38          4.9         3.6          1.4         0.1  setosa\n## 39          4.4         3.0          1.3         0.2  setosa\n## 40          5.1         3.4          1.5         0.2  setosa\n## 41          5.0         3.5          1.3         0.3  setosa\n## 42          4.5         2.3          1.3         0.3  setosa\n## 43          4.4         3.2          1.3         0.2  setosa\n## 44          5.0         3.5          1.6         0.6  setosa\n## 45          5.1         3.8          1.9         0.4  setosa\n## 46          4.8         3.0          1.4         0.3  setosa\n## 47          5.1         3.8          1.6         0.2  setosa\n## 48          4.6         3.2          1.4         0.2  setosa\n## 49          5.3         3.7          1.5         0.2  setosa\n## 50          5.0         3.3          1.4         0.2  setosa\nsetosa <- sp$setosa\n\n\nsubset(iris, Species == \"setosa\")\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1           5.1         3.5          1.4         0.2  setosa\n## 2           4.9         3.0          1.4         0.2  setosa\n## 3           4.7         3.2          1.3         0.2  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\n## 5           5.0         3.6          1.4         0.2  setosa\n## 6           5.4         3.9          1.7         0.4  setosa\n## 7           4.6         3.4          1.4         0.3  setosa\n## 8           5.0         3.4          1.5         0.2  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\n## 11          5.4         3.7          1.5         0.2  setosa\n## 12          4.8         3.4          1.6         0.2  setosa\n## 13          4.8         3.0          1.4         0.1  setosa\n## 14          4.3         3.0          1.1         0.1  setosa\n## 15          5.8         4.0          1.2         0.2  setosa\n## 16          5.7         4.4          1.5         0.4  setosa\n## 17          5.4         3.9          1.3         0.4  setosa\n## 18          5.1         3.5          1.4         0.3  setosa\n## 19          5.7         3.8          1.7         0.3  setosa\n## 20          5.1         3.8          1.5         0.3  setosa\n## 21          5.4         3.4          1.7         0.2  setosa\n## 22          5.1         3.7          1.5         0.4  setosa\n## 23          4.6         3.6          1.0         0.2  setosa\n## 24          5.1         3.3          1.7         0.5  setosa\n## 25          4.8         3.4          1.9         0.2  setosa\n## 26          5.0         3.0          1.6         0.2  setosa\n## 27          5.0         3.4          1.6         0.4  setosa\n## 28          5.2         3.5          1.5         0.2  setosa\n## 29          5.2         3.4          1.4         0.2  setosa\n## 30          4.7         3.2          1.6         0.2  setosa\n## 31          4.8         3.1          1.6         0.2  setosa\n## 32          5.4         3.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\n## 35          4.9         3.1          1.5         0.2  setosa\n## 36          5.0         3.2          1.2         0.2  setosa\n## 37          5.5         3.5          1.3         0.2  setosa\n## 38          4.9         3.6          1.4         0.1  setosa\n## 39          4.4         3.0          1.3         0.2  setosa\n## 40          5.1         3.4          1.5         0.2  setosa\n## 41          5.0         3.5          1.3         0.3  setosa\n## 42          4.5         2.3          1.3         0.3  setosa\n## 43          4.4         3.2          1.3         0.2  setosa\n## 44          5.0         3.5          1.6         0.6  setosa\n## 45          5.1         3.8          1.9         0.4  setosa\n## 46          4.8         3.0          1.4         0.3  setosa\n## 47          5.1         3.8          1.6         0.2  setosa\n## 48          4.6         3.2          1.4         0.2  setosa\n## 49          5.3         3.7          1.5         0.2  setosa\n## 50          5.0         3.3          1.4         0.2  setosa\nsubset(iris, Sepal.Length > 7.5)\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n## 106          7.6         3.0          6.6         2.1 virginica\n## 118          7.7         3.8          6.7         2.2 virginica\n## 119          7.7         2.6          6.9         2.3 virginica\n## 123          7.7         2.8          6.7         2.0 virginica\n## 132          7.9         3.8          6.4         2.0 virginica\n## 136          7.7         3.0          6.1         2.3 virginica\nsubset(iris, Sepal.Length > 5.1 &\n           Sepal.Width > 3.9)\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 15          5.8         4.0          1.2         0.2  setosa\n## 16          5.7         4.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\nsubset(iris, Sepal.Length > 5.1 |\n           Sepal.Width > 3.9)\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 6            5.4         3.9          1.7         0.4     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 15           5.8         4.0          1.2         0.2     setosa\n## 16           5.7         4.4          1.5         0.4     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 34           5.5         4.2          1.4         0.2     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 49           5.3         3.7          1.5         0.2     setosa\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 114          5.7         2.5          5.0         2.0  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 120          6.0         2.2          5.0         1.5  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 122          5.6         2.8          4.9         2.0  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 127          6.2         2.8          4.8         1.8  virginica\n## 128          6.1         3.0          4.9         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\nsubset(iris, Sepal.Length > 7.6,\n       select = c(Petal.Length, Petal.Width))\n##     Petal.Length Petal.Width\n## 118          6.7         2.2\n## 119          6.9         2.3\n## 123          6.7         2.0\n## 132          6.4         2.0\n## 136          6.1         2.3\n\n\nx <- 1:10\nsample(x, size = 5, replace = FALSE) # 비복원추출\n## [1] 6 4 8 7 2\nsample(x, size = 5, replace = TRUE)\n## [1] 9 5 6 3 7\n\nx <- 1:45\nsample(x, size = 6, replace = FALSE)\n## [1] 31 21 37  7  9 13\n\n\nidx <- sample(1:nrow(iris), size = 50,\n              replace = FALSE)\niris.50 <- iris[idx, ]  # 50개의 행 추출\ndim(iris.50)            # 행과 열의 개수 확인\n## [1] 50  5\nhead(iris.50)\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 107          4.9         2.5          4.5         1.7  virginica\n## 4            4.6         3.1          1.5         0.2     setosa\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 142          6.9         3.1          5.1         2.3  virginica\n## 87           6.7         3.1          4.7         1.5 versicolor\n\n\nsample(1:20, size = 5)\n## [1] 14 18 20  2  4\nsample(1:20, size = 5)\n## [1] 19  8  1 16 11\nsample(1:20, size = 5)\n## [1] 11  7 20 12 18\n\n# 같은 값이 추출되도록 고정시키고 싶다면\n# set.seed() 함수를 이용하여 seed값을 지정해주면 된다.\nset.seed(100)\nsample(1:20, size = 5)\n## [1] 10  6 16 14 12\nset.seed(100)\nsample(1:20, size = 5)\n## [1] 10  6 16 14 12\nset.seed(100)\nsample(1:20, size = 5)\n## [1] 10  6 16 14 12\n\n\ncombn(1:5, 3) # 1~5에서 3개를 뽑는 조합\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n## [1,]    1    1    1    1    1    1    2    2    2     3\n## [2,]    2    2    2    3    3    4    3    3    4     4\n## [3,]    3    4    5    4    5    5    4    5    5     5\n\nx = c(\"red\", \"green\", \"blue\", \"black\", \"white\")\ncom <- combn(x, 2) # x의 원소를 2개씩 뽑는 조합\ncom\n##      [,1]    [,2]   [,3]    [,4]    [,5]    [,6]    [,7]    [,8]    [,9]   \n## [1,] \"red\"   \"red\"  \"red\"   \"red\"   \"green\" \"green\" \"green\" \"blue\"  \"blue\" \n## [2,] \"green\" \"blue\" \"black\" \"white\" \"blue\"  \"black\" \"white\" \"black\" \"white\"\n##      [,10]  \n## [1,] \"black\"\n## [2,] \"white\"\n\nfor (i in 1:ncol(com)) {\n    # 조합을 출력\n    cat(com[, i], \"\\n\")\n}\n## red green \n## red blue \n## red black \n## red white \n## green blue \n## green black \n## green white \n## blue black \n## blue white \n## black white\n\n\n# aggregate(data, by = '기준이 되는 컬럼', FUN)\nagg <- aggregate(iris[, -5], by = list(iris$Species), FUN = mean)\nagg\n##      Group.1 Sepal.Length Sepal.Width Petal.Length Petal.Width\n## 1     setosa        5.006       3.428        1.462       0.246\n## 2 versicolor        5.936       2.770        4.260       1.326\n## 3  virginica        6.588       2.974        5.552       2.026\n\n\n# aggregate는 데이터의 특정 컬럼을 기준으로 통계량을 구해주는 함수\nagg <- aggregate(iris[, -5], by = list(표준편차 = iris$Species), FUN = sd)\nagg\n##     표준편차 Sepal.Length Sepal.Width Petal.Length Petal.Width\n## 1     setosa    0.3524897   0.3790644    0.1736640   0.1053856\n## 2 versicolor    0.5161711   0.3137983    0.4699110   0.1977527\n## 3  virginica    0.6358796   0.3224966    0.5518947   0.2746501\n\n\nhead(mtcars)\n##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\nagg <- aggregate(mtcars, by = list(cyl = mtcars$cyl, vs = mtcars$vs), FUN = max)\nagg\n##   cyl vs  mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## 1   4  0 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n## 2   6  0 21.0   6 160.0 175 3.90 2.875 17.02  0  1    5    6\n## 3   8  0 19.2   8 472.0 335 4.22 5.424 18.00  0  1    5    8\n## 4   4  1 33.9   4 146.7 113 4.93 3.190 22.90  1  1    5    2\n## 5   6  1 21.4   6 258.0 123 3.92 3.460 20.22  1  0    4    4\n\nagg <- aggregate(mtcars, by = list(cyl = mtcars$cyl, vs = mtcars$vs), FUN = mean)\nagg\n##   cyl vs      mpg cyl   disp       hp     drat       wt     qsec vs        am\n## 1   4  0 26.00000   4 120.30  91.0000 4.430000 2.140000 16.70000  0 1.0000000\n## 2   6  0 20.56667   6 155.00 131.6667 3.806667 2.755000 16.32667  0 1.0000000\n## 3   8  0 15.10000   8 353.10 209.2143 3.229286 3.999214 16.77214  0 0.1428571\n## 4   4  1 26.73000   4 103.62  81.8000 4.035000 2.300300 19.38100  1 0.7000000\n## 5   6  1 19.12500   6 204.55 115.2500 3.420000 3.388750 19.21500  1 0.0000000\n##       gear     carb\n## 1 5.000000 2.000000\n## 2 4.333333 4.666667\n## 3 3.285714 3.500000\n## 4 4.000000 1.500000\n## 5 3.500000 2.500000\n\n\nx <- data.frame(name = c(\"a\", \"b\", \"c\"), math = c(90, 80, 40))\ny <- data.frame(name = c(\"a\", \"b\", \"d\"), korean = c(75, 60, 90))\nx ; y\n##   name math\n## 1    a   90\n## 2    b   80\n## 3    c   40\n##   name korean\n## 1    a     75\n## 2    b     60\n## 3    d     90\n\n\nz <- merge(x, y, by = c(\"name\"))\nz\n##   name math korean\n## 1    a   90     75\n## 2    b   80     60\n\n\nmerge(x, y, all.x = T)  # 첫 번째 데이터셋의 행들은 모두 표시되도록\n##   name math korean\n## 1    a   90     75\n## 2    b   80     60\n## 3    c   40     NA\nmerge(x, y, all.y = T)  # 두 번째 데이터셋의 행들은 모두 표시되도록\n##   name math korean\n## 1    a   90     75\n## 2    b   80     60\n## 3    d   NA     90\nmerge(x, y, all = T)    # 두 데이터셋의 모든 행들이 표시되도록\n##   name math korean\n## 1    a   90     75\n## 2    b   80     60\n## 3    c   40     NA\n## 4    d   NA     90\n\n\nx <- data.frame(name = c(\"a\", \"b\", \"c\"), math = c(90, 80, 40))\ny <- data.frame(sname = c(\"a\", \"b\", \"d\"), korean = c(75, 60, 90))\nx # 병합 기준 열의 이름이 name\n##   name math\n## 1    a   90\n## 2    b   80\n## 3    c   40\ny # 병합 기준 열의 이름이 sname\n##   sname korean\n## 1     a     75\n## 2     b     60\n## 3     d     90\nmerge(x, y, by.x = c(\"name\"), by.y = c(\"sname\"))\n##   name math korean\n## 1    a   90     75\n## 2    b   80     60"
  },
  {
    "objectID": "Spatial_Information_Analysis/실기_1유형.html",
    "href": "Spatial_Information_Analysis/실기_1유형.html",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "",
    "text": "실기 1 유형"
  },
  {
    "objectID": "Spatial_Information_Analysis/실기_1유형.html#데이터-다루기-유형",
    "href": "Spatial_Information_Analysis/실기_1유형.html#데이터-다루기-유형",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "✅ 데이터 다루기 유형",
    "text": "✅ 데이터 다루기 유형\n\n\n데이터 타입(object, int, float, bool 등)\n기초통계량(평균, 중앙값, 사분위수, IQR, 표준편차 등)\n데이터 인덱싱, 필터링, 정렬, 변경 등\n중복값, 결측치, 이상치 처리 (제거 or 대체)\n데이터 Scaling (데이터 표준화(z), 데이터 정규화(min-max))\n데이터 합치기\n날짜/시간 데이터, index 다루기"
  },
  {
    "objectID": "Spatial_Information_Analysis/실기_1유형.html#핵심문제-27개-110",
    "href": "Spatial_Information_Analysis/실기_1유형.html#핵심문제-27개-110",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "✅ 핵심문제 27개 (1~10)",
    "text": "✅ 핵심문제 27개 (1~10)\n\n# 데이터 불러오기\nimport pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"mtcars.txt\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      Hornet Sportabout\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\n# 문제 1\n# mpg 변수의 제 1사분위수를 구하고 정수값으로 출력하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\nQ1 = df['mpg'].quantile(.25)\nprint(round(Q1))\n\n15\n\n\n\n# 문제 2\n# mpg 값이 19이상 21이하인 데이터의 수를 구하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이 1)\ncond1 = (df[(df['mpg']>=19) & (df['mpg']<=21)])\nprint(len(cond1))\n\n# (풀이 2)\n# cond1 = (df['mpg']>=19)\n# cond2 = (df['mpg']<=21)\n# print(len(df[cond1&cond2]))\n\n5\n\n\n\n# 문제 3\n# hp 변수의 IQR 값을 구하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\nQ1 = df['hp'].quantile(.25)\nQ3 = df['hp'].quantile(.75)\nIQR = Q3 - Q1\nprint(IQR)\n\n83.5\n\n\n\n# 문제 4 \n# wt 변수의 상위 10개 값의 총합을 구하여 소수점을 버리고 정수로 출력하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\ntop10 = df.sort_values('wt', ascending=False)\nsum_top10 = sum(top10['wt'].head(10))\nprint(int(sum_top10))\n\n# top_10 = df.sort_values('wt',ascending=False).head(10)\n# print(int(sum(top_10['wt']))) #주의: 소수점 반올림이 아니라 버리는 문제\n\n42\n\n\n\n# 문제 5\n# 전체 자동차에서 cyl가 6인 비율이 얼마인지 소수점 첫째짜리까지 출력하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\ncond1 = len(df[df['cyl']==6])\ncond2 = len(df['cyl'])\nprint(round(cond1/cond2, 1))\n\n0.2\n\n\n\n# 문제 6\n# 첫번째 행부터 순서대로 10개 뽑은 후 mpg 열의 평균값을 반올림하여 정수로 출력하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\ncond1 = df.head(10) # df[:10], df.loc[0:9]\nmean_mpg = cond1['mpg'].mean()\nprint(round(mean_mpg))\n\n20\n\n\n\n# 문제 7\n# 첫번째 행부터 순서대로 50% 까지 데이터를 뽑아 wt 변수의 중앙값을 구하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\n# 50% 데이터에 해당하는 행의 수 (정수로 구하기)\np50 = int(len(df)*0.5)\ndf50 = df[:p50]\nprint(df50['wt'].median())\n\n# len(df)/2\n# cond1 = df[:17]\n# cond2 = cond1['wt'].median()\n# print(cond2)\n\n3.44\n\n\n\n# 문제 8\n# 결측값이 있는 데이터의 수를 출력하시오\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    '판매수' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      날짜\n      제품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3.0\n      300\n    \n    \n      1\n      20220105\n      B\n      NaN\n      400\n    \n    \n      2\n      None\n      None\n      5.0\n      500\n    \n    \n      3\n      20230127\n      B\n      10.0\n      600\n    \n    \n      4\n      20220203\n      A\n      10.0\n      400\n    \n  \n\n\n\n\n\n# (풀이)\nm_value = df.isnull().sum()\nprint(m_value.sum())\n\n5\n\n\n\n# 문제 9 \n# '판매수' 컬럼의 결측값을 판매수의 중앙값으로 대체하고 판매수의 \n#  평균값을 정수로 출력하시오\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    '판매수' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\n\n\n# (풀이)\ndf[df['판매수'].isnull()]\nmedian = df['판매수'].median()\ndf['판매수'] = df['판매수'].fillna(median)\nmean = df['판매수'].mean()\nprint(int(mean))\n\n15\n\n\n\n# 문제 10\n# 판매수 컬럼에 결측치가 있는 행을 제거하고\n# 첫번째 행부터 순서대로 50%까지 데이터를 추출하여\n# 판매수 변수의 Q1(제1사분위수) 값을 정수로 출력하시오\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    '판매수' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\n\n\n# (풀이)\n# 결측치 삭제(행 기준)\ndf = df['판매수'].dropna()\n# 첫번째 행부터 순서대로 50%까지의 데이터 추출\nper50 = int(len(df)*0.5)\ndf = df[:per50]\n# 값구하기\nprint(round(df.quantile(.25)))\n\n# df[df['판매수'].isnull()]\n# df['판매수'] = df['판매수'].dropna()\n# int(len(df['판매수'])/2)\n# cond1 = df['판매수'].head(6)\n# Q1 = cond1.quantile(.25)\n# print(int(Q1))\n\n5"
  },
  {
    "objectID": "Spatial_Information_Analysis/실기_1유형.html#핵심문제-27개-1120",
    "href": "Spatial_Information_Analysis/실기_1유형.html#핵심문제-27개-1120",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "✅ 핵심문제 27개 (11~20)",
    "text": "✅ 핵심문제 27개 (11~20)\n\ndf = pd.read_csv('mtcars.txt')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      Hornet Sportabout\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\n# 문제 11\n# cyl가 4인 자동차와 6인 자동차 그룹의 mpg 평균값이 차이를 절대값으로 정수로 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\ncyl_4 = df[df['cyl']==4]\ncyl_4_mean = cyl_4['mpg'].mean()\n\ncyl_6 = df[df['cyl']==6]\ncyl_6_mean = cyl_6['mpg'].mean()\n\nprint(round(abs(cyl_4_mean - cyl_6_mean)))\n\n7\n\n\n\n# 문제 12\n# hp 변수에 대해 데이터표준화(Z-score)를 진행하고 이상치의 수를 구하시오\n# (단, 이상치는 Z값이 1.5를 초과하거나 -1.5미만인 값이다)\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\n# Z = (X-평균) / 표준편차\nstd = df['hp'].std()\nmean_hp = df['hp'].mean()\ndf['zscore'] = (df['hp']-mean_hp)/std\n\ncond1 = (df['zscore']>1.5)\ncond2 = (df['zscore']<-1.5)\nprint(len(df[cond1] + df[cond2]))\n\n2\n\n\n\n# 문제 13\n# mpg 컬럼을 최소최대 Scaling을 진행한 후 0.7보다 큰 값을 가지는 레코드 수를 구하라\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\nfrom sklearn.preprocessing import MinMaxScaler\nmscaler = MinMaxScaler()\ndf['mpg'] = mscaler.fit_transform(df[['mpg']])\nprint(len(df[df['mpg']>0.7]))\n\n# 공식 : (x-min) / (max-min)\n# min_mpg = df['mpg'].min()\n# max_mpg = df['mpg'].max()\n# df['mpg'] = (df['mpg'] - min_mpg)/(max_mpg - min_mpg)\n# cond1 = (df['mpg']>0.7)\n# print(len(df[cond1]))\n\n5\n\n\n\n# 문제 14\n# wt 컬럼에 대해 상자그림 기준으로 이상치의 개수를 구하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\n# 이상치 : Q1, Q3 로부터 1.5*IQR을 넘어가는 값\nQ1 = df['wt'].quantile(.25)\nQ3 = df['wt'].quantile(.75)\nIQR = Q3-Q1\n\nupper = Q3 + (1.5*IQR)\nlower = Q1 - (1.5*IQR)\n\ncond1 = (df['wt'] > upper)\ncond2 = (df['wt'] < lower)\n\nprint(len(df[cond1] + df[cond2]))\n\n3\n\n\n\n# 문제 15\n# 판매수 컬럼의 결측치를 최소값으로 대체하고\n# 결측치가 있을 때와 최소값으로 대체했을 때\n# 평균값의 차이를 절대값으로 정수로 출력하시오\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    '판매수' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf\n\n\n\n\n\n  \n    \n      \n      날짜\n      제품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3.0\n      300\n    \n    \n      1\n      20220105\n      B\n      NaN\n      400\n    \n    \n      2\n      None\n      None\n      5.0\n      500\n    \n    \n      3\n      20230127\n      B\n      10.0\n      600\n    \n    \n      4\n      20220203\n      A\n      10.0\n      400\n    \n    \n      5\n      20220205\n      None\n      10.0\n      500\n    \n    \n      6\n      20230210\n      A\n      15.0\n      500\n    \n    \n      7\n      20230223\n      B\n      15.0\n      600\n    \n    \n      8\n      20230312\n      A\n      20.0\n      600\n    \n    \n      9\n      20230422\n      B\n      NaN\n      700\n    \n    \n      10\n      20220505\n      A\n      30.0\n      600\n    \n    \n      11\n      20230511\n      A\n      40.0\n      600\n    \n  \n\n\n\n\n\n# (풀이)\n# 데이터 복사\ndf2 = df.copy()\n# 최소값으로 결측치 대체\nmin = df['판매수'].min()\ndf2['판매수'] = df2['판매수'].fillna(min)\n\n# 결측치가 있을때, 대체했을때 평균\nm_yes = df['판매수'].mean()\nm_no = df2['판매수'].mean()\n\nprint(round(abs(m_yes - m_no)))\n\n2\n\n\n\n# 문제 16\n# vs변수가 0이 아닌 차량 중에 mpg 값이 가장 큰 차량의 hp 값을 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\ncond1 = df[df['vs']!=0]\n# mpg 값이 가장 큰 차량(내림차순)\ncond1 = cond1.sort_values('mpg',ascending=False)\n# cond1\nprint(cond1['hp'].iloc[0])\n\n65\n\n\n\n# 문제 17\n# gear 변수값이 3, 4인 두 그룹의 hp 표준편차값의 차이를 절대값으로 \n# 소수점 첫째자리까지 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\n# 두개 그룹으로 필터링\ncond1 = df[df['gear']==3]\ncond2 = df[df['gear']==4]\n# std 구하기\nstd3 = cond1['hp'].std()\nstd4 = cond2['hp'].std()\n\nprint(round(abs(std3 - std4),1))\n\n21.8\n\n\n\n# 문제 18\n# gear 변수의 갑별로 그룹화하여 mpg 평균값을 산출하고\n# 평균값이 높은 그룹의 mpg 제3사분위수 값을 구하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\ndf['gear'].unique()\ngear3 = df[df['gear']==3]\ngear4 = df[df['gear']==4]\ngear5 = df[df['gear']==5]\n\nm_3 = gear3['mpg'].mean()\nm_4 = gear4['mpg'].mean()\nm_5 = gear5['mpg'].mean()\n\n(m_3, m_4, m_5) # m_4의 평균값이 가장 큼\nQ3 = gear4['mpg'].quantile(.75)\nprint(Q3)\n\n28.075\n\n\n\n# (풀이_v2)\n# gear, mpg 변수만 필터링\ndf = df.loc[:, ['gear', 'mpg']]\n\n# gear 변수로 그룹화하여 mpg 평균값 보기\n# print(df.groupby('gear').mean()) # gear 4그룹이 제일 높음\n\n# gear=4인 그룹의 mpg Q3 구하기\ngear4 = df[df['gear']==4]\nprint(gear4['mpg'].quantile(.75))\n\n28.075\n\n\n\n# 문제 19\n# hp 항목의 상위 7번째 값으로 상위 7개 값을 변환한 후,\n# hp가 150 이상인 데이터를 추출하여 hp의 평균값을 반올림하여 정수로 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\n# hp 열 기준으로 내림차순 정렬\ndf = df.sort_values('hp', ascending=False)\n\n# 인덱스 초기화 - 내림차순으로 정렬시 최초의 인덱스로 있기에\ndf = df.reset_index(drop=True) # drop=True 는 기존 index 삭제\n# print(df)\n\n# hp 상위 7번째 값 확인\ntop7 = df['hp'].loc[6] # top7 = 205\n\n# np.where 활용\nimport numpy as np\ndf['hp'] = np.where(df['hp'] >= 205, top7, df['hp'])\n# np.where(조건, 조건에 해당할 때 값, 그렇지 않을 때 값)\n\n# hp 150이상인 데이터\ncond1 = (df['hp']>=150)\ndf = df[cond1]\nprint(round(df['hp'].mean()))\n\n187\n\n\n\n# 문제 20\n# car 변수에 Merc 무구가 포함된 자동차의 mpg 평균값을 정수로 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\ndf2 = df[df['car'].str.contains('Merc')] # 문자열 추출 알아두기\nprint(round(df2['mpg'].mean()))\n\n# 시험환경에서 답구하는 방법(reset_index() 사용 후)\n# 시험에서는 car가 index로 되어 있음\n# df.reset_index(inplace=True)\n# df2 = df[df['index'].str.contains(\"Merc\")] \n# print(round(df2['mpg'].mean()))\n\n19"
  },
  {
    "objectID": "Spatial_Information_Analysis/실기_1유형.html#핵심문제-27개-2127",
    "href": "Spatial_Information_Analysis/실기_1유형.html#핵심문제-27개-2127",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "✅ 핵심문제 27개 (21~27)",
    "text": "✅ 핵심문제 27개 (21~27)\n\n# 문제 21\n# 22년 1분기 A제품의 매출액을 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    '판매수' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      날짜\n      제품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# # (풀이)\n# cond1 = df[df['날짜']<='20220431']\n# cond2 = cond1[cond1['제품']=='A']\n# cond2['매출액'] = (cond2['판매수']*cond2['개당수익'])\n# cond2\n\n# print(cond2['매출액'].sum())\n\n\n# (풀이_v2)\n# 데이터 타입 datetime으로 변경\ndf['날짜'] = pd.to_datetime(df['날짜'])\n\n# 년, 월, 일 변수(열) 추가하기\ndf['year'] = df['날짜'].dt.year\ndf['month'] = df['날짜'].dt.month\ndf['day'] = df['날짜'].dt.day\n\n# 매출액 변수 추가하기\ndf['매출액'] = df['판매수'] * df['개당수익']\n\n# 22년으로 필터링\ndf = df[df['year']==2022]\ndf.head()\n\n# 1,2,3월 매출액 계산\nm1 = df[df['month']==1]['매출액'].sum()\nm2 = df[df['month']==2]['매출액'].sum()\nm3 = df[df['month']==3]['매출액'].sum()\nprint(m1+m2+m3)\n\n11900\n\n\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['날짜'] = pd.to_datetime(df['날짜'])\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['year'] = df['날짜'].dt.year\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['month'] = df['날짜'].dt.month\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['day'] = df['날짜'].dt.day\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['매출액'] = df['판매수'] * df['개당수익']\n\n\n\n# 문제 22\n# 22년과 23년의 총 매출액 차이를 절대값으로 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    '판매수' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      날짜\n      제품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# (풀이)\n# 데이터 타입 datetime으로 변경\ndf['날짜'] = pd.to_datetime(df['날짜'])\n\n# 년 변수(열) 추가하기\ndf['year'] = df['날짜'].dt.year\n\n# 매출액 변수 추가하기\ndf['매출액'] = df['판매수'] * df['개당수익']\n\ncond22 = df[df['year']==2022]\ncond23 = df[df['year']==2023]\n\nprint(abs((cond22['매출액'].sum()) - (cond23['매출액'].sum())))\n\n48600\n\n\n\n# 문제 23\n# 23년 총 매출액이 큰 제품의 23년 판매수를 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    '판매수' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      날짜\n      제품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# 데이터 타입 datetime으로 변경\ndf['날짜'] = pd.to_datetime(df['날짜'])\n\n# 년 변수(열) 추가하기\ndf['year'] = df['날짜'].dt.year\n\n# 매출액 변수 추가하기\ndf['매출액'] = df['판매수'] * df['개당수익']\n\ndf = df[df['year']==2023]\n# df.head()\n\n# 23년 A 매출액과 B 매출액 별도로 구하기\n# A 매출액\ndf_a = df[df['제품']=='A']\na_sales = df_a['매출액'].sum()\n# print(a_sales) # 46000\n\n# B 매출액\ndf_b = df[df['제품']=='B']\nb_sales = df_b['매출액'].sum()\n# print(b_sales) # 32500\n\n# A 제품의 23년 판매수\na_sum = df_a['판매수'].sum()\nprint(a_sum)\n\n80\n\n\n\n# 문제 24\n# 매출액이 4천원 초과, 1만원 미만인 데이터 수를 출력하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    '판매수' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      날짜\n      제품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# (풀이)\ndf['매출액'] = df['판매수'] * df['개당수익']\ncond1 = df['매출액']>4000\ncond2 = df['매출액']<10000\n\ndf = df[cond1&cond2]\nprint(len(df))\n\n4\n\n\n\n# 문제 25\n# 23년 9월 24일 16:00~22:00 사이에 전체 제품의 판매수를 구하시오\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600]})\ntime = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf['time'] = time\ndf = df[['time', '물품', '판매수', '개당수익']]\ndf\n\n\n\n\n\n  \n    \n      \n      time\n      물품\n      판매수\n      개당수익\n    \n  \n  \n    \n      0\n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      1\n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2\n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      3\n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      4\n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      5\n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      6\n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# (풀이)\n# 컬럼과 인덱스에 둘다 time 변수를 놓고 풀이\n# 데이터 타입 datetime으로 변경 (항상 df.info()로 데이터 타입 확인할 것)\ndf['time'] = pd.to_datetime(df['time'])\n\n# index 새로 지정\ndf = df.set_index('time', drop=False) # default는 True\n\n# 9월 24일 필터링 // df['변수'].between( , )\ndf_after = df[df['time'].between('2023-09-24', '2023-09-25')] # 25일은 미포함\n\n# 시간 필터링 16:00 ~ 22:00 (주의: 시간이 index에 위치해야 함)\ndf = df_after.between_time(start_time='16:00', end_time='22:00') # 포함 기준\n\n# 전체 판매수 구하기\nprint(df['판매수'].sum())\n\n25\n\n\n\n# (풀이2) // 위에 df 새로 불러와서 실행해보기\n# 데이터 타입 datetime으로 변경\ndf['time'] = pd.to_datetime(df['time'])\n\n# index 새로 지정\ndf = df.set_index('time')\n\n# loc 함수로 필터링\ndf = df.loc[(df.index >= '2023-09-24 16:00:00') & (df.index <= '2023-09-24 22:00:00')]\n\n# 전체 판매수 구하기\nprint(df['판매수'].sum())\n\n25\n\n\n\n# 문제 26\n# 9월 25일 00:10~12:00까지의 B물품의 매출액 총합을 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600]})\ndf['time'] = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf = df[['time', '물품', '판매수', '개당수익']]\ndf = df.set_index('time',drop=True)\ndf\n\n\n\n\n\n  \n    \n      \n      물품\n      판매수\n      개당수익\n    \n    \n      time\n      \n      \n      \n    \n  \n  \n    \n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# (풀이)\n# 매출액 변수 추가\ndf['매출액'] = df['판매수'] * df['개당수익']\n\n# loc 함수로 필터링\ndf = df.loc[(df.index >= '2023-09-25 00:10:00') & (df.index <= '2023-09-25 12:00:00')]\n\n# B 물품의 매출액 총합\ncond1 = df[df['물품']=='B']\nprint(cond1['매출액'].sum())\n\n26500\n\n\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\1645811142.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['매출액'] = df['판매수'] * df['개당수익']\n\n\n\n# 문제 27\n# 9월 24일 12:00~24:00까지의 A물품의 매출액 총합을 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600]})\ndf['time'] = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf = df[['time', '물품', '판매수', '개당수익']]\ndf = df.set_index('time',drop=True)\ndf\n\n\n\n\n\n  \n    \n      \n      물품\n      판매수\n      개당수익\n    \n    \n      time\n      \n      \n      \n    \n  \n  \n    \n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# (풀이)\n# 매출액 변수 추가\ndf['매출액'] = df['판매수'] * df['개당수익']\n\n# loc 함수로 필터링\ndf = df.loc[(df.index >= '2023-09-24 12:00:00') & (df.index < '2023-09-25 00:00:00')]\n\n# A 물품의 매출액 총합\ncond1 = df[df['물품']=='A']\nprint(cond1['매출액'].sum())\n\n10000"
  }
]