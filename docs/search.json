[
  {
    "objectID": "Spatial_Information_Analysis.html",
    "href": "Spatial_Information_Analysis.html",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "Geocomputation with R\n\n\n\n\n\n\n\n\n공간정보란 ? 사람들이 생활하고 있는 공간 상에서 사건이나 사물에 대한 위치를 나타내는 정보\n\n위치를 나타내는 정보는 (1) 위치를 표현하는 정보 (2) 해당 위치에 나타나는 특성에 대한 정보\n\n위치를 표현하는 정보 : 공간 상에서 사건이나 사물의 위치가 어디에 있는지를 나타내는 정보\n\nex) 주소, 위경도, x,y 좌표 등\n\n해당 위치에 나타나는 특성에 대한 정보 : 특정 위치에 있는 사건이나 사물을 설명하는 정보\n\nex) 학교, 회사, 학생 수 , 교사 수, 사고 건 수, 사고 유형 등\n\n\n\n지리정보 시스템(Geographic Information System) : 공간정보데이터를 처리, 가공하여 새로운 정보를 도출하는 일련의 과정 또는 기법\n\nex) 교통사고 데이터 분석 (TIMS)\n\n공간정보를 이용하여 GIS 분석을 수행하기 위한 소프트웨어\n\n전용 소프트웨어\n\nArcGIS : 전문적인 공간정보의 처리와 분석 가능, 고가(유료)\nQGIS : 오픈소스 GIS 소프트웨어, 최근 많은 분야에서 GIS 소프트웨어로 활용\n\n오픈소스 소프트웨어\n\nR 소프트웨어 : 오픈소스 기반의 통계 프로그램, 공간정보의 처리와 븐석에도 강력한 기능\nPython 소프트웨어 : 배우기 쉽고, 강력한 프로그래밍 언어, 공간정보를 다루는데 유용한 라이브러리가 개발\n\n\n\n\n\n\n\n위치정보와 속성정보로 구분\n\n위치정보\n\n좌표체계를 이용한 위치정보\n\n지리좌표계에서 이용하는 경도와 위도로 표현 ex) 경위도좌표\n수학적으로 X좌표와 Y좌표로 위치 정보를 표현 ex) 평면직각좌표(지도좌표)\n\n공간정보 데이터의 위치정보 표현 방식\n\n벡터 (점, 선, 면)\n래스터 (일정한 격자 또는 화소)\n\n\n속성정보\n\n주어진 위치에 있는 사건이나 사물에 대한 자료\n\n\n\n\n\n\n\n지리좌표체계 : 경도와 위도로 위치를 표현하는 지리좌표체계\n투영좌표체계 : 지도투영법을 적용하여 둥근 지구를 평면으로 변환한 후, 직각좌표체계를 이용하여 x좌표와 y좌표의 직각좌표체계로 위치를 표현\n\n원통도법, 원추도법, 평면도법이 있음.\nUTM 좌표체계, TM 좌표계, UTM-K 좌표계\n우리나라는 ITRF2000 지구중심좌표계를 따르고 타원체로는 GRS80 타원체를 적용\n\n\n\n\n\n\nshapefile\n\n.shp : 공간정보(점, 선, 다각형)\n.shx : geometry와 속성 정보 연결\n.dbf : 속성정보\n.drj : 좌표계 정보 저장\n.sbn : 위치 정보 저장\n\ngeojson : json 또는 xml 파일 포맷 필요요\n\n\n\n\n\n\n\n패키지\n\nsf : 지리 공간 벡터 데이터(vector data) 분석을 위한 패키지\nraster : 지리 공간 레스터 데이터(raster data)를 처리 및 분석하는데 사용\nspData : 37개의 지리 공간 데이터셋이 내장\nspDataLarge : 지리공간 데이터 샘플을 내장\n\nvignetee(package = \" \") : 설치된 모든 패키지에 대한 이용가능한 모든 목록을 출력\nst_as_sf() : st 데이터를 sf로 변환하는 함수\nst_centroid : 폴리곤의 중심점을 계산하는 함수\nplot 함수 위에 다른 지도 층을 추가 : plot() 함수 안에 add = TRUE 사용\n\n\n\n\nst_point() : A point\nst_linestring() : A linestring\nst_polygon() : A polygon\nst_multipoint() : A multipoint\nst_multilinestring() : A multilinestring\nst_multipolygon() : A multipolygon\nst_geometrycollection() : A geometry collection\n\n\n\n\n\nst_sfc() : 두 개의 지리특성(feature)을 하나의 칼럼 객체로 합치는 함수\nst_geometry_type() : 기하유형을 확인\nst_crs() : 특정 CRS를 지정\n\n특정 CRS를 지정하기 위해 epsg(SRID) 또는 proj4string 속성을 사용\n\nepsg 코드\n\n장점 : 짧아서 기억하기 쉬움\nsfc 객체 내의 모든 geometries는 동일한 CRS를 가져야 함.\nEPSG : 4326 : GPS가 사용하는 좌표계\n\nproj4string 정의\n\n장점 : 투사 유형이나 datum, 타원체 등의 다른 모수들을 구체화할 수 있는 유연성이 있음\n단점 : 사용자가 구체화를 해야하므로 길고 복잡하며 기억하기 어려움\n\nst_sf() : sfc와 class sf의 객체들을 하나로 통합\n\n\nlibrary(raster)\nlibrary(rgdal)\n\nraster_filepath &lt;- system.file(\"raster/srtm.tif\", package = \"spDataLarge\")\nnew_raster &lt;- raster(raster_filepath)\nnew_raster\n# class      : RasterLayer\n# dimensions : 457, 465, 212505  (nrow, ncol, ncell)\n# resolution : 0.0008333333, 0.0008333333  (x, y)\n# extent     : -113.2396, -112.8521, 37.13208, 37.51292  (xmin, xmax, ymin, ymax)\n# crs        : +proj=longlat +datum=WGS84 +no_defs\n# source     : srtm.tif\n# names      : srtm\n# values     : 1024, 2892  (min, max)\n\n\ndim() : 행, 열, 층의 수\nncell() : 셀의 수\nres() : 해상도\nextent() : 경계값\ncrs() : 좌표계\ninMemory() : 래스터 데이터가 메모리에 저장되어 있는지(논리값 출력)\n\n\n\n\n\nRasterLayer class\nRasterBrick Class\nRasterStack class\n\n\nRasterLayer : 한 개의 층으로 구성되어 있는 래스터\nRasterBrick : 여러개의 층으로 구성되어 있는 래스터\n\n단일 다중 스펙트럼 위성 파일, 메모리의 단일 다층 객체의 형태\nbrick() 함수를 사용하여 다층 래스터 파일을 로드\n\nRasterStack : 여러개의 층으로 구성되어 있는 래스터\nnlayers() : 래스터 데이터의 층의 수\n\n\n\n\nRasterBrick : 동일한 복수 개의 RasterLayer 층으로 구성\nRasterStack : 여러 개의 RasterLayer과 RasterBrick 객체가 혼합\n\n\n\n\n\nRasterBrick : 하나의 다층 래스터 파일이나 객체를 처리\nRasterStack : 여러 개의 래스터 파일들이나 여러 종류의 래스터 클래스를 한꺼번에 연걸해서 연산하고 처리\n\n\n\n\n\n\n지리 좌표계\n\n위도와 경도를 이용해 지구 표면의 위치를 정의\n미터가 아니라, 각도로 거리 측정\n타원 표면, 구면 표면\nWGS84\n\n투영(투사) 좌표계\n\n암묵적으로 “평평한 표면” 위의 데카르트 좌표 기반 -&gt; 왜곡 발생\n원점, x축, y축\n미터와 같은 선형 측정 단위\n평면, 원뿔, 원통의 3가지 투영 유형\n\nst_set_crs() : 좌표계가 비어있거나 잘못 입력되어 있는 경우에 좌표계를 설정\nst_transform() : 투영 데이터 변환\nst_area() : 벡터 데이터의 면적 계산 -&gt; [m^2] 단위가 같이 반환\n좌표계 설정할 때,\n\n벡터 데이터 : epsg코드나 proj4string정의 모두 사용 가능\n래스터 데이터 : proj4string 정의만 사용\n\n\n\n\n\n\n\n\n\n\nsf 객체에서 속성 정보만 가져오기 : st_drop_geometry()\n\n\nBase R 구문으로 벡터 데이터 속성 정보의 행과 열 가져오기\n\n\ndplyr로 벡터 데이터 속성 정보의 행과 열 가져오기\n\n\n한 개 컬럼만 가져온 결과를 벡터로 반환하기\n\n\n\n\n\n지리공간 sf 객체는 항상 점, 선, 면 등의 지리기하 데이터를 리스트로 가지고 있는 geometry 칼럼이 항상 따라다님\nsf 객체로부터 이 geometry 칼럼을 제거하고 나머지 속성 정보만으로 Dataframe을 만들고 싶다면 sf패키지의 st_drop_geometry()를 사용\ngeometry 칼럼의 경우 지리기하 점, 선, 면 등의 리스트 정보를 가지고 있어 메모리 점유가 크기때문에, 사용할 필요가 없다면 geometry 칼럼을 제거하고 속성 정보만으로 Dataframe으로 만들어서 분석을 진행하는게 좋음\n\n\n\n\n\nR Dataframe에서 i행과 j열을 가져올 때 : df[i, j], subset(), $을 사용\n\n\ni행과 j열 위치를 지정 ex) world[1:6, ]\n\n\nj행의 이름을 이용 ex) world[, c(\"name_long\", \"lifeExp\")]\n\n\n논리 벡터를 사용해서 i행의 부분집합 ex) sel_area &lt;- world$area_km2 &lt; 10000\n\n\n\n\n\n\n\ndplyr 패키지에서는 체인(%&gt;%)으로 파이프 연산자를 사용하여 가독성이 좋고, 속도가 빠름\n\n\nselect() 함수를 사용하여 특정 열 선택\n\n\nselect(sf, name)\nselect(sf, name1:name2)\nselect(sf, position) ex) select(world, 2, 7)\nselect(sf, -name)\nselect(sf, name_new = name_old) : 열 선택하여 이름 변경\nselect(sf, contain(string)) : 특정 문자열을 포함한 칼럼을 선택\n\ncontain(), starts_with(), ends_with(), matches(), num_range()\n\n\n\nfilter() 함수를 사용하여 조건을 만족하는 특정 행 추출\n\n\nsubset() 함수와 동일한 기능\n\n\naggregate() 함수를 사용하여 지리 벡터 데이터의 속성 정보를 그룹별로 집계\n\n\naggregate(x ~ group, FUN, data, ...)\ndata.frame을 반환하며, 집계된 결과에 지리 기하(geometry) 정보는 없음\nworld[‘pop’]은 “sf” 객체이기 때문에 집계 결과가 “sf” 객체로 반환\nworld$pop은 숫자형 벡터이므로 aggregate() 함수를 적용하면 집계 결과가 “data.frame”으로 반환\n\n\nsummarize(), group_by() 함수를 이용한 지리벡터 데이터의 속성 정보를 그룹별로 집계\n\n\ngroup_by() : 기준이 되는 그룹을 지정\nsummarize() : 다양한 집계 함수를 사용\n\nsum(), n() : 합계와 개수 집계\ntop_n() : 상위 n개 추출\narrange() : 오름차순 정렬, desc()를 사용하면 내림차순 정렬\nst_drop_geometry() : geometry 열 제거\n\n\n\n\n\n\n\n\nR의 sf클래스 객체인 지리공간 벡터 데이터를 dplyr의 함수를 사용해서 두 테이블을 join하면 속성과 함께 지리공간 geometry 칼럼과 정보도 join된 후의 테이블에 자동으로 그대로 따라감\n\n\n## 두 데이터 셋에 같은 이름을 가지는 변수가 없는 경우\n\n```         \na)  하나의 key variable의 이름을 바꿔서 통일시켜줌\n```\n\n-   \n\n    b)  `by`를 사용하여 결합변수를 지정\n\n\n\n# coffee_data의 name_long변수 이름을 nm으로 변경\ncoffee_renamed &lt;- rename(coffee_data, nm = name_long)\n# by 사용하여 결합 변수를 지정하여 다른이름변수를 기준으로 조인하기\nworld_coffee1 &lt;- left_join(world, coffee_renamed, by = c(name_long = \"nm\"))\n\n\ninner_join() 함수를 사용하면 겹치는 행만 추출\n\nsetdiff() : 일치하지 않는 행 추출\ngrepl() : 텍스트 찾는 함수 (논리값으로 출력)\ngrep() : 텍스트 찾는 함수 (행 번호 출력)\n\n\n\n\n\n\ndplyr로 지리 벡터 데이터에 새로운 속성 만들기\n\nmutate() : 기존 데이터 셋에 새로 만든 변수(열) 추가\ntransmute() : 기존의 열은 모두 제거하고 새로 만든 열과 지리기하 geometry열만을 반환\n\ntidyr로 지리 벡터 데이터의 기존 속성을 합치거나 분리하기\n\nunite(data, 병합 열, sep = \"_\", remove = TRUE) : 기존 속성 열을 합쳐서 새로운 속성 열을 만듦\n\nremove = TRUE를 설정해주면 기존의 합치려는 두 개의 열은 제거되고, 새로 만들어진 열만 남음\n\nseparate() : 기존에 존재하는 열을 구분자를 기준으로 두 개의 열로 분리\n\n\n\nworld_unite &lt;- world %&gt;%\n  unite(\"con_reg\", continent:region_un, sep = \":\", remove = TRUE)\nnames(world_unite)\n# \"iso_a2\"    \"name_long\" \"con_reg\"   \"subregion\" \"type\"\n# \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\" \"geom\"\n\nworld_separate &lt;- world_unite %&gt;%\n  separate(con_reg, c(\"continent\", \"region_un\"), sep = \":\")\nnames(world_separate)\n# \"iso_a2\"    \"name_long\" \"continent\" \"region_un\" \"subregion\" \"type\"\n# \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\" \"geom\" \n\ndplyr로 지리 벡터 데이터의 속성 이름 바꾸기\n\nrename(data, new_name = old_name) : 특정 속성 변수 이름 변경\nsetNames(object = nm, nm) : 여러개의 속성 칼럼을 한꺼번에 변경 또는 부여\n\n\nworld %&gt;% rename(name = name_long)\n\nnew_names &lt;- c(\"i\", \"n\", \"c\", \"r\", \"s\", \"t\", \"a\", \"p\", \"l\", \"gP\", \"geom\")\nworld %&gt;% setNames(new_names)\n\n\n\n\n\n\n\n래스터 객체의 데이터 속성은 숫자형(numeric), 정수형(integer), 논리형(logical), 요인형(factor) 데이터를 지원하며, 문자형(character)은 지원하지 않음\n\n1.  **문자형을 요인형으로 변환**(또는 논리형으로 변환) -\\&gt; `factor()` 함수 사용\n\n\n요인형 값을 속성 값으로 하여 래스터 객체를 만듦\n\n\n래스터 객체의 모든 값을 추출하거나 전체 행을 추출 : values(), getValues()\n\n\n\n\n\n\n\n\n\n\nst_intersects() : 공간 부분집합 추출(교집합)\n\n\n\n\n\n\n\n\n\n\nst_intersects() : 공간적으로 관련이 있는 객체를 출력\nst_disjoint() : 공간적으로 관련되지 않은 객체만 반환\nst_within() : 공간적으로 완전히 객체 내부에 있는 객체들만 출력\nst_touches() : 공간적으로 테두리에 있는 객체들만 출력\nst_is_within_distance() : 공간적으로 주어진 거리보다 가까운 객체들을 반환\nsparse = FALSE 매개변수를 설정하면 논리값으로 출력\n\n\nst_intersects(p, a)\n#&gt; Sparse geometry binary predicate list of length 4, where the predicate\n#&gt; was `intersects'\n#&gt;  1: 1\n#&gt;  2: 1\n#&gt;  3: (empty)\n#&gt;  4: (empty)\n\nst_intersects(p, a, sparse = FALSE)\n#&gt;       [,1]\n#&gt; [1,]  TRUE\n#&gt; [2,]  TRUE\n#&gt; [3,] FALSE\n#&gt; [4,] FALSE\n\nst_disjoint(p, a, sparse = FALSE)[, 1]\n#&gt; [1] FALSE FALSE  TRUE  TRUE\n\nst_within(p, a, sparse = FALSE )[, 1]\n#&gt; [1]  TRUE FALSE FALSE FALSE\n\nst_touches(p, a, sparse = FALSE)[, 1]\n#&gt; [1] FALSE  TRUE FALSE FALSE\n\nsel &lt;- st_is_within_distance(p, a, dist = 0.9) # can only return a sparse matrix\nlengths(sel) &gt; 0\n#&gt; [1]  TRUE  TRUE FALSE  TRUE\n\n\n\n\n\nst_join() : 공간 결합 함수\n\n\nrandom_joined = st_join(random_points, world[\"name_long\"]) ; random_joined\n#&gt; Simple feature collection with 10 features and 1 field\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -158.1893 ymin: -42.91501 xmax: 165.1157 ymax: 80.5408\n#&gt; Geodetic CRS:  WGS 84\n#&gt; # A tibble: 10 × 2\n#&gt;                 geometry name_long\n#&gt;  *           &lt;POINT [°]&gt; &lt;chr&gt;    \n#&gt;  1 (-58.98475 -21.24278) Paraguay \n#&gt;  2  (-13.05963 25.42744) Morocco  \n#&gt;  3   (-158.1893 80.5408) &lt;NA&gt;     \n#&gt;  4  (-108.9239 27.80098) Mexico   \n#&gt;  5   (-9.246895 49.9822) &lt;NA&gt;     \n#&gt;  6  (-71.62251 20.15883) &lt;NA&gt;     \n#&gt;  7  (38.43318 -42.91501) &lt;NA&gt;     \n#&gt;  8  (-133.1956 6.053818) &lt;NA&gt;     \n#&gt;  9   (165.1157 38.16862) &lt;NA&gt;     \n#&gt; 10   (16.86581 53.86485) Poland\n\n\n\n\n\n\n기호(plotting symbols, characters) : pch\n기호의 크기 : cex\n선 두께 : lwd\n선 유형 : lty\n\n\n\n\nany() : 특정 값이 포함되어 있는지 확인할 때 유용, 여기서 TRUE가 있는지 확인 가능\n\n\nany(st_touches(cycle_hire, cycle_hire_osm, sparse = FALSE))\n#&gt; [1] FALSE\n\n\nlibrary(mapview)\nlibrary(tmap)\ntmap_mode(\"view\")\ntm_basemap(\"Stamen.Terrain\") +\n  tm_shape(cycle_hire) +\n  tm_symbols(col = \"red\", shape = 16, size = 0.5, alpha = .5) +\n  tm_shape(cycle_hire_osm) +\n  tm_symbols(col = \"blue\", shape = 16, size = 0.5, alpha = .5) +\n  tm_tiles(\"Stamen.TonerLabels\")\n\n\n\n\n\n\n\nst_transform() : 투영데이터로 변환을 위한 함수\nst_is_within_distance() : 임계 거리보다 가까운 객체들을 반환\n\n\ncycle_hire_P &lt;- st_transform(cycle_hire, 27700)\ncycle_hire_osm_P &lt;- st_transform(cycle_hire_osm, 27700)\nsel &lt;- st_is_within_distance(cycle_hire_P, cycle_hire_osm_P, dist = 20)\nsummary(lengths(sel) &gt; 0)\n#&gt;    Mode   FALSE    TRUE \n#&gt; logical     304     438\n\n\nst_join()을 사용하여 dist 인수를 추가하여 구할 수도 있음\n\nst_join()을 사용하면 조인된 결과의 행 수가 더 크다.\n이는 cycle_hire_P의 일부 자전거 대여소가 cycle_hire_osm_P와 여러개가 겹치기 때문임\n겹치는 점에 대한 값을 집계하고 평균을 반환하여 문제를 해결 가능\n\n\nz = st_join(cycle_hire_P, cycle_hire_osm_P,\n            join = st_is_within_distance, dist = 20)\nnrow(cycle_hire) ; nrow(z)\n#&gt; [1] 742\n#&gt; [1] 762\n\nz = z %&gt;%\n  group_by(id) %&gt;%\n  summarize(capacity = mean(capacity))\nnrow(z) == nrow(cycle_hire)\n#&gt; [1] TRUE\n\n\n\n\n\n\naggregate()와 group_by() %&gt;% summarize()를 활용하여 그룹별 통계값 계산(평균, 합 등)\n\n\n# aggregate() 사용\nnz_avheight &lt;- aggregate(x = nz_height, by = nz, FUN = mean)\nplot(nz_avheight[2])\n\n\n\n# group_by() %&gt;% summarize() 사용\nnz_avheight2 &lt;- nz %&gt;%\n  st_join(nz_height) %&gt;%\n  group_by(Name) %&gt;%\n  summarize(elevation = mean(elevation, na.rm = TRUE))\nplot(nz_avheight2[2])\n\n\n\n\n\nst_interpolate_aw() : 면적의 크기에 비례하게 계산(면적 가중 공간 보간)\n\n\nsum(incongruent$value)\n#&gt; [1] 45.41184\n\nagg_aw = st_interpolate_aw(incongruent[, \"value\"],\n                           aggregating_zones,\n                           extensive = TRUE)\n#&gt; Warning in st_interpolate_aw.sf(incongruent[, \"value\"], aggregating_zones, :\n#&gt; st_interpolate_aw assumes attributes are constant or uniform over areas of x\nagg_aw$value\n#&gt; [1] 19.61613 25.66872\n\n\n\n\n\n위상 관계는 binary인 반면 거리 관계는 연속적임\nst_distance() : 두 객체 사이의 거리 계산\n\n\nnz_heighest &lt;- nz_height %&gt;% top_n(n = 1, wt = elevation)\ncanterbury_centroid &lt;- st_centroid(canterbury)\n#&gt; Warning: st_centroid assumes attributes are constant over geometries\n\nst_distance(nz_heighest, canterbury_centroid)\n#&gt; Units: [m]\n#&gt;        [,1]\n#&gt; [1,] 115540\n\nco &lt;- filter(nz, grepl(\"Canter|Otag\", Name))\nst_distance(nz_height[1:3, ], co)\n#&gt; Units: [m]\n#&gt;           [,1]     [,2]\n#&gt; [1,] 123537.16 15497.72\n#&gt; [2,]  94282.77     0.00\n#&gt; [3,]  93018.56     0.00\n\nplot(st_geometry(co)[2])\nplot(st_geometry(nz_height)[2:3], add = TRUE)\n\n\n\n\n\n\n\n\n\n\n\ncellFromXY() or raster::extract() : 좌표값을 Cell ID로 변환\n\n\n\n\n\n\n\nid = cellFromXY(elev, xy = matrix(c(0.1, 0.1), ncol = 2))\nelev[id]\n#&gt;   elev\n#&gt; 1   16\nterra::extract(elev, matrix(c(0.1, 0.1), ncol = 2))\n#&gt;   elev\n#&gt; 1   16\n\nclip = rast(xmin = 0.9, xmax = 1.8, ymin = -0.45, ymax = 0.45,\n            res = 0.3, vals = rep(1, 9))\nelev[clip]\n#&gt;   elev\n#&gt; 1   18\n#&gt; 2   24\nterra::extract(elev, ext(clip))\n\n\noperator는 raster의 다양한 inputs을 받고, drop=FALSE로 설정했을 때, raster 객체를 반환\n\n\nelev[1:2]\n#&gt;   elev\n#&gt; 1    1\n#&gt; 2    2\nelev[2, 1:2]\n#&gt;   elev\n#&gt; 1    7\n#&gt; 2    8\nelev[1:2, drop = FALSE] # spatial subsetting with cell IDs\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 1, 2, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 0.5, 0.5  (x, y)\n#&gt; extent      : -1.5, -0.5, 1, 1.5  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : lon/lat WGS 84 (EPSG:4326) \n#&gt; source(s)   : memory\n#&gt; name        : elev \n#&gt; min value   :    1 \n#&gt; max value   :    2\nelev[2, 1:2, drop = FALSE] # spatial subsetting by row,column indices\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 1, 2, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 0.5, 0.5  (x, y)\n#&gt; extent      : -1.5, -0.5, 0.5, 1  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : lon/lat WGS 84 (EPSG:4326) \n#&gt; source(s)   : memory\n#&gt; name        : elev \n#&gt; min value   :    7 \n#&gt; max value   :    8\n\n\n\n\n\nelev + elev # 더하기\nelev^2      # 제곱\nlog(elev)   # 로그\nelev &gt; 5    # 논리\n\n\n\n\n\n\n\ntmap을 plot하기 위해서는 우선 tm_shape()로 지정해야하며, + 연산자로 레이어를 추가해야함\n\nex) tm_polygons(), tm_raster(), tm_borders(), tm_symbols() 등\n\nInteractive maps : tmap_mode()를 사용하여 \"plot\",과 \"view\"모드 사용 가능\nFacet : 하나의 창에 여러 맵을 동시에 그리기\n\nFacet 하는 3가지 방법\n\n여러변수 이름 추가\nby argument of tm_facets로 공간 데이터를 나누기\ntmap_arrange() 사용\n\n\n\ntm_basemap() : 지도를 표현할 수 있는 바탕이 되는 지도\n\n\n\n# 1. 여러 변수 이름 추가\ntmap_mode(\"plot\")\ndata(World)\ntm_shape(World) +\n  tm_polygons(c(\"HPI\", \"economy\")) +\n  tm_facets(sync = TRUE, ncol = 2)\n\n\n\n\n\n# 2. by argument of `tm_facets`로 공간 데이터 나누기\ntmap_mode(\"plot\")\ndata(NLD_muni)\nNLD_muni$perc_men &lt;- NLD_muni$pop_men / NLD_muni$population * 100\ntm_shape(NLD_muni) +\n  tm_polygons(\"perc_men\", palette = \"RdYlBu\") +\n  tm_facets(by = \"province\")\n\n\n\n\n\n# 3. `tmap_arrange` 함수 사용 : 각각 그린다음에 배치\ntmap_mode(\"plot\")\ndata(NLD_muni)\ntm1 &lt;- tm_shape(NLD_muni) + tm_polygons(\"population\", convert2density = TRUE)\ntm2 &lt;- tm_shape(NLD_muni) + tm_bubbles(size = \"population\")\ntmap_arrange(tm1, tm2)\n\n\n\n\n\ntmap_mode(\"view\")\ndata(World, metro, rivers, land)\ntm_basemap(\"Stamen.Watercolor\") +\n  tm_shape(metro) + tm_bubbles(size = \"pop2020\", col = \"red\") +\n  tm_tiles(\"Stamen.TonerLabels\")\n\n\n\n\n\n\n\nOption and styles\n\ntm_layout() : map layout 지정\ntm_options() 내에서 설정\n\ntmap_options_diff() : default tmap options과 차이점 출력\ntmap_options_reset() : default tmap options으로 설정\n\nreset을 해주지 않으면 option이 계속 설정되어있음\n\n\ntmap_style() : 지도 스타일 설정\n\n\ntmap_mode(\"plot\")\ntm_shape(World) +\n  tm_polygons(\"HPI\") +\n  tm_layout(bg.color = \"skyblue\", inner.margins = c(0, .02, .02, .02))\n\n\n\n\n\ntmap_options(bg.color = \"black\", legend.text.color = \"white\")\ntm_shape(World) + tm_polygons(\"HPI\", legend.title = \"Happy Planet Index\")\n\n\n\n\n\ntmap_style(\"classic\")\n## tmap style set to \"classic\"\n## other available styles are: \"white\", \"gray\", \"natural\", \"cobalt\",\n## \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"watercolor\"\n\ntm_shape(World) +\n  tm_polygons(\"HPI\", legend.title = \"Happy Planet Index\")\n\n\n\n\nExporting maps\n\n\ntm &lt;- tm_shape(World) +\n  tm_polygons(\"HPI\", legend.title = \"Happy Planet Index\")\n\n## save an image (\"plot\" mode)\ntmap_save(tm, filename = \"./Spatial_Information_Analysis/world_map.png\")\n\n## save as stand-alone HTML file (\"view\" mode)\ntmap_save(tm, filename = \"./Spatial_Information_Analysis/world_map.html\")\n\n\nQuick thematic map\n\n\nqtm(World, fill = \"HPI\", fill.pallete = \"RdYlGn\")\n\n\n\n\n\n\n\n\n\n\n\n\n단순화는 일반적으로 더 작은 축척 지도에서 사용하기 위한 벡터 객체(선, 다각형)의 일반화를 위한 프로세스\nst_simplify() : 정점을 제거하여 선을 단순화시킴\n\ndTolerance : 단위가 m이며 커질수록 더 단순화\n\n\nseine_simp &lt;- st_simplify(seine, dTolerance = 2000) # 2000m\nplot(seine)\nplot(seine_simp)\nobject.size(seine) ; object.size(seine_simp)\n#&gt; 18096 bytes  9112 bytes\n\n\n\n\n\n\n단순화는 다각형에도 적용 가능\nst_simplify()를 사용하였을 때, 영역이 겹치는 경우도 발생\nrmapshaper 패키지의 ms_simplify() 함수를 사용\nkeep_shapes = TRUE : 개체 수는 그대로 유지\n\n\nus_states\nus_states2163 &lt;- st_transform(us_states, 2163)\nus_states2163\n\nus_states_simp1 &lt;- st_simplify(us_states2163, dTolerance = 100000)\nplot(us_states[1])\nplot(us_states_simp1[1])\n\nus_states2163$AREA &lt;- as.numeric(us_states2163$AREA)\n\nlibrary(rmapshaper)\nus_states_simp2 &lt;- rmapshaper::ms_simplify(us_states2163, keep = 0.01,\n                                           keep_shapes = FALSE)\nplot(us_states_simp2[1])\n\n\n\n\n\n\n\n\n\n\n가장 일반적으로 사용되는 중심 연산은 지리적 중심 : 공간객체의 질량 중심\nst_centroid() : 지리적 중심을 생성하지만, 때때로 지리적 중심이 상위 개체의 경계를 벗어나는 경우가 발생\nst_point_on_surface() : 상위 개체 위에 중심이 생성\n\n\nnz_centroid &lt;- st_centroid(nz)\nseine_centroid &lt;- st_centroid(seine)\n\nnz_pos &lt;- st_point_on_surface(nz)\nseine_pos &lt;- st_point_on_surface(seine)\n\nplot(st_geometry(nz), main = \"nz\")\nplot(nz_centroid ,add=T, col=\"black\")\nplot(nz_pos ,add=T, col=\"red\")\n\nplot(st_geometry(seine), main = \"seine\")\nplot(seine_centroid ,add=T, col=\"black\")\nplot(seine_pos ,add=T, col=\"red\")\n\n\n\n\n\n\n\n\n\n\n버퍼 : 기하학적 특징의 주어진 거리 내 영역을 나타내는 다각형\n지리데이터 분석에 자주 활용됨\nst_buffer() : 버퍼 생성 함수, 최소 두 개의 인수가 필요함\n\n\nseine_buff_5km &lt;- st_buffer(seine, joinStyle = \"ROUND\", dist = 5000)\nseine_buff_20km &lt;- st_buffer(seine, dist = 20000)\n\nplot(seine,col=\"black\", reset = FALSE)\nplot(seine_buff_5km, col=adjustcolor(1:3, alpha = 0.2), add=T)\n\nplot(seine,col=\"black\", reset = FALSE)\ncol1 &lt;- adjustcolor(\"red\", alpha=0.2)\ncol2 &lt;- adjustcolor(\"blue\", alpha=0.2)\ncol3 &lt;- adjustcolor(\"green\", alpha=0.2)\nplot(seine_buff_20km, col=c(col1,col2,col3), add=T)\n\n\n\n\n\n\n\n\n\n\n왜곡되거나 잘못 투영된 지도를 기반으로 생성된 geometry를 재투영하거나 개선할 때 많은 Affine 변환이 적용\n이동 : 맵 단위로 모든 포인트가 동일한 거리만큼 이동\n\n\nnz_sfc &lt;- st_geometry(nz)\nnz_shift &lt;- nz_sfc + c(0, 100000)\nplot(nz_sfc)\nplot(nz_shift,add=T, col=\"Red\")\n\n\n\n\n\n배율 조정 : 개체를 요소만큼 확대하거나 축소\n\n모든 기하 도형의 토폴로지 관계를 그대로 유지하면서 원점 좌표와 관련된 모든 좌표값을 늘리거나 줄일 수 있음\n중심점을 기준으로 기하 도형의 차이 만큼을 늘리고 0.5배 줄인 다음 다시 중심점을 더해줌\n\n\nnz_centroid_sfc &lt;- st_centroid(nz_sfc)\nnz_scale &lt;- (nz_sfc - nz_centroid_sfc) * 0.5 + nz_centroid_sfc\n\nplot(nz_sfc)\nplot(nz_scale, add=T, col=\"Red\")\n\n\n\n\n회전 : 2차원 좌표의 회전하기 위한 회전변환행렬\n\n\nmatrix(c(cos(30), sin(30), -sin(30), cos(30)), nrow = 2, ncol = 2)\n#&gt;            [,1]      [,2]\n#&gt; [1,]  0.1542514 0.9880316\n#&gt; [2,] -0.9880316 0.1542514\n\nrotation &lt;- function(a){\n  r = a * pi / 180 #degrees to radians\n  matrix(c(cos(r), sin(r), -sin(r), cos(r)), nrow = 2, ncol = 2)\n}\nnz_rotate &lt;- (nz_sfc - nz_centroid_sfc) * rotation(30) + nz_centroid_sfc\n\nplot(nz_sfc)\nplot(nz_rotate, add=T, col=\"red\")\n\n\n\n\n\n\n\n\n공간 클리핑은 영향을 받는 일부 형상의 지오메트리 열의 변경을 수반하는 공간 부분 집합의 한 형태\n\n\nb &lt;- st_sfc(st_point(c(0, 1)), st_point(c(1, 1))) # create 2 points\nb &lt;- st_buffer(b, dist = 1) # convert points to circles\nplot(b, border = \"grey\")\ntext(x = c(-0.5, 1.5), y = 1, labels = c(\"x\", \"y\"), cex = 3) # add text\n\n\n\n\n\nst_intersection() : X∩Y (x와 y의 교집합)\nst_difference() : X-Y (x와 y의 차집합)\nst_union() : X∪Y (x와 y의 합집합)\nst_sym_difference() : (X∩Y)^c (드모르간의 법칙)\n\n\npar(mfrow = c(2,2))\n\nx &lt;- b[1] ; y &lt;- b[2]\n\n# X ∩ Y\nx_and_y &lt;- st_intersection(x, y)\nplot(b, border = \"grey\", main = \"X ∩ Y\")\nplot(x_and_y, col = \"lightgrey\", border = \"grey\", add = TRUE)\ntext(x = c(-0.5, 1.5), y = 1, labels = c(\"x\", \"y\"), cex = 3)\n\n# X - Y\nx_dif_y &lt;- st_difference(x,y)\nplot(b, border = \"grey\", main = \"X - Y\")\nplot(x_dif_y, col = \"lightgrey\", border = \"grey\", add = TRUE)\ntext(x = c(-0.5, 1.5), y = 1, labels = c(\"x\", \"y\"), cex = 3)\n\n# X U Y\nx_union_y &lt;- st_union(x,y)\nplot(b, border = \"grey\", main = \"X U Y\")\nplot(x_union_y, col = \"lightgrey\", border = \"grey\", add = TRUE)\ntext(x = c(-0.5, 1.5), y = 1, labels = c(\"x\", \"y\"), cex = 3)\n\n# (X ∩ Y)^c\nx_sdif_y &lt;- st_sym_difference(x,y)\nplot(b, border = \"grey\", main = \"(X ∩ Y)^c\")\nplot(x_sdif_y, col = \"lightgrey\", border = \"grey\", add = TRUE)\ntext(x = c(-0.5, 1.5), y = 1, labels = c(\"x\", \"y\"), cex = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n클리핑 오브젝트는 지오메트리를 변경할 수 있지만 오브젝트의 부분 집합을 지정할 수도 있으며 클리핑/하위 설정 오브젝트와 교차하는 피쳐만 반환할 수도 있음\nst_sample() : x와 y의 범위 내에서 점들의 간단한 무작위 분포를 생성\n\n\nbb = st_bbox(st_union(x, y))\nbox = st_as_sfc(bb)\nset.seed(2017)\n\np = st_sample(x = box, size = 10)\nx_and_y = st_intersection(x, y)\n\nplot(b, border = \"grey\")\nplot(p, add=T)\n\n\n\n\n\nX와 Y 둘 다와 교차하는 점만을 반환하는 방법\n\n\n\n## 1번째방법\np_xy1 &lt;- p[x_and_y]\nplot(p_xy1, add=T, col=\"red\")\n\n## 2번째방법\np_xy2 &lt;- st_intersection(p, x_and_y)\nplot(p_xy2, add=T, col=\"blue\")\n\n## 3번째방법\nsel_p_xy &lt;- st_intersects(p, x, sparse = FALSE)[, 1] &\n  st_intersects(p, y, sparse = FALSE)[, 1]\np_xy3 &lt;- p[sel_p_xy]\nplot(p_xy3, add=T, col=\"green\")\n\n\n\n\n\n\n\n\n\n\n미국의 49개 주의 정보를 4개 지역으로 재구분\n\n\nplot(us_states[6])\n\n\n\n## 1. aggregate함수\nregions &lt;- aggregate(x = us_states[, \"total_pop_15\"], by = list(us_states$REGION),\n                     FUN = sum, na.rm = TRUE)\nplot(regions[2])\n\n\n\n## 2. group_by, summarize함수\nregions2 &lt;- us_states %&gt;% group_by(REGION) %&gt;%\n  summarize(pop = sum(total_pop_15, na.rm = TRUE))\n\nplot(regions2[2])\n\n\n\n\n\n위에서 aggregate()와 summarize()가 모두 지오메트리를 결합하고 st_union()을 사용하면 지오메트리만을 분해\n\n\nus_west &lt;- us_states[us_states$REGION == \"West\", ]\nplot(us_west[6])\n\n\n\nus_west_union &lt;- st_union(us_west)\nplot(us_west_union)\n\n\n\ntexas &lt;- us_states[us_states$NAME == \"Texas\", ]\ntexas_union &lt;- st_union(us_west_union, texas)\nplot(texas_union)\n\n\n\n\n\n\n\n\nst_cast() : 지오메트리 유형을 변환\n\n\nmultipoint &lt;- st_multipoint(matrix(c(1, 3, 5, 1, 3, 1), ncol = 2))\nlinestring &lt;- st_cast(multipoint, \"LINESTRING\")\npolyg &lt;- st_cast(multipoint, \"POLYGON\")\n\nplot(multipoint)\nplot(linestring)\nplot(polyg)\n\nst_length(linestring) # 길이 계산\n# [1] 5.656854\nst_area(polyg) # 면적 계산\n# [1] 4\n\n\n\n\n\n\n\nmultilinestring : 여러 개의 linestring을 하나의 묶음으로 처리\n\n\n\n\n\n\n\nmultilinestring은 각 선 세그먼트에 이름을 추가하거나 단일 선 길이를 계산할 수 없는 등 수행할 수 있는 작업 수가 제한됨\nst_cast() 함수를 사용하여 하나의 multilinestring을 세 개의 linestring로 분리\n\n\nlinestring_sf2 = st_cast(multilinestring_sf, \"LINESTRING\")\nlinestring_sf2\n#&gt; Simple feature collection with 3 features and 0 fields\n#&gt; Geometry type: LINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 1 ymin: 1 xmax: 4 ymax: 5\n#&gt; CRS:           NA\n#&gt;                    geom\n#&gt; 1 LINESTRING (1 5, 4 3)\n#&gt; 2 LINESTRING (4 4, 4 1)\n#&gt; 3 LINESTRING (2 2, 4 2)\n\n\nname과 length 추가\n\n\nlinestring_sf2$name &lt;- c(\"Riddle Rd\", \"Marshall Ave\", \"Foulke St\")\nlinestring_sf2$length &lt;- st_length(linestring_sf2)\nlinestring_sf2\n#&gt; Simple feature collection with 3 features and 2 fields\n#&gt; Geometry type: LINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 1 ymin: 1 xmax: 4 ymax: 5\n#&gt; CRS:           NA\n#&gt;                    geom         name   length\n#&gt; 1 LINESTRING (1 5, 4 3)    Riddle Rd 3.605551\n#&gt; 2 LINESTRING (4 4, 4 1) Marshall Ave 3.000000\n#&gt; 3 LINESTRING (2 2, 4 2)    Foulke St 2.000000\nplot(linestring_sf2[2])\n\n\n\n\n\n\n\n\n\n\n\n다른 공간 객체에 의해 중첩된 래스터에서 값을 추출하는 방법\n공간 출력을 검색하기 위해 거의 동일한 부분 집합 구문(많이 겹치는 부분)을 사용\ndrop = FALSE를 설정하여 행렬 구조를 유지\ncell 중간점이 clip과 겹치는 셀을 포함하는 래스터 개체를 반환\n\n\nelev &lt;- rast(system.file(\"raster/elev.tif\", package = \"spData\"))\nclip &lt;- rast(xmin = 0.9, xmax = 1.8, ymin = -0.45, ymax = 0.45,\n             resolution = 0.3, vals = rep(1, 9))\nplot(elev)\nplot(clip, add=T)\n\n\n\nelve_clip &lt;- elev[clip, drop = FALSE]\nplot(elve_clip)\n\n\n\nelev_raster &lt;- rast(system.file(\"raster/elev.tif\", package = \"spData\"))\nrcc &lt;- vect(xyFromCell(elev_raster, cell = 1:ncell(elev_raster))) # 셀의 중앙점 표시\nxyFromCell(elev_raster,1) # 1번 셀의 중앙점 좌표\n#&gt;          x    y\n#&gt; [1,] -1.25 1.25\nplot(elev)\nplot(rcc,add=T)\nplot(clip, add=T)\n\n\n\n\n\n\n\n\n다른 투사 및 해상도를 가진 두 이미지를 병합하려할 때 사용\nextend() : 래스터 범위 확장\n\n새로 추가된 행과 열은 값 매개변수의 기본값(예 : NA)를 가짐\n\norigin() : 래스터의 원점 좌표를 반환\n\n래스터의 원점은 좌표(0,0)에 가장 가까운 셀 모서리\n\n\n\nelev &lt;- rast(system.file(\"raster/elev.tif\", package = \"spData\"))\nelev_2 &lt;- extend(elev, c(1,2), snap=\"near\") # 아래/위 1행, 좌/우 2열 확장\nplot(elev)\n\n\n\nplot(elev_2, colNA=\"gray\")\n\nelev_3 &lt;- elev + elev_2\n#&gt; Error: [+] extents do not match\n\nelev_4 &lt;- extend(elev, elev_2)\nplot(elev_4, colNA=\"gray\")\n\norigin(elev_4)\n#&gt; [1] 0 0\n\norigin(elev_4) &lt;- c(0.25, 0.25)\nplot(elev_4, colNA=\"black\", add=T)\n\n\n\n\n\n\n\n\n\n래스터 데이터 셋은 해상도가 서로 다를 수 있음\n해상도를 match 시키기 위해 하나의 래스터 해상도를 감소(aggregate())시키거나 증가(disagg()) 시켜야 함\n\n\n# devtools::install_github(\"geocompr/geocompkg\")\ndem &lt;- rast(system.file(\"raster/dem.tif\", package = \"spDataLarge\"))\ndem_agg &lt;- aggregate(dem, fact = 5, fun = mean)\ndem_disagg &lt;- disagg(dem_agg, fact = 5, method = \"bilinear\")\nplot(dem)\n\n\n\nplot(dem_agg)\n\n\n\nplot(dem_disagg)\n\n\n\nidentical(dem, dem_disagg)\n#&gt; [1] FALSE\n\n\n새롭게 만들어지는 cell의 값을 만드는 두가지 방법\n\nDefault method(method = “near”) : 입력 셀의 값을 모든 출력 셀에 제공\nbilinear method : 입력 이미지의 가장 가까운 4개의 픽셀 중심을 사용하여 거리에 의해 가중된 평균을 계산\n\n\n\n\n\n\nResampling : 원래 그리드에서 다른 그리드로 래스터 값을 전송하는 프로세스\n이 프로세스는 원래 래스터의 값을 가지고, 사용자 지정 해상도와 원점을 가지고 대상 래스터의 새 값을 다시 계산함\n해상도/원점이 다른 래스터의 값을 재계산(추정)하는 방법\n\nNearest neighbor : 원래 래스터의 가장 가까운 셀 값을 대상 래스터의 셀에 할당. 속도가 빠르고 일반적으로 범주형 래스터에 적합\nBilinear interpolation(이중선형보간) : 원래 래스터에서 가장 가까운 4개의 셀의 가중 평균을 대상 1개의 셀에 할당. 연속 래스터를 위한 가장 빠른 방법\nCubic interpolation(큐빅 보간) : 본 래스터의 가장 가까운 16개 셀의 값을 사용하여 출력 셀 값을 결정하고 3차 다항식 함수를 적용. 연속 래스터에 사용. 2선형 보간보다 더 매끄러운 표면을 만들지만, 계산적으로 까다로움\nCubic spline interpolation(큐빅 스플라인 보간) : 원래 래스터의 가장 가까운 16개의 셀의 값을 사용하여 출력 셀 값을 결정하지만 큐빅 스플라인(3차 다항식 함수)을 적용\nLanczos windowed sinc resampling(Lanczos 윈도우 재샘플링) : 원래 래스터의 가장 가까운 셀 36개의 값을 사용하여 출력 셀 값을 결정\nsum\nmin, q1, med, q3, max, average, mode, rms\n\nNearest neighbor은 범주형 래스터에 적합한 반면, 모든 방법은 연속형 래스터에 사용\nresample(x, y, method = \"bilinear\", filename = \"\", ...) : 리샘플링 함수\n\n\nlibrary(terra)\n\ntarget_rast &lt;- rast(xmin = 794600, xmax = 798200,\n                    ymin = 8931800, ymax = 8935400,\n                    resolution = 150, crs = \"EPSG:32717\")\ntarget_rast\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 24, 24, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 150, 150  (x, y)\n#&gt; extent      : 794600, 798200, 8931800, 8935400  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : WGS 84 / UTM zone 17S (EPSG:32717)\n\nplot(dem)\n\n\n\nplot(target_rast)\n\n\n\n\n\n\"near\" : 셀에 가장 가까운 픽셀에서 값을 가져옴\n\n\ndem_resampl_1 &lt;- resample(dem, y = target_rast, method = \"near\")\nplot(dem_resampl_1)\n\n\n\n\n\n\"bilinear\" : 네 개의 가장 가까운 셀의 가중 평균\n\n\ndem_resampl_2 &lt;- resample(dem, y = target_rast, method = \"bilinear\")\nplot(dem_resampl_2)\n\n\n\n\n\n\"average\" : 각각의 새로운 셀이 중복되는 모든 입력 셀의 가중 평균\n\n\ndem_resampl_3 &lt;- resample(dem, y = target_rast, method = \"average\")\nplot(dem_resampl_3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n입력 래스터 데이터 세트의 범위가 관심 영역보다 클 경우 래스터 자르기(Cropping) 및 마스킹(Masking)은 입력 데이터의 공간 범위를 통합하는 데 유용함\n두 작업 모두 후속 분석 단계에 대한 객체 메모리 사용 및 관련 계산 리소스를 줄이고 래스터 데이터를 포함하는 매력적인 맵을 만들기 전에 필요한 전처리 단계임\n대상 개체와 자르기 개체는 모두 동일한 투영을 가져야 함\ncrop() : 두 번째 인수에 대한 래스터를 잘라냄\nmask() : 두 번째 인수에 전달된 개체의 경계를 벗어나는 값을 NA로 설정\n\n대부분의 경우 crop()과 mask()를 함께 사용\n\n\nsrtm &lt;- rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\nzion &lt;- read_sf(system.file(\"vector/zion.gpkg\", package = \"spDataLarge\"))\nzion &lt;- st_transform(zion, crs(srtm)) # zion을 srtm 좌표계랑 동일하게\nplot(srtm)\nplot(vect(zion),add=T)\n\n\n\nsrtm_cropped &lt;- crop(srtm, vect(zion))\nplot(srtm_cropped)\n\n\n\nsrtm_masked &lt;- mask(srtm, vect(zion))\nplot(srtm_masked)\n\n\n\nsrtm_cropped &lt;- crop(srtm, vect(zion))\nsrtm_final &lt;- mask(srtm_cropped, vect(zion))\nplot(srtm_final)\n\n\n\n\nupdatevalue = 0 : 외부의 모든 픽셀이 0으로 설정\ninverse = TRUE : 경계 내에 있는 것들이 마스킹\n\n\nsrtm_update0 &lt;- mask(srtm, vect(zion), updatevalue = 0)\nplot(srtm_update0)\n\n\n\nsrtm_inv_masked &lt;- mask(srtm, vect(zion), inverse = TRUE)\nplot(srtm_inv_masked)\n\n\n\n\n\n\n\n## Original / Crop / Mask / Inverse Map\nlibrary(tmap)\nlibrary(rcartocolor)\n\nterrain_colors = carto_pal(7, \"Geyser\")\n\npz1 = tm_shape(srtm) +\n  tm_raster(palette = terrain_colors, legend.show = FALSE, style = \"cont\") +\n  tm_shape(zion) +\n  tm_borders(lwd = 2) +\n  tm_layout(main.title = \"A. Original\", inner.margins = 0)\n\npz2 = tm_shape(srtm_cropped) +\n  tm_raster(palette = terrain_colors, legend.show = FALSE, style = \"cont\") +\n  tm_shape(zion) +\n  tm_borders(lwd = 2) +\n  tm_layout(main.title = \"B. Crop\", inner.margins = 0)\n\npz3 = tm_shape(srtm_masked) +\n  tm_raster(palette = terrain_colors, legend.show = FALSE, style = \"cont\") +\n  tm_shape(zion) +\n  tm_borders(lwd = 2) +\n  tm_layout(main.title = \"C. Mask\", inner.margins = 0)\n\npz4 = tm_shape(srtm_inv_masked) +\n  tm_raster(palette = terrain_colors, legend.show = FALSE, style = \"cont\") +\n  tm_shape(zion) +\n  tm_borders(lwd = 2) +\n  tm_layout(main.title = \"D. Inverse mask\", inner.margins = 0)\n\ntmap_arrange(pz1, pz2, pz3, pz4, ncol = 4, asp = NA)\n\n\n\n\n\n\n\n\n\n특정 위치에 있는 대상 래스터와 관련된 값을 식별하여 반환\n\n\ndata(\"zion_points\", package = \"spDataLarge\")\nelevation &lt;-terra::extract(srtm, vect(zion_points))\nzion_points &lt;- cbind(zion_points, elevation)\nplot(srtm)\nplot(vect(zion),add=T)\nplot(zion_points,col=\"black\", pch = 19, cex = 0.5, add=T)\n#&gt; Warning in plot.sf(zion_points, col = \"black\", pch = 19, cex = 0.5, add = T):\n#&gt; ignoring all but the first attribute\n\n\n\n\n\nst_segmentize() : 제공된 density로 line을 따라 point를 추가\n\ndfMaxLength : 최대 점의 개수\n\nst_cast() : 추가된 point를 “POINT” 형식으로 변환\n\n\nzion_transect &lt;- cbind(c(-113.2, -112.9), c(37.45, 37.2)) %&gt;%\n  st_linestring() %&gt;%\n  st_sfc(crs = crs(srtm)) %&gt;%\n  st_sf()\nzion_transect$id &lt;- 1:nrow(zion_transect)\nzion_transect &lt;- st_segmentize(zion_transect, dfMaxLength = 250)\nzion_transect &lt;- st_cast(zion_transect, \"POINT\")\n#&gt; Warning in st_cast.sf(zion_transect, \"POINT\"): repeating attributes for all\n#&gt; sub-geometries for which they may not be constant\n\n\nzion_transect &lt;- zion_transect %&gt;%\n  group_by(id) %&gt;%\n  mutate(dist = st_distance(geometry)[, 1])\n\nzion_elev &lt;- terra::extract(srtm, vect(zion_transect))\nzion_transect &lt;- cbind(zion_transect, zion_elev)\n\n\n많은 Point들 간의 거리를 산출 : 첫번째 점들과 이후의 각각의 점들 사이의 거리 계산하기\n횡단면의 각 점에 대한 고도값을 추출하고 이 정보를 주요 객체와 결합\n\n\n\n\nlibrary(tmap)\nlibrary(grid)\nlibrary(ggplot2)\nzion_transect_line &lt;- cbind(c(-113.2, -112.9), c(37.45, 37.2)) %&gt;%\n  st_linestring() %&gt;%\n  st_sfc(crs = crs(srtm)) %&gt;%\n  st_sf()\nzion_transect_points &lt;- st_cast(zion_transect, \"POINT\")[c(1, nrow(zion_transect)), ]\nzion_transect_points$name &lt;- c(\"start\", \"end\")\nrast_poly_line &lt;- tm_shape(srtm) +\n  tm_raster(palette = terrain_colors, title = \"Elevation (m)\",\n            legend.show = TRUE, style = \"cont\") +\n  tm_shape(zion) +\n  tm_borders(lwd = 2) +\n  tm_shape(zion_transect_line) +\n  tm_lines(col = \"black\", lwd = 4) +\n  tm_shape(zion_transect_points) +\n  tm_text(\"name\", bg.color = \"white\", bg.alpha = 0.75, auto.placement = TRUE) +\n  tm_layout(legend.frame = TRUE, legend.position = c(\"right\", \"top\"))\nrast_poly_line\n\n\n\nplot_transect &lt;- ggplot(zion_transect, aes(as.numeric(dist), srtm)) +\n  geom_line() +\n  labs(x = \"Distance (m)\", y = \"Elevation (m a.s.l.)\") +\n  theme_bw() +\n  # facet_wrap(~id) +\n  theme(plot.margin = unit(c(5.5, 15.5, 5.5, 5.5), \"pt\"))\nplot_transect\n\n\n\n\n## grid 그리기\ngrid.newpage() #This function erases the current device or moves to a new page.\npushViewport(viewport(layout = grid.layout(2, 2, heights = unit(c(0.25, 5), \"null\"))))\ngrid.text(\"A. Line extraction\", vp = viewport(layout.pos.row = 1, layout.pos.col = 1))\ngrid.text(\"B. Elevation along the line\", vp = viewport(layout.pos.row = 1, layout.pos.col = 2))\nprint(rast_poly_line, vp = viewport(layout.pos.row = 2, layout.pos.col = 1))\nprint(plot_transect, vp = viewport(layout.pos.row = 2, layout.pos.col = 2))\n\n\n\n\n\nzion_srtm_values &lt;- terra::extract(x = srtm, y = vect(zion))\ngroup_by(zion_srtm_values, ID) %&gt;%\n  summarize(across(srtm, list(min = min, mean = mean, max = max)))\n#&gt; # A tibble: 1 × 4\n#&gt;      ID srtm_min srtm_mean srtm_max\n#&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n#&gt; 1     1     1122     1818.     2661\n\n\n단일 영역을 특성화하거나 여러 영역을 비교하기 위해 폴리곤 당 래스터 값에 대한 요약 통계 생성\n\n\n\n\n\n\n벡터 객체를 래스터 객체의 표현으로 변환\n\n\ncycle_hire_osm &lt;- spData::cycle_hire_osm\ncycle_hire_osm_projected &lt;- st_transform(cycle_hire_osm, \"EPSG:27700\")\nraster_template &lt;- rast(ext(cycle_hire_osm_projected), resolution = 1000,\n                        crs = st_crs(cycle_hire_osm_projected)$wkt) # ext : 경계값\nch_raster1 &lt;- rasterize(vect(cycle_hire_osm_projected), raster_template,\n                        field = 1)\nch_raster2 &lt;- rasterize(vect(cycle_hire_osm_projected), raster_template,\n                        fun = \"length\")\nch_raster3 &lt;- rasterize(vect(cycle_hire_osm_projected), raster_template,\n                        field = \"capacity\", fun = sum)\n\n\n\n\n\n\n\n폴리곤 객체를 여러 줄 문자열로 casting한 후 0.5도의 해상도로 탬플릿 래스터 생성\n\ntouches = TRUE : 경계에 해당되는 래스터만 색칠(FALSE이면 경계 내부까지)\n\n\n\ncalifornia &lt;- dplyr::filter(us_states, NAME == \"California\")\ncalifornia_borders &lt;- st_cast(california, \"MULTILINESTRING\")\nraster_template2 &lt;- rast(ext(california),\n                         resolution = 0.5,\n                         crs = st_crs(california)$wkt)\ncalifornia_raster1 &lt;-\n  rasterize(vect(california_borders), raster_template2,\n            touches = TRUE) # touches = TRUE : 경계값만\ncalifornia_raster2 &lt;-\n  rasterize(vect(california), raster_template2)\n# with `touches = FALSE` by default, which selects only cell\n\n\n\n\n\n\n\n\n\n\n공간적으로 연속적인 래스터 데이터를 점, 선 또는 다각형과 같은 공간적으로 분리된 벡터 데이터로 변환\n벡터화의 가장 간단한 형태는 래스터 셀의 중심부를 점으로 변환하는 것\nas.points() : 모든 raster grid 셀에 대해 중심점으로 반환\n\n\nelev &lt;- rast(system.file(\"raster/elev.tif\", package = \"spData\"))\nelev_point &lt;- as.points(elev) %&gt;%\n  st_as_sf()\nplot(elev)\n\n\n\nplot(elev_point)\n\n\n\n\n\ncontour() : 선에 해당하는 수치 표현\n등고선의 생성 : 공간 벡터화의 또 다른 일반적인 유형은 연속적인 높이 또는 온도(등온선)의 선을 나타내는 등고선 생성\n\n\ndem = rast(system.file(\"raster/dem.tif\", package = \"spDataLarge\"))\ncl = as.contour(dem)\nplot(dem, axes = FALSE)\nplot(cl, add = TRUE)\n\n\n\nplot(dem, axes = FALSE)\ncontour(dem, add = T) # 수치까지 표현\n\n\n\n\n\nas.polygons() : 래스터를 다각형으로 변환하는 것\n\n\ngrain &lt;- rast(system.file(\"raster/grain.tif\", package = \"spData\"))\ngrain_poly &lt;- as.polygons(grain) %&gt;%\n  st_as_sf()\nplot(grain)\n\n\n\nplot(grain_poly)\n\n\n\n\n\n\n\n\n\n\n\nCRS를 설명할 수 있는 여러가지 방법\n\n단순하지만 “lon/lat 좌표”와 같이 모호할 수 있는 문장\n\n\n공식화되었지만 지금은 구식인 proj4 strings\n\n\nproj=lonlat +ellps=WGS84 +datum=WGS84 +no_defs\n\n\nEPSG:4326과 같이 식별되는 authority:code 텍스트 문자열\n\n-&gt; 3번째 방법이 가장 정확(짧고 기억하기 쉬우며 온라인에서 찾기 쉬움)\n\n\nst_crs(\"EPSG:4326\")\n\n\n\n\n\n벡터 지리 데이터 객체에서 CRS를 가져오고 설정\n\n\nvector_filepath &lt;- system.file(\"shapes/world.gpkg\", package = \"spData\")\nnew_vector &lt;- read_sf(vector_filepath)\n\nst_crs(new_vector)\n#&gt; Coordinate Reference System:\n#&gt;   User input: WGS 84 \n#&gt;   wkt:\n#&gt; GEOGCRS[\"WGS 84\",\n#&gt;     ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n#&gt;         MEMBER[\"World Geodetic System 1984 (Transit)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G730)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G873)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G1150)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G1674)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G1762)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G2139)\"],\n#&gt;         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n#&gt;             LENGTHUNIT[\"metre\",1]],\n#&gt;         ENSEMBLEACCURACY[2.0]],\n#&gt;     PRIMEM[\"Greenwich\",0,\n#&gt;         ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;     CS[ellipsoidal,2],\n#&gt;         AXIS[\"geodetic latitude (Lat)\",north,\n#&gt;             ORDER[1],\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;         AXIS[\"geodetic longitude (Lon)\",east,\n#&gt;             ORDER[2],\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;     USAGE[\n#&gt;         SCOPE[\"Horizontal component of 3D system.\"],\n#&gt;         AREA[\"World.\"],\n#&gt;         BBOX[-90,-180,90,180]],\n#&gt;     ID[\"EPSG\",4326]]\n\n\nUser input : CRS식별자 (WGS 84, 입력 파일에서 가져온 EPSG:4326의 동의어)\nwkt : CRS에 대한 모든 관련 정보와 함께 전체 WKT 문자열을 포함\ninput 요소는 유연함(AUTHORITY:CODE (ex. EPSG:4326), CRS 이름(ex. WGS84), proj4string 정의)\nwkt 요소는 객체를 파일에 저장하거나 좌표 연산을 수행할 때 사용되는 WKT 표현을 저장\nnew_vector 객체가 WGS84 타원체를 가지며, 그리니치 프라임 자오선을 사용하고, 위도와 경도의 축 순서를 사용하는 것을 볼 수 있음\n이 경우 이 CRS 사용에 적합한 영역을 설명하는 USAGE와 CRS 식별자 EPSG:4326을 가리키는 ID와 같은 추가 요소도 있음\n\n\nst_crs(new_vector)$IsGeographic\n#&gt; [1] TRUE\nst_crs(new_vector)$units_gdal\n#&gt; [1] \"degree\"\nst_crs(new_vector)$srid\n#&gt; [1] \"EPSG:4326\"\nst_crs(new_vector)$proj4string\n#&gt; [1] \"+proj=longlat +datum=WGS84 +no_defs\"\n\n\nst_crs 함수에는 유용한 기능이 하나 있는데, 사용된 CRS에 대한 추가 정보를 검색할 수 있음.\n\nst_crs(new_vector)$IsGeographic : CRS가 지리적 상태인지 확인\nst_crs(new_vector)$units_gdal : CRS 단위\nst_crs(new_vector)$srid : 해당 ‘SRID’ 식별자를 추출(사용 가능한 경우)\nst_crs(new_vector)$proj4string : proj4string 표현을 추출\n\nst_set_crs() : CRS가 없거나 잘못 설정되어 있는 경우 CRS 설정\n\n\nnew_vector &lt;- st_set_crs(new_vector, \"EPSG:4326\") # set CRS\n\n\nterra::crs() : 래스터 객체에 대한 CRS를 설정\n하지만, crs() 함수를 사용하면 좌표계는 바뀌지만 값이 바뀌지는 않음.\n\n\nraster_filepath &lt;- system.file(\"raster/srtm.tif\", package = \"spDataLarge\")\nmy_rast &lt;- rast(raster_filepath)\ncrs(my_rast)\n#&gt; [1] \"GEOGCRS[\\\"WGS 84\\\",\\n    ENSEMBLE[\\\"World Geodetic System 1984 ensemble\\\",\\n        MEMBER[\\\"World Geodetic System 1984 (Transit)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G730)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G873)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1150)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1674)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1762)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G2139)\\\"],\\n        ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        ENSEMBLEACCURACY[2.0]],\\n    PRIMEM[\\\"Greenwich\\\",0,\\n        ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    CS[ellipsoidal,2],\\n        AXIS[\\\"geodetic latitude (Lat)\\\",north,\\n            ORDER[1],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        AXIS[\\\"geodetic longitude (Lon)\\\",east,\\n            ORDER[2],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    USAGE[\\n        SCOPE[\\\"Horizontal component of 3D system.\\\"],\\n        AREA[\\\"World.\\\"],\\n        BBOX[-90,-180,90,180]],\\n    ID[\\\"EPSG\\\",4326]]\"\ncat(crs(my_rast)) # get CRS\n#&gt; GEOGCRS[\"WGS 84\",\n#&gt;     ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n#&gt;         MEMBER[\"World Geodetic System 1984 (Transit)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G730)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G873)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G1150)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G1674)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G1762)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G2139)\"],\n#&gt;         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n#&gt;             LENGTHUNIT[\"metre\",1]],\n#&gt;         ENSEMBLEACCURACY[2.0]],\n#&gt;     PRIMEM[\"Greenwich\",0,\n#&gt;         ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;     CS[ellipsoidal,2],\n#&gt;         AXIS[\"geodetic latitude (Lat)\",north,\n#&gt;             ORDER[1],\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;         AXIS[\"geodetic longitude (Lon)\",east,\n#&gt;             ORDER[2],\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;     USAGE[\n#&gt;         SCOPE[\"Horizontal component of 3D system.\"],\n#&gt;         AREA[\"World.\"],\n#&gt;         BBOX[-90,-180,90,180]],\n#&gt;     ID[\"EPSG\",4326]]\ncrs(my_rast) &lt;- \"EPSG:26912\" # set CRS\n\nlondon &lt;- data.frame(lon = -0.1, lat = 51.5) %&gt;%\n  st_as_sf(coords = c(\"lon\", \"lat\"))\nst_is_longlat(london)\n#&gt; [1] NA\n\nlondon_geo &lt;- st_set_crs(london, \"EPSG:4326\")\nst_is_longlat(london_geo)\n#&gt; [1] TRUE\n\n\n\n\n\n\nsf는 지리 벡터 데이터에 대한 클래스와 지리 계산을 위한 중요한 하위 수준 라이브러리에 대한 일관된 명령줄 인터페이스 제공\n\n구면 geometry 연산을 sf:sf_use_sf(FALSE) 명령으로 끄면 버퍼는 미터와 같은 적절한 거리 단위를 대체하지 못하는 위도와 경도의 단위를 사용하기 때문에 쓸모없는 출력이 됨.\n공간 및 기하학적 연산을 수행하는 것은 경우에 따라 거의 또는 전혀 차이가 없음. (ex: 공간 부분 집합) 그러나 버퍼링과 같은 거리가 포함된 연산의 경우 (구면 지오메트리 엔진을 사용하지 않고) 좋은 결과를 보장하는 유일한 방법은 데이터의 투영된 복사본을 만들고 그에 대한 연산을 실행하는 것임.\n그 결과 런던과 동일하지만 미터 단위의 EPSG 코드를 가진 적절한 CRS(영국 국가 그리드)에 재투사된 새로운 물체가 되었음.\nCRS의 단위가 (도가 아닌) 미터라는 사실은 이것이 투영된 CRS임을 알려줌\n\n\n\nlondon_buff_no_crs &lt;-\n  st_buffer(london, dist = 1) # incorrect: no CRS\nlondon_buff_no_crs\n#&gt; Simple feature collection with 1 feature and 0 fields\n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -1.1 ymin: 50.5 xmax: 0.9 ymax: 52.5\n#&gt; CRS:           NA\n#&gt;                         geometry\n#&gt; 1 POLYGON ((0.9 51.5, 0.89862...\nlondon_buff_s2 &lt;-\n  st_buffer(london_geo, dist = 1e5) # silent use of s2 (1e5 : 10^5m = 100,000m)\nlondon_buff_s2\n#&gt; Simple feature collection with 1 feature and 0 fields\n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -1.552818 ymin: 50.59609 xmax: 1.356603 ymax: 52.40393\n#&gt; Geodetic CRS:  WGS 84\n#&gt;                         geometry\n#&gt; 1 POLYGON ((-0.3523255 52.392...\nlondon_buff_s2_100_cells &lt;-\n  st_buffer(london_geo, dist = 1e5, max_cells = 100)\nlondon_buff_s2_100_cells\n#&gt; Simple feature collection with 1 feature and 0 fields\n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -1.718303 ymin: 50.51128 xmax: 1.524546 ymax: 52.53186\n#&gt; Geodetic CRS:  WGS 84\n#&gt;                         geometry\n#&gt; 1 POLYGON ((-0.3908656 52.531...\n\nsf::sf_use_s2(FALSE)\n#&gt; Spherical geometry (s2) switched off\n\nlondon_buff_lonlat &lt;-\n  st_buffer(london_geo, dist = 1) # incorrect result\n#&gt; Warning in st_buffer.sfc(st_geometry(x), dist, nQuadSegs, endCapStyle =\n#&gt; endCapStyle, : st_buffer does not correctly buffer longitude/latitude data\n#&gt; dist is assumed to be in decimal degrees (arc_degrees).\n\nsf::sf_use_s2(TRUE)\n#&gt; Spherical geometry (s2) switched on\n\nlondon_proj &lt;- data.frame(x = 530000, y = 180000) %&gt;%\n  st_as_sf(coords = 1:2, crs = \"EPSG:27700\")\n\nst_crs(london_proj)\n#&gt; Coordinate Reference System:\n#&gt;   User input: EPSG:27700 \n#&gt;   wkt:\n#&gt; PROJCRS[\"OSGB36 / British National Grid\",\n#&gt;     BASEGEOGCRS[\"OSGB36\",\n#&gt;         DATUM[\"Ordnance Survey of Great Britain 1936\",\n#&gt;             ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,\n#&gt;                 LENGTHUNIT[\"metre\",1]]],\n#&gt;         PRIMEM[\"Greenwich\",0,\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;         ID[\"EPSG\",4277]],\n#&gt;     CONVERSION[\"British National Grid\",\n#&gt;         METHOD[\"Transverse Mercator\",\n#&gt;             ID[\"EPSG\",9807]],\n#&gt;         PARAMETER[\"Latitude of natural origin\",49,\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433],\n#&gt;             ID[\"EPSG\",8801]],\n#&gt;         PARAMETER[\"Longitude of natural origin\",-2,\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433],\n#&gt;             ID[\"EPSG\",8802]],\n#&gt;         PARAMETER[\"Scale factor at natural origin\",0.9996012717,\n#&gt;             SCALEUNIT[\"unity\",1],\n#&gt;             ID[\"EPSG\",8805]],\n#&gt;         PARAMETER[\"False easting\",400000,\n#&gt;             LENGTHUNIT[\"metre\",1],\n#&gt;             ID[\"EPSG\",8806]],\n#&gt;         PARAMETER[\"False northing\",-100000,\n#&gt;             LENGTHUNIT[\"metre\",1],\n#&gt;             ID[\"EPSG\",8807]]],\n#&gt;     CS[Cartesian,2],\n#&gt;         AXIS[\"(E)\",east,\n#&gt;             ORDER[1],\n#&gt;             LENGTHUNIT[\"metre\",1]],\n#&gt;         AXIS[\"(N)\",north,\n#&gt;             ORDER[2],\n#&gt;             LENGTHUNIT[\"metre\",1]],\n#&gt;     USAGE[\n#&gt;         SCOPE[\"Engineering survey, topographic mapping.\"],\n#&gt;         AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],\n#&gt;         BBOX[49.75,-9.01,61.01,2.01]],\n#&gt;     ID[\"EPSG\",27700]]\n\n\n\n\n\n\n\ndownload.file(url = \"https://irma.nps.gov/DataStore/DownloadFile/666527\",\n              destfile = \"nps_boundary.zip\")\nunzip(zipfile = \"nps_boundary.zip\")\nusa_parks = read_sf(dsn = \"nps_boundary.shp\")\n\n\n해외여서 접속이 막혀있음\n공공데이터포털에서 shape 파일 다운받아 불러오기\n\n공공데이터포털에서 데이터를 작업 공간에 다운 받기\n\n\n# unzip(zipfile=\"C:/202201/GIS/data/부산광역시_교통정보서비스센터 보유 ITS CCTV 현황(SHP)_20210601.zip\")\n#busan &lt;- read_sf(dsn = \"./Spatial_Information_Analysis/tl_tracffic_cctv_info.shp\", options = \"ENCODING:CP949\")\n#busan\n#plot(busan)\n\n# unzip(zipfile = \"C:/202201/GIS/data/CTPRVN_20220324.zip\")\n#sido &lt;- read_sf(dsn = \"./Spatial_Information_Analysis/ctp_rvn.shp\", options = \"ENCODING:CP949\")\n#sido\n#plot(sido)\n\n\n\n\n\n\nrnaturalearth 패키지의 ne_countries() 기능을 사용하면 국가 경계 기능을 사용할 수 있음\nosmdata 패키지는 속도가 제한되어 있다는 단점이 있음\n\n이러한 한계를 극복하기 위해 osmextract 패키지가 개발\n\n\nlibrary(rnaturalearth)\n#&gt; Support for Spatial objects (`sp`) will be deprecated in {rnaturalearth} and will be removed in a future release of the package. Please use `sf` objects with {rnaturalearth}. For example: `ne_download(returnclass = 'sf')`\nusa &lt;- ne_countries(country = \"United States of America\") # United States borders\n#&gt; Warning: The `returnclass` argument of `ne_download()` sp as of rnaturalearth 1.0.0.\n#&gt; ℹ Please use `sf` objects with {rnaturalearth}, support for Spatial objects\n#&gt;   (sp) will be removed in a future release of the package.\nclass(usa)\n#&gt; [1] \"SpatialPolygonsDataFrame\"\n#&gt; attr(,\"package\")\n#&gt; [1] \"sp\"\n\nusa_sf &lt;- st_as_sf(usa)\nplot(usa_sf[1])\n\n\n\nkorea &lt;- ne_countries(country = \"South Korea\") # United States borders\nclass(korea)\n#&gt; [1] \"SpatialPolygonsDataFrame\"\n#&gt; attr(,\"package\")\n#&gt; [1] \"sp\"\nkorea_sf &lt;- st_as_sf(korea)\nplot(korea_sf[1])\n\n\n\n\n\n\n\n\n\nhttps://r.geocompx.org/read-write.html#file-formats\n\n\n\n\n\ngpkg 형식 불러오기\n\n\nf &lt;- system.file(\"shapes/world.gpkg\", package = \"spData\")\nworld = read_sf(f, quiet = TRUE)\ntanzania = read_sf(f, query = 'SELECT * FROM world WHERE name_long = \"Tanzania\"')\ntanzania_buf = st_buffer(tanzania, 50000)\ntanzania_buf_geom = st_geometry(tanzania_buf)\ntanzania_buf_wkt = st_as_text(tanzania_buf_geom)\ntanzania_neigh = read_sf(f, wkt_filter = tanzania_buf_wkt)\n\n\ncsv 형식 불러오기\n\n\ncycle_hire_txt = system.file(\"misc/cycle_hire_xy.csv\", package = \"spData\")\ncycle_hire_xy = read_sf(cycle_hire_txt,\n                        options = c(\"X_POSSIBLE_NAMES=X\", \"Y_POSSIBLE_NAMES=Y\"))\n\n\nWell-known text(WKT), Well-known binary(WKB), and the GeoJSON formats\n\n\nworld_txt = system.file(\"misc/world_wkt.csv\", package = \"spData\")\nworld_wkt = read_sf(world_txt, options = \"GEOM_POSSIBLE_NAMES=WKT\")\n# the same as\nworld_wkt2 = st_read(world_txt, options = \"GEOM_POSSIBLE_NAMES=WKT\",\n                     quiet = TRUE, stringsAsFactors = FALSE, as_tibble = TRUE)\n\n\nKML file stores geographic information in XML format\n\n\nu = \"https://developers.google.com/kml/documentation/KML_Samples.kml\"\ndownload.file(u, \"./Spatial_Information_Analysis/KML_Samples.kml\")\nst_layers(\"./Spatial_Information_Analysis/KML_Samples.kml\")\n#&gt; Driver: KML \n#&gt; Available layers:\n#&gt;              layer_name  geometry_type features fields crs_name\n#&gt; 1            Placemarks       3D Point        3      2   WGS 84\n#&gt; 2      Highlighted Icon       3D Point        1      2   WGS 84\n#&gt; 3                 Paths 3D Line String        6      2   WGS 84\n#&gt; 4         Google Campus     3D Polygon        4      2   WGS 84\n#&gt; 5      Extruded Polygon     3D Polygon        1      2   WGS 84\n#&gt; 6 Absolute and Relative     3D Polygon        4      2   WGS 84\nkml = read_sf(\"./Spatial_Information_Analysis/KML_Samples.kml\", layer = \"Placemarks\")\n\n\n\n\n\n\n\n\n정적인 지도는 지리 계산의 가장 일반적인 시각적 출력 유형\nplot() 또는 tmap_mode(plot)\n\n\n\n\n# Add fill layer to nz shape\ntm_shape(nz) +\n  tm_fill()\n# Add border layer to nz shape\ntm_shape(nz) +\n  tm_borders()\n# Add fill and border layers to nz shape\ntm_shape(nz) +\n  tm_fill() +\n  tm_borders()\n\n\n\n\n\n\n\n\n\n\nmap_nz &lt;- tm_shape(nz) + tm_polygons()\nclass(map_nz)\n#&gt; [1] \"tmap\"\nmap_nz\n\n\n\n\nnz_elev = rast(system.file(\"raster/nz_elev.tif\", package = \"spDataLarge\"))\n\nmap_nz1 &lt;- map_nz + tm_shape(nz_elev) + tm_raster(alpha = 0.7)\n\nnz_water &lt;- st_union(nz) %&gt;% st_buffer(22200) %&gt;%\n  st_cast(to = \"LINESTRING\")\n\nmap_nz2 &lt;- map_nz1 +\n  tm_shape(nz_water) + tm_lines()\n\nmap_nz3 &lt;- map_nz2 +\n  tm_shape(nz_height) + tm_dots()\n\ntmap_arrange(map_nz1, map_nz2, map_nz3)\n\n\n\n\n\nalpha : 레이어를 반투명하게 만들기 위해 설정\n\n\n\n\n\nma1 &lt;- tm_shape(nz) + tm_fill(col = \"red\")\nma2 &lt;- tm_shape(nz) + tm_fill(col = \"red\", alpha = 0.3)\nma3 &lt;- tm_shape(nz) + tm_borders(col = \"blue\")\nma4 &lt;- tm_shape(nz) + tm_borders(lwd = 3)\nma5 &lt;- tm_shape(nz) + tm_borders(lty = 2)\nma6 &lt;- tm_shape(nz) + tm_fill(col = \"red\", alpha = 0.3) +\n  tm_borders(col = \"blue\", lwd = 3, lty = 2)\n\ntmap_arrange(ma1, ma2, ma3, ma4, ma5, ma6)\n\n\n\n\n\ntm_fill()과 tm_bubbles()에서 레이어는 기본적으로 회색으로 채워지고 tm_lines()은 검은선으로 그려짐\ntmap의 인수는 숫자 벡터를 허용하지 않음\n\n\nplot(st_geometry(nz), col = nz$Land_area) # works\ntm_shape(nz) + tm_fill(col = nz$Land_area) # fails\n&gt; Error: Fill argument neither colors nor valid variable name(s)\ntm_shape(nz) + tm_fill(col = \"Land_area\")\n\n\n\n\n\n\n\n범례의 제목 설정\n\n\nlegend_title &lt;- expression(\"Area (km\"^2*\")\")\nmap_nza &lt;- tm_shape(nz) +\n  tm_fill(col = \"Land_area\", title = legend_title) + tm_borders()\nmap_nza\n\n\n\n\n\n\n\n\nbreaks : 색상의 표현 값 범위를 수동으로 설정\nn : 숫자 변수가 범주화되는 Bin의 수 설정\npalette : 색 구성표를 정의 (ex. BuGn)\n\n\ntm1 &lt;- tm_shape(nz) + tm_polygons(col = \"Median_income\")\nbreaks = c(0, 3, 4, 5) * 10000\ntm2 &lt;- tm_shape(nz) + tm_polygons(col = \"Median_income\", breaks = breaks)\ntm3 &lt;- tm_shape(nz) + tm_polygons(col = \"Median_income\", n = 10)\ntm4 &lt;- tm_shape(nz) + tm_polygons(col = \"Median_income\", palette = \"BuGn\")\n\ntmap_arrange(tm1, tm2, tm3, tm4)\n\n\n\n\n\ntm_shape(nz) + tm_polygons(col = \"Median_income\", style = \"pretty\")\ntm_shape(nz) + tm_polygons(col = \"Median_income\", style = \"equal\")\ntm_shape(nz) + tm_polygons(col = \"Median_income\", style = \"quantile\")\ntm_shape(nz) + tm_polygons(col = \"Median_income\", style = \"jenks\")\ntm_shape(nz) + tm_polygons(col = \"Median_income\", style = \"cont\")\ntm_shape(nz) + tm_polygons(col = \"Median_income\", style = \"cat\")\n\n\n\n\n\n\n\nstyle = \"pretty\" : 기본 설정은 가능한 경우 정수로 반올림하고 간격을 균등하게 유지\nstyle = \"equal\" : 입력 값을 동일한 범위의 빈으로 나누고 균일한 분포의 변수에 적합(결과 맵이 색상 다양성이 거의 없을 수 있으므로 분포가 치우친 변수에는 권장하지 않음)\nstyle = \"quantile\" : 동일한 수의 관찰이 각 범주에 포함되도록 함(빈 범위가 크게 다를 수 있다는 잠재적인 단점이 있음).\nstyle = \"jenks\" : 데이터에서 유사한 값의 그룹을 식별하고 범주 간의 차이를 최대화\nstyle = \"cont\" : 연속 색상 필드에 많은 색상을 표시하고 연속 래스터에 특히 적합\nstyle = \"cat\" : 범주 값을 나타내도록 설계되었으며 각 범주가 고유한 색상을 받도록 함\n\n\ntm_p1 &lt;- tm_shape(nz) + tm_polygons(\"Population\", palette = \"Blues\")\ntm_p2 &lt;- tm_shape(nz) + tm_polygons(\"Population\", palette = \"YlOrBr\")\n\ntmap_arrange(tm_p1, tm_p2)\n\n\n\n\n\n순차 팔레트는 단일(ex. Blues : 밝은 파란색에서 진한 파란색으로 이동) 또는 다중 색상/색조(ex. YlOrBr : 주황색을 통해 밝은 노란색에서 갈색으로 그라데이션)\n\n\n\n\n\nmap_nz +\n  tm_compass(type = \"8star\", position = c(\"left\", \"top\")) +\n  tm_scale_bar(breaks = c(0, 100, 200), text.size = 1)\n\n\n\n\ntm_l1 &lt;- map_nz + tm_layout(title = \"New Zealand\")\ntm_l2 &lt;- map_nz + tm_layout(scale = 5)\ntm_l3 &lt;- map_nz + tm_layout(bg.color = \"lightblue\")\ntm_l4 &lt;- map_nz + tm_layout(frame = FALSE)\n\ntmap_arrange(tm_l1, tm_l2, tm_l3, tm_l4)\n\n\n\n\n\ntm_layout()의 다양한 옵션\n\nframe.lwd : 프레임 너비\nframe.double.line : 이중선 허용 옵션\nouter.margin, inner.margin : 여백 설정\nfontface, fontfamily : 글꼴 설정\nlegend.show : 범례 표시 여부\nlegend.position : 범례 위치 변경\n\n\n\n\n\n\n\n\ntm_s1 &lt;- map_nza + tm_style(\"bw\")\ntm_s2 &lt;- map_nza + tm_style(\"classic\")\ntm_s3 &lt;- map_nza + tm_style(\"cobalt\")\ntm_s4 &lt;- map_nza + tm_style(\"col_blind\")\n\ntmap_arrange(tm_s1, tm_s2, tm_s3, tm_s4)\n\n\n\n\n\n\n\n\nurb_1970_2030 &lt;- urban_agglomerations %&gt;%\n  filter(year %in% c(1970, 1990, 2010, 2030))\ntm_shape(world) +\n  tm_polygons() +\n  tm_shape(urb_1970_2030) +\n  tm_symbols(col = \"black\", border.col = \"white\", size = \"population_millions\") +\n  tm_facets(by = \"year\", nrow = 2, free.coords = TRUE)\n\n\n\n#free.coords : 지도에 자체 경계 상자가 있는지 여부를 지정\n\n\n\n\n\nnz_region &lt;- st_bbox(c(xmin = 1340000, xmax = 1450000,\n                       ymin = 5130000, ymax = 5210000),\n                     crs = st_crs(nz_height)) %&gt;% st_as_sfc()\n\nnz_height_map &lt;- tm_shape(nz_elev, bbox = nz_region) +\n  tm_raster(style = \"cont\", palette = \"YlGn\", legend.show = TRUE) +\n  tm_shape(nz_height) + tm_symbols(shape = 2, col = \"red\", size = 1) +\n  tm_scale_bar(position = c(\"left\", \"bottom\"))\n\nnz_map &lt;- tm_shape(nz) + tm_polygons() +\n  tm_shape(nz_height) + tm_symbols(shape = 2, col = \"red\", size = 0.1) +\n  tm_shape(nz_region) + tm_borders(lwd = 3)\n\nlibrary(grid)\nnz_height_map\nprint(nz_map, vp = viewport(0.8, 0.27, width = 0.5, height = 0.5))\n\n\n\n\n\nviewport() : 두개의 맵을 결합\n\n\n\n\n\n\nurb_anim &lt;- tm_shape(world) + tm_polygons() +\n  tm_shape(urban_agglomerations) + tm_dots(size = \"population_millions\") +\n  tm_facets(along = \"year\", free.coords = FALSE)\n\ntmap_animation(urb_anim, filename = \"./Spatial_Information_Analysis/urb_anim.gif\", delay = 25)\n#&gt; Creating frames\n#&gt; =========\n#&gt; ====\n#&gt; =====\n#&gt; ====\n#&gt; =====\n#&gt; ====\n#&gt; =====\n#&gt; ====\n#&gt; ====\n#&gt; =====\n#&gt; ====\n#&gt; =====\n#&gt; ====\n#&gt; =====\n#&gt; ====\n#&gt; =====\n#&gt; ====\n#&gt; \n#&gt; Creating animation\n#&gt; Animation saved to C:\\Users\\Hyunsoo Kim\\Desktop\\senior_grade\\blog\\my-quarto-website\\Spatial_Information_Analysis\\urb_anim.gif\n\n\nby = year대신 along = year을 사용\nfree.coords = FALSE : 각 맵 반복에 대한 맵 범위 유지\ntmap_animation()을 사용하여 .gif로 저장\n\n\n\n\n\n대화형 지도는 데이터 세트를 새로운 차원으로 끌어올릴 수 있음\n지도를 기울이고 회전하는 기능과 사용자가 이동 및 확대/축소 할 때 자동으로 업데이트\ntmap, mapview, mapdeck, leaflet으로 표현 가능\n\n\n\n\ntmap_mode(\"view\") #interactive mode\n#&gt; tmap mode set to interactive viewing\nmap_nz\n\n\n\n\n\n\nmap_nz + tm_basemap(server = \"OpenTopoMap\")\n\n\n\n\n\n\nworld_coffee = left_join(world, coffee_data, by = \"name_long\")\nfacets = c(\"coffee_production_2016\", \"coffee_production_2017\")\ntm_shape(world_coffee) + tm_polygons(facets) +\n  tm_facets(nrow = 1, sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntm_basemap() 또는 tm_options()로 basemap 지정 가능\ntm_facets()에서 sync옵션을 TRUE로 선택하면 여러개의 맵을 동시에 확대/축소할 수 있음\n\n\n\n\n\nmapview::mapview(nz)\n\ntrails %&gt;%\n  st_transform(st_crs(franconia)) %&gt;%\n  st_intersection(franconia[franconia$district == \"Oberfranken\", ][1]) %&gt;%\n  st_collection_extract(\"LINE\") %&gt;%\n  mapview(color = \"red\", lwd = 3, layer.name = \"trails\") +\n  mapview(franconiWa, zcol = \"district\", burst = TRUE) +\n  breweries\n\n\n\n\n\nset_token(Sys.getenv(\"pk.eyJ1IjoiancwMTEyIiwiYSI6ImNsM2ppbzYzNzBrbjQzZHBjMmlocnY2dDUifQ.58-gXpPtvcCGmMt2xEW-ig\"))\ncrash_data = read.csv(\"https://git.io/geocompr-mapdeck\")\ncrash_data = na.omit(crash_data)\nms = mapdeck_style(\"dark\")\nmapdeck(style = ms, pitch = 45, location = c(0, 52), zoom = 4) %&gt;%\n  add_grid(data = crash_data, lat = \"lat\", lon = \"lng\", cell_size = 1000,\n           elevation_scale = 50, layer_id = \"grid_layer\",\n           colour_range = viridisLite::plasma(6))\n#&gt; Registered S3 method overwritten by 'jsonify':\n#&gt;   method     from    \n#&gt;   print.json jsonlite\n\n\n\n\n\n\n\n\n\nadd_arc() 함수\n\n\nurl &lt;- 'https://raw.githubusercontent.com/plotly/datasets/master/2011_february_aa_flight_paths.csv'\nflights &lt;- read.csv(url)\nflights$id &lt;- seq_len(nrow(flights))\nflights$stroke &lt;- sample(1:3, size = nrow(flights), replace = T)\nkey = \"pk.eyJ1IjoiancwMTEyIiwiYSI6ImNsM2ppbzYzNzBrbjQzZHBjMmlocnY2dDUifQ.58-gXpPtvcCGmMt2xEW-ig\"\n\nmapdeck(token = key, style = mapdeck_style(\"dark\"), pitch = 45 ) %&gt;%\n  add_arc(\n    data = flights\n    , layer_id = \"arc_layer\"\n    , origin = c(\"start_lon\", \"start_lat\")\n    , destination = c(\"end_lon\", \"end_lat\")\n    , stroke_from = \"airport1\"\n    , stroke_to = \"airport2\"\n    , stroke_width = \"stroke\"\n  )\n\n\nadd_animated_arc() 함수\n\n\nmapdeck(token = key, style = 'mapbox://styles/mapbox/dark-v9', pitch = 45 ) %&gt;%\n  add_animated_arc(\n    data = flights\n    , layer_id = \"arc_layer\"\n    , origin = c(\"start_lon\", \"start_lat\")\n    , destination = c(\"end_lon\", \"end_lat\")\n    , stroke_from = \"airport1\"\n    , stroke_to = \"airport2\"\n    , stroke_width = \"stroke\"\n  )\n\n\nadd_heatmap() 함수\n\n\nmapdeck(token = key, style = mapdeck_style('dark'), pitch = 45 ) %&gt;%\n  add_heatmap(\n    data = df[1:30000, ]\n    , lat = \"lat\"\n    , lon = \"lng\"\n    , weight = \"weight\"\n    , colour_range = colourvalues::colour_values(1:6, palette = \"inferno\")\n  )\n\n\nadd_path() 함수\n\n\nmapdeck(\n  token = key\n  , style = mapdeck_style(\"dark\")\n  , zoom = 10) %&gt;%\n  add_path(\n    data = roads\n    , stroke_colour = \"RIGHT_LOC\"\n    , layer_id = \"path_layer\"\n  )\n\n\nadd_geojson(), add_scatterplot(), add_text() 등이 있음\n\n\n\n\n\npal = colorNumeric(\"RdYlBu\", domain = cycle_hire$nbikes)\nleaflet(data = cycle_hire) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%       # Background Map\n  addCircles(col = ~pal(nbikes), opacity = 0.9) %&gt;%      # nbikes의 값으로 색이 다르게 circle 생성\n  addPolygons(data = lnd, fill = FALSE) %&gt;%              # land에 따라 Polygon 생성\n  addLegend(pal = pal, values = ~nbikes) %&gt;%             # 범례 생성\n  setView(lng = -0.1, 51.5, zoom = 12) %&gt;%               # zoom\n  addMiniMap()                                           # minimap 생성\n\n\n\n\n\n\n# create a basic map\n\nleaflet() %&gt;%\n  addTiles() %&gt;% # add default OpenStreetMap map tiles\n  setView(lng=127.063, lat=37.513, zoom = 6) # korea, zoom 6\n\n\n\n\n\n# map style: NASA\n\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng=127.063, lat=37.513, zoom = 6) %&gt;%\n  addProviderTiles(\"NASAGIBS.ViirsEarthAtNight2012\")\n\n\n\n\n\n# map style: Esri.WorldImagery\n\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng=127.063, lat=37.513, zoom = 16) %&gt;%\n  addProviderTiles(\"Esri.WorldImagery\")\n\n\n\n\n\n# adding Popup\n\npopup = c(\"한남대학교 빅데이터응용학과\")\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  addMarkers(lng = c(127.4219), # longitude\n             lat = c(36.3548), # latitude\n             popup = popup)\n\n\n\n\n\n\nzoom : 확대/축소 비율 설정\naddProviderTiles() : 외부 지도 타일 추가\naddMarkers() : 커서를 클릭했을 때 팝업으로 나타나는 설명을 추가\n\n\n\n\n\n\n\n\nR을 사용하여 한걸음 더 나아가 웹 어플리케이션을 제작할 수 있게 해주는 패키지\nui 라고 말하는 화면은 실제로 사용자가 보는 화면\nshiny에서는 크게 titlePanel과 sidebarPanel, mainPanal의 세 가지로 구성\n\n\nui = fluidPage(\n  sliderInput(inputId = \"life\", \"Life expectancy\", 49, 84, value = 80),\n  leafletOutput(outputId = \"map\")\n)\nserver = function(input, output) {\n  output$map = renderLeaflet({\n    leaflet() %&gt;%\n      # addProviderTiles(\"OpenStreetMap.BlackAndWhite\") %&gt;%\n      addPolygons(data = world[world$lifeExp &lt; input$life,])\n  })\n}\nshinyApp(ui, server)\n#&gt; PhantomJS not found. You can install it with webshot::install_phantomjs(). If it is installed, please make sure the phantomjs executable can be found via the PATH variable.\n\nShiny applications not supported in static R Markdown documents\n\n\n\nui &lt;- fluidPage(#Application title\n  titlePanel(\"Hello Shiny!\"),\n  #Sidebar with a slider input for the number of bins\n  sidebarLayout(sidebarPanel(\n    sliderInput(\n      \"bins\",\n      \"Number of bins:\",\n      min = 1,\n      max = 50,\n      value = 30\n    )\n  ),\n  #Show a plot of the generated distribution\n  mainPanel(plotOutput(\"distPlot\"))))\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    x &lt;- faithful[, 2]\n    bins &lt;- seq(min(x), max(x), length.out = input$bins + 1)\n    hist(x,\n         breaks = bins,\n         col = 'darkgray',\n         border = 'white')\n  })\n}\nshinyApp(ui, server)\n\nShiny applications not supported in static R Markdown documents\n\n\n\n\n\n\n\n\n지도 공간 기법으로 시각화하는 ggmap 패키지는 Google Maps, Stamen Maps, 네이버 맵, 등의 다양한 온라인 소스로부터 가져온 정적인 지도 위에 특별한 데이터나 모형을 시각화하는 함수들을 제공함\nggmap()의 주요 함수\n\ngeocode() : 거리주소 또는 장소 이름을 이용하여 이용 지도 정보(위도, 경도) 획득\nget_googlemap() : 구글 지도 서비스 API에 접근하여 정적 지도 다운로드 지원과 지도에 marker 등을 삽입하고 자신이 원하는 줌 레벨과 center를 지정하여 지도 정보 생성\nget_map() : 지도 서비스 관련 서버에 관련 질의어를 지능형으로 인식하여 지도 정보 생성\nget_navermap() : 네이버 지도 서비스 API에 접근하여 정적 지도 다운로드 지원\nggimage() : ggplot2 패키지의 이미지와 동등한 수준으로 지도 이미지 생성\nggmap(), ggmapplot() : get_map() 함수에 의해서 생성된 픽셀 객체를 지도 이미지로 시각화\nqmap() : ggmap()함수와 get_map() 함수의 통합기능\nqmplot() : ggplot2 패키지의 qplot()와 동등한 수준으로 빠르게 지도 이미지 시각화\n\n\n\n\n\nget_googlemap() 함수를 통해 불러오고 싶은 곳의 장소를 문자열 값으로 첫 번째 인자에 넣어 실행해 이를 객체화 함\nggmap() 함수 안에 방금 만든 객체를 입력시킨 후 실행하면 원하는 장소를 중심으로 구글 지도가 plotting 됨\n\n\n# install.packages(\"ggmap\")\nlibrary(ggmap)\nregister_google(key = 'AIzaSyB4jjrVVAzb9fl8FQrQqUONAsaRBppWuSA')\n\n# 우리나라 지도 호출\ngetmap &lt;- get_googlemap(\"seoul\")\nggmap(getmap)\n\n\n\n\n\nggmap() 으로 반환되는 결과물은 ggplot2 패키지의 함수와 조합해 지도 위에 새로운 정보들을 추가할 수 있음\n\n\n\n\ndaejeon_map &lt;- get_googlemap(\"daejeon\") %&gt;% ggmap\nlocation &lt;- data.frame(\n  Name = c(\"한남대학교\", \"대전신세계\"),\n  lon = c(127.4219, 127.3821), #경도\n  lat = c(36.3548, 36.3752)    #위도\n)\n\ndaejeon_map + geom_point(data = location, aes(x = lon, y = lat))\n\ndaejeon_map &lt;- get_googlemap(\"daejeon\", zoom = 13) %&gt;% ggmap\nlocation &lt;- data.frame(\n  Name = c(\"한남대학교\", \"대전신세계\"),\n  lon = c(127.4219, 127.3821),\n  lat = c(36.3548, 36.3752)\n)\n\ndaejeon_map + geom_point(data = location, aes(x = lon, y = lat)) +\n  geom_text(data = location,\n            aes(label = Name),\n            size = 5,   # text 크기\n            vjust = -1) # text 위치\n\n\ngeom_point() 내의 옵션을 선택하여 점의 크기, 색깔, 모양 등 변경 가능\n\n\ndaejeon_map + geom_point(data = location, aes(x = lon, y = lat),\n                         size = 5, color = \"red\", alpha = 0.4) +\ngeom_text(data = location, aes(label = Name), size = 5, vjust = -1)\n\n\n한남대학교를 중심으로 그리기(center)\n\nenc2utf8 : UTF-8로 인코딩\nmaptype : “terrain”, “satellite”, “roadmap”, “hybrid”\ncenter : 맵의 중심\n\n\n# 한남대학교를 중심으로 그리기\ngc &lt;- geocode(enc2utf8(\"한남대학교\"))\n\nmap &lt;- get_googlemap(\n  center = as.numeric(gc),\n  maptype = \"roadmap\",\n  zoom = 13,\n  size = c(640, 640),\n  markers = gc) %&gt;% ggmap\n\nmap + geom_point(data = location, aes(x = lon, y = lat)) +\ngeom_text(data = location, aes(label = Name), size = 5, vjust = -1)\n\nPath(경로)\n\n\nmap + geom_path(data = location, aes(x = lon, y = lat), color = \"blue\", alpha = .5, lwd = 1)\n\n\n두 지역 사이의 경로 좌표 추출\n\nggmap::route : find a route from Google using different possible modes (\"driving\", \"walking\", \"bicycling\", \"transit\")\n\n\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(tmap)\nlibrary(stplanr)\n\ngc_st &lt;- geocode(enc2utf8(\"한남대학교\"))\ngc_ed &lt;- geocode(enc2utf8(\"신세계백화점 대전신세계아트앤사이언스\"))\ngc_od &lt;- st_linestring(rbind(as.numeric(gc_st), as.numeric(gc_ed)))\n\nst_sfc(gc_od) # Linestring, CRS 없음\nst_crs(gc_od)\ngc_od &lt;- st_sfc(gc_od, crs = 4326)\n# st_sfc() : 좌표계가 비어있는 경우에 좌표계 지정\nst_crs(gc_od)\n\nqtm(gc_od)\ngc_od &lt;- st_sf(gc_od)\n# st_sf() : sfc와 sf class의 객체들을 하나로 통합\ngc_od$distance &lt;- as.numeric(st_length(gc_od))\n\nroute_od = route(l = gc_od,             # l : linestring\n                 route_fun = route_osrm,\n                 osrm.profile = \"car\")  # foot, bike, car\nqtm(route_od)\n\nmap &lt;- get_googlemap(\n  center = c(127.41, 36.37),\n  maptype = \"roadmap\",\n  zoom = 14,\n  size = c(640, 640),\n  markers = gc\n) %&gt;% ggmap(extent = \"device\")\n\nmap\n\nmap + geom_sf(data = route_od, inherit.aes = F)\n# inherit.aes = F : sf형식의 데이터를 그릴 때 필수 옵션\n\n지도를 꽉 채워서 출력(x, y축 삭제하고 그림만 출력)\n\nextent = \"device\"\n+ theme_void()\n\n\nmap &lt;- get_googlemap(\n  center = as.numeric(gc),\n  maptype = \"roadmap\",\n  zoom = 13,\n  size = c(640, 640),\n  markers = gc\n) %&gt;% ggmap(extent = \"device\")\nmap\n\nmap &lt;- get_googlemap(\n  center = as.numeric(gc),\n  maptype = \"roadmap\",\n  zoom = 13,\n  size = c(640, 640),\n  markers = gc\n) %&gt;% ggmap() + theme_void()\nmap\n\n\n\n\n\n\n# Houston 범죄 데이터\nstr(crime)\nHoustonmap &lt;- get_map(\"Houston\")\nggmap(Houstonmap)\nggmap(Houstonmap) + geom_point(data = crime, aes(x = lon, y = lat))\nggmap(Houstonmap) + geom_point(data = crime, aes(x = lon, y = lat), size = 0.1, alpha = 0.1) # 점의 크기, 점의 투명도 조절\n\n#지도 확대 & 특정 지역 데이터만 추출하기\nHoustonmap &lt;- get_map(\"Houston\", zoom = 14)\ncrime1 &lt;- crime[(crime$lon &lt; -95.344 & crime$lon &gt; -95.395) & (crime$lat &lt; 29.783 & crime$lat &gt; 29.738), ]\ncrime11 &lt;- crime %&gt;% filter((lon &lt; -95.344 & lon &gt; -95.395) & (lat &lt; 29.783 & lat &gt; 29.738))\nnrow(crime1) ; nrow(crime11)\ncrime1 %&gt;% arrange(desc(lon)) %&gt;% nrow()\ncrime11 %&gt;% arrange(desc(lon)) %&gt;% nrow()\n\nggmap(Houstonmap) + geom_point(data = crime1, aes(x = lon, y = lat), alpha = 0.3)\nggmap(Houstonmap) + geom_point(data = crime1, aes(x = lon, y = lat, colour = offense))\n\ncrime2 &lt;- crime1[!duplicated(crime1[, c(\"lon\", \"lat\")]), ] # 위, 경도에 대해 중복되지 않게 하나의 관측치만 선택\n\ncrime2$offense &lt;- as.character(crime2$offense) # 범죄 종류 문자형으로 변경\n\ncrime2$offense[crime2$offense == \"murder\" | crime2$offense == \"rape\"] &lt;- \"4\"\ncrime2$offense[crime2$offense == \"robbery\" | crime2$offense == \"aggravated assault\"] &lt;- \"3\"\ncrime2$offense[crime2$offense == \"burglary\" | crime2$offense == \"auto theft\"] &lt;- \"2\"\ncrime2$offense[crime2$offense == \"theft\"] &lt;- \"1\"\n\ncrime2$offense &lt;- as.numeric(crime2$offense) # 범죄 종류 문자형을 숫자형으로 변경\n\nggmap(Houstonmap) + geom_point(data = crime2, aes(x = lon, y = lat, size = offense), alpha = 0.2)\n\n# 범죄 위험도에 따라 점의 크기 및 색깔로 구별\nggmap(Houstonmap) + geom_point(data = crime2, aes(x = lon, y = lat, size = offense, colour = offense), alpha = 0.5) +\n  scale_colour_gradient(low = \"white\", high = \"red\")\n\ncrime3 &lt;- crime2[crime2$date == \"1/1/2010\", ]\n\ncrime4 &lt;- crime3[!duplicated(crime3[, c(\"hour\")]), ]\n\nnrow(crime3) ; nrow(crime4)\n\nggmap(Houstonmap) + geom_point(data = crime3, aes(x = lon, y = lat)) +\n  geom_text(data = crime4, aes(label = street), vjust = 1.2) +\n  geom_path(data = crime4, aes(x = lon, y = lat), color = \"red\")\n\n\n\n\n\n\n\nnames(bristol_zones) ; names(bristol_od)\n#&gt; [1] \"geo_code\" \"name\"     \"geometry\"\n#&gt; [1] \"o\"          \"d\"          \"all\"        \"bicycle\"    \"foot\"      \n#&gt; [6] \"car_driver\" \"train\"\nnrow(bristol_zones)  ; nrow(bristol_od)\n#&gt; [1] 102\n#&gt; [1] 2910\n\n# O : Zone of the Origin / D : Zone of the Dest\n\nzones_attr = bristol_od %&gt;%\n  group_by(o) %&gt;%\n  summarize_if(is.numeric, sum) %&gt;%\n  dplyr::rename(geo_code = o)\n\nsummary(zones_attr$geo_code %in% bristol_zones$geo_code) # 일치하는지 확인\n#&gt;    Mode    TRUE \n#&gt; logical     102\n\n\nzones_joined = left_join(bristol_zones, zones_attr, by = \"geo_code\")\nnrow(zones_joined)\n#&gt; [1] 102\nsum(zones_joined$all)\n#&gt; [1] 238805\n\nnames(zones_joined)\n#&gt; [1] \"geo_code\"   \"name\"       \"all\"        \"bicycle\"    \"foot\"      \n#&gt; [6] \"car_driver\" \"train\"      \"geometry\"\n#&gt; [1] \"geo_code\" \"name\" \"all\" \"bicycle\" \"foot\" \"car_driver\" \"train\" \"geometry\"\nnames(zones_joined)[3] &lt;- c(\"all_orig\")\nnames(zones_joined)\n#&gt; [1] \"geo_code\"   \"name\"       \"all_orig\"   \"bicycle\"    \"foot\"      \n#&gt; [6] \"car_driver\" \"train\"      \"geometry\"\n\nzones_od = bristol_od %&gt;%\n  group_by(d) %&gt;%\n  summarize_if(is.numeric, sum) %&gt;%\n  dplyr::select(geo_code = d, all_dest = all) %&gt;%\n  inner_join(zones_joined, ., by = \"geo_code\")\n\nzones_od\n#&gt; Simple feature collection with 102 features and 8 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -2.845847 ymin: 51.28248 xmax: -2.252388 ymax: 51.73982\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;     geo_code                             name all_orig bicycle foot car_driver\n#&gt; 1  E02002985 Bath and North East Somerset 001      868      30  173        414\n#&gt; 2  E02002987 Bath and North East Somerset 003      898      34  117        523\n#&gt; 3  E02003005 Bath and North East Somerset 021      786      19   91        593\n#&gt; 4  E02003012                      Bristol 001     3312     161  330       2058\n#&gt; 5  E02003013                      Bristol 002     3715     188  615       2021\n#&gt; 6  E02003014                      Bristol 003     2220     126  270       1239\n#&gt; 7  E02003015                      Bristol 004     1633     166  307        786\n#&gt; 8  E02003016                      Bristol 005     2411     218  440       1105\n#&gt; 9  E02003017                      Bristol 006     1590     187  208        898\n#&gt; 10 E02003018                      Bristol 007     1690      96  143       1048\n#&gt;    train all_dest                       geometry\n#&gt; 1     43      744 MULTIPOLYGON (((-2.510462 5...\n#&gt; 2     58      561 MULTIPOLYGON (((-2.476122 5...\n#&gt; 3      8      427 MULTIPOLYGON (((-2.55073 51...\n#&gt; 4     12      701 MULTIPOLYGON (((-2.595763 5...\n#&gt; 5      6      940 MULTIPOLYGON (((-2.593783 5...\n#&gt; 6      5     3469 MULTIPOLYGON (((-2.639581 5...\n#&gt; 7      7     4980 MULTIPOLYGON (((-2.584973 5...\n#&gt; 8     23      297 MULTIPOLYGON (((-2.565948 5...\n#&gt; 9      9     1459 MULTIPOLYGON (((-2.616485 5...\n#&gt; 10    20      128 MULTIPOLYGON (((-2.637681 5...\n\nqtm(zones_od, c(\"all_orig\", \"all_dest\")) +\ntm_layout(panel.labels = c(\"Origin\", \"Destination\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nod_top5 = bristol_od %&gt;%\n  arrange(desc(all)) %&gt;%\n  top_n(5, wt = all)\n\nbristol_od$Active = (bristol_od$bicycle + bristol_od$foot) / bristol_od$all * 100\n\nod_intra = filter(bristol_od, o == d) # 지역 내 이동\nod_inter = filter(bristol_od, o != d) # 지역 외 이동\nod_intra ; od_inter # 102행 / 2808행\n#&gt; # A tibble: 102 × 8\n#&gt;    o         d           all bicycle  foot car_driver train Active\n#&gt;    &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1 E02002985 E02002985   209       5   127         59     0   63.2\n#&gt;  2 E02002987 E02002987   166       8    61         89     2   41.6\n#&gt;  3 E02003005 E02003005   383       8    87        256     1   24.8\n#&gt;  4 E02003012 E02003012   315       5   181        102     0   59.0\n#&gt;  5 E02003013 E02003013   318       7   165        112     0   54.1\n#&gt;  6 E02003014 E02003014   414      35   139        185     0   42.0\n#&gt;  7 E02003015 E02003015   240      18   142         61     0   66.7\n#&gt;  8 E02003016 E02003016   119       7    65         30     2   60.5\n#&gt;  9 E02003017 E02003017   147       8    70         60     1   53.1\n#&gt; 10 E02003018 E02003018    67       0    39         24     1   58.2\n#&gt; # ℹ 92 more rows\n#&gt; # A tibble: 2,808 × 8\n#&gt;    o         d           all bicycle  foot car_driver train Active\n#&gt;    &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1 E02002985 E02002987   121       7    35         62     0  34.7 \n#&gt;  2 E02002985 E02003036    32       2     1         10     1   9.38\n#&gt;  3 E02002985 E02003043   141       1     2         56    17   2.13\n#&gt;  4 E02002985 E02003049    56       2     4         36     0  10.7 \n#&gt;  5 E02002985 E02003054    42       4     0         21     0   9.52\n#&gt;  6 E02002985 E02003100    22       0     0         19     3   0   \n#&gt;  7 E02002985 E02003106    48       3     1         33     8   8.33\n#&gt;  8 E02002985 E02003108    31       0     0         29     1   0   \n#&gt;  9 E02002985 E02003121    42       1     2         34     0   7.14\n#&gt; 10 E02002985 E02006887   103       5     1         36    13   5.83\n#&gt; # ℹ 2,798 more rows\n\ndesire_lines = od2line(od_inter, zones_od)\n#&gt; Creating centroids representing desire line start and end points.\n# od2line : polygon으로 되어있는 두 지역의 중심점을 계산해서 linestring으로 변환\n#&gt; Creating centroids representing desire line start and end points.\nqtm(desire_lines, lines.lwd = \"all\")\n#&gt; Legend for line widths not available in view mode.\n\n\n\n\n\n\ndesire_lines\\(distance = as.numeric(st_length(desire_lines)) desire_carshort = dplyr::filter(desire_lines, car_driver &gt; 300 & distance &lt; 5000) route_carshort = route(l = desire_carshort, route_fun = route_osrm, osrm.profile = \"car\") # foot, bike, car desire_carshort\\)geom_car = st_geometry(route_carshort)\nplot(st_geometry(desire_carshort)) plot(desire_carshort$geom_car, col = “red”, add = TRUE) plot(st_geometry(st_centroid(zones_od)), add = TRUE)\n\ngetmap &lt;- get_googlemap(\"bristol\", zoom = 11)\nbristol_map &lt;- ggmap(getmap)\n\n# 센터 조정\ngetmap &lt;- get_googlemap(center = c(-2.56, 51.53), zoom = 12)\nbristol_map &lt;- ggmap(getmap)\nbristol_map + geom_sf(data = desire_carshort, inherit.aes = F) +\n  geom_sf(data = desire_carshort$geom_car,\n          inherit.aes = F,\n          col = \"red\") +\n  geom_sf(data = st_geometry(st_centroid(zones_od)), inherit.aes = F)\n\n\n\n\n\n도로교통공단 TAAS에서는 사망교통사고 정보를 공개하고 있음\n\n교통사고 일시 부터 30일이내 사망한 경우를 사망교통사고라 정의하고 사고정보를 선택한 조건에 따라 json/xml형식으로 제공\n사망 교통 사고 정보\n\n사망사고 년, 월, 일, 시, 주야\n사망사고 건수\n사망사고 사망자수, 부상자수, 중상자수, 경상자수, 부상신고자수\n사망사고 위치 좌표 및 지역명\n사망사고 유형, 위반사항, 차량 종류, 도로 형태\n\n\n데이터 불러오기(https://taas.koroad.or.kr/api/selectDeathDataSet.do)\n\n다운받은 데이터를 R로 불러온 뒤 데이터 속성 확인하세요. 어떤 정보가 있는지, 활용할 위치 정보가 있는지 확인하세요\n\n\nSys.setlocale(\"LC_ALL\",\"Korean\")\n#&gt; Warning in Sys.setlocale(\"LC_ALL\", \"Korean\"): using locale code page other than\n#&gt; 65001 (\"UTF-8\") may cause problems\n#&gt; [1] \"LC_COLLATE=Korean_Korea.949;LC_CTYPE=Korean_Korea.949;LC_MONETARY=Korean_Korea.949;LC_NUMERIC=C;LC_TIME=Korean_Korea.949\"\ngetwd()\n#&gt; [1] \"C:/Users/Hyunsoo Kim/Desktop/senior_grade/blog/my-quarto-website\"\nraw.data &lt;- read.csv(\"./Spatial_Information_Analysis/12_20_death.csv\", header = TRUE, fileEncoding = \"EUC-KR\")\n## 구조 확인\nstr(raw.data)\n#&gt; 'data.frame':    37128 obs. of  23 variables:\n#&gt;  $ 발생년               : int  2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 ...\n#&gt;  $ 발생년월일시         : int  2012010101 2012010101 2012010108 2012010110 2012010103 2012010116 2012010210 2012010104 2012010104 2012010102 ...\n#&gt;  $ 주야                 : chr  \"야간\" \"야간\" \"주간\" \"주간\" ...\n#&gt;  $ 요일                 : chr  \"일\" \"일\" \"일\" \"일\" ...\n#&gt;  $ 사망자수             : int  1 1 1 2 1 1 2 1 1 1 ...\n#&gt;  $ 사상자수             : int  1 6 1 2 1 1 2 1 2 4 ...\n#&gt;  $ 중상자수             : int  0 5 0 0 0 0 0 0 1 0 ...\n#&gt;  $ 경상자수             : int  0 0 0 0 0 0 0 0 0 3 ...\n#&gt;  $ 부상신고자수         : int  0 0 0 0 0 0 0 0 0 0 ...\n#&gt;  $ 발생지시도           : chr  \"서울\" \"전북\" \"충남\" \"경남\" ...\n#&gt;  $ 발생지시군구         : chr  \"은평구\" \"정읍시\" \"청양군\" \"합천군\" ...\n#&gt;  $ 사고유형_대분류      : chr  \"차대사람\" \"차대차\" \"차량단독\" \"차대차\" ...\n#&gt;  $ 사고유형_중분류      : chr  \"차도통행중\" \"정면충돌\" \"공작물충돌\" \"측면충돌\" ...\n#&gt;  $ 사고유형             : chr  \"차도통행중\" \"정면충돌\" \"공작물충돌\" \"측면충돌\" ...\n#&gt;  $ 법규위반             : chr  \"안전운전 의무 불이행\" \"중앙선 침범\" \"안전운전 의무 불이행\" \"과속\" ...\n#&gt;  $ 도로형태_대분류      : chr  \"단일로\" \"단일로\" \"단일로\" \"교차로\" ...\n#&gt;  $ 도로형태             : chr  \"기타단일로\" \"기타단일로\" \"기타단일로\" \"교차로내\" ...\n#&gt;  $ 당사자종별_1당_대분류: chr  \"승용차\" \"승용차\" \"승용차\" \"승합차\" ...\n#&gt;  $ 당사자종별_2당_대분류: chr  \"보행자\" \"승용차\" \"없음\" \"승용차\" ...\n#&gt;  $ 발생위치X_UTMK       : int  949860 946537 940016 1059321 1070222 1036880 1079124 1114053 911131 955269 ...\n#&gt;  $ 발생위치Y_UTMK       : int  1957179 1737695 1832833 1748774 1834630 1827821 1708218 1761943 1861851 1952221 ...\n#&gt;  $ 경도                 : num  127 127 127 128 128 ...\n#&gt;  $ 위도                 : num  37.6 35.6 36.5 35.7 36.5 ...\n## 테이블 확인\nView(raw.data)\n\n데이터 추출하기\n\n다운받은 데이터는 전국에 대한 사망교통사고 정보이다. 대전지역에 2016년부터 2020년까지의 정보만을 추출하세요.\n\n추출한 데이터의 경도, 위도에 결측값 및 0인 데이터가 있는지 확인하세요.\n\n\n## 1. 대전 지역 2016 ~ 2020년 데이터 추출\ndaejeon &lt;- filter(raw.data,  발생지시도 == \"대전\" &  발생년 &gt; 2015)\n\n## 2. 사고 발생 시작점 경도/위도 데이터의 범위 살펴보기\nrange(daejeon$경도) ; range(daejeon$위도)\n#&gt; [1] 127.2653 127.5278\n#&gt; [1] 36.22335 36.45634\n\n## 3. 경도/위도 데이터가 NA인 데이터 확인하기\nsum(is.na(daejeon$경도)) ; sum(is.na(daejeon$위도))\n#&gt; [1] 0\n#&gt; [1] 0\n\n## 4. 경도/위도 데이터가 0인 데이터 확인하기\ndaejeon[daejeon$경도 == 0,]\n#&gt;  [1] 발생년                발생년월일시          주야                 \n#&gt;  [4] 요일                  사망자수              사상자수             \n#&gt;  [7] 중상자수              경상자수              부상신고자수         \n#&gt; [10] 발생지시도            발생지시군구          사고유형_대분류      \n#&gt; [13] 사고유형_중분류       사고유형              법규위반             \n#&gt; [16] 도로형태_대분류       도로형태              당사자종별_1당_대분류\n#&gt; [19] 당사자종별_2당_대분류 발생위치X_UTMK        발생위치Y_UTMK       \n#&gt; [22] 경도                  위도                 \n#&gt; &lt;0 rows&gt; (or 0-length row.names)\ndaejeon[daejeon$위도 == 0,]\n#&gt;  [1] 발생년                발생년월일시          주야                 \n#&gt;  [4] 요일                  사망자수              사상자수             \n#&gt;  [7] 중상자수              경상자수              부상신고자수         \n#&gt; [10] 발생지시도            발생지시군구          사고유형_대분류      \n#&gt; [13] 사고유형_중분류       사고유형              법규위반             \n#&gt; [16] 도로형태_대분류       도로형태              당사자종별_1당_대분류\n#&gt; [19] 당사자종별_2당_대분류 발생위치X_UTMK        발생위치Y_UTMK       \n#&gt; [22] 경도                  위도                 \n#&gt; &lt;0 rows&gt; (or 0-length row.names)\n\n\n\n\n\n\n\n\n\n## 5. 년도별 사고 위치 정보 지도 상에 표출하기\nlibrary(ggmap)\nregister_google(key = 'AIzaSyB4jjrVVAzb9fl8FQrQqUONAsaRBppWuSA')\nmap &lt;- qmap(location = enc2utf8(\"대전\"),\n            zoom = 12,\n            maptype = \"roadmap\")\np &lt;-\n  map + geom_point(\n    data = daejeon,\n    aes(x = 경도, y = 위도, colour = factor(발생년)),\n    size = 2,\n    alpha = 0.7\n  )\np + ggtitle(\"대전시 사망사고 위치(2016-2020)\")\n\n\nstat_bin2d() 함수 활용하여 Grid 내 사고횟수 출력\n\n\n## stat_bin2d() 함수 활용하여 특정 영역 내 사고 횟수 출력\np &lt;- map + stat_bin2d(data = daejeon,\n                      aes(x = 경도, y = 위도),\n                      bins = 30,   # bins : grid의 개수\n                      alpha = 0.5) # binwidth 로도 가능\np\n## stat_bin2d() 함수 활용하여 특정 영역 내 사고 횟수 출력/위성지도/컬러 변경\nmap &lt;- qmap(location = \"Daejeon\",\n            zoom = 12,\n            maptype = \"satellite\")\np &lt;- map + stat_bin2d(data = daejeon,\n                      aes(x = 경도, y = 위도),\n                      bins = 30,\n                      alpha = 0.5) # binwidth 로도 가능\np + scale_fill_gradient(low = \"yellow\", high = \"red\")\n\n\n\n\nGrid 내에 Count된 값 및 위치 확인하기\n\n\np_count &lt;- ggplot_build(p)$data[[4]] # cell의 값 출력\np_count &lt;- arrange(p_count, desc(value))\nhead(p_count)\n\n\nGrid 내(중심)에 Count값 표출\n\n\np + scale_fill_gradient(low = \"yellow\", high = \"red\") +\n  geom_text(data = p_count, aes((xmin + xmax) / 2, (ymin + ymax) / 2,\n                                label = count), col = \"white\")\n\n\n사고 유형 별로 표시하기\n\n\np &lt;-\n  map + stat_bin2d(\n    data = daejeon,\n    aes(\n      x = 경도,\n      y = 위도,\n      colour = factor(사고유형),\n      fill = factor(사고유형)\n    ),\n    bins = 30,\n    alpha = 0.5\n  )\np\n\n\nstat_density2d() 함수 활용하여 등고선으로 지도 위에 출력하기\n\n\nmap &lt;-\n  qmap(\n    location = \"Daejeon\",\n    zoom = 12,\n    maptype = 'roadmap',\n    color = 'bw'\n  )\np &lt;-\n  map + stat_density2d(\n    data = daejeon,\n    aes(x = 경도, y = 위도, fill = ..level..),\n    bins = 5,\n    alpha = 0.45,\n    size = 2,\n    geom = \"polygon\"\n  )\n# level : 레벨이 높을수록 더 진한색, size : 선 굵기, bins: 선 간격\np\n\n\ngeom_hex() 함수 활용하여 벌집 블롯으로 출력하기\n\n\n## 벌집 블롯으로 출력(geom_hex(), scale_fill_gradientn())\nlibrary(hexbin)\nmap &lt;- qmap(location = \"Daejeon\",\n            zoom = 11,\n            maptype = \"roadmap\")\np &lt;-\n  map + coord_cartesian() + # coord_cartesian() : 데카르트 좌표계\n  geom_hex(\n    data = daejeon,\n    aes(x = 경도, y = 위도),\n    bins = 12,\n    alpha = 0.6,\n    color = \"white\"\n  ) # geom_hex() only works with Cartesian coordinates\np\np &lt;- p + scale_fill_gradientn(colours = terrain.colors(15))\np\n\n# binwidth로 출력\np &lt;-\n  map + coord_cartesian() + geom_hex(\n    data = daejeon,\n    binwidth = c(0.05, 0.05), # binwidth : bin의 크기 설정\n    aes(x = 경도, y = 위도),\n    alpha = 0.6,\n    color = \"white\"\n  ) # geom_hex() only works with Cartesian coordinates\np\np &lt;- p + scale_fill_gradientn(colours = terrain.colors(15))\np\n\np_count &lt;- ggplot_build(p)$data[[4]] # cell의 값 출력\np_count &lt;- arrange(p_count, desc(count))\nhead(p_count)\n\n\n\n\n\n\n행정구역시군구 경계를 얻기 위해 데이터로 대전시 구 경계 shape 파일 획득\n\n\nlibrary(raster)\nlibrary(rgdal)\nlibrary(sf)\n\n## 동별 사망사고 추출하기\nmap &lt;-\n  qmap(\n    location = \"Daejeon\",\n    zoom = 11,\n    maptype = 'roadmap',\n    color = 'bw'\n  )\n\ndaejeon_area &lt;- shapefile('./Spatial_Information_Analysis/LARD_ADM_SECT_SGG_30/LARD_ADM_SECT_SGG_30.shp')\ndaejeon_area # 좌표체계 확인\n# str(daejeon_area)\nplot(daejeon_area, axes = T)\n\n\n위 plot의 좌표단위를 보면 평면직각좌표계(Projected Coordinate)를 기준으로 측정할 때 나올 수 있는 단위\n앞에서 사고 데이터의 좌표는 위경도 좌표이므로, 두 자료의 위치 좌표체계를 통일 시켜줄 필요가 있음\nspTransform() 를 통해 좌표변형 가능\n\nto_crs = CRS(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\")\n\n\nto_crs = CRS(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\")\ndaejeon_area2 &lt;- spTransform(daejeon_area, to_crs)\ndaejeon_area2\ndaejeon_area2@data # SP 데이터 내에서 출력을 하려면 @로 호출해야함\n\nplot(daejeon_area2, axes = T)\nmap + geom_polygon(\n  data = daejeon_area2,\n  aes(x = long, y = lat, group = group),\n  fill = 'white',\n  color = 'black'\n)\n\n구를 기준으로 사고 발생 횟수 계산\n\n\ngu_accident &lt;- daejeon %&gt;% group_by(발생지시군구) %&gt;% summarise(n = n())\ngu_accident\n#&gt; # A tibble: 5 x 2\n#&gt;   발생지시군구     n\n#&gt;   &lt;chr&gt;        &lt;int&gt;\n#&gt; 1 대덕구          81\n#&gt; 2 동구            98\n#&gt; 3 서구           100\n#&gt; 4 유성구          73\n#&gt; 5 중구            58\n\n\ndaejeon_area2 객체의 클래스는 SpatitalPloygonsDataFrame임\n이것을 데이터 프레임 형태로 변환해줄 때 사용하는 함수로는 ggplot2 패키지의 fortify() 함수가 있음\n구를 나타내는 SGG_NM 열로 기준\n\n\nclass(daejeon_area2)\ndaejeon_area2 &lt;- fortify(daejeon_area2, region = 'SGG_NM')\nclass(daejeon_area2)\nhead(daejeon_area2)\n\n\ndaejeon_area2의 “id”열과 gu_accident의 “발생지시군구”열을 기준으로 합치기 위해서 열Name을 “id”로 통일\nid열을 기준으로 두 데이터셋을 합쳐줌\n\n\nnames(gu_accident)[1] &lt;- \"id\"\ndaejeon_area3 &lt;- merge(daejeon_area2, gu_accident, by = 'id')\nhead(daejeon_area3)\n\ndaejeon_area3 %&gt;% group_by(id) %&gt;% summarise(n = mean(n))\n\n\ngeom_polygon()을 이용한 시각화\n\n\np &lt;-\n  map + geom_polygon(data = daejeon_area3,\n                     aes(\n                       x = long,\n                       y = lat,\n                       group = group,\n                       fill = n\n                     ),\n                     alpha = .5)\np\np + scale_fill_gradient(low = 'yellow', high = 'red')\nlibrary(viridis)\np + scale_fill_viridis()\n\n\n\n\n\n\n\n\n구경계 데이터(daejeon_area2)를 sf클래스로 변환\nst_as_sf() : sp클래스를 sf클래스로 변환\n\n\ndaejeon_area2 &lt;- spTransform(daejeon_area, to_crs)\ndaejeon_area2\ndaejeon_area_sf &lt;- st_as_sf(daejeon_area2) # sp 클래스를 sf 클래스로 전환하기\ndaejeon_area_sf\nplot(st_geometry(daejeon_area_sf))\n\n\nst_point_on_surface() : 각 구별 지도상 중심점 구한 뒤 지도상에 표출\n\n\n# 각 구별 Center\ndaejeon_area_center &lt;- st_point_on_surface(daejeon_area_sf)\nplot(st_geometry(daejeon_area_sf))\nplot(daejeon_area_center , add = T, col = \"black\")\n\n\n사망사고데이터(point)를 sf클래스로 변환\n\n\ndaejeon_acc_sf &lt;-\n  daejeon %&gt;% st_as_sf(coords = c(\"경도\", \"위도\"),\n                       crs = 4326,\n                       remove = FALSE)\ndaejeon_acc_sf ## CRS : # WGS84\n\n# daejeon_acc &lt;- daejeon %&gt;% st_as_sf(coords = c(\"발생위치X_UTMK\", \"발생위치Y_UTMK\"),\n#                                     crs = 4326,\n#                                     remove = FALSE)\n# daejeon_acc\n\n\nst_intersection을 통해서 폴리곤(구경계)와 포인트(사망사고지점)데이터 합치기\n\n\n## Intersection between polygon and points\nintersection &lt;- st_intersection(daejeon_area_sf, daejeon_acc_sf)\nhead(intersection)\n\n## Plot intersection\nplot(st_geometry(daejeon_area_sf))\nplot(intersection, add = T, pch = 1)\n\n\n구별 사망사고 건수 Count하기\n\n\n## View result\ntable(intersection$SGG_NM)\n\n## Using dplyr\nint_result &lt;- intersection %&gt;%\n  group_by(SGG_NM) %&gt;%\n  count()\nint_result\n\n\nst_join() : 경계 데이터(daejeon_area_sf)에 결과(int_result) 합치기\n\n\nint_result0 &lt;- st_join(daejeon_area_sf, int_result)\nint_result0\n\n\nmap 위에 시각화\n\n\nmap &lt;-\n  qmap(\n    location = \"Daejeon\",\n    zoom = 11,\n    maptype = 'roadmap',\n    color = 'bw'\n  )\np &lt;-\n  map + geom_sf(data = int_result0,\n                inherit.aes = F, # sf형태 data 그릴 때 반드시 필요\n                aes(fill = n),\n                alpha = .5)\np\np + scale_fill_gradient(low = 'yellow', high = 'red')"
  },
  {
    "objectID": "Spatial_Information_Analysis.html#chapter-1-r을-이용한-공간정보-분석",
    "href": "Spatial_Information_Analysis.html#chapter-1-r을-이용한-공간정보-분석",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "공간정보란 ? 사람들이 생활하고 있는 공간 상에서 사건이나 사물에 대한 위치를 나타내는 정보\n\n위치를 나타내는 정보는 (1) 위치를 표현하는 정보 (2) 해당 위치에 나타나는 특성에 대한 정보\n\n위치를 표현하는 정보 : 공간 상에서 사건이나 사물의 위치가 어디에 있는지를 나타내는 정보\n\nex) 주소, 위경도, x,y 좌표 등\n\n해당 위치에 나타나는 특성에 대한 정보 : 특정 위치에 있는 사건이나 사물을 설명하는 정보\n\nex) 학교, 회사, 학생 수 , 교사 수, 사고 건 수, 사고 유형 등\n\n\n\n지리정보 시스템(Geographic Information System) : 공간정보데이터를 처리, 가공하여 새로운 정보를 도출하는 일련의 과정 또는 기법\n\nex) 교통사고 데이터 분석 (TIMS)\n\n공간정보를 이용하여 GIS 분석을 수행하기 위한 소프트웨어\n\n전용 소프트웨어\n\nArcGIS : 전문적인 공간정보의 처리와 분석 가능, 고가(유료)\nQGIS : 오픈소스 GIS 소프트웨어, 최근 많은 분야에서 GIS 소프트웨어로 활용\n\n오픈소스 소프트웨어\n\nR 소프트웨어 : 오픈소스 기반의 통계 프로그램, 공간정보의 처리와 븐석에도 강력한 기능\nPython 소프트웨어 : 배우기 쉽고, 강력한 프로그래밍 언어, 공간정보를 다루는데 유용한 라이브러리가 개발\n\n\n\n\n\n\n\n위치정보와 속성정보로 구분\n\n위치정보\n\n좌표체계를 이용한 위치정보\n\n지리좌표계에서 이용하는 경도와 위도로 표현 ex) 경위도좌표\n수학적으로 X좌표와 Y좌표로 위치 정보를 표현 ex) 평면직각좌표(지도좌표)\n\n공간정보 데이터의 위치정보 표현 방식\n\n벡터 (점, 선, 면)\n래스터 (일정한 격자 또는 화소)\n\n\n속성정보\n\n주어진 위치에 있는 사건이나 사물에 대한 자료\n\n\n\n\n\n\n\n지리좌표체계 : 경도와 위도로 위치를 표현하는 지리좌표체계\n투영좌표체계 : 지도투영법을 적용하여 둥근 지구를 평면으로 변환한 후, 직각좌표체계를 이용하여 x좌표와 y좌표의 직각좌표체계로 위치를 표현\n\n원통도법, 원추도법, 평면도법이 있음.\nUTM 좌표체계, TM 좌표계, UTM-K 좌표계\n우리나라는 ITRF2000 지구중심좌표계를 따르고 타원체로는 GRS80 타원체를 적용\n\n\n\n\n\n\nshapefile\n\n.shp : 공간정보(점, 선, 다각형)\n.shx : geometry와 속성 정보 연결\n.dbf : 속성정보\n.drj : 좌표계 정보 저장\n.sbn : 위치 정보 저장\n\ngeojson : json 또는 xml 파일 포맷 필요요"
  },
  {
    "objectID": "Spatial_Information_Analysis.html#chapter2-geographic-data-in-r",
    "href": "Spatial_Information_Analysis.html#chapter2-geographic-data-in-r",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "패키지\n\nsf : 지리 공간 벡터 데이터(vector data) 분석을 위한 패키지\nraster : 지리 공간 레스터 데이터(raster data)를 처리 및 분석하는데 사용\nspData : 37개의 지리 공간 데이터셋이 내장\nspDataLarge : 지리공간 데이터 샘플을 내장\n\nvignetee(package = \" \") : 설치된 모든 패키지에 대한 이용가능한 모든 목록을 출력\nst_as_sf() : st 데이터를 sf로 변환하는 함수\nst_centroid : 폴리곤의 중심점을 계산하는 함수\nplot 함수 위에 다른 지도 층을 추가 : plot() 함수 안에 add = TRUE 사용\n\n\n\n\nst_point() : A point\nst_linestring() : A linestring\nst_polygon() : A polygon\nst_multipoint() : A multipoint\nst_multilinestring() : A multilinestring\nst_multipolygon() : A multipolygon\nst_geometrycollection() : A geometry collection\n\n\n\n\n\nst_sfc() : 두 개의 지리특성(feature)을 하나의 칼럼 객체로 합치는 함수\nst_geometry_type() : 기하유형을 확인\nst_crs() : 특정 CRS를 지정\n\n특정 CRS를 지정하기 위해 epsg(SRID) 또는 proj4string 속성을 사용\n\nepsg 코드\n\n장점 : 짧아서 기억하기 쉬움\nsfc 객체 내의 모든 geometries는 동일한 CRS를 가져야 함.\nEPSG : 4326 : GPS가 사용하는 좌표계\n\nproj4string 정의\n\n장점 : 투사 유형이나 datum, 타원체 등의 다른 모수들을 구체화할 수 있는 유연성이 있음\n단점 : 사용자가 구체화를 해야하므로 길고 복잡하며 기억하기 어려움\n\nst_sf() : sfc와 class sf의 객체들을 하나로 통합\n\n\nlibrary(raster)\nlibrary(rgdal)\n\nraster_filepath &lt;- system.file(\"raster/srtm.tif\", package = \"spDataLarge\")\nnew_raster &lt;- raster(raster_filepath)\nnew_raster\n# class      : RasterLayer\n# dimensions : 457, 465, 212505  (nrow, ncol, ncell)\n# resolution : 0.0008333333, 0.0008333333  (x, y)\n# extent     : -113.2396, -112.8521, 37.13208, 37.51292  (xmin, xmax, ymin, ymax)\n# crs        : +proj=longlat +datum=WGS84 +no_defs\n# source     : srtm.tif\n# names      : srtm\n# values     : 1024, 2892  (min, max)\n\n\ndim() : 행, 열, 층의 수\nncell() : 셀의 수\nres() : 해상도\nextent() : 경계값\ncrs() : 좌표계\ninMemory() : 래스터 데이터가 메모리에 저장되어 있는지(논리값 출력)\n\n\n\n\n\nRasterLayer class\nRasterBrick Class\nRasterStack class\n\n\nRasterLayer : 한 개의 층으로 구성되어 있는 래스터\nRasterBrick : 여러개의 층으로 구성되어 있는 래스터\n\n단일 다중 스펙트럼 위성 파일, 메모리의 단일 다층 객체의 형태\nbrick() 함수를 사용하여 다층 래스터 파일을 로드\n\nRasterStack : 여러개의 층으로 구성되어 있는 래스터\nnlayers() : 래스터 데이터의 층의 수\n\n\n\n\nRasterBrick : 동일한 복수 개의 RasterLayer 층으로 구성\nRasterStack : 여러 개의 RasterLayer과 RasterBrick 객체가 혼합\n\n\n\n\n\nRasterBrick : 하나의 다층 래스터 파일이나 객체를 처리\nRasterStack : 여러 개의 래스터 파일들이나 여러 종류의 래스터 클래스를 한꺼번에 연걸해서 연산하고 처리\n\n\n\n\n\n\n지리 좌표계\n\n위도와 경도를 이용해 지구 표면의 위치를 정의\n미터가 아니라, 각도로 거리 측정\n타원 표면, 구면 표면\nWGS84\n\n투영(투사) 좌표계\n\n암묵적으로 “평평한 표면” 위의 데카르트 좌표 기반 -&gt; 왜곡 발생\n원점, x축, y축\n미터와 같은 선형 측정 단위\n평면, 원뿔, 원통의 3가지 투영 유형\n\nst_set_crs() : 좌표계가 비어있거나 잘못 입력되어 있는 경우에 좌표계를 설정\nst_transform() : 투영 데이터 변환\nst_area() : 벡터 데이터의 면적 계산 -&gt; [m^2] 단위가 같이 반환\n좌표계 설정할 때,\n\n벡터 데이터 : epsg코드나 proj4string정의 모두 사용 가능\n래스터 데이터 : proj4string 정의만 사용"
  },
  {
    "objectID": "Spatial_Information_Analysis.html#chapter3-attribute-data-operations",
    "href": "Spatial_Information_Analysis.html#chapter3-attribute-data-operations",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "sf 객체에서 속성 정보만 가져오기 : st_drop_geometry()\n\n\nBase R 구문으로 벡터 데이터 속성 정보의 행과 열 가져오기\n\n\ndplyr로 벡터 데이터 속성 정보의 행과 열 가져오기\n\n\n한 개 컬럼만 가져온 결과를 벡터로 반환하기\n\n\n\n\n\n지리공간 sf 객체는 항상 점, 선, 면 등의 지리기하 데이터를 리스트로 가지고 있는 geometry 칼럼이 항상 따라다님\nsf 객체로부터 이 geometry 칼럼을 제거하고 나머지 속성 정보만으로 Dataframe을 만들고 싶다면 sf패키지의 st_drop_geometry()를 사용\ngeometry 칼럼의 경우 지리기하 점, 선, 면 등의 리스트 정보를 가지고 있어 메모리 점유가 크기때문에, 사용할 필요가 없다면 geometry 칼럼을 제거하고 속성 정보만으로 Dataframe으로 만들어서 분석을 진행하는게 좋음\n\n\n\n\n\nR Dataframe에서 i행과 j열을 가져올 때 : df[i, j], subset(), $을 사용\n\n\ni행과 j열 위치를 지정 ex) world[1:6, ]\n\n\nj행의 이름을 이용 ex) world[, c(\"name_long\", \"lifeExp\")]\n\n\n논리 벡터를 사용해서 i행의 부분집합 ex) sel_area &lt;- world$area_km2 &lt; 10000\n\n\n\n\n\n\n\ndplyr 패키지에서는 체인(%&gt;%)으로 파이프 연산자를 사용하여 가독성이 좋고, 속도가 빠름\n\n\nselect() 함수를 사용하여 특정 열 선택\n\n\nselect(sf, name)\nselect(sf, name1:name2)\nselect(sf, position) ex) select(world, 2, 7)\nselect(sf, -name)\nselect(sf, name_new = name_old) : 열 선택하여 이름 변경\nselect(sf, contain(string)) : 특정 문자열을 포함한 칼럼을 선택\n\ncontain(), starts_with(), ends_with(), matches(), num_range()\n\n\n\nfilter() 함수를 사용하여 조건을 만족하는 특정 행 추출\n\n\nsubset() 함수와 동일한 기능\n\n\naggregate() 함수를 사용하여 지리 벡터 데이터의 속성 정보를 그룹별로 집계\n\n\naggregate(x ~ group, FUN, data, ...)\ndata.frame을 반환하며, 집계된 결과에 지리 기하(geometry) 정보는 없음\nworld[‘pop’]은 “sf” 객체이기 때문에 집계 결과가 “sf” 객체로 반환\nworld$pop은 숫자형 벡터이므로 aggregate() 함수를 적용하면 집계 결과가 “data.frame”으로 반환\n\n\nsummarize(), group_by() 함수를 이용한 지리벡터 데이터의 속성 정보를 그룹별로 집계\n\n\ngroup_by() : 기준이 되는 그룹을 지정\nsummarize() : 다양한 집계 함수를 사용\n\nsum(), n() : 합계와 개수 집계\ntop_n() : 상위 n개 추출\narrange() : 오름차순 정렬, desc()를 사용하면 내림차순 정렬\nst_drop_geometry() : geometry 열 제거\n\n\n\n\n\n\n\n\nR의 sf클래스 객체인 지리공간 벡터 데이터를 dplyr의 함수를 사용해서 두 테이블을 join하면 속성과 함께 지리공간 geometry 칼럼과 정보도 join된 후의 테이블에 자동으로 그대로 따라감\n\n\n## 두 데이터 셋에 같은 이름을 가지는 변수가 없는 경우\n\n```         \na)  하나의 key variable의 이름을 바꿔서 통일시켜줌\n```\n\n-   \n\n    b)  `by`를 사용하여 결합변수를 지정\n\n\n\n# coffee_data의 name_long변수 이름을 nm으로 변경\ncoffee_renamed &lt;- rename(coffee_data, nm = name_long)\n# by 사용하여 결합 변수를 지정하여 다른이름변수를 기준으로 조인하기\nworld_coffee1 &lt;- left_join(world, coffee_renamed, by = c(name_long = \"nm\"))\n\n\ninner_join() 함수를 사용하면 겹치는 행만 추출\n\nsetdiff() : 일치하지 않는 행 추출\ngrepl() : 텍스트 찾는 함수 (논리값으로 출력)\ngrep() : 텍스트 찾는 함수 (행 번호 출력)\n\n\n\n\n\n\ndplyr로 지리 벡터 데이터에 새로운 속성 만들기\n\nmutate() : 기존 데이터 셋에 새로 만든 변수(열) 추가\ntransmute() : 기존의 열은 모두 제거하고 새로 만든 열과 지리기하 geometry열만을 반환\n\ntidyr로 지리 벡터 데이터의 기존 속성을 합치거나 분리하기\n\nunite(data, 병합 열, sep = \"_\", remove = TRUE) : 기존 속성 열을 합쳐서 새로운 속성 열을 만듦\n\nremove = TRUE를 설정해주면 기존의 합치려는 두 개의 열은 제거되고, 새로 만들어진 열만 남음\n\nseparate() : 기존에 존재하는 열을 구분자를 기준으로 두 개의 열로 분리\n\n\n\nworld_unite &lt;- world %&gt;%\n  unite(\"con_reg\", continent:region_un, sep = \":\", remove = TRUE)\nnames(world_unite)\n# \"iso_a2\"    \"name_long\" \"con_reg\"   \"subregion\" \"type\"\n# \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\" \"geom\"\n\nworld_separate &lt;- world_unite %&gt;%\n  separate(con_reg, c(\"continent\", \"region_un\"), sep = \":\")\nnames(world_separate)\n# \"iso_a2\"    \"name_long\" \"continent\" \"region_un\" \"subregion\" \"type\"\n# \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\" \"geom\" \n\ndplyr로 지리 벡터 데이터의 속성 이름 바꾸기\n\nrename(data, new_name = old_name) : 특정 속성 변수 이름 변경\nsetNames(object = nm, nm) : 여러개의 속성 칼럼을 한꺼번에 변경 또는 부여\n\n\nworld %&gt;% rename(name = name_long)\n\nnew_names &lt;- c(\"i\", \"n\", \"c\", \"r\", \"s\", \"t\", \"a\", \"p\", \"l\", \"gP\", \"geom\")\nworld %&gt;% setNames(new_names)\n\n\n\n\n\n\n\n래스터 객체의 데이터 속성은 숫자형(numeric), 정수형(integer), 논리형(logical), 요인형(factor) 데이터를 지원하며, 문자형(character)은 지원하지 않음\n\n1.  **문자형을 요인형으로 변환**(또는 논리형으로 변환) -\\&gt; `factor()` 함수 사용\n\n\n요인형 값을 속성 값으로 하여 래스터 객체를 만듦\n\n\n래스터 객체의 모든 값을 추출하거나 전체 행을 추출 : values(), getValues()"
  },
  {
    "objectID": "Spatial_Information_Analysis.html#chapter4-spatial-data-operations",
    "href": "Spatial_Information_Analysis.html#chapter4-spatial-data-operations",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "st_intersects() : 공간 부분집합 추출(교집합)\n\n\n\n\n\n\n\n\n\n\nst_intersects() : 공간적으로 관련이 있는 객체를 출력\nst_disjoint() : 공간적으로 관련되지 않은 객체만 반환\nst_within() : 공간적으로 완전히 객체 내부에 있는 객체들만 출력\nst_touches() : 공간적으로 테두리에 있는 객체들만 출력\nst_is_within_distance() : 공간적으로 주어진 거리보다 가까운 객체들을 반환\nsparse = FALSE 매개변수를 설정하면 논리값으로 출력\n\n\nst_intersects(p, a)\n#&gt; Sparse geometry binary predicate list of length 4, where the predicate\n#&gt; was `intersects'\n#&gt;  1: 1\n#&gt;  2: 1\n#&gt;  3: (empty)\n#&gt;  4: (empty)\n\nst_intersects(p, a, sparse = FALSE)\n#&gt;       [,1]\n#&gt; [1,]  TRUE\n#&gt; [2,]  TRUE\n#&gt; [3,] FALSE\n#&gt; [4,] FALSE\n\nst_disjoint(p, a, sparse = FALSE)[, 1]\n#&gt; [1] FALSE FALSE  TRUE  TRUE\n\nst_within(p, a, sparse = FALSE )[, 1]\n#&gt; [1]  TRUE FALSE FALSE FALSE\n\nst_touches(p, a, sparse = FALSE)[, 1]\n#&gt; [1] FALSE  TRUE FALSE FALSE\n\nsel &lt;- st_is_within_distance(p, a, dist = 0.9) # can only return a sparse matrix\nlengths(sel) &gt; 0\n#&gt; [1]  TRUE  TRUE FALSE  TRUE\n\n\n\n\n\nst_join() : 공간 결합 함수\n\n\nrandom_joined = st_join(random_points, world[\"name_long\"]) ; random_joined\n#&gt; Simple feature collection with 10 features and 1 field\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -158.1893 ymin: -42.91501 xmax: 165.1157 ymax: 80.5408\n#&gt; Geodetic CRS:  WGS 84\n#&gt; # A tibble: 10 × 2\n#&gt;                 geometry name_long\n#&gt;  *           &lt;POINT [°]&gt; &lt;chr&gt;    \n#&gt;  1 (-58.98475 -21.24278) Paraguay \n#&gt;  2  (-13.05963 25.42744) Morocco  \n#&gt;  3   (-158.1893 80.5408) &lt;NA&gt;     \n#&gt;  4  (-108.9239 27.80098) Mexico   \n#&gt;  5   (-9.246895 49.9822) &lt;NA&gt;     \n#&gt;  6  (-71.62251 20.15883) &lt;NA&gt;     \n#&gt;  7  (38.43318 -42.91501) &lt;NA&gt;     \n#&gt;  8  (-133.1956 6.053818) &lt;NA&gt;     \n#&gt;  9   (165.1157 38.16862) &lt;NA&gt;     \n#&gt; 10   (16.86581 53.86485) Poland\n\n\n\n\n\n\n기호(plotting symbols, characters) : pch\n기호의 크기 : cex\n선 두께 : lwd\n선 유형 : lty\n\n\n\n\nany() : 특정 값이 포함되어 있는지 확인할 때 유용, 여기서 TRUE가 있는지 확인 가능\n\n\nany(st_touches(cycle_hire, cycle_hire_osm, sparse = FALSE))\n#&gt; [1] FALSE\n\n\nlibrary(mapview)\nlibrary(tmap)\ntmap_mode(\"view\")\ntm_basemap(\"Stamen.Terrain\") +\n  tm_shape(cycle_hire) +\n  tm_symbols(col = \"red\", shape = 16, size = 0.5, alpha = .5) +\n  tm_shape(cycle_hire_osm) +\n  tm_symbols(col = \"blue\", shape = 16, size = 0.5, alpha = .5) +\n  tm_tiles(\"Stamen.TonerLabels\")\n\n\n\n\n\n\n\nst_transform() : 투영데이터로 변환을 위한 함수\nst_is_within_distance() : 임계 거리보다 가까운 객체들을 반환\n\n\ncycle_hire_P &lt;- st_transform(cycle_hire, 27700)\ncycle_hire_osm_P &lt;- st_transform(cycle_hire_osm, 27700)\nsel &lt;- st_is_within_distance(cycle_hire_P, cycle_hire_osm_P, dist = 20)\nsummary(lengths(sel) &gt; 0)\n#&gt;    Mode   FALSE    TRUE \n#&gt; logical     304     438\n\n\nst_join()을 사용하여 dist 인수를 추가하여 구할 수도 있음\n\nst_join()을 사용하면 조인된 결과의 행 수가 더 크다.\n이는 cycle_hire_P의 일부 자전거 대여소가 cycle_hire_osm_P와 여러개가 겹치기 때문임\n겹치는 점에 대한 값을 집계하고 평균을 반환하여 문제를 해결 가능\n\n\nz = st_join(cycle_hire_P, cycle_hire_osm_P,\n            join = st_is_within_distance, dist = 20)\nnrow(cycle_hire) ; nrow(z)\n#&gt; [1] 742\n#&gt; [1] 762\n\nz = z %&gt;%\n  group_by(id) %&gt;%\n  summarize(capacity = mean(capacity))\nnrow(z) == nrow(cycle_hire)\n#&gt; [1] TRUE\n\n\n\n\n\n\naggregate()와 group_by() %&gt;% summarize()를 활용하여 그룹별 통계값 계산(평균, 합 등)\n\n\n# aggregate() 사용\nnz_avheight &lt;- aggregate(x = nz_height, by = nz, FUN = mean)\nplot(nz_avheight[2])\n\n\n\n# group_by() %&gt;% summarize() 사용\nnz_avheight2 &lt;- nz %&gt;%\n  st_join(nz_height) %&gt;%\n  group_by(Name) %&gt;%\n  summarize(elevation = mean(elevation, na.rm = TRUE))\nplot(nz_avheight2[2])\n\n\n\n\n\nst_interpolate_aw() : 면적의 크기에 비례하게 계산(면적 가중 공간 보간)\n\n\nsum(incongruent$value)\n#&gt; [1] 45.41184\n\nagg_aw = st_interpolate_aw(incongruent[, \"value\"],\n                           aggregating_zones,\n                           extensive = TRUE)\n#&gt; Warning in st_interpolate_aw.sf(incongruent[, \"value\"], aggregating_zones, :\n#&gt; st_interpolate_aw assumes attributes are constant or uniform over areas of x\nagg_aw$value\n#&gt; [1] 19.61613 25.66872\n\n\n\n\n\n위상 관계는 binary인 반면 거리 관계는 연속적임\nst_distance() : 두 객체 사이의 거리 계산\n\n\nnz_heighest &lt;- nz_height %&gt;% top_n(n = 1, wt = elevation)\ncanterbury_centroid &lt;- st_centroid(canterbury)\n#&gt; Warning: st_centroid assumes attributes are constant over geometries\n\nst_distance(nz_heighest, canterbury_centroid)\n#&gt; Units: [m]\n#&gt;        [,1]\n#&gt; [1,] 115540\n\nco &lt;- filter(nz, grepl(\"Canter|Otag\", Name))\nst_distance(nz_height[1:3, ], co)\n#&gt; Units: [m]\n#&gt;           [,1]     [,2]\n#&gt; [1,] 123537.16 15497.72\n#&gt; [2,]  94282.77     0.00\n#&gt; [3,]  93018.56     0.00\n\nplot(st_geometry(co)[2])\nplot(st_geometry(nz_height)[2:3], add = TRUE)\n\n\n\n\n\n\n\n\n\n\n\ncellFromXY() or raster::extract() : 좌표값을 Cell ID로 변환\n\n\n\n\n\n\n\nid = cellFromXY(elev, xy = matrix(c(0.1, 0.1), ncol = 2))\nelev[id]\n#&gt;   elev\n#&gt; 1   16\nterra::extract(elev, matrix(c(0.1, 0.1), ncol = 2))\n#&gt;   elev\n#&gt; 1   16\n\nclip = rast(xmin = 0.9, xmax = 1.8, ymin = -0.45, ymax = 0.45,\n            res = 0.3, vals = rep(1, 9))\nelev[clip]\n#&gt;   elev\n#&gt; 1   18\n#&gt; 2   24\nterra::extract(elev, ext(clip))\n\n\noperator는 raster의 다양한 inputs을 받고, drop=FALSE로 설정했을 때, raster 객체를 반환\n\n\nelev[1:2]\n#&gt;   elev\n#&gt; 1    1\n#&gt; 2    2\nelev[2, 1:2]\n#&gt;   elev\n#&gt; 1    7\n#&gt; 2    8\nelev[1:2, drop = FALSE] # spatial subsetting with cell IDs\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 1, 2, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 0.5, 0.5  (x, y)\n#&gt; extent      : -1.5, -0.5, 1, 1.5  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : lon/lat WGS 84 (EPSG:4326) \n#&gt; source(s)   : memory\n#&gt; name        : elev \n#&gt; min value   :    1 \n#&gt; max value   :    2\nelev[2, 1:2, drop = FALSE] # spatial subsetting by row,column indices\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 1, 2, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 0.5, 0.5  (x, y)\n#&gt; extent      : -1.5, -0.5, 0.5, 1  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : lon/lat WGS 84 (EPSG:4326) \n#&gt; source(s)   : memory\n#&gt; name        : elev \n#&gt; min value   :    7 \n#&gt; max value   :    8\n\n\n\n\n\nelev + elev # 더하기\nelev^2      # 제곱\nlog(elev)   # 로그\nelev &gt; 5    # 논리"
  },
  {
    "objectID": "Spatial_Information_Analysis.html#tmap을-활용한-시각화",
    "href": "Spatial_Information_Analysis.html#tmap을-활용한-시각화",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "tmap을 plot하기 위해서는 우선 tm_shape()로 지정해야하며, + 연산자로 레이어를 추가해야함\n\nex) tm_polygons(), tm_raster(), tm_borders(), tm_symbols() 등\n\nInteractive maps : tmap_mode()를 사용하여 \"plot\",과 \"view\"모드 사용 가능\nFacet : 하나의 창에 여러 맵을 동시에 그리기\n\nFacet 하는 3가지 방법\n\n여러변수 이름 추가\nby argument of tm_facets로 공간 데이터를 나누기\ntmap_arrange() 사용\n\n\n\ntm_basemap() : 지도를 표현할 수 있는 바탕이 되는 지도\n\n\n\n# 1. 여러 변수 이름 추가\ntmap_mode(\"plot\")\ndata(World)\ntm_shape(World) +\n  tm_polygons(c(\"HPI\", \"economy\")) +\n  tm_facets(sync = TRUE, ncol = 2)\n\n\n\n\n\n# 2. by argument of `tm_facets`로 공간 데이터 나누기\ntmap_mode(\"plot\")\ndata(NLD_muni)\nNLD_muni$perc_men &lt;- NLD_muni$pop_men / NLD_muni$population * 100\ntm_shape(NLD_muni) +\n  tm_polygons(\"perc_men\", palette = \"RdYlBu\") +\n  tm_facets(by = \"province\")\n\n\n\n\n\n# 3. `tmap_arrange` 함수 사용 : 각각 그린다음에 배치\ntmap_mode(\"plot\")\ndata(NLD_muni)\ntm1 &lt;- tm_shape(NLD_muni) + tm_polygons(\"population\", convert2density = TRUE)\ntm2 &lt;- tm_shape(NLD_muni) + tm_bubbles(size = \"population\")\ntmap_arrange(tm1, tm2)\n\n\n\n\n\ntmap_mode(\"view\")\ndata(World, metro, rivers, land)\ntm_basemap(\"Stamen.Watercolor\") +\n  tm_shape(metro) + tm_bubbles(size = \"pop2020\", col = \"red\") +\n  tm_tiles(\"Stamen.TonerLabels\")\n\n\n\n\n\n\n\nOption and styles\n\ntm_layout() : map layout 지정\ntm_options() 내에서 설정\n\ntmap_options_diff() : default tmap options과 차이점 출력\ntmap_options_reset() : default tmap options으로 설정\n\nreset을 해주지 않으면 option이 계속 설정되어있음\n\n\ntmap_style() : 지도 스타일 설정\n\n\ntmap_mode(\"plot\")\ntm_shape(World) +\n  tm_polygons(\"HPI\") +\n  tm_layout(bg.color = \"skyblue\", inner.margins = c(0, .02, .02, .02))\n\n\n\n\n\ntmap_options(bg.color = \"black\", legend.text.color = \"white\")\ntm_shape(World) + tm_polygons(\"HPI\", legend.title = \"Happy Planet Index\")\n\n\n\n\n\ntmap_style(\"classic\")\n## tmap style set to \"classic\"\n## other available styles are: \"white\", \"gray\", \"natural\", \"cobalt\",\n## \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"watercolor\"\n\ntm_shape(World) +\n  tm_polygons(\"HPI\", legend.title = \"Happy Planet Index\")\n\n\n\n\nExporting maps\n\n\ntm &lt;- tm_shape(World) +\n  tm_polygons(\"HPI\", legend.title = \"Happy Planet Index\")\n\n## save an image (\"plot\" mode)\ntmap_save(tm, filename = \"./Spatial_Information_Analysis/world_map.png\")\n\n## save as stand-alone HTML file (\"view\" mode)\ntmap_save(tm, filename = \"./Spatial_Information_Analysis/world_map.html\")\n\n\nQuick thematic map\n\n\nqtm(World, fill = \"HPI\", fill.pallete = \"RdYlGn\")"
  },
  {
    "objectID": "Spatial_Information_Analysis.html#chapter5-geometry-operations",
    "href": "Spatial_Information_Analysis.html#chapter5-geometry-operations",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "단순화는 일반적으로 더 작은 축척 지도에서 사용하기 위한 벡터 객체(선, 다각형)의 일반화를 위한 프로세스\nst_simplify() : 정점을 제거하여 선을 단순화시킴\n\ndTolerance : 단위가 m이며 커질수록 더 단순화\n\n\nseine_simp &lt;- st_simplify(seine, dTolerance = 2000) # 2000m\nplot(seine)\nplot(seine_simp)\nobject.size(seine) ; object.size(seine_simp)\n#&gt; 18096 bytes  9112 bytes\n\n\n\n\n\n\n단순화는 다각형에도 적용 가능\nst_simplify()를 사용하였을 때, 영역이 겹치는 경우도 발생\nrmapshaper 패키지의 ms_simplify() 함수를 사용\nkeep_shapes = TRUE : 개체 수는 그대로 유지\n\n\nus_states\nus_states2163 &lt;- st_transform(us_states, 2163)\nus_states2163\n\nus_states_simp1 &lt;- st_simplify(us_states2163, dTolerance = 100000)\nplot(us_states[1])\nplot(us_states_simp1[1])\n\nus_states2163$AREA &lt;- as.numeric(us_states2163$AREA)\n\nlibrary(rmapshaper)\nus_states_simp2 &lt;- rmapshaper::ms_simplify(us_states2163, keep = 0.01,\n                                           keep_shapes = FALSE)\nplot(us_states_simp2[1])\n\n\n\n\n\n\n\n\n\n\n가장 일반적으로 사용되는 중심 연산은 지리적 중심 : 공간객체의 질량 중심\nst_centroid() : 지리적 중심을 생성하지만, 때때로 지리적 중심이 상위 개체의 경계를 벗어나는 경우가 발생\nst_point_on_surface() : 상위 개체 위에 중심이 생성\n\n\nnz_centroid &lt;- st_centroid(nz)\nseine_centroid &lt;- st_centroid(seine)\n\nnz_pos &lt;- st_point_on_surface(nz)\nseine_pos &lt;- st_point_on_surface(seine)\n\nplot(st_geometry(nz), main = \"nz\")\nplot(nz_centroid ,add=T, col=\"black\")\nplot(nz_pos ,add=T, col=\"red\")\n\nplot(st_geometry(seine), main = \"seine\")\nplot(seine_centroid ,add=T, col=\"black\")\nplot(seine_pos ,add=T, col=\"red\")\n\n\n\n\n\n\n\n\n\n\n버퍼 : 기하학적 특징의 주어진 거리 내 영역을 나타내는 다각형\n지리데이터 분석에 자주 활용됨\nst_buffer() : 버퍼 생성 함수, 최소 두 개의 인수가 필요함\n\n\nseine_buff_5km &lt;- st_buffer(seine, joinStyle = \"ROUND\", dist = 5000)\nseine_buff_20km &lt;- st_buffer(seine, dist = 20000)\n\nplot(seine,col=\"black\", reset = FALSE)\nplot(seine_buff_5km, col=adjustcolor(1:3, alpha = 0.2), add=T)\n\nplot(seine,col=\"black\", reset = FALSE)\ncol1 &lt;- adjustcolor(\"red\", alpha=0.2)\ncol2 &lt;- adjustcolor(\"blue\", alpha=0.2)\ncol3 &lt;- adjustcolor(\"green\", alpha=0.2)\nplot(seine_buff_20km, col=c(col1,col2,col3), add=T)\n\n\n\n\n\n\n\n\n\n\n왜곡되거나 잘못 투영된 지도를 기반으로 생성된 geometry를 재투영하거나 개선할 때 많은 Affine 변환이 적용\n이동 : 맵 단위로 모든 포인트가 동일한 거리만큼 이동\n\n\nnz_sfc &lt;- st_geometry(nz)\nnz_shift &lt;- nz_sfc + c(0, 100000)\nplot(nz_sfc)\nplot(nz_shift,add=T, col=\"Red\")\n\n\n\n\n\n배율 조정 : 개체를 요소만큼 확대하거나 축소\n\n모든 기하 도형의 토폴로지 관계를 그대로 유지하면서 원점 좌표와 관련된 모든 좌표값을 늘리거나 줄일 수 있음\n중심점을 기준으로 기하 도형의 차이 만큼을 늘리고 0.5배 줄인 다음 다시 중심점을 더해줌\n\n\nnz_centroid_sfc &lt;- st_centroid(nz_sfc)\nnz_scale &lt;- (nz_sfc - nz_centroid_sfc) * 0.5 + nz_centroid_sfc\n\nplot(nz_sfc)\nplot(nz_scale, add=T, col=\"Red\")\n\n\n\n\n회전 : 2차원 좌표의 회전하기 위한 회전변환행렬\n\n\nmatrix(c(cos(30), sin(30), -sin(30), cos(30)), nrow = 2, ncol = 2)\n#&gt;            [,1]      [,2]\n#&gt; [1,]  0.1542514 0.9880316\n#&gt; [2,] -0.9880316 0.1542514\n\nrotation &lt;- function(a){\n  r = a * pi / 180 #degrees to radians\n  matrix(c(cos(r), sin(r), -sin(r), cos(r)), nrow = 2, ncol = 2)\n}\nnz_rotate &lt;- (nz_sfc - nz_centroid_sfc) * rotation(30) + nz_centroid_sfc\n\nplot(nz_sfc)\nplot(nz_rotate, add=T, col=\"red\")\n\n\n\n\n\n\n\n\n공간 클리핑은 영향을 받는 일부 형상의 지오메트리 열의 변경을 수반하는 공간 부분 집합의 한 형태\n\n\nb &lt;- st_sfc(st_point(c(0, 1)), st_point(c(1, 1))) # create 2 points\nb &lt;- st_buffer(b, dist = 1) # convert points to circles\nplot(b, border = \"grey\")\ntext(x = c(-0.5, 1.5), y = 1, labels = c(\"x\", \"y\"), cex = 3) # add text\n\n\n\n\n\nst_intersection() : X∩Y (x와 y의 교집합)\nst_difference() : X-Y (x와 y의 차집합)\nst_union() : X∪Y (x와 y의 합집합)\nst_sym_difference() : (X∩Y)^c (드모르간의 법칙)\n\n\npar(mfrow = c(2,2))\n\nx &lt;- b[1] ; y &lt;- b[2]\n\n# X ∩ Y\nx_and_y &lt;- st_intersection(x, y)\nplot(b, border = \"grey\", main = \"X ∩ Y\")\nplot(x_and_y, col = \"lightgrey\", border = \"grey\", add = TRUE)\ntext(x = c(-0.5, 1.5), y = 1, labels = c(\"x\", \"y\"), cex = 3)\n\n# X - Y\nx_dif_y &lt;- st_difference(x,y)\nplot(b, border = \"grey\", main = \"X - Y\")\nplot(x_dif_y, col = \"lightgrey\", border = \"grey\", add = TRUE)\ntext(x = c(-0.5, 1.5), y = 1, labels = c(\"x\", \"y\"), cex = 3)\n\n# X U Y\nx_union_y &lt;- st_union(x,y)\nplot(b, border = \"grey\", main = \"X U Y\")\nplot(x_union_y, col = \"lightgrey\", border = \"grey\", add = TRUE)\ntext(x = c(-0.5, 1.5), y = 1, labels = c(\"x\", \"y\"), cex = 3)\n\n# (X ∩ Y)^c\nx_sdif_y &lt;- st_sym_difference(x,y)\nplot(b, border = \"grey\", main = \"(X ∩ Y)^c\")\nplot(x_sdif_y, col = \"lightgrey\", border = \"grey\", add = TRUE)\ntext(x = c(-0.5, 1.5), y = 1, labels = c(\"x\", \"y\"), cex = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n클리핑 오브젝트는 지오메트리를 변경할 수 있지만 오브젝트의 부분 집합을 지정할 수도 있으며 클리핑/하위 설정 오브젝트와 교차하는 피쳐만 반환할 수도 있음\nst_sample() : x와 y의 범위 내에서 점들의 간단한 무작위 분포를 생성\n\n\nbb = st_bbox(st_union(x, y))\nbox = st_as_sfc(bb)\nset.seed(2017)\n\np = st_sample(x = box, size = 10)\nx_and_y = st_intersection(x, y)\n\nplot(b, border = \"grey\")\nplot(p, add=T)\n\n\n\n\n\nX와 Y 둘 다와 교차하는 점만을 반환하는 방법\n\n\n\n## 1번째방법\np_xy1 &lt;- p[x_and_y]\nplot(p_xy1, add=T, col=\"red\")\n\n## 2번째방법\np_xy2 &lt;- st_intersection(p, x_and_y)\nplot(p_xy2, add=T, col=\"blue\")\n\n## 3번째방법\nsel_p_xy &lt;- st_intersects(p, x, sparse = FALSE)[, 1] &\n  st_intersects(p, y, sparse = FALSE)[, 1]\np_xy3 &lt;- p[sel_p_xy]\nplot(p_xy3, add=T, col=\"green\")\n\n\n\n\n\n\n\n\n\n\n미국의 49개 주의 정보를 4개 지역으로 재구분\n\n\nplot(us_states[6])\n\n\n\n## 1. aggregate함수\nregions &lt;- aggregate(x = us_states[, \"total_pop_15\"], by = list(us_states$REGION),\n                     FUN = sum, na.rm = TRUE)\nplot(regions[2])\n\n\n\n## 2. group_by, summarize함수\nregions2 &lt;- us_states %&gt;% group_by(REGION) %&gt;%\n  summarize(pop = sum(total_pop_15, na.rm = TRUE))\n\nplot(regions2[2])\n\n\n\n\n\n위에서 aggregate()와 summarize()가 모두 지오메트리를 결합하고 st_union()을 사용하면 지오메트리만을 분해\n\n\nus_west &lt;- us_states[us_states$REGION == \"West\", ]\nplot(us_west[6])\n\n\n\nus_west_union &lt;- st_union(us_west)\nplot(us_west_union)\n\n\n\ntexas &lt;- us_states[us_states$NAME == \"Texas\", ]\ntexas_union &lt;- st_union(us_west_union, texas)\nplot(texas_union)\n\n\n\n\n\n\n\n\nst_cast() : 지오메트리 유형을 변환\n\n\nmultipoint &lt;- st_multipoint(matrix(c(1, 3, 5, 1, 3, 1), ncol = 2))\nlinestring &lt;- st_cast(multipoint, \"LINESTRING\")\npolyg &lt;- st_cast(multipoint, \"POLYGON\")\n\nplot(multipoint)\nplot(linestring)\nplot(polyg)\n\nst_length(linestring) # 길이 계산\n# [1] 5.656854\nst_area(polyg) # 면적 계산\n# [1] 4\n\n\n\n\n\n\n\nmultilinestring : 여러 개의 linestring을 하나의 묶음으로 처리\n\n\n\n\n\n\n\nmultilinestring은 각 선 세그먼트에 이름을 추가하거나 단일 선 길이를 계산할 수 없는 등 수행할 수 있는 작업 수가 제한됨\nst_cast() 함수를 사용하여 하나의 multilinestring을 세 개의 linestring로 분리\n\n\nlinestring_sf2 = st_cast(multilinestring_sf, \"LINESTRING\")\nlinestring_sf2\n#&gt; Simple feature collection with 3 features and 0 fields\n#&gt; Geometry type: LINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 1 ymin: 1 xmax: 4 ymax: 5\n#&gt; CRS:           NA\n#&gt;                    geom\n#&gt; 1 LINESTRING (1 5, 4 3)\n#&gt; 2 LINESTRING (4 4, 4 1)\n#&gt; 3 LINESTRING (2 2, 4 2)\n\n\nname과 length 추가\n\n\nlinestring_sf2$name &lt;- c(\"Riddle Rd\", \"Marshall Ave\", \"Foulke St\")\nlinestring_sf2$length &lt;- st_length(linestring_sf2)\nlinestring_sf2\n#&gt; Simple feature collection with 3 features and 2 fields\n#&gt; Geometry type: LINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 1 ymin: 1 xmax: 4 ymax: 5\n#&gt; CRS:           NA\n#&gt;                    geom         name   length\n#&gt; 1 LINESTRING (1 5, 4 3)    Riddle Rd 3.605551\n#&gt; 2 LINESTRING (4 4, 4 1) Marshall Ave 3.000000\n#&gt; 3 LINESTRING (2 2, 4 2)    Foulke St 2.000000\nplot(linestring_sf2[2])\n\n\n\n\n\n\n\n\n\n\n\n다른 공간 객체에 의해 중첩된 래스터에서 값을 추출하는 방법\n공간 출력을 검색하기 위해 거의 동일한 부분 집합 구문(많이 겹치는 부분)을 사용\ndrop = FALSE를 설정하여 행렬 구조를 유지\ncell 중간점이 clip과 겹치는 셀을 포함하는 래스터 개체를 반환\n\n\nelev &lt;- rast(system.file(\"raster/elev.tif\", package = \"spData\"))\nclip &lt;- rast(xmin = 0.9, xmax = 1.8, ymin = -0.45, ymax = 0.45,\n             resolution = 0.3, vals = rep(1, 9))\nplot(elev)\nplot(clip, add=T)\n\n\n\nelve_clip &lt;- elev[clip, drop = FALSE]\nplot(elve_clip)\n\n\n\nelev_raster &lt;- rast(system.file(\"raster/elev.tif\", package = \"spData\"))\nrcc &lt;- vect(xyFromCell(elev_raster, cell = 1:ncell(elev_raster))) # 셀의 중앙점 표시\nxyFromCell(elev_raster,1) # 1번 셀의 중앙점 좌표\n#&gt;          x    y\n#&gt; [1,] -1.25 1.25\nplot(elev)\nplot(rcc,add=T)\nplot(clip, add=T)\n\n\n\n\n\n\n\n\n다른 투사 및 해상도를 가진 두 이미지를 병합하려할 때 사용\nextend() : 래스터 범위 확장\n\n새로 추가된 행과 열은 값 매개변수의 기본값(예 : NA)를 가짐\n\norigin() : 래스터의 원점 좌표를 반환\n\n래스터의 원점은 좌표(0,0)에 가장 가까운 셀 모서리\n\n\n\nelev &lt;- rast(system.file(\"raster/elev.tif\", package = \"spData\"))\nelev_2 &lt;- extend(elev, c(1,2), snap=\"near\") # 아래/위 1행, 좌/우 2열 확장\nplot(elev)\n\n\n\nplot(elev_2, colNA=\"gray\")\n\nelev_3 &lt;- elev + elev_2\n#&gt; Error: [+] extents do not match\n\nelev_4 &lt;- extend(elev, elev_2)\nplot(elev_4, colNA=\"gray\")\n\norigin(elev_4)\n#&gt; [1] 0 0\n\norigin(elev_4) &lt;- c(0.25, 0.25)\nplot(elev_4, colNA=\"black\", add=T)\n\n\n\n\n\n\n\n\n\n래스터 데이터 셋은 해상도가 서로 다를 수 있음\n해상도를 match 시키기 위해 하나의 래스터 해상도를 감소(aggregate())시키거나 증가(disagg()) 시켜야 함\n\n\n# devtools::install_github(\"geocompr/geocompkg\")\ndem &lt;- rast(system.file(\"raster/dem.tif\", package = \"spDataLarge\"))\ndem_agg &lt;- aggregate(dem, fact = 5, fun = mean)\ndem_disagg &lt;- disagg(dem_agg, fact = 5, method = \"bilinear\")\nplot(dem)\n\n\n\nplot(dem_agg)\n\n\n\nplot(dem_disagg)\n\n\n\nidentical(dem, dem_disagg)\n#&gt; [1] FALSE\n\n\n새롭게 만들어지는 cell의 값을 만드는 두가지 방법\n\nDefault method(method = “near”) : 입력 셀의 값을 모든 출력 셀에 제공\nbilinear method : 입력 이미지의 가장 가까운 4개의 픽셀 중심을 사용하여 거리에 의해 가중된 평균을 계산\n\n\n\n\n\n\nResampling : 원래 그리드에서 다른 그리드로 래스터 값을 전송하는 프로세스\n이 프로세스는 원래 래스터의 값을 가지고, 사용자 지정 해상도와 원점을 가지고 대상 래스터의 새 값을 다시 계산함\n해상도/원점이 다른 래스터의 값을 재계산(추정)하는 방법\n\nNearest neighbor : 원래 래스터의 가장 가까운 셀 값을 대상 래스터의 셀에 할당. 속도가 빠르고 일반적으로 범주형 래스터에 적합\nBilinear interpolation(이중선형보간) : 원래 래스터에서 가장 가까운 4개의 셀의 가중 평균을 대상 1개의 셀에 할당. 연속 래스터를 위한 가장 빠른 방법\nCubic interpolation(큐빅 보간) : 본 래스터의 가장 가까운 16개 셀의 값을 사용하여 출력 셀 값을 결정하고 3차 다항식 함수를 적용. 연속 래스터에 사용. 2선형 보간보다 더 매끄러운 표면을 만들지만, 계산적으로 까다로움\nCubic spline interpolation(큐빅 스플라인 보간) : 원래 래스터의 가장 가까운 16개의 셀의 값을 사용하여 출력 셀 값을 결정하지만 큐빅 스플라인(3차 다항식 함수)을 적용\nLanczos windowed sinc resampling(Lanczos 윈도우 재샘플링) : 원래 래스터의 가장 가까운 셀 36개의 값을 사용하여 출력 셀 값을 결정\nsum\nmin, q1, med, q3, max, average, mode, rms\n\nNearest neighbor은 범주형 래스터에 적합한 반면, 모든 방법은 연속형 래스터에 사용\nresample(x, y, method = \"bilinear\", filename = \"\", ...) : 리샘플링 함수\n\n\nlibrary(terra)\n\ntarget_rast &lt;- rast(xmin = 794600, xmax = 798200,\n                    ymin = 8931800, ymax = 8935400,\n                    resolution = 150, crs = \"EPSG:32717\")\ntarget_rast\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 24, 24, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 150, 150  (x, y)\n#&gt; extent      : 794600, 798200, 8931800, 8935400  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : WGS 84 / UTM zone 17S (EPSG:32717)\n\nplot(dem)\n\n\n\nplot(target_rast)\n\n\n\n\n\n\"near\" : 셀에 가장 가까운 픽셀에서 값을 가져옴\n\n\ndem_resampl_1 &lt;- resample(dem, y = target_rast, method = \"near\")\nplot(dem_resampl_1)\n\n\n\n\n\n\"bilinear\" : 네 개의 가장 가까운 셀의 가중 평균\n\n\ndem_resampl_2 &lt;- resample(dem, y = target_rast, method = \"bilinear\")\nplot(dem_resampl_2)\n\n\n\n\n\n\"average\" : 각각의 새로운 셀이 중복되는 모든 입력 셀의 가중 평균\n\n\ndem_resampl_3 &lt;- resample(dem, y = target_rast, method = \"average\")\nplot(dem_resampl_3)"
  },
  {
    "objectID": "Spatial_Information_Analysis.html#chapter-6-raster-vector-interactions",
    "href": "Spatial_Information_Analysis.html#chapter-6-raster-vector-interactions",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "입력 래스터 데이터 세트의 범위가 관심 영역보다 클 경우 래스터 자르기(Cropping) 및 마스킹(Masking)은 입력 데이터의 공간 범위를 통합하는 데 유용함\n두 작업 모두 후속 분석 단계에 대한 객체 메모리 사용 및 관련 계산 리소스를 줄이고 래스터 데이터를 포함하는 매력적인 맵을 만들기 전에 필요한 전처리 단계임\n대상 개체와 자르기 개체는 모두 동일한 투영을 가져야 함\ncrop() : 두 번째 인수에 대한 래스터를 잘라냄\nmask() : 두 번째 인수에 전달된 개체의 경계를 벗어나는 값을 NA로 설정\n\n대부분의 경우 crop()과 mask()를 함께 사용\n\n\nsrtm &lt;- rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\nzion &lt;- read_sf(system.file(\"vector/zion.gpkg\", package = \"spDataLarge\"))\nzion &lt;- st_transform(zion, crs(srtm)) # zion을 srtm 좌표계랑 동일하게\nplot(srtm)\nplot(vect(zion),add=T)\n\n\n\nsrtm_cropped &lt;- crop(srtm, vect(zion))\nplot(srtm_cropped)\n\n\n\nsrtm_masked &lt;- mask(srtm, vect(zion))\nplot(srtm_masked)\n\n\n\nsrtm_cropped &lt;- crop(srtm, vect(zion))\nsrtm_final &lt;- mask(srtm_cropped, vect(zion))\nplot(srtm_final)\n\n\n\n\nupdatevalue = 0 : 외부의 모든 픽셀이 0으로 설정\ninverse = TRUE : 경계 내에 있는 것들이 마스킹\n\n\nsrtm_update0 &lt;- mask(srtm, vect(zion), updatevalue = 0)\nplot(srtm_update0)\n\n\n\nsrtm_inv_masked &lt;- mask(srtm, vect(zion), inverse = TRUE)\nplot(srtm_inv_masked)\n\n\n\n\n\n\n\n## Original / Crop / Mask / Inverse Map\nlibrary(tmap)\nlibrary(rcartocolor)\n\nterrain_colors = carto_pal(7, \"Geyser\")\n\npz1 = tm_shape(srtm) +\n  tm_raster(palette = terrain_colors, legend.show = FALSE, style = \"cont\") +\n  tm_shape(zion) +\n  tm_borders(lwd = 2) +\n  tm_layout(main.title = \"A. Original\", inner.margins = 0)\n\npz2 = tm_shape(srtm_cropped) +\n  tm_raster(palette = terrain_colors, legend.show = FALSE, style = \"cont\") +\n  tm_shape(zion) +\n  tm_borders(lwd = 2) +\n  tm_layout(main.title = \"B. Crop\", inner.margins = 0)\n\npz3 = tm_shape(srtm_masked) +\n  tm_raster(palette = terrain_colors, legend.show = FALSE, style = \"cont\") +\n  tm_shape(zion) +\n  tm_borders(lwd = 2) +\n  tm_layout(main.title = \"C. Mask\", inner.margins = 0)\n\npz4 = tm_shape(srtm_inv_masked) +\n  tm_raster(palette = terrain_colors, legend.show = FALSE, style = \"cont\") +\n  tm_shape(zion) +\n  tm_borders(lwd = 2) +\n  tm_layout(main.title = \"D. Inverse mask\", inner.margins = 0)\n\ntmap_arrange(pz1, pz2, pz3, pz4, ncol = 4, asp = NA)\n\n\n\n\n\n\n\n\n\n특정 위치에 있는 대상 래스터와 관련된 값을 식별하여 반환\n\n\ndata(\"zion_points\", package = \"spDataLarge\")\nelevation &lt;-terra::extract(srtm, vect(zion_points))\nzion_points &lt;- cbind(zion_points, elevation)\nplot(srtm)\nplot(vect(zion),add=T)\nplot(zion_points,col=\"black\", pch = 19, cex = 0.5, add=T)\n#&gt; Warning in plot.sf(zion_points, col = \"black\", pch = 19, cex = 0.5, add = T):\n#&gt; ignoring all but the first attribute\n\n\n\n\n\nst_segmentize() : 제공된 density로 line을 따라 point를 추가\n\ndfMaxLength : 최대 점의 개수\n\nst_cast() : 추가된 point를 “POINT” 형식으로 변환\n\n\nzion_transect &lt;- cbind(c(-113.2, -112.9), c(37.45, 37.2)) %&gt;%\n  st_linestring() %&gt;%\n  st_sfc(crs = crs(srtm)) %&gt;%\n  st_sf()\nzion_transect$id &lt;- 1:nrow(zion_transect)\nzion_transect &lt;- st_segmentize(zion_transect, dfMaxLength = 250)\nzion_transect &lt;- st_cast(zion_transect, \"POINT\")\n#&gt; Warning in st_cast.sf(zion_transect, \"POINT\"): repeating attributes for all\n#&gt; sub-geometries for which they may not be constant\n\n\nzion_transect &lt;- zion_transect %&gt;%\n  group_by(id) %&gt;%\n  mutate(dist = st_distance(geometry)[, 1])\n\nzion_elev &lt;- terra::extract(srtm, vect(zion_transect))\nzion_transect &lt;- cbind(zion_transect, zion_elev)\n\n\n많은 Point들 간의 거리를 산출 : 첫번째 점들과 이후의 각각의 점들 사이의 거리 계산하기\n횡단면의 각 점에 대한 고도값을 추출하고 이 정보를 주요 객체와 결합\n\n\n\n\nlibrary(tmap)\nlibrary(grid)\nlibrary(ggplot2)\nzion_transect_line &lt;- cbind(c(-113.2, -112.9), c(37.45, 37.2)) %&gt;%\n  st_linestring() %&gt;%\n  st_sfc(crs = crs(srtm)) %&gt;%\n  st_sf()\nzion_transect_points &lt;- st_cast(zion_transect, \"POINT\")[c(1, nrow(zion_transect)), ]\nzion_transect_points$name &lt;- c(\"start\", \"end\")\nrast_poly_line &lt;- tm_shape(srtm) +\n  tm_raster(palette = terrain_colors, title = \"Elevation (m)\",\n            legend.show = TRUE, style = \"cont\") +\n  tm_shape(zion) +\n  tm_borders(lwd = 2) +\n  tm_shape(zion_transect_line) +\n  tm_lines(col = \"black\", lwd = 4) +\n  tm_shape(zion_transect_points) +\n  tm_text(\"name\", bg.color = \"white\", bg.alpha = 0.75, auto.placement = TRUE) +\n  tm_layout(legend.frame = TRUE, legend.position = c(\"right\", \"top\"))\nrast_poly_line\n\n\n\nplot_transect &lt;- ggplot(zion_transect, aes(as.numeric(dist), srtm)) +\n  geom_line() +\n  labs(x = \"Distance (m)\", y = \"Elevation (m a.s.l.)\") +\n  theme_bw() +\n  # facet_wrap(~id) +\n  theme(plot.margin = unit(c(5.5, 15.5, 5.5, 5.5), \"pt\"))\nplot_transect\n\n\n\n\n## grid 그리기\ngrid.newpage() #This function erases the current device or moves to a new page.\npushViewport(viewport(layout = grid.layout(2, 2, heights = unit(c(0.25, 5), \"null\"))))\ngrid.text(\"A. Line extraction\", vp = viewport(layout.pos.row = 1, layout.pos.col = 1))\ngrid.text(\"B. Elevation along the line\", vp = viewport(layout.pos.row = 1, layout.pos.col = 2))\nprint(rast_poly_line, vp = viewport(layout.pos.row = 2, layout.pos.col = 1))\nprint(plot_transect, vp = viewport(layout.pos.row = 2, layout.pos.col = 2))\n\n\n\n\n\nzion_srtm_values &lt;- terra::extract(x = srtm, y = vect(zion))\ngroup_by(zion_srtm_values, ID) %&gt;%\n  summarize(across(srtm, list(min = min, mean = mean, max = max)))\n#&gt; # A tibble: 1 × 4\n#&gt;      ID srtm_min srtm_mean srtm_max\n#&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n#&gt; 1     1     1122     1818.     2661\n\n\n단일 영역을 특성화하거나 여러 영역을 비교하기 위해 폴리곤 당 래스터 값에 대한 요약 통계 생성\n\n\n\n\n\n\n벡터 객체를 래스터 객체의 표현으로 변환\n\n\ncycle_hire_osm &lt;- spData::cycle_hire_osm\ncycle_hire_osm_projected &lt;- st_transform(cycle_hire_osm, \"EPSG:27700\")\nraster_template &lt;- rast(ext(cycle_hire_osm_projected), resolution = 1000,\n                        crs = st_crs(cycle_hire_osm_projected)$wkt) # ext : 경계값\nch_raster1 &lt;- rasterize(vect(cycle_hire_osm_projected), raster_template,\n                        field = 1)\nch_raster2 &lt;- rasterize(vect(cycle_hire_osm_projected), raster_template,\n                        fun = \"length\")\nch_raster3 &lt;- rasterize(vect(cycle_hire_osm_projected), raster_template,\n                        field = \"capacity\", fun = sum)\n\n\n\n\n\n\n\n폴리곤 객체를 여러 줄 문자열로 casting한 후 0.5도의 해상도로 탬플릿 래스터 생성\n\ntouches = TRUE : 경계에 해당되는 래스터만 색칠(FALSE이면 경계 내부까지)\n\n\n\ncalifornia &lt;- dplyr::filter(us_states, NAME == \"California\")\ncalifornia_borders &lt;- st_cast(california, \"MULTILINESTRING\")\nraster_template2 &lt;- rast(ext(california),\n                         resolution = 0.5,\n                         crs = st_crs(california)$wkt)\ncalifornia_raster1 &lt;-\n  rasterize(vect(california_borders), raster_template2,\n            touches = TRUE) # touches = TRUE : 경계값만\ncalifornia_raster2 &lt;-\n  rasterize(vect(california), raster_template2)\n# with `touches = FALSE` by default, which selects only cell\n\n\n\n\n\n\n\n\n\n\n공간적으로 연속적인 래스터 데이터를 점, 선 또는 다각형과 같은 공간적으로 분리된 벡터 데이터로 변환\n벡터화의 가장 간단한 형태는 래스터 셀의 중심부를 점으로 변환하는 것\nas.points() : 모든 raster grid 셀에 대해 중심점으로 반환\n\n\nelev &lt;- rast(system.file(\"raster/elev.tif\", package = \"spData\"))\nelev_point &lt;- as.points(elev) %&gt;%\n  st_as_sf()\nplot(elev)\n\n\n\nplot(elev_point)\n\n\n\n\n\ncontour() : 선에 해당하는 수치 표현\n등고선의 생성 : 공간 벡터화의 또 다른 일반적인 유형은 연속적인 높이 또는 온도(등온선)의 선을 나타내는 등고선 생성\n\n\ndem = rast(system.file(\"raster/dem.tif\", package = \"spDataLarge\"))\ncl = as.contour(dem)\nplot(dem, axes = FALSE)\nplot(cl, add = TRUE)\n\n\n\nplot(dem, axes = FALSE)\ncontour(dem, add = T) # 수치까지 표현\n\n\n\n\n\nas.polygons() : 래스터를 다각형으로 변환하는 것\n\n\ngrain &lt;- rast(system.file(\"raster/grain.tif\", package = \"spData\"))\ngrain_poly &lt;- as.polygons(grain) %&gt;%\n  st_as_sf()\nplot(grain)\n\n\n\nplot(grain_poly)"
  },
  {
    "objectID": "Spatial_Information_Analysis.html#chapter-7-reprojecting-geographic-data",
    "href": "Spatial_Information_Analysis.html#chapter-7-reprojecting-geographic-data",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "CRS를 설명할 수 있는 여러가지 방법\n\n단순하지만 “lon/lat 좌표”와 같이 모호할 수 있는 문장\n\n\n공식화되었지만 지금은 구식인 proj4 strings\n\n\nproj=lonlat +ellps=WGS84 +datum=WGS84 +no_defs\n\n\nEPSG:4326과 같이 식별되는 authority:code 텍스트 문자열\n\n-&gt; 3번째 방법이 가장 정확(짧고 기억하기 쉬우며 온라인에서 찾기 쉬움)\n\n\nst_crs(\"EPSG:4326\")\n\n\n\n\n\n벡터 지리 데이터 객체에서 CRS를 가져오고 설정\n\n\nvector_filepath &lt;- system.file(\"shapes/world.gpkg\", package = \"spData\")\nnew_vector &lt;- read_sf(vector_filepath)\n\nst_crs(new_vector)\n#&gt; Coordinate Reference System:\n#&gt;   User input: WGS 84 \n#&gt;   wkt:\n#&gt; GEOGCRS[\"WGS 84\",\n#&gt;     ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n#&gt;         MEMBER[\"World Geodetic System 1984 (Transit)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G730)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G873)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G1150)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G1674)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G1762)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G2139)\"],\n#&gt;         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n#&gt;             LENGTHUNIT[\"metre\",1]],\n#&gt;         ENSEMBLEACCURACY[2.0]],\n#&gt;     PRIMEM[\"Greenwich\",0,\n#&gt;         ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;     CS[ellipsoidal,2],\n#&gt;         AXIS[\"geodetic latitude (Lat)\",north,\n#&gt;             ORDER[1],\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;         AXIS[\"geodetic longitude (Lon)\",east,\n#&gt;             ORDER[2],\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;     USAGE[\n#&gt;         SCOPE[\"Horizontal component of 3D system.\"],\n#&gt;         AREA[\"World.\"],\n#&gt;         BBOX[-90,-180,90,180]],\n#&gt;     ID[\"EPSG\",4326]]\n\n\nUser input : CRS식별자 (WGS 84, 입력 파일에서 가져온 EPSG:4326의 동의어)\nwkt : CRS에 대한 모든 관련 정보와 함께 전체 WKT 문자열을 포함\ninput 요소는 유연함(AUTHORITY:CODE (ex. EPSG:4326), CRS 이름(ex. WGS84), proj4string 정의)\nwkt 요소는 객체를 파일에 저장하거나 좌표 연산을 수행할 때 사용되는 WKT 표현을 저장\nnew_vector 객체가 WGS84 타원체를 가지며, 그리니치 프라임 자오선을 사용하고, 위도와 경도의 축 순서를 사용하는 것을 볼 수 있음\n이 경우 이 CRS 사용에 적합한 영역을 설명하는 USAGE와 CRS 식별자 EPSG:4326을 가리키는 ID와 같은 추가 요소도 있음\n\n\nst_crs(new_vector)$IsGeographic\n#&gt; [1] TRUE\nst_crs(new_vector)$units_gdal\n#&gt; [1] \"degree\"\nst_crs(new_vector)$srid\n#&gt; [1] \"EPSG:4326\"\nst_crs(new_vector)$proj4string\n#&gt; [1] \"+proj=longlat +datum=WGS84 +no_defs\"\n\n\nst_crs 함수에는 유용한 기능이 하나 있는데, 사용된 CRS에 대한 추가 정보를 검색할 수 있음.\n\nst_crs(new_vector)$IsGeographic : CRS가 지리적 상태인지 확인\nst_crs(new_vector)$units_gdal : CRS 단위\nst_crs(new_vector)$srid : 해당 ‘SRID’ 식별자를 추출(사용 가능한 경우)\nst_crs(new_vector)$proj4string : proj4string 표현을 추출\n\nst_set_crs() : CRS가 없거나 잘못 설정되어 있는 경우 CRS 설정\n\n\nnew_vector &lt;- st_set_crs(new_vector, \"EPSG:4326\") # set CRS\n\n\nterra::crs() : 래스터 객체에 대한 CRS를 설정\n하지만, crs() 함수를 사용하면 좌표계는 바뀌지만 값이 바뀌지는 않음.\n\n\nraster_filepath &lt;- system.file(\"raster/srtm.tif\", package = \"spDataLarge\")\nmy_rast &lt;- rast(raster_filepath)\ncrs(my_rast)\n#&gt; [1] \"GEOGCRS[\\\"WGS 84\\\",\\n    ENSEMBLE[\\\"World Geodetic System 1984 ensemble\\\",\\n        MEMBER[\\\"World Geodetic System 1984 (Transit)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G730)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G873)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1150)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1674)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1762)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G2139)\\\"],\\n        ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        ENSEMBLEACCURACY[2.0]],\\n    PRIMEM[\\\"Greenwich\\\",0,\\n        ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    CS[ellipsoidal,2],\\n        AXIS[\\\"geodetic latitude (Lat)\\\",north,\\n            ORDER[1],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        AXIS[\\\"geodetic longitude (Lon)\\\",east,\\n            ORDER[2],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    USAGE[\\n        SCOPE[\\\"Horizontal component of 3D system.\\\"],\\n        AREA[\\\"World.\\\"],\\n        BBOX[-90,-180,90,180]],\\n    ID[\\\"EPSG\\\",4326]]\"\ncat(crs(my_rast)) # get CRS\n#&gt; GEOGCRS[\"WGS 84\",\n#&gt;     ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n#&gt;         MEMBER[\"World Geodetic System 1984 (Transit)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G730)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G873)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G1150)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G1674)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G1762)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G2139)\"],\n#&gt;         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n#&gt;             LENGTHUNIT[\"metre\",1]],\n#&gt;         ENSEMBLEACCURACY[2.0]],\n#&gt;     PRIMEM[\"Greenwich\",0,\n#&gt;         ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;     CS[ellipsoidal,2],\n#&gt;         AXIS[\"geodetic latitude (Lat)\",north,\n#&gt;             ORDER[1],\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;         AXIS[\"geodetic longitude (Lon)\",east,\n#&gt;             ORDER[2],\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;     USAGE[\n#&gt;         SCOPE[\"Horizontal component of 3D system.\"],\n#&gt;         AREA[\"World.\"],\n#&gt;         BBOX[-90,-180,90,180]],\n#&gt;     ID[\"EPSG\",4326]]\ncrs(my_rast) &lt;- \"EPSG:26912\" # set CRS\n\nlondon &lt;- data.frame(lon = -0.1, lat = 51.5) %&gt;%\n  st_as_sf(coords = c(\"lon\", \"lat\"))\nst_is_longlat(london)\n#&gt; [1] NA\n\nlondon_geo &lt;- st_set_crs(london, \"EPSG:4326\")\nst_is_longlat(london_geo)\n#&gt; [1] TRUE"
  },
  {
    "objectID": "Spatial_Information_Analysis.html#geometry-operations-on-projected-and-unprojected-data",
    "href": "Spatial_Information_Analysis.html#geometry-operations-on-projected-and-unprojected-data",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "sf는 지리 벡터 데이터에 대한 클래스와 지리 계산을 위한 중요한 하위 수준 라이브러리에 대한 일관된 명령줄 인터페이스 제공\n\n구면 geometry 연산을 sf:sf_use_sf(FALSE) 명령으로 끄면 버퍼는 미터와 같은 적절한 거리 단위를 대체하지 못하는 위도와 경도의 단위를 사용하기 때문에 쓸모없는 출력이 됨.\n공간 및 기하학적 연산을 수행하는 것은 경우에 따라 거의 또는 전혀 차이가 없음. (ex: 공간 부분 집합) 그러나 버퍼링과 같은 거리가 포함된 연산의 경우 (구면 지오메트리 엔진을 사용하지 않고) 좋은 결과를 보장하는 유일한 방법은 데이터의 투영된 복사본을 만들고 그에 대한 연산을 실행하는 것임.\n그 결과 런던과 동일하지만 미터 단위의 EPSG 코드를 가진 적절한 CRS(영국 국가 그리드)에 재투사된 새로운 물체가 되었음.\nCRS의 단위가 (도가 아닌) 미터라는 사실은 이것이 투영된 CRS임을 알려줌\n\n\n\nlondon_buff_no_crs &lt;-\n  st_buffer(london, dist = 1) # incorrect: no CRS\nlondon_buff_no_crs\n#&gt; Simple feature collection with 1 feature and 0 fields\n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -1.1 ymin: 50.5 xmax: 0.9 ymax: 52.5\n#&gt; CRS:           NA\n#&gt;                         geometry\n#&gt; 1 POLYGON ((0.9 51.5, 0.89862...\nlondon_buff_s2 &lt;-\n  st_buffer(london_geo, dist = 1e5) # silent use of s2 (1e5 : 10^5m = 100,000m)\nlondon_buff_s2\n#&gt; Simple feature collection with 1 feature and 0 fields\n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -1.552818 ymin: 50.59609 xmax: 1.356603 ymax: 52.40393\n#&gt; Geodetic CRS:  WGS 84\n#&gt;                         geometry\n#&gt; 1 POLYGON ((-0.3523255 52.392...\nlondon_buff_s2_100_cells &lt;-\n  st_buffer(london_geo, dist = 1e5, max_cells = 100)\nlondon_buff_s2_100_cells\n#&gt; Simple feature collection with 1 feature and 0 fields\n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -1.718303 ymin: 50.51128 xmax: 1.524546 ymax: 52.53186\n#&gt; Geodetic CRS:  WGS 84\n#&gt;                         geometry\n#&gt; 1 POLYGON ((-0.3908656 52.531...\n\nsf::sf_use_s2(FALSE)\n#&gt; Spherical geometry (s2) switched off\n\nlondon_buff_lonlat &lt;-\n  st_buffer(london_geo, dist = 1) # incorrect result\n#&gt; Warning in st_buffer.sfc(st_geometry(x), dist, nQuadSegs, endCapStyle =\n#&gt; endCapStyle, : st_buffer does not correctly buffer longitude/latitude data\n#&gt; dist is assumed to be in decimal degrees (arc_degrees).\n\nsf::sf_use_s2(TRUE)\n#&gt; Spherical geometry (s2) switched on\n\nlondon_proj &lt;- data.frame(x = 530000, y = 180000) %&gt;%\n  st_as_sf(coords = 1:2, crs = \"EPSG:27700\")\n\nst_crs(london_proj)\n#&gt; Coordinate Reference System:\n#&gt;   User input: EPSG:27700 \n#&gt;   wkt:\n#&gt; PROJCRS[\"OSGB36 / British National Grid\",\n#&gt;     BASEGEOGCRS[\"OSGB36\",\n#&gt;         DATUM[\"Ordnance Survey of Great Britain 1936\",\n#&gt;             ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,\n#&gt;                 LENGTHUNIT[\"metre\",1]]],\n#&gt;         PRIMEM[\"Greenwich\",0,\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;         ID[\"EPSG\",4277]],\n#&gt;     CONVERSION[\"British National Grid\",\n#&gt;         METHOD[\"Transverse Mercator\",\n#&gt;             ID[\"EPSG\",9807]],\n#&gt;         PARAMETER[\"Latitude of natural origin\",49,\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433],\n#&gt;             ID[\"EPSG\",8801]],\n#&gt;         PARAMETER[\"Longitude of natural origin\",-2,\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433],\n#&gt;             ID[\"EPSG\",8802]],\n#&gt;         PARAMETER[\"Scale factor at natural origin\",0.9996012717,\n#&gt;             SCALEUNIT[\"unity\",1],\n#&gt;             ID[\"EPSG\",8805]],\n#&gt;         PARAMETER[\"False easting\",400000,\n#&gt;             LENGTHUNIT[\"metre\",1],\n#&gt;             ID[\"EPSG\",8806]],\n#&gt;         PARAMETER[\"False northing\",-100000,\n#&gt;             LENGTHUNIT[\"metre\",1],\n#&gt;             ID[\"EPSG\",8807]]],\n#&gt;     CS[Cartesian,2],\n#&gt;         AXIS[\"(E)\",east,\n#&gt;             ORDER[1],\n#&gt;             LENGTHUNIT[\"metre\",1]],\n#&gt;         AXIS[\"(N)\",north,\n#&gt;             ORDER[2],\n#&gt;             LENGTHUNIT[\"metre\",1]],\n#&gt;     USAGE[\n#&gt;         SCOPE[\"Engineering survey, topographic mapping.\"],\n#&gt;         AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],\n#&gt;         BBOX[49.75,-9.01,61.01,2.01]],\n#&gt;     ID[\"EPSG\",27700]]"
  },
  {
    "objectID": "Spatial_Information_Analysis.html#chapter-8-geographic-data-i-and-oinput-and-output",
    "href": "Spatial_Information_Analysis.html#chapter-8-geographic-data-i-and-oinput-and-output",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "download.file(url = \"https://irma.nps.gov/DataStore/DownloadFile/666527\",\n              destfile = \"nps_boundary.zip\")\nunzip(zipfile = \"nps_boundary.zip\")\nusa_parks = read_sf(dsn = \"nps_boundary.shp\")\n\n\n해외여서 접속이 막혀있음\n공공데이터포털에서 shape 파일 다운받아 불러오기\n\n공공데이터포털에서 데이터를 작업 공간에 다운 받기\n\n\n# unzip(zipfile=\"C:/202201/GIS/data/부산광역시_교통정보서비스센터 보유 ITS CCTV 현황(SHP)_20210601.zip\")\n#busan &lt;- read_sf(dsn = \"./Spatial_Information_Analysis/tl_tracffic_cctv_info.shp\", options = \"ENCODING:CP949\")\n#busan\n#plot(busan)\n\n# unzip(zipfile = \"C:/202201/GIS/data/CTPRVN_20220324.zip\")\n#sido &lt;- read_sf(dsn = \"./Spatial_Information_Analysis/ctp_rvn.shp\", options = \"ENCODING:CP949\")\n#sido\n#plot(sido)\n\n\n\n\n\n\nrnaturalearth 패키지의 ne_countries() 기능을 사용하면 국가 경계 기능을 사용할 수 있음\nosmdata 패키지는 속도가 제한되어 있다는 단점이 있음\n\n이러한 한계를 극복하기 위해 osmextract 패키지가 개발\n\n\nlibrary(rnaturalearth)\n#&gt; Support for Spatial objects (`sp`) will be deprecated in {rnaturalearth} and will be removed in a future release of the package. Please use `sf` objects with {rnaturalearth}. For example: `ne_download(returnclass = 'sf')`\nusa &lt;- ne_countries(country = \"United States of America\") # United States borders\n#&gt; Warning: The `returnclass` argument of `ne_download()` sp as of rnaturalearth 1.0.0.\n#&gt; ℹ Please use `sf` objects with {rnaturalearth}, support for Spatial objects\n#&gt;   (sp) will be removed in a future release of the package.\nclass(usa)\n#&gt; [1] \"SpatialPolygonsDataFrame\"\n#&gt; attr(,\"package\")\n#&gt; [1] \"sp\"\n\nusa_sf &lt;- st_as_sf(usa)\nplot(usa_sf[1])\n\n\n\nkorea &lt;- ne_countries(country = \"South Korea\") # United States borders\nclass(korea)\n#&gt; [1] \"SpatialPolygonsDataFrame\"\n#&gt; attr(,\"package\")\n#&gt; [1] \"sp\"\nkorea_sf &lt;- st_as_sf(korea)\nplot(korea_sf[1])\n\n\n\n\n\n\n\n\n\nhttps://r.geocompx.org/read-write.html#file-formats\n\n\n\n\n\ngpkg 형식 불러오기\n\n\nf &lt;- system.file(\"shapes/world.gpkg\", package = \"spData\")\nworld = read_sf(f, quiet = TRUE)\ntanzania = read_sf(f, query = 'SELECT * FROM world WHERE name_long = \"Tanzania\"')\ntanzania_buf = st_buffer(tanzania, 50000)\ntanzania_buf_geom = st_geometry(tanzania_buf)\ntanzania_buf_wkt = st_as_text(tanzania_buf_geom)\ntanzania_neigh = read_sf(f, wkt_filter = tanzania_buf_wkt)\n\n\ncsv 형식 불러오기\n\n\ncycle_hire_txt = system.file(\"misc/cycle_hire_xy.csv\", package = \"spData\")\ncycle_hire_xy = read_sf(cycle_hire_txt,\n                        options = c(\"X_POSSIBLE_NAMES=X\", \"Y_POSSIBLE_NAMES=Y\"))\n\n\nWell-known text(WKT), Well-known binary(WKB), and the GeoJSON formats\n\n\nworld_txt = system.file(\"misc/world_wkt.csv\", package = \"spData\")\nworld_wkt = read_sf(world_txt, options = \"GEOM_POSSIBLE_NAMES=WKT\")\n# the same as\nworld_wkt2 = st_read(world_txt, options = \"GEOM_POSSIBLE_NAMES=WKT\",\n                     quiet = TRUE, stringsAsFactors = FALSE, as_tibble = TRUE)\n\n\nKML file stores geographic information in XML format\n\n\nu = \"https://developers.google.com/kml/documentation/KML_Samples.kml\"\ndownload.file(u, \"./Spatial_Information_Analysis/KML_Samples.kml\")\nst_layers(\"./Spatial_Information_Analysis/KML_Samples.kml\")\n#&gt; Driver: KML \n#&gt; Available layers:\n#&gt;              layer_name  geometry_type features fields crs_name\n#&gt; 1            Placemarks       3D Point        3      2   WGS 84\n#&gt; 2      Highlighted Icon       3D Point        1      2   WGS 84\n#&gt; 3                 Paths 3D Line String        6      2   WGS 84\n#&gt; 4         Google Campus     3D Polygon        4      2   WGS 84\n#&gt; 5      Extruded Polygon     3D Polygon        1      2   WGS 84\n#&gt; 6 Absolute and Relative     3D Polygon        4      2   WGS 84\nkml = read_sf(\"./Spatial_Information_Analysis/KML_Samples.kml\", layer = \"Placemarks\")"
  },
  {
    "objectID": "Spatial_Information_Analysis.html#chapter-9-making-maps-with-r",
    "href": "Spatial_Information_Analysis.html#chapter-9-making-maps-with-r",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "정적인 지도는 지리 계산의 가장 일반적인 시각적 출력 유형\nplot() 또는 tmap_mode(plot)\n\n\n\n\n# Add fill layer to nz shape\ntm_shape(nz) +\n  tm_fill()\n# Add border layer to nz shape\ntm_shape(nz) +\n  tm_borders()\n# Add fill and border layers to nz shape\ntm_shape(nz) +\n  tm_fill() +\n  tm_borders()\n\n\n\n\n\n\n\n\n\n\nmap_nz &lt;- tm_shape(nz) + tm_polygons()\nclass(map_nz)\n#&gt; [1] \"tmap\"\nmap_nz\n\n\n\n\nnz_elev = rast(system.file(\"raster/nz_elev.tif\", package = \"spDataLarge\"))\n\nmap_nz1 &lt;- map_nz + tm_shape(nz_elev) + tm_raster(alpha = 0.7)\n\nnz_water &lt;- st_union(nz) %&gt;% st_buffer(22200) %&gt;%\n  st_cast(to = \"LINESTRING\")\n\nmap_nz2 &lt;- map_nz1 +\n  tm_shape(nz_water) + tm_lines()\n\nmap_nz3 &lt;- map_nz2 +\n  tm_shape(nz_height) + tm_dots()\n\ntmap_arrange(map_nz1, map_nz2, map_nz3)\n\n\n\n\n\nalpha : 레이어를 반투명하게 만들기 위해 설정\n\n\n\n\n\nma1 &lt;- tm_shape(nz) + tm_fill(col = \"red\")\nma2 &lt;- tm_shape(nz) + tm_fill(col = \"red\", alpha = 0.3)\nma3 &lt;- tm_shape(nz) + tm_borders(col = \"blue\")\nma4 &lt;- tm_shape(nz) + tm_borders(lwd = 3)\nma5 &lt;- tm_shape(nz) + tm_borders(lty = 2)\nma6 &lt;- tm_shape(nz) + tm_fill(col = \"red\", alpha = 0.3) +\n  tm_borders(col = \"blue\", lwd = 3, lty = 2)\n\ntmap_arrange(ma1, ma2, ma3, ma4, ma5, ma6)\n\n\n\n\n\ntm_fill()과 tm_bubbles()에서 레이어는 기본적으로 회색으로 채워지고 tm_lines()은 검은선으로 그려짐\ntmap의 인수는 숫자 벡터를 허용하지 않음\n\n\nplot(st_geometry(nz), col = nz$Land_area) # works\ntm_shape(nz) + tm_fill(col = nz$Land_area) # fails\n&gt; Error: Fill argument neither colors nor valid variable name(s)\ntm_shape(nz) + tm_fill(col = \"Land_area\")\n\n\n\n\n\n\n\n범례의 제목 설정\n\n\nlegend_title &lt;- expression(\"Area (km\"^2*\")\")\nmap_nza &lt;- tm_shape(nz) +\n  tm_fill(col = \"Land_area\", title = legend_title) + tm_borders()\nmap_nza\n\n\n\n\n\n\n\n\nbreaks : 색상의 표현 값 범위를 수동으로 설정\nn : 숫자 변수가 범주화되는 Bin의 수 설정\npalette : 색 구성표를 정의 (ex. BuGn)\n\n\ntm1 &lt;- tm_shape(nz) + tm_polygons(col = \"Median_income\")\nbreaks = c(0, 3, 4, 5) * 10000\ntm2 &lt;- tm_shape(nz) + tm_polygons(col = \"Median_income\", breaks = breaks)\ntm3 &lt;- tm_shape(nz) + tm_polygons(col = \"Median_income\", n = 10)\ntm4 &lt;- tm_shape(nz) + tm_polygons(col = \"Median_income\", palette = \"BuGn\")\n\ntmap_arrange(tm1, tm2, tm3, tm4)\n\n\n\n\n\ntm_shape(nz) + tm_polygons(col = \"Median_income\", style = \"pretty\")\ntm_shape(nz) + tm_polygons(col = \"Median_income\", style = \"equal\")\ntm_shape(nz) + tm_polygons(col = \"Median_income\", style = \"quantile\")\ntm_shape(nz) + tm_polygons(col = \"Median_income\", style = \"jenks\")\ntm_shape(nz) + tm_polygons(col = \"Median_income\", style = \"cont\")\ntm_shape(nz) + tm_polygons(col = \"Median_income\", style = \"cat\")\n\n\n\n\n\n\n\nstyle = \"pretty\" : 기본 설정은 가능한 경우 정수로 반올림하고 간격을 균등하게 유지\nstyle = \"equal\" : 입력 값을 동일한 범위의 빈으로 나누고 균일한 분포의 변수에 적합(결과 맵이 색상 다양성이 거의 없을 수 있으므로 분포가 치우친 변수에는 권장하지 않음)\nstyle = \"quantile\" : 동일한 수의 관찰이 각 범주에 포함되도록 함(빈 범위가 크게 다를 수 있다는 잠재적인 단점이 있음).\nstyle = \"jenks\" : 데이터에서 유사한 값의 그룹을 식별하고 범주 간의 차이를 최대화\nstyle = \"cont\" : 연속 색상 필드에 많은 색상을 표시하고 연속 래스터에 특히 적합\nstyle = \"cat\" : 범주 값을 나타내도록 설계되었으며 각 범주가 고유한 색상을 받도록 함\n\n\ntm_p1 &lt;- tm_shape(nz) + tm_polygons(\"Population\", palette = \"Blues\")\ntm_p2 &lt;- tm_shape(nz) + tm_polygons(\"Population\", palette = \"YlOrBr\")\n\ntmap_arrange(tm_p1, tm_p2)\n\n\n\n\n\n순차 팔레트는 단일(ex. Blues : 밝은 파란색에서 진한 파란색으로 이동) 또는 다중 색상/색조(ex. YlOrBr : 주황색을 통해 밝은 노란색에서 갈색으로 그라데이션)\n\n\n\n\n\nmap_nz +\n  tm_compass(type = \"8star\", position = c(\"left\", \"top\")) +\n  tm_scale_bar(breaks = c(0, 100, 200), text.size = 1)\n\n\n\n\ntm_l1 &lt;- map_nz + tm_layout(title = \"New Zealand\")\ntm_l2 &lt;- map_nz + tm_layout(scale = 5)\ntm_l3 &lt;- map_nz + tm_layout(bg.color = \"lightblue\")\ntm_l4 &lt;- map_nz + tm_layout(frame = FALSE)\n\ntmap_arrange(tm_l1, tm_l2, tm_l3, tm_l4)\n\n\n\n\n\ntm_layout()의 다양한 옵션\n\nframe.lwd : 프레임 너비\nframe.double.line : 이중선 허용 옵션\nouter.margin, inner.margin : 여백 설정\nfontface, fontfamily : 글꼴 설정\nlegend.show : 범례 표시 여부\nlegend.position : 범례 위치 변경\n\n\n\n\n\n\n\n\ntm_s1 &lt;- map_nza + tm_style(\"bw\")\ntm_s2 &lt;- map_nza + tm_style(\"classic\")\ntm_s3 &lt;- map_nza + tm_style(\"cobalt\")\ntm_s4 &lt;- map_nza + tm_style(\"col_blind\")\n\ntmap_arrange(tm_s1, tm_s2, tm_s3, tm_s4)\n\n\n\n\n\n\n\n\nurb_1970_2030 &lt;- urban_agglomerations %&gt;%\n  filter(year %in% c(1970, 1990, 2010, 2030))\ntm_shape(world) +\n  tm_polygons() +\n  tm_shape(urb_1970_2030) +\n  tm_symbols(col = \"black\", border.col = \"white\", size = \"population_millions\") +\n  tm_facets(by = \"year\", nrow = 2, free.coords = TRUE)\n\n\n\n#free.coords : 지도에 자체 경계 상자가 있는지 여부를 지정\n\n\n\n\n\nnz_region &lt;- st_bbox(c(xmin = 1340000, xmax = 1450000,\n                       ymin = 5130000, ymax = 5210000),\n                     crs = st_crs(nz_height)) %&gt;% st_as_sfc()\n\nnz_height_map &lt;- tm_shape(nz_elev, bbox = nz_region) +\n  tm_raster(style = \"cont\", palette = \"YlGn\", legend.show = TRUE) +\n  tm_shape(nz_height) + tm_symbols(shape = 2, col = \"red\", size = 1) +\n  tm_scale_bar(position = c(\"left\", \"bottom\"))\n\nnz_map &lt;- tm_shape(nz) + tm_polygons() +\n  tm_shape(nz_height) + tm_symbols(shape = 2, col = \"red\", size = 0.1) +\n  tm_shape(nz_region) + tm_borders(lwd = 3)\n\nlibrary(grid)\nnz_height_map\nprint(nz_map, vp = viewport(0.8, 0.27, width = 0.5, height = 0.5))\n\n\n\n\n\nviewport() : 두개의 맵을 결합\n\n\n\n\n\n\nurb_anim &lt;- tm_shape(world) + tm_polygons() +\n  tm_shape(urban_agglomerations) + tm_dots(size = \"population_millions\") +\n  tm_facets(along = \"year\", free.coords = FALSE)\n\ntmap_animation(urb_anim, filename = \"./Spatial_Information_Analysis/urb_anim.gif\", delay = 25)\n#&gt; Creating frames\n#&gt; =========\n#&gt; ====\n#&gt; =====\n#&gt; ====\n#&gt; =====\n#&gt; ====\n#&gt; =====\n#&gt; ====\n#&gt; ====\n#&gt; =====\n#&gt; ====\n#&gt; =====\n#&gt; ====\n#&gt; =====\n#&gt; ====\n#&gt; =====\n#&gt; ====\n#&gt; \n#&gt; Creating animation\n#&gt; Animation saved to C:\\Users\\Hyunsoo Kim\\Desktop\\senior_grade\\blog\\my-quarto-website\\Spatial_Information_Analysis\\urb_anim.gif\n\n\nby = year대신 along = year을 사용\nfree.coords = FALSE : 각 맵 반복에 대한 맵 범위 유지\ntmap_animation()을 사용하여 .gif로 저장\n\n\n\n\n\n대화형 지도는 데이터 세트를 새로운 차원으로 끌어올릴 수 있음\n지도를 기울이고 회전하는 기능과 사용자가 이동 및 확대/축소 할 때 자동으로 업데이트\ntmap, mapview, mapdeck, leaflet으로 표현 가능\n\n\n\n\ntmap_mode(\"view\") #interactive mode\n#&gt; tmap mode set to interactive viewing\nmap_nz\n\n\n\n\n\n\nmap_nz + tm_basemap(server = \"OpenTopoMap\")\n\n\n\n\n\n\nworld_coffee = left_join(world, coffee_data, by = \"name_long\")\nfacets = c(\"coffee_production_2016\", \"coffee_production_2017\")\ntm_shape(world_coffee) + tm_polygons(facets) +\n  tm_facets(nrow = 1, sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntm_basemap() 또는 tm_options()로 basemap 지정 가능\ntm_facets()에서 sync옵션을 TRUE로 선택하면 여러개의 맵을 동시에 확대/축소할 수 있음\n\n\n\n\n\nmapview::mapview(nz)\n\ntrails %&gt;%\n  st_transform(st_crs(franconia)) %&gt;%\n  st_intersection(franconia[franconia$district == \"Oberfranken\", ][1]) %&gt;%\n  st_collection_extract(\"LINE\") %&gt;%\n  mapview(color = \"red\", lwd = 3, layer.name = \"trails\") +\n  mapview(franconiWa, zcol = \"district\", burst = TRUE) +\n  breweries\n\n\n\n\n\nset_token(Sys.getenv(\"pk.eyJ1IjoiancwMTEyIiwiYSI6ImNsM2ppbzYzNzBrbjQzZHBjMmlocnY2dDUifQ.58-gXpPtvcCGmMt2xEW-ig\"))\ncrash_data = read.csv(\"https://git.io/geocompr-mapdeck\")\ncrash_data = na.omit(crash_data)\nms = mapdeck_style(\"dark\")\nmapdeck(style = ms, pitch = 45, location = c(0, 52), zoom = 4) %&gt;%\n  add_grid(data = crash_data, lat = \"lat\", lon = \"lng\", cell_size = 1000,\n           elevation_scale = 50, layer_id = \"grid_layer\",\n           colour_range = viridisLite::plasma(6))\n#&gt; Registered S3 method overwritten by 'jsonify':\n#&gt;   method     from    \n#&gt;   print.json jsonlite\n\n\n\n\n\n\n\n\n\nadd_arc() 함수\n\n\nurl &lt;- 'https://raw.githubusercontent.com/plotly/datasets/master/2011_february_aa_flight_paths.csv'\nflights &lt;- read.csv(url)\nflights$id &lt;- seq_len(nrow(flights))\nflights$stroke &lt;- sample(1:3, size = nrow(flights), replace = T)\nkey = \"pk.eyJ1IjoiancwMTEyIiwiYSI6ImNsM2ppbzYzNzBrbjQzZHBjMmlocnY2dDUifQ.58-gXpPtvcCGmMt2xEW-ig\"\n\nmapdeck(token = key, style = mapdeck_style(\"dark\"), pitch = 45 ) %&gt;%\n  add_arc(\n    data = flights\n    , layer_id = \"arc_layer\"\n    , origin = c(\"start_lon\", \"start_lat\")\n    , destination = c(\"end_lon\", \"end_lat\")\n    , stroke_from = \"airport1\"\n    , stroke_to = \"airport2\"\n    , stroke_width = \"stroke\"\n  )\n\n\nadd_animated_arc() 함수\n\n\nmapdeck(token = key, style = 'mapbox://styles/mapbox/dark-v9', pitch = 45 ) %&gt;%\n  add_animated_arc(\n    data = flights\n    , layer_id = \"arc_layer\"\n    , origin = c(\"start_lon\", \"start_lat\")\n    , destination = c(\"end_lon\", \"end_lat\")\n    , stroke_from = \"airport1\"\n    , stroke_to = \"airport2\"\n    , stroke_width = \"stroke\"\n  )\n\n\nadd_heatmap() 함수\n\n\nmapdeck(token = key, style = mapdeck_style('dark'), pitch = 45 ) %&gt;%\n  add_heatmap(\n    data = df[1:30000, ]\n    , lat = \"lat\"\n    , lon = \"lng\"\n    , weight = \"weight\"\n    , colour_range = colourvalues::colour_values(1:6, palette = \"inferno\")\n  )\n\n\nadd_path() 함수\n\n\nmapdeck(\n  token = key\n  , style = mapdeck_style(\"dark\")\n  , zoom = 10) %&gt;%\n  add_path(\n    data = roads\n    , stroke_colour = \"RIGHT_LOC\"\n    , layer_id = \"path_layer\"\n  )\n\n\nadd_geojson(), add_scatterplot(), add_text() 등이 있음\n\n\n\n\n\npal = colorNumeric(\"RdYlBu\", domain = cycle_hire$nbikes)\nleaflet(data = cycle_hire) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%       # Background Map\n  addCircles(col = ~pal(nbikes), opacity = 0.9) %&gt;%      # nbikes의 값으로 색이 다르게 circle 생성\n  addPolygons(data = lnd, fill = FALSE) %&gt;%              # land에 따라 Polygon 생성\n  addLegend(pal = pal, values = ~nbikes) %&gt;%             # 범례 생성\n  setView(lng = -0.1, 51.5, zoom = 12) %&gt;%               # zoom\n  addMiniMap()                                           # minimap 생성\n\n\n\n\n\n\n# create a basic map\n\nleaflet() %&gt;%\n  addTiles() %&gt;% # add default OpenStreetMap map tiles\n  setView(lng=127.063, lat=37.513, zoom = 6) # korea, zoom 6\n\n\n\n\n\n# map style: NASA\n\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng=127.063, lat=37.513, zoom = 6) %&gt;%\n  addProviderTiles(\"NASAGIBS.ViirsEarthAtNight2012\")\n\n\n\n\n\n# map style: Esri.WorldImagery\n\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng=127.063, lat=37.513, zoom = 16) %&gt;%\n  addProviderTiles(\"Esri.WorldImagery\")\n\n\n\n\n\n# adding Popup\n\npopup = c(\"한남대학교 빅데이터응용학과\")\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  addMarkers(lng = c(127.4219), # longitude\n             lat = c(36.3548), # latitude\n             popup = popup)\n\n\n\n\n\n\nzoom : 확대/축소 비율 설정\naddProviderTiles() : 외부 지도 타일 추가\naddMarkers() : 커서를 클릭했을 때 팝업으로 나타나는 설명을 추가\n\n\n\n\n\n\n\n\nR을 사용하여 한걸음 더 나아가 웹 어플리케이션을 제작할 수 있게 해주는 패키지\nui 라고 말하는 화면은 실제로 사용자가 보는 화면\nshiny에서는 크게 titlePanel과 sidebarPanel, mainPanal의 세 가지로 구성\n\n\nui = fluidPage(\n  sliderInput(inputId = \"life\", \"Life expectancy\", 49, 84, value = 80),\n  leafletOutput(outputId = \"map\")\n)\nserver = function(input, output) {\n  output$map = renderLeaflet({\n    leaflet() %&gt;%\n      # addProviderTiles(\"OpenStreetMap.BlackAndWhite\") %&gt;%\n      addPolygons(data = world[world$lifeExp &lt; input$life,])\n  })\n}\nshinyApp(ui, server)\n#&gt; PhantomJS not found. You can install it with webshot::install_phantomjs(). If it is installed, please make sure the phantomjs executable can be found via the PATH variable.\n\nShiny applications not supported in static R Markdown documents\n\n\n\nui &lt;- fluidPage(#Application title\n  titlePanel(\"Hello Shiny!\"),\n  #Sidebar with a slider input for the number of bins\n  sidebarLayout(sidebarPanel(\n    sliderInput(\n      \"bins\",\n      \"Number of bins:\",\n      min = 1,\n      max = 50,\n      value = 30\n    )\n  ),\n  #Show a plot of the generated distribution\n  mainPanel(plotOutput(\"distPlot\"))))\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    x &lt;- faithful[, 2]\n    bins &lt;- seq(min(x), max(x), length.out = input$bins + 1)\n    hist(x,\n         breaks = bins,\n         col = 'darkgray',\n         border = 'white')\n  })\n}\nshinyApp(ui, server)\n\nShiny applications not supported in static R Markdown documents"
  },
  {
    "objectID": "Spatial_Information_Analysis.html#ggmap",
    "href": "Spatial_Information_Analysis.html#ggmap",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "지도 공간 기법으로 시각화하는 ggmap 패키지는 Google Maps, Stamen Maps, 네이버 맵, 등의 다양한 온라인 소스로부터 가져온 정적인 지도 위에 특별한 데이터나 모형을 시각화하는 함수들을 제공함\nggmap()의 주요 함수\n\ngeocode() : 거리주소 또는 장소 이름을 이용하여 이용 지도 정보(위도, 경도) 획득\nget_googlemap() : 구글 지도 서비스 API에 접근하여 정적 지도 다운로드 지원과 지도에 marker 등을 삽입하고 자신이 원하는 줌 레벨과 center를 지정하여 지도 정보 생성\nget_map() : 지도 서비스 관련 서버에 관련 질의어를 지능형으로 인식하여 지도 정보 생성\nget_navermap() : 네이버 지도 서비스 API에 접근하여 정적 지도 다운로드 지원\nggimage() : ggplot2 패키지의 이미지와 동등한 수준으로 지도 이미지 생성\nggmap(), ggmapplot() : get_map() 함수에 의해서 생성된 픽셀 객체를 지도 이미지로 시각화\nqmap() : ggmap()함수와 get_map() 함수의 통합기능\nqmplot() : ggplot2 패키지의 qplot()와 동등한 수준으로 빠르게 지도 이미지 시각화\n\n\n\n\n\nget_googlemap() 함수를 통해 불러오고 싶은 곳의 장소를 문자열 값으로 첫 번째 인자에 넣어 실행해 이를 객체화 함\nggmap() 함수 안에 방금 만든 객체를 입력시킨 후 실행하면 원하는 장소를 중심으로 구글 지도가 plotting 됨\n\n\n# install.packages(\"ggmap\")\nlibrary(ggmap)\nregister_google(key = 'AIzaSyB4jjrVVAzb9fl8FQrQqUONAsaRBppWuSA')\n\n# 우리나라 지도 호출\ngetmap &lt;- get_googlemap(\"seoul\")\nggmap(getmap)\n\n\n\n\n\nggmap() 으로 반환되는 결과물은 ggplot2 패키지의 함수와 조합해 지도 위에 새로운 정보들을 추가할 수 있음\n\n\n\n\ndaejeon_map &lt;- get_googlemap(\"daejeon\") %&gt;% ggmap\nlocation &lt;- data.frame(\n  Name = c(\"한남대학교\", \"대전신세계\"),\n  lon = c(127.4219, 127.3821), #경도\n  lat = c(36.3548, 36.3752)    #위도\n)\n\ndaejeon_map + geom_point(data = location, aes(x = lon, y = lat))\n\ndaejeon_map &lt;- get_googlemap(\"daejeon\", zoom = 13) %&gt;% ggmap\nlocation &lt;- data.frame(\n  Name = c(\"한남대학교\", \"대전신세계\"),\n  lon = c(127.4219, 127.3821),\n  lat = c(36.3548, 36.3752)\n)\n\ndaejeon_map + geom_point(data = location, aes(x = lon, y = lat)) +\n  geom_text(data = location,\n            aes(label = Name),\n            size = 5,   # text 크기\n            vjust = -1) # text 위치\n\n\ngeom_point() 내의 옵션을 선택하여 점의 크기, 색깔, 모양 등 변경 가능\n\n\ndaejeon_map + geom_point(data = location, aes(x = lon, y = lat),\n                         size = 5, color = \"red\", alpha = 0.4) +\ngeom_text(data = location, aes(label = Name), size = 5, vjust = -1)\n\n\n한남대학교를 중심으로 그리기(center)\n\nenc2utf8 : UTF-8로 인코딩\nmaptype : “terrain”, “satellite”, “roadmap”, “hybrid”\ncenter : 맵의 중심\n\n\n# 한남대학교를 중심으로 그리기\ngc &lt;- geocode(enc2utf8(\"한남대학교\"))\n\nmap &lt;- get_googlemap(\n  center = as.numeric(gc),\n  maptype = \"roadmap\",\n  zoom = 13,\n  size = c(640, 640),\n  markers = gc) %&gt;% ggmap\n\nmap + geom_point(data = location, aes(x = lon, y = lat)) +\ngeom_text(data = location, aes(label = Name), size = 5, vjust = -1)\n\nPath(경로)\n\n\nmap + geom_path(data = location, aes(x = lon, y = lat), color = \"blue\", alpha = .5, lwd = 1)\n\n\n두 지역 사이의 경로 좌표 추출\n\nggmap::route : find a route from Google using different possible modes (\"driving\", \"walking\", \"bicycling\", \"transit\")\n\n\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(tmap)\nlibrary(stplanr)\n\ngc_st &lt;- geocode(enc2utf8(\"한남대학교\"))\ngc_ed &lt;- geocode(enc2utf8(\"신세계백화점 대전신세계아트앤사이언스\"))\ngc_od &lt;- st_linestring(rbind(as.numeric(gc_st), as.numeric(gc_ed)))\n\nst_sfc(gc_od) # Linestring, CRS 없음\nst_crs(gc_od)\ngc_od &lt;- st_sfc(gc_od, crs = 4326)\n# st_sfc() : 좌표계가 비어있는 경우에 좌표계 지정\nst_crs(gc_od)\n\nqtm(gc_od)\ngc_od &lt;- st_sf(gc_od)\n# st_sf() : sfc와 sf class의 객체들을 하나로 통합\ngc_od$distance &lt;- as.numeric(st_length(gc_od))\n\nroute_od = route(l = gc_od,             # l : linestring\n                 route_fun = route_osrm,\n                 osrm.profile = \"car\")  # foot, bike, car\nqtm(route_od)\n\nmap &lt;- get_googlemap(\n  center = c(127.41, 36.37),\n  maptype = \"roadmap\",\n  zoom = 14,\n  size = c(640, 640),\n  markers = gc\n) %&gt;% ggmap(extent = \"device\")\n\nmap\n\nmap + geom_sf(data = route_od, inherit.aes = F)\n# inherit.aes = F : sf형식의 데이터를 그릴 때 필수 옵션\n\n지도를 꽉 채워서 출력(x, y축 삭제하고 그림만 출력)\n\nextent = \"device\"\n+ theme_void()\n\n\nmap &lt;- get_googlemap(\n  center = as.numeric(gc),\n  maptype = \"roadmap\",\n  zoom = 13,\n  size = c(640, 640),\n  markers = gc\n) %&gt;% ggmap(extent = \"device\")\nmap\n\nmap &lt;- get_googlemap(\n  center = as.numeric(gc),\n  maptype = \"roadmap\",\n  zoom = 13,\n  size = c(640, 640),\n  markers = gc\n) %&gt;% ggmap() + theme_void()\nmap\n\n\n\n\n\n\n# Houston 범죄 데이터\nstr(crime)\nHoustonmap &lt;- get_map(\"Houston\")\nggmap(Houstonmap)\nggmap(Houstonmap) + geom_point(data = crime, aes(x = lon, y = lat))\nggmap(Houstonmap) + geom_point(data = crime, aes(x = lon, y = lat), size = 0.1, alpha = 0.1) # 점의 크기, 점의 투명도 조절\n\n#지도 확대 & 특정 지역 데이터만 추출하기\nHoustonmap &lt;- get_map(\"Houston\", zoom = 14)\ncrime1 &lt;- crime[(crime$lon &lt; -95.344 & crime$lon &gt; -95.395) & (crime$lat &lt; 29.783 & crime$lat &gt; 29.738), ]\ncrime11 &lt;- crime %&gt;% filter((lon &lt; -95.344 & lon &gt; -95.395) & (lat &lt; 29.783 & lat &gt; 29.738))\nnrow(crime1) ; nrow(crime11)\ncrime1 %&gt;% arrange(desc(lon)) %&gt;% nrow()\ncrime11 %&gt;% arrange(desc(lon)) %&gt;% nrow()\n\nggmap(Houstonmap) + geom_point(data = crime1, aes(x = lon, y = lat), alpha = 0.3)\nggmap(Houstonmap) + geom_point(data = crime1, aes(x = lon, y = lat, colour = offense))\n\ncrime2 &lt;- crime1[!duplicated(crime1[, c(\"lon\", \"lat\")]), ] # 위, 경도에 대해 중복되지 않게 하나의 관측치만 선택\n\ncrime2$offense &lt;- as.character(crime2$offense) # 범죄 종류 문자형으로 변경\n\ncrime2$offense[crime2$offense == \"murder\" | crime2$offense == \"rape\"] &lt;- \"4\"\ncrime2$offense[crime2$offense == \"robbery\" | crime2$offense == \"aggravated assault\"] &lt;- \"3\"\ncrime2$offense[crime2$offense == \"burglary\" | crime2$offense == \"auto theft\"] &lt;- \"2\"\ncrime2$offense[crime2$offense == \"theft\"] &lt;- \"1\"\n\ncrime2$offense &lt;- as.numeric(crime2$offense) # 범죄 종류 문자형을 숫자형으로 변경\n\nggmap(Houstonmap) + geom_point(data = crime2, aes(x = lon, y = lat, size = offense), alpha = 0.2)\n\n# 범죄 위험도에 따라 점의 크기 및 색깔로 구별\nggmap(Houstonmap) + geom_point(data = crime2, aes(x = lon, y = lat, size = offense, colour = offense), alpha = 0.5) +\n  scale_colour_gradient(low = \"white\", high = \"red\")\n\ncrime3 &lt;- crime2[crime2$date == \"1/1/2010\", ]\n\ncrime4 &lt;- crime3[!duplicated(crime3[, c(\"hour\")]), ]\n\nnrow(crime3) ; nrow(crime4)\n\nggmap(Houstonmap) + geom_point(data = crime3, aes(x = lon, y = lat)) +\n  geom_text(data = crime4, aes(label = street), vjust = 1.2) +\n  geom_path(data = crime4, aes(x = lon, y = lat), color = \"red\")"
  },
  {
    "objectID": "Spatial_Information_Analysis.html#chapter13-transportation",
    "href": "Spatial_Information_Analysis.html#chapter13-transportation",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "names(bristol_zones) ; names(bristol_od)\n#&gt; [1] \"geo_code\" \"name\"     \"geometry\"\n#&gt; [1] \"o\"          \"d\"          \"all\"        \"bicycle\"    \"foot\"      \n#&gt; [6] \"car_driver\" \"train\"\nnrow(bristol_zones)  ; nrow(bristol_od)\n#&gt; [1] 102\n#&gt; [1] 2910\n\n# O : Zone of the Origin / D : Zone of the Dest\n\nzones_attr = bristol_od %&gt;%\n  group_by(o) %&gt;%\n  summarize_if(is.numeric, sum) %&gt;%\n  dplyr::rename(geo_code = o)\n\nsummary(zones_attr$geo_code %in% bristol_zones$geo_code) # 일치하는지 확인\n#&gt;    Mode    TRUE \n#&gt; logical     102\n\n\nzones_joined = left_join(bristol_zones, zones_attr, by = \"geo_code\")\nnrow(zones_joined)\n#&gt; [1] 102\nsum(zones_joined$all)\n#&gt; [1] 238805\n\nnames(zones_joined)\n#&gt; [1] \"geo_code\"   \"name\"       \"all\"        \"bicycle\"    \"foot\"      \n#&gt; [6] \"car_driver\" \"train\"      \"geometry\"\n#&gt; [1] \"geo_code\" \"name\" \"all\" \"bicycle\" \"foot\" \"car_driver\" \"train\" \"geometry\"\nnames(zones_joined)[3] &lt;- c(\"all_orig\")\nnames(zones_joined)\n#&gt; [1] \"geo_code\"   \"name\"       \"all_orig\"   \"bicycle\"    \"foot\"      \n#&gt; [6] \"car_driver\" \"train\"      \"geometry\"\n\nzones_od = bristol_od %&gt;%\n  group_by(d) %&gt;%\n  summarize_if(is.numeric, sum) %&gt;%\n  dplyr::select(geo_code = d, all_dest = all) %&gt;%\n  inner_join(zones_joined, ., by = \"geo_code\")\n\nzones_od\n#&gt; Simple feature collection with 102 features and 8 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -2.845847 ymin: 51.28248 xmax: -2.252388 ymax: 51.73982\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;     geo_code                             name all_orig bicycle foot car_driver\n#&gt; 1  E02002985 Bath and North East Somerset 001      868      30  173        414\n#&gt; 2  E02002987 Bath and North East Somerset 003      898      34  117        523\n#&gt; 3  E02003005 Bath and North East Somerset 021      786      19   91        593\n#&gt; 4  E02003012                      Bristol 001     3312     161  330       2058\n#&gt; 5  E02003013                      Bristol 002     3715     188  615       2021\n#&gt; 6  E02003014                      Bristol 003     2220     126  270       1239\n#&gt; 7  E02003015                      Bristol 004     1633     166  307        786\n#&gt; 8  E02003016                      Bristol 005     2411     218  440       1105\n#&gt; 9  E02003017                      Bristol 006     1590     187  208        898\n#&gt; 10 E02003018                      Bristol 007     1690      96  143       1048\n#&gt;    train all_dest                       geometry\n#&gt; 1     43      744 MULTIPOLYGON (((-2.510462 5...\n#&gt; 2     58      561 MULTIPOLYGON (((-2.476122 5...\n#&gt; 3      8      427 MULTIPOLYGON (((-2.55073 51...\n#&gt; 4     12      701 MULTIPOLYGON (((-2.595763 5...\n#&gt; 5      6      940 MULTIPOLYGON (((-2.593783 5...\n#&gt; 6      5     3469 MULTIPOLYGON (((-2.639581 5...\n#&gt; 7      7     4980 MULTIPOLYGON (((-2.584973 5...\n#&gt; 8     23      297 MULTIPOLYGON (((-2.565948 5...\n#&gt; 9      9     1459 MULTIPOLYGON (((-2.616485 5...\n#&gt; 10    20      128 MULTIPOLYGON (((-2.637681 5...\n\nqtm(zones_od, c(\"all_orig\", \"all_dest\")) +\ntm_layout(panel.labels = c(\"Origin\", \"Destination\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nod_top5 = bristol_od %&gt;%\n  arrange(desc(all)) %&gt;%\n  top_n(5, wt = all)\n\nbristol_od$Active = (bristol_od$bicycle + bristol_od$foot) / bristol_od$all * 100\n\nod_intra = filter(bristol_od, o == d) # 지역 내 이동\nod_inter = filter(bristol_od, o != d) # 지역 외 이동\nod_intra ; od_inter # 102행 / 2808행\n#&gt; # A tibble: 102 × 8\n#&gt;    o         d           all bicycle  foot car_driver train Active\n#&gt;    &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1 E02002985 E02002985   209       5   127         59     0   63.2\n#&gt;  2 E02002987 E02002987   166       8    61         89     2   41.6\n#&gt;  3 E02003005 E02003005   383       8    87        256     1   24.8\n#&gt;  4 E02003012 E02003012   315       5   181        102     0   59.0\n#&gt;  5 E02003013 E02003013   318       7   165        112     0   54.1\n#&gt;  6 E02003014 E02003014   414      35   139        185     0   42.0\n#&gt;  7 E02003015 E02003015   240      18   142         61     0   66.7\n#&gt;  8 E02003016 E02003016   119       7    65         30     2   60.5\n#&gt;  9 E02003017 E02003017   147       8    70         60     1   53.1\n#&gt; 10 E02003018 E02003018    67       0    39         24     1   58.2\n#&gt; # ℹ 92 more rows\n#&gt; # A tibble: 2,808 × 8\n#&gt;    o         d           all bicycle  foot car_driver train Active\n#&gt;    &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1 E02002985 E02002987   121       7    35         62     0  34.7 \n#&gt;  2 E02002985 E02003036    32       2     1         10     1   9.38\n#&gt;  3 E02002985 E02003043   141       1     2         56    17   2.13\n#&gt;  4 E02002985 E02003049    56       2     4         36     0  10.7 \n#&gt;  5 E02002985 E02003054    42       4     0         21     0   9.52\n#&gt;  6 E02002985 E02003100    22       0     0         19     3   0   \n#&gt;  7 E02002985 E02003106    48       3     1         33     8   8.33\n#&gt;  8 E02002985 E02003108    31       0     0         29     1   0   \n#&gt;  9 E02002985 E02003121    42       1     2         34     0   7.14\n#&gt; 10 E02002985 E02006887   103       5     1         36    13   5.83\n#&gt; # ℹ 2,798 more rows\n\ndesire_lines = od2line(od_inter, zones_od)\n#&gt; Creating centroids representing desire line start and end points.\n# od2line : polygon으로 되어있는 두 지역의 중심점을 계산해서 linestring으로 변환\n#&gt; Creating centroids representing desire line start and end points.\nqtm(desire_lines, lines.lwd = \"all\")\n#&gt; Legend for line widths not available in view mode.\n\n\n\n\n\n\ndesire_lines\\(distance = as.numeric(st_length(desire_lines)) desire_carshort = dplyr::filter(desire_lines, car_driver &gt; 300 & distance &lt; 5000) route_carshort = route(l = desire_carshort, route_fun = route_osrm, osrm.profile = \"car\") # foot, bike, car desire_carshort\\)geom_car = st_geometry(route_carshort)\nplot(st_geometry(desire_carshort)) plot(desire_carshort$geom_car, col = “red”, add = TRUE) plot(st_geometry(st_centroid(zones_od)), add = TRUE)\n\ngetmap &lt;- get_googlemap(\"bristol\", zoom = 11)\nbristol_map &lt;- ggmap(getmap)\n\n# 센터 조정\ngetmap &lt;- get_googlemap(center = c(-2.56, 51.53), zoom = 12)\nbristol_map &lt;- ggmap(getmap)\nbristol_map + geom_sf(data = desire_carshort, inherit.aes = F) +\n  geom_sf(data = desire_carshort$geom_car,\n          inherit.aes = F,\n          col = \"red\") +\n  geom_sf(data = st_geometry(st_centroid(zones_od)), inherit.aes = F)"
  },
  {
    "objectID": "Spatial_Information_Analysis.html#사망교통사고-정보-분석",
    "href": "Spatial_Information_Analysis.html#사망교통사고-정보-분석",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "도로교통공단 TAAS에서는 사망교통사고 정보를 공개하고 있음\n\n교통사고 일시 부터 30일이내 사망한 경우를 사망교통사고라 정의하고 사고정보를 선택한 조건에 따라 json/xml형식으로 제공\n사망 교통 사고 정보\n\n사망사고 년, 월, 일, 시, 주야\n사망사고 건수\n사망사고 사망자수, 부상자수, 중상자수, 경상자수, 부상신고자수\n사망사고 위치 좌표 및 지역명\n사망사고 유형, 위반사항, 차량 종류, 도로 형태\n\n\n데이터 불러오기(https://taas.koroad.or.kr/api/selectDeathDataSet.do)\n\n다운받은 데이터를 R로 불러온 뒤 데이터 속성 확인하세요. 어떤 정보가 있는지, 활용할 위치 정보가 있는지 확인하세요\n\n\nSys.setlocale(\"LC_ALL\",\"Korean\")\n#&gt; Warning in Sys.setlocale(\"LC_ALL\", \"Korean\"): using locale code page other than\n#&gt; 65001 (\"UTF-8\") may cause problems\n#&gt; [1] \"LC_COLLATE=Korean_Korea.949;LC_CTYPE=Korean_Korea.949;LC_MONETARY=Korean_Korea.949;LC_NUMERIC=C;LC_TIME=Korean_Korea.949\"\ngetwd()\n#&gt; [1] \"C:/Users/Hyunsoo Kim/Desktop/senior_grade/blog/my-quarto-website\"\nraw.data &lt;- read.csv(\"./Spatial_Information_Analysis/12_20_death.csv\", header = TRUE, fileEncoding = \"EUC-KR\")\n## 구조 확인\nstr(raw.data)\n#&gt; 'data.frame':    37128 obs. of  23 variables:\n#&gt;  $ 발생년               : int  2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 ...\n#&gt;  $ 발생년월일시         : int  2012010101 2012010101 2012010108 2012010110 2012010103 2012010116 2012010210 2012010104 2012010104 2012010102 ...\n#&gt;  $ 주야                 : chr  \"야간\" \"야간\" \"주간\" \"주간\" ...\n#&gt;  $ 요일                 : chr  \"일\" \"일\" \"일\" \"일\" ...\n#&gt;  $ 사망자수             : int  1 1 1 2 1 1 2 1 1 1 ...\n#&gt;  $ 사상자수             : int  1 6 1 2 1 1 2 1 2 4 ...\n#&gt;  $ 중상자수             : int  0 5 0 0 0 0 0 0 1 0 ...\n#&gt;  $ 경상자수             : int  0 0 0 0 0 0 0 0 0 3 ...\n#&gt;  $ 부상신고자수         : int  0 0 0 0 0 0 0 0 0 0 ...\n#&gt;  $ 발생지시도           : chr  \"서울\" \"전북\" \"충남\" \"경남\" ...\n#&gt;  $ 발생지시군구         : chr  \"은평구\" \"정읍시\" \"청양군\" \"합천군\" ...\n#&gt;  $ 사고유형_대분류      : chr  \"차대사람\" \"차대차\" \"차량단독\" \"차대차\" ...\n#&gt;  $ 사고유형_중분류      : chr  \"차도통행중\" \"정면충돌\" \"공작물충돌\" \"측면충돌\" ...\n#&gt;  $ 사고유형             : chr  \"차도통행중\" \"정면충돌\" \"공작물충돌\" \"측면충돌\" ...\n#&gt;  $ 법규위반             : chr  \"안전운전 의무 불이행\" \"중앙선 침범\" \"안전운전 의무 불이행\" \"과속\" ...\n#&gt;  $ 도로형태_대분류      : chr  \"단일로\" \"단일로\" \"단일로\" \"교차로\" ...\n#&gt;  $ 도로형태             : chr  \"기타단일로\" \"기타단일로\" \"기타단일로\" \"교차로내\" ...\n#&gt;  $ 당사자종별_1당_대분류: chr  \"승용차\" \"승용차\" \"승용차\" \"승합차\" ...\n#&gt;  $ 당사자종별_2당_대분류: chr  \"보행자\" \"승용차\" \"없음\" \"승용차\" ...\n#&gt;  $ 발생위치X_UTMK       : int  949860 946537 940016 1059321 1070222 1036880 1079124 1114053 911131 955269 ...\n#&gt;  $ 발생위치Y_UTMK       : int  1957179 1737695 1832833 1748774 1834630 1827821 1708218 1761943 1861851 1952221 ...\n#&gt;  $ 경도                 : num  127 127 127 128 128 ...\n#&gt;  $ 위도                 : num  37.6 35.6 36.5 35.7 36.5 ...\n## 테이블 확인\nView(raw.data)\n\n데이터 추출하기\n\n다운받은 데이터는 전국에 대한 사망교통사고 정보이다. 대전지역에 2016년부터 2020년까지의 정보만을 추출하세요.\n\n추출한 데이터의 경도, 위도에 결측값 및 0인 데이터가 있는지 확인하세요.\n\n\n## 1. 대전 지역 2016 ~ 2020년 데이터 추출\ndaejeon &lt;- filter(raw.data,  발생지시도 == \"대전\" &  발생년 &gt; 2015)\n\n## 2. 사고 발생 시작점 경도/위도 데이터의 범위 살펴보기\nrange(daejeon$경도) ; range(daejeon$위도)\n#&gt; [1] 127.2653 127.5278\n#&gt; [1] 36.22335 36.45634\n\n## 3. 경도/위도 데이터가 NA인 데이터 확인하기\nsum(is.na(daejeon$경도)) ; sum(is.na(daejeon$위도))\n#&gt; [1] 0\n#&gt; [1] 0\n\n## 4. 경도/위도 데이터가 0인 데이터 확인하기\ndaejeon[daejeon$경도 == 0,]\n#&gt;  [1] 발생년                발생년월일시          주야                 \n#&gt;  [4] 요일                  사망자수              사상자수             \n#&gt;  [7] 중상자수              경상자수              부상신고자수         \n#&gt; [10] 발생지시도            발생지시군구          사고유형_대분류      \n#&gt; [13] 사고유형_중분류       사고유형              법규위반             \n#&gt; [16] 도로형태_대분류       도로형태              당사자종별_1당_대분류\n#&gt; [19] 당사자종별_2당_대분류 발생위치X_UTMK        발생위치Y_UTMK       \n#&gt; [22] 경도                  위도                 \n#&gt; &lt;0 rows&gt; (or 0-length row.names)\ndaejeon[daejeon$위도 == 0,]\n#&gt;  [1] 발생년                발생년월일시          주야                 \n#&gt;  [4] 요일                  사망자수              사상자수             \n#&gt;  [7] 중상자수              경상자수              부상신고자수         \n#&gt; [10] 발생지시도            발생지시군구          사고유형_대분류      \n#&gt; [13] 사고유형_중분류       사고유형              법규위반             \n#&gt; [16] 도로형태_대분류       도로형태              당사자종별_1당_대분류\n#&gt; [19] 당사자종별_2당_대분류 발생위치X_UTMK        발생위치Y_UTMK       \n#&gt; [22] 경도                  위도                 \n#&gt; &lt;0 rows&gt; (or 0-length row.names)"
  },
  {
    "objectID": "Spatial_Information_Analysis.html#spspatial-objects-객체클래스로-문제-풀기",
    "href": "Spatial_Information_Analysis.html#spspatial-objects-객체클래스로-문제-풀기",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "## 5. 년도별 사고 위치 정보 지도 상에 표출하기\nlibrary(ggmap)\nregister_google(key = 'AIzaSyB4jjrVVAzb9fl8FQrQqUONAsaRBppWuSA')\nmap &lt;- qmap(location = enc2utf8(\"대전\"),\n            zoom = 12,\n            maptype = \"roadmap\")\np &lt;-\n  map + geom_point(\n    data = daejeon,\n    aes(x = 경도, y = 위도, colour = factor(발생년)),\n    size = 2,\n    alpha = 0.7\n  )\np + ggtitle(\"대전시 사망사고 위치(2016-2020)\")\n\n\nstat_bin2d() 함수 활용하여 Grid 내 사고횟수 출력\n\n\n## stat_bin2d() 함수 활용하여 특정 영역 내 사고 횟수 출력\np &lt;- map + stat_bin2d(data = daejeon,\n                      aes(x = 경도, y = 위도),\n                      bins = 30,   # bins : grid의 개수\n                      alpha = 0.5) # binwidth 로도 가능\np\n## stat_bin2d() 함수 활용하여 특정 영역 내 사고 횟수 출력/위성지도/컬러 변경\nmap &lt;- qmap(location = \"Daejeon\",\n            zoom = 12,\n            maptype = \"satellite\")\np &lt;- map + stat_bin2d(data = daejeon,\n                      aes(x = 경도, y = 위도),\n                      bins = 30,\n                      alpha = 0.5) # binwidth 로도 가능\np + scale_fill_gradient(low = \"yellow\", high = \"red\")\n\n\n\n\nGrid 내에 Count된 값 및 위치 확인하기\n\n\np_count &lt;- ggplot_build(p)$data[[4]] # cell의 값 출력\np_count &lt;- arrange(p_count, desc(value))\nhead(p_count)\n\n\nGrid 내(중심)에 Count값 표출\n\n\np + scale_fill_gradient(low = \"yellow\", high = \"red\") +\n  geom_text(data = p_count, aes((xmin + xmax) / 2, (ymin + ymax) / 2,\n                                label = count), col = \"white\")\n\n\n사고 유형 별로 표시하기\n\n\np &lt;-\n  map + stat_bin2d(\n    data = daejeon,\n    aes(\n      x = 경도,\n      y = 위도,\n      colour = factor(사고유형),\n      fill = factor(사고유형)\n    ),\n    bins = 30,\n    alpha = 0.5\n  )\np\n\n\nstat_density2d() 함수 활용하여 등고선으로 지도 위에 출력하기\n\n\nmap &lt;-\n  qmap(\n    location = \"Daejeon\",\n    zoom = 12,\n    maptype = 'roadmap',\n    color = 'bw'\n  )\np &lt;-\n  map + stat_density2d(\n    data = daejeon,\n    aes(x = 경도, y = 위도, fill = ..level..),\n    bins = 5,\n    alpha = 0.45,\n    size = 2,\n    geom = \"polygon\"\n  )\n# level : 레벨이 높을수록 더 진한색, size : 선 굵기, bins: 선 간격\np\n\n\ngeom_hex() 함수 활용하여 벌집 블롯으로 출력하기\n\n\n## 벌집 블롯으로 출력(geom_hex(), scale_fill_gradientn())\nlibrary(hexbin)\nmap &lt;- qmap(location = \"Daejeon\",\n            zoom = 11,\n            maptype = \"roadmap\")\np &lt;-\n  map + coord_cartesian() + # coord_cartesian() : 데카르트 좌표계\n  geom_hex(\n    data = daejeon,\n    aes(x = 경도, y = 위도),\n    bins = 12,\n    alpha = 0.6,\n    color = \"white\"\n  ) # geom_hex() only works with Cartesian coordinates\np\np &lt;- p + scale_fill_gradientn(colours = terrain.colors(15))\np\n\n# binwidth로 출력\np &lt;-\n  map + coord_cartesian() + geom_hex(\n    data = daejeon,\n    binwidth = c(0.05, 0.05), # binwidth : bin의 크기 설정\n    aes(x = 경도, y = 위도),\n    alpha = 0.6,\n    color = \"white\"\n  ) # geom_hex() only works with Cartesian coordinates\np\np &lt;- p + scale_fill_gradientn(colours = terrain.colors(15))\np\n\np_count &lt;- ggplot_build(p)$data[[4]] # cell의 값 출력\np_count &lt;- arrange(p_count, desc(count))\nhead(p_count)\n\n\n\n\n\n\n행정구역시군구 경계를 얻기 위해 데이터로 대전시 구 경계 shape 파일 획득\n\n\nlibrary(raster)\nlibrary(rgdal)\nlibrary(sf)\n\n## 동별 사망사고 추출하기\nmap &lt;-\n  qmap(\n    location = \"Daejeon\",\n    zoom = 11,\n    maptype = 'roadmap',\n    color = 'bw'\n  )\n\ndaejeon_area &lt;- shapefile('./Spatial_Information_Analysis/LARD_ADM_SECT_SGG_30/LARD_ADM_SECT_SGG_30.shp')\ndaejeon_area # 좌표체계 확인\n# str(daejeon_area)\nplot(daejeon_area, axes = T)\n\n\n위 plot의 좌표단위를 보면 평면직각좌표계(Projected Coordinate)를 기준으로 측정할 때 나올 수 있는 단위\n앞에서 사고 데이터의 좌표는 위경도 좌표이므로, 두 자료의 위치 좌표체계를 통일 시켜줄 필요가 있음\nspTransform() 를 통해 좌표변형 가능\n\nto_crs = CRS(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\")\n\n\nto_crs = CRS(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\")\ndaejeon_area2 &lt;- spTransform(daejeon_area, to_crs)\ndaejeon_area2\ndaejeon_area2@data # SP 데이터 내에서 출력을 하려면 @로 호출해야함\n\nplot(daejeon_area2, axes = T)\nmap + geom_polygon(\n  data = daejeon_area2,\n  aes(x = long, y = lat, group = group),\n  fill = 'white',\n  color = 'black'\n)\n\n구를 기준으로 사고 발생 횟수 계산\n\n\ngu_accident &lt;- daejeon %&gt;% group_by(발생지시군구) %&gt;% summarise(n = n())\ngu_accident\n#&gt; # A tibble: 5 x 2\n#&gt;   발생지시군구     n\n#&gt;   &lt;chr&gt;        &lt;int&gt;\n#&gt; 1 대덕구          81\n#&gt; 2 동구            98\n#&gt; 3 서구           100\n#&gt; 4 유성구          73\n#&gt; 5 중구            58\n\n\ndaejeon_area2 객체의 클래스는 SpatitalPloygonsDataFrame임\n이것을 데이터 프레임 형태로 변환해줄 때 사용하는 함수로는 ggplot2 패키지의 fortify() 함수가 있음\n구를 나타내는 SGG_NM 열로 기준\n\n\nclass(daejeon_area2)\ndaejeon_area2 &lt;- fortify(daejeon_area2, region = 'SGG_NM')\nclass(daejeon_area2)\nhead(daejeon_area2)\n\n\ndaejeon_area2의 “id”열과 gu_accident의 “발생지시군구”열을 기준으로 합치기 위해서 열Name을 “id”로 통일\nid열을 기준으로 두 데이터셋을 합쳐줌\n\n\nnames(gu_accident)[1] &lt;- \"id\"\ndaejeon_area3 &lt;- merge(daejeon_area2, gu_accident, by = 'id')\nhead(daejeon_area3)\n\ndaejeon_area3 %&gt;% group_by(id) %&gt;% summarise(n = mean(n))\n\n\ngeom_polygon()을 이용한 시각화\n\n\np &lt;-\n  map + geom_polygon(data = daejeon_area3,\n                     aes(\n                       x = long,\n                       y = lat,\n                       group = group,\n                       fill = n\n                     ),\n                     alpha = .5)\np\np + scale_fill_gradient(low = 'yellow', high = 'red')\nlibrary(viridis)\np + scale_fill_viridis()"
  },
  {
    "objectID": "Spatial_Information_Analysis.html#sfsimple-features-객체클래스로-문제-풀어보기",
    "href": "Spatial_Information_Analysis.html#sfsimple-features-객체클래스로-문제-풀어보기",
    "title": "Spatial Information Analysis",
    "section": "",
    "text": "구경계 데이터(daejeon_area2)를 sf클래스로 변환\nst_as_sf() : sp클래스를 sf클래스로 변환\n\n\ndaejeon_area2 &lt;- spTransform(daejeon_area, to_crs)\ndaejeon_area2\ndaejeon_area_sf &lt;- st_as_sf(daejeon_area2) # sp 클래스를 sf 클래스로 전환하기\ndaejeon_area_sf\nplot(st_geometry(daejeon_area_sf))\n\n\nst_point_on_surface() : 각 구별 지도상 중심점 구한 뒤 지도상에 표출\n\n\n# 각 구별 Center\ndaejeon_area_center &lt;- st_point_on_surface(daejeon_area_sf)\nplot(st_geometry(daejeon_area_sf))\nplot(daejeon_area_center , add = T, col = \"black\")\n\n\n사망사고데이터(point)를 sf클래스로 변환\n\n\ndaejeon_acc_sf &lt;-\n  daejeon %&gt;% st_as_sf(coords = c(\"경도\", \"위도\"),\n                       crs = 4326,\n                       remove = FALSE)\ndaejeon_acc_sf ## CRS : # WGS84\n\n# daejeon_acc &lt;- daejeon %&gt;% st_as_sf(coords = c(\"발생위치X_UTMK\", \"발생위치Y_UTMK\"),\n#                                     crs = 4326,\n#                                     remove = FALSE)\n# daejeon_acc\n\n\nst_intersection을 통해서 폴리곤(구경계)와 포인트(사망사고지점)데이터 합치기\n\n\n## Intersection between polygon and points\nintersection &lt;- st_intersection(daejeon_area_sf, daejeon_acc_sf)\nhead(intersection)\n\n## Plot intersection\nplot(st_geometry(daejeon_area_sf))\nplot(intersection, add = T, pch = 1)\n\n\n구별 사망사고 건수 Count하기\n\n\n## View result\ntable(intersection$SGG_NM)\n\n## Using dplyr\nint_result &lt;- intersection %&gt;%\n  group_by(SGG_NM) %&gt;%\n  count()\nint_result\n\n\nst_join() : 경계 데이터(daejeon_area_sf)에 결과(int_result) 합치기\n\n\nint_result0 &lt;- st_join(daejeon_area_sf, int_result)\nint_result0\n\n\nmap 위에 시각화\n\n\nmap &lt;-\n  qmap(\n    location = \"Daejeon\",\n    zoom = 11,\n    maptype = 'roadmap',\n    color = 'bw'\n  )\np &lt;-\n  map + geom_sf(data = int_result0,\n                inherit.aes = F, # sf형태 data 그릴 때 반드시 필요\n                aes(fill = n),\n                alpha = .5)\np\np + scale_fill_gradient(low = 'yellow', high = 'red')"
  },
  {
    "objectID": "Regression_Analysis.html",
    "href": "Regression_Analysis.html",
    "title": "Regression Analysis",
    "section": "",
    "text": "예제를 통한 회귀분석\n\n\ngetwd() #\"C:/Users/Hyunsoo Kim/Documents/lecture/regression_analysis\"\n\n[1] \"C:/Users/Hyunsoo Kim/Desktop/senior_grade/blog/my-quarto-website\"\n\n\n\n\n\n\n\ndata_2.5&lt;-read.table(\"All_Data/p031.txt\",header=TRUE,sep=\"\\t\")\n\ndim(data_2.5) #14 2\n\n[1] 14  2\n\nhead(data_2.5)\n\n  Minutes Units\n1      23     1\n2      29     2\n3      49     3\n4      64     4\n5      74     4\n6      87     5\n\nX&lt;-data_2.5$Units\n\nY&lt;-data_2.5$Minutes\n\n\n\n\n\ndf&lt;-data.frame(\n\n  #1:length(X),\n\n  Y,\n\n  X,\n\n  Y-mean(Y),\n\n  X-mean(X),\n\n  (Y-mean(Y))^2,\n\n  (X-mean(X))^2,\n\n  (Y-mean(Y))*(X-mean(X))\n\n)\n\ndf\n\n     Y  X Y...mean.Y. X...mean.X. X.Y...mean.Y...2 X.X...mean.X...2\n1   23  1 -74.2142857          -5     5.507760e+03               25\n2   29  2 -68.2142857          -4     4.653189e+03               16\n3   49  3 -48.2142857          -3     2.324617e+03                9\n4   64  4 -33.2142857          -2     1.103189e+03                4\n5   74  4 -23.2142857          -2     5.389031e+02                4\n6   87  5 -10.2142857          -1     1.043316e+02                1\n7   96  6  -1.2142857           0     1.474490e+00                0\n8   97  6  -0.2142857           0     4.591837e-02                0\n9  109  7  11.7857143           1     1.389031e+02                1\n10 119  8  21.7857143           2     4.746173e+02                4\n11 149  9  51.7857143           3     2.681760e+03                9\n12 145  9  47.7857143           3     2.283474e+03                9\n13 154 10  56.7857143           4     3.224617e+03               16\n14 166 10  68.7857143           4     4.731474e+03               16\n   X.Y...mean.Y......X...mean.X..\n1                       371.07143\n2                       272.85714\n3                       144.64286\n4                        66.42857\n5                        46.42857\n6                        10.21429\n7                         0.00000\n8                         0.00000\n9                        11.78571\n10                       43.57143\n11                      155.35714\n12                      143.35714\n13                      227.14286\n14                      275.14286\n\n\n\n\n\n\nCOV_XY&lt;-sum((Y-mean(Y))*(X-mean(X))) / (length(X)-1) #136\n\n### cov() 함수\n\ncov(X,Y) #136\n\n[1] 136\n\n### 상관계수(correalationship)\n\n### cor() 함수\n\ncor(X,Y) #0.9936987 \n\n[1] 0.9936987\n\n\n\n\n\n\n\ndata_2.5&lt;-read.table(\"All_Data/p031.txt\",header=TRUE,sep=\"\\t\")\n\nx&lt;-data_2.5$Units\n\ny&lt;-data_2.5$Minutes\n\n\n\n\ncor_xy&lt;- COV_XY / (sd(x)*sd(y))\n\ncor_xy\n\n[1] 0.9936987\n\n### cor() 함수\n\ncor(x,y)\n\n[1] 0.9936987\n\ncor(y,x)\n\n[1] 0.9936987\n\ndata_2.5\n\n   Minutes Units\n1       23     1\n2       29     2\n3       49     3\n4       64     4\n5       74     4\n6       87     5\n7       96     6\n8       97     6\n9      109     7\n10     119     8\n11     149     9\n12     145     9\n13     154    10\n14     166    10\n\ncor(data_2.5)\n\n          Minutes     Units\nMinutes 1.0000000 0.9936987\nUnits   0.9936987 1.0000000\n\n\n\n\n\n\nclass(X)\n\n[1] \"numeric\"\n\nclass(Y) #both numeric\n\n[1] \"numeric\"\n\nplot(X,Y, pch=19,xlab=\"Units\",ylab=\"Minutes\") \n\n\n\n\n\n\n\n\ndata_2.3&lt;-read.table(\"All_Data/p029a.txt\",header=TRUE,sep=\"\\t\")\n\ndata_2.3\n\n    Y  X\n1   1 -7\n2  14 -6\n3  25 -5\n4  34 -4\n5  41 -3\n6  46 -2\n7  49 -1\n8  50  0\n9  49  1\n10 46  2\n11 41  3\n12 34  4\n13 25  5\n14 14  6\n15  1  7\n\nX&lt;-data_2.3$X\n\nY&lt;-data_2.3$Y\n\n\n\n\n\nplot(X,Y)\n\n\n\ncor(X,Y) # 0 완벽하게 2차함수의 형태도 0이 나옴(직선의 형태가 아닌것만)\n\n[1] 0\n\n\n\n\n\n\ndata_2.4&lt;-read.table(\"All_Data/p029b.txt\",header=TRUE,sep=\"\\t\")\n\n\n\n\n\nplot(data_2.4$X1,data_2.4$Y1, pch=19); abline(3,0.5) #기울기 3 절편0.5인 선을 추가해라 \n\n\n\nplot(data_2.4$X2,data_2.4$Y2, pch=19); abline(3,0.5)\n\n\n\nplot(data_2.4$X3,data_2.4$Y3, pch=19); abline(3,0.5)\n\n\n\nplot(data_2.4$X4,data_2.4$Y4, pch=19); abline(3,0.5)\n\n\n\nm&lt;-matrix(1:4,ncol=2,byrow=TRUE) #2행의 매트릭스 생성 \n\nm\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nlayout(m)\n\nplot(Y1~X1, data=data_2.4,pch=19); abline(3,0.5)\n\n#y~x  -&gt; y=ax+b 이러한 형태를 가지는 모형식이라는 의미 \n\nplot(Y2~X2, data=data_2.4,pch=19); abline(3,0.5)\n\nplot(Y3~X3, data=data_2.4,pch=19); abline(3,0.5)\n\nplot(Y4~X4, data=data_2.4,pch=19); abline(3,0.5)\n\n\n\nlayout(1) #변환을 다시 하지 않으면 설정한 매트릭스의 비율로 그래프가 그려짐 해제 필요 \n\n# cor()\n\ncor(data_2.4$X1,data_2.4$Y1) #0.8164205\n\n[1] 0.8164205\n\ncor(data_2.4$X2,data_2.4$Y2) #0.8162365\n\n[1] 0.8162365\n\ncor(data_2.4$X3,data_2.4$Y3) #0.8162867\n\n[1] 0.8162867\n\ncor(data_2.4$X4,data_2.4$Y4) #0.8165214\n\n[1] 0.8165214\n\ncor(data_2.4) #이렇게 한번에 할 수 있으나 가독성 떨어짐 \n\n           Y1         X1         Y2         X2         Y3         X3         Y4\nY1  1.0000000  0.8164205  0.7500054  0.8164205  0.4687167  0.8164205 -0.4891162\nX1  0.8164205  1.0000000  0.8162365  1.0000000  0.8162867  1.0000000 -0.3140467\nY2  0.7500054  0.8162365  1.0000000  0.8162365  0.5879193  0.8162365 -0.4780949\nX2  0.8164205  1.0000000  0.8162365  1.0000000  0.8162867  1.0000000 -0.3140467\nY3  0.4687167  0.8162867  0.5879193  0.8162867  1.0000000  0.8162867 -0.1554718\nX3  0.8164205  1.0000000  0.8162365  1.0000000  0.8162867  1.0000000 -0.3140467\nY4 -0.4891162 -0.3140467 -0.4780949 -0.3140467 -0.1554718 -0.3140467  1.0000000\nX4 -0.5290927 -0.5000000 -0.7184365 -0.5000000 -0.3446610 -0.5000000  0.8165214\n           X4\nY1 -0.5290927\nX1 -0.5000000\nY2 -0.7184365\nX2 -0.5000000\nY3 -0.3446610\nX3 -0.5000000\nY4  0.8165214\nX4  1.0000000\n\n\n\n\n\n\n\n\n\ndata_2.5&lt;-read.table(\"All_Data/p031.txt\",header=TRUE,sep=\"\\t\")\n\nx&lt;-data_2.5$Units\n\ny&lt;-data_2.5$Minutes\n\n\n\n\n\nsum((y-mean(y))*(x-mean(x))) #1768\n\n[1] 1768\n\nsum((x-mean(x))^2) #114\n\n[1] 114\n\nbeta1_hat&lt;-sum((y-mean(y))*(x-mean(x))) / sum((x-mean(x))^2)\n\nbeta1_hat #15.50877\n\n[1] 15.50877\n\nbeta0_hat &lt;- mean(y) - (beta1_hat*mean(x))\n\nbeta0_hat #4.161654\n\n[1] 4.161654\n\n### 최소제곱회귀 방정식\n\n# Minutes = 4.161654 + 15.50877 * Units\n\nplot(beta0_hat+beta1_hat*x,pch=19);\n\n\n\n# 4개의 고장 난 부품을 수리하는데 걸리는 에측시간\n\n4.161654 + 15.50877 * 4 #66.19673\n\n[1] 66.19673\n\nunits&lt;-4\n\nbeta0_hat + beta1_hat*units\n\n[1] 66.19674\n\n### 적합값(Fitted value)\n\ny_hat&lt;-beta0_hat + beta1_hat*(x)\n\n### 최소 제곱 잔차(residual)\n\ne&lt;-y-y_hat\n\ne #합이 0이라는 특징이 존재\n\n [1]  3.3295739 -6.1791980 -1.6879699 -2.1967419  7.8032581  5.2944862\n [7] -1.2142857 -0.2142857 -3.7230576 -9.2318296  5.2593985  1.2593985\n[13] -5.2493734  6.7506266\n\nsum(e) #1.278977e-13 0에 근사한 추지가 나옴\n\n[1] 1.278977e-13\n\n\n\n\n\n\ndf_2.7&lt;-data.frame(\n\n  x=x,\n\n  y=y,\n\n  y_hat,\n\n  e\n\n)\n\ndf_2.7\n\n    x   y     y_hat          e\n1   1  23  19.67043  3.3295739\n2   2  29  35.17920 -6.1791980\n3   3  49  50.68797 -1.6879699\n4   4  64  66.19674 -2.1967419\n5   4  74  66.19674  7.8032581\n6   5  87  81.70551  5.2944862\n7   6  96  97.21429 -1.2142857\n8   6  97  97.21429 -0.2142857\n9   7 109 112.72306 -3.7230576\n10  8 119 128.23183 -9.2318296\n11  9 149 143.74060  5.2593985\n12  9 145 143.74060  1.2593985\n13 10 154 159.24937 -5.2493734\n14 10 166 159.24937  6.7506266\n\n### lm() 함수 (linear model)\n\n# Minutes = beta0 + beta1 * Units + epsilon\n\n# 모형식 : y~x\n\nlm(y~x)\n\n\nCall:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)            x  \n      4.162       15.509  \n\nres_lm&lt;-lm(Minutes~Units,data=data_2.5)\n\nres_lm\n\n\nCall:\nlm(formula = Minutes ~ Units, data = data_2.5)\n\nCoefficients:\n(Intercept)        Units  \n      4.162       15.509  \n\n# 리스트의 이름 \n\nnames(res_lm)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n# 회귀계수\n\nres_lm$coefficients\n\n(Intercept)       Units \n   4.161654   15.508772 \n\ncoef(res_lm)\n\n(Intercept)       Units \n   4.161654   15.508772 \n\n# 적합값\n\nres_lm$fitted.values\n\n        1         2         3         4         5         6         7         8 \n 19.67043  35.17920  50.68797  66.19674  66.19674  81.70551  97.21429  97.21429 \n        9        10        11        12        13        14 \n112.72306 128.23183 143.74060 143.74060 159.24937 159.24937 \n\nfitted(res_lm)\n\n        1         2         3         4         5         6         7         8 \n 19.67043  35.17920  50.68797  66.19674  66.19674  81.70551  97.21429  97.21429 \n        9        10        11        12        13        14 \n112.72306 128.23183 143.74060 143.74060 159.24937 159.24937 \n\n# 최소제곱잔차\n\nres_lm$residuals\n\n         1          2          3          4          5          6          7 \n 3.3295739 -6.1791980 -1.6879699 -2.1967419  7.8032581  5.2944862 -1.2142857 \n         8          9         10         11         12         13         14 \n-0.2142857 -3.7230576 -9.2318296  5.2593985  1.2593985 -5.2493734  6.7506266 \n\nresid(res_lm)\n\n         1          2          3          4          5          6          7 \n 3.3295739 -6.1791980 -1.6879699 -2.1967419  7.8032581  5.2944862 -1.2142857 \n         8          9         10         11         12         13         14 \n-0.2142857 -3.7230576 -9.2318296  5.2593985  1.2593985 -5.2493734  6.7506266 \n\nresiduals(res_lm)\n\n         1          2          3          4          5          6          7 \n 3.3295739 -6.1791980 -1.6879699 -2.1967419  7.8032581  5.2944862 -1.2142857 \n         8          9         10         11         12         13         14 \n-0.2142857 -3.7230576 -9.2318296  5.2593985  1.2593985 -5.2493734  6.7506266 \n\n\n\n\n\n\nplot(beta0_hat+beta1_hat*x,pch=19);\n\n#abline(beta0_hat,beta1_hat)\n\nabline(res_lm)\n\n\n\n\n\n\n\n\n\ndata_2.5&lt;-read.table(\"All_Data/p031.txt\",header=TRUE,sep=\"\\t\")\n\nres_lm &lt;- lm(Minutes ~ Units, data = data_2.5)\n\nres_lm\n\n\nCall:\nlm(formula = Minutes ~ Units, data = data_2.5)\n\nCoefficients:\n(Intercept)        Units  \n      4.162       15.509  \n\nres_lm_summ&lt;-summary(res_lm)\n\nres_lm_summ #Pr(&gt;|t|) - p-value\n\n\nCall:\nlm(formula = Minutes ~ Units, data = data_2.5)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.2318 -3.3415 -0.7143  4.7769  7.8033 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    4.162      3.355    1.24    0.239    \nUnits         15.509      0.505   30.71 8.92e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.392 on 12 degrees of freedom\nMultiple R-squared:  0.9874,    Adjusted R-squared:  0.9864 \nF-statistic: 943.2 on 1 and 12 DF,  p-value: 8.916e-13\n\n# unit은 시간에 영향을준다 약15.5분 만큼씩 \n\n# coefficient에서 p-value에 대해서 알 수 있음 \n\n# beta_0는 0이라고 보면되느냐? p-value가 0.05보다 크기에 \n\n\n\n\n\n\n\nconfint(res_lm) # beta_0,1의 95% 신뢰구간을 뽑아줌 \n\n                2.5 %   97.5 %\n(Intercept) -3.148482 11.47179\nUnits       14.408512 16.60903\n\n?confint #level = 1-alpha\n\nstarting httpd help server ... done\n\nconfint(res_lm, level=0.90) # 90%의 신뢰구간\n\n                 5 %     95 %\n(Intercept) -1.81810 10.14141\nUnits       14.60875 16.40879\n\n\n\n\n\n\n# 4개의 고장난 부품을 수리하는 데에 걸리는 시간 예측\n\nx&lt;-4\n\n4.161654 + 15.508772 *4\n\n[1] 66.19674\n\nres_lm$coefficients[1]+(res_lm$coefficients[2]*x)\n\n(Intercept) \n   66.19674 \n\n### predict()\n\ndf&lt;-data.frame(Units=4) \n\npredict(res_lm,newdata=df) # res_lm을 만들때 사용한 데이터형식으로 만들어주어야함\n\n       1 \n66.19674 \n\nres_lm_pred&lt;-predict(res_lm,newdata=df,se.fit=TRUE)\n\n### 예측값\n\nres_lm_pred$fit\n\n       1 \n66.19674 \n\n### 표준오차\n\nres_lm_pred$se.fit # 평균반응에 대한 표준오차 \n\n[1] 1.759688\n\n### 예측한계\n\ndf&lt;-data.frame(Units=4) #예제서는 4대기준\n\ndf&lt;-data.frame(Units=1:10) #다른것도 보고 싶은경우 \n\nres_lm_pred_int_p&lt;-predict(res_lm,newdata=df,interval=\"prediction\")\n\n### 신뢰한계\n\ndf&lt;-data.frame(Units=1:10) #다른것도 보고 싶은경우 \n\nres_lm_pred_int_c&lt;-predict(res_lm,newdata=df,interval=\"confidence\") #둘의 차이를 보면 예측한계의 범위가 더큼 \n\n### 예측한계 & 신뢰한계\n\n# 신뢰한계는 평균에서 멀어지만 오차의범위가 커지고 평균에 다가갈수록 오차가 줄어듬\n\nplot(Minutes~Units,data=data_2.5,pch=19)\n\nabline(res_lm,col=\"red\",lwd=2)\n\nlines(1:10,res_lm_pred_int_p[,\"lwr\"],col=\"darkgreen\")\n\nlines(1:10,res_lm_pred_int_p[,\"upr\"],col=\"darkgreen\")\n\nlines(1:10,res_lm_pred_int_c[,\"lwr\"],col=\"blue\")\n\nlines(1:10,res_lm_pred_int_c[,\"upr\"],col=\"blue\")\n\n\n\n\n\n\n\n\nres_lm_summ&lt;-summary(res_lm)\n\nres_lm_summ #Multiple R-squared:0.9874 -&gt; 반응변수의 전체변이중 98.94%가 예측변수에 의해 설명된다\n\n\nCall:\nlm(formula = Minutes ~ Units, data = data_2.5)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.2318 -3.3415 -0.7143  4.7769  7.8033 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    4.162      3.355    1.24    0.239    \nUnits         15.509      0.505   30.71 8.92e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.392 on 12 degrees of freedom\nMultiple R-squared:  0.9874,    Adjusted R-squared:  0.9864 \nF-statistic: 943.2 on 1 and 12 DF,  p-value: 8.916e-13\n\n# 만약 R-squared가 1이면 완벽한 선형의 관계 100%라는 것을 의미한다.\n\n# R-squared는 변수가 들어갈수록 커지기에 adjust R-squared를 사용 추후 설명 \n\n\n\n\n\n# Minutes = beta1 + Units + epsilon\n\nres_lm_no&lt;-lm(Minutes~Units-1,data=data_2.5)\n\nres_lm_no\n\n\nCall:\nlm(formula = Minutes ~ Units - 1, data = data_2.5)\n\nCoefficients:\nUnits  \n16.07  \n\nsummary(res_lm_no)\n\n\nCall:\nlm(formula = Minutes ~ Units - 1, data = data_2.5)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5955 -2.4733  0.4417  5.0243  9.7023 \n\nCoefficients:\n      Estimate Std. Error t value Pr(&gt;|t|)    \nUnits  16.0744     0.2213   72.62   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.502 on 13 degrees of freedom\nMultiple R-squared:  0.9975,    Adjusted R-squared:  0.9974 \nF-statistic:  5274 on 1 and 13 DF,  p-value: &lt; 2.2e-16\n\ncoef(summary(res_lm_no)) #rsquared=0.9975\n\n      Estimate Std. Error  t value     Pr(&gt;|t|)\nUnits 16.07443  0.2213341 72.62519 2.380325e-18\n\n\n\n\n\n\ny&lt;-rnorm(30)\n\nt.test(y,mu=0)\n\n\n    One Sample t-test\n\ndata:  y\nt = -0.36971, df = 29, p-value = 0.7143\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.3936633  0.2731294\nsample estimates:\n  mean of x \n-0.06026695 \n\nsummary(lm(y~1))\n\n\nCall:\nlm(formula = y ~ 1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.7055 -0.5113 -0.1068  0.5783  1.5982 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -0.06027    0.16301   -0.37    0.714\n\nResidual standard error: 0.8929 on 29 degrees of freedom\n\n\n\n\n\n\n\n\n\ndata_3.3&lt;-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\ndim(data_3.3)\n\n[1] 30  7\n\nclass(data_3.3)\n\n[1] \"data.frame\"\n\nsapply(data_3.3,class) #all numeric\n\n        Y        X1        X2        X3        X4        X5        X6 \n\"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \n\nsummary(data_3.3) #모든변수가 numeric이면 분위수도 보여준다 \n\n       Y               X1             X2              X3              X4       \n Min.   :40.00   Min.   :37.0   Min.   :30.00   Min.   :34.00   Min.   :43.00  \n 1st Qu.:58.75   1st Qu.:58.5   1st Qu.:45.00   1st Qu.:47.00   1st Qu.:58.25  \n Median :65.50   Median :65.0   Median :51.50   Median :56.50   Median :63.50  \n Mean   :64.63   Mean   :66.6   Mean   :53.13   Mean   :56.37   Mean   :64.63  \n 3rd Qu.:71.75   3rd Qu.:77.0   3rd Qu.:62.50   3rd Qu.:66.75   3rd Qu.:71.00  \n Max.   :85.00   Max.   :90.0   Max.   :83.00   Max.   :75.00   Max.   :88.00  \n       X5              X6       \n Min.   :49.00   Min.   :25.00  \n 1st Qu.:69.25   1st Qu.:35.00  \n Median :77.50   Median :41.00  \n Mean   :74.77   Mean   :42.93  \n 3rd Qu.:80.00   3rd Qu.:47.75  \n Max.   :92.00   Max.   :72.00  \n\n### 산점도 행렬\n\nplot(data_3.3)\n\n\n\n\n\n\n\n\nlm(Y~X1+X2+X3+X4+X5+X6,data=data_3.3)\n\n\nCall:\nlm(formula = Y ~ X1 + X2 + X3 + X4 + X5 + X6, data = data_3.3)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n   10.78708      0.61319     -0.07305      0.32033      0.08173      0.03838  \n         X6  \n   -0.21706  \n\nlm(Y~.,data=data_3.3) # X1+X2+X3+X4+X5+X6쓰는 것이 아니라 .을 써서 모든 변수를 다써줌 \n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n   10.78708      0.61319     -0.07305      0.32033      0.08173      0.03838  \n         X6  \n   -0.21706  \n\n\n\n\n\n\ndata_3.3&lt;-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\nlm(Y~X1+X2,data=data_3.3)\n\n\nCall:\nlm(formula = Y ~ X1 + X2, data = data_3.3)\n\nCoefficients:\n(Intercept)           X1           X2  \n   15.32762      0.78034     -0.05016  \n\n# (Intercept)         X1           X2  \n\n#  15.32762      0.78034     -0.05016\n\n# 1) Y에서 X1 효과 제거\n\nm1&lt;-lm(Y~X1,data=data_3.3) # y prime\n\nm1$residuals # x1이 설명하지 못한값 / x1의 효과를 제거한 값\n\n           1            2            3            4            5            6 \n -9.86142016   0.32865220   3.80099328  -0.91673799   7.76411473 -12.87985944 \n           7            8            9           10           11           12 \n -6.93517726   0.02794419  -4.25432454   6.59248165   9.62936020   7.34709147 \n          13           14           15           16           17           18 \n  7.83787183  -9.00893436   4.51872455  -1.29120309  -4.51815400   5.34709147 \n          19           20           21           22           23           24 \n -2.19900672  -8.14368889   5.43928784   3.59248165 -11.18056744  -2.29688270 \n          25           26           27           28           29           30 \n  7.87475038  -6.48127545   7.02794419  -9.38907907   6.48184600   5.74567546 \n\n# 2) X2에서 X1 효과 제거\n\nm2&lt;-lm(X2~X1,data=data_3.3) # x2 prime \n\nm2$residuals \n\n          1           2           3           4           5           6 \n-15.1300345  -0.7994502  13.1223579  -6.2864182  -2.9818979   1.8178376 \n          7           8           9          10          11          12 \n-11.3385461  -7.4428019  10.9659742  -5.2603543   6.8439016  -2.7473223 \n         13          14          15          16          17          18 \n  6.2266138  21.4529422  -4.4688659 -15.1382816   1.4268783  15.2526777 \n         19          20          21          22          23          24 \n -8.8776421  19.2787417  -6.4866827   1.7396457  -0.8255141   4.0524132 \n         25          26          27          28          29          30 \n -4.6691304   7.5311341   0.5571981  -4.2082264   8.4268783 -22.0340258 \n\n# 3) X1의 효과가 제거된 Y와 X2의 적합 - 원점을 지나는 회귀선\n\nlm(m1$residuals~m2$residuals-1) # 원점을 지나면 -1를 하고 진행 // -3.25e-17\n\n\nCall:\nlm(formula = m1$residuals ~ m2$residuals - 1)\n\nCoefficients:\nm2$residuals  \n    -0.05016  \n\n# 다른 효과 없이(다른값이 고정) Y에 영향을 주는 순수한 X2의 값\n\n# m2$residuals  : -0.05016  ==  X2 : -0.05016  \n\n### 단위길이 척도화 - 잘사용하지않음\n\nfn_scaling_len&lt;-function(x){\n\n  x0&lt;-x-mean(x)\n\n  x0/sqrt(sum(x0^2))\n\n}\n\ndata_3.3_len&lt;-sapply(data_3.3, fn_scaling_len)\n\ndata_3.3_len&lt;-data.frame(data_3.3_len)\n\nsummary(data_3.3_len)\n\n       Y                  X1                 X2                 X3           \n Min.   :-0.37579   Min.   :-0.41282   Min.   :-0.35109   Min.   :-0.353871  \n 1st Qu.:-0.08975   1st Qu.:-0.11297   1st Qu.:-0.12344   1st Qu.:-0.148193  \n Median : 0.01322   Median :-0.02231   Median :-0.02479   Median : 0.002109  \n Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.000000  \n 3rd Qu.: 0.10857   3rd Qu.: 0.14504   3rd Qu.: 0.14216   3rd Qu.: 0.164278  \n Max.   : 0.31070   Max.   : 0.32635   Max.   : 0.45328   Max.   : 0.294804  \n       X4                 X5                 X6          \n Min.   :-0.38637   Min.   :-0.48356   Min.   :-0.32367  \n 1st Qu.:-0.11401   1st Qu.:-0.10353   1st Qu.:-0.14318  \n Median :-0.02024   Median : 0.05130   Median :-0.03489  \n Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.00000  \n 3rd Qu.: 0.11371   3rd Qu.: 0.09821   3rd Qu.: 0.08693  \n Max.   : 0.41733   Max.   : 0.32341   Max.   : 0.52461  \n\nlm(Y~.,data=data_3.3_len)\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3_len)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n -1.259e-16    6.707e-01   -7.343e-02    3.089e-01    6.981e-02    3.120e-02  \n         X6  \n -1.835e-01  \n\n### 표준화\n\n# scale()\n\ndata_3.3_std&lt;-scale(data_3.3)\n\n#summary(data_3.3_std)\n\n#sapply(data_3.3_std, sd, na.rm=T)\n\n#class(data_3.3_std) #\"matrix\"\n\ndata_3.3_std&lt;-data.frame(data_3.3_std)\n\n#class(data_3.3_std) #\"data.frame\"\n\n#sapply(data_3.3_std, sd, na.rm=T)\n\nlm(Y~.,data=data_3.3_std) # beta게수 구하기 \n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3_std)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n -7.717e-16    6.707e-01   -7.343e-02    3.089e-01    6.981e-02    3.120e-02  \n         X6  \n -1.835e-01  \n\n\n\n\n\n\nres_lm&lt;-lm(Y~.,data=data_3.3)\n\nres_lm\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n   10.78708      0.61319     -0.07305      0.32033      0.08173      0.03838  \n         X6  \n   -0.21706  \n\nsummary(res_lm)\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.9418  -4.3555   0.3158   5.5425  11.5990 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10.78708   11.58926   0.931 0.361634    \nX1           0.61319    0.16098   3.809 0.000903 ***\nX2          -0.07305    0.13572  -0.538 0.595594    \nX3           0.32033    0.16852   1.901 0.069925 .  \nX4           0.08173    0.22148   0.369 0.715480    \nX5           0.03838    0.14700   0.261 0.796334    \nX6          -0.21706    0.17821  -1.218 0.235577    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.068 on 23 degrees of freedom\nMultiple R-squared:  0.7326,    Adjusted R-squared:  0.6628 \nF-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05\n\nm1&lt;-summary(lm(Y~X1+X2+X3+X4+X5+X6,data=data_3.3)) #Adjusted R-squared:  0.6628 \n\nm2&lt;-summary(lm(Y~X1+X2+X3+X4+X5,data=data_3.3)) #Adjusted R-squared:  0.6561 \n\n# X6가 들어가는 것이 더 좋은 모델 \n\nm1$adj.r.squared\n\n[1] 0.662846\n\nm2$adj.r.squared #summary에서 보다 더 정확하게 수치가 나옴 \n\n[1] 0.6560539\n\n\n\n\n\n\nres_lm_summ&lt;-summary(res_lm)\n\nres_lm_summ #p-value의 존재는 무언가를 검정했다라는 반증\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.9418  -4.3555   0.3158   5.5425  11.5990 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10.78708   11.58926   0.931 0.361634    \nX1           0.61319    0.16098   3.809 0.000903 ***\nX2          -0.07305    0.13572  -0.538 0.595594    \nX3           0.32033    0.16852   1.901 0.069925 .  \nX4           0.08173    0.22148   0.369 0.715480    \nX5           0.03838    0.14700   0.261 0.796334    \nX6          -0.21706    0.17821  -1.218 0.235577    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.068 on 23 degrees of freedom\nMultiple R-squared:  0.7326,    Adjusted R-squared:  0.6628 \nF-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05\n\n# p-value&lt;0.05 H_1 귀무가설 채택 \n\n# p-value&gt;0.05 H_0 영가설 채택 // X1을 제외하고는 영가설 유의한 의미가 없음(Y에영향주는)\n\n# 모두다 0이라는 가설을 가지고 분모 분자의 오차가 카이제곱을 따르고 거기서 나온 통계량\n\n# F-분포 자유도는 분자 분모 두개를 가짐 //모아서 계산을 하기에 각각 계산하는것과 결과다름 \n\n# 영가설-모든 회귀계수가 0이다.\n\n# 대립가설-적어도 하나는 0이 아니다. p-value: 1.24e-05 &lt;0.05 대립가설 채택 \n\n# p-value가 0.05보다 작으면 대립가설 채택!!!!!! 기억해 \n\n# 회귀계수에 대한 신뢰구간 - 95% 신뢰한계\n\nconfint(res_lm) #-13.18712881 ~ 34.7612816\n\n                   2.5 %     97.5 %\n(Intercept) -13.18712881 34.7612816\nX1            0.28016866  0.9462066\nX2           -0.35381806  0.2077178\nX3           -0.02827872  0.6689430\nX4           -0.37642935  0.5398936\nX5           -0.26570179  0.3424647\nX6           -0.58571106  0.1515977\n\n#X1  0.28016866  0.9462066  사이에 0이 들어가있으면 영향을 준다라느걸 의미\n\n#X2 -0.35381806  0.2077178  p-value없이도 알 수 있음 \n\n#X5가 가장 영향이 적음 p-value가 가장 크기에(영향 효과의 크기를 비교할때)\n\n#p-value가 작을 수록 영향을 많이 준다 beta값을 보는 것이 아닌 p-value를 보는 것 중요\n\n#가장 의미있는 변수?-&gt;p-value가장 작은거 // 대립가설채택 Y에 영향을 가장\n\n\n\n\n\n\n\n\n# H_0: beta_1:beta_6=0\n\nmodel_reduce&lt;-lm(Y~1,data=data_3.3)\n\nmodel_full&lt;-lm(Y~.,data=data_3.3)\n\nanova(model_reduce,model_full)\n\nAnalysis of Variance Table\n\nModel 1: Y ~ 1\nModel 2: Y ~ X1 + X2 + X3 + X4 + X5 + X6\n  Res.Df  RSS Df Sum of Sq      F   Pr(&gt;F)    \n1     29 4297                                 \n2     23 1149  6      3148 10.502 1.24e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#대립가설 = 완전모형이 적절하다 / 1.24e-05 *** &lt; 0.05 \n\n#의미 있는 예측 변수가 한개 이상 존재한다 \n\nsummary(model_full) #summary에서 beta_1~beta_6까지 모두가 0이라는 가설로 진행을 이미함\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.9418  -4.3555   0.3158   5.5425  11.5990 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10.78708   11.58926   0.931 0.361634    \nX1           0.61319    0.16098   3.809 0.000903 ***\nX2          -0.07305    0.13572  -0.538 0.595594    \nX3           0.32033    0.16852   1.901 0.069925 .  \nX4           0.08173    0.22148   0.369 0.715480    \nX5           0.03838    0.14700   0.261 0.796334    \nX6          -0.21706    0.17821  -1.218 0.235577    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.068 on 23 degrees of freedom\nMultiple R-squared:  0.7326,    Adjusted R-squared:  0.6628 \nF-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05\n\n#F-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05\n\n#예상// 가장의미있는변수? -&gt; X1 이유?-&gt; p-value 0.000903 로 가장 작기에 영향많이 줄것으로 예측 \n\n\n\n\n\nmodel_reduce&lt;-lm(Y~X1+X3,data=data_3.3)\n\nmodel_full&lt;-lm(Y~.,data=data_3.3)\n\nanova(model_reduce,model_full) #0.7158 &gt; 0.05\n\nAnalysis of Variance Table\n\nModel 1: Y ~ X1 + X3\nModel 2: Y ~ X1 + X2 + X3 + X4 + X5 + X6\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     27 1254.7                           \n2     23 1149.0  4    105.65 0.5287 0.7158\n\n#영가설은 H_0: b_2=b_4=b_5=b_6=0 이라는 사실을 알 수 있다 \n\n#b_1&b_3는 반응변수에 유의한 반응을 준다라는 것도 연계하여 알 수 있다 \n\n\n\n\n\n#해당 조건이 주어지고 만족할 때 beta_1=beta_3은 맞는가?\n\nmodel_reduce&lt;-lm(Y~I(X1-X3),data=data_3.3) #I를 씌우면 새로운 변수를 만든것과 동일\n\n# X1-X3를 한 그자체를 분석하라는 의미//본래는 X1-X3 해서 새로운 변수를 만들어서 해야함 \n\nmodel_full&lt;-lm(Y~X1+X3,data=data_3.3)\n\nanova(model_reduce,model_full) \n\nAnalysis of Variance Table\n\nModel 1: Y ~ I(X1 - X3)\nModel 2: Y ~ X1 + X3\n  Res.Df    RSS Df Sum of Sq     F    Pr(&gt;F)    \n1     28 3846.7                                 \n2     27 1254.6  1      2592 55.78 4.925e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#install.packages(\"car\")\n\nlibrary(car)\n\nLoading required package: carData\n\nmodel_full&lt;-lm(Y~X1+X3,data=data_3.3)\n\ncar::linearHypothesis(model_full,c(\"X1=X3\"))\n\nLinear hypothesis test\n\nHypothesis:\nX1 - X3 = 0\n\nModel 1: restricted model\nModel 2: Y ~ X1 + X3\n\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     28 1424.6                              \n2     27 1254.7  1    169.95 3.6572 0.06649 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n# H_0: beta_1+beta_3=1 | beta_2=beta_3:beta_6=0\n\nmodel_full&lt;-lm(Y~X1+X3,data=data_3.3)\n\ncar::linearHypothesis(model_full,c(\"X1 + X3 = 1\"))\n\nLinear hypothesis test\n\nHypothesis:\nX1  + X3 = 1\n\nModel 1: restricted model\nModel 2: Y ~ X1 + X3\n\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     28 1329.5                           \n2     27 1254.7  1    74.898 1.6118 0.2151\n\n# x1의 효과가 증가하면 x3의 효과는 감소한다 상대적인 관계 (반대로도 가능)\n\n\n\n\n\nmodel_full&lt;-lm(Y~.,data_3.3)\n\n# 예측값 - 적합값\n\nmodel_full$fitted.values\n\n       1        2        3        4        5        6        7        8 \n51.11030 61.35277 69.93944 61.22684 74.45380 53.94185 67.14841 70.09701 \n       9       10       11       12       13       14       15       16 \n79.53099 59.19846 57.92572 55.40103 59.58168 70.21401 76.54933 84.54785 \n      17       18       19       20       21       22       23       24 \n76.15013 61.39736 68.01656 55.62014 42.60324 63.81902 63.66400 44.62475 \n      25       26       27       28       29       30 \n57.31710 67.84347 75.14036 56.04535 77.66053 76.87850 \n\n# 예측한계(Prediction Limits)\n\npredict(model_full,newdata = data_3.3,interval = \"prediction\")\n\n        fit      lwr       upr\n1  51.11030 34.16999  68.05060\n2  61.35277 46.34536  76.36018\n3  69.93944 53.94267  85.93622\n4  61.22684 45.44586  77.00783\n5  74.45380 59.17630  89.73129\n6  53.94185 37.21813  70.66557\n7  67.14841 51.64493  82.65189\n8  70.09701 54.54384  85.65017\n9  79.53099 62.71383  96.34814\n10 59.19846 44.03506  74.36185\n11 57.92572 42.00674  73.84470\n12 55.40103 39.79333  71.00873\n13 59.58168 43.39853  75.76483\n14 70.21401 52.06636  88.36167\n15 76.54933 60.79444  92.30422\n16 84.54785 66.41374 102.68197\n17 76.15013 59.99991  92.30036\n18 61.39736 43.23384  79.56088\n19 68.01656 52.44673  83.58639\n20 55.62014 39.63744  71.60284\n21 42.60324 26.35046  58.85603\n22 63.81902 48.40145  79.23659\n23 63.66400 48.56222  78.76578\n24 44.62475 27.25435  61.99514\n25 57.31710 41.29380  73.34041\n26 67.84347 49.98605  85.70089\n27 75.14036 59.31975  90.96097\n28 56.04535 40.18723  71.90348\n29 77.66053 61.97564  93.34541\n30 76.87850 60.27441  93.48258\n\n# 신뢰한계(Confidence limits)\n\npredict(model_full,newdata = data_3.3,interval = \"confidence\")\n\n        fit      lwr      upr\n1  51.11030 42.55502 59.66557\n2  61.35277 57.97029 64.73524\n3  69.93944 63.44979 76.42909\n4  61.22684 55.28897 67.16471\n5  74.45380 70.02428 78.88332\n6  53.94185 45.82386 62.05984\n7  67.14841 61.99316 72.30367\n8  70.09701 64.79421 75.39980\n9  79.53099 71.22222 87.83975\n10 59.19846 55.18008 63.21683\n11 57.92572 51.63028 64.22116\n12 55.40103 49.94035 60.86171\n13 59.58168 52.64531 66.51805\n14 70.21401 59.46431 80.96372\n15 76.54933 70.68118 82.41748\n16 84.54785 73.82102 95.27468\n17 76.15013 69.29093 83.00933\n18 61.39736 50.62090 72.17383\n19 68.01656 62.66507 73.36805\n20 55.62014 49.16527 62.07502\n21 42.60324 35.50593 49.70055\n22 63.81902 58.92819 68.70985\n23 63.66400 59.88479 67.44321\n24 44.62475 35.24662 54.00288\n25 57.31710 50.76233 63.87187\n26 67.84347 57.59134 78.09561\n27 75.14036 69.09798 81.18275\n28 56.04535 49.90540 62.18531\n29 77.66053 71.98300 83.33806\n30 76.87850 69.00992 84.74707\n\n\n\n\n\n\n\ndata_3.3&lt;-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\nY&lt;-data_3.3$Y\n\nX&lt;-data_3.3[,-1]\n\nX&lt;-cbind(1,X)\n\nX&lt;-as.matrix(X)\n\n#beta_hat&lt;-solve(t(X) %*% X) %*% t(X) %*% Y # %*%행렬 계산 \n\nP&lt;-solve(t(X) %*% X) %*% t(X)\n\nbeta_hat&lt;- P %*% Y\n\nlm(Y~.,data_3.3)\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n   10.78708      0.61319     -0.07305      0.32033      0.08173      0.03838  \n         X6  \n   -0.21706  \n\n\n\n\n\n\n\n\n# 표준화 잔차\n\ndata_3.3&lt;-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\nres_lm&lt;-lm(Y~.,data=data_3.3)\n\nclass(res_lm)\n\n[1] \"lm\"\n\nmode(res_lm)\n\n[1] \"list\"\n\nnames(res_lm)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\nres_lm$fitted.values\n\n       1        2        3        4        5        6        7        8 \n51.11030 61.35277 69.93944 61.22684 74.45380 53.94185 67.14841 70.09701 \n       9       10       11       12       13       14       15       16 \n79.53099 59.19846 57.92572 55.40103 59.58168 70.21401 76.54933 84.54785 \n      17       18       19       20       21       22       23       24 \n76.15013 61.39736 68.01656 55.62014 42.60324 63.81902 63.66400 44.62475 \n      25       26       27       28       29       30 \n57.31710 67.84347 75.14036 56.04535 77.66053 76.87850 \n\nstr(res_lm)\n\nList of 12\n $ coefficients : Named num [1:7] 10.7871 0.6132 -0.0731 0.3203 0.0817 ...\n  ..- attr(*, \"names\")= chr [1:7] \"(Intercept)\" \"X1\" \"X2\" \"X3\" ...\n $ residuals    : Named num [1:30] -8.11 1.647 1.061 -0.227 6.546 ...\n  ..- attr(*, \"names\")= chr [1:30] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:30] -354.011 54.107 2.742 11.715 -0.971 ...\n  ..- attr(*, \"names\")= chr [1:30] \"(Intercept)\" \"X1\" \"X2\" \"X3\" ...\n $ rank         : int 7\n $ fitted.values: Named num [1:30] 51.1 61.4 69.9 61.2 74.5 ...\n  ..- attr(*, \"names\")= chr [1:30] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:7] 0 1 2 3 4 5 6\n $ qr           :List of 5\n  ..$ qr   : num [1:30, 1:7] -5.477 0.183 0.183 0.183 0.183 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:30] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ : chr [1:7] \"(Intercept)\" \"X1\" \"X2\" \"X3\" ...\n  .. ..- attr(*, \"assign\")= int [1:7] 0 1 2 3 4 5 6\n  ..$ qraux: num [1:7] 1.18 1 1.29 1.1 1.07 ...\n  ..$ pivot: int [1:7] 1 2 3 4 5 6 7\n  ..$ tol  : num 1e-07\n  ..$ rank : int 7\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 23\n $ xlevels      : Named list()\n $ call         : language lm(formula = Y ~ ., data = data_3.3)\n $ terms        :Classes 'terms', 'formula'  language Y ~ X1 + X2 + X3 + X4 + X5 + X6\n  .. ..- attr(*, \"variables\")= language list(Y, X1, X2, X3, X4, X5, X6)\n  .. ..- attr(*, \"factors\")= int [1:7, 1:6] 0 1 0 0 0 0 0 0 0 1 ...\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:7] \"Y\" \"X1\" \"X2\" \"X3\" ...\n  .. .. .. ..$ : chr [1:6] \"X1\" \"X2\" \"X3\" \"X4\" ...\n  .. ..- attr(*, \"term.labels\")= chr [1:6] \"X1\" \"X2\" \"X3\" \"X4\" ...\n  .. ..- attr(*, \"order\")= int [1:6] 1 1 1 1 1 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. ..- attr(*, \"predvars\")= language list(Y, X1, X2, X3, X4, X5, X6)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:7] \"numeric\" \"numeric\" \"numeric\" \"numeric\" ...\n  .. .. ..- attr(*, \"names\")= chr [1:7] \"Y\" \"X1\" \"X2\" \"X3\" ...\n $ model        :'data.frame':  30 obs. of  7 variables:\n  ..$ Y : num [1:30] 43 63 71 61 81 43 58 71 72 67 ...\n  ..$ X1: num [1:30] 51 64 70 63 78 55 67 75 82 61 ...\n  ..$ X2: num [1:30] 30 51 68 45 56 49 42 50 72 45 ...\n  ..$ X3: num [1:30] 39 54 69 47 66 44 56 55 67 47 ...\n  ..$ X4: num [1:30] 61 63 76 54 71 54 66 70 71 62 ...\n  ..$ X5: num [1:30] 92 73 86 84 83 49 68 66 83 80 ...\n  ..$ X6: num [1:30] 45 47 48 35 47 34 35 41 31 41 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language Y ~ X1 + X2 + X3 + X4 + X5 + X6\n  .. .. ..- attr(*, \"variables\")= language list(Y, X1, X2, X3, X4, X5, X6)\n  .. .. ..- attr(*, \"factors\")= int [1:7, 1:6] 0 1 0 0 0 0 0 0 0 1 ...\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:7] \"Y\" \"X1\" \"X2\" \"X3\" ...\n  .. .. .. .. ..$ : chr [1:6] \"X1\" \"X2\" \"X3\" \"X4\" ...\n  .. .. ..- attr(*, \"term.labels\")= chr [1:6] \"X1\" \"X2\" \"X3\" \"X4\" ...\n  .. .. ..- attr(*, \"order\")= int [1:6] 1 1 1 1 1 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(Y, X1, X2, X3, X4, X5, X6)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:7] \"numeric\" \"numeric\" \"numeric\" \"numeric\" ...\n  .. .. .. ..- attr(*, \"names\")= chr [1:7] \"Y\" \"X1\" \"X2\" \"X3\" ...\n - attr(*, \"class\")= chr \"lm\"\n\n# 잔차 \n\nres_lm$residuals\n\n          1           2           3           4           5           6 \n -8.1102953   1.6472337   1.0605589  -0.2268416   6.5462010 -10.9418499 \n          7           8           9          10          11          12 \n -9.1484140   0.9029929  -7.5309862   7.8015424   6.0742817  11.5989723 \n         13          14          15          16          17          18 \n  9.4183197  -2.2140147   0.4506705  -3.5478519  -2.1501319   3.6026355 \n         19          20          21          22          23          24 \n -3.0165587  -5.6201442   7.3967582   0.1809831 -10.6639999  -4.6247464 \n         25          26          27          28          29          30 \n  5.6828983  -1.8434727   2.8596385  -8.0453540   7.3394730   5.1215016 \n\nresid(res_lm) #실제값에서 예측된 값을 뺸값\n\n          1           2           3           4           5           6 \n -8.1102953   1.6472337   1.0605589  -0.2268416   6.5462010 -10.9418499 \n          7           8           9          10          11          12 \n -9.1484140   0.9029929  -7.5309862   7.8015424   6.0742817  11.5989723 \n         13          14          15          16          17          18 \n  9.4183197  -2.2140147   0.4506705  -3.5478519  -2.1501319   3.6026355 \n         19          20          21          22          23          24 \n -3.0165587  -5.6201442   7.3967582   0.1809831 -10.6639999  -4.6247464 \n         25          26          27          28          29          30 \n  5.6828983  -1.8434727   2.8596385  -8.0453540   7.3394730   5.1215016 \n\n### 내적 표준화잔차\n\nrstandard(res_lm)\n\n          1           2           3           4           5           6 \n-1.41498026  0.23955370  0.16744867 -0.03512080  0.97184596 -1.86133876 \n          7           8           9          10          11          12 \n-1.38317210  0.13709194 -1.29490454  1.14799070  0.95218982  1.76906521 \n         13          14          15          16          17          18 \n 1.51371017 -0.46212316  0.06961486 -0.73868563 -0.34446368  0.75418016 \n         19          20          21          22          23          24 \n-0.45861365 -0.88618779  1.19699287  0.02717120 -1.56184734 -0.85286680 \n         25          26          27          28          29          30 \n 0.89948517 -0.36581416  0.44430497 -1.25422677  1.12683185  0.85971512 \n\n### 외적 표준화잔차\n\nMASS::studres(res_lm)\n\n          1           2           3           4           5           6 \n-1.44835328  0.23458097  0.16386794 -0.03434974  0.97062209 -1.97526518 \n          7           8           9          10          11          12 \n-1.41280382  0.13413337 -1.31529351  1.15637546  0.95017640  1.86145176 \n         13          14          15          16          17          18 \n 1.56019127 -0.45407837  0.06809185 -0.73117411 -0.33776450  0.74689589 \n         19          20          21          22          23          24 \n-0.45059801 -0.88189556  1.20894332  0.02657438 -1.61559196 -0.84763116 \n         25          26          27          28          29          30 \n 0.89560731 -0.35881868  0.43641573 -1.27088888  1.13380428  0.85466249 \n\nredsid_df&lt;-data.frame(\n\n  Y=data_3.3$Y,\n\n  Y_hat=res_lm$fitted.values,\n\n  resid=resid(res_lm),\n\n  rstandard=rstandard(res_lm),\n\n  studres=MASS::studres(res_lm)\n\n)\n\nredsid_df\n\n    Y    Y_hat       resid   rstandard     studres\n1  43 51.11030  -8.1102953 -1.41498026 -1.44835328\n2  63 61.35277   1.6472337  0.23955370  0.23458097\n3  71 69.93944   1.0605589  0.16744867  0.16386794\n4  61 61.22684  -0.2268416 -0.03512080 -0.03434974\n5  81 74.45380   6.5462010  0.97184596  0.97062209\n6  43 53.94185 -10.9418499 -1.86133876 -1.97526518\n7  58 67.14841  -9.1484140 -1.38317210 -1.41280382\n8  71 70.09701   0.9029929  0.13709194  0.13413337\n9  72 79.53099  -7.5309862 -1.29490454 -1.31529351\n10 67 59.19846   7.8015424  1.14799070  1.15637546\n11 64 57.92572   6.0742817  0.95218982  0.95017640\n12 67 55.40103  11.5989723  1.76906521  1.86145176\n13 69 59.58168   9.4183197  1.51371017  1.56019127\n14 68 70.21401  -2.2140147 -0.46212316 -0.45407837\n15 77 76.54933   0.4506705  0.06961486  0.06809185\n16 81 84.54785  -3.5478519 -0.73868563 -0.73117411\n17 74 76.15013  -2.1501319 -0.34446368 -0.33776450\n18 65 61.39736   3.6026355  0.75418016  0.74689589\n19 65 68.01656  -3.0165587 -0.45861365 -0.45059801\n20 50 55.62014  -5.6201442 -0.88618779 -0.88189556\n21 50 42.60324   7.3967582  1.19699287  1.20894332\n22 64 63.81902   0.1809831  0.02717120  0.02657438\n23 53 63.66400 -10.6639999 -1.56184734 -1.61559196\n24 40 44.62475  -4.6247464 -0.85286680 -0.84763116\n25 63 57.31710   5.6828983  0.89948517  0.89560731\n26 66 67.84347  -1.8434727 -0.36581416 -0.35881868\n27 78 75.14036   2.8596385  0.44430497  0.43641573\n28 48 56.04535  -8.0453540 -1.25422677 -1.27088888\n29 85 77.66053   7.3394730  1.12683185  1.13380428\n30 82 76.87850   5.1215016  0.85971512  0.85466249\n\n\n\n\n\n\n\n\n\na&lt;-rnorm(100,70,10) #연속형 데이터\n\n# 히스토그램 \n\nhist(a)\n\n\n\nhist(a,breaks=5) #범위를 조절 막대의 5번 자름 \n\n\n\n# 줄기 잎 그림 \n\nstem(a)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  4 | 6\n  5 | 033\n  5 | 66678899\n  6 | 0011122333333344\n  6 | 555666677777888899999999\n  7 | 0000001111123444\n  7 | 556666778888999\n  8 | 0000122223344\n  8 | 5568\n\nstem(round(a)) #줄기잎을 그릴때는 반올림을 하고 항상 진행 \n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  4 | 6\n  5 | 033\n  5 | 66678899\n  6 | 0011122333333344\n  6 | 555666677777888899999999\n  7 | 0000001111123444\n  7 | 556666778888999\n  8 | 0000122223344\n  8 | 5568\n\nstem(round(a),scale=2) #scale을 2배로 늘려라 5기준으로 반으로 잘라서 \n\n\n  The decimal point is at the |\n\n  46 | 0\n  48 | \n  50 | 0\n  52 | 00\n  54 | \n  56 | 0000\n  58 | 0000\n  60 | 00000\n  62 | 000000000\n  64 | 00000\n  66 | 000000000\n  68 | 000000000000\n  70 | 00000000000\n  72 | 00\n  74 | 00000\n  76 | 000000\n  78 | 0000000\n  80 | 00000\n  82 | 000000\n  84 | 0000\n  86 | 0\n  88 | 0\n\n# 모든데이터를 볼 수있는 장점 데이터가 많으면 구림 \n\n# 점플롯\n\nidx&lt;-rep(1,length(a)) #a의 갯수에 맞춰서 1를 반복 \n\nplot(idx,a)\n\n\n\nplot(jitter(idx),a,xlim=c(0.5,1.5))\n\n\n\n# 상자그림\n\nboxplot(a) #사분위수에 대해서 알 수 있음 \n\n# 상자그림 + 점플롯\n\nboxplot(a)\n\npoints(jitter(idx),a)\n\n\n\n\n\n\n\n\ndata_4.1&lt;-read.table(\"All_Data/p103.txt\",header=T,sep=\"\\t\")\n\ndata_4.1\n\n       Y   X1   X2\n1  12.37 2.23 9.66\n2  12.66 2.57 8.94\n3  12.00 3.87 4.40\n4  11.93 3.10 6.64\n5  11.06 3.39 4.91\n6  13.03 2.83 8.52\n7  13.13 3.02 8.04\n8  11.44 2.14 9.05\n9  12.86 3.04 7.71\n10 10.84 3.26 5.11\n11 11.20 3.39 5.05\n12 11.56 2.35 8.51\n13 10.83 2.76 6.59\n14 12.63 3.90 4.90\n15 12.46 3.16 6.96\n\nclass(data_4.1) #data.frame\n\n[1] \"data.frame\"\n\n# 산점도 행렬\n\nplot(data_4.1)\n\ncor(data_4.1) #상관계수\n\n             Y           X1         X2\nY  1.000000000  0.002497966  0.4340688\nX1 0.002497966  1.000000000 -0.8997765\nX2 0.434068758 -0.899776481  1.0000000\n\npairs(data_4.1)\n\n\n\n# Correlation panel\n\npanel.cor&lt;-function(x,y){\n\n  par(usr=c(0,1,0,1))\n\n  r&lt;-round(cor(x,y),digits = 3)\n\n  text(0.5,0.5,r,cex=1.5)\n\n}\n\npairs(data_4.1,lower.panel = panel.cor)\n\n\n\n# 회전도표, 동적 그래프(3차원)\n\n#install.packages(\"rgl\")\n\nlibrary(rgl)\n\nplot3d(x=data_4.1$X1,y=data_4.1$X2,z=data_4.1$Y) \n\n\n\n\n\ndata_3.3&lt;-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\nres_lm&lt;-lm(Y~X1+X3,data=data_3.3)\n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(res_lm) #회귀진단 플랏이 나옴 \n\n\n\nlayout(1)\n\n# 1. 표준화잔차의 정규확률분포\n\nplot(res_lm,2)\n\n\n\n# 2. 표준화잔차 대 각 예측변수들의 산점도\n\nplot(res_lm,3) #이런경우에는 데이터를 추가한다 좌측하단이 없어서 \n\n\n\n# 3. 표준화잔차 대 적합값의 플롯\n\n# 4. 표준화잔차의 인덱스 플롯\n\n\n\n\n\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:car':\n\n    recode\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n\n\ndata_3.3&lt;-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\nres_lm&lt;-lm(Y~X1+X3,data=data_3.3) #두개의 변수만 의미있다고 가정하고 진행 \n\n# 회귀진단 그래프들\n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(res_lm) #회귀진단 플랏이 나옴 / 총6개임 \n\n\n\nlayout(1) #다시 한개의 플랏만 그리도록 \n\n# 1. 표준화잔차의 정규확률분포\n\nplot(res_lm,2) #QQ-plot y=x 기울기의 직선위에 점들이 있어야 한다 눈대중으로 \n\n\n\n# 2. 표준화잔차 대 각 예측변수들의 산점도\n\nplot(res_lm,3) #이런경우에는 데이터를 추가한다 좌측하단이 없어서 \n\n\n\n#랜덤하게 데이터가 흩어져 있어야 한다\n\n# 내적 표준화잔차\n\nplot(data_3.3$X1,rstandard(res_lm))\n\n\n\nplot(data_3.3$X3,rstandard(res_lm)) #각각의 잔차들이 랜덤하게 잘 퍼져야함 \n\n\n\n# 3. 표준화잔차 대 적합값의 플롯\n\nplot(res_lm,1) #잔차와 적합값은 상관성이 없어야하며 랜덤하게 퍼져야함 \n\n\n\n# 4. 표준화잔차의 인덱스 플롯 \n\nplot(res_lm,5)\n\n\n\ndata_2.4&lt;-read.table(\"All_Data/p029b.txt\",header=T,sep=\"\\t\")\n\nm&lt;-matrix(1:4,ncol=2,byrow=TRUE)  \n\nlayout(m)\n\nplot(Y1~X1, data=data_2.4,pch=19); abline(3,0.5)\n\nplot(Y2~X2, data=data_2.4,pch=19); abline(3,0.5)\n\nplot(Y3~X3, data=data_2.4,pch=19); abline(3,0.5)\n\nplot(Y4~X4, data=data_2.4,pch=19); abline(3,0.5)\n\n\n\nlayout(1)\n\n\n\n\n\nres_lm&lt;-lm(Y1~X1,data=data_2.4) \n\n#1번플랏은 적당히 잘퍼짐,2번플랏은 어느정도 선형성 있음(데이터적어서그런거임)\n\nres_lm&lt;-lm(Y2~X2,data=data_2.4)\n\nres_lm&lt;-lm(Y3~X3,data=data_2.4)\n\nres_lm&lt;-lm(Y4~X4,data=data_2.4)\n\nres_lm\n\n\nCall:\nlm(formula = Y4 ~ X4, data = data_2.4)\n\nCoefficients:\n(Intercept)           X4  \n     3.0017       0.4999  \n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(res_lm) \n\nWarning: not plotting observations with leverage one:\n  8\n\n\n\n\nlayout(1)\n\n\n\n\n\n# 사례: 뉴욕 강 데이터\n\n# agr-농업, forest-숲, rsdntial-주거, comlndl-산업, nitrogen-질소\n\ndata_1.9&lt;-read.table(\"All_Data/p010.txt\",header=T,sep=\"\\t\")\n\nhead(data_1.9)\n\n       River Agr Forest Rsdntial ComIndl Nitrogen\n1      Olean  26     63      1.2    0.29     1.10\n2  Cassadaga  29     57      0.7    0.09     1.01\n3      Oatka  54     26      1.8    0.58     1.90\n4  Neversink   2     84      1.9    1.98     1.00\n5 Hackensack   3     27     29.4    3.11     1.99\n6  Wappinger  19     61      3.4    0.56     1.42\n\nplot(data_1.9[-1],pch=19) #river라는 첫번째 컬럼을 제외하고 진행 \n\n\n\nres_1&lt;-lm(Nitrogen~.,data=data_1.9[-1]) #모든데이터 사용\n\nres_2&lt;-lm(Nitrogen~.,data=data_1.9[-4,-1]) #4번쨰 데이터 제외\n\nres_3&lt;-lm(Nitrogen~.,data=data_1.9[-5,-1]) #5번쨰 데이터 제외 \n\n#회귀계수\n\ndata.frame(all=coef(res_1),\n\n           rm4=coef(res_2),\n\n           rm5=coef(res_3))\n\n                     all          rm4          rm5\n(Intercept)  1.722213529  1.099471134  1.626014115\nAgr          0.005809126  0.010136685  0.002352222\nForest      -0.012967887 -0.007589231 -0.012760349\nRsdntial    -0.007226768 -0.123792917  0.181160986\nComIndl      0.305027765  1.528956204  0.075617570\n\n#p-value\n\ndata.frame(all=coef(summary(res_1))[,4],\n\n           rm4=coef(summary(res_2))[,4],\n\n           rm5=coef(summary(res_3))[,4])\n\n                   all          rm4        rm5\n(Intercept) 0.18316946 0.2477883387 0.05619948\nAgr         0.70462624 0.3717054741 0.80880737\nForest      0.36667966 0.4700975391 0.16975563\nRsdntial    0.83372002 0.0071342930 0.00112280\nComIndl     0.08230952 0.0005512227 0.51774981\n\n#4,5번째 데이터를 각각 뺴고 진행을 해보니 영향을 끼치는 값임을 알수있고\n\n#5번째는 주거 관련해서는 부호를 바꿀 정도로 강력하다 \n\n# 단순선형회구모형\n\nres&lt;-lm(Nitrogen~ComIndl,data=data_1.9)\n\n\n\n\n\nplot(Nitrogen~ComIndl,data=data_1.9,pch=19)\n\nabline(res)\n\n#leverage values 지레값\n\np_ii&lt;-hatvalues(res)\n\nhiegh_leverage&lt;-ifelse(p_ii&gt;2*2/30,data_1.9$River,\"\")\n\nhiegh_leverage #높은 지레값을 가지고 있는 강의 이름을 표시(이는 보기에 편하기 위해서함)\n\n           1            2            3            4            5            6 \n          \"\"           \"\"           \"\"  \"Neversink\" \"Hackensack\"           \"\" \n           7            8            9           10           11           12 \n          \"\"           \"\"           \"\"           \"\"           \"\"           \"\" \n          13           14           15           16           17           18 \n          \"\"           \"\"           \"\"           \"\"           \"\"           \"\" \n          19           20 \n          \"\"           \"\" \n\ntext(data_1.9$ComIndl,data_1.9$Nitrogen-0.1,hiegh_leverage)\n\n\n\n\n\n\n\n\nplot(rstandard(res),pch=19) #2또는 3시그마를 넘으면 특이값이라 함 \n\n\n\n\n\n\n\n\nplot(p_ii,pch=19) #평균의 2배를 기준으로 비교함 \n\nabline(h=2*2/30,col=\"red\") #이보다 높은것이 지레값이 높은것 높은 영향력을 가진것 \n\n\n\n# 단순선형회귀모형\n\nres&lt;-lm(Nitrogen~ComIndl,data=data_1.9)\n\nplot(Nitrogen~ComIndl,data=data_1.9,pch=19)\n\nabline(res)\n\n\n\nres&lt;-lm(Nitrogen~ComIndl,data=data_1.9[-4,-1])\n\nplot(Nitrogen~ComIndl,data=data_1.9[-4,-1],pch=19)\n\nabline(res) #4번째 제외\n\n\n\nres&lt;-lm(Nitrogen~ComIndl,data=data_1.9[-5,-1])\n\nplot(Nitrogen~ComIndl,data=data_1.9[-5,-1],pch=19)\n\nabline(res) #5번쨰 제외\n\n\n\n#이처럼 4,5번을 빼고 진행을 하면 조금더 잘 나타냄\n\nres&lt;-lm(Nitrogen~ComIndl,data=data_1.9[c(-4,-5),-1])\n\nplot(Nitrogen~ComIndl,data=data_1.9[-5,-1],pch=19)\n\nabline(res) #4,5번째 제외 \n\n\n\n\n\n\n\n\n\n\n\nres&lt;-lm(Nitrogen~ComIndl,data=data_1.9)\n\nplot(res,4)\n\n#install.packages(\"olsrr\")\n\nlibrary(olsrr)\n\n\nAttaching package: 'olsrr'\n\n\nThe following object is masked from 'package:datasets':\n\n    rivers\n\n\n\n\nols_plot_cooksd_chart(res)\n\n\n\n\n\n\n\n\nolsrr::ols_plot_dffits(res)\n\n\n\n\n\n\n\n\nolsrr::ols_plot_hadi(res)\n\n\n\n# Residual & Leverage & Cook's distance\n\nplot(res,5) #영향력 관측치를 보기 위한 플랏 \n\n\n\n\n\n\n\n\nolsrr::ols_plot_resid_pot(res) #2.0에 있는 값외에도 x축 0.2이상의 것들도 특이값으로 \n\n\n\n#지레값이 커도 영향력이 없는 애들은 신경 안써도 되나 영향력이 큰애들을 탐색해 보아야 함\n\n\n\n\n\n#특이값이 큰경우 그것이 TRUE데이터면 오류가 있는 경우 수정을 하거나 가중치를 변화를\n\n#하거나 데이터를 수정을 시켜주거나 다시 실험을 하는 등 여러가지 방법을 사용하여.. \n\n## 첨가변수플롯(added-variable plot), 편회귀플롯(partial regression plot)\n\nres&lt;-lm(Nitrogen~ComIndl,data=data_1.9)\n\ncar::avPlots(res,pch=19)\n\n\n\nres&lt;-lm(Nitrogen~.,data=data_1.9[-1])\n\ncar::avPlots(res,pch=19)\n\n\n\n#beta별로 각각의 어떤 변수가 영향력을 많이 주는지 알게하는 함수 \n\n\n\n\n\ndata_4.5&lt;-read.table(\"All_Data/p120.txt\",header=T,sep=\"\\t\")\n\ndim(data_4.5)\n\n[1] 35  4\n\nnames(data_4.5)\n\n[1] \"Hill.Race\" \"Time\"      \"Distance\"  \"Climb\"    \n\nhead(data_4.5)\n\n                    Hill.Race Time Distance Climb\n1 Greenmantle New Year Dash    965      2.5   650\n2                  Carnethy   2901      6.0  2500\n3              Craig Dunain   2019      6.0   900\n4                   Ben Rha   2736      7.5   800\n5                Ben Lomond   3736      8.0  3070\n6                  Goatfell   4393      8.0  2866\n\n# 회전도표 \n\nlibrary(rgl)\n\nwith(data_4.5,plot3d(x=Distance,y=Climb,z=Time))\n\nres&lt;-lm(Time~Distance+Climb,data=data_4.5)\n\nsummary(res) #beta_0가 마이너스여도 신경안씀 관심있는 것은 회귀계수임 \n\n\nCall:\nlm(formula = Time ~ Distance + Climb, data = data_4.5)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-973.0 -427.7  -71.2  142.2 3907.3 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -539.4829   258.1607  -2.090   0.0447 *  \nDistance     373.0727    36.0684  10.343 9.86e-12 ***\nClimb          0.6629     0.1231   5.387 6.44e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 880.5 on 32 degrees of freedom\nMultiple R-squared:  0.9191,    Adjusted R-squared:  0.914 \nF-statistic: 181.7 on 2 and 32 DF,  p-value: &lt; 2.2e-16\n\n#Time=-539.4829+373.0727*Distance+0.6629*Climb \n\n\n### 첨가변수플롯(added-variable plot), 편회귀플롯(partial regression plot)\n\ncar::avPlots(res,pch=19)\n\n\n\n### 성분잔차플롯(component plus residual plots), 편자차플롯(partial residual plot)\n\ncar::crPlots(res,id=T,pch=19) #첨가변수보다 성분잔차를 더 많이 사용 / 비선형여부를 확인\n\n\n\n# 점선에 비해서 분홍선이 크게 떨어져 있지 않아 선형적인 추세를 가지고 있다고 추정이 가능 \n\n### 잠재성-잔차플롯\n\nolsrr::ols_plot_resid_pot(res)\n\n\n\n### Hadi의 영향력 측도\n\nolsrr::ols_plot_hadi(res)\n\n\n\n### Cook의 거리\n\nolsrr::ols_plot_cooksd_chart(res) #전체적은 플롯을 보면서 이상치에 대한 확인을 함 \n\n\n\n# 이러한 값들의 제외 여부는 연구자가 선택해서 진행을 함 \n\n# outlier에 대한 처리를 어떻게 했다고 말을 해야함 \n\n# 보고서는 옆에 사람이 보고 쉽게 따라할 수 있을 정도로 \n\n# 어떤 속성으로 어떻게 진행을 했다라는 것을 중시 코드 보단 결과를 보여줘라 \n\n\n\n\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:olsrr':\n\n    cement\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nres&lt;-lm(Time~Distance+Climb,data=data_4.5)\n\nres\n\n\nCall:\nlm(formula = Time ~ Distance + Climb, data = data_4.5)\n\nCoefficients:\n(Intercept)     Distance        Climb  \n  -539.4829     373.0727       0.6629  \n\nres_rlm&lt;-MASS::rlm(Time~Distance+Climb,data=data_4.5)\n\nres_rlm\n\nCall:\nrlm(formula = Time ~ Distance + Climb, data = data_4.5)\nConverged in 10 iterations\n\nCoefficients:\n (Intercept)     Distance        Climb \n-576.3836570  393.0374614    0.4977894 \n\nDegrees of freedom: 35 total; 32 residual\nScale estimate: 313 \n\nsummary(res)\n\n\nCall:\nlm(formula = Time ~ Distance + Climb, data = data_4.5)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-973.0 -427.7  -71.2  142.2 3907.3 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -539.4829   258.1607  -2.090   0.0447 *  \nDistance     373.0727    36.0684  10.343 9.86e-12 ***\nClimb          0.6629     0.1231   5.387 6.44e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 880.5 on 32 degrees of freedom\nMultiple R-squared:  0.9191,    Adjusted R-squared:  0.914 \nF-statistic: 181.7 on 2 and 32 DF,  p-value: &lt; 2.2e-16\n\nsummary(res_rlm)\n\n\nCall: rlm(formula = Time ~ Distance + Climb, data = data_4.5)\nResiduals:\n     Min       1Q   Median       3Q      Max \n-645.074 -197.082   -2.035  212.266 3942.045 \n\nCoefficients:\n            Value     Std. Error t value  \n(Intercept) -576.3837  105.2774    -5.4749\nDistance     393.0375   14.7086    26.7216\nClimb          0.4978    0.0502     9.9200\n\nResidual standard error: 312.6 on 32 degrees of freedom\n\n#install.packages(\"robustbase\")\n\nlibrary(robustbase)\n\nres_lmrob&lt;-lmrob(Time~Distance+Climb,data=data_4.5)\n\nres_lmrob\n\n\nCall:\nlmrob(formula = Time ~ Distance + Climb, data = data_4.5)\n \\--&gt; method = \"MM\"\nCoefficients:\n(Intercept)     Distance        Climb  \n  -487.3793     398.2784       0.3901  \n\nsummary(res_lmrob)\n\n\nCall:\nlmrob(formula = Time ~ Distance + Climb, data = data_4.5)\n \\--&gt; method = \"MM\"\nResiduals:\n    Min      1Q  Median      3Q     Max \n-650.01 -160.60   23.37  216.11 3875.00 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -487.37929   86.33384  -5.645 3.04e-06 ***\nDistance     398.27843    5.98522  66.544  &lt; 2e-16 ***\nClimb          0.39013    0.04165   9.368 1.09e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nRobust residual standard error: 290.8 \nMultiple R-squared:  0.9868,    Adjusted R-squared:  0.986 \nConvergence in 9 IRWLS iterations\n\nRobustness weights: \n 3 observations c(7,18,33) are outliers with |weight| = 0 ( &lt; 0.0029); \n 2 weights are ~= 1. The remaining 30 ones are summarized as\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.5965  0.9098  0.9623  0.9214  0.9875  0.9990 \nAlgorithmic parameters: \n       tuning.chi                bb        tuning.psi        refine.tol \n        1.548e+00         5.000e-01         4.685e+00         1.000e-07 \n          rel.tol         scale.tol         solve.tol          zero.tol \n        1.000e-07         1.000e-10         1.000e-07         1.000e-10 \n      eps.outlier             eps.x warn.limit.reject warn.limit.meanrw \n        2.857e-03         1.364e-08         5.000e-01         5.000e-01 \n     nResample         max.it       best.r.s       k.fast.s          k.max \n           500             50              2              1            200 \n   maxit.scale      trace.lev            mts     compute.rd fast.s.large.n \n           200              0           1000              0           2000 \n                  psi           subsampling                   cov \n           \"bisquare\"         \"nonsingular\"         \".vcov.avar1\" \ncompute.outlier.stats \n                 \"SM\" \nseed : int(0) \n\n#standard error가 res &gt; res_rlm &gt; res_lmrob 순으로 되어 있음 \n\n# 4장 연습문제 해보기\n\n\n\n\n\n\nlibrary(dplyr)\n\n\n\n\n\n#install.packages(\"fastDummies\")\n\nlibrary(fastDummies)\n\nThank you for using fastDummies!\n\n\nTo acknowledge our work, please cite the package:\n\n\nKaplan, J. & Schlegel, B. (2023). fastDummies: Fast Creation of Dummy (Binary) Columns and Rows from Categorical Variables. Version 1.7.1. URL: https://github.com/jacobkap/fastDummies, https://jacobkap.github.io/fastDummies/.\n\n\n\n\n\n\ndata_5.1&lt;-read.table(\"All_Data/p130.txt\",header=T,sep=\"\\t\")\n\n# S:급료 X:경력 E:교육수준 M:관리(형태) / E,M은 범주형 변수\n\nnames(data_5.1)\n\n[1] \"S\" \"X\" \"E\" \"M\"\n\n# 범주형 질적 변수를 수치형으로 변형시켜서 예측하는데 사용한것이 질적 예측변수이다 \n\n# E_1,E_2,E_3이런식으로 나누어서 0,1로 분류를 한다 (이것이 가변수)\n\n# 더미변수를 만들 경우에는 역행렬의 조건에 의해서 -1개의 변수만 만들면 된다 \n\n# 이는 공산성의 문제또한 있기에 이를 위해서 -1를 한것임 \n\n### 자료형 변경 : 정수 -&gt; 범주\n\ndata_5.1$E&lt;-as.factor(data_5.1$E)\n\ndata_5.1$M&lt;-as.factor(data_5.1$M)\n\nhead(data_5.1)\n\n      S X E M\n1 13876 1 1 1\n2 11608 1 3 0\n3 18701 1 3 1\n4 11283 1 2 0\n5 11767 1 3 0\n6 20872 2 2 1\n\ndata_5.1$E #Levels: 1 2 3이라는 것이 생김 문자로 처리한다는 의미 \n\n [1] 1 3 3 2 3 2 2 1 3 2 1 2 3 1 3 3 2 2 3 1 1 3 2 2 1 2 1 3 1 1 2 3 2 2 1 2 3 1\n[39] 2 2 3 2 2 1 2 1\nLevels: 1 2 3\n\n### 가변수 생성\n\ndata_5.1$E&lt;-factor(as.character(data_5.1$E),levels = c(\"3\",\"1\",\"2\")) \n\n#3번을 베이스 카테고리로 쓰기위한 설정 / 설정을 안하면 베이스는 e_1이 된다\n\ndata_5.1$M&lt;-factor(as.character(data_5.1$M),levels = c(\"0\",\"1\")) \n\ndata_dummy&lt;-dummy_cols(data_5.1,\n\n                       select_columns = c(\"E\",\"M\"),\n\n                       remove_first_dummy = T,\n\n                       remove_selected_columns = T) #첫번째 생성되는 더미 변수를 제거\n\ndata_dummy #가변수의 더미는 n-1개를 하는 것이 역행렬을 위한 것이기에 지워준다 \n\n       S  X E_1 E_2 M_1\n1  13876  1   1   0   1\n2  11608  1   0   0   0\n3  18701  1   0   0   1\n4  11283  1   0   1   0\n5  11767  1   0   0   0\n6  20872  2   0   1   1\n7  11772  2   0   1   0\n8  10535  2   1   0   0\n9  12195  2   0   0   0\n10 12313  3   0   1   0\n11 14975  3   1   0   1\n12 21371  3   0   1   1\n13 19800  3   0   0   1\n14 11417  4   1   0   0\n15 20263  4   0   0   1\n16 13231  4   0   0   0\n17 12884  4   0   1   0\n18 13245  5   0   1   0\n19 13677  5   0   0   0\n20 15965  5   1   0   1\n21 12336  6   1   0   0\n22 21352  6   0   0   1\n23 13839  6   0   1   0\n24 22884  6   0   1   1\n25 16978  7   1   0   1\n26 14803  8   0   1   0\n27 17404  8   1   0   1\n28 22184  8   0   0   1\n29 13548  8   1   0   0\n30 14467 10   1   0   0\n31 15942 10   0   1   0\n32 23174 10   0   0   1\n33 23780 10   0   1   1\n34 25410 11   0   1   1\n35 14861 11   1   0   0\n36 16882 12   0   1   0\n37 24170 12   0   0   1\n38 15990 13   1   0   0\n39 26330 13   0   1   1\n40 17949 14   0   1   0\n41 25685 15   0   0   1\n42 27837 16   0   1   1\n43 18838 16   0   1   0\n44 17483 16   1   0   0\n45 19207 17   0   1   0\n46 19346 20   1   0   0\n\n# 더미를 만든 이후 더미의 모체 변수인 E M을 지워주어야 한다 \n\n### 회귀분석(1) - 가변수\n\nres&lt;-lm(S~.,data = data_dummy)\n\nres \n\n\nCall:\nlm(formula = S ~ ., data = data_dummy)\n\nCoefficients:\n(Intercept)            X          E_1          E_2          M_1  \n    11031.8        546.2      -2996.2        147.8       6883.5  \n\n### 회귀분석(2) - lm()\n\nres_1&lt;-lm(S~.,data=data_5.1)\n\nres_1 \n\n\nCall:\nlm(formula = S ~ ., data = data_5.1)\n\nCoefficients:\n(Intercept)            X           E1           E2           M1  \n    11031.8        546.2      -2996.2        147.8       6883.5  \n\n#더미변수를 많이 쓰기에 factor로 바꾸어주고 분석하면 알아서 더미변수를 만들어서 진행함\n\nsummary(res_1)\n\n\nCall:\nlm(formula = S ~ ., data = data_5.1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1884.60  -653.60    22.23   844.85  1716.47 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11031.81     383.22  28.787  &lt; 2e-16 ***\nX             546.18      30.52  17.896  &lt; 2e-16 ***\nE1          -2996.21     411.75  -7.277 6.72e-09 ***\nE2            147.82     387.66   0.381    0.705    \nM1           6883.53     313.92  21.928  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1027 on 41 degrees of freedom\nMultiple R-squared:  0.9568,    Adjusted R-squared:  0.9525 \nF-statistic: 226.8 on 4 and 41 DF,  p-value: &lt; 2.2e-16\n\n#E_3와 M_0는 Intercept에 포함이 되어있음 그렇기에 베이스 카테고리라고 한다 \n\n#E가 3이고 M이 0이면 대학원이상 일반관리 직급 -&gt; Intercept(Beta_0) + X*Beta_1 = Y\n\n### 표준화잔차 대 경력연수\n\nplot(data_5.1$X, rstandard(res_1),pch=19,xlab=\"범주\",ylab=\"잔차\")\n\n\n\n# 0을 중심으로 잘 퍼져있는가를 확인해야함 \n\n#### 표준화잔차 대 교육수준 - 관리 조합\n\nEM&lt;-paste0(data_5.1$E,data_5.1$M)\n\nplot(EM,rstandard(res_1),pch=19,xlabl=\"범주\",ylab=\"잔차\")\n\nWarning in plot.window(...): \"xlabl\" is not a graphical parameter\n\n\nWarning in plot.xy(xy, type, ...): \"xlabl\" is not a graphical parameter\n\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"xlabl\" is not a\ngraphical parameter\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"xlabl\" is not a\ngraphical parameter\n\n\nWarning in box(...): \"xlabl\" is not a graphical parameter\n\n\nWarning in title(...): \"xlabl\" is not a graphical parameter\n\n\n\n\n### 상호작용 효과(Interaction Effect)\n\nres&lt;-lm(S~X+E+M+E*M,data=data_5.1)\n\nres\n\n\nCall:\nlm(formula = S ~ X + E + M + E * M, data = data_5.1)\n\nCoefficients:\n(Intercept)            X           E1           E2           M1        E1:M1  \n    11203.4        497.0      -1730.7       -349.1       7047.4      -3066.0  \n      E2:M1  \n     1836.5  \n\nsummary(res)\n\n\nCall:\nlm(formula = S ~ X + E + M + E * M, data = data_5.1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-928.13  -46.21   24.33   65.88  204.89 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11203.434     79.065 141.698  &lt; 2e-16 ***\nX             496.987      5.566  89.283  &lt; 2e-16 ***\nE1          -1730.748    105.334 -16.431  &lt; 2e-16 ***\nE2           -349.078     97.568  -3.578 0.000945 ***\nM1           7047.412    102.589  68.695  &lt; 2e-16 ***\nE1:M1       -3066.035    149.330 -20.532  &lt; 2e-16 ***\nE2:M1        1836.488    131.167  14.001  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 173.8 on 39 degrees of freedom\nMultiple R-squared:  0.9988,    Adjusted R-squared:  0.9986 \nF-statistic:  5517 on 6 and 39 DF,  p-value: &lt; 2.2e-16\n\n### 표준화잔차 대 경력연수\n\nplot(data_5.1$X,rstandard(res),pch=19,xlab=\"범주\",ylab=\"잔차\")\n\n\n\n### Cook의 거리\n\nplot(res,4) #33번째 데이터만 제외하고 다시 회귀모형을 생성예정\n\n\n\n### 상호작용 효과 - 관측개체 33 제외 \n\ndata_use&lt;-data_5.1[-33,]\n\nres&lt;-lm(S~X+E+M+E*M,data=data_use)\n\nres\n\n\nCall:\nlm(formula = S ~ X + E + M + E * M, data = data_use)\n\nCoefficients:\n(Intercept)            X           E1           E2           M1        E1:M1  \n    11199.7        498.4      -1741.3       -357.0       7040.6      -3051.8  \n      E2:M1  \n     1997.5  \n\nsummary(res)\n\n\nCall:\nlm(formula = S ~ X + E + M + E * M, data = data_use)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-112.884  -43.636   -5.036   46.622  128.480 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11199.714     30.533 366.802  &lt; 2e-16 ***\nX             498.418      2.152 231.640  &lt; 2e-16 ***\nE1          -1741.336     40.683 -42.803  &lt; 2e-16 ***\nE2           -357.042     37.681  -9.475 1.49e-11 ***\nM1           7040.580     39.619 177.707  &lt; 2e-16 ***\nE1:M1       -3051.763     57.674 -52.914  &lt; 2e-16 ***\nE2:M1        1997.531     51.785  38.574  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 67.12 on 38 degrees of freedom\nMultiple R-squared:  0.9998,    Adjusted R-squared:  0.9998 \nF-statistic: 3.543e+04 on 6 and 38 DF,  p-value: &lt; 2.2e-16\n\n### 표준화잔차 대 경력연수\n\nplot(data_use$X,rstandard(res),pch=19,xlab=\"범주\",ylab=\"잔차\")\n\n\n\n#### 표준화잔차 대 교육수준 - 관리 조합\n\nEM&lt;-paste0(data_use$E,data_use$M)\n\nplot(EM,rstandard(res),pch=19,xlabl=\"범주\",ylab=\"잔차\")\n\nWarning in plot.window(...): \"xlabl\" is not a graphical parameter\n\n\nWarning in plot.xy(xy, type, ...): \"xlabl\" is not a graphical parameter\n\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"xlabl\" is not a\ngraphical parameter\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"xlabl\" is not a\ngraphical parameter\n\n\nWarning in box(...): \"xlabl\" is not a graphical parameter\n\n\nWarning in title(...): \"xlabl\" is not a graphical parameter\n\n\n\n\n#상호 호과가 들어간 이 모형이 더 괜찮은 모양이라고 판단이 된다 \n\n### 기본 급료 추정 - 표 5.6\n\nres&lt;-lm(S~X+E+M+E*M,data=data_use)\n\ndf_new&lt;-data.frame(X=rep(0,6),\n\n                   E=rep(1:3,c(2,2,2)),\n\n                   M=rep(c(0,1),3))\n\n### 가변수 생성 - 분석용\n\ndf_new$E&lt;-factor(as.character(df_new$E),levels = c(\"3\",\"1\",\"2\")) \n\ndf_new$M&lt;-factor(as.character(df_new$M),levels = c(\"0\",\"1\")) \n\ncbind(df_new,predict = predict(res,df_new,interval = \"confidence\"))\n\n  X E M predict.fit predict.lwr predict.upr\n1 0 1 0    9458.378    9395.539    9521.216\n2 0 1 1   13447.195   13382.933   13511.456\n3 0 2 0   10842.672   10789.719   10895.624\n4 0 2 1   19880.782   19814.090   19947.474\n5 0 3 0   11199.714   11137.902   11261.525\n6 0 3 1   18240.294   18182.503   18298.084\n\n# 이를 보고 각 레벨에 따른 차이를 보고 얼마나 나는 지 분석이 가능해야 한다 \n\n# ex) 고졸과 대학원졸의 관리자 직급의 급여의 차이는?(평균적으로)\n\n\n\n\n\n\n\ndata_5.7&lt;-read.table(\"All_Data/p140.txt\",header=T,sep=\"\\t\")\n\nhead(data_5.7)\n\n  TEST RACE JPERF\n1 0.28    1  1.83\n2 0.97    1  4.59\n3 1.25    1  2.97\n4 2.46    1  8.14\n5 2.51    1  8.00\n6 1.17    1  3.30\n\n# 모형 1 - 통합모형 인종간 차이가 없을때\n\nmodel_1&lt;-lm(JPERF~TEST,data=data_5.7)\n\nsummary(model_1) \n\n\nCall:\nlm(formula = JPERF ~ TEST, data = data_5.7)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3558 -0.8798 -0.1897  1.2735  2.3312 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.0350     0.8680   1.192 0.248617    \nTEST          2.3605     0.5381   4.387 0.000356 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.591 on 18 degrees of freedom\nMultiple R-squared:  0.5167,    Adjusted R-squared:  0.4899 \nF-statistic: 19.25 on 1 and 18 DF,  p-value: 0.0003555\n\n# 모형 3 \n\nmodel_3&lt;-lm(JPERF~TEST+RACE+TEST*RACE,data=data_5.7)\n\nsummary(model_3)\n\n\nCall:\nlm(formula = JPERF ~ TEST + RACE + TEST * RACE, data = data_5.7)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0734 -1.0594 -0.2548  1.2830  2.1980 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   2.0103     1.0501   1.914   0.0736 .\nTEST          1.3134     0.6704   1.959   0.0677 .\nRACE         -1.9132     1.5403  -1.242   0.2321  \nTEST:RACE     1.9975     0.9544   2.093   0.0527 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.407 on 16 degrees of freedom\nMultiple R-squared:  0.6643,    Adjusted R-squared:  0.6013 \nF-statistic: 10.55 on 3 and 16 DF,  p-value: 0.0004511\n\n# 인종적인 차이가 있는지 없는지를 확인해야 한다 어떤 모형을 사용할지 \n\n### H_0:gamma=delta=0\n\nanova(model_1,model_3) #model_3가 FM(완전모형)\n\nAnalysis of Variance Table\n\nModel 1: JPERF ~ TEST\nModel 2: JPERF ~ TEST + RACE + TEST * RACE\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     18 45.568                              \n2     16 31.655  2    13.913 3.5161 0.05424 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#P-value(0.05424)&lt;0.05이기에 H_0 이기에 모형1을 선택하는 것이 옳다고 판단(그러나 확신x)\n\n#서로의 R-squared롤 보면 model_3가 더 좋음 / ANOVA는 참고용 절대적이지는 않다 \n\n\n\n\n\nplot(data_5.7$TEST,rstandard(model_1),\n\n     pch=19,xlab=\"검사점수\",ylab=\"잔차\",\n\n     main=\"그림 5.7 - 표준화잔차 대 검사점수 : 모형 1\")\n\n\n\n\n\n\n\n\nplot(data_5.7$TEST,rstandard(model_3),\n\n     pch=19,xlab=\"검사점수\",ylab=\"잔차\",\n\n     main=\"그림 5.8 - 표준화잔차 대 검사점수 : 모형 3\")\n\n\n\n# 통계에서 나오는 결과는 결정의 보조수단이지 절대적이지 않아 \n\n# 3번 모형을 선택한다고 결정한다고 진행 \n\nsummary(model_3) # Multiple R-squared:  0.6643\n\n\nCall:\nlm(formula = JPERF ~ TEST + RACE + TEST * RACE, data = data_5.7)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0734 -1.0594 -0.2548  1.2830  2.1980 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   2.0103     1.0501   1.914   0.0736 .\nTEST          1.3134     0.6704   1.959   0.0677 .\nRACE         -1.9132     1.5403  -1.242   0.2321  \nTEST:RACE     1.9975     0.9544   2.093   0.0527 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.407 on 16 degrees of freedom\nMultiple R-squared:  0.6643,    Adjusted R-squared:  0.6013 \nF-statistic: 10.55 on 3 and 16 DF,  p-value: 0.0004511\n\n\n\n\n\n\nplot(data_5.7$RACE,rstandard(model_1),\n\n     pch=19,xlab=\"검사점수\",ylab=\"잔차\",\n\n     main=\"그림 5.9 - 표준화잔차 대 검사점수 : 모형 1\")\n\n\n\n# 분리된 회귀분석 결과\n\ndata_5.7_R1&lt;-subset(data_5.7,RACE==1)\n\nmodel_R1&lt;-lm(JPERF~TEST,data=data_5.7_R1)\n\nsummary(model_R1) #소수민족\n\n\nCall:\nlm(formula = JPERF ~ TEST, data = data_5.7_R1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0734 -0.6267 -0.2548  1.1624  1.5394 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.09712    1.03519   0.094 0.927564    \nTEST         3.31095    0.62411   5.305 0.000724 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.292 on 8 degrees of freedom\nMultiple R-squared:  0.7787,    Adjusted R-squared:  0.751 \nF-statistic: 28.14 on 1 and 8 DF,  p-value: 0.0007239\n\ndata_5.7_R0&lt;-subset(data_5.7,RACE==0)\n\nmodel_R0&lt;-lm(JPERF~TEST,data=data_5.7_R0)\n\nsummary(model_R0) #백인\n\n\nCall:\nlm(formula = JPERF ~ TEST, data = data_5.7_R0)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8599 -1.0663 -0.3061  1.0957  2.1980 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   2.0103     1.1291   1.780    0.113\nTEST          1.3134     0.7208   1.822    0.106\n\nResidual standard error: 1.512 on 8 degrees of freedom\nMultiple R-squared:  0.2933,    Adjusted R-squared:  0.205 \nF-statistic:  3.32 on 1 and 8 DF,  p-value: 0.1059\n\n# 통합모형에서 나온 각각의 회귀식이 통합모형에서 나온것과 동일함 따라서 인종별로 나누어서\n\n# 진행할 필요없이 통일모형을 사용해서 진행을 하면 된다(이는 데이터셋을 나눈경우와 동일함)\n\n\n\n\n\nplot(data_5.7_R1$TEST,rstandard(model_R1),\n\n     pch=19,xlab=\"검사점수\",ylab=\"잔차\",\n\n     main=\"그림 5.10 - 표준화잔차 대 검사점수 : 모형 1. 소수민족만만\")\n\n\n\n\n\n\n\n\nplot(data_5.7_R0$TEST,rstandard(model_R0),\n\n     pch=19,xlab=\"검사점수\",ylab=\"잔차\",\n\n     main=\"그림 5.11 - 표준화잔차 대 검사점수 : 모형 1. 백인만\")\n\n\n\n# 적절한 합격점수의 결정 - 소수민족\n\n# 고용전 검사점수의 합격점에 대한 95% 신뢰구간\n\nym&lt;-4\n\nxm&lt;-(ym-0.09712)/3.31095\n\ns&lt;-1.292\n\nn&lt;-10\n\nt&lt;-qt(1-0.05/2,8)\n\nc(xm-(t*s/n)/3.31095, xm+(t*s/n)/3.31095) #신뢰구간\n\n[1] 1.088795 1.268764\n\n\n\n\n\n\nmodel_3&lt;-lm(JPERF~TEST+RACE,data=data_5.7)\n\nsummary(model_3)\n\n\nCall:\nlm(formula = JPERF ~ TEST + RACE, data = data_5.7)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7872 -1.0370 -0.2095  0.9198  2.3645 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.6120     0.8870   0.690 0.499578    \nTEST          2.2988     0.5225   4.400 0.000391 ***\nRACE          1.0276     0.6909   1.487 0.155246    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.54 on 17 degrees of freedom\nMultiple R-squared:  0.5724,    Adjusted R-squared:  0.5221 \nF-statistic: 11.38 on 2 and 17 DF,  p-value: 0.0007312\n\n#Intercept = BETA_0 / TEST = BETA_1 / RACE = gamma\n\n## H_0: gamma=0\n\nanova(model_1,model_3) #p-value(0.1552)&gt;0.05 이기에 H_0은 참이다//gamma=0\n\nAnalysis of Variance Table\n\nModel 1: JPERF ~ TEST\nModel 2: JPERF ~ TEST + RACE\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     18 45.568                           \n2     17 40.322  1    5.2468 2.2121 0.1552\n\n#기울기가 같고 절편이 다른 모형은 아니라고 데이터가 이야기하고 있다 \n\n# 소수민족(RACE=1): (0.6120+1.0276)+2.2988*TEST = 1.6396+2.2988*TEST\n\n# 백인(RACE=0): (0.6120)+2.2988*TEST\n\n\n\n\n\nmodel_3&lt;-lm(JPERF~TEST+I(TEST*RACE),data=data_5.7)\n\nsummary(model_3) #I()를 하면 교호작용하는 것만 보이게 하려고 없으면 RACE항이 자동추가됨\n\n\nCall:\nlm(formula = JPERF ~ TEST + I(TEST * RACE), data = data_5.7)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.41100 -0.88871 -0.03359  0.97720  2.44440 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)      1.1211     0.7804   1.437  0.16900   \nTEST             1.8276     0.5356   3.412  0.00332 **\nI(TEST * RACE)   0.9161     0.3972   2.306  0.03395 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.429 on 17 degrees of freedom\nMultiple R-squared:  0.6319,    Adjusted R-squared:  0.5886 \nF-statistic: 14.59 on 2 and 17 DF,  p-value: 0.0002045\n\n## H_0: delta=0\n\nanova(model_1,model_3) #p-value(0.03395)&lt;0.05보다 작기에 delta항은 필요한 변수라는 사실 \n\nAnalysis of Variance Table\n\nModel 1: JPERF ~ TEST\nModel 2: JPERF ~ TEST + I(TEST * RACE)\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     18 45.568                              \n2     17 34.708  1    10.861 5.3196 0.03395 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#Intercept = BETA_0 / TEST = BETA_1 / I(TEST*RACE) = delta\n\n\n\n\n\n\nlibrary(dplyr)\n\n\n\n\ndata_6.2&lt;-read.table(\"All_Data/p168.txt\",header=T,sep=\"\\t\")\n\nhead(data_6.2)\n\n  t N_t\n1 1 355\n2 2 211\n3 3 197\n4 4 166\n5 5 142\n6 6 106\n\n\n\n\n\n\nplot(N_t~t,data=data_6.2,pch=19)\n\n\n\n\n\n\n\n\nres&lt;-lm(N_t~t,data=data_6.2)\n\nsummary(res)\n\n\nCall:\nlm(formula = N_t ~ t, data = data_6.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-43.867 -23.599  -9.652  10.223 114.883 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   259.58      22.73  11.420 3.78e-08 ***\nt             -19.46       2.50  -7.786 3.01e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 41.83 on 13 degrees of freedom\nMultiple R-squared:  0.8234,    Adjusted R-squared:  0.8098 \nF-statistic: 60.62 on 1 and 13 DF,  p-value: 3.006e-06\n\n\n\n\n\n\nplot(data_6.2$t,rstandard(res),pch=19) # 표준화 잔차의 플랏\n\n\n\n# 잔차가 랜덤하게 잘 퍼져있어야 하는데 적절한 회귀모형이 아니라는 사실이 나옴\n\n\n\n\n\n\n\n\nplot(log(N_t)~t,data=data_6.2,pch=19)\n\n\n\n# 박테리아의 수에 로그를 취하니 선형성이 보인다 \n\nres&lt;-lm(log(N_t)~t,data=data_6.2)\n\nsummary(res)\n\n\nCall:\nlm(formula = log(N_t) ~ t, data = data_6.2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.18445 -0.06189  0.01253  0.05201  0.20021 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.973160   0.059778   99.92  &lt; 2e-16 ***\nt           -0.218425   0.006575  -33.22 5.86e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.11 on 13 degrees of freedom\nMultiple R-squared:  0.9884,    Adjusted R-squared:  0.9875 \nF-statistic:  1104 on 1 and 13 DF,  p-value: 5.86e-14\n\nplot(data_6.2$t,rstandard(res),pch=19)\n\n\n\n# 로그를 취해주니 잔차가 랜덤하게 이루어져 있음 \n\n# n_0에 대한 추론\n\nexp(5.973160)\n\n[1] 392.7448\n\nexp(coef(res)[1]) #로그를 취해주고 하는 부분이 잘 이해가 안됨 \n\n(Intercept) \n   392.7449 \n\nexp(coef(res)[1]-0.0588/2) #불편추정량 구하기 (참고용)\n\n(Intercept) \n   381.3663 \n\n\n\n\n\n\ndata_6.6&lt;-read.table(\"All_Data/p174.txt\",header=T,sep=\"\\t\")\n\ndata_6.6 #운항률과 사고 발생 건수에 대한 자료\n\n   Y      N\n1 11 0.0950\n2  7 0.1920\n3  7 0.0750\n4 19 0.2078\n5  9 0.1382\n6  4 0.0540\n7  3 0.1292\n8  1 0.0503\n9  3 0.0629\n\nplot(Y~N,data=data_6.6,pch=19) #잔차의 분산이 계속 커지는 효과를 보임 \n\n\n\nres_1&lt;-lm(Y~N,data=data_6.6)\n\nsummary(res_1)\n\n\nCall:\nlm(formula = Y ~ N, data = data_6.6)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.3351 -2.1281  0.1605  2.2670  5.6382 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  -0.1402     3.1412  -0.045   0.9657  \nN            64.9755    25.1959   2.579   0.0365 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.201 on 7 degrees of freedom\nMultiple R-squared:  0.4872,    Adjusted R-squared:  0.4139 \nF-statistic:  6.65 on 1 and 7 DF,  p-value: 0.03654\n\n# 표준화잔차 대 N의 플롯 / 그림 6.11\n\nplot(data_6.6$N,rstandard(res_1),pch=19) #등분산성에 만족하지 못하는 모형을 보인다 \n\n\n\n# N에 대한 sqrt(Y)의 회귀\n\n# N에 대한 Y의 회귀\n\nres_2&lt;-lm(sqrt(Y)~N,data=data_6.6)\n\nsummary(res_2)\n\n\nCall:\nlm(formula = sqrt(Y) ~ N, data = data_6.6)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.9690 -0.7655  0.1906  0.5874  1.0211 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   1.1692     0.5783   2.022   0.0829 .\nN            11.8564     4.6382   2.556   0.0378 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7733 on 7 degrees of freedom\nMultiple R-squared:  0.4828,    Adjusted R-squared:  0.4089 \nF-statistic: 6.535 on 1 and 7 DF,  p-value: 0.03776\n\n\n\n\n\n\nplot(data_6.6$N,rstandard(res_2),pch=19) #0을 중심으로 잔차가 보다 잘 퍼져있음이 보임 \n\n\n\n\n\n\n\n\ndata_6.9&lt;-read.table(\"All_Data/p176.txt\",header=T,sep=\"\\t\")\n\ndata_6.9\n\n      X   Y\n1   294  30\n2   247  32\n3   267  37\n4   358  44\n5   423  47\n6   311  49\n7   450  56\n8   534  62\n9   438  68\n10  697  78\n11  688  80\n12  630  84\n13  709  88\n14  627  97\n15  615 100\n16  999 109\n17 1022 114\n18 1015 117\n19  700 106\n20  850 128\n21  980 130\n22 1025 160\n23 1021  97\n24 1200 180\n25 1250 112\n26 1500 210\n27 1650 135\n\n# Y 대 X의 플로\n\nplot(Y~X,data=data_6.9,pch=19,main=\"그림 6.13\")\n\n\n\n# X에 대한 Y의 회귀\n\nres_1&lt;-lm(Y~X,data=data_6.9)\n\nsummary(res_1)\n\n\nCall:\nlm(formula = Y ~ X, data = data_6.9)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-53.294  -9.298  -5.579  14.394  39.119 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 14.44806    9.56201   1.511    0.143    \nX            0.10536    0.01133   9.303 1.35e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.73 on 25 degrees of freedom\nMultiple R-squared:  0.7759,    Adjusted R-squared:  0.7669 \nF-statistic: 86.54 on 1 and 25 DF,  p-value: 1.35e-09\n\n# 표준화잔차 대 X의 플롯\n\nplot(data_6.9$X, rstandard(res_1),pch=19,main = \"그림 6.14\")\n\n\n\n\n\n\n\n\n# 변환된 Y/X와 1/X를 적합한 회귀\n\ndata_6.9_1&lt;-data.frame(Y=data_6.9$Y/data_6.9$X,\n\n                       X=1/data_6.9$X)\n\nres_2&lt;-lm(Y~X,data=data_6.9_1)\n\nsummary(res_2) #지금은 변환하고 프라임 값들의 추정치가 나온것이기에 원래 회귀식으로 돌아가야함 \n\n\nCall:\nlm(formula = Y ~ X, data = data_6.9_1)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.041477 -0.013852 -0.004998  0.024671  0.035427 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.120990   0.008999  13.445 6.04e-13 ***\nX           3.803296   4.569745   0.832    0.413    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02266 on 25 degrees of freedom\nMultiple R-squared:  0.02696,   Adjusted R-squared:  -0.01196 \nF-statistic: 0.6927 on 1 and 25 DF,  p-value: 0.4131\n\n# 본래 변환시 X를 나눴기에 X를 곱해준다 -&gt; B_0,B_1의 추정값이 바뀐다 서로 \n\nplot(data_6.9_1$X, rstandard(res_2),pch=19,\n\n     xlab=\"1/X\",ylab=\"잔차\",main = \"[그림 6.15]\")\n\n\n\n\n\n\n\n\nwt&lt;-1/data_6.9$X^2 #가중값\n\nres_3&lt;-lm(Y~X,data=data_6.9,weights = wt)\n\nsummary(res_3) #결과는 위의 6.6과 동일한 값이 나옴 \n\n\nCall:\nlm(formula = Y ~ X, data = data_6.9, weights = wt)\n\nWeighted Residuals:\n      Min        1Q    Median        3Q       Max \n-0.041477 -0.013852 -0.004998  0.024671  0.035427 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 3.803296   4.569745   0.832    0.413    \nX           0.120990   0.008999  13.445 6.04e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02266 on 25 degrees of freedom\nMultiple R-squared:  0.8785,    Adjusted R-squared:  0.8737 \nF-statistic: 180.8 on 1 and 25 DF,  p-value: 6.044e-13\n\n\n\n\n\n\ndata_6.9&lt;-read.table(\"All_Data/p176.txt\",header=T,sep=\"\\t\")\n\nhead(data_6.9)\n\n    X  Y\n1 294 30\n2 247 32\n3 267 37\n4 358 44\n5 423 47\n6 311 49\n\n# log(Y) 대 X의 산점도\n\nplot((Y)~X,data=data_6.9,pch=19,main=\"식 6.9\")\n\n\n\nplot(log(Y)~X,data=data_6.9,pch=19,main=\"그림 6.16\")\n\n\n\nres_4&lt;-lm(log(Y)~X,data=data_6.9)\n\nsummary(res_4)\n\n\nCall:\nlm(formula = log(Y) ~ X, data = data_6.9)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.59648 -0.16578  0.00244  0.17481  0.34964 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 3.5150232  0.1110670  31.648  &lt; 2e-16 ***\nX           0.0012041  0.0001316   9.153 1.85e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2524 on 25 degrees of freedom\nMultiple R-squared:  0.7702,    Adjusted R-squared:  0.761 \nF-statistic: 83.77 on 1 and 25 DF,  p-value: 1.855e-09\n\n# X에 대한 log(Y)의 회귀로 부터 얻은 표준화잔차플롯\n\nplot(data_6.9$X,rstandard(res_4),pch=19,\n\n     xlab=\"X\",ylab=\"잔차\",main=\"그림 6.17\")\n\n\n\n# X와 X^2에 대한 log(Y)의 회귀\n\ndf&lt;-data.frame(log_Y=log(data_6.9$Y),\n\n               X=data_6.9$X,\n\n               X2=data_6.9$X^2)\n\nres_5&lt;-lm(log_Y~X+X2,data=df) #그러나 이러한 과정은 필요업고 아래방식으로..\n\nres_5&lt;-lm(log(Y)~X+I(X^2),data=data_6.9)\n\nsummary(res_5)\n\n\nCall:\nlm(formula = log(Y) ~ X + I(X^2), data = data_6.9)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.30589 -0.11705 -0.02707  0.17593  0.30657 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.852e+00  1.566e-01  18.205 1.50e-15 ***\nX            3.113e-03  3.989e-04   7.803 4.90e-08 ***\nI(X^2)      -1.102e-06  2.238e-07  -4.925 5.03e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1817 on 24 degrees of freedom\nMultiple R-squared:  0.8857,    Adjusted R-squared:  0.8762 \nF-statistic: 92.98 on 2 and 24 DF,  p-value: 4.976e-12\n\n# 표준화잔차 대 적합값 플롯 : X와 X^2에 대한 log(Y)의 회귀\n\nplot(fitted(res_5),rstandard(res_5),pch=19,\n\n     xlab=\"X\",ylab=\"잔차\",main=\"그림 6.18\")\n\n\n\n# 표준화잔차 대 X의 플롯 : X와 X^2에 대한 log(Y)의 회귀\n\nplot(data_6.9$X,rstandard(res_5),pch=19,\n\n     xlab=\"X\",ylab=\"잔차\",main=\"그림 6.19\")\n\n\n\n# 표준화잔차 대 X^2의 플롯 : X와 X^2에 대한 log(Y)의 회귀\n\nplot(data_6.9$X^2,rstandard(res_5),pch=19,\n\n     xlab=\"X\",ylab=\"잔차\",main=\"그림 6.20\") #2차항을 추가함으로서 더 잘 피팅됨 \n\n\n\n# 7장은 스킵 / 8장 스킵\n\n\n\n\n\ndata_9.1&lt;-read.table(\"All_Data/p236.txt\",header=T,sep=\"\\t\")\n\nhead(data_9.1)\n\n      ACHV      FAM     PEER   SCHOOL\n1 -0.43148  0.60814  0.03509  0.16607\n2  0.79969  0.79369  0.47924  0.53356\n3 -0.92467 -0.82630 -0.61951 -0.78635\n4 -2.19081 -1.25310 -1.21675 -1.04076\n5 -2.84818  0.17399 -0.18517  0.14229\n6 -0.66233  0.20246  0.12764  0.27311\n\nres&lt;-lm(ACHV~.,data_9.1)\n\nsummary(res)\n\n\nCall:\nlm(formula = ACHV ~ ., data = data_9.1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.2096 -1.3934 -0.2947  1.1415  4.5881 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -0.06996    0.25064  -0.279    0.781\nFAM          1.10126    1.41056   0.781    0.438\nPEER         2.32206    1.48129   1.568    0.122\nSCHOOL      -2.28100    2.22045  -1.027    0.308\n\nResidual standard error: 2.07 on 66 degrees of freedom\nMultiple R-squared:  0.2063,    Adjusted R-squared:  0.1702 \nF-statistic: 5.717 on 3 and 66 DF,  p-value: 0.001535\n\n#F-statistic: 5.717 on 3 and 66 DF, p-value: 0.001535\n\n#이는 의미있는 beta가 존재한다라는 의미\n\n#그러나 각 계수들의 회귀계수를 보니 모두 0이라는 결과가 나옴 이는 다중공선성이다\n\n# Correlation panel\n\npanel.cor&lt;-function(x,y){\n\npar(usr=c(0,1,0,1))\n\nr&lt;-round(cor(x,y),digits = 3)\n\ntext(0.5,0.5,r,cex=1.5)\n\n}\n\npairs(data_9.1[-1],lower.panel = panel.cor)\n\n\n\n\n\n\n\n\nplot(fitted(res),rstandard(res),pch=19,\n\nxlab=\"예측값\",ylab=\"잔차\",main=\"[그림 9.1]\")\n\n\n\n#이는 회귀모형에 동시에 들어가면 안된다라는 의미 (제거가 필요)\n\n\n\n\n\ndata_9.5&lt;-read.table(\"All_Data/p241.txt\",header=T,sep=\"\\t\")\n\nhead(data_9.5)\n\n  YEAR IMPORT DOPROD STOCK CONSUM\n1   49   15.9  149.3   4.2  108.1\n2   50   16.4  161.2   4.1  114.8\n3   51   19.0  171.5   3.1  123.2\n4   52   19.1  175.5   3.1  126.9\n5   53   18.8  180.8   1.1  132.1\n6   54   20.4  190.7   2.2  137.7\n\n### 산점도\n\npairs(data_9.5[-1],lower.panel = panel.cor)\n\n\n\n# 회귀분석(1) : 데이터 1949~1966\n\nres&lt;-lm(IMPORT~.,data_9.5[-1])\n\nsummary(res) #다중공선성이 존재한다고 예측을 일단함\n\n\nCall:\nlm(formula = IMPORT ~ ., data = data_9.5[-1])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7208 -1.8354 -0.3479  1.2973  4.1008 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -19.7251     4.1253  -4.782 0.000293 ***\nDOPROD        0.0322     0.1869   0.172 0.865650    \nSTOCK         0.4142     0.3223   1.285 0.219545    \nCONSUM        0.2427     0.2854   0.851 0.409268    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.258 on 14 degrees of freedom\nMultiple R-squared:  0.973, Adjusted R-squared:  0.9673 \nF-statistic: 168.4 on 3 and 14 DF,  p-value: 3.212e-11\n\n\n\n\n\n\nplot(1:nrow(data_9.5),rstandard(res),\n\npch=19,type=\"b\",\n\nxla=\"번호\",ylab=\"잔차\",main=\"[그림 9.3]\")\n\n\n\n# 회귀분석(2) : 데이터 1949~1959\n\ndata_use&lt;-subset(data_9.5,YEAR&lt;=59)\n\nres&lt;-lm(IMPORT~.,data_use[-1])\n\nsummary(res)\n\n\nCall:\nlm(formula = IMPORT ~ ., data = data_use[-1])\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.52367 -0.38953  0.05424  0.22644  0.78313 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -10.12799    1.21216  -8.355  6.9e-05 ***\nDOPROD       -0.05140    0.07028  -0.731 0.488344    \nSTOCK         0.58695    0.09462   6.203 0.000444 ***\nCONSUM        0.28685    0.10221   2.807 0.026277 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4889 on 7 degrees of freedom\nMultiple R-squared:  0.9919,    Adjusted R-squared:  0.9884 \nF-statistic: 285.6 on 3 and 7 DF,  p-value: 1.112e-07\n\n\n\n\n\n\nplot(1:nrow(data_use),rstandard(res),\n\npch=19,type=\"b\",\n\nxla=\"번호\",ylab=\"잔차\",main=\"[그림 9.4]\")\n\n\n\n\n\n\n\n\n# 분산확대인자\n\nlibrary(olsrr)\n\n# 교육기회 균등(EEO)\n\nres_9.1&lt;-lm(ACHV~.,data_9.1)\n\nsummary(res_9.1)\n\n\nCall:\nlm(formula = ACHV ~ ., data = data_9.1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.2096 -1.3934 -0.2947  1.1415  4.5881 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -0.06996    0.25064  -0.279    0.781\nFAM          1.10126    1.41056   0.781    0.438\nPEER         2.32206    1.48129   1.568    0.122\nSCHOOL      -2.28100    2.22045  -1.027    0.308\n\nResidual standard error: 2.07 on 66 degrees of freedom\nMultiple R-squared:  0.2063,    Adjusted R-squared:  0.1702 \nF-statistic: 5.717 on 3 and 66 DF,  p-value: 0.001535\n\n#car::vif(res_9.1)\n\nolsrr::ols_vif_tol(res_9.1) # 10보다 크면 유의한 영향을 준다 제거필요?\n\n  Variables  Tolerance      VIF\n1       FAM 0.02660945 37.58064\n2      PEER 0.03309981 30.21166\n3    SCHOOL 0.01202567 83.15544\n\n\n\n\n\n\ndata_use&lt;-subset(data_9.5,YEAR&lt;=59)\n\nres_9.5&lt;-lm(IMPORT~.,data_use[-1])\n\n#car::vif(res_9.1)\n\nolsrr::ols_vif_tol(res_9.5) #작으면 새로운 변수를 만들거나 제거를 통해서 진행\n\n  Variables   Tolerance        VIF\n1    DOPROD 0.005376417 185.997470\n2     STOCK 0.981441657   1.018909\n3    CONSUM 0.005373166 186.110015\n\n# 상태 지수\n\ncnd.idx&lt;-olsrr::ols_eigen_cindex(res_9.1)\n\nround(cnd.idx,4)\n\n  Eigenvalue Condition Index intercept    FAM   PEER SCHOOL\n1     2.9547          1.0000    0.0005 0.0030 0.0037 0.0014\n2     0.9974          1.7211    0.9756 0.0000 0.0000 0.0000\n3     0.0400          8.5996    0.0004 0.3068 0.4428 0.0008\n4     0.0079         19.2826    0.0235 0.6903 0.5535 0.9978\n\ncnd.idx&lt;-olsrr::ols_eigen_cindex(res_9.5)\n\nround(cnd.idx,4)\n\n  Eigenvalue Condition Index intercept DOPROD  STOCK CONSUM\n1     3.8384          1.0000    0.0010 0.0000 0.0109 0.0000\n2     0.1484          5.0863    0.0053 0.0001 0.9385 0.0001\n3     0.0132         17.0732    0.7743 0.0015 0.0330 0.0011\n4     0.0001        265.4613    0.2193 0.9984 0.0175 0.9989\n\n\n\n\n\n\n\ndata_9.5&lt;-read.table(\"All_Data/p241.txt\",header=T,sep=\"\\t\")\n\nhead(data_9.5)\n\n  YEAR IMPORT DOPROD STOCK CONSUM\n1   49   15.9  149.3   4.2  108.1\n2   50   16.4  161.2   4.1  114.8\n3   51   19.0  171.5   3.1  123.2\n4   52   19.1  175.5   3.1  126.9\n5   53   18.8  180.8   1.1  132.1\n6   54   20.4  190.7   2.2  137.7\n\n# 회귀분석(2) : 데이터 1949~1959\n\ndata_use&lt;-subset(data_9.5,YEAR&lt;=59)\n\nres&lt;-lm(IMPORT~.,data_use[-1]) #1열 제거 \n\nsummary(res)\n\n\nCall:\nlm(formula = IMPORT ~ ., data = data_use[-1])\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.52367 -0.38953  0.05424  0.22644  0.78313 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -10.12799    1.21216  -8.355  6.9e-05 ***\nDOPROD       -0.05140    0.07028  -0.731 0.488344    \nSTOCK         0.58695    0.09462   6.203 0.000444 ***\nCONSUM        0.28685    0.10221   2.807 0.026277 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4889 on 7 degrees of freedom\nMultiple R-squared:  0.9919,    Adjusted R-squared:  0.9884 \nF-statistic: 285.6 on 3 and 7 DF,  p-value: 1.112e-07\n\nhead(data_use[-c(1:2)])\n\n  DOPROD STOCK CONSUM\n1  149.3   4.2  108.1\n2  161.2   4.1  114.8\n3  171.5   3.1  123.2\n4  175.5   3.1  126.9\n5  180.8   1.1  132.1\n6  190.7   2.2  137.7\n\n### 주성분(principle component)\n\npc&lt;-prcomp(data_use[-c(1:2)],scale.=T)\n\npc$rotation\n\n              PC1         PC2          PC3\nDOPROD 0.70633041 -0.03568867 -0.706982083\nSTOCK  0.04350059  0.99902908 -0.006970795\nCONSUM 0.70654444 -0.02583046  0.707197102\n\npc$x #변화된 새로운 변수가 저장된곳\n\n          PC1         PC2         PC3\n1  -2.1258872  0.63865815 -0.02072230\n2  -1.6189273  0.55553922 -0.07111317\n3  -1.1151675 -0.07297970 -0.02173008\n4  -0.8942966 -0.08236998  0.01081318\n5  -0.6442081 -1.30668523  0.07258248\n6  -0.1903514 -0.65914745  0.02655252\n7   0.3596219 -0.74367447  0.04278124\n8   0.9718018  1.35405877  0.06286252\n9   1.5593159  0.96404558  0.02357446\n10  1.7669951  1.01521706 -0.04498818\n11  1.9311034 -1.66266195 -0.08061267\n\n### 주성분회귀 - 원래데이터를 주성분분석을 통해 새로운 데이터를 생성 이를 가지고 회귀 \n\ndf&lt;-data.frame(IMPORT = scale(data_use$IMPORT),\n\n               pc$x)\n\ndf\n\n       IMPORT        PC1         PC2         PC3\n1  -1.3185185 -2.1258872  0.63865815 -0.02072230\n2  -1.2084753 -1.6189273  0.55553922 -0.07111317\n3  -0.6362502 -1.1151675 -0.07297970 -0.02173008\n4  -0.6142416 -0.8942966 -0.08236998  0.01081318\n5  -0.6802675 -0.6442081 -1.30668523  0.07258248\n6  -0.3281290 -0.1903514 -0.65914745  0.02655252\n7   0.1780700  0.3596219 -0.74367447  0.04278124\n8   1.0143989  0.9718018  1.35405877  0.06286252\n9   1.3665374  1.5593159  0.96404558  0.02357446\n10  1.2564942  1.7669951  1.01521706 -0.04498818\n11  0.9703816  1.9311034 -1.66266195 -0.08061267\n\nres&lt;-lm(IMPORT~.,data=df)\n\nsummary(res) #유의한 1,2번째 계수들만 사용하고 3번째 것은 없이 해도 되겠다..\n\n\nCall:\nlm(formula = IMPORT ~ ., data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.11525 -0.08573  0.01194  0.04984  0.17236 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 8.900e-16  3.244e-02   0.000 1.000000    \nPC1         6.900e-01  2.406e-02  28.673 1.61e-08 ***\nPC2         1.913e-01  3.406e-02   5.617 0.000801 ***\nPC3         1.160e+00  6.559e-01   1.768 0.120376    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1076 on 7 degrees of freedom\nMultiple R-squared:  0.9919,    Adjusted R-squared:  0.9884 \nF-statistic: 285.6 on 3 and 7 DF,  p-value: 1.112e-07\n\n#중간고사 이후의것이 주가나오겠지만 앞에것도 알아야함 연습문제에서 나올것 예상\n\n\n\n\n\n# 11.10 - \n\ndata_3.3&lt;-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\nhead(data_3.3)\n\n   Y X1 X2 X3 X4 X5 X6\n1 43 51 30 39 61 92 45\n2 63 64 51 54 63 73 47\n3 71 70 68 69 76 86 48\n4 61 63 45 47 54 84 35\n5 81 78 56 66 71 83 47\n6 43 55 49 44 54 49 34\n\n# 분산확대 인자(VIF) : 10초과 &gt; 심각한 공산성의 문제가 있음\n\nres&lt;-lm(Y~.,data=data_3.3)\n\nsummary(res)\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.9418  -4.3555   0.3158   5.5425  11.5990 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10.78708   11.58926   0.931 0.361634    \nX1           0.61319    0.16098   3.809 0.000903 ***\nX2          -0.07305    0.13572  -0.538 0.595594    \nX3           0.32033    0.16852   1.901 0.069925 .  \nX4           0.08173    0.22148   0.369 0.715480    \nX5           0.03838    0.14700   0.261 0.796334    \nX6          -0.21706    0.17821  -1.218 0.235577    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.068 on 23 degrees of freedom\nMultiple R-squared:  0.7326,    Adjusted R-squared:  0.6628 \nF-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05\n\nolsrr::ols_vif_tol(res) #10을 넘는 것이 없기에 공산성의 문제는 없다 \n\n  Variables Tolerance      VIF\n1        X1 0.3749447 2.667060\n2        X2 0.6246520 1.600891\n3        X3 0.4403263 2.271043\n4        X4 0.3248624 3.078226\n5        X5 0.8142600 1.228109\n6        X6 0.5124025 1.951591\n\n# 수정결정계수\n\nres_summ&lt;-summary(res)\n\nres_summ$adj.r.squared #0.662846\n\n[1] 0.662846\n\n# Mallow's Cp - 작을수록 좋음\n\n# 축소모형\n\nres_subset&lt;-lm(Y~X1+X3,data=data_3.3)\n\nolsrr::ols_mallows_cp(res_subset,res) \n\n[1] 1.114811\n\n# AIC - 참고만\n\nolsrr::ols_aic(res_subset,method = \"SAS\")\n\n[1] 118.0024\n\n# BIC\n\nolsrr::ols_sbic(res_subset,res)\n\n[1] 121.0938\n\n### 전진적 선택법 - AIC\n\nres_step&lt;-step(res,direction = \"forward\")\n\nStart:  AIC=123.36\nY ~ X1 + X2 + X3 + X4 + X5 + X6\n\n### 후진적 제거법 -AIC\n\nres_step&lt;-step(res,direction = \"backward\")\n\nStart:  AIC=123.36\nY ~ X1 + X2 + X3 + X4 + X5 + X6\n\n       Df Sum of Sq    RSS    AIC\n- X5    1      3.41 1152.4 121.45\n- X4    1      6.80 1155.8 121.54\n- X2    1     14.47 1163.5 121.74\n- X6    1     74.11 1223.1 123.24\n&lt;none&gt;              1149.0 123.36\n- X3    1    180.50 1329.5 125.74\n- X1    1    724.80 1873.8 136.04\n\nStep:  AIC=121.45\nY ~ X1 + X2 + X3 + X4 + X6\n\n       Df Sum of Sq    RSS    AIC\n- X4    1     10.61 1163.0 119.73\n- X2    1     14.16 1166.6 119.82\n- X6    1     71.27 1223.7 121.25\n&lt;none&gt;              1152.4 121.45\n- X3    1    177.74 1330.1 123.75\n- X1    1    724.70 1877.1 134.09\n\nStep:  AIC=119.73\nY ~ X1 + X2 + X3 + X6\n\n       Df Sum of Sq    RSS    AIC\n- X2    1     16.10 1179.1 118.14\n- X6    1     61.60 1224.6 119.28\n&lt;none&gt;              1163.0 119.73\n- X3    1    197.03 1360.0 122.42\n- X1    1   1165.94 2328.9 138.56\n\nStep:  AIC=118.14\nY ~ X1 + X3 + X6\n\n       Df Sum of Sq    RSS    AIC\n- X6    1     75.54 1254.7 118.00\n&lt;none&gt;              1179.1 118.14\n- X3    1    186.12 1365.2 120.54\n- X1    1   1259.91 2439.0 137.94\n\nStep:  AIC=118\nY ~ X1 + X3\n\n       Df Sum of Sq    RSS    AIC\n&lt;none&gt;              1254.7 118.00\n- X3    1    114.73 1369.4 118.63\n- X1    1   1370.91 2625.6 138.16\n\n### 단계적 방법\n\nres_step&lt;-step(res) #최종적으로는 X1+X3이 선택됨 이거 사용하는 것이 좋을듯 \n\nStart:  AIC=123.36\nY ~ X1 + X2 + X3 + X4 + X5 + X6\n\n       Df Sum of Sq    RSS    AIC\n- X5    1      3.41 1152.4 121.45\n- X4    1      6.80 1155.8 121.54\n- X2    1     14.47 1163.5 121.74\n- X6    1     74.11 1223.1 123.24\n&lt;none&gt;              1149.0 123.36\n- X3    1    180.50 1329.5 125.74\n- X1    1    724.80 1873.8 136.04\n\nStep:  AIC=121.45\nY ~ X1 + X2 + X3 + X4 + X6\n\n       Df Sum of Sq    RSS    AIC\n- X4    1     10.61 1163.0 119.73\n- X2    1     14.16 1166.6 119.82\n- X6    1     71.27 1223.7 121.25\n&lt;none&gt;              1152.4 121.45\n- X3    1    177.74 1330.1 123.75\n- X1    1    724.70 1877.1 134.09\n\nStep:  AIC=119.73\nY ~ X1 + X2 + X3 + X6\n\n       Df Sum of Sq    RSS    AIC\n- X2    1     16.10 1179.1 118.14\n- X6    1     61.60 1224.6 119.28\n&lt;none&gt;              1163.0 119.73\n- X3    1    197.03 1360.0 122.42\n- X1    1   1165.94 2328.9 138.56\n\nStep:  AIC=118.14\nY ~ X1 + X3 + X6\n\n       Df Sum of Sq    RSS    AIC\n- X6    1     75.54 1254.7 118.00\n&lt;none&gt;              1179.1 118.14\n- X3    1    186.12 1365.2 120.54\n- X1    1   1259.91 2439.0 137.94\n\nStep:  AIC=118\nY ~ X1 + X3\n\n       Df Sum of Sq    RSS    AIC\n&lt;none&gt;              1254.7 118.00\n- X3    1    114.73 1369.4 118.63\n- X1    1   1370.91 2625.6 138.16\n\nsummary(res_step) #X3가 의미 없다고 나와도 아님 검증된것임\n\n\nCall:\nlm(formula = Y ~ X1 + X3, data = data_3.3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.5568  -5.7331   0.6701   6.5341  10.3610 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   9.8709     7.0612   1.398    0.174    \nX1            0.6435     0.1185   5.432 9.57e-06 ***\nX3            0.2112     0.1344   1.571    0.128    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.817 on 27 degrees of freedom\nMultiple R-squared:  0.708, Adjusted R-squared:  0.6864 \nF-statistic: 32.74 on 2 and 27 DF,  p-value: 6.058e-08\n\n\n\n\n\n\n\n\ndata_use&lt;-read.table(\"All_Data/p329.txt\",header=T,sep=\"\\t\")\n\nhead(data_use)\n\n     X1 X2    X3    X4 X5 X6 X7 X8 X9    Y\n1 4.918  1 3.472 0.998  1  7  4 42  0 25.9\n2 5.021  1 3.531 1.500  2  7  4 62  0 29.5\n3 4.543  1 2.275 1.175  1  6  3 40  0 27.9\n4 4.557  1 4.050 1.232  1  6  3 54  0 25.9\n5 5.060  1 4.455 1.121  1  6  3 42  0 29.9\n6 3.891  1 4.455 0.988  1  6  3 56  0 29.9\n\n### 데이터 탐색 - 자료형\n\nsapply(data_use,class)\n\n       X1        X2        X3        X4        X5        X6        X7        X8 \n\"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \n       X9         Y \n\"numeric\" \"numeric\" \n\n### 데이터 탐색 - 기초 통계량\n\nsummary(data_use) #결측값의 여부를 확인\n\n       X1              X2              X3              X4       \n Min.   :3.891   Min.   :1.000   Min.   :2.275   Min.   :0.975  \n 1st Qu.:5.058   1st Qu.:1.000   1st Qu.:4.855   1st Qu.:1.161  \n Median :5.974   Median :1.000   Median :5.685   Median :1.432  \n Mean   :6.405   Mean   :1.167   Mean   :6.033   Mean   :1.384  \n 3rd Qu.:7.873   3rd Qu.:1.500   3rd Qu.:7.158   3rd Qu.:1.577  \n Max.   :9.142   Max.   :1.500   Max.   :9.890   Max.   :1.831  \n       X5              X6            X7              X8              X9      \n Min.   :0.000   Min.   :5.0   Min.   :2.000   Min.   : 3.00   Min.   :0.00  \n 1st Qu.:1.000   1st Qu.:6.0   1st Qu.:3.000   1st Qu.:30.00   1st Qu.:0.00  \n Median :1.000   Median :6.0   Median :3.000   Median :40.00   Median :0.00  \n Mean   :1.312   Mean   :6.5   Mean   :3.167   Mean   :37.46   Mean   :0.25  \n 3rd Qu.:2.000   3rd Qu.:7.0   3rd Qu.:3.250   3rd Qu.:48.50   3rd Qu.:0.25  \n Max.   :2.000   Max.   :8.0   Max.   :4.000   Max.   :62.00   Max.   :1.00  \n       Y        \n Min.   :25.90  \n 1st Qu.:29.90  \n Median :33.70  \n Mean   :34.63  \n 3rd Qu.:38.15  \n Max.   :45.80  \n\n### 데이터 탐색 - 히스토그램 & 상자그림\n\nhist(data_use$Y,main=\"histogram of data_use$Y\")\n\n\n\nboxplot(data_use$Y)\n\n\n\n### 데이터 탐색 - 산점도 행렬 & 상관계수\n\nplot(data_use)\n\ncor(data_use) \n\n           X1         X2         X3         X4          X5        X6        X7\nX1  1.0000000  0.6512669  0.6892117  0.7342737  0.45855650 0.6406157 0.3671126\nX2  0.6512669  1.0000000  0.4129558  0.7285916  0.22402204 0.5103104 0.4264014\nX3  0.6892117  0.4129558  1.0000000  0.5715520  0.20466375 0.3921244 0.1516093\nX4  0.7342737  0.7285916  0.5715520  1.0000000  0.35888351 0.6788606 0.5743353\nX5  0.4585565  0.2240220  0.2046638  0.3588835  1.00000000 0.5893871 0.5412988\nX6  0.6406157  0.5103104  0.3921244  0.6788606  0.58938707 1.0000000 0.8703883\nX7  0.3671126  0.4264014  0.1516093  0.5743353  0.54129880 0.8703883 1.0000000\nX8 -0.4371012 -0.1007485 -0.3527514 -0.1390869 -0.02016883 0.1242663 0.3135114\nX9  0.1466825  0.2041241  0.3059946  0.1065612  0.10161846 0.2222222 0.0000000\nY   0.8739117  0.7097771  0.6476364  0.7077656  0.46146792 0.5284436 0.2815200\n            X8        X9          Y\nX1 -0.43710116 0.1466825  0.8739117\nX2 -0.10074847 0.2041241  0.7097771\nX3 -0.35275139 0.3059946  0.6476364\nX4 -0.13908686 0.1065612  0.7077656\nX5 -0.02016883 0.1016185  0.4614679\nX6  0.12426629 0.2222222  0.5284436\nX7  0.31351144 0.0000000  0.2815200\nX8  1.00000000 0.2257796 -0.3974034\nX9  0.22577960 1.0000000  0.2668783\nY  -0.39740338 0.2668783  1.0000000\n\npairs(data_use)\n\n\n\npanel.cor&lt;-function(x,y){\n\n  par(usr=c(0,1,0,1))\n\n  r&lt;-round(cor(x,y),digits = 3)\n\n  text(0.5,0.5,r,cex=1.5)\n\n}\n\npairs(data_use,lower.panel = panel.cor)\n\n\n\n### (a) 모든 변수들이 모형에 포함시킬 것인가?\n\ncor(data_use)\n\n           X1         X2         X3         X4          X5        X6        X7\nX1  1.0000000  0.6512669  0.6892117  0.7342737  0.45855650 0.6406157 0.3671126\nX2  0.6512669  1.0000000  0.4129558  0.7285916  0.22402204 0.5103104 0.4264014\nX3  0.6892117  0.4129558  1.0000000  0.5715520  0.20466375 0.3921244 0.1516093\nX4  0.7342737  0.7285916  0.5715520  1.0000000  0.35888351 0.6788606 0.5743353\nX5  0.4585565  0.2240220  0.2046638  0.3588835  1.00000000 0.5893871 0.5412988\nX6  0.6406157  0.5103104  0.3921244  0.6788606  0.58938707 1.0000000 0.8703883\nX7  0.3671126  0.4264014  0.1516093  0.5743353  0.54129880 0.8703883 1.0000000\nX8 -0.4371012 -0.1007485 -0.3527514 -0.1390869 -0.02016883 0.1242663 0.3135114\nX9  0.1466825  0.2041241  0.3059946  0.1065612  0.10161846 0.2222222 0.0000000\nY   0.8739117  0.7097771  0.6476364  0.7077656  0.46146792 0.5284436 0.2815200\n            X8        X9          Y\nX1 -0.43710116 0.1466825  0.8739117\nX2 -0.10074847 0.2041241  0.7097771\nX3 -0.35275139 0.3059946  0.6476364\nX4 -0.13908686 0.1065612  0.7077656\nX5 -0.02016883 0.1016185  0.4614679\nX6  0.12426629 0.2222222  0.5284436\nX7  0.31351144 0.0000000  0.2815200\nX8  1.00000000 0.2257796 -0.3974034\nX9  0.22577960 1.0000000  0.2668783\nY  -0.39740338 0.2668783  1.0000000\n\nmodel_1&lt;-lm(data_use$Y~.,data_use)\n\nsummary(model_1) #전형적인 다중공선성이 보인다 \n\n\nCall:\nlm(formula = data_use$Y ~ ., data = data_use)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7729 -1.9801 -0.0868  1.6615  4.2618 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) 15.31044    5.96093   2.568   0.0223 *\nX1           1.95413    1.03833   1.882   0.0808 .\nX2           6.84552    4.33529   1.579   0.1367  \nX3           0.13761    0.49436   0.278   0.7848  \nX4           2.78143    4.39482   0.633   0.5370  \nX5           2.05076    1.38457   1.481   0.1607  \nX6          -0.55590    2.39791  -0.232   0.8200  \nX7          -1.24516    3.42293  -0.364   0.7215  \nX8          -0.03800    0.06726  -0.565   0.5810  \nX9           1.70446    1.95317   0.873   0.3976  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.973 on 14 degrees of freedom\nMultiple R-squared:  0.8512,    Adjusted R-squared:  0.7555 \nF-statistic: 8.898 on 9 and 14 DF,  p-value: 0.0002015\n\n#coef가 X6 0.820이므로 확인 필요\n\nolsrr::ols_vif_tol(model_1) #X6,X7이 10에 가깝기에 제거해야하는 대상중 최우선 \n\n  Variables  Tolerance       VIF\n1        X1 0.14241176  7.021892\n2        X2 0.35267392  2.835480\n3        X3 0.40735454  2.454864\n4        X4 0.26066568  3.836332\n5        X5 0.54842184  1.823414\n6        X6 0.08539078 11.710866\n7        X7 0.10286111  9.721847\n8        X8 0.43083904  2.321052\n9        X9 0.51482060  1.942424\n\n#다중공선성이 보이기에 모든 변수를 모형에 포함 시킬수는 없다 \n\n#10보다 크면 심각한 공선성이 존재 \n\n### (b) 지방세(X1) 방의 수(X6), 건물의 나이(X8)가 판매가격(Y)을 \n\n#       설명하는데 적절하다는 의견에 동의 하는가?\n\nmodel_2&lt;-lm(Y~X1+X6+X8,data=data_use)\n\nsummary(model_2)\n\n\nCall:\nlm(formula = Y ~ X1 + X6 + X8, data = data_use)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7486 -2.4082 -0.3594  2.1378  6.5353 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 14.796013   4.971105   2.976 0.007462 ** \nX1           3.489464   0.729368   4.784 0.000113 ***\nX6          -0.415515   1.182262  -0.351 0.728921    \nX8           0.004923   0.063597   0.077 0.939062    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.123 on 20 degrees of freedom\nMultiple R-squared:  0.7655,    Adjusted R-squared:  0.7303 \nF-statistic: 21.76 on 3 and 20 DF,  p-value: 1.653e-06\n\n# VIF\n\nolsrr::ols_vif_tol(model_2)\n\n  Variables Tolerance      VIF\n1        X1 0.3184367 3.140342\n2        X6 0.3875669 2.580200\n3        X8 0.5317389 1.880622\n\n# 회귀진단 그래프들\n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(model_2)\n\n\n\nlayout(1)\n\n# Cook의 거리 - 1을 넘으면 확인을 해야한다 \n\nolsrr::ols_plot_cooksd_chart(model_2) \n\n\n\n#동의는 할 수 있으나(적절하지만) 최고의 모형이라는데는 동의 못함\n\n### (c) 지방세 X1가 단독으로 판매가격 Y을 설명하는데 적절하다는 의견에 동의?\n\nmodel_3&lt;-lm(Y~X1,data=data_use)\n\nsummary(model_3) #adj-rsquared는 변수를 선택할때 사용 \n\n\nCall:\nlm(formula = Y ~ X1, data = data_use)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8445 -2.3340 -0.3841  1.9689  6.3005 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  13.3553     2.5955   5.146 3.71e-05 ***\nX1            3.3215     0.3939   8.433 2.44e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.988 on 22 degrees of freedom\nMultiple R-squared:  0.7637,    Adjusted R-squared:  0.753 \nF-statistic: 71.11 on 1 and 22 DF,  p-value: 2.435e-08\n\n# 회귀진단 그래프들\n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(model_3)\n\n\n\nlayout(1)\n\n# Cook의 거리 - 1을 넘으면 확인을 해야한다 \n\nolsrr::ols_plot_cooksd_chart(model_3) \n\n\n\n### 적절한 모형 제시\n\ndata_use_2&lt;-data_use[-6]\n\nmodel_4&lt;-lm(Y~.,data_use_2)\n\nsummary(model_4)\n\n\nCall:\nlm(formula = Y ~ ., data = data_use_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7340 -1.8513 -0.0154  1.5472  4.2113 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) 14.40540    4.36026   3.304  0.00482 **\nX1           1.81642    0.82431   2.204  0.04360 * \nX2           7.13892    4.01353   1.779  0.09556 . \nX3           0.14721    0.47683   0.309  0.76177   \nX4           2.73339    4.24921   0.643  0.52976   \nX5           2.06520    1.33883   1.543  0.14377   \nX7          -1.91236    1.79355  -1.066  0.30318   \nX8          -0.03832    0.06509  -0.589  0.56481   \nX9           1.48746    1.65931   0.896  0.38418   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.878 on 15 degrees of freedom\nMultiple R-squared:  0.8506,    Adjusted R-squared:  0.771 \nF-statistic: 10.68 on 8 and 15 DF,  p-value: 5.936e-05\n\n# VIF\n\nolsrr::ols_vif_tol(model_4)\n\n  Variables Tolerance      VIF\n1        X1 0.2117072 4.723504\n2        X2 0.3855308 2.593827\n3        X3 0.4102334 2.437637\n4        X4 0.2612467 3.827800\n5        X5 0.5495336 1.819725\n6        X7 0.3510102 2.848920\n7        X8 0.4310175 2.320091\n8        X9 0.6683173 1.496295\n\n# 회귀진단 그래프들\n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(model_4)\n\n\n\nlayout(1)\n\n# Cook의 거리 - 1을 넘으면 확인을 해야한다 \n\nolsrr::ols_plot_cooksd_chart(model_4) \n\n\n\n### 단계적 선택법 - AIC\n\nmodel_5&lt;-step(model_4)\n\nStart:  AIC=57.45\nY ~ X1 + X2 + X3 + X4 + X5 + X7 + X8 + X9\n\n       Df Sum of Sq    RSS    AIC\n- X3    1     0.789 125.00 55.605\n- X8    1     2.870 127.08 56.001\n- X4    1     3.426 127.63 56.106\n- X9    1     6.654 130.86 56.706\n- X7    1     9.414 133.62 57.207\n&lt;none&gt;              124.21 57.453\n- X5    1    19.702 143.91 58.987\n- X2    1    26.198 150.40 60.046\n- X1    1    40.207 164.41 62.184\n\nStep:  AIC=55.61\nY ~ X1 + X2 + X4 + X5 + X7 + X8 + X9\n\n       Df Sum of Sq    RSS    AIC\n- X8    1     3.369 128.36 54.243\n- X4    1     4.816 129.81 54.513\n- X9    1     9.581 134.58 55.378\n- X7    1     9.655 134.65 55.391\n&lt;none&gt;              125.00 55.605\n- X5    1    18.957 143.95 56.994\n- X2    1    25.474 150.47 58.057\n- X1    1    53.245 178.24 62.122\n\nStep:  AIC=54.24\nY ~ X1 + X2 + X4 + X5 + X7 + X9\n\n       Df Sum of Sq    RSS    AIC\n- X4    1     5.011 133.38 53.163\n- X9    1     6.543 134.91 53.437\n&lt;none&gt;              128.36 54.243\n- X5    1    20.033 148.40 55.724\n- X7    1    22.819 151.18 56.170\n- X2    1    24.299 152.66 56.404\n- X1    1    95.134 223.50 65.552\n\nStep:  AIC=53.16\nY ~ X1 + X2 + X5 + X7 + X9\n\n       Df Sum of Sq    RSS    AIC\n- X9    1     6.223 139.60 52.257\n&lt;none&gt;              133.38 53.163\n- X5    1    17.801 151.18 54.169\n- X7    1    17.873 151.25 54.181\n- X2    1    39.335 172.71 57.365\n- X1    1   157.972 291.35 69.915\n\nStep:  AIC=52.26\nY ~ X1 + X2 + X5 + X7\n\n       Df Sum of Sq    RSS    AIC\n&lt;none&gt;              139.60 52.257\n- X5    1    20.836 160.43 53.596\n- X7    1    21.669 161.27 53.720\n- X2    1    47.409 187.01 57.274\n- X1    1   156.606 296.20 68.312\n\nsummary(model_5)\n\n\nCall:\nlm(formula = Y ~ X1 + X2 + X5 + X7, data = data_use_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5605 -2.0856  0.0238  1.8580  3.8981 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  13.6212     3.6725   3.709 0.001489 ** \nX1            2.4123     0.5225   4.617 0.000188 ***\nX2            8.4589     3.3300   2.540 0.019970 *  \nX5            2.0604     1.2235   1.684 0.108541    \nX7           -2.2154     1.2901  -1.717 0.102176    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.711 on 19 degrees of freedom\nMultiple R-squared:  0.8321,    Adjusted R-squared:  0.7968 \nF-statistic: 23.54 on 4 and 19 DF,  p-value: 3.866e-07\n\n# 회귀진단 그래프들\n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(model_5)\n\n\n\nlayout(1)\n\n# Cook의 거리 - 0.5보다 작으면 괜찮음 / 1보다 큰 값을 고려 / 전체적으로 봤을때 튀는 값이 있는 경우 \n\nolsrr::ols_plot_cooksd_chart(model_5) \n\n\n\n### 17번 제거\n\ndata_use_3&lt;-model_5$model[-17,]\n\nmodel_6&lt;-lm(Y~.,data_use_3)\n\nsummary(model_6)\n\n\nCall:\nlm(formula = Y ~ ., data = data_use_3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.4577 -1.6655  0.1575  1.7978  4.1865 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  13.2263     3.4659   3.816  0.00127 **\nX1            1.7014     0.6247   2.723  0.01394 * \nX2           12.0705     3.6958   3.266  0.00429 **\nX5            2.4602     1.1726   2.098  0.05028 . \nX7           -2.2310     1.2152  -1.836  0.08294 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.553 on 18 degrees of freedom\nMultiple R-squared:  0.8418,    Adjusted R-squared:  0.8067 \nF-statistic: 23.95 on 4 and 18 DF,  p-value: 5.316e-07\n\n# VIF\n\nolsrr::ols_vif_tol(model_6)\n\n  Variables Tolerance      VIF\n1        X1 0.3319061 3.012900\n2        X2 0.3658998 2.732989\n3        X5 0.5664655 1.765333\n4        X7 0.6043771 1.654596\n\n# 회귀진단 그래프들\n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(model_6)\n\n\n\nlayout(1)\n\n# Cook의 거리 - 1을 넘으면 확인을 해야한다 \n\nolsrr::ols_plot_cooksd_chart(model_6)"
  },
  {
    "objectID": "Regression_Analysis.html#장",
    "href": "Regression_Analysis.html#장",
    "title": "Regression Analysis",
    "section": "",
    "text": "data_2.5&lt;-read.table(\"All_Data/p031.txt\",header=TRUE,sep=\"\\t\")\n\ndim(data_2.5) #14 2\n\n[1] 14  2\n\nhead(data_2.5)\n\n  Minutes Units\n1      23     1\n2      29     2\n3      49     3\n4      64     4\n5      74     4\n6      87     5\n\nX&lt;-data_2.5$Units\n\nY&lt;-data_2.5$Minutes\n\n\n\n\n\ndf&lt;-data.frame(\n\n  #1:length(X),\n\n  Y,\n\n  X,\n\n  Y-mean(Y),\n\n  X-mean(X),\n\n  (Y-mean(Y))^2,\n\n  (X-mean(X))^2,\n\n  (Y-mean(Y))*(X-mean(X))\n\n)\n\ndf\n\n     Y  X Y...mean.Y. X...mean.X. X.Y...mean.Y...2 X.X...mean.X...2\n1   23  1 -74.2142857          -5     5.507760e+03               25\n2   29  2 -68.2142857          -4     4.653189e+03               16\n3   49  3 -48.2142857          -3     2.324617e+03                9\n4   64  4 -33.2142857          -2     1.103189e+03                4\n5   74  4 -23.2142857          -2     5.389031e+02                4\n6   87  5 -10.2142857          -1     1.043316e+02                1\n7   96  6  -1.2142857           0     1.474490e+00                0\n8   97  6  -0.2142857           0     4.591837e-02                0\n9  109  7  11.7857143           1     1.389031e+02                1\n10 119  8  21.7857143           2     4.746173e+02                4\n11 149  9  51.7857143           3     2.681760e+03                9\n12 145  9  47.7857143           3     2.283474e+03                9\n13 154 10  56.7857143           4     3.224617e+03               16\n14 166 10  68.7857143           4     4.731474e+03               16\n   X.Y...mean.Y......X...mean.X..\n1                       371.07143\n2                       272.85714\n3                       144.64286\n4                        66.42857\n5                        46.42857\n6                        10.21429\n7                         0.00000\n8                         0.00000\n9                        11.78571\n10                       43.57143\n11                      155.35714\n12                      143.35714\n13                      227.14286\n14                      275.14286\n\n\n\n\n\n\nCOV_XY&lt;-sum((Y-mean(Y))*(X-mean(X))) / (length(X)-1) #136\n\n### cov() 함수\n\ncov(X,Y) #136\n\n[1] 136\n\n### 상관계수(correalationship)\n\n### cor() 함수\n\ncor(X,Y) #0.9936987 \n\n[1] 0.9936987"
  },
  {
    "objectID": "Regression_Analysis.html#선형모형-1",
    "href": "Regression_Analysis.html#선형모형-1",
    "title": "Regression Analysis",
    "section": "",
    "text": "data_2.5&lt;-read.table(\"All_Data/p031.txt\",header=TRUE,sep=\"\\t\")\n\nx&lt;-data_2.5$Units\n\ny&lt;-data_2.5$Minutes\n\n\n\n\ncor_xy&lt;- COV_XY / (sd(x)*sd(y))\n\ncor_xy\n\n[1] 0.9936987\n\n### cor() 함수\n\ncor(x,y)\n\n[1] 0.9936987\n\ncor(y,x)\n\n[1] 0.9936987\n\ndata_2.5\n\n   Minutes Units\n1       23     1\n2       29     2\n3       49     3\n4       64     4\n5       74     4\n6       87     5\n7       96     6\n8       97     6\n9      109     7\n10     119     8\n11     149     9\n12     145     9\n13     154    10\n14     166    10\n\ncor(data_2.5)\n\n          Minutes     Units\nMinutes 1.0000000 0.9936987\nUnits   0.9936987 1.0000000\n\n\n\n\n\n\nclass(X)\n\n[1] \"numeric\"\n\nclass(Y) #both numeric\n\n[1] \"numeric\"\n\nplot(X,Y, pch=19,xlab=\"Units\",ylab=\"Minutes\") \n\n\n\n\n\n\n\n\ndata_2.3&lt;-read.table(\"All_Data/p029a.txt\",header=TRUE,sep=\"\\t\")\n\ndata_2.3\n\n    Y  X\n1   1 -7\n2  14 -6\n3  25 -5\n4  34 -4\n5  41 -3\n6  46 -2\n7  49 -1\n8  50  0\n9  49  1\n10 46  2\n11 41  3\n12 34  4\n13 25  5\n14 14  6\n15  1  7\n\nX&lt;-data_2.3$X\n\nY&lt;-data_2.3$Y\n\n\n\n\n\nplot(X,Y)\n\n\n\ncor(X,Y) # 0 완벽하게 2차함수의 형태도 0이 나옴(직선의 형태가 아닌것만)\n\n[1] 0\n\n\n\n\n\n\ndata_2.4&lt;-read.table(\"All_Data/p029b.txt\",header=TRUE,sep=\"\\t\")\n\n\n\n\n\nplot(data_2.4$X1,data_2.4$Y1, pch=19); abline(3,0.5) #기울기 3 절편0.5인 선을 추가해라 \n\n\n\nplot(data_2.4$X2,data_2.4$Y2, pch=19); abline(3,0.5)\n\n\n\nplot(data_2.4$X3,data_2.4$Y3, pch=19); abline(3,0.5)\n\n\n\nplot(data_2.4$X4,data_2.4$Y4, pch=19); abline(3,0.5)\n\n\n\nm&lt;-matrix(1:4,ncol=2,byrow=TRUE) #2행의 매트릭스 생성 \n\nm\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nlayout(m)\n\nplot(Y1~X1, data=data_2.4,pch=19); abline(3,0.5)\n\n#y~x  -&gt; y=ax+b 이러한 형태를 가지는 모형식이라는 의미 \n\nplot(Y2~X2, data=data_2.4,pch=19); abline(3,0.5)\n\nplot(Y3~X3, data=data_2.4,pch=19); abline(3,0.5)\n\nplot(Y4~X4, data=data_2.4,pch=19); abline(3,0.5)\n\n\n\nlayout(1) #변환을 다시 하지 않으면 설정한 매트릭스의 비율로 그래프가 그려짐 해제 필요 \n\n# cor()\n\ncor(data_2.4$X1,data_2.4$Y1) #0.8164205\n\n[1] 0.8164205\n\ncor(data_2.4$X2,data_2.4$Y2) #0.8162365\n\n[1] 0.8162365\n\ncor(data_2.4$X3,data_2.4$Y3) #0.8162867\n\n[1] 0.8162867\n\ncor(data_2.4$X4,data_2.4$Y4) #0.8165214\n\n[1] 0.8165214\n\ncor(data_2.4) #이렇게 한번에 할 수 있으나 가독성 떨어짐 \n\n           Y1         X1         Y2         X2         Y3         X3         Y4\nY1  1.0000000  0.8164205  0.7500054  0.8164205  0.4687167  0.8164205 -0.4891162\nX1  0.8164205  1.0000000  0.8162365  1.0000000  0.8162867  1.0000000 -0.3140467\nY2  0.7500054  0.8162365  1.0000000  0.8162365  0.5879193  0.8162365 -0.4780949\nX2  0.8164205  1.0000000  0.8162365  1.0000000  0.8162867  1.0000000 -0.3140467\nY3  0.4687167  0.8162867  0.5879193  0.8162867  1.0000000  0.8162867 -0.1554718\nX3  0.8164205  1.0000000  0.8162365  1.0000000  0.8162867  1.0000000 -0.3140467\nY4 -0.4891162 -0.3140467 -0.4780949 -0.3140467 -0.1554718 -0.3140467  1.0000000\nX4 -0.5290927 -0.5000000 -0.7184365 -0.5000000 -0.3446610 -0.5000000  0.8165214\n           X4\nY1 -0.5290927\nX1 -0.5000000\nY2 -0.7184365\nX2 -0.5000000\nY3 -0.3446610\nX3 -0.5000000\nY4  0.8165214\nX4  1.0000000"
  },
  {
    "objectID": "Regression_Analysis.html#단순선형회귀모형-2.4",
    "href": "Regression_Analysis.html#단순선형회귀모형-2.4",
    "title": "Regression Analysis",
    "section": "",
    "text": "data_2.5&lt;-read.table(\"All_Data/p031.txt\",header=TRUE,sep=\"\\t\")\n\nx&lt;-data_2.5$Units\n\ny&lt;-data_2.5$Minutes\n\n\n\n\n\nsum((y-mean(y))*(x-mean(x))) #1768\n\n[1] 1768\n\nsum((x-mean(x))^2) #114\n\n[1] 114\n\nbeta1_hat&lt;-sum((y-mean(y))*(x-mean(x))) / sum((x-mean(x))^2)\n\nbeta1_hat #15.50877\n\n[1] 15.50877\n\nbeta0_hat &lt;- mean(y) - (beta1_hat*mean(x))\n\nbeta0_hat #4.161654\n\n[1] 4.161654\n\n### 최소제곱회귀 방정식\n\n# Minutes = 4.161654 + 15.50877 * Units\n\nplot(beta0_hat+beta1_hat*x,pch=19);\n\n\n\n# 4개의 고장 난 부품을 수리하는데 걸리는 에측시간\n\n4.161654 + 15.50877 * 4 #66.19673\n\n[1] 66.19673\n\nunits&lt;-4\n\nbeta0_hat + beta1_hat*units\n\n[1] 66.19674\n\n### 적합값(Fitted value)\n\ny_hat&lt;-beta0_hat + beta1_hat*(x)\n\n### 최소 제곱 잔차(residual)\n\ne&lt;-y-y_hat\n\ne #합이 0이라는 특징이 존재\n\n [1]  3.3295739 -6.1791980 -1.6879699 -2.1967419  7.8032581  5.2944862\n [7] -1.2142857 -0.2142857 -3.7230576 -9.2318296  5.2593985  1.2593985\n[13] -5.2493734  6.7506266\n\nsum(e) #1.278977e-13 0에 근사한 추지가 나옴\n\n[1] 1.278977e-13\n\n\n\n\n\n\ndf_2.7&lt;-data.frame(\n\n  x=x,\n\n  y=y,\n\n  y_hat,\n\n  e\n\n)\n\ndf_2.7\n\n    x   y     y_hat          e\n1   1  23  19.67043  3.3295739\n2   2  29  35.17920 -6.1791980\n3   3  49  50.68797 -1.6879699\n4   4  64  66.19674 -2.1967419\n5   4  74  66.19674  7.8032581\n6   5  87  81.70551  5.2944862\n7   6  96  97.21429 -1.2142857\n8   6  97  97.21429 -0.2142857\n9   7 109 112.72306 -3.7230576\n10  8 119 128.23183 -9.2318296\n11  9 149 143.74060  5.2593985\n12  9 145 143.74060  1.2593985\n13 10 154 159.24937 -5.2493734\n14 10 166 159.24937  6.7506266\n\n### lm() 함수 (linear model)\n\n# Minutes = beta0 + beta1 * Units + epsilon\n\n# 모형식 : y~x\n\nlm(y~x)\n\n\nCall:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)            x  \n      4.162       15.509  \n\nres_lm&lt;-lm(Minutes~Units,data=data_2.5)\n\nres_lm\n\n\nCall:\nlm(formula = Minutes ~ Units, data = data_2.5)\n\nCoefficients:\n(Intercept)        Units  \n      4.162       15.509  \n\n# 리스트의 이름 \n\nnames(res_lm)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n# 회귀계수\n\nres_lm$coefficients\n\n(Intercept)       Units \n   4.161654   15.508772 \n\ncoef(res_lm)\n\n(Intercept)       Units \n   4.161654   15.508772 \n\n# 적합값\n\nres_lm$fitted.values\n\n        1         2         3         4         5         6         7         8 \n 19.67043  35.17920  50.68797  66.19674  66.19674  81.70551  97.21429  97.21429 \n        9        10        11        12        13        14 \n112.72306 128.23183 143.74060 143.74060 159.24937 159.24937 \n\nfitted(res_lm)\n\n        1         2         3         4         5         6         7         8 \n 19.67043  35.17920  50.68797  66.19674  66.19674  81.70551  97.21429  97.21429 \n        9        10        11        12        13        14 \n112.72306 128.23183 143.74060 143.74060 159.24937 159.24937 \n\n# 최소제곱잔차\n\nres_lm$residuals\n\n         1          2          3          4          5          6          7 \n 3.3295739 -6.1791980 -1.6879699 -2.1967419  7.8032581  5.2944862 -1.2142857 \n         8          9         10         11         12         13         14 \n-0.2142857 -3.7230576 -9.2318296  5.2593985  1.2593985 -5.2493734  6.7506266 \n\nresid(res_lm)\n\n         1          2          3          4          5          6          7 \n 3.3295739 -6.1791980 -1.6879699 -2.1967419  7.8032581  5.2944862 -1.2142857 \n         8          9         10         11         12         13         14 \n-0.2142857 -3.7230576 -9.2318296  5.2593985  1.2593985 -5.2493734  6.7506266 \n\nresiduals(res_lm)\n\n         1          2          3          4          5          6          7 \n 3.3295739 -6.1791980 -1.6879699 -2.1967419  7.8032581  5.2944862 -1.2142857 \n         8          9         10         11         12         13         14 \n-0.2142857 -3.7230576 -9.2318296  5.2593985  1.2593985 -5.2493734  6.7506266 \n\n\n\n\n\n\nplot(beta0_hat+beta1_hat*x,pch=19);\n\n#abline(beta0_hat,beta1_hat)\n\nabline(res_lm)"
  },
  {
    "objectID": "Regression_Analysis.html#section",
    "href": "Regression_Analysis.html#section",
    "title": "Regression Analysis",
    "section": "",
    "text": "data_2.5&lt;-read.table(\"All_Data/p031.txt\",header=TRUE,sep=\"\\t\")\n\nres_lm &lt;- lm(Minutes ~ Units, data = data_2.5)\n\nres_lm\n\n\nCall:\nlm(formula = Minutes ~ Units, data = data_2.5)\n\nCoefficients:\n(Intercept)        Units  \n      4.162       15.509  \n\nres_lm_summ&lt;-summary(res_lm)\n\nres_lm_summ #Pr(&gt;|t|) - p-value\n\n\nCall:\nlm(formula = Minutes ~ Units, data = data_2.5)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.2318 -3.3415 -0.7143  4.7769  7.8033 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    4.162      3.355    1.24    0.239    \nUnits         15.509      0.505   30.71 8.92e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.392 on 12 degrees of freedom\nMultiple R-squared:  0.9874,    Adjusted R-squared:  0.9864 \nF-statistic: 943.2 on 1 and 12 DF,  p-value: 8.916e-13\n\n# unit은 시간에 영향을준다 약15.5분 만큼씩 \n\n# coefficient에서 p-value에 대해서 알 수 있음 \n\n# beta_0는 0이라고 보면되느냐? p-value가 0.05보다 크기에"
  },
  {
    "objectID": "Regression_Analysis.html#section-1",
    "href": "Regression_Analysis.html#section-1",
    "title": "Regression Analysis",
    "section": "",
    "text": "confint(res_lm) # beta_0,1의 95% 신뢰구간을 뽑아줌 \n\n                2.5 %   97.5 %\n(Intercept) -3.148482 11.47179\nUnits       14.408512 16.60903\n\n?confint #level = 1-alpha\n\nstarting httpd help server ... done\n\nconfint(res_lm, level=0.90) # 90%의 신뢰구간\n\n                 5 %     95 %\n(Intercept) -1.81810 10.14141\nUnits       14.60875 16.40879\n\n\n\n\n\n\n# 4개의 고장난 부품을 수리하는 데에 걸리는 시간 예측\n\nx&lt;-4\n\n4.161654 + 15.508772 *4\n\n[1] 66.19674\n\nres_lm$coefficients[1]+(res_lm$coefficients[2]*x)\n\n(Intercept) \n   66.19674 \n\n### predict()\n\ndf&lt;-data.frame(Units=4) \n\npredict(res_lm,newdata=df) # res_lm을 만들때 사용한 데이터형식으로 만들어주어야함\n\n       1 \n66.19674 \n\nres_lm_pred&lt;-predict(res_lm,newdata=df,se.fit=TRUE)\n\n### 예측값\n\nres_lm_pred$fit\n\n       1 \n66.19674 \n\n### 표준오차\n\nres_lm_pred$se.fit # 평균반응에 대한 표준오차 \n\n[1] 1.759688\n\n### 예측한계\n\ndf&lt;-data.frame(Units=4) #예제서는 4대기준\n\ndf&lt;-data.frame(Units=1:10) #다른것도 보고 싶은경우 \n\nres_lm_pred_int_p&lt;-predict(res_lm,newdata=df,interval=\"prediction\")\n\n### 신뢰한계\n\ndf&lt;-data.frame(Units=1:10) #다른것도 보고 싶은경우 \n\nres_lm_pred_int_c&lt;-predict(res_lm,newdata=df,interval=\"confidence\") #둘의 차이를 보면 예측한계의 범위가 더큼 \n\n### 예측한계 & 신뢰한계\n\n# 신뢰한계는 평균에서 멀어지만 오차의범위가 커지고 평균에 다가갈수록 오차가 줄어듬\n\nplot(Minutes~Units,data=data_2.5,pch=19)\n\nabline(res_lm,col=\"red\",lwd=2)\n\nlines(1:10,res_lm_pred_int_p[,\"lwr\"],col=\"darkgreen\")\n\nlines(1:10,res_lm_pred_int_p[,\"upr\"],col=\"darkgreen\")\n\nlines(1:10,res_lm_pred_int_c[,\"lwr\"],col=\"blue\")\n\nlines(1:10,res_lm_pred_int_c[,\"upr\"],col=\"blue\")\n\n\n\n\n\n\n\n\nres_lm_summ&lt;-summary(res_lm)\n\nres_lm_summ #Multiple R-squared:0.9874 -&gt; 반응변수의 전체변이중 98.94%가 예측변수에 의해 설명된다\n\n\nCall:\nlm(formula = Minutes ~ Units, data = data_2.5)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.2318 -3.3415 -0.7143  4.7769  7.8033 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    4.162      3.355    1.24    0.239    \nUnits         15.509      0.505   30.71 8.92e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.392 on 12 degrees of freedom\nMultiple R-squared:  0.9874,    Adjusted R-squared:  0.9864 \nF-statistic: 943.2 on 1 and 12 DF,  p-value: 8.916e-13\n\n# 만약 R-squared가 1이면 완벽한 선형의 관계 100%라는 것을 의미한다.\n\n# R-squared는 변수가 들어갈수록 커지기에 adjust R-squared를 사용 추후 설명 \n\n\n\n\n\n# Minutes = beta1 + Units + epsilon\n\nres_lm_no&lt;-lm(Minutes~Units-1,data=data_2.5)\n\nres_lm_no\n\n\nCall:\nlm(formula = Minutes ~ Units - 1, data = data_2.5)\n\nCoefficients:\nUnits  \n16.07  \n\nsummary(res_lm_no)\n\n\nCall:\nlm(formula = Minutes ~ Units - 1, data = data_2.5)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5955 -2.4733  0.4417  5.0243  9.7023 \n\nCoefficients:\n      Estimate Std. Error t value Pr(&gt;|t|)    \nUnits  16.0744     0.2213   72.62   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.502 on 13 degrees of freedom\nMultiple R-squared:  0.9975,    Adjusted R-squared:  0.9974 \nF-statistic:  5274 on 1 and 13 DF,  p-value: &lt; 2.2e-16\n\ncoef(summary(res_lm_no)) #rsquared=0.9975\n\n      Estimate Std. Error  t value     Pr(&gt;|t|)\nUnits 16.07443  0.2213341 72.62519 2.380325e-18\n\n\n\n\n\n\ny&lt;-rnorm(30)\n\nt.test(y,mu=0)\n\n\n    One Sample t-test\n\ndata:  y\nt = -0.36971, df = 29, p-value = 0.7143\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.3936633  0.2731294\nsample estimates:\n  mean of x \n-0.06026695 \n\nsummary(lm(y~1))\n\n\nCall:\nlm(formula = y ~ 1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.7055 -0.5113 -0.1068  0.5783  1.5982 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -0.06027    0.16301   -0.37    0.714\n\nResidual standard error: 0.8929 on 29 degrees of freedom"
  },
  {
    "objectID": "Regression_Analysis.html#장-다중선형회귀",
    "href": "Regression_Analysis.html#장-다중선형회귀",
    "title": "Regression Analysis",
    "section": "",
    "text": "data_3.3&lt;-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\ndim(data_3.3)\n\n[1] 30  7\n\nclass(data_3.3)\n\n[1] \"data.frame\"\n\nsapply(data_3.3,class) #all numeric\n\n        Y        X1        X2        X3        X4        X5        X6 \n\"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \n\nsummary(data_3.3) #모든변수가 numeric이면 분위수도 보여준다 \n\n       Y               X1             X2              X3              X4       \n Min.   :40.00   Min.   :37.0   Min.   :30.00   Min.   :34.00   Min.   :43.00  \n 1st Qu.:58.75   1st Qu.:58.5   1st Qu.:45.00   1st Qu.:47.00   1st Qu.:58.25  \n Median :65.50   Median :65.0   Median :51.50   Median :56.50   Median :63.50  \n Mean   :64.63   Mean   :66.6   Mean   :53.13   Mean   :56.37   Mean   :64.63  \n 3rd Qu.:71.75   3rd Qu.:77.0   3rd Qu.:62.50   3rd Qu.:66.75   3rd Qu.:71.00  \n Max.   :85.00   Max.   :90.0   Max.   :83.00   Max.   :75.00   Max.   :88.00  \n       X5              X6       \n Min.   :49.00   Min.   :25.00  \n 1st Qu.:69.25   1st Qu.:35.00  \n Median :77.50   Median :41.00  \n Mean   :74.77   Mean   :42.93  \n 3rd Qu.:80.00   3rd Qu.:47.75  \n Max.   :92.00   Max.   :72.00  \n\n### 산점도 행렬\n\nplot(data_3.3)\n\n\n\n\n\n\n\n\nlm(Y~X1+X2+X3+X4+X5+X6,data=data_3.3)\n\n\nCall:\nlm(formula = Y ~ X1 + X2 + X3 + X4 + X5 + X6, data = data_3.3)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n   10.78708      0.61319     -0.07305      0.32033      0.08173      0.03838  \n         X6  \n   -0.21706  \n\nlm(Y~.,data=data_3.3) # X1+X2+X3+X4+X5+X6쓰는 것이 아니라 .을 써서 모든 변수를 다써줌 \n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n   10.78708      0.61319     -0.07305      0.32033      0.08173      0.03838  \n         X6  \n   -0.21706  \n\n\n\n\n\n\ndata_3.3&lt;-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\nlm(Y~X1+X2,data=data_3.3)\n\n\nCall:\nlm(formula = Y ~ X1 + X2, data = data_3.3)\n\nCoefficients:\n(Intercept)           X1           X2  \n   15.32762      0.78034     -0.05016  \n\n# (Intercept)         X1           X2  \n\n#  15.32762      0.78034     -0.05016\n\n# 1) Y에서 X1 효과 제거\n\nm1&lt;-lm(Y~X1,data=data_3.3) # y prime\n\nm1$residuals # x1이 설명하지 못한값 / x1의 효과를 제거한 값\n\n           1            2            3            4            5            6 \n -9.86142016   0.32865220   3.80099328  -0.91673799   7.76411473 -12.87985944 \n           7            8            9           10           11           12 \n -6.93517726   0.02794419  -4.25432454   6.59248165   9.62936020   7.34709147 \n          13           14           15           16           17           18 \n  7.83787183  -9.00893436   4.51872455  -1.29120309  -4.51815400   5.34709147 \n          19           20           21           22           23           24 \n -2.19900672  -8.14368889   5.43928784   3.59248165 -11.18056744  -2.29688270 \n          25           26           27           28           29           30 \n  7.87475038  -6.48127545   7.02794419  -9.38907907   6.48184600   5.74567546 \n\n# 2) X2에서 X1 효과 제거\n\nm2&lt;-lm(X2~X1,data=data_3.3) # x2 prime \n\nm2$residuals \n\n          1           2           3           4           5           6 \n-15.1300345  -0.7994502  13.1223579  -6.2864182  -2.9818979   1.8178376 \n          7           8           9          10          11          12 \n-11.3385461  -7.4428019  10.9659742  -5.2603543   6.8439016  -2.7473223 \n         13          14          15          16          17          18 \n  6.2266138  21.4529422  -4.4688659 -15.1382816   1.4268783  15.2526777 \n         19          20          21          22          23          24 \n -8.8776421  19.2787417  -6.4866827   1.7396457  -0.8255141   4.0524132 \n         25          26          27          28          29          30 \n -4.6691304   7.5311341   0.5571981  -4.2082264   8.4268783 -22.0340258 \n\n# 3) X1의 효과가 제거된 Y와 X2의 적합 - 원점을 지나는 회귀선\n\nlm(m1$residuals~m2$residuals-1) # 원점을 지나면 -1를 하고 진행 // -3.25e-17\n\n\nCall:\nlm(formula = m1$residuals ~ m2$residuals - 1)\n\nCoefficients:\nm2$residuals  \n    -0.05016  \n\n# 다른 효과 없이(다른값이 고정) Y에 영향을 주는 순수한 X2의 값\n\n# m2$residuals  : -0.05016  ==  X2 : -0.05016  \n\n### 단위길이 척도화 - 잘사용하지않음\n\nfn_scaling_len&lt;-function(x){\n\n  x0&lt;-x-mean(x)\n\n  x0/sqrt(sum(x0^2))\n\n}\n\ndata_3.3_len&lt;-sapply(data_3.3, fn_scaling_len)\n\ndata_3.3_len&lt;-data.frame(data_3.3_len)\n\nsummary(data_3.3_len)\n\n       Y                  X1                 X2                 X3           \n Min.   :-0.37579   Min.   :-0.41282   Min.   :-0.35109   Min.   :-0.353871  \n 1st Qu.:-0.08975   1st Qu.:-0.11297   1st Qu.:-0.12344   1st Qu.:-0.148193  \n Median : 0.01322   Median :-0.02231   Median :-0.02479   Median : 0.002109  \n Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.000000  \n 3rd Qu.: 0.10857   3rd Qu.: 0.14504   3rd Qu.: 0.14216   3rd Qu.: 0.164278  \n Max.   : 0.31070   Max.   : 0.32635   Max.   : 0.45328   Max.   : 0.294804  \n       X4                 X5                 X6          \n Min.   :-0.38637   Min.   :-0.48356   Min.   :-0.32367  \n 1st Qu.:-0.11401   1st Qu.:-0.10353   1st Qu.:-0.14318  \n Median :-0.02024   Median : 0.05130   Median :-0.03489  \n Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.00000  \n 3rd Qu.: 0.11371   3rd Qu.: 0.09821   3rd Qu.: 0.08693  \n Max.   : 0.41733   Max.   : 0.32341   Max.   : 0.52461  \n\nlm(Y~.,data=data_3.3_len)\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3_len)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n -1.259e-16    6.707e-01   -7.343e-02    3.089e-01    6.981e-02    3.120e-02  \n         X6  \n -1.835e-01  \n\n### 표준화\n\n# scale()\n\ndata_3.3_std&lt;-scale(data_3.3)\n\n#summary(data_3.3_std)\n\n#sapply(data_3.3_std, sd, na.rm=T)\n\n#class(data_3.3_std) #\"matrix\"\n\ndata_3.3_std&lt;-data.frame(data_3.3_std)\n\n#class(data_3.3_std) #\"data.frame\"\n\n#sapply(data_3.3_std, sd, na.rm=T)\n\nlm(Y~.,data=data_3.3_std) # beta게수 구하기 \n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3_std)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n -7.717e-16    6.707e-01   -7.343e-02    3.089e-01    6.981e-02    3.120e-02  \n         X6  \n -1.835e-01  \n\n\n\n\n\n\nres_lm&lt;-lm(Y~.,data=data_3.3)\n\nres_lm\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n   10.78708      0.61319     -0.07305      0.32033      0.08173      0.03838  \n         X6  \n   -0.21706  \n\nsummary(res_lm)\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.9418  -4.3555   0.3158   5.5425  11.5990 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10.78708   11.58926   0.931 0.361634    \nX1           0.61319    0.16098   3.809 0.000903 ***\nX2          -0.07305    0.13572  -0.538 0.595594    \nX3           0.32033    0.16852   1.901 0.069925 .  \nX4           0.08173    0.22148   0.369 0.715480    \nX5           0.03838    0.14700   0.261 0.796334    \nX6          -0.21706    0.17821  -1.218 0.235577    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.068 on 23 degrees of freedom\nMultiple R-squared:  0.7326,    Adjusted R-squared:  0.6628 \nF-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05\n\nm1&lt;-summary(lm(Y~X1+X2+X3+X4+X5+X6,data=data_3.3)) #Adjusted R-squared:  0.6628 \n\nm2&lt;-summary(lm(Y~X1+X2+X3+X4+X5,data=data_3.3)) #Adjusted R-squared:  0.6561 \n\n# X6가 들어가는 것이 더 좋은 모델 \n\nm1$adj.r.squared\n\n[1] 0.662846\n\nm2$adj.r.squared #summary에서 보다 더 정확하게 수치가 나옴 \n\n[1] 0.6560539\n\n\n\n\n\n\nres_lm_summ&lt;-summary(res_lm)\n\nres_lm_summ #p-value의 존재는 무언가를 검정했다라는 반증\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.9418  -4.3555   0.3158   5.5425  11.5990 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10.78708   11.58926   0.931 0.361634    \nX1           0.61319    0.16098   3.809 0.000903 ***\nX2          -0.07305    0.13572  -0.538 0.595594    \nX3           0.32033    0.16852   1.901 0.069925 .  \nX4           0.08173    0.22148   0.369 0.715480    \nX5           0.03838    0.14700   0.261 0.796334    \nX6          -0.21706    0.17821  -1.218 0.235577    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.068 on 23 degrees of freedom\nMultiple R-squared:  0.7326,    Adjusted R-squared:  0.6628 \nF-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05\n\n# p-value&lt;0.05 H_1 귀무가설 채택 \n\n# p-value&gt;0.05 H_0 영가설 채택 // X1을 제외하고는 영가설 유의한 의미가 없음(Y에영향주는)\n\n# 모두다 0이라는 가설을 가지고 분모 분자의 오차가 카이제곱을 따르고 거기서 나온 통계량\n\n# F-분포 자유도는 분자 분모 두개를 가짐 //모아서 계산을 하기에 각각 계산하는것과 결과다름 \n\n# 영가설-모든 회귀계수가 0이다.\n\n# 대립가설-적어도 하나는 0이 아니다. p-value: 1.24e-05 &lt;0.05 대립가설 채택 \n\n# p-value가 0.05보다 작으면 대립가설 채택!!!!!! 기억해 \n\n# 회귀계수에 대한 신뢰구간 - 95% 신뢰한계\n\nconfint(res_lm) #-13.18712881 ~ 34.7612816\n\n                   2.5 %     97.5 %\n(Intercept) -13.18712881 34.7612816\nX1            0.28016866  0.9462066\nX2           -0.35381806  0.2077178\nX3           -0.02827872  0.6689430\nX4           -0.37642935  0.5398936\nX5           -0.26570179  0.3424647\nX6           -0.58571106  0.1515977\n\n#X1  0.28016866  0.9462066  사이에 0이 들어가있으면 영향을 준다라느걸 의미\n\n#X2 -0.35381806  0.2077178  p-value없이도 알 수 있음 \n\n#X5가 가장 영향이 적음 p-value가 가장 크기에(영향 효과의 크기를 비교할때)\n\n#p-value가 작을 수록 영향을 많이 준다 beta값을 보는 것이 아닌 p-value를 보는 것 중요\n\n#가장 의미있는 변수?-&gt;p-value가장 작은거 // 대립가설채택 Y에 영향을 가장\n\n\n\n\n\n\n\n\n# H_0: beta_1:beta_6=0\n\nmodel_reduce&lt;-lm(Y~1,data=data_3.3)\n\nmodel_full&lt;-lm(Y~.,data=data_3.3)\n\nanova(model_reduce,model_full)\n\nAnalysis of Variance Table\n\nModel 1: Y ~ 1\nModel 2: Y ~ X1 + X2 + X3 + X4 + X5 + X6\n  Res.Df  RSS Df Sum of Sq      F   Pr(&gt;F)    \n1     29 4297                                 \n2     23 1149  6      3148 10.502 1.24e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#대립가설 = 완전모형이 적절하다 / 1.24e-05 *** &lt; 0.05 \n\n#의미 있는 예측 변수가 한개 이상 존재한다 \n\nsummary(model_full) #summary에서 beta_1~beta_6까지 모두가 0이라는 가설로 진행을 이미함\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.9418  -4.3555   0.3158   5.5425  11.5990 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10.78708   11.58926   0.931 0.361634    \nX1           0.61319    0.16098   3.809 0.000903 ***\nX2          -0.07305    0.13572  -0.538 0.595594    \nX3           0.32033    0.16852   1.901 0.069925 .  \nX4           0.08173    0.22148   0.369 0.715480    \nX5           0.03838    0.14700   0.261 0.796334    \nX6          -0.21706    0.17821  -1.218 0.235577    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.068 on 23 degrees of freedom\nMultiple R-squared:  0.7326,    Adjusted R-squared:  0.6628 \nF-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05\n\n#F-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05\n\n#예상// 가장의미있는변수? -&gt; X1 이유?-&gt; p-value 0.000903 로 가장 작기에 영향많이 줄것으로 예측 \n\n\n\n\n\nmodel_reduce&lt;-lm(Y~X1+X3,data=data_3.3)\n\nmodel_full&lt;-lm(Y~.,data=data_3.3)\n\nanova(model_reduce,model_full) #0.7158 &gt; 0.05\n\nAnalysis of Variance Table\n\nModel 1: Y ~ X1 + X3\nModel 2: Y ~ X1 + X2 + X3 + X4 + X5 + X6\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     27 1254.7                           \n2     23 1149.0  4    105.65 0.5287 0.7158\n\n#영가설은 H_0: b_2=b_4=b_5=b_6=0 이라는 사실을 알 수 있다 \n\n#b_1&b_3는 반응변수에 유의한 반응을 준다라는 것도 연계하여 알 수 있다 \n\n\n\n\n\n#해당 조건이 주어지고 만족할 때 beta_1=beta_3은 맞는가?\n\nmodel_reduce&lt;-lm(Y~I(X1-X3),data=data_3.3) #I를 씌우면 새로운 변수를 만든것과 동일\n\n# X1-X3를 한 그자체를 분석하라는 의미//본래는 X1-X3 해서 새로운 변수를 만들어서 해야함 \n\nmodel_full&lt;-lm(Y~X1+X3,data=data_3.3)\n\nanova(model_reduce,model_full) \n\nAnalysis of Variance Table\n\nModel 1: Y ~ I(X1 - X3)\nModel 2: Y ~ X1 + X3\n  Res.Df    RSS Df Sum of Sq     F    Pr(&gt;F)    \n1     28 3846.7                                 \n2     27 1254.6  1      2592 55.78 4.925e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#install.packages(\"car\")\n\nlibrary(car)\n\nLoading required package: carData\n\nmodel_full&lt;-lm(Y~X1+X3,data=data_3.3)\n\ncar::linearHypothesis(model_full,c(\"X1=X3\"))\n\nLinear hypothesis test\n\nHypothesis:\nX1 - X3 = 0\n\nModel 1: restricted model\nModel 2: Y ~ X1 + X3\n\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     28 1424.6                              \n2     27 1254.7  1    169.95 3.6572 0.06649 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n# H_0: beta_1+beta_3=1 | beta_2=beta_3:beta_6=0\n\nmodel_full&lt;-lm(Y~X1+X3,data=data_3.3)\n\ncar::linearHypothesis(model_full,c(\"X1 + X3 = 1\"))\n\nLinear hypothesis test\n\nHypothesis:\nX1  + X3 = 1\n\nModel 1: restricted model\nModel 2: Y ~ X1 + X3\n\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     28 1329.5                           \n2     27 1254.7  1    74.898 1.6118 0.2151\n\n# x1의 효과가 증가하면 x3의 효과는 감소한다 상대적인 관계 (반대로도 가능)\n\n\n\n\n\nmodel_full&lt;-lm(Y~.,data_3.3)\n\n# 예측값 - 적합값\n\nmodel_full$fitted.values\n\n       1        2        3        4        5        6        7        8 \n51.11030 61.35277 69.93944 61.22684 74.45380 53.94185 67.14841 70.09701 \n       9       10       11       12       13       14       15       16 \n79.53099 59.19846 57.92572 55.40103 59.58168 70.21401 76.54933 84.54785 \n      17       18       19       20       21       22       23       24 \n76.15013 61.39736 68.01656 55.62014 42.60324 63.81902 63.66400 44.62475 \n      25       26       27       28       29       30 \n57.31710 67.84347 75.14036 56.04535 77.66053 76.87850 \n\n# 예측한계(Prediction Limits)\n\npredict(model_full,newdata = data_3.3,interval = \"prediction\")\n\n        fit      lwr       upr\n1  51.11030 34.16999  68.05060\n2  61.35277 46.34536  76.36018\n3  69.93944 53.94267  85.93622\n4  61.22684 45.44586  77.00783\n5  74.45380 59.17630  89.73129\n6  53.94185 37.21813  70.66557\n7  67.14841 51.64493  82.65189\n8  70.09701 54.54384  85.65017\n9  79.53099 62.71383  96.34814\n10 59.19846 44.03506  74.36185\n11 57.92572 42.00674  73.84470\n12 55.40103 39.79333  71.00873\n13 59.58168 43.39853  75.76483\n14 70.21401 52.06636  88.36167\n15 76.54933 60.79444  92.30422\n16 84.54785 66.41374 102.68197\n17 76.15013 59.99991  92.30036\n18 61.39736 43.23384  79.56088\n19 68.01656 52.44673  83.58639\n20 55.62014 39.63744  71.60284\n21 42.60324 26.35046  58.85603\n22 63.81902 48.40145  79.23659\n23 63.66400 48.56222  78.76578\n24 44.62475 27.25435  61.99514\n25 57.31710 41.29380  73.34041\n26 67.84347 49.98605  85.70089\n27 75.14036 59.31975  90.96097\n28 56.04535 40.18723  71.90348\n29 77.66053 61.97564  93.34541\n30 76.87850 60.27441  93.48258\n\n# 신뢰한계(Confidence limits)\n\npredict(model_full,newdata = data_3.3,interval = \"confidence\")\n\n        fit      lwr      upr\n1  51.11030 42.55502 59.66557\n2  61.35277 57.97029 64.73524\n3  69.93944 63.44979 76.42909\n4  61.22684 55.28897 67.16471\n5  74.45380 70.02428 78.88332\n6  53.94185 45.82386 62.05984\n7  67.14841 61.99316 72.30367\n8  70.09701 64.79421 75.39980\n9  79.53099 71.22222 87.83975\n10 59.19846 55.18008 63.21683\n11 57.92572 51.63028 64.22116\n12 55.40103 49.94035 60.86171\n13 59.58168 52.64531 66.51805\n14 70.21401 59.46431 80.96372\n15 76.54933 70.68118 82.41748\n16 84.54785 73.82102 95.27468\n17 76.15013 69.29093 83.00933\n18 61.39736 50.62090 72.17383\n19 68.01656 62.66507 73.36805\n20 55.62014 49.16527 62.07502\n21 42.60324 35.50593 49.70055\n22 63.81902 58.92819 68.70985\n23 63.66400 59.88479 67.44321\n24 44.62475 35.24662 54.00288\n25 57.31710 50.76233 63.87187\n26 67.84347 57.59134 78.09561\n27 75.14036 69.09798 81.18275\n28 56.04535 49.90540 62.18531\n29 77.66053 71.98300 83.33806\n30 76.87850 69.00992 84.74707"
  },
  {
    "objectID": "Regression_Analysis.html#부록-행렬을-이용한-회귀계수-추정",
    "href": "Regression_Analysis.html#부록-행렬을-이용한-회귀계수-추정",
    "title": "Regression Analysis",
    "section": "",
    "text": "data_3.3&lt;-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\nY&lt;-data_3.3$Y\n\nX&lt;-data_3.3[,-1]\n\nX&lt;-cbind(1,X)\n\nX&lt;-as.matrix(X)\n\n#beta_hat&lt;-solve(t(X) %*% X) %*% t(X) %*% Y # %*%행렬 계산 \n\nP&lt;-solve(t(X) %*% X) %*% t(X)\n\nbeta_hat&lt;- P %*% Y\n\nlm(Y~.,data_3.3)\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n   10.78708      0.61319     -0.07305      0.32033      0.08173      0.03838  \n         X6  \n   -0.21706"
  },
  {
    "objectID": "Regression_Analysis.html#장-회귀진단-모형위반의-검출",
    "href": "Regression_Analysis.html#장-회귀진단-모형위반의-검출",
    "title": "Regression Analysis",
    "section": "",
    "text": "# 표준화 잔차\n\ndata_3.3&lt;-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\nres_lm&lt;-lm(Y~.,data=data_3.3)\n\nclass(res_lm)\n\n[1] \"lm\"\n\nmode(res_lm)\n\n[1] \"list\"\n\nnames(res_lm)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\nres_lm$fitted.values\n\n       1        2        3        4        5        6        7        8 \n51.11030 61.35277 69.93944 61.22684 74.45380 53.94185 67.14841 70.09701 \n       9       10       11       12       13       14       15       16 \n79.53099 59.19846 57.92572 55.40103 59.58168 70.21401 76.54933 84.54785 \n      17       18       19       20       21       22       23       24 \n76.15013 61.39736 68.01656 55.62014 42.60324 63.81902 63.66400 44.62475 \n      25       26       27       28       29       30 \n57.31710 67.84347 75.14036 56.04535 77.66053 76.87850 \n\nstr(res_lm)\n\nList of 12\n $ coefficients : Named num [1:7] 10.7871 0.6132 -0.0731 0.3203 0.0817 ...\n  ..- attr(*, \"names\")= chr [1:7] \"(Intercept)\" \"X1\" \"X2\" \"X3\" ...\n $ residuals    : Named num [1:30] -8.11 1.647 1.061 -0.227 6.546 ...\n  ..- attr(*, \"names\")= chr [1:30] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:30] -354.011 54.107 2.742 11.715 -0.971 ...\n  ..- attr(*, \"names\")= chr [1:30] \"(Intercept)\" \"X1\" \"X2\" \"X3\" ...\n $ rank         : int 7\n $ fitted.values: Named num [1:30] 51.1 61.4 69.9 61.2 74.5 ...\n  ..- attr(*, \"names\")= chr [1:30] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:7] 0 1 2 3 4 5 6\n $ qr           :List of 5\n  ..$ qr   : num [1:30, 1:7] -5.477 0.183 0.183 0.183 0.183 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:30] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ : chr [1:7] \"(Intercept)\" \"X1\" \"X2\" \"X3\" ...\n  .. ..- attr(*, \"assign\")= int [1:7] 0 1 2 3 4 5 6\n  ..$ qraux: num [1:7] 1.18 1 1.29 1.1 1.07 ...\n  ..$ pivot: int [1:7] 1 2 3 4 5 6 7\n  ..$ tol  : num 1e-07\n  ..$ rank : int 7\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 23\n $ xlevels      : Named list()\n $ call         : language lm(formula = Y ~ ., data = data_3.3)\n $ terms        :Classes 'terms', 'formula'  language Y ~ X1 + X2 + X3 + X4 + X5 + X6\n  .. ..- attr(*, \"variables\")= language list(Y, X1, X2, X3, X4, X5, X6)\n  .. ..- attr(*, \"factors\")= int [1:7, 1:6] 0 1 0 0 0 0 0 0 0 1 ...\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:7] \"Y\" \"X1\" \"X2\" \"X3\" ...\n  .. .. .. ..$ : chr [1:6] \"X1\" \"X2\" \"X3\" \"X4\" ...\n  .. ..- attr(*, \"term.labels\")= chr [1:6] \"X1\" \"X2\" \"X3\" \"X4\" ...\n  .. ..- attr(*, \"order\")= int [1:6] 1 1 1 1 1 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. ..- attr(*, \"predvars\")= language list(Y, X1, X2, X3, X4, X5, X6)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:7] \"numeric\" \"numeric\" \"numeric\" \"numeric\" ...\n  .. .. ..- attr(*, \"names\")= chr [1:7] \"Y\" \"X1\" \"X2\" \"X3\" ...\n $ model        :'data.frame':  30 obs. of  7 variables:\n  ..$ Y : num [1:30] 43 63 71 61 81 43 58 71 72 67 ...\n  ..$ X1: num [1:30] 51 64 70 63 78 55 67 75 82 61 ...\n  ..$ X2: num [1:30] 30 51 68 45 56 49 42 50 72 45 ...\n  ..$ X3: num [1:30] 39 54 69 47 66 44 56 55 67 47 ...\n  ..$ X4: num [1:30] 61 63 76 54 71 54 66 70 71 62 ...\n  ..$ X5: num [1:30] 92 73 86 84 83 49 68 66 83 80 ...\n  ..$ X6: num [1:30] 45 47 48 35 47 34 35 41 31 41 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language Y ~ X1 + X2 + X3 + X4 + X5 + X6\n  .. .. ..- attr(*, \"variables\")= language list(Y, X1, X2, X3, X4, X5, X6)\n  .. .. ..- attr(*, \"factors\")= int [1:7, 1:6] 0 1 0 0 0 0 0 0 0 1 ...\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:7] \"Y\" \"X1\" \"X2\" \"X3\" ...\n  .. .. .. .. ..$ : chr [1:6] \"X1\" \"X2\" \"X3\" \"X4\" ...\n  .. .. ..- attr(*, \"term.labels\")= chr [1:6] \"X1\" \"X2\" \"X3\" \"X4\" ...\n  .. .. ..- attr(*, \"order\")= int [1:6] 1 1 1 1 1 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(Y, X1, X2, X3, X4, X5, X6)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:7] \"numeric\" \"numeric\" \"numeric\" \"numeric\" ...\n  .. .. .. ..- attr(*, \"names\")= chr [1:7] \"Y\" \"X1\" \"X2\" \"X3\" ...\n - attr(*, \"class\")= chr \"lm\"\n\n# 잔차 \n\nres_lm$residuals\n\n          1           2           3           4           5           6 \n -8.1102953   1.6472337   1.0605589  -0.2268416   6.5462010 -10.9418499 \n          7           8           9          10          11          12 \n -9.1484140   0.9029929  -7.5309862   7.8015424   6.0742817  11.5989723 \n         13          14          15          16          17          18 \n  9.4183197  -2.2140147   0.4506705  -3.5478519  -2.1501319   3.6026355 \n         19          20          21          22          23          24 \n -3.0165587  -5.6201442   7.3967582   0.1809831 -10.6639999  -4.6247464 \n         25          26          27          28          29          30 \n  5.6828983  -1.8434727   2.8596385  -8.0453540   7.3394730   5.1215016 \n\nresid(res_lm) #실제값에서 예측된 값을 뺸값\n\n          1           2           3           4           5           6 \n -8.1102953   1.6472337   1.0605589  -0.2268416   6.5462010 -10.9418499 \n          7           8           9          10          11          12 \n -9.1484140   0.9029929  -7.5309862   7.8015424   6.0742817  11.5989723 \n         13          14          15          16          17          18 \n  9.4183197  -2.2140147   0.4506705  -3.5478519  -2.1501319   3.6026355 \n         19          20          21          22          23          24 \n -3.0165587  -5.6201442   7.3967582   0.1809831 -10.6639999  -4.6247464 \n         25          26          27          28          29          30 \n  5.6828983  -1.8434727   2.8596385  -8.0453540   7.3394730   5.1215016 \n\n### 내적 표준화잔차\n\nrstandard(res_lm)\n\n          1           2           3           4           5           6 \n-1.41498026  0.23955370  0.16744867 -0.03512080  0.97184596 -1.86133876 \n          7           8           9          10          11          12 \n-1.38317210  0.13709194 -1.29490454  1.14799070  0.95218982  1.76906521 \n         13          14          15          16          17          18 \n 1.51371017 -0.46212316  0.06961486 -0.73868563 -0.34446368  0.75418016 \n         19          20          21          22          23          24 \n-0.45861365 -0.88618779  1.19699287  0.02717120 -1.56184734 -0.85286680 \n         25          26          27          28          29          30 \n 0.89948517 -0.36581416  0.44430497 -1.25422677  1.12683185  0.85971512 \n\n### 외적 표준화잔차\n\nMASS::studres(res_lm)\n\n          1           2           3           4           5           6 \n-1.44835328  0.23458097  0.16386794 -0.03434974  0.97062209 -1.97526518 \n          7           8           9          10          11          12 \n-1.41280382  0.13413337 -1.31529351  1.15637546  0.95017640  1.86145176 \n         13          14          15          16          17          18 \n 1.56019127 -0.45407837  0.06809185 -0.73117411 -0.33776450  0.74689589 \n         19          20          21          22          23          24 \n-0.45059801 -0.88189556  1.20894332  0.02657438 -1.61559196 -0.84763116 \n         25          26          27          28          29          30 \n 0.89560731 -0.35881868  0.43641573 -1.27088888  1.13380428  0.85466249 \n\nredsid_df&lt;-data.frame(\n\n  Y=data_3.3$Y,\n\n  Y_hat=res_lm$fitted.values,\n\n  resid=resid(res_lm),\n\n  rstandard=rstandard(res_lm),\n\n  studres=MASS::studres(res_lm)\n\n)\n\nredsid_df\n\n    Y    Y_hat       resid   rstandard     studres\n1  43 51.11030  -8.1102953 -1.41498026 -1.44835328\n2  63 61.35277   1.6472337  0.23955370  0.23458097\n3  71 69.93944   1.0605589  0.16744867  0.16386794\n4  61 61.22684  -0.2268416 -0.03512080 -0.03434974\n5  81 74.45380   6.5462010  0.97184596  0.97062209\n6  43 53.94185 -10.9418499 -1.86133876 -1.97526518\n7  58 67.14841  -9.1484140 -1.38317210 -1.41280382\n8  71 70.09701   0.9029929  0.13709194  0.13413337\n9  72 79.53099  -7.5309862 -1.29490454 -1.31529351\n10 67 59.19846   7.8015424  1.14799070  1.15637546\n11 64 57.92572   6.0742817  0.95218982  0.95017640\n12 67 55.40103  11.5989723  1.76906521  1.86145176\n13 69 59.58168   9.4183197  1.51371017  1.56019127\n14 68 70.21401  -2.2140147 -0.46212316 -0.45407837\n15 77 76.54933   0.4506705  0.06961486  0.06809185\n16 81 84.54785  -3.5478519 -0.73868563 -0.73117411\n17 74 76.15013  -2.1501319 -0.34446368 -0.33776450\n18 65 61.39736   3.6026355  0.75418016  0.74689589\n19 65 68.01656  -3.0165587 -0.45861365 -0.45059801\n20 50 55.62014  -5.6201442 -0.88618779 -0.88189556\n21 50 42.60324   7.3967582  1.19699287  1.20894332\n22 64 63.81902   0.1809831  0.02717120  0.02657438\n23 53 63.66400 -10.6639999 -1.56184734 -1.61559196\n24 40 44.62475  -4.6247464 -0.85286680 -0.84763116\n25 63 57.31710   5.6828983  0.89948517  0.89560731\n26 66 67.84347  -1.8434727 -0.36581416 -0.35881868\n27 78 75.14036   2.8596385  0.44430497  0.43641573\n28 48 56.04535  -8.0453540 -1.25422677 -1.27088888\n29 85 77.66053   7.3394730  1.12683185  1.13380428\n30 82 76.87850   5.1215016  0.85971512  0.85466249\n\n\n\n\n\n\n\n\n\na&lt;-rnorm(100,70,10) #연속형 데이터\n\n# 히스토그램 \n\nhist(a)\n\n\n\nhist(a,breaks=5) #범위를 조절 막대의 5번 자름 \n\n\n\n# 줄기 잎 그림 \n\nstem(a)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  4 | 6\n  5 | 033\n  5 | 66678899\n  6 | 0011122333333344\n  6 | 555666677777888899999999\n  7 | 0000001111123444\n  7 | 556666778888999\n  8 | 0000122223344\n  8 | 5568\n\nstem(round(a)) #줄기잎을 그릴때는 반올림을 하고 항상 진행 \n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  4 | 6\n  5 | 033\n  5 | 66678899\n  6 | 0011122333333344\n  6 | 555666677777888899999999\n  7 | 0000001111123444\n  7 | 556666778888999\n  8 | 0000122223344\n  8 | 5568\n\nstem(round(a),scale=2) #scale을 2배로 늘려라 5기준으로 반으로 잘라서 \n\n\n  The decimal point is at the |\n\n  46 | 0\n  48 | \n  50 | 0\n  52 | 00\n  54 | \n  56 | 0000\n  58 | 0000\n  60 | 00000\n  62 | 000000000\n  64 | 00000\n  66 | 000000000\n  68 | 000000000000\n  70 | 00000000000\n  72 | 00\n  74 | 00000\n  76 | 000000\n  78 | 0000000\n  80 | 00000\n  82 | 000000\n  84 | 0000\n  86 | 0\n  88 | 0\n\n# 모든데이터를 볼 수있는 장점 데이터가 많으면 구림 \n\n# 점플롯\n\nidx&lt;-rep(1,length(a)) #a의 갯수에 맞춰서 1를 반복 \n\nplot(idx,a)\n\n\n\nplot(jitter(idx),a,xlim=c(0.5,1.5))\n\n\n\n# 상자그림\n\nboxplot(a) #사분위수에 대해서 알 수 있음 \n\n# 상자그림 + 점플롯\n\nboxplot(a)\n\npoints(jitter(idx),a)\n\n\n\n\n\n\n\n\ndata_4.1&lt;-read.table(\"All_Data/p103.txt\",header=T,sep=\"\\t\")\n\ndata_4.1\n\n       Y   X1   X2\n1  12.37 2.23 9.66\n2  12.66 2.57 8.94\n3  12.00 3.87 4.40\n4  11.93 3.10 6.64\n5  11.06 3.39 4.91\n6  13.03 2.83 8.52\n7  13.13 3.02 8.04\n8  11.44 2.14 9.05\n9  12.86 3.04 7.71\n10 10.84 3.26 5.11\n11 11.20 3.39 5.05\n12 11.56 2.35 8.51\n13 10.83 2.76 6.59\n14 12.63 3.90 4.90\n15 12.46 3.16 6.96\n\nclass(data_4.1) #data.frame\n\n[1] \"data.frame\"\n\n# 산점도 행렬\n\nplot(data_4.1)\n\ncor(data_4.1) #상관계수\n\n             Y           X1         X2\nY  1.000000000  0.002497966  0.4340688\nX1 0.002497966  1.000000000 -0.8997765\nX2 0.434068758 -0.899776481  1.0000000\n\npairs(data_4.1)\n\n\n\n# Correlation panel\n\npanel.cor&lt;-function(x,y){\n\n  par(usr=c(0,1,0,1))\n\n  r&lt;-round(cor(x,y),digits = 3)\n\n  text(0.5,0.5,r,cex=1.5)\n\n}\n\npairs(data_4.1,lower.panel = panel.cor)\n\n\n\n# 회전도표, 동적 그래프(3차원)\n\n#install.packages(\"rgl\")\n\nlibrary(rgl)\n\nplot3d(x=data_4.1$X1,y=data_4.1$X2,z=data_4.1$Y) \n\n\n\n\n\ndata_3.3&lt;-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\nres_lm&lt;-lm(Y~X1+X3,data=data_3.3)\n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(res_lm) #회귀진단 플랏이 나옴 \n\n\n\nlayout(1)\n\n# 1. 표준화잔차의 정규확률분포\n\nplot(res_lm,2)\n\n\n\n# 2. 표준화잔차 대 각 예측변수들의 산점도\n\nplot(res_lm,3) #이런경우에는 데이터를 추가한다 좌측하단이 없어서 \n\n\n\n# 3. 표준화잔차 대 적합값의 플롯\n\n# 4. 표준화잔차의 인덱스 플롯"
  },
  {
    "objectID": "Regression_Analysis.html#선형모형-2",
    "href": "Regression_Analysis.html#선형모형-2",
    "title": "Regression Analysis",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:car':\n\n    recode\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n\n\ndata_3.3&lt;-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\nres_lm&lt;-lm(Y~X1+X3,data=data_3.3) #두개의 변수만 의미있다고 가정하고 진행 \n\n# 회귀진단 그래프들\n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(res_lm) #회귀진단 플랏이 나옴 / 총6개임 \n\n\n\nlayout(1) #다시 한개의 플랏만 그리도록 \n\n# 1. 표준화잔차의 정규확률분포\n\nplot(res_lm,2) #QQ-plot y=x 기울기의 직선위에 점들이 있어야 한다 눈대중으로 \n\n\n\n# 2. 표준화잔차 대 각 예측변수들의 산점도\n\nplot(res_lm,3) #이런경우에는 데이터를 추가한다 좌측하단이 없어서 \n\n\n\n#랜덤하게 데이터가 흩어져 있어야 한다\n\n# 내적 표준화잔차\n\nplot(data_3.3$X1,rstandard(res_lm))\n\n\n\nplot(data_3.3$X3,rstandard(res_lm)) #각각의 잔차들이 랜덤하게 잘 퍼져야함 \n\n\n\n# 3. 표준화잔차 대 적합값의 플롯\n\nplot(res_lm,1) #잔차와 적합값은 상관성이 없어야하며 랜덤하게 퍼져야함 \n\n\n\n# 4. 표준화잔차의 인덱스 플롯 \n\nplot(res_lm,5)\n\n\n\ndata_2.4&lt;-read.table(\"All_Data/p029b.txt\",header=T,sep=\"\\t\")\n\nm&lt;-matrix(1:4,ncol=2,byrow=TRUE)  \n\nlayout(m)\n\nplot(Y1~X1, data=data_2.4,pch=19); abline(3,0.5)\n\nplot(Y2~X2, data=data_2.4,pch=19); abline(3,0.5)\n\nplot(Y3~X3, data=data_2.4,pch=19); abline(3,0.5)\n\nplot(Y4~X4, data=data_2.4,pch=19); abline(3,0.5)\n\n\n\nlayout(1)\n\n\n\n\n\nres_lm&lt;-lm(Y1~X1,data=data_2.4) \n\n#1번플랏은 적당히 잘퍼짐,2번플랏은 어느정도 선형성 있음(데이터적어서그런거임)\n\nres_lm&lt;-lm(Y2~X2,data=data_2.4)\n\nres_lm&lt;-lm(Y3~X3,data=data_2.4)\n\nres_lm&lt;-lm(Y4~X4,data=data_2.4)\n\nres_lm\n\n\nCall:\nlm(formula = Y4 ~ X4, data = data_2.4)\n\nCoefficients:\n(Intercept)           X4  \n     3.0017       0.4999  \n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(res_lm) \n\nWarning: not plotting observations with leverage one:\n  8\n\n\n\n\nlayout(1)\n\n\n\n\n\n# 사례: 뉴욕 강 데이터\n\n# agr-농업, forest-숲, rsdntial-주거, comlndl-산업, nitrogen-질소\n\ndata_1.9&lt;-read.table(\"All_Data/p010.txt\",header=T,sep=\"\\t\")\n\nhead(data_1.9)\n\n       River Agr Forest Rsdntial ComIndl Nitrogen\n1      Olean  26     63      1.2    0.29     1.10\n2  Cassadaga  29     57      0.7    0.09     1.01\n3      Oatka  54     26      1.8    0.58     1.90\n4  Neversink   2     84      1.9    1.98     1.00\n5 Hackensack   3     27     29.4    3.11     1.99\n6  Wappinger  19     61      3.4    0.56     1.42\n\nplot(data_1.9[-1],pch=19) #river라는 첫번째 컬럼을 제외하고 진행 \n\n\n\nres_1&lt;-lm(Nitrogen~.,data=data_1.9[-1]) #모든데이터 사용\n\nres_2&lt;-lm(Nitrogen~.,data=data_1.9[-4,-1]) #4번쨰 데이터 제외\n\nres_3&lt;-lm(Nitrogen~.,data=data_1.9[-5,-1]) #5번쨰 데이터 제외 \n\n#회귀계수\n\ndata.frame(all=coef(res_1),\n\n           rm4=coef(res_2),\n\n           rm5=coef(res_3))\n\n                     all          rm4          rm5\n(Intercept)  1.722213529  1.099471134  1.626014115\nAgr          0.005809126  0.010136685  0.002352222\nForest      -0.012967887 -0.007589231 -0.012760349\nRsdntial    -0.007226768 -0.123792917  0.181160986\nComIndl      0.305027765  1.528956204  0.075617570\n\n#p-value\n\ndata.frame(all=coef(summary(res_1))[,4],\n\n           rm4=coef(summary(res_2))[,4],\n\n           rm5=coef(summary(res_3))[,4])\n\n                   all          rm4        rm5\n(Intercept) 0.18316946 0.2477883387 0.05619948\nAgr         0.70462624 0.3717054741 0.80880737\nForest      0.36667966 0.4700975391 0.16975563\nRsdntial    0.83372002 0.0071342930 0.00112280\nComIndl     0.08230952 0.0005512227 0.51774981\n\n#4,5번째 데이터를 각각 뺴고 진행을 해보니 영향을 끼치는 값임을 알수있고\n\n#5번째는 주거 관련해서는 부호를 바꿀 정도로 강력하다 \n\n# 단순선형회구모형\n\nres&lt;-lm(Nitrogen~ComIndl,data=data_1.9)\n\n\n\n\n\nplot(Nitrogen~ComIndl,data=data_1.9,pch=19)\n\nabline(res)\n\n#leverage values 지레값\n\np_ii&lt;-hatvalues(res)\n\nhiegh_leverage&lt;-ifelse(p_ii&gt;2*2/30,data_1.9$River,\"\")\n\nhiegh_leverage #높은 지레값을 가지고 있는 강의 이름을 표시(이는 보기에 편하기 위해서함)\n\n           1            2            3            4            5            6 \n          \"\"           \"\"           \"\"  \"Neversink\" \"Hackensack\"           \"\" \n           7            8            9           10           11           12 \n          \"\"           \"\"           \"\"           \"\"           \"\"           \"\" \n          13           14           15           16           17           18 \n          \"\"           \"\"           \"\"           \"\"           \"\"           \"\" \n          19           20 \n          \"\"           \"\" \n\ntext(data_1.9$ComIndl,data_1.9$Nitrogen-0.1,hiegh_leverage)\n\n\n\n\n\n\n\n\nplot(rstandard(res),pch=19) #2또는 3시그마를 넘으면 특이값이라 함 \n\n\n\n\n\n\n\n\nplot(p_ii,pch=19) #평균의 2배를 기준으로 비교함 \n\nabline(h=2*2/30,col=\"red\") #이보다 높은것이 지레값이 높은것 높은 영향력을 가진것 \n\n\n\n# 단순선형회귀모형\n\nres&lt;-lm(Nitrogen~ComIndl,data=data_1.9)\n\nplot(Nitrogen~ComIndl,data=data_1.9,pch=19)\n\nabline(res)\n\n\n\nres&lt;-lm(Nitrogen~ComIndl,data=data_1.9[-4,-1])\n\nplot(Nitrogen~ComIndl,data=data_1.9[-4,-1],pch=19)\n\nabline(res) #4번째 제외\n\n\n\nres&lt;-lm(Nitrogen~ComIndl,data=data_1.9[-5,-1])\n\nplot(Nitrogen~ComIndl,data=data_1.9[-5,-1],pch=19)\n\nabline(res) #5번쨰 제외\n\n\n\n#이처럼 4,5번을 빼고 진행을 하면 조금더 잘 나타냄\n\nres&lt;-lm(Nitrogen~ComIndl,data=data_1.9[c(-4,-5),-1])\n\nplot(Nitrogen~ComIndl,data=data_1.9[-5,-1],pch=19)\n\nabline(res) #4,5번째 제외 \n\n\n\n\n\n\n\n\n\n\n\nres&lt;-lm(Nitrogen~ComIndl,data=data_1.9)\n\nplot(res,4)\n\n#install.packages(\"olsrr\")\n\nlibrary(olsrr)\n\n\nAttaching package: 'olsrr'\n\n\nThe following object is masked from 'package:datasets':\n\n    rivers\n\n\n\n\nols_plot_cooksd_chart(res)\n\n\n\n\n\n\n\n\nolsrr::ols_plot_dffits(res)\n\n\n\n\n\n\n\n\nolsrr::ols_plot_hadi(res)\n\n\n\n# Residual & Leverage & Cook's distance\n\nplot(res,5) #영향력 관측치를 보기 위한 플랏 \n\n\n\n\n\n\n\n\nolsrr::ols_plot_resid_pot(res) #2.0에 있는 값외에도 x축 0.2이상의 것들도 특이값으로 \n\n\n\n#지레값이 커도 영향력이 없는 애들은 신경 안써도 되나 영향력이 큰애들을 탐색해 보아야 함\n\n\n\n\n\n#특이값이 큰경우 그것이 TRUE데이터면 오류가 있는 경우 수정을 하거나 가중치를 변화를\n\n#하거나 데이터를 수정을 시켜주거나 다시 실험을 하는 등 여러가지 방법을 사용하여.. \n\n## 첨가변수플롯(added-variable plot), 편회귀플롯(partial regression plot)\n\nres&lt;-lm(Nitrogen~ComIndl,data=data_1.9)\n\ncar::avPlots(res,pch=19)\n\n\n\nres&lt;-lm(Nitrogen~.,data=data_1.9[-1])\n\ncar::avPlots(res,pch=19)\n\n\n\n#beta별로 각각의 어떤 변수가 영향력을 많이 주는지 알게하는 함수 \n\n\n\n\n\ndata_4.5&lt;-read.table(\"All_Data/p120.txt\",header=T,sep=\"\\t\")\n\ndim(data_4.5)\n\n[1] 35  4\n\nnames(data_4.5)\n\n[1] \"Hill.Race\" \"Time\"      \"Distance\"  \"Climb\"    \n\nhead(data_4.5)\n\n                    Hill.Race Time Distance Climb\n1 Greenmantle New Year Dash    965      2.5   650\n2                  Carnethy   2901      6.0  2500\n3              Craig Dunain   2019      6.0   900\n4                   Ben Rha   2736      7.5   800\n5                Ben Lomond   3736      8.0  3070\n6                  Goatfell   4393      8.0  2866\n\n# 회전도표 \n\nlibrary(rgl)\n\nwith(data_4.5,plot3d(x=Distance,y=Climb,z=Time))\n\nres&lt;-lm(Time~Distance+Climb,data=data_4.5)\n\nsummary(res) #beta_0가 마이너스여도 신경안씀 관심있는 것은 회귀계수임 \n\n\nCall:\nlm(formula = Time ~ Distance + Climb, data = data_4.5)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-973.0 -427.7  -71.2  142.2 3907.3 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -539.4829   258.1607  -2.090   0.0447 *  \nDistance     373.0727    36.0684  10.343 9.86e-12 ***\nClimb          0.6629     0.1231   5.387 6.44e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 880.5 on 32 degrees of freedom\nMultiple R-squared:  0.9191,    Adjusted R-squared:  0.914 \nF-statistic: 181.7 on 2 and 32 DF,  p-value: &lt; 2.2e-16\n\n#Time=-539.4829+373.0727*Distance+0.6629*Climb \n\n\n### 첨가변수플롯(added-variable plot), 편회귀플롯(partial regression plot)\n\ncar::avPlots(res,pch=19)\n\n\n\n### 성분잔차플롯(component plus residual plots), 편자차플롯(partial residual plot)\n\ncar::crPlots(res,id=T,pch=19) #첨가변수보다 성분잔차를 더 많이 사용 / 비선형여부를 확인\n\n\n\n# 점선에 비해서 분홍선이 크게 떨어져 있지 않아 선형적인 추세를 가지고 있다고 추정이 가능 \n\n### 잠재성-잔차플롯\n\nolsrr::ols_plot_resid_pot(res)\n\n\n\n### Hadi의 영향력 측도\n\nolsrr::ols_plot_hadi(res)\n\n\n\n### Cook의 거리\n\nolsrr::ols_plot_cooksd_chart(res) #전체적은 플롯을 보면서 이상치에 대한 확인을 함 \n\n\n\n# 이러한 값들의 제외 여부는 연구자가 선택해서 진행을 함 \n\n# outlier에 대한 처리를 어떻게 했다고 말을 해야함 \n\n# 보고서는 옆에 사람이 보고 쉽게 따라할 수 있을 정도로 \n\n# 어떤 속성으로 어떻게 진행을 했다라는 것을 중시 코드 보단 결과를 보여줘라 \n\n\n\n\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:olsrr':\n\n    cement\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nres&lt;-lm(Time~Distance+Climb,data=data_4.5)\n\nres\n\n\nCall:\nlm(formula = Time ~ Distance + Climb, data = data_4.5)\n\nCoefficients:\n(Intercept)     Distance        Climb  \n  -539.4829     373.0727       0.6629  \n\nres_rlm&lt;-MASS::rlm(Time~Distance+Climb,data=data_4.5)\n\nres_rlm\n\nCall:\nrlm(formula = Time ~ Distance + Climb, data = data_4.5)\nConverged in 10 iterations\n\nCoefficients:\n (Intercept)     Distance        Climb \n-576.3836570  393.0374614    0.4977894 \n\nDegrees of freedom: 35 total; 32 residual\nScale estimate: 313 \n\nsummary(res)\n\n\nCall:\nlm(formula = Time ~ Distance + Climb, data = data_4.5)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-973.0 -427.7  -71.2  142.2 3907.3 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -539.4829   258.1607  -2.090   0.0447 *  \nDistance     373.0727    36.0684  10.343 9.86e-12 ***\nClimb          0.6629     0.1231   5.387 6.44e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 880.5 on 32 degrees of freedom\nMultiple R-squared:  0.9191,    Adjusted R-squared:  0.914 \nF-statistic: 181.7 on 2 and 32 DF,  p-value: &lt; 2.2e-16\n\nsummary(res_rlm)\n\n\nCall: rlm(formula = Time ~ Distance + Climb, data = data_4.5)\nResiduals:\n     Min       1Q   Median       3Q      Max \n-645.074 -197.082   -2.035  212.266 3942.045 \n\nCoefficients:\n            Value     Std. Error t value  \n(Intercept) -576.3837  105.2774    -5.4749\nDistance     393.0375   14.7086    26.7216\nClimb          0.4978    0.0502     9.9200\n\nResidual standard error: 312.6 on 32 degrees of freedom\n\n#install.packages(\"robustbase\")\n\nlibrary(robustbase)\n\nres_lmrob&lt;-lmrob(Time~Distance+Climb,data=data_4.5)\n\nres_lmrob\n\n\nCall:\nlmrob(formula = Time ~ Distance + Climb, data = data_4.5)\n \\--&gt; method = \"MM\"\nCoefficients:\n(Intercept)     Distance        Climb  \n  -487.3793     398.2784       0.3901  \n\nsummary(res_lmrob)\n\n\nCall:\nlmrob(formula = Time ~ Distance + Climb, data = data_4.5)\n \\--&gt; method = \"MM\"\nResiduals:\n    Min      1Q  Median      3Q     Max \n-650.01 -160.60   23.37  216.11 3875.00 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -487.37929   86.33384  -5.645 3.04e-06 ***\nDistance     398.27843    5.98522  66.544  &lt; 2e-16 ***\nClimb          0.39013    0.04165   9.368 1.09e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nRobust residual standard error: 290.8 \nMultiple R-squared:  0.9868,    Adjusted R-squared:  0.986 \nConvergence in 9 IRWLS iterations\n\nRobustness weights: \n 3 observations c(7,18,33) are outliers with |weight| = 0 ( &lt; 0.0029); \n 2 weights are ~= 1. The remaining 30 ones are summarized as\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.5965  0.9098  0.9623  0.9214  0.9875  0.9990 \nAlgorithmic parameters: \n       tuning.chi                bb        tuning.psi        refine.tol \n        1.548e+00         5.000e-01         4.685e+00         1.000e-07 \n          rel.tol         scale.tol         solve.tol          zero.tol \n        1.000e-07         1.000e-10         1.000e-07         1.000e-10 \n      eps.outlier             eps.x warn.limit.reject warn.limit.meanrw \n        2.857e-03         1.364e-08         5.000e-01         5.000e-01 \n     nResample         max.it       best.r.s       k.fast.s          k.max \n           500             50              2              1            200 \n   maxit.scale      trace.lev            mts     compute.rd fast.s.large.n \n           200              0           1000              0           2000 \n                  psi           subsampling                   cov \n           \"bisquare\"         \"nonsingular\"         \".vcov.avar1\" \ncompute.outlier.stats \n                 \"SM\" \nseed : int(0) \n\n#standard error가 res &gt; res_rlm &gt; res_lmrob 순으로 되어 있음 \n\n# 4장 연습문제 해보기"
  },
  {
    "objectID": "Regression_Analysis.html#regression-analysis",
    "href": "Regression_Analysis.html#regression-analysis",
    "title": "Regression Analysis",
    "section": "",
    "text": "library(dplyr)"
  },
  {
    "objectID": "Regression_Analysis.html#장-질적-예측-변수",
    "href": "Regression_Analysis.html#장-질적-예측-변수",
    "title": "Regression Analysis",
    "section": "",
    "text": "#install.packages(\"fastDummies\")\n\nlibrary(fastDummies)\n\nThank you for using fastDummies!\n\n\nTo acknowledge our work, please cite the package:\n\n\nKaplan, J. & Schlegel, B. (2023). fastDummies: Fast Creation of Dummy (Binary) Columns and Rows from Categorical Variables. Version 1.7.1. URL: https://github.com/jacobkap/fastDummies, https://jacobkap.github.io/fastDummies/."
  },
  {
    "objectID": "Regression_Analysis.html#급료조사-데이터",
    "href": "Regression_Analysis.html#급료조사-데이터",
    "title": "Regression Analysis",
    "section": "",
    "text": "data_5.1&lt;-read.table(\"All_Data/p130.txt\",header=T,sep=\"\\t\")\n\n# S:급료 X:경력 E:교육수준 M:관리(형태) / E,M은 범주형 변수\n\nnames(data_5.1)\n\n[1] \"S\" \"X\" \"E\" \"M\"\n\n# 범주형 질적 변수를 수치형으로 변형시켜서 예측하는데 사용한것이 질적 예측변수이다 \n\n# E_1,E_2,E_3이런식으로 나누어서 0,1로 분류를 한다 (이것이 가변수)\n\n# 더미변수를 만들 경우에는 역행렬의 조건에 의해서 -1개의 변수만 만들면 된다 \n\n# 이는 공산성의 문제또한 있기에 이를 위해서 -1를 한것임 \n\n### 자료형 변경 : 정수 -&gt; 범주\n\ndata_5.1$E&lt;-as.factor(data_5.1$E)\n\ndata_5.1$M&lt;-as.factor(data_5.1$M)\n\nhead(data_5.1)\n\n      S X E M\n1 13876 1 1 1\n2 11608 1 3 0\n3 18701 1 3 1\n4 11283 1 2 0\n5 11767 1 3 0\n6 20872 2 2 1\n\ndata_5.1$E #Levels: 1 2 3이라는 것이 생김 문자로 처리한다는 의미 \n\n [1] 1 3 3 2 3 2 2 1 3 2 1 2 3 1 3 3 2 2 3 1 1 3 2 2 1 2 1 3 1 1 2 3 2 2 1 2 3 1\n[39] 2 2 3 2 2 1 2 1\nLevels: 1 2 3\n\n### 가변수 생성\n\ndata_5.1$E&lt;-factor(as.character(data_5.1$E),levels = c(\"3\",\"1\",\"2\")) \n\n#3번을 베이스 카테고리로 쓰기위한 설정 / 설정을 안하면 베이스는 e_1이 된다\n\ndata_5.1$M&lt;-factor(as.character(data_5.1$M),levels = c(\"0\",\"1\")) \n\ndata_dummy&lt;-dummy_cols(data_5.1,\n\n                       select_columns = c(\"E\",\"M\"),\n\n                       remove_first_dummy = T,\n\n                       remove_selected_columns = T) #첫번째 생성되는 더미 변수를 제거\n\ndata_dummy #가변수의 더미는 n-1개를 하는 것이 역행렬을 위한 것이기에 지워준다 \n\n       S  X E_1 E_2 M_1\n1  13876  1   1   0   1\n2  11608  1   0   0   0\n3  18701  1   0   0   1\n4  11283  1   0   1   0\n5  11767  1   0   0   0\n6  20872  2   0   1   1\n7  11772  2   0   1   0\n8  10535  2   1   0   0\n9  12195  2   0   0   0\n10 12313  3   0   1   0\n11 14975  3   1   0   1\n12 21371  3   0   1   1\n13 19800  3   0   0   1\n14 11417  4   1   0   0\n15 20263  4   0   0   1\n16 13231  4   0   0   0\n17 12884  4   0   1   0\n18 13245  5   0   1   0\n19 13677  5   0   0   0\n20 15965  5   1   0   1\n21 12336  6   1   0   0\n22 21352  6   0   0   1\n23 13839  6   0   1   0\n24 22884  6   0   1   1\n25 16978  7   1   0   1\n26 14803  8   0   1   0\n27 17404  8   1   0   1\n28 22184  8   0   0   1\n29 13548  8   1   0   0\n30 14467 10   1   0   0\n31 15942 10   0   1   0\n32 23174 10   0   0   1\n33 23780 10   0   1   1\n34 25410 11   0   1   1\n35 14861 11   1   0   0\n36 16882 12   0   1   0\n37 24170 12   0   0   1\n38 15990 13   1   0   0\n39 26330 13   0   1   1\n40 17949 14   0   1   0\n41 25685 15   0   0   1\n42 27837 16   0   1   1\n43 18838 16   0   1   0\n44 17483 16   1   0   0\n45 19207 17   0   1   0\n46 19346 20   1   0   0\n\n# 더미를 만든 이후 더미의 모체 변수인 E M을 지워주어야 한다 \n\n### 회귀분석(1) - 가변수\n\nres&lt;-lm(S~.,data = data_dummy)\n\nres \n\n\nCall:\nlm(formula = S ~ ., data = data_dummy)\n\nCoefficients:\n(Intercept)            X          E_1          E_2          M_1  \n    11031.8        546.2      -2996.2        147.8       6883.5  \n\n### 회귀분석(2) - lm()\n\nres_1&lt;-lm(S~.,data=data_5.1)\n\nres_1 \n\n\nCall:\nlm(formula = S ~ ., data = data_5.1)\n\nCoefficients:\n(Intercept)            X           E1           E2           M1  \n    11031.8        546.2      -2996.2        147.8       6883.5  \n\n#더미변수를 많이 쓰기에 factor로 바꾸어주고 분석하면 알아서 더미변수를 만들어서 진행함\n\nsummary(res_1)\n\n\nCall:\nlm(formula = S ~ ., data = data_5.1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1884.60  -653.60    22.23   844.85  1716.47 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11031.81     383.22  28.787  &lt; 2e-16 ***\nX             546.18      30.52  17.896  &lt; 2e-16 ***\nE1          -2996.21     411.75  -7.277 6.72e-09 ***\nE2            147.82     387.66   0.381    0.705    \nM1           6883.53     313.92  21.928  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1027 on 41 degrees of freedom\nMultiple R-squared:  0.9568,    Adjusted R-squared:  0.9525 \nF-statistic: 226.8 on 4 and 41 DF,  p-value: &lt; 2.2e-16\n\n#E_3와 M_0는 Intercept에 포함이 되어있음 그렇기에 베이스 카테고리라고 한다 \n\n#E가 3이고 M이 0이면 대학원이상 일반관리 직급 -&gt; Intercept(Beta_0) + X*Beta_1 = Y\n\n### 표준화잔차 대 경력연수\n\nplot(data_5.1$X, rstandard(res_1),pch=19,xlab=\"범주\",ylab=\"잔차\")\n\n\n\n# 0을 중심으로 잘 퍼져있는가를 확인해야함 \n\n#### 표준화잔차 대 교육수준 - 관리 조합\n\nEM&lt;-paste0(data_5.1$E,data_5.1$M)\n\nplot(EM,rstandard(res_1),pch=19,xlabl=\"범주\",ylab=\"잔차\")\n\nWarning in plot.window(...): \"xlabl\" is not a graphical parameter\n\n\nWarning in plot.xy(xy, type, ...): \"xlabl\" is not a graphical parameter\n\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"xlabl\" is not a\ngraphical parameter\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"xlabl\" is not a\ngraphical parameter\n\n\nWarning in box(...): \"xlabl\" is not a graphical parameter\n\n\nWarning in title(...): \"xlabl\" is not a graphical parameter\n\n\n\n\n### 상호작용 효과(Interaction Effect)\n\nres&lt;-lm(S~X+E+M+E*M,data=data_5.1)\n\nres\n\n\nCall:\nlm(formula = S ~ X + E + M + E * M, data = data_5.1)\n\nCoefficients:\n(Intercept)            X           E1           E2           M1        E1:M1  \n    11203.4        497.0      -1730.7       -349.1       7047.4      -3066.0  \n      E2:M1  \n     1836.5  \n\nsummary(res)\n\n\nCall:\nlm(formula = S ~ X + E + M + E * M, data = data_5.1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-928.13  -46.21   24.33   65.88  204.89 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11203.434     79.065 141.698  &lt; 2e-16 ***\nX             496.987      5.566  89.283  &lt; 2e-16 ***\nE1          -1730.748    105.334 -16.431  &lt; 2e-16 ***\nE2           -349.078     97.568  -3.578 0.000945 ***\nM1           7047.412    102.589  68.695  &lt; 2e-16 ***\nE1:M1       -3066.035    149.330 -20.532  &lt; 2e-16 ***\nE2:M1        1836.488    131.167  14.001  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 173.8 on 39 degrees of freedom\nMultiple R-squared:  0.9988,    Adjusted R-squared:  0.9986 \nF-statistic:  5517 on 6 and 39 DF,  p-value: &lt; 2.2e-16\n\n### 표준화잔차 대 경력연수\n\nplot(data_5.1$X,rstandard(res),pch=19,xlab=\"범주\",ylab=\"잔차\")\n\n\n\n### Cook의 거리\n\nplot(res,4) #33번째 데이터만 제외하고 다시 회귀모형을 생성예정\n\n\n\n### 상호작용 효과 - 관측개체 33 제외 \n\ndata_use&lt;-data_5.1[-33,]\n\nres&lt;-lm(S~X+E+M+E*M,data=data_use)\n\nres\n\n\nCall:\nlm(formula = S ~ X + E + M + E * M, data = data_use)\n\nCoefficients:\n(Intercept)            X           E1           E2           M1        E1:M1  \n    11199.7        498.4      -1741.3       -357.0       7040.6      -3051.8  \n      E2:M1  \n     1997.5  \n\nsummary(res)\n\n\nCall:\nlm(formula = S ~ X + E + M + E * M, data = data_use)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-112.884  -43.636   -5.036   46.622  128.480 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11199.714     30.533 366.802  &lt; 2e-16 ***\nX             498.418      2.152 231.640  &lt; 2e-16 ***\nE1          -1741.336     40.683 -42.803  &lt; 2e-16 ***\nE2           -357.042     37.681  -9.475 1.49e-11 ***\nM1           7040.580     39.619 177.707  &lt; 2e-16 ***\nE1:M1       -3051.763     57.674 -52.914  &lt; 2e-16 ***\nE2:M1        1997.531     51.785  38.574  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 67.12 on 38 degrees of freedom\nMultiple R-squared:  0.9998,    Adjusted R-squared:  0.9998 \nF-statistic: 3.543e+04 on 6 and 38 DF,  p-value: &lt; 2.2e-16\n\n### 표준화잔차 대 경력연수\n\nplot(data_use$X,rstandard(res),pch=19,xlab=\"범주\",ylab=\"잔차\")\n\n\n\n#### 표준화잔차 대 교육수준 - 관리 조합\n\nEM&lt;-paste0(data_use$E,data_use$M)\n\nplot(EM,rstandard(res),pch=19,xlabl=\"범주\",ylab=\"잔차\")\n\nWarning in plot.window(...): \"xlabl\" is not a graphical parameter\n\n\nWarning in plot.xy(xy, type, ...): \"xlabl\" is not a graphical parameter\n\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"xlabl\" is not a\ngraphical parameter\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"xlabl\" is not a\ngraphical parameter\n\n\nWarning in box(...): \"xlabl\" is not a graphical parameter\n\n\nWarning in title(...): \"xlabl\" is not a graphical parameter\n\n\n\n\n#상호 호과가 들어간 이 모형이 더 괜찮은 모양이라고 판단이 된다 \n\n### 기본 급료 추정 - 표 5.6\n\nres&lt;-lm(S~X+E+M+E*M,data=data_use)\n\ndf_new&lt;-data.frame(X=rep(0,6),\n\n                   E=rep(1:3,c(2,2,2)),\n\n                   M=rep(c(0,1),3))\n\n### 가변수 생성 - 분석용\n\ndf_new$E&lt;-factor(as.character(df_new$E),levels = c(\"3\",\"1\",\"2\")) \n\ndf_new$M&lt;-factor(as.character(df_new$M),levels = c(\"0\",\"1\")) \n\ncbind(df_new,predict = predict(res,df_new,interval = \"confidence\"))\n\n  X E M predict.fit predict.lwr predict.upr\n1 0 1 0    9458.378    9395.539    9521.216\n2 0 1 1   13447.195   13382.933   13511.456\n3 0 2 0   10842.672   10789.719   10895.624\n4 0 2 1   19880.782   19814.090   19947.474\n5 0 3 0   11199.714   11137.902   11261.525\n6 0 3 1   18240.294   18182.503   18298.084\n\n# 이를 보고 각 레벨에 따른 차이를 보고 얼마나 나는 지 분석이 가능해야 한다 \n\n# ex) 고졸과 대학원졸의 관리자 직급의 급여의 차이는?(평균적으로)\n\n\n\n\n\n\n\ndata_5.7&lt;-read.table(\"All_Data/p140.txt\",header=T,sep=\"\\t\")\n\nhead(data_5.7)\n\n  TEST RACE JPERF\n1 0.28    1  1.83\n2 0.97    1  4.59\n3 1.25    1  2.97\n4 2.46    1  8.14\n5 2.51    1  8.00\n6 1.17    1  3.30\n\n# 모형 1 - 통합모형 인종간 차이가 없을때\n\nmodel_1&lt;-lm(JPERF~TEST,data=data_5.7)\n\nsummary(model_1) \n\n\nCall:\nlm(formula = JPERF ~ TEST, data = data_5.7)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3558 -0.8798 -0.1897  1.2735  2.3312 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.0350     0.8680   1.192 0.248617    \nTEST          2.3605     0.5381   4.387 0.000356 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.591 on 18 degrees of freedom\nMultiple R-squared:  0.5167,    Adjusted R-squared:  0.4899 \nF-statistic: 19.25 on 1 and 18 DF,  p-value: 0.0003555\n\n# 모형 3 \n\nmodel_3&lt;-lm(JPERF~TEST+RACE+TEST*RACE,data=data_5.7)\n\nsummary(model_3)\n\n\nCall:\nlm(formula = JPERF ~ TEST + RACE + TEST * RACE, data = data_5.7)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0734 -1.0594 -0.2548  1.2830  2.1980 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   2.0103     1.0501   1.914   0.0736 .\nTEST          1.3134     0.6704   1.959   0.0677 .\nRACE         -1.9132     1.5403  -1.242   0.2321  \nTEST:RACE     1.9975     0.9544   2.093   0.0527 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.407 on 16 degrees of freedom\nMultiple R-squared:  0.6643,    Adjusted R-squared:  0.6013 \nF-statistic: 10.55 on 3 and 16 DF,  p-value: 0.0004511\n\n# 인종적인 차이가 있는지 없는지를 확인해야 한다 어떤 모형을 사용할지 \n\n### H_0:gamma=delta=0\n\nanova(model_1,model_3) #model_3가 FM(완전모형)\n\nAnalysis of Variance Table\n\nModel 1: JPERF ~ TEST\nModel 2: JPERF ~ TEST + RACE + TEST * RACE\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     18 45.568                              \n2     16 31.655  2    13.913 3.5161 0.05424 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#P-value(0.05424)&lt;0.05이기에 H_0 이기에 모형1을 선택하는 것이 옳다고 판단(그러나 확신x)\n\n#서로의 R-squared롤 보면 model_3가 더 좋음 / ANOVA는 참고용 절대적이지는 않다 \n\n\n\n\n\nplot(data_5.7$TEST,rstandard(model_1),\n\n     pch=19,xlab=\"검사점수\",ylab=\"잔차\",\n\n     main=\"그림 5.7 - 표준화잔차 대 검사점수 : 모형 1\")\n\n\n\n\n\n\n\n\nplot(data_5.7$TEST,rstandard(model_3),\n\n     pch=19,xlab=\"검사점수\",ylab=\"잔차\",\n\n     main=\"그림 5.8 - 표준화잔차 대 검사점수 : 모형 3\")\n\n\n\n# 통계에서 나오는 결과는 결정의 보조수단이지 절대적이지 않아 \n\n# 3번 모형을 선택한다고 결정한다고 진행 \n\nsummary(model_3) # Multiple R-squared:  0.6643\n\n\nCall:\nlm(formula = JPERF ~ TEST + RACE + TEST * RACE, data = data_5.7)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0734 -1.0594 -0.2548  1.2830  2.1980 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   2.0103     1.0501   1.914   0.0736 .\nTEST          1.3134     0.6704   1.959   0.0677 .\nRACE         -1.9132     1.5403  -1.242   0.2321  \nTEST:RACE     1.9975     0.9544   2.093   0.0527 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.407 on 16 degrees of freedom\nMultiple R-squared:  0.6643,    Adjusted R-squared:  0.6013 \nF-statistic: 10.55 on 3 and 16 DF,  p-value: 0.0004511\n\n\n\n\n\n\nplot(data_5.7$RACE,rstandard(model_1),\n\n     pch=19,xlab=\"검사점수\",ylab=\"잔차\",\n\n     main=\"그림 5.9 - 표준화잔차 대 검사점수 : 모형 1\")\n\n\n\n# 분리된 회귀분석 결과\n\ndata_5.7_R1&lt;-subset(data_5.7,RACE==1)\n\nmodel_R1&lt;-lm(JPERF~TEST,data=data_5.7_R1)\n\nsummary(model_R1) #소수민족\n\n\nCall:\nlm(formula = JPERF ~ TEST, data = data_5.7_R1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0734 -0.6267 -0.2548  1.1624  1.5394 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.09712    1.03519   0.094 0.927564    \nTEST         3.31095    0.62411   5.305 0.000724 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.292 on 8 degrees of freedom\nMultiple R-squared:  0.7787,    Adjusted R-squared:  0.751 \nF-statistic: 28.14 on 1 and 8 DF,  p-value: 0.0007239\n\ndata_5.7_R0&lt;-subset(data_5.7,RACE==0)\n\nmodel_R0&lt;-lm(JPERF~TEST,data=data_5.7_R0)\n\nsummary(model_R0) #백인\n\n\nCall:\nlm(formula = JPERF ~ TEST, data = data_5.7_R0)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8599 -1.0663 -0.3061  1.0957  2.1980 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   2.0103     1.1291   1.780    0.113\nTEST          1.3134     0.7208   1.822    0.106\n\nResidual standard error: 1.512 on 8 degrees of freedom\nMultiple R-squared:  0.2933,    Adjusted R-squared:  0.205 \nF-statistic:  3.32 on 1 and 8 DF,  p-value: 0.1059\n\n# 통합모형에서 나온 각각의 회귀식이 통합모형에서 나온것과 동일함 따라서 인종별로 나누어서\n\n# 진행할 필요없이 통일모형을 사용해서 진행을 하면 된다(이는 데이터셋을 나눈경우와 동일함)\n\n\n\n\n\nplot(data_5.7_R1$TEST,rstandard(model_R1),\n\n     pch=19,xlab=\"검사점수\",ylab=\"잔차\",\n\n     main=\"그림 5.10 - 표준화잔차 대 검사점수 : 모형 1. 소수민족만만\")\n\n\n\n\n\n\n\n\nplot(data_5.7_R0$TEST,rstandard(model_R0),\n\n     pch=19,xlab=\"검사점수\",ylab=\"잔차\",\n\n     main=\"그림 5.11 - 표준화잔차 대 검사점수 : 모형 1. 백인만\")\n\n\n\n# 적절한 합격점수의 결정 - 소수민족\n\n# 고용전 검사점수의 합격점에 대한 95% 신뢰구간\n\nym&lt;-4\n\nxm&lt;-(ym-0.09712)/3.31095\n\ns&lt;-1.292\n\nn&lt;-10\n\nt&lt;-qt(1-0.05/2,8)\n\nc(xm-(t*s/n)/3.31095, xm+(t*s/n)/3.31095) #신뢰구간\n\n[1] 1.088795 1.268764\n\n\n\n\n\n\nmodel_3&lt;-lm(JPERF~TEST+RACE,data=data_5.7)\n\nsummary(model_3)\n\n\nCall:\nlm(formula = JPERF ~ TEST + RACE, data = data_5.7)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7872 -1.0370 -0.2095  0.9198  2.3645 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.6120     0.8870   0.690 0.499578    \nTEST          2.2988     0.5225   4.400 0.000391 ***\nRACE          1.0276     0.6909   1.487 0.155246    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.54 on 17 degrees of freedom\nMultiple R-squared:  0.5724,    Adjusted R-squared:  0.5221 \nF-statistic: 11.38 on 2 and 17 DF,  p-value: 0.0007312\n\n#Intercept = BETA_0 / TEST = BETA_1 / RACE = gamma\n\n## H_0: gamma=0\n\nanova(model_1,model_3) #p-value(0.1552)&gt;0.05 이기에 H_0은 참이다//gamma=0\n\nAnalysis of Variance Table\n\nModel 1: JPERF ~ TEST\nModel 2: JPERF ~ TEST + RACE\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     18 45.568                           \n2     17 40.322  1    5.2468 2.2121 0.1552\n\n#기울기가 같고 절편이 다른 모형은 아니라고 데이터가 이야기하고 있다 \n\n# 소수민족(RACE=1): (0.6120+1.0276)+2.2988*TEST = 1.6396+2.2988*TEST\n\n# 백인(RACE=0): (0.6120)+2.2988*TEST\n\n\n\n\n\nmodel_3&lt;-lm(JPERF~TEST+I(TEST*RACE),data=data_5.7)\n\nsummary(model_3) #I()를 하면 교호작용하는 것만 보이게 하려고 없으면 RACE항이 자동추가됨\n\n\nCall:\nlm(formula = JPERF ~ TEST + I(TEST * RACE), data = data_5.7)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.41100 -0.88871 -0.03359  0.97720  2.44440 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)      1.1211     0.7804   1.437  0.16900   \nTEST             1.8276     0.5356   3.412  0.00332 **\nI(TEST * RACE)   0.9161     0.3972   2.306  0.03395 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.429 on 17 degrees of freedom\nMultiple R-squared:  0.6319,    Adjusted R-squared:  0.5886 \nF-statistic: 14.59 on 2 and 17 DF,  p-value: 0.0002045\n\n## H_0: delta=0\n\nanova(model_1,model_3) #p-value(0.03395)&lt;0.05보다 작기에 delta항은 필요한 변수라는 사실 \n\nAnalysis of Variance Table\n\nModel 1: JPERF ~ TEST\nModel 2: JPERF ~ TEST + I(TEST * RACE)\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     18 45.568                              \n2     17 34.708  1    10.861 5.3196 0.03395 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#Intercept = BETA_0 / TEST = BETA_1 / I(TEST*RACE) = delta"
  },
  {
    "objectID": "Regression_Analysis.html#장-변수변환-transformation-of-varibables",
    "href": "Regression_Analysis.html#장-변수변환-transformation-of-varibables",
    "title": "Regression Analysis",
    "section": "",
    "text": "library(dplyr)\n\n\n\n\ndata_6.2&lt;-read.table(\"All_Data/p168.txt\",header=T,sep=\"\\t\")\n\nhead(data_6.2)\n\n  t N_t\n1 1 355\n2 2 211\n3 3 197\n4 4 166\n5 5 142\n6 6 106\n\n\n\n\n\n\nplot(N_t~t,data=data_6.2,pch=19)\n\n\n\n\n\n\n\n\nres&lt;-lm(N_t~t,data=data_6.2)\n\nsummary(res)\n\n\nCall:\nlm(formula = N_t ~ t, data = data_6.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-43.867 -23.599  -9.652  10.223 114.883 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   259.58      22.73  11.420 3.78e-08 ***\nt             -19.46       2.50  -7.786 3.01e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 41.83 on 13 degrees of freedom\nMultiple R-squared:  0.8234,    Adjusted R-squared:  0.8098 \nF-statistic: 60.62 on 1 and 13 DF,  p-value: 3.006e-06\n\n\n\n\n\n\nplot(data_6.2$t,rstandard(res),pch=19) # 표준화 잔차의 플랏\n\n\n\n# 잔차가 랜덤하게 잘 퍼져있어야 하는데 적절한 회귀모형이 아니라는 사실이 나옴\n\n\n\n\n\n\n\n\nplot(log(N_t)~t,data=data_6.2,pch=19)\n\n\n\n# 박테리아의 수에 로그를 취하니 선형성이 보인다 \n\nres&lt;-lm(log(N_t)~t,data=data_6.2)\n\nsummary(res)\n\n\nCall:\nlm(formula = log(N_t) ~ t, data = data_6.2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.18445 -0.06189  0.01253  0.05201  0.20021 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.973160   0.059778   99.92  &lt; 2e-16 ***\nt           -0.218425   0.006575  -33.22 5.86e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.11 on 13 degrees of freedom\nMultiple R-squared:  0.9884,    Adjusted R-squared:  0.9875 \nF-statistic:  1104 on 1 and 13 DF,  p-value: 5.86e-14\n\nplot(data_6.2$t,rstandard(res),pch=19)\n\n\n\n# 로그를 취해주니 잔차가 랜덤하게 이루어져 있음 \n\n# n_0에 대한 추론\n\nexp(5.973160)\n\n[1] 392.7448\n\nexp(coef(res)[1]) #로그를 취해주고 하는 부분이 잘 이해가 안됨 \n\n(Intercept) \n   392.7449 \n\nexp(coef(res)[1]-0.0588/2) #불편추정량 구하기 (참고용)\n\n(Intercept) \n   381.3663 \n\n\n\n\n\n\ndata_6.6&lt;-read.table(\"All_Data/p174.txt\",header=T,sep=\"\\t\")\n\ndata_6.6 #운항률과 사고 발생 건수에 대한 자료\n\n   Y      N\n1 11 0.0950\n2  7 0.1920\n3  7 0.0750\n4 19 0.2078\n5  9 0.1382\n6  4 0.0540\n7  3 0.1292\n8  1 0.0503\n9  3 0.0629\n\nplot(Y~N,data=data_6.6,pch=19) #잔차의 분산이 계속 커지는 효과를 보임 \n\n\n\nres_1&lt;-lm(Y~N,data=data_6.6)\n\nsummary(res_1)\n\n\nCall:\nlm(formula = Y ~ N, data = data_6.6)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.3351 -2.1281  0.1605  2.2670  5.6382 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  -0.1402     3.1412  -0.045   0.9657  \nN            64.9755    25.1959   2.579   0.0365 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.201 on 7 degrees of freedom\nMultiple R-squared:  0.4872,    Adjusted R-squared:  0.4139 \nF-statistic:  6.65 on 1 and 7 DF,  p-value: 0.03654\n\n# 표준화잔차 대 N의 플롯 / 그림 6.11\n\nplot(data_6.6$N,rstandard(res_1),pch=19) #등분산성에 만족하지 못하는 모형을 보인다 \n\n\n\n# N에 대한 sqrt(Y)의 회귀\n\n# N에 대한 Y의 회귀\n\nres_2&lt;-lm(sqrt(Y)~N,data=data_6.6)\n\nsummary(res_2)\n\n\nCall:\nlm(formula = sqrt(Y) ~ N, data = data_6.6)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.9690 -0.7655  0.1906  0.5874  1.0211 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   1.1692     0.5783   2.022   0.0829 .\nN            11.8564     4.6382   2.556   0.0378 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7733 on 7 degrees of freedom\nMultiple R-squared:  0.4828,    Adjusted R-squared:  0.4089 \nF-statistic: 6.535 on 1 and 7 DF,  p-value: 0.03776\n\n\n\n\n\n\nplot(data_6.6$N,rstandard(res_2),pch=19) #0을 중심으로 잔차가 보다 잘 퍼져있음이 보임 \n\n\n\n\n\n\n\n\ndata_6.9&lt;-read.table(\"All_Data/p176.txt\",header=T,sep=\"\\t\")\n\ndata_6.9\n\n      X   Y\n1   294  30\n2   247  32\n3   267  37\n4   358  44\n5   423  47\n6   311  49\n7   450  56\n8   534  62\n9   438  68\n10  697  78\n11  688  80\n12  630  84\n13  709  88\n14  627  97\n15  615 100\n16  999 109\n17 1022 114\n18 1015 117\n19  700 106\n20  850 128\n21  980 130\n22 1025 160\n23 1021  97\n24 1200 180\n25 1250 112\n26 1500 210\n27 1650 135\n\n# Y 대 X의 플로\n\nplot(Y~X,data=data_6.9,pch=19,main=\"그림 6.13\")\n\n\n\n# X에 대한 Y의 회귀\n\nres_1&lt;-lm(Y~X,data=data_6.9)\n\nsummary(res_1)\n\n\nCall:\nlm(formula = Y ~ X, data = data_6.9)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-53.294  -9.298  -5.579  14.394  39.119 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 14.44806    9.56201   1.511    0.143    \nX            0.10536    0.01133   9.303 1.35e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.73 on 25 degrees of freedom\nMultiple R-squared:  0.7759,    Adjusted R-squared:  0.7669 \nF-statistic: 86.54 on 1 and 25 DF,  p-value: 1.35e-09\n\n# 표준화잔차 대 X의 플롯\n\nplot(data_6.9$X, rstandard(res_1),pch=19,main = \"그림 6.14\")\n\n\n\n\n\n\n\n\n# 변환된 Y/X와 1/X를 적합한 회귀\n\ndata_6.9_1&lt;-data.frame(Y=data_6.9$Y/data_6.9$X,\n\n                       X=1/data_6.9$X)\n\nres_2&lt;-lm(Y~X,data=data_6.9_1)\n\nsummary(res_2) #지금은 변환하고 프라임 값들의 추정치가 나온것이기에 원래 회귀식으로 돌아가야함 \n\n\nCall:\nlm(formula = Y ~ X, data = data_6.9_1)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.041477 -0.013852 -0.004998  0.024671  0.035427 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.120990   0.008999  13.445 6.04e-13 ***\nX           3.803296   4.569745   0.832    0.413    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02266 on 25 degrees of freedom\nMultiple R-squared:  0.02696,   Adjusted R-squared:  -0.01196 \nF-statistic: 0.6927 on 1 and 25 DF,  p-value: 0.4131\n\n# 본래 변환시 X를 나눴기에 X를 곱해준다 -&gt; B_0,B_1의 추정값이 바뀐다 서로 \n\nplot(data_6.9_1$X, rstandard(res_2),pch=19,\n\n     xlab=\"1/X\",ylab=\"잔차\",main = \"[그림 6.15]\")\n\n\n\n\n\n\n\n\nwt&lt;-1/data_6.9$X^2 #가중값\n\nres_3&lt;-lm(Y~X,data=data_6.9,weights = wt)\n\nsummary(res_3) #결과는 위의 6.6과 동일한 값이 나옴 \n\n\nCall:\nlm(formula = Y ~ X, data = data_6.9, weights = wt)\n\nWeighted Residuals:\n      Min        1Q    Median        3Q       Max \n-0.041477 -0.013852 -0.004998  0.024671  0.035427 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 3.803296   4.569745   0.832    0.413    \nX           0.120990   0.008999  13.445 6.04e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02266 on 25 degrees of freedom\nMultiple R-squared:  0.8785,    Adjusted R-squared:  0.8737 \nF-statistic: 180.8 on 1 and 25 DF,  p-value: 6.044e-13\n\n\n\n\n\n\ndata_6.9&lt;-read.table(\"All_Data/p176.txt\",header=T,sep=\"\\t\")\n\nhead(data_6.9)\n\n    X  Y\n1 294 30\n2 247 32\n3 267 37\n4 358 44\n5 423 47\n6 311 49\n\n# log(Y) 대 X의 산점도\n\nplot((Y)~X,data=data_6.9,pch=19,main=\"식 6.9\")\n\n\n\nplot(log(Y)~X,data=data_6.9,pch=19,main=\"그림 6.16\")\n\n\n\nres_4&lt;-lm(log(Y)~X,data=data_6.9)\n\nsummary(res_4)\n\n\nCall:\nlm(formula = log(Y) ~ X, data = data_6.9)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.59648 -0.16578  0.00244  0.17481  0.34964 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 3.5150232  0.1110670  31.648  &lt; 2e-16 ***\nX           0.0012041  0.0001316   9.153 1.85e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2524 on 25 degrees of freedom\nMultiple R-squared:  0.7702,    Adjusted R-squared:  0.761 \nF-statistic: 83.77 on 1 and 25 DF,  p-value: 1.855e-09\n\n# X에 대한 log(Y)의 회귀로 부터 얻은 표준화잔차플롯\n\nplot(data_6.9$X,rstandard(res_4),pch=19,\n\n     xlab=\"X\",ylab=\"잔차\",main=\"그림 6.17\")\n\n\n\n# X와 X^2에 대한 log(Y)의 회귀\n\ndf&lt;-data.frame(log_Y=log(data_6.9$Y),\n\n               X=data_6.9$X,\n\n               X2=data_6.9$X^2)\n\nres_5&lt;-lm(log_Y~X+X2,data=df) #그러나 이러한 과정은 필요업고 아래방식으로..\n\nres_5&lt;-lm(log(Y)~X+I(X^2),data=data_6.9)\n\nsummary(res_5)\n\n\nCall:\nlm(formula = log(Y) ~ X + I(X^2), data = data_6.9)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.30589 -0.11705 -0.02707  0.17593  0.30657 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.852e+00  1.566e-01  18.205 1.50e-15 ***\nX            3.113e-03  3.989e-04   7.803 4.90e-08 ***\nI(X^2)      -1.102e-06  2.238e-07  -4.925 5.03e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1817 on 24 degrees of freedom\nMultiple R-squared:  0.8857,    Adjusted R-squared:  0.8762 \nF-statistic: 92.98 on 2 and 24 DF,  p-value: 4.976e-12\n\n# 표준화잔차 대 적합값 플롯 : X와 X^2에 대한 log(Y)의 회귀\n\nplot(fitted(res_5),rstandard(res_5),pch=19,\n\n     xlab=\"X\",ylab=\"잔차\",main=\"그림 6.18\")\n\n\n\n# 표준화잔차 대 X의 플롯 : X와 X^2에 대한 log(Y)의 회귀\n\nplot(data_6.9$X,rstandard(res_5),pch=19,\n\n     xlab=\"X\",ylab=\"잔차\",main=\"그림 6.19\")\n\n\n\n# 표준화잔차 대 X^2의 플롯 : X와 X^2에 대한 log(Y)의 회귀\n\nplot(data_6.9$X^2,rstandard(res_5),pch=19,\n\n     xlab=\"X\",ylab=\"잔차\",main=\"그림 6.20\") #2차항을 추가함으로서 더 잘 피팅됨 \n\n\n\n# 7장은 스킵 / 8장 스킵\n\n\n\n\n\ndata_9.1&lt;-read.table(\"All_Data/p236.txt\",header=T,sep=\"\\t\")\n\nhead(data_9.1)\n\n      ACHV      FAM     PEER   SCHOOL\n1 -0.43148  0.60814  0.03509  0.16607\n2  0.79969  0.79369  0.47924  0.53356\n3 -0.92467 -0.82630 -0.61951 -0.78635\n4 -2.19081 -1.25310 -1.21675 -1.04076\n5 -2.84818  0.17399 -0.18517  0.14229\n6 -0.66233  0.20246  0.12764  0.27311\n\nres&lt;-lm(ACHV~.,data_9.1)\n\nsummary(res)\n\n\nCall:\nlm(formula = ACHV ~ ., data = data_9.1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.2096 -1.3934 -0.2947  1.1415  4.5881 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -0.06996    0.25064  -0.279    0.781\nFAM          1.10126    1.41056   0.781    0.438\nPEER         2.32206    1.48129   1.568    0.122\nSCHOOL      -2.28100    2.22045  -1.027    0.308\n\nResidual standard error: 2.07 on 66 degrees of freedom\nMultiple R-squared:  0.2063,    Adjusted R-squared:  0.1702 \nF-statistic: 5.717 on 3 and 66 DF,  p-value: 0.001535\n\n#F-statistic: 5.717 on 3 and 66 DF, p-value: 0.001535\n\n#이는 의미있는 beta가 존재한다라는 의미\n\n#그러나 각 계수들의 회귀계수를 보니 모두 0이라는 결과가 나옴 이는 다중공선성이다\n\n# Correlation panel\n\npanel.cor&lt;-function(x,y){\n\npar(usr=c(0,1,0,1))\n\nr&lt;-round(cor(x,y),digits = 3)\n\ntext(0.5,0.5,r,cex=1.5)\n\n}\n\npairs(data_9.1[-1],lower.panel = panel.cor)\n\n\n\n\n\n\n\n\nplot(fitted(res),rstandard(res),pch=19,\n\nxlab=\"예측값\",ylab=\"잔차\",main=\"[그림 9.1]\")\n\n\n\n#이는 회귀모형에 동시에 들어가면 안된다라는 의미 (제거가 필요)\n\n\n\n\n\ndata_9.5&lt;-read.table(\"All_Data/p241.txt\",header=T,sep=\"\\t\")\n\nhead(data_9.5)\n\n  YEAR IMPORT DOPROD STOCK CONSUM\n1   49   15.9  149.3   4.2  108.1\n2   50   16.4  161.2   4.1  114.8\n3   51   19.0  171.5   3.1  123.2\n4   52   19.1  175.5   3.1  126.9\n5   53   18.8  180.8   1.1  132.1\n6   54   20.4  190.7   2.2  137.7\n\n### 산점도\n\npairs(data_9.5[-1],lower.panel = panel.cor)\n\n\n\n# 회귀분석(1) : 데이터 1949~1966\n\nres&lt;-lm(IMPORT~.,data_9.5[-1])\n\nsummary(res) #다중공선성이 존재한다고 예측을 일단함\n\n\nCall:\nlm(formula = IMPORT ~ ., data = data_9.5[-1])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7208 -1.8354 -0.3479  1.2973  4.1008 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -19.7251     4.1253  -4.782 0.000293 ***\nDOPROD        0.0322     0.1869   0.172 0.865650    \nSTOCK         0.4142     0.3223   1.285 0.219545    \nCONSUM        0.2427     0.2854   0.851 0.409268    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.258 on 14 degrees of freedom\nMultiple R-squared:  0.973, Adjusted R-squared:  0.9673 \nF-statistic: 168.4 on 3 and 14 DF,  p-value: 3.212e-11\n\n\n\n\n\n\nplot(1:nrow(data_9.5),rstandard(res),\n\npch=19,type=\"b\",\n\nxla=\"번호\",ylab=\"잔차\",main=\"[그림 9.3]\")\n\n\n\n# 회귀분석(2) : 데이터 1949~1959\n\ndata_use&lt;-subset(data_9.5,YEAR&lt;=59)\n\nres&lt;-lm(IMPORT~.,data_use[-1])\n\nsummary(res)\n\n\nCall:\nlm(formula = IMPORT ~ ., data = data_use[-1])\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.52367 -0.38953  0.05424  0.22644  0.78313 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -10.12799    1.21216  -8.355  6.9e-05 ***\nDOPROD       -0.05140    0.07028  -0.731 0.488344    \nSTOCK         0.58695    0.09462   6.203 0.000444 ***\nCONSUM        0.28685    0.10221   2.807 0.026277 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4889 on 7 degrees of freedom\nMultiple R-squared:  0.9919,    Adjusted R-squared:  0.9884 \nF-statistic: 285.6 on 3 and 7 DF,  p-value: 1.112e-07\n\n\n\n\n\n\nplot(1:nrow(data_use),rstandard(res),\n\npch=19,type=\"b\",\n\nxla=\"번호\",ylab=\"잔차\",main=\"[그림 9.4]\")\n\n\n\n\n\n\n\n\n# 분산확대인자\n\nlibrary(olsrr)\n\n# 교육기회 균등(EEO)\n\nres_9.1&lt;-lm(ACHV~.,data_9.1)\n\nsummary(res_9.1)\n\n\nCall:\nlm(formula = ACHV ~ ., data = data_9.1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.2096 -1.3934 -0.2947  1.1415  4.5881 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -0.06996    0.25064  -0.279    0.781\nFAM          1.10126    1.41056   0.781    0.438\nPEER         2.32206    1.48129   1.568    0.122\nSCHOOL      -2.28100    2.22045  -1.027    0.308\n\nResidual standard error: 2.07 on 66 degrees of freedom\nMultiple R-squared:  0.2063,    Adjusted R-squared:  0.1702 \nF-statistic: 5.717 on 3 and 66 DF,  p-value: 0.001535\n\n#car::vif(res_9.1)\n\nolsrr::ols_vif_tol(res_9.1) # 10보다 크면 유의한 영향을 준다 제거필요?\n\n  Variables  Tolerance      VIF\n1       FAM 0.02660945 37.58064\n2      PEER 0.03309981 30.21166\n3    SCHOOL 0.01202567 83.15544\n\n\n\n\n\n\ndata_use&lt;-subset(data_9.5,YEAR&lt;=59)\n\nres_9.5&lt;-lm(IMPORT~.,data_use[-1])\n\n#car::vif(res_9.1)\n\nolsrr::ols_vif_tol(res_9.5) #작으면 새로운 변수를 만들거나 제거를 통해서 진행\n\n  Variables   Tolerance        VIF\n1    DOPROD 0.005376417 185.997470\n2     STOCK 0.981441657   1.018909\n3    CONSUM 0.005373166 186.110015\n\n# 상태 지수\n\ncnd.idx&lt;-olsrr::ols_eigen_cindex(res_9.1)\n\nround(cnd.idx,4)\n\n  Eigenvalue Condition Index intercept    FAM   PEER SCHOOL\n1     2.9547          1.0000    0.0005 0.0030 0.0037 0.0014\n2     0.9974          1.7211    0.9756 0.0000 0.0000 0.0000\n3     0.0400          8.5996    0.0004 0.3068 0.4428 0.0008\n4     0.0079         19.2826    0.0235 0.6903 0.5535 0.9978\n\ncnd.idx&lt;-olsrr::ols_eigen_cindex(res_9.5)\n\nround(cnd.idx,4)\n\n  Eigenvalue Condition Index intercept DOPROD  STOCK CONSUM\n1     3.8384          1.0000    0.0010 0.0000 0.0109 0.0000\n2     0.1484          5.0863    0.0053 0.0001 0.9385 0.0001\n3     0.0132         17.0732    0.7743 0.0015 0.0330 0.0011\n4     0.0001        265.4613    0.2193 0.9984 0.0175 0.9989"
  },
  {
    "objectID": "Regression_Analysis.html#장---공선형-데이터의-처리",
    "href": "Regression_Analysis.html#장---공선형-데이터의-처리",
    "title": "Regression Analysis",
    "section": "",
    "text": "data_9.5&lt;-read.table(\"All_Data/p241.txt\",header=T,sep=\"\\t\")\n\nhead(data_9.5)\n\n  YEAR IMPORT DOPROD STOCK CONSUM\n1   49   15.9  149.3   4.2  108.1\n2   50   16.4  161.2   4.1  114.8\n3   51   19.0  171.5   3.1  123.2\n4   52   19.1  175.5   3.1  126.9\n5   53   18.8  180.8   1.1  132.1\n6   54   20.4  190.7   2.2  137.7\n\n# 회귀분석(2) : 데이터 1949~1959\n\ndata_use&lt;-subset(data_9.5,YEAR&lt;=59)\n\nres&lt;-lm(IMPORT~.,data_use[-1]) #1열 제거 \n\nsummary(res)\n\n\nCall:\nlm(formula = IMPORT ~ ., data = data_use[-1])\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.52367 -0.38953  0.05424  0.22644  0.78313 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -10.12799    1.21216  -8.355  6.9e-05 ***\nDOPROD       -0.05140    0.07028  -0.731 0.488344    \nSTOCK         0.58695    0.09462   6.203 0.000444 ***\nCONSUM        0.28685    0.10221   2.807 0.026277 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4889 on 7 degrees of freedom\nMultiple R-squared:  0.9919,    Adjusted R-squared:  0.9884 \nF-statistic: 285.6 on 3 and 7 DF,  p-value: 1.112e-07\n\nhead(data_use[-c(1:2)])\n\n  DOPROD STOCK CONSUM\n1  149.3   4.2  108.1\n2  161.2   4.1  114.8\n3  171.5   3.1  123.2\n4  175.5   3.1  126.9\n5  180.8   1.1  132.1\n6  190.7   2.2  137.7\n\n### 주성분(principle component)\n\npc&lt;-prcomp(data_use[-c(1:2)],scale.=T)\n\npc$rotation\n\n              PC1         PC2          PC3\nDOPROD 0.70633041 -0.03568867 -0.706982083\nSTOCK  0.04350059  0.99902908 -0.006970795\nCONSUM 0.70654444 -0.02583046  0.707197102\n\npc$x #변화된 새로운 변수가 저장된곳\n\n          PC1         PC2         PC3\n1  -2.1258872  0.63865815 -0.02072230\n2  -1.6189273  0.55553922 -0.07111317\n3  -1.1151675 -0.07297970 -0.02173008\n4  -0.8942966 -0.08236998  0.01081318\n5  -0.6442081 -1.30668523  0.07258248\n6  -0.1903514 -0.65914745  0.02655252\n7   0.3596219 -0.74367447  0.04278124\n8   0.9718018  1.35405877  0.06286252\n9   1.5593159  0.96404558  0.02357446\n10  1.7669951  1.01521706 -0.04498818\n11  1.9311034 -1.66266195 -0.08061267\n\n### 주성분회귀 - 원래데이터를 주성분분석을 통해 새로운 데이터를 생성 이를 가지고 회귀 \n\ndf&lt;-data.frame(IMPORT = scale(data_use$IMPORT),\n\n               pc$x)\n\ndf\n\n       IMPORT        PC1         PC2         PC3\n1  -1.3185185 -2.1258872  0.63865815 -0.02072230\n2  -1.2084753 -1.6189273  0.55553922 -0.07111317\n3  -0.6362502 -1.1151675 -0.07297970 -0.02173008\n4  -0.6142416 -0.8942966 -0.08236998  0.01081318\n5  -0.6802675 -0.6442081 -1.30668523  0.07258248\n6  -0.3281290 -0.1903514 -0.65914745  0.02655252\n7   0.1780700  0.3596219 -0.74367447  0.04278124\n8   1.0143989  0.9718018  1.35405877  0.06286252\n9   1.3665374  1.5593159  0.96404558  0.02357446\n10  1.2564942  1.7669951  1.01521706 -0.04498818\n11  0.9703816  1.9311034 -1.66266195 -0.08061267\n\nres&lt;-lm(IMPORT~.,data=df)\n\nsummary(res) #유의한 1,2번째 계수들만 사용하고 3번째 것은 없이 해도 되겠다..\n\n\nCall:\nlm(formula = IMPORT ~ ., data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.11525 -0.08573  0.01194  0.04984  0.17236 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 8.900e-16  3.244e-02   0.000 1.000000    \nPC1         6.900e-01  2.406e-02  28.673 1.61e-08 ***\nPC2         1.913e-01  3.406e-02   5.617 0.000801 ***\nPC3         1.160e+00  6.559e-01   1.768 0.120376    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1076 on 7 degrees of freedom\nMultiple R-squared:  0.9919,    Adjusted R-squared:  0.9884 \nF-statistic: 285.6 on 3 and 7 DF,  p-value: 1.112e-07\n\n#중간고사 이후의것이 주가나오겠지만 앞에것도 알아야함 연습문제에서 나올것 예상"
  },
  {
    "objectID": "Regression_Analysis.html#장-1",
    "href": "Regression_Analysis.html#장-1",
    "title": "Regression Analysis",
    "section": "",
    "text": "# 11.10 - \n\ndata_3.3&lt;-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\nhead(data_3.3)\n\n   Y X1 X2 X3 X4 X5 X6\n1 43 51 30 39 61 92 45\n2 63 64 51 54 63 73 47\n3 71 70 68 69 76 86 48\n4 61 63 45 47 54 84 35\n5 81 78 56 66 71 83 47\n6 43 55 49 44 54 49 34\n\n# 분산확대 인자(VIF) : 10초과 &gt; 심각한 공산성의 문제가 있음\n\nres&lt;-lm(Y~.,data=data_3.3)\n\nsummary(res)\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.9418  -4.3555   0.3158   5.5425  11.5990 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10.78708   11.58926   0.931 0.361634    \nX1           0.61319    0.16098   3.809 0.000903 ***\nX2          -0.07305    0.13572  -0.538 0.595594    \nX3           0.32033    0.16852   1.901 0.069925 .  \nX4           0.08173    0.22148   0.369 0.715480    \nX5           0.03838    0.14700   0.261 0.796334    \nX6          -0.21706    0.17821  -1.218 0.235577    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.068 on 23 degrees of freedom\nMultiple R-squared:  0.7326,    Adjusted R-squared:  0.6628 \nF-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05\n\nolsrr::ols_vif_tol(res) #10을 넘는 것이 없기에 공산성의 문제는 없다 \n\n  Variables Tolerance      VIF\n1        X1 0.3749447 2.667060\n2        X2 0.6246520 1.600891\n3        X3 0.4403263 2.271043\n4        X4 0.3248624 3.078226\n5        X5 0.8142600 1.228109\n6        X6 0.5124025 1.951591\n\n# 수정결정계수\n\nres_summ&lt;-summary(res)\n\nres_summ$adj.r.squared #0.662846\n\n[1] 0.662846\n\n# Mallow's Cp - 작을수록 좋음\n\n# 축소모형\n\nres_subset&lt;-lm(Y~X1+X3,data=data_3.3)\n\nolsrr::ols_mallows_cp(res_subset,res) \n\n[1] 1.114811\n\n# AIC - 참고만\n\nolsrr::ols_aic(res_subset,method = \"SAS\")\n\n[1] 118.0024\n\n# BIC\n\nolsrr::ols_sbic(res_subset,res)\n\n[1] 121.0938\n\n### 전진적 선택법 - AIC\n\nres_step&lt;-step(res,direction = \"forward\")\n\nStart:  AIC=123.36\nY ~ X1 + X2 + X3 + X4 + X5 + X6\n\n### 후진적 제거법 -AIC\n\nres_step&lt;-step(res,direction = \"backward\")\n\nStart:  AIC=123.36\nY ~ X1 + X2 + X3 + X4 + X5 + X6\n\n       Df Sum of Sq    RSS    AIC\n- X5    1      3.41 1152.4 121.45\n- X4    1      6.80 1155.8 121.54\n- X2    1     14.47 1163.5 121.74\n- X6    1     74.11 1223.1 123.24\n&lt;none&gt;              1149.0 123.36\n- X3    1    180.50 1329.5 125.74\n- X1    1    724.80 1873.8 136.04\n\nStep:  AIC=121.45\nY ~ X1 + X2 + X3 + X4 + X6\n\n       Df Sum of Sq    RSS    AIC\n- X4    1     10.61 1163.0 119.73\n- X2    1     14.16 1166.6 119.82\n- X6    1     71.27 1223.7 121.25\n&lt;none&gt;              1152.4 121.45\n- X3    1    177.74 1330.1 123.75\n- X1    1    724.70 1877.1 134.09\n\nStep:  AIC=119.73\nY ~ X1 + X2 + X3 + X6\n\n       Df Sum of Sq    RSS    AIC\n- X2    1     16.10 1179.1 118.14\n- X6    1     61.60 1224.6 119.28\n&lt;none&gt;              1163.0 119.73\n- X3    1    197.03 1360.0 122.42\n- X1    1   1165.94 2328.9 138.56\n\nStep:  AIC=118.14\nY ~ X1 + X3 + X6\n\n       Df Sum of Sq    RSS    AIC\n- X6    1     75.54 1254.7 118.00\n&lt;none&gt;              1179.1 118.14\n- X3    1    186.12 1365.2 120.54\n- X1    1   1259.91 2439.0 137.94\n\nStep:  AIC=118\nY ~ X1 + X3\n\n       Df Sum of Sq    RSS    AIC\n&lt;none&gt;              1254.7 118.00\n- X3    1    114.73 1369.4 118.63\n- X1    1   1370.91 2625.6 138.16\n\n### 단계적 방법\n\nres_step&lt;-step(res) #최종적으로는 X1+X3이 선택됨 이거 사용하는 것이 좋을듯 \n\nStart:  AIC=123.36\nY ~ X1 + X2 + X3 + X4 + X5 + X6\n\n       Df Sum of Sq    RSS    AIC\n- X5    1      3.41 1152.4 121.45\n- X4    1      6.80 1155.8 121.54\n- X2    1     14.47 1163.5 121.74\n- X6    1     74.11 1223.1 123.24\n&lt;none&gt;              1149.0 123.36\n- X3    1    180.50 1329.5 125.74\n- X1    1    724.80 1873.8 136.04\n\nStep:  AIC=121.45\nY ~ X1 + X2 + X3 + X4 + X6\n\n       Df Sum of Sq    RSS    AIC\n- X4    1     10.61 1163.0 119.73\n- X2    1     14.16 1166.6 119.82\n- X6    1     71.27 1223.7 121.25\n&lt;none&gt;              1152.4 121.45\n- X3    1    177.74 1330.1 123.75\n- X1    1    724.70 1877.1 134.09\n\nStep:  AIC=119.73\nY ~ X1 + X2 + X3 + X6\n\n       Df Sum of Sq    RSS    AIC\n- X2    1     16.10 1179.1 118.14\n- X6    1     61.60 1224.6 119.28\n&lt;none&gt;              1163.0 119.73\n- X3    1    197.03 1360.0 122.42\n- X1    1   1165.94 2328.9 138.56\n\nStep:  AIC=118.14\nY ~ X1 + X3 + X6\n\n       Df Sum of Sq    RSS    AIC\n- X6    1     75.54 1254.7 118.00\n&lt;none&gt;              1179.1 118.14\n- X3    1    186.12 1365.2 120.54\n- X1    1   1259.91 2439.0 137.94\n\nStep:  AIC=118\nY ~ X1 + X3\n\n       Df Sum of Sq    RSS    AIC\n&lt;none&gt;              1254.7 118.00\n- X3    1    114.73 1369.4 118.63\n- X1    1   1370.91 2625.6 138.16\n\nsummary(res_step) #X3가 의미 없다고 나와도 아님 검증된것임\n\n\nCall:\nlm(formula = Y ~ X1 + X3, data = data_3.3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.5568  -5.7331   0.6701   6.5341  10.3610 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   9.8709     7.0612   1.398    0.174    \nX1            0.6435     0.1185   5.432 9.57e-06 ***\nX3            0.2112     0.1344   1.571    0.128    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.817 on 27 degrees of freedom\nMultiple R-squared:  0.708, Adjusted R-squared:  0.6864 \nF-statistic: 32.74 on 2 and 27 DF,  p-value: 6.058e-08"
  },
  {
    "objectID": "Regression_Analysis.html#regression-analysis-1",
    "href": "Regression_Analysis.html#regression-analysis-1",
    "title": "Regression Analysis",
    "section": "",
    "text": "data_use&lt;-read.table(\"All_Data/p329.txt\",header=T,sep=\"\\t\")\n\nhead(data_use)\n\n     X1 X2    X3    X4 X5 X6 X7 X8 X9    Y\n1 4.918  1 3.472 0.998  1  7  4 42  0 25.9\n2 5.021  1 3.531 1.500  2  7  4 62  0 29.5\n3 4.543  1 2.275 1.175  1  6  3 40  0 27.9\n4 4.557  1 4.050 1.232  1  6  3 54  0 25.9\n5 5.060  1 4.455 1.121  1  6  3 42  0 29.9\n6 3.891  1 4.455 0.988  1  6  3 56  0 29.9\n\n### 데이터 탐색 - 자료형\n\nsapply(data_use,class)\n\n       X1        X2        X3        X4        X5        X6        X7        X8 \n\"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \n       X9         Y \n\"numeric\" \"numeric\" \n\n### 데이터 탐색 - 기초 통계량\n\nsummary(data_use) #결측값의 여부를 확인\n\n       X1              X2              X3              X4       \n Min.   :3.891   Min.   :1.000   Min.   :2.275   Min.   :0.975  \n 1st Qu.:5.058   1st Qu.:1.000   1st Qu.:4.855   1st Qu.:1.161  \n Median :5.974   Median :1.000   Median :5.685   Median :1.432  \n Mean   :6.405   Mean   :1.167   Mean   :6.033   Mean   :1.384  \n 3rd Qu.:7.873   3rd Qu.:1.500   3rd Qu.:7.158   3rd Qu.:1.577  \n Max.   :9.142   Max.   :1.500   Max.   :9.890   Max.   :1.831  \n       X5              X6            X7              X8              X9      \n Min.   :0.000   Min.   :5.0   Min.   :2.000   Min.   : 3.00   Min.   :0.00  \n 1st Qu.:1.000   1st Qu.:6.0   1st Qu.:3.000   1st Qu.:30.00   1st Qu.:0.00  \n Median :1.000   Median :6.0   Median :3.000   Median :40.00   Median :0.00  \n Mean   :1.312   Mean   :6.5   Mean   :3.167   Mean   :37.46   Mean   :0.25  \n 3rd Qu.:2.000   3rd Qu.:7.0   3rd Qu.:3.250   3rd Qu.:48.50   3rd Qu.:0.25  \n Max.   :2.000   Max.   :8.0   Max.   :4.000   Max.   :62.00   Max.   :1.00  \n       Y        \n Min.   :25.90  \n 1st Qu.:29.90  \n Median :33.70  \n Mean   :34.63  \n 3rd Qu.:38.15  \n Max.   :45.80  \n\n### 데이터 탐색 - 히스토그램 & 상자그림\n\nhist(data_use$Y,main=\"histogram of data_use$Y\")\n\n\n\nboxplot(data_use$Y)\n\n\n\n### 데이터 탐색 - 산점도 행렬 & 상관계수\n\nplot(data_use)\n\ncor(data_use) \n\n           X1         X2         X3         X4          X5        X6        X7\nX1  1.0000000  0.6512669  0.6892117  0.7342737  0.45855650 0.6406157 0.3671126\nX2  0.6512669  1.0000000  0.4129558  0.7285916  0.22402204 0.5103104 0.4264014\nX3  0.6892117  0.4129558  1.0000000  0.5715520  0.20466375 0.3921244 0.1516093\nX4  0.7342737  0.7285916  0.5715520  1.0000000  0.35888351 0.6788606 0.5743353\nX5  0.4585565  0.2240220  0.2046638  0.3588835  1.00000000 0.5893871 0.5412988\nX6  0.6406157  0.5103104  0.3921244  0.6788606  0.58938707 1.0000000 0.8703883\nX7  0.3671126  0.4264014  0.1516093  0.5743353  0.54129880 0.8703883 1.0000000\nX8 -0.4371012 -0.1007485 -0.3527514 -0.1390869 -0.02016883 0.1242663 0.3135114\nX9  0.1466825  0.2041241  0.3059946  0.1065612  0.10161846 0.2222222 0.0000000\nY   0.8739117  0.7097771  0.6476364  0.7077656  0.46146792 0.5284436 0.2815200\n            X8        X9          Y\nX1 -0.43710116 0.1466825  0.8739117\nX2 -0.10074847 0.2041241  0.7097771\nX3 -0.35275139 0.3059946  0.6476364\nX4 -0.13908686 0.1065612  0.7077656\nX5 -0.02016883 0.1016185  0.4614679\nX6  0.12426629 0.2222222  0.5284436\nX7  0.31351144 0.0000000  0.2815200\nX8  1.00000000 0.2257796 -0.3974034\nX9  0.22577960 1.0000000  0.2668783\nY  -0.39740338 0.2668783  1.0000000\n\npairs(data_use)\n\n\n\npanel.cor&lt;-function(x,y){\n\n  par(usr=c(0,1,0,1))\n\n  r&lt;-round(cor(x,y),digits = 3)\n\n  text(0.5,0.5,r,cex=1.5)\n\n}\n\npairs(data_use,lower.panel = panel.cor)\n\n\n\n### (a) 모든 변수들이 모형에 포함시킬 것인가?\n\ncor(data_use)\n\n           X1         X2         X3         X4          X5        X6        X7\nX1  1.0000000  0.6512669  0.6892117  0.7342737  0.45855650 0.6406157 0.3671126\nX2  0.6512669  1.0000000  0.4129558  0.7285916  0.22402204 0.5103104 0.4264014\nX3  0.6892117  0.4129558  1.0000000  0.5715520  0.20466375 0.3921244 0.1516093\nX4  0.7342737  0.7285916  0.5715520  1.0000000  0.35888351 0.6788606 0.5743353\nX5  0.4585565  0.2240220  0.2046638  0.3588835  1.00000000 0.5893871 0.5412988\nX6  0.6406157  0.5103104  0.3921244  0.6788606  0.58938707 1.0000000 0.8703883\nX7  0.3671126  0.4264014  0.1516093  0.5743353  0.54129880 0.8703883 1.0000000\nX8 -0.4371012 -0.1007485 -0.3527514 -0.1390869 -0.02016883 0.1242663 0.3135114\nX9  0.1466825  0.2041241  0.3059946  0.1065612  0.10161846 0.2222222 0.0000000\nY   0.8739117  0.7097771  0.6476364  0.7077656  0.46146792 0.5284436 0.2815200\n            X8        X9          Y\nX1 -0.43710116 0.1466825  0.8739117\nX2 -0.10074847 0.2041241  0.7097771\nX3 -0.35275139 0.3059946  0.6476364\nX4 -0.13908686 0.1065612  0.7077656\nX5 -0.02016883 0.1016185  0.4614679\nX6  0.12426629 0.2222222  0.5284436\nX7  0.31351144 0.0000000  0.2815200\nX8  1.00000000 0.2257796 -0.3974034\nX9  0.22577960 1.0000000  0.2668783\nY  -0.39740338 0.2668783  1.0000000\n\nmodel_1&lt;-lm(data_use$Y~.,data_use)\n\nsummary(model_1) #전형적인 다중공선성이 보인다 \n\n\nCall:\nlm(formula = data_use$Y ~ ., data = data_use)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7729 -1.9801 -0.0868  1.6615  4.2618 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) 15.31044    5.96093   2.568   0.0223 *\nX1           1.95413    1.03833   1.882   0.0808 .\nX2           6.84552    4.33529   1.579   0.1367  \nX3           0.13761    0.49436   0.278   0.7848  \nX4           2.78143    4.39482   0.633   0.5370  \nX5           2.05076    1.38457   1.481   0.1607  \nX6          -0.55590    2.39791  -0.232   0.8200  \nX7          -1.24516    3.42293  -0.364   0.7215  \nX8          -0.03800    0.06726  -0.565   0.5810  \nX9           1.70446    1.95317   0.873   0.3976  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.973 on 14 degrees of freedom\nMultiple R-squared:  0.8512,    Adjusted R-squared:  0.7555 \nF-statistic: 8.898 on 9 and 14 DF,  p-value: 0.0002015\n\n#coef가 X6 0.820이므로 확인 필요\n\nolsrr::ols_vif_tol(model_1) #X6,X7이 10에 가깝기에 제거해야하는 대상중 최우선 \n\n  Variables  Tolerance       VIF\n1        X1 0.14241176  7.021892\n2        X2 0.35267392  2.835480\n3        X3 0.40735454  2.454864\n4        X4 0.26066568  3.836332\n5        X5 0.54842184  1.823414\n6        X6 0.08539078 11.710866\n7        X7 0.10286111  9.721847\n8        X8 0.43083904  2.321052\n9        X9 0.51482060  1.942424\n\n#다중공선성이 보이기에 모든 변수를 모형에 포함 시킬수는 없다 \n\n#10보다 크면 심각한 공선성이 존재 \n\n### (b) 지방세(X1) 방의 수(X6), 건물의 나이(X8)가 판매가격(Y)을 \n\n#       설명하는데 적절하다는 의견에 동의 하는가?\n\nmodel_2&lt;-lm(Y~X1+X6+X8,data=data_use)\n\nsummary(model_2)\n\n\nCall:\nlm(formula = Y ~ X1 + X6 + X8, data = data_use)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7486 -2.4082 -0.3594  2.1378  6.5353 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 14.796013   4.971105   2.976 0.007462 ** \nX1           3.489464   0.729368   4.784 0.000113 ***\nX6          -0.415515   1.182262  -0.351 0.728921    \nX8           0.004923   0.063597   0.077 0.939062    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.123 on 20 degrees of freedom\nMultiple R-squared:  0.7655,    Adjusted R-squared:  0.7303 \nF-statistic: 21.76 on 3 and 20 DF,  p-value: 1.653e-06\n\n# VIF\n\nolsrr::ols_vif_tol(model_2)\n\n  Variables Tolerance      VIF\n1        X1 0.3184367 3.140342\n2        X6 0.3875669 2.580200\n3        X8 0.5317389 1.880622\n\n# 회귀진단 그래프들\n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(model_2)\n\n\n\nlayout(1)\n\n# Cook의 거리 - 1을 넘으면 확인을 해야한다 \n\nolsrr::ols_plot_cooksd_chart(model_2) \n\n\n\n#동의는 할 수 있으나(적절하지만) 최고의 모형이라는데는 동의 못함\n\n### (c) 지방세 X1가 단독으로 판매가격 Y을 설명하는데 적절하다는 의견에 동의?\n\nmodel_3&lt;-lm(Y~X1,data=data_use)\n\nsummary(model_3) #adj-rsquared는 변수를 선택할때 사용 \n\n\nCall:\nlm(formula = Y ~ X1, data = data_use)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8445 -2.3340 -0.3841  1.9689  6.3005 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  13.3553     2.5955   5.146 3.71e-05 ***\nX1            3.3215     0.3939   8.433 2.44e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.988 on 22 degrees of freedom\nMultiple R-squared:  0.7637,    Adjusted R-squared:  0.753 \nF-statistic: 71.11 on 1 and 22 DF,  p-value: 2.435e-08\n\n# 회귀진단 그래프들\n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(model_3)\n\n\n\nlayout(1)\n\n# Cook의 거리 - 1을 넘으면 확인을 해야한다 \n\nolsrr::ols_plot_cooksd_chart(model_3) \n\n\n\n### 적절한 모형 제시\n\ndata_use_2&lt;-data_use[-6]\n\nmodel_4&lt;-lm(Y~.,data_use_2)\n\nsummary(model_4)\n\n\nCall:\nlm(formula = Y ~ ., data = data_use_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7340 -1.8513 -0.0154  1.5472  4.2113 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) 14.40540    4.36026   3.304  0.00482 **\nX1           1.81642    0.82431   2.204  0.04360 * \nX2           7.13892    4.01353   1.779  0.09556 . \nX3           0.14721    0.47683   0.309  0.76177   \nX4           2.73339    4.24921   0.643  0.52976   \nX5           2.06520    1.33883   1.543  0.14377   \nX7          -1.91236    1.79355  -1.066  0.30318   \nX8          -0.03832    0.06509  -0.589  0.56481   \nX9           1.48746    1.65931   0.896  0.38418   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.878 on 15 degrees of freedom\nMultiple R-squared:  0.8506,    Adjusted R-squared:  0.771 \nF-statistic: 10.68 on 8 and 15 DF,  p-value: 5.936e-05\n\n# VIF\n\nolsrr::ols_vif_tol(model_4)\n\n  Variables Tolerance      VIF\n1        X1 0.2117072 4.723504\n2        X2 0.3855308 2.593827\n3        X3 0.4102334 2.437637\n4        X4 0.2612467 3.827800\n5        X5 0.5495336 1.819725\n6        X7 0.3510102 2.848920\n7        X8 0.4310175 2.320091\n8        X9 0.6683173 1.496295\n\n# 회귀진단 그래프들\n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(model_4)\n\n\n\nlayout(1)\n\n# Cook의 거리 - 1을 넘으면 확인을 해야한다 \n\nolsrr::ols_plot_cooksd_chart(model_4) \n\n\n\n### 단계적 선택법 - AIC\n\nmodel_5&lt;-step(model_4)\n\nStart:  AIC=57.45\nY ~ X1 + X2 + X3 + X4 + X5 + X7 + X8 + X9\n\n       Df Sum of Sq    RSS    AIC\n- X3    1     0.789 125.00 55.605\n- X8    1     2.870 127.08 56.001\n- X4    1     3.426 127.63 56.106\n- X9    1     6.654 130.86 56.706\n- X7    1     9.414 133.62 57.207\n&lt;none&gt;              124.21 57.453\n- X5    1    19.702 143.91 58.987\n- X2    1    26.198 150.40 60.046\n- X1    1    40.207 164.41 62.184\n\nStep:  AIC=55.61\nY ~ X1 + X2 + X4 + X5 + X7 + X8 + X9\n\n       Df Sum of Sq    RSS    AIC\n- X8    1     3.369 128.36 54.243\n- X4    1     4.816 129.81 54.513\n- X9    1     9.581 134.58 55.378\n- X7    1     9.655 134.65 55.391\n&lt;none&gt;              125.00 55.605\n- X5    1    18.957 143.95 56.994\n- X2    1    25.474 150.47 58.057\n- X1    1    53.245 178.24 62.122\n\nStep:  AIC=54.24\nY ~ X1 + X2 + X4 + X5 + X7 + X9\n\n       Df Sum of Sq    RSS    AIC\n- X4    1     5.011 133.38 53.163\n- X9    1     6.543 134.91 53.437\n&lt;none&gt;              128.36 54.243\n- X5    1    20.033 148.40 55.724\n- X7    1    22.819 151.18 56.170\n- X2    1    24.299 152.66 56.404\n- X1    1    95.134 223.50 65.552\n\nStep:  AIC=53.16\nY ~ X1 + X2 + X5 + X7 + X9\n\n       Df Sum of Sq    RSS    AIC\n- X9    1     6.223 139.60 52.257\n&lt;none&gt;              133.38 53.163\n- X5    1    17.801 151.18 54.169\n- X7    1    17.873 151.25 54.181\n- X2    1    39.335 172.71 57.365\n- X1    1   157.972 291.35 69.915\n\nStep:  AIC=52.26\nY ~ X1 + X2 + X5 + X7\n\n       Df Sum of Sq    RSS    AIC\n&lt;none&gt;              139.60 52.257\n- X5    1    20.836 160.43 53.596\n- X7    1    21.669 161.27 53.720\n- X2    1    47.409 187.01 57.274\n- X1    1   156.606 296.20 68.312\n\nsummary(model_5)\n\n\nCall:\nlm(formula = Y ~ X1 + X2 + X5 + X7, data = data_use_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5605 -2.0856  0.0238  1.8580  3.8981 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  13.6212     3.6725   3.709 0.001489 ** \nX1            2.4123     0.5225   4.617 0.000188 ***\nX2            8.4589     3.3300   2.540 0.019970 *  \nX5            2.0604     1.2235   1.684 0.108541    \nX7           -2.2154     1.2901  -1.717 0.102176    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.711 on 19 degrees of freedom\nMultiple R-squared:  0.8321,    Adjusted R-squared:  0.7968 \nF-statistic: 23.54 on 4 and 19 DF,  p-value: 3.866e-07\n\n# 회귀진단 그래프들\n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(model_5)\n\n\n\nlayout(1)\n\n# Cook의 거리 - 0.5보다 작으면 괜찮음 / 1보다 큰 값을 고려 / 전체적으로 봤을때 튀는 값이 있는 경우 \n\nolsrr::ols_plot_cooksd_chart(model_5) \n\n\n\n### 17번 제거\n\ndata_use_3&lt;-model_5$model[-17,]\n\nmodel_6&lt;-lm(Y~.,data_use_3)\n\nsummary(model_6)\n\n\nCall:\nlm(formula = Y ~ ., data = data_use_3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.4577 -1.6655  0.1575  1.7978  4.1865 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  13.2263     3.4659   3.816  0.00127 **\nX1            1.7014     0.6247   2.723  0.01394 * \nX2           12.0705     3.6958   3.266  0.00429 **\nX5            2.4602     1.1726   2.098  0.05028 . \nX7           -2.2310     1.2152  -1.836  0.08294 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.553 on 18 degrees of freedom\nMultiple R-squared:  0.8418,    Adjusted R-squared:  0.8067 \nF-statistic: 23.95 on 4 and 18 DF,  p-value: 5.316e-07\n\n# VIF\n\nolsrr::ols_vif_tol(model_6)\n\n  Variables Tolerance      VIF\n1        X1 0.3319061 3.012900\n2        X2 0.3658998 2.732989\n3        X5 0.5664655 1.765333\n4        X7 0.6043771 1.654596\n\n# 회귀진단 그래프들\n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(model_6)\n\n\n\nlayout(1)\n\n# Cook의 거리 - 1을 넘으면 확인을 해야한다 \n\nolsrr::ols_plot_cooksd_chart(model_6)"
  },
  {
    "objectID": "Machine_Learning/19_training_and_deploying_at_scale.html",
    "href": "Machine_Learning/19_training_and_deploying_at_scale.html",
    "title": "19_training_and_deploying_at_scale",
    "section": "",
    "text": "19장 – 대규모 텐서플로 모델 훈련과 배포\n이 노트북은 19장에 있는 모든 샘플 코드를 담고 있습니다."
  },
  {
    "objectID": "Machine_Learning/19_training_and_deploying_at_scale.html#savedmodel-저장과-로딩",
    "href": "Machine_Learning/19_training_and_deploying_at_scale.html#savedmodel-저장과-로딩",
    "title": "19_training_and_deploying_at_scale",
    "section": "SavedModel 저장과 로딩",
    "text": "SavedModel 저장과 로딩\n\n(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\nX_train_full = X_train_full[..., np.newaxis].astype(np.float32) / 255.\nX_test = X_test[..., np.newaxis].astype(np.float32) / 255.\nX_valid, X_train = X_train_full[:5000], X_train_full[5000:]\ny_valid, y_train = y_train_full[:5000], y_train_full[5000:]\nX_new = X_test[:3]\n\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28, 1]),\n    keras.layers.Dense(100, activation=\"relu\"),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=keras.optimizers.SGD(learning_rate=1e-2),\n              metrics=[\"accuracy\"])\nmodel.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n\nEpoch 1/10\n1719/1719 [==============================] - 2s 1ms/step - loss: 1.1140 - accuracy: 0.7066 - val_loss: 0.3715 - val_accuracy: 0.9024\nEpoch 2/10\n1719/1719 [==============================] - 1s 713us/step - loss: 0.3695 - accuracy: 0.8981 - val_loss: 0.2990 - val_accuracy: 0.9144\nEpoch 3/10\n1719/1719 [==============================] - 1s 718us/step - loss: 0.3154 - accuracy: 0.9100 - val_loss: 0.2651 - val_accuracy: 0.9272\nEpoch 4/10\n1719/1719 [==============================] - 1s 706us/step - loss: 0.2765 - accuracy: 0.9223 - val_loss: 0.2436 - val_accuracy: 0.9334\nEpoch 5/10\n1719/1719 [==============================] - 1s 711us/step - loss: 0.2556 - accuracy: 0.9276 - val_loss: 0.2257 - val_accuracy: 0.9364\nEpoch 6/10\n1719/1719 [==============================] - 1s 715us/step - loss: 0.2367 - accuracy: 0.9321 - val_loss: 0.2121 - val_accuracy: 0.9396\nEpoch 7/10\n1719/1719 [==============================] - 1s 729us/step - loss: 0.2198 - accuracy: 0.9390 - val_loss: 0.1970 - val_accuracy: 0.9454\nEpoch 8/10\n1719/1719 [==============================] - 1s 716us/step - loss: 0.2057 - accuracy: 0.9425 - val_loss: 0.1880 - val_accuracy: 0.9476\nEpoch 9/10\n1719/1719 [==============================] - 1s 704us/step - loss: 0.1940 - accuracy: 0.9459 - val_loss: 0.1777 - val_accuracy: 0.9524\nEpoch 10/10\n1719/1719 [==============================] - 1s 711us/step - loss: 0.1798 - accuracy: 0.9482 - val_loss: 0.1684 - val_accuracy: 0.9546\n\n\n&lt;tensorflow.python.keras.callbacks.History at 0x7fe3b8718590&gt;\n\n\n\nnp.round(model.predict(X_new), 2)\n\narray([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n       [0.  , 0.97, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]],\n      dtype=float32)\n\n\n\nmodel_version = \"0001\"\nmodel_name = \"my_mnist_model\"\nmodel_path = os.path.join(model_name, model_version)\nmodel_path\n\n'my_mnist_model/0001'\n\n\n\n!rm -rf {model_name}\n\n\ntf.saved_model.save(model, model_path)\n\nINFO:tensorflow:Assets written to: my_mnist_model/0001/assets\n\n\n\nfor root, dirs, files in os.walk(model_name):\n    indent = '    ' * root.count(os.sep)\n    print('{}{}/'.format(indent, os.path.basename(root)))\n    for filename in files:\n        print('{}{}'.format(indent + '    ', filename))\n\nmy_mnist_model/\n    0001/\n        saved_model.pb\n        variables/\n            variables.data-00000-of-00001\n            variables.index\n        assets/\n\n\n\n!saved_model_cli show --dir {model_path}\n\nThe given SavedModel contains the following tag-sets:\n'serve'\n\n\n\n!saved_model_cli show --dir {model_path} --tag_set serve\n\nThe given SavedModel MetaGraphDef contains SignatureDefs with the following keys:\nSignatureDef key: \"__saved_model_init_op\"\nSignatureDef key: \"serving_default\"\n\n\n\n!saved_model_cli show --dir {model_path} --tag_set serve \\\n                      --signature_def serving_default\n\nThe given SavedModel SignatureDef contains the following input(s):\n  inputs['flatten_input'] tensor_info:\n      dtype: DT_FLOAT\n      shape: (-1, 28, 28, 1)\n      name: serving_default_flatten_input:0\nThe given SavedModel SignatureDef contains the following output(s):\n  outputs['dense_1'] tensor_info:\n      dtype: DT_FLOAT\n      shape: (-1, 10)\n      name: StatefulPartitionedCall:0\nMethod name is: tensorflow/serving/predict\n\n\n\n!saved_model_cli show --dir {model_path} --all\n\n\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n\nsignature_def['__saved_model_init_op']:\n  The given SavedModel SignatureDef contains the following input(s):\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['__saved_model_init_op'] tensor_info:\n        dtype: DT_INVALID\n        shape: unknown_rank\n        name: NoOp\n  Method name is: \n\nsignature_def['serving_default']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['flatten_input'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 28, 28, 1)\n        name: serving_default_flatten_input:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['dense_1'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 10)\n        name: StatefulPartitionedCall:0\n  Method name is: tensorflow/serving/predict\n\nDefined Functions:\n  Function Name: '__call__'\n    Option #1\n      Callable with:\n        Argument #1\n          flatten_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='flatten_input')\n        Argument #2\n          DType: bool\n          Value: True\n        Argument #3\n&lt;&lt;45 more lines&gt;&gt;\n          DType: bool\n          Value: False\n        Argument #3\n          DType: NoneType\n          Value: None\n    Option #2\n      Callable with:\n        Argument #1\n          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='inputs')\n        Argument #2\n          DType: bool\n          Value: True\n        Argument #3\n          DType: NoneType\n          Value: None\n    Option #3\n      Callable with:\n        Argument #1\n          flatten_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='flatten_input')\n        Argument #2\n          DType: bool\n          Value: True\n        Argument #3\n          DType: NoneType\n          Value: None\n    Option #4\n      Callable with:\n        Argument #1\n          flatten_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='flatten_input')\n        Argument #2\n          DType: bool\n          Value: False\n        Argument #3\n          DType: NoneType\n          Value: None\n\n\nX_new를 npy 파일로 만들면 모델에 쉽게 전달할 수 있습니다:\n\nnp.save(\"my_mnist_tests.npy\", X_new)\n\n\ninput_name = model.input_names[0]\ninput_name\n\n'flatten_input'\n\n\n그리고 이제 saved_model_cli를 사용해 방금 저장한 샘플에 대한 예측을 만듭니다:\n\n!saved_model_cli run --dir {model_path} --tag_set serve \\\n                     --signature_def serving_default    \\\n                     --inputs {input_name}=my_mnist_tests.npy\n\n2021-02-18 22:15:30.294109: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2021-02-18 22:15:30.294306: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\nWARNING:tensorflow:From /Users/ageron/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_cli.py:445: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\nINFO:tensorflow:Restoring parameters from my_mnist_model/0001/variables/variables\n2021-02-18 22:15:30.323498: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\nResult for output key dense_1:\n[[1.1347984e-04 1.5187356e-07 9.7032893e-04 2.7640699e-03 3.7826971e-06\n  7.6876910e-05 3.9140293e-08 9.9559116e-01 5.3502394e-05 4.2665208e-04]\n [8.2443521e-04 3.5493889e-05 9.8826385e-01 7.0466995e-03 1.2957400e-07\n  2.3389691e-04 2.5639210e-03 9.5886099e-10 1.0314899e-03 8.7952529e-08]\n [4.4693781e-05 9.7028232e-01 9.0526715e-03 2.2641101e-03 4.8766597e-04\n  2.8800720e-03 2.2714981e-03 8.3753867e-03 4.0439744e-03 2.9759688e-04]]\n\n\n\nnp.round([[1.1347984e-04, 1.5187356e-07, 9.7032893e-04, 2.7640699e-03, 3.7826971e-06,\n           7.6876910e-05, 3.9140293e-08, 9.9559116e-01, 5.3502394e-05, 4.2665208e-04],\n          [8.2443521e-04, 3.5493889e-05, 9.8826385e-01, 7.0466995e-03, 1.2957400e-07,\n           2.3389691e-04, 2.5639210e-03, 9.5886099e-10, 1.0314899e-03, 8.7952529e-08],\n          [4.4693781e-05, 9.7028232e-01, 9.0526715e-03, 2.2641101e-03, 4.8766597e-04,\n           2.8800720e-03, 2.2714981e-03, 8.3753867e-03, 4.0439744e-03, 2.9759688e-04]], 2)\n\narray([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n       [0.  , 0.97, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]])"
  },
  {
    "objectID": "Machine_Learning/19_training_and_deploying_at_scale.html#텐서플로-서빙",
    "href": "Machine_Learning/19_training_and_deploying_at_scale.html#텐서플로-서빙",
    "title": "19_training_and_deploying_at_scale",
    "section": "텐서플로 서빙",
    "text": "텐서플로 서빙\n도커가 없다면 설치하세요. 그리고 다음을 실행하세요:\ndocker pull tensorflow/serving\n\nexport ML_PATH=$HOME/ml # or wherever this project is\ndocker run -it --rm -p 8500:8500 -p 8501:8501 \\\n   -v \"$ML_PATH/my_mnist_model:/models/my_mnist_model\" \\\n   -e MODEL_NAME=my_mnist_model \\\n   tensorflow/serving\n사용이 끝나면 Ctrl-C를 눌러 서버를 종료하세요.\n또는 tensorflow_model_server가 설치되어 있다면 (예를 들어, 이 노트북을 코랩에서 실행하는 경우) 다음 세 개의 셀을 실행하여 서버를 시작하세요:\n\nos.environ[\"MODEL_DIR\"] = os.path.split(os.path.abspath(model_path))[0]\n\n\n%%bash --bg\nnohup tensorflow_model_server \\\n     --rest_api_port=8501 \\\n     --model_name=my_mnist_model \\\n     --model_base_path=\"${MODEL_DIR}\" &gt;server.log 2&gt;&1\n\n\n!tail server.log\n\n2021-02-16 22:33:09.323538: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:93] Reading SavedModel debug info (if present) from: /models/my_mnist_model/0001\n2021-02-16 22:33:09.323642: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2021-02-16 22:33:09.360572: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:206] Restoring SavedModel bundle.\n2021-02-16 22:33:09.361764: I external/org_tensorflow/tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2200000000 Hz\n2021-02-16 22:33:09.387713: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:190] Running initialization op on SavedModel bundle at path: /models/my_mnist_model/0001\n2021-02-16 22:33:09.392739: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags { serve }; Status: success: OK. Took 71106 microseconds.\n2021-02-16 22:33:09.393390: I tensorflow_serving/servables/tensorflow/saved_model_warmup_util.cc:59] No warmup data file found at /models/my_mnist_model/0001/assets.extra/tf_serving_warmup_requests\n2021-02-16 22:33:09.393847: I tensorflow_serving/core/loader_harness.cc:87] Successfully loaded servable version {name: my_mnist_model version: 1}\n2021-02-16 22:33:09.398470: I tensorflow_serving/model_servers/server.cc:371] Running gRPC ModelServer at 0.0.0.0:8500 ...\n[warn] getaddrinfo: address family for nodename not supported\n2021-02-16 22:33:09.405622: I tensorflow_serving/model_servers/server.cc:391] Exporting HTTP/REST API at:localhost:8501 ...\n[evhttp_server.cc : 238] NET_LOG: Entering the event loop ...\n\n\n\nimport json\n\ninput_data_json = json.dumps({\n    \"signature_name\": \"serving_default\",\n    \"instances\": X_new.tolist(),\n})\n\n\nrepr(input_data_json)[:1500] + \"...\"\n\n'\\'{\"signature_name\": \"serving_default\", \"instances\": [[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.32941177487373...'\n\n\n이제 텐서플로 서빙의 REST API를 사용해 예측을 만들어 보죠:\n\nimport requests\n\nSERVER_URL = 'http://localhost:8501/v1/models/my_mnist_model:predict'\nresponse = requests.post(SERVER_URL, data=input_data_json)\nresponse.raise_for_status() # raise an exception in case of error\nresponse = response.json()\n\n\nresponse.keys()\n\ndict_keys(['predictions'])\n\n\n\ny_proba = np.array(response[\"predictions\"])\ny_proba.round(2)\n\narray([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n       [0.  , 0.97, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]])\n\n\n\ngRPC API 사용하기\n\nfrom tensorflow_serving.apis.predict_pb2 import PredictRequest\n\nrequest = PredictRequest()\nrequest.model_spec.name = model_name\nrequest.model_spec.signature_name = \"serving_default\"\ninput_name = model.input_names[0]\nrequest.inputs[input_name].CopyFrom(tf.make_tensor_proto(X_new))\n\n\nimport grpc\nfrom tensorflow_serving.apis import prediction_service_pb2_grpc\n\nchannel = grpc.insecure_channel('localhost:8500')\npredict_service = prediction_service_pb2_grpc.PredictionServiceStub(channel)\nresponse = predict_service.Predict(request, timeout=10.0)\n\n\nresponse\n\noutputs {\n  key: \"dense_1\"\n  value {\n    dtype: DT_FLOAT\n    tensor_shape {\n      dim {\n        size: 3\n      }\n      dim {\n        size: 10\n      }\n    }\n    float_val: 0.00011425172124290839\n    float_val: 1.513665068841874e-07\n    float_val: 0.0009818424005061388\n    float_val: 0.0027773496694862843\n    float_val: 3.758880893656169e-06\n    float_val: 7.6266449468676e-05\n    float_val: 3.9139514740327286e-08\n    float_val: 0.995561957359314\n    float_val: 5.344580131350085e-05\n    float_val: 0.00043088122038170695\n    float_val: 0.0008194865076802671\n    float_val: 3.5498320357874036e-05\n    float_val: 0.9882420897483826\n    float_val: 0.00705744931474328\n    float_val: 1.2937064752804872e-07\n    float_val: 0.00023402832448482513\n    float_val: 0.0025743397418409586\n    float_val: 9.668431610876382e-10\n    float_val: 0.0010369382798671722\n    float_val: 8.833576004008137e-08\n    float_val: 4.441547571332194e-05\n    float_val: 0.970328688621521\n    float_val: 0.009044423699378967\n    float_val: 0.0022599005606025457\n    float_val: 0.00048672096454538405\n    float_val: 0.002873610006645322\n    float_val: 0.002268279204145074\n    float_val: 0.008354829624295235\n    float_val: 0.004041312728077173\n    float_val: 0.0002978229313157499\n  }\n}\nmodel_spec {\n  name: \"my_mnist_model\"\n  version {\n    value: 1\n  }\n  signature_name: \"serving_default\"\n}\n\n\n응답을 텐서로 변환합니다:\n\noutput_name = model.output_names[0]\noutputs_proto = response.outputs[output_name]\ny_proba = tf.make_ndarray(outputs_proto)\ny_proba.round(2)\n\narray([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n       [0.  , 0.97, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]],\n      dtype=float32)\n\n\n클라이언트가 텐서플로 라이브러리를 사용하지 않는다면 넘파이 배열로 변환합니다:\n\noutput_name = model.output_names[0]\noutputs_proto = response.outputs[output_name]\nshape = [dim.size for dim in outputs_proto.tensor_shape.dim]\ny_proba = np.array(outputs_proto.float_val).reshape(shape)\ny_proba.round(2)\n\narray([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n       [0.  , 0.97, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]])"
  },
  {
    "objectID": "Machine_Learning/19_training_and_deploying_at_scale.html#새로운-버전의-모델-배포하기",
    "href": "Machine_Learning/19_training_and_deploying_at_scale.html#새로운-버전의-모델-배포하기",
    "title": "19_training_and_deploying_at_scale",
    "section": "새로운 버전의 모델 배포하기",
    "text": "새로운 버전의 모델 배포하기\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28, 1]),\n    keras.layers.Dense(50, activation=\"relu\"),\n    keras.layers.Dense(50, activation=\"relu\"),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=keras.optimizers.SGD(learning_rate=1e-2),\n              metrics=[\"accuracy\"])\nhistory = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n\nEpoch 1/10\n1719/1719 [==============================] - 1s 748us/step - loss: 1.1567 - accuracy: 0.6691 - val_loss: 0.3418 - val_accuracy: 0.9042\nEpoch 2/10\n1719/1719 [==============================] - 1s 697us/step - loss: 0.3376 - accuracy: 0.9032 - val_loss: 0.2674 - val_accuracy: 0.9242\nEpoch 3/10\n1719/1719 [==============================] - 1s 676us/step - loss: 0.2779 - accuracy: 0.9187 - val_loss: 0.2227 - val_accuracy: 0.9368\nEpoch 4/10\n1719/1719 [==============================] - 1s 669us/step - loss: 0.2362 - accuracy: 0.9318 - val_loss: 0.2032 - val_accuracy: 0.9432\nEpoch 5/10\n1719/1719 [==============================] - 1s 670us/step - loss: 0.2109 - accuracy: 0.9389 - val_loss: 0.1833 - val_accuracy: 0.9482\nEpoch 6/10\n1719/1719 [==============================] - 1s 675us/step - loss: 0.1951 - accuracy: 0.9430 - val_loss: 0.1740 - val_accuracy: 0.9498\nEpoch 7/10\n1719/1719 [==============================] - 1s 667us/step - loss: 0.1799 - accuracy: 0.9474 - val_loss: 0.1605 - val_accuracy: 0.9540\nEpoch 8/10\n1719/1719 [==============================] - 1s 673us/step - loss: 0.1654 - accuracy: 0.9519 - val_loss: 0.1543 - val_accuracy: 0.9558\nEpoch 9/10\n1719/1719 [==============================] - 1s 671us/step - loss: 0.1570 - accuracy: 0.9554 - val_loss: 0.1460 - val_accuracy: 0.9572\nEpoch 10/10\n1719/1719 [==============================] - 1s 672us/step - loss: 0.1420 - accuracy: 0.9583 - val_loss: 0.1359 - val_accuracy: 0.9616\n\n\n\nmodel_version = \"0002\"\nmodel_name = \"my_mnist_model\"\nmodel_path = os.path.join(model_name, model_version)\nmodel_path\n\n'my_mnist_model/0002'\n\n\n\ntf.saved_model.save(model, model_path)\n\nINFO:tensorflow:Assets written to: my_mnist_model/0002/assets\n\n\n\nfor root, dirs, files in os.walk(model_name):\n    indent = '    ' * root.count(os.sep)\n    print('{}{}/'.format(indent, os.path.basename(root)))\n    for filename in files:\n        print('{}{}'.format(indent + '    ', filename))\n\nmy_mnist_model/\n    0001/\n        saved_model.pb\n        variables/\n            variables.data-00000-of-00001\n            variables.index\n        assets/\n    0002/\n        saved_model.pb\n        variables/\n            variables.data-00000-of-00001\n            variables.index\n        assets/\n\n\n경고: 새로운 모델이 텐서플로 서빙에 로드되기 전까지 잠시 기다려야 할 수 있습니다:\n\nimport requests\n\nSERVER_URL = 'http://localhost:8501/v1/models/my_mnist_model:predict'\n            \nresponse = requests.post(SERVER_URL, data=input_data_json)\nresponse.raise_for_status()\nresponse = response.json()\n\n\nresponse.keys()\n\ndict_keys(['predictions'])\n\n\n\ny_proba = np.array(response[\"predictions\"])\ny_proba.round(2)\n\narray([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n       [0.  , 0.99, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]])"
  },
  {
    "objectID": "Machine_Learning/19_training_and_deploying_at_scale.html#여러-서버에서-훈련하기",
    "href": "Machine_Learning/19_training_and_deploying_at_scale.html#여러-서버에서-훈련하기",
    "title": "19_training_and_deploying_at_scale",
    "section": "여러 서버에서 훈련하기",
    "text": "여러 서버에서 훈련하기\n텐서플로 클러스터는 일반적으로 여러 서버에서 병렬로 실행되는 텐서플로 프로세스의 그룹입니다. 신경망을 훈련하거나 실행하는 작업을 완료하기 위해 프로세스가 서로 대화합니다. 클러스터에 있는 개별 TF 프로세스를 “태스크”라고 부릅니다(또는 “TF 서버”). 태스크는 IP 주소, 포트, 타입(또는 역할이나 잡(job)으로 부릅니다). 타입은 \"worker\", \"chief\", \"ps\"(파라미터 서버), \"evaluator\"가 있습니다: * 각 워커는 일반적으로 하나 이상의 GPU를 가진 머신에서 계산을 수행합니다. * 치프도 계산을 수행합니다. 하지만 텐서 보드 로그를 기록하거나 체크포인트를 저장하는 등의 추가적인 작업을 처리합니다. 클러스터에는 하나의 치프가 있고 일반적으로 첫 번째 워커가 치프입니다(즉, 워커 #0). * 파라미터 서버(ps)는 변수 값만 가지고 있습니다. 일반적으로 CPU만 가진 머신입니다. * evaluator는 평가를 담당합ㄴ디ㅏ. 일반적으로 클러스터 내에 하나의 evaluator가 있습니다.\n동일한 타입을 공유하는 작업을 종종 “잡”(job)이라고 부릅니다. 예를 들어, “워커” 잡은 모든 워커의 집합입니다.\n텐서플로 클러스터를 시작하려면 먼저 이를 정의해야 합니다. 모든 태스크(IP 주소, TCP 포트, 타입)를 지정한다는 것을 의미합니다. 예를 들어 다음 클러스터 명세는 세 개의 태스크로 구성된 클러스터를 정의합니다(두 대의 워커와 한 대의 파라미터 서버). 잡마다 하나의 키를 가진 딕셔너리이며 값은 태스크 주소의 리스트입니다:\n\ncluster_spec = {\n    \"worker\": [\n        \"machine-a.example.com:2222\",  # /job:worker/task:0\n        \"machine-b.example.com:2222\"   # /job:worker/task:1\n    ],\n    \"ps\": [\"machine-c.example.com:2222\"] # /job:ps/task:0\n}\n\n클러스터에 있는 각 태스크는 서버에 있는 다른 태스크와 통신할 수 있습니다. 따라서 해당 머신의 포트 사이에 모든 통신이 가능하도록 방화벽을 설정해야 합니다(모든 머신에서 동일한 포트를 사용하면 간단히 설정할 수 있습니다).\n태스크가 시작될 때, 타입과 인덱스(태스크 인덱스를 태스크 아이디라고도 합니다)를 알려야 합니다. 한 번에 (클러스터 스펙과 현재 작업의 타입, 아이디를) 모두 정의하는 일반적인 방법은 프로그램을 시작하기 전에 TF_CONFIG 환경 변수를 설정하는 것입니다. (\"cluster\" 키 아래) 클러스터 스펙과 (\"task\" 키 아래) 시작할 태스크의 타입과 인덱스를 담은 JSON으로 인코딩된 딕셔너리입니다. 예를 들어 다음 TF_CONFIG 환경 변수는 위와 동일한 클러스터를 정의합니다. 두 대의 워커와 한 대의 파라미터 서버, 그리고 시작할 태스크는 워커 #1입니다:\n\nimport os\nimport json\n\nos.environ[\"TF_CONFIG\"] = json.dumps({\n    \"cluster\": cluster_spec,\n    \"task\": {\"type\": \"worker\", \"index\": 1}\n})\nos.environ[\"TF_CONFIG\"]\n\n'{\"cluster\": {\"worker\": [\"machine-a.example.com:2222\", \"machine-b.example.com:2222\"], \"ps\": [\"machine-c.example.com:2222\"]}, \"task\": {\"type\": \"worker\", \"index\": 1}}'\n\n\n일부 플랫폼(예를 들면, 구글 클라우드 ML 엔진)은 자동으로 이런 환경을 설정합니다.\n텐서플로의 TFConfigClusterResolver 클래스는 환경 변수에서 클러스터 스펙을 읽습니다:\n\nimport tensorflow as tf\n\nresolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\nresolver.cluster_spec()\n\nClusterSpec({'ps': ['machine-c.example.com:2222'], 'worker': ['machine-a.example.com:2222', 'machine-b.example.com:2222']})\n\n\n\nresolver.task_type\n\n'worker'\n\n\n\nresolver.task_id\n\n1\n\n\n이제 간단한 클러스터를 시작해 보죠. 두 개의 워커 태스크를 로컬 머신에서 실행합니다. MultiWorkerMirroredStrategy를 사용해 두 태스크에서 모델을 훈련하겠습니다.\n첫 번째 단계로 훈련 코드를 작성합니다. 이 코드는 자체 프로세스를 가진 두 워커를 실행하는데 사용되기 때문에 별도의 파이썬 파일 my_mnist_multiworker_task.py로 이 코드를 저장합니다. 이 코드는 비교적 간단하지만 몇 가지 중요한 점이 있습니다: * 텐서플로로 무엇을 하기전에 MultiWorkerMirroredStrategy를 생성합니다. * 워커 중 한 대에서 텐서보드 로깅과 체크포인트 저장을 담당합니다. 앞서 언급한대로 이 워커를 치프 라고 부릅니다. 관례적으로 워커 #0입니다.\n\n%%writefile my_mnist_multiworker_task.py\n\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nimport time\n\n# 프로그램 시작 부분에\ndistribution = tf.distribute.MultiWorkerMirroredStrategy()\n\nresolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\nprint(\"Starting task {}{}\".format(resolver.task_type, resolver.task_id))\n\n# 워커 #0이 체크포인트 저장과 텐서보드 로깅을 수행합니다\nif resolver.task_id == 0:\n    root_logdir = os.path.join(os.curdir, \"my_mnist_multiworker_logs\")\n    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n    run_dir = os.path.join(root_logdir, run_id)\n    callbacks = [\n        keras.callbacks.TensorBoard(run_dir),\n        keras.callbacks.ModelCheckpoint(\"my_mnist_multiworker_model.h5\",\n                                        save_best_only=True),\n    ]\nelse:\n    callbacks = []\n\n# MNIST 데이터셋을 로드하고 준비합니다\n(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\nX_train_full = X_train_full[..., np.newaxis] / 255.\nX_valid, X_train = X_train_full[:5000], X_train_full[5000:]\ny_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n\nwith distribution.scope():\n    model = keras.models.Sequential([\n        keras.layers.Conv2D(filters=64, kernel_size=7, activation=\"relu\",\n                            padding=\"same\", input_shape=[28, 28, 1]),\n        keras.layers.MaxPooling2D(pool_size=2),\n        keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\",\n                            padding=\"same\"), \n        keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\",\n                            padding=\"same\"),\n        keras.layers.MaxPooling2D(pool_size=2),\n        keras.layers.Flatten(),\n        keras.layers.Dense(units=64, activation='relu'),\n        keras.layers.Dropout(0.5),\n        keras.layers.Dense(units=10, activation='softmax'),\n    ])\n    model.compile(loss=\"sparse_categorical_crossentropy\",\n                  optimizer=keras.optimizers.SGD(learning_rate=1e-2),\n                  metrics=[\"accuracy\"])\n\nmodel.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n          epochs=10, callbacks=callbacks)\n\nOverwriting my_mnist_multiworker_task.py\n\n\n실제 애플리케이션에서는 일반적으로 머신마다 하나의 워커가 있지만 이 예에서는 동일한 머신에 두 워커를 실행합니다. 따라서 (GPU가 있다면) 두 워커가 모두 가용한 GPU 램을 사용하려고 하기 때문에 메모리 부족 에러가 날 수 있습니다. 이를 피하려면 CUDA_VISIBLE_DEVICES 환경 변수를 사용해 워커마다 다른 GPU를 할당할 수 있습니다. 또는 다음처럼 간단하게 GPU를 사용하지 않게 설정할 수 있습니다:\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n\n이제 파이썬의 subprocess 모델을 사용해 두 워커를 각각의 개별 프로세스로 시작할 준비가 되었습니다. 프로세스를 시작하기 전에 TF_CONFIG 환경 변수에 태스크 인덱스를 적절히 설정해야 합니다:\n\nimport subprocess\n\ncluster_spec = {\"worker\": [\"127.0.0.1:9901\", \"127.0.0.1:9902\"]}\n\nfor index, worker_address in enumerate(cluster_spec[\"worker\"]):\n    os.environ[\"TF_CONFIG\"] = json.dumps({\n        \"cluster\": cluster_spec,\n        \"task\": {\"type\": \"worker\", \"index\": index}\n    })\n    subprocess.Popen(\"python my_mnist_multiworker_task.py\", shell=True)\n\n이제 됐습니다! 텐서플로 클러스터가 실행되었습니다. 하지만 별도의 프로세스로 실행되기 때문에 이 노트북에서는 볼 수 없습니다(하지만 이 노트북을 주피터에서 실행한다면 주피터 서버 로그에서 워커 로그를 볼 수 있습니다).\n치프(워커 #0)가 텐서보드 로그를 작성하기 때문에 텐서보드로 훈련 과정을 볼 수 있습니다. 다음 셀을 실행하고 텐서보드 인터페이스의 설정(setting) 버튼을 누르고, “Reload data” 체크박스를 선택하면 텐서보드가 자동으로 30초마다 리프레시됩니다. 첫 번째 훈련 에포크가 끝나고 (몇 분 걸립니다) 텐서보드가 리프레시되면 SCALARS 탭이 나타날 것입니다. 이 탭을 클릭하고 모델의 훈련과 검증 정확도를 확인하세요.\n\n%load_ext tensorboard\n%tensorboard --logdir=./my_mnist_multiworker_logs --port=6006\n\nThe tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n\n\n\n      \n      \n      \n    \n\n\n훈련이 끝나면 최상의 모델 체크포인트가 my_mnist_multiworker_model.h5 파일에 저장됩니다. keras.models.load_model()를 사용해 이를 로드하고 예측에 사용할 수 있습니다:\n\nfrom tensorflow import keras\n\nmodel = keras.models.load_model(\"my_mnist_multiworker_model.h5\")\nY_pred = model.predict(X_new)\nnp.argmax(Y_pred, axis=-1)\n\narray([7, 2, 1])\n\n\n이 장의 노트북은 여기까지입니다! 이 내용이 도움이 되었으면 좋겠습니다. 😊"
  },
  {
    "objectID": "Machine_Learning/19_training_and_deploying_at_scale.html#to-8.",
    "href": "Machine_Learning/19_training_and_deploying_at_scale.html#to-8.",
    "title": "19_training_and_deploying_at_scale",
    "section": "1. to 8.",
    "text": "1. to 8.\n부록 A 참조"
  },
  {
    "objectID": "Machine_Learning/19_training_and_deploying_at_scale.html#section",
    "href": "Machine_Learning/19_training_and_deploying_at_scale.html#section",
    "title": "19_training_and_deploying_at_scale",
    "section": "9.",
    "text": "9.\n연습문제: (원하는 어떤 모델이든) 모델을 훈련하고 TF 서빙이나 구글 클라우드 AI 플랫폼에 배포해보세요. REST API나 gRPC API를 사용해 쿼리하는 클라이언트 코드를 작성해보세요. 모델을 업데이트하고 새로운 버전을 배포해보세요. 클라이언트 코드가 새로운 버전으로 쿼리할 것입니다. 첫 번째 버전으로 롤백해보세요.\n텐서플로 서빙(TFS)으로 텐서플로 모델 배포하기 절에 있는 단계를 따라해 보세요."
  },
  {
    "objectID": "Machine_Learning/17_autoencoders_and_gans.html",
    "href": "Machine_Learning/17_autoencoders_and_gans.html",
    "title": "17_autoencoders_and_gans",
    "section": "",
    "text": "17장 – 오토인코더와 GAN\n이 노트북은 17장에 있는 모든 샘플 코드를 담고 있습니다."
  },
  {
    "objectID": "Machine_Learning/17_autoencoders_and_gans.html#한-번에-모든-층을-훈련하기",
    "href": "Machine_Learning/17_autoencoders_and_gans.html#한-번에-모든-층을-훈련하기",
    "title": "17_autoencoders_and_gans",
    "section": "한 번에 모든 층을 훈련하기",
    "text": "한 번에 모든 층을 훈련하기\n3개의 은닉층과 1개의 출력층(즉, 두 개를 적층)을 가진 적층 오토인코더를 만들어 보겠습니다.\n\ndef rounded_accuracy(y_true, y_pred):\n    return keras.metrics.binary_accuracy(tf.round(y_true), tf.round(y_pred))\n\n\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nstacked_encoder = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(100, activation=\"selu\"),\n    keras.layers.Dense(30, activation=\"selu\"),\n])\nstacked_decoder = keras.models.Sequential([\n    keras.layers.Dense(100, activation=\"selu\", input_shape=[30]),\n    keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n    keras.layers.Reshape([28, 28])\n])\nstacked_ae = keras.models.Sequential([stacked_encoder, stacked_decoder])\nstacked_ae.compile(loss=\"binary_crossentropy\",\n                   optimizer=keras.optimizers.SGD(learning_rate=1.5), metrics=[rounded_accuracy])\nhistory = stacked_ae.fit(X_train, X_train, epochs=20,\n                         validation_data=(X_valid, X_valid))\n\n/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n\n\nEpoch 1/20\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.3381 - rounded_accuracy: 0.8870 - val_loss: 0.3173 - val_rounded_accuracy: 0.8989\nEpoch 2/20\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.3056 - rounded_accuracy: 0.9151 - val_loss: 0.3020 - val_rounded_accuracy: 0.9199\nEpoch 3/20\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.2986 - rounded_accuracy: 0.9215 - val_loss: 0.2987 - val_rounded_accuracy: 0.9196\nEpoch 4/20\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.2948 - rounded_accuracy: 0.9249 - val_loss: 0.2937 - val_rounded_accuracy: 0.9285\nEpoch 5/20\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.2923 - rounded_accuracy: 0.9272 - val_loss: 0.2919 - val_rounded_accuracy: 0.9284\nEpoch 6/20\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2904 - rounded_accuracy: 0.9289 - val_loss: 0.2915 - val_rounded_accuracy: 0.9306\nEpoch 7/20\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.2890 - rounded_accuracy: 0.9301 - val_loss: 0.2906 - val_rounded_accuracy: 0.9281\nEpoch 8/20\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.2877 - rounded_accuracy: 0.9313 - val_loss: 0.2946 - val_rounded_accuracy: 0.9296\nEpoch 9/20\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.2869 - rounded_accuracy: 0.9321 - val_loss: 0.2907 - val_rounded_accuracy: 0.9257\nEpoch 10/20\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.2859 - rounded_accuracy: 0.9330 - val_loss: 0.2878 - val_rounded_accuracy: 0.9309\nEpoch 11/20\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.2851 - rounded_accuracy: 0.9336 - val_loss: 0.2864 - val_rounded_accuracy: 0.9348\nEpoch 12/20\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.2845 - rounded_accuracy: 0.9341 - val_loss: 0.2856 - val_rounded_accuracy: 0.9355\nEpoch 13/20\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.2839 - rounded_accuracy: 0.9346 - val_loss: 0.2846 - val_rounded_accuracy: 0.9353\nEpoch 14/20\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.2835 - rounded_accuracy: 0.9350 - val_loss: 0.2844 - val_rounded_accuracy: 0.9365\nEpoch 15/20\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2830 - rounded_accuracy: 0.9355 - val_loss: 0.2841 - val_rounded_accuracy: 0.9353\nEpoch 16/20\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2826 - rounded_accuracy: 0.9358 - val_loss: 0.2846 - val_rounded_accuracy: 0.9355\nEpoch 17/20\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.2822 - rounded_accuracy: 0.9361 - val_loss: 0.2835 - val_rounded_accuracy: 0.9363\nEpoch 18/20\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.2819 - rounded_accuracy: 0.9363 - val_loss: 0.2831 - val_rounded_accuracy: 0.9363\nEpoch 19/20\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.2816 - rounded_accuracy: 0.9365 - val_loss: 0.2827 - val_rounded_accuracy: 0.9374\nEpoch 20/20\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.2813 - rounded_accuracy: 0.9368 - val_loss: 0.2840 - val_rounded_accuracy: 0.9367\n\n\n이 함수는 오토인코더를 사용해 몇 개의 테스트 이미지를 처리합니다. 그런 다음 원본 이미지와 재구성 이미지를 그립니다:\n\ndef show_reconstructions(model, images=X_valid, n_images=5):\n    reconstructions = model.predict(images[:n_images])\n    fig = plt.figure(figsize=(n_images * 1.5, 3))\n    for image_index in range(n_images):\n        plt.subplot(2, n_images, 1 + image_index)\n        plot_image(images[image_index])\n        plt.subplot(2, n_images, 1 + n_images + image_index)\n        plot_image(reconstructions[image_index])\n\n\nshow_reconstructions(stacked_ae)\nsave_fig(\"reconstruction_plot\")\n\nSaving figure reconstruction_plot"
  },
  {
    "objectID": "Machine_Learning/17_autoencoders_and_gans.html#가중치-묶기",
    "href": "Machine_Learning/17_autoencoders_and_gans.html#가중치-묶기",
    "title": "17_autoencoders_and_gans",
    "section": "가중치 묶기",
    "text": "가중치 묶기\n인코더의 가중치를 전치(transpose)하여 디코더의 가중치로 사용하는 식으로 인코더와 디코더의 가중치를 묶는 일은 흔합니다. 이렇게 하려면 사용자 정의 층을 사용해야 합니다.\n\nclass DenseTranspose(keras.layers.Layer):\n    def __init__(self, dense, activation=None, **kwargs):\n        self.dense = dense\n        self.activation = keras.activations.get(activation)\n        super().__init__(**kwargs)\n    def build(self, batch_input_shape):\n        self.biases = self.add_weight(name=\"bias\",\n                                      shape=[self.dense.input_shape[-1]],\n                                      initializer=\"zeros\")\n        super().build(batch_input_shape)\n    def call(self, inputs):\n        z = tf.matmul(inputs, self.dense.weights[0], transpose_b=True)\n        return self.activation(z + self.biases)\n\n\nkeras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\ndense_1 = keras.layers.Dense(100, activation=\"selu\")\ndense_2 = keras.layers.Dense(30, activation=\"selu\")\n\ntied_encoder = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    dense_1,\n    dense_2\n])\n\ntied_decoder = keras.models.Sequential([\n    DenseTranspose(dense_2, activation=\"selu\"),\n    DenseTranspose(dense_1, activation=\"sigmoid\"),\n    keras.layers.Reshape([28, 28])\n])\n\ntied_ae = keras.models.Sequential([tied_encoder, tied_decoder])\n\ntied_ae.compile(loss=\"binary_crossentropy\",\n                optimizer=keras.optimizers.SGD(learning_rate=1.5), metrics=[rounded_accuracy])\nhistory = tied_ae.fit(X_train, X_train, epochs=10,\n                      validation_data=(X_valid, X_valid))\n\n/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n\n\nEpoch 1/10\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.3269 - rounded_accuracy: 0.8960 - val_loss: 0.3081 - val_rounded_accuracy: 0.9077\nEpoch 2/10\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.2975 - rounded_accuracy: 0.9224 - val_loss: 0.2952 - val_rounded_accuracy: 0.9284\nEpoch 3/10\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2920 - rounded_accuracy: 0.9275 - val_loss: 0.3016 - val_rounded_accuracy: 0.9090\nEpoch 4/10\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.2889 - rounded_accuracy: 0.9302 - val_loss: 0.2880 - val_rounded_accuracy: 0.9332\nEpoch 5/10\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2865 - rounded_accuracy: 0.9325 - val_loss: 0.2873 - val_rounded_accuracy: 0.9316\nEpoch 6/10\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.2850 - rounded_accuracy: 0.9340 - val_loss: 0.2861 - val_rounded_accuracy: 0.9353\nEpoch 7/10\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.2838 - rounded_accuracy: 0.9350 - val_loss: 0.2858 - val_rounded_accuracy: 0.9364\nEpoch 8/10\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2830 - rounded_accuracy: 0.9358 - val_loss: 0.2837 - val_rounded_accuracy: 0.9370\nEpoch 9/10\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.2823 - rounded_accuracy: 0.9364 - val_loss: 0.2860 - val_rounded_accuracy: 0.9301\nEpoch 10/10\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2817 - rounded_accuracy: 0.9369 - val_loss: 0.2842 - val_rounded_accuracy: 0.9330\n\n\n\nshow_reconstructions(tied_ae)\nplt.show()"
  },
  {
    "objectID": "Machine_Learning/17_autoencoders_and_gans.html#한-번에-하나의-오토인코더-훈련하기",
    "href": "Machine_Learning/17_autoencoders_and_gans.html#한-번에-하나의-오토인코더-훈련하기",
    "title": "17_autoencoders_and_gans",
    "section": "한 번에 하나의 오토인코더 훈련하기",
    "text": "한 번에 하나의 오토인코더 훈련하기\n\ndef train_autoencoder(n_neurons, X_train, X_valid, loss, optimizer,\n                      n_epochs=10, output_activation=None, metrics=None):\n    n_inputs = X_train.shape[-1]\n    encoder = keras.models.Sequential([\n        keras.layers.Dense(n_neurons, activation=\"selu\", input_shape=[n_inputs])\n    ])\n    decoder = keras.models.Sequential([\n        keras.layers.Dense(n_inputs, activation=output_activation),\n    ])\n    autoencoder = keras.models.Sequential([encoder, decoder])\n    autoencoder.compile(optimizer, loss, metrics=metrics)\n    autoencoder.fit(X_train, X_train, epochs=n_epochs,\n                    validation_data=(X_valid, X_valid))\n    return encoder, decoder, encoder(X_train), encoder(X_valid)\n\n\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nK = keras.backend\nX_train_flat = K.batch_flatten(X_train) # equivalent to .reshape(-1, 28 * 28)\nX_valid_flat = K.batch_flatten(X_valid)\nenc1, dec1, X_train_enc1, X_valid_enc1 = train_autoencoder(\n    100, X_train_flat, X_valid_flat, \"binary_crossentropy\",\n    keras.optimizers.SGD(learning_rate=1.5), output_activation=\"sigmoid\",\n    metrics=[rounded_accuracy])\nenc2, dec2, _, _ = train_autoencoder(\n    30, X_train_enc1, X_valid_enc1, \"mse\", keras.optimizers.SGD(learning_rate=0.05),\n    output_activation=\"selu\")\n\n/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n\n\nEpoch 1/10\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.3445 - rounded_accuracy: 0.8874 - val_loss: 0.3123 - val_rounded_accuracy: 0.9146\nEpoch 2/10\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.3039 - rounded_accuracy: 0.9203 - val_loss: 0.3006 - val_rounded_accuracy: 0.9246\nEpoch 3/10\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.2949 - rounded_accuracy: 0.9286 - val_loss: 0.2934 - val_rounded_accuracy: 0.9317\nEpoch 4/10\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.2891 - rounded_accuracy: 0.9342 - val_loss: 0.2888 - val_rounded_accuracy: 0.9363\nEpoch 5/10\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.2853 - rounded_accuracy: 0.9378 - val_loss: 0.2857 - val_rounded_accuracy: 0.9392\nEpoch 6/10\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.2827 - rounded_accuracy: 0.9403 - val_loss: 0.2834 - val_rounded_accuracy: 0.9409\nEpoch 7/10\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.2807 - rounded_accuracy: 0.9422 - val_loss: 0.2817 - val_rounded_accuracy: 0.9427\nEpoch 8/10\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2792 - rounded_accuracy: 0.9437 - val_loss: 0.2803 - val_rounded_accuracy: 0.9440\nEpoch 9/10\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.2779 - rounded_accuracy: 0.9449 - val_loss: 0.2792 - val_rounded_accuracy: 0.9450\nEpoch 10/10\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.2769 - rounded_accuracy: 0.9459 - val_loss: 0.2783 - val_rounded_accuracy: 0.9462\nEpoch 1/10\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.5618 - val_loss: 0.3441\nEpoch 2/10\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2613 - val_loss: 0.2370\nEpoch 3/10\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2254 - val_loss: 0.2174\nEpoch 4/10\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2110 - val_loss: 0.2060\nEpoch 5/10\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2035 - val_loss: 0.1973\nEpoch 6/10\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.1988 - val_loss: 0.1978\nEpoch 7/10\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.1971 - val_loss: 0.1995\nEpoch 8/10\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.1955 - val_loss: 0.2004\nEpoch 9/10\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.1949 - val_loss: 0.1933\nEpoch 10/10\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.1939 - val_loss: 0.1952\n\n\n\nstacked_ae_1_by_1 = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    enc1, enc2, dec2, dec1,\n    keras.layers.Reshape([28, 28])\n])\n\n\nshow_reconstructions(stacked_ae_1_by_1)\nplt.show()\n\n\n\n\n\nstacked_ae_1_by_1.compile(loss=\"binary_crossentropy\",\n                          optimizer=keras.optimizers.SGD(learning_rate=0.1), metrics=[rounded_accuracy])\nhistory = stacked_ae_1_by_1.fit(X_train, X_train, epochs=10,\n                                validation_data=(X_valid, X_valid))\n\n/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n\n\nEpoch 1/10\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2867 - rounded_accuracy: 0.9343 - val_loss: 0.2883 - val_rounded_accuracy: 0.9341\nEpoch 2/10\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2863 - rounded_accuracy: 0.9347 - val_loss: 0.2881 - val_rounded_accuracy: 0.9347\nEpoch 3/10\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.2861 - rounded_accuracy: 0.9349 - val_loss: 0.2879 - val_rounded_accuracy: 0.9347\nEpoch 4/10\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2859 - rounded_accuracy: 0.9351 - val_loss: 0.2877 - val_rounded_accuracy: 0.9350\nEpoch 5/10\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.2858 - rounded_accuracy: 0.9353 - val_loss: 0.2876 - val_rounded_accuracy: 0.9351\nEpoch 6/10\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.2857 - rounded_accuracy: 0.9354 - val_loss: 0.2874 - val_rounded_accuracy: 0.9351\nEpoch 7/10\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2856 - rounded_accuracy: 0.9355 - val_loss: 0.2874 - val_rounded_accuracy: 0.9352\nEpoch 8/10\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2854 - rounded_accuracy: 0.9356 - val_loss: 0.2873 - val_rounded_accuracy: 0.9353\nEpoch 9/10\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2854 - rounded_accuracy: 0.9357 - val_loss: 0.2871 - val_rounded_accuracy: 0.9353\nEpoch 10/10\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.2853 - rounded_accuracy: 0.9358 - val_loss: 0.2871 - val_rounded_accuracy: 0.9356\n\n\n\nshow_reconstructions(stacked_ae_1_by_1)\nplt.show()"
  },
  {
    "objectID": "Machine_Learning/17_autoencoders_and_gans.html#밀집-층-대신에-합성곱-층-사용하기",
    "href": "Machine_Learning/17_autoencoders_and_gans.html#밀집-층-대신에-합성곱-층-사용하기",
    "title": "17_autoencoders_and_gans",
    "section": "밀집 층 대신에 합성곱 층 사용하기",
    "text": "밀집 층 대신에 합성곱 층 사용하기\n3개의 은닉층과 1개의 출력층(즉, 두 개를 적층)을 가진 적층 오토인코더를 만들어 보겠습니다.\n\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nconv_encoder = keras.models.Sequential([\n    keras.layers.Reshape([28, 28, 1], input_shape=[28, 28]),\n    keras.layers.Conv2D(16, kernel_size=3, padding=\"SAME\", activation=\"selu\"),\n    keras.layers.MaxPool2D(pool_size=2),\n    keras.layers.Conv2D(32, kernel_size=3, padding=\"SAME\", activation=\"selu\"),\n    keras.layers.MaxPool2D(pool_size=2),\n    keras.layers.Conv2D(64, kernel_size=3, padding=\"SAME\", activation=\"selu\"),\n    keras.layers.MaxPool2D(pool_size=2)\n])\nconv_decoder = keras.models.Sequential([\n    keras.layers.Conv2DTranspose(32, kernel_size=3, strides=2, padding=\"VALID\", activation=\"selu\",\n                                 input_shape=[3, 3, 64]),\n    keras.layers.Conv2DTranspose(16, kernel_size=3, strides=2, padding=\"SAME\", activation=\"selu\"),\n    keras.layers.Conv2DTranspose(1, kernel_size=3, strides=2, padding=\"SAME\", activation=\"sigmoid\"),\n    keras.layers.Reshape([28, 28])\n])\nconv_ae = keras.models.Sequential([conv_encoder, conv_decoder])\n\nconv_ae.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.SGD(learning_rate=1.0),\n                metrics=[rounded_accuracy])\nhistory = conv_ae.fit(X_train, X_train, epochs=5,\n                      validation_data=(X_valid, X_valid))\n\n/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n\n\nEpoch 1/5\n1719/1719 [==============================] - 22s 4ms/step - loss: 0.3018 - rounded_accuracy: 0.9187 - val_loss: 0.2848 - val_rounded_accuracy: 0.9287\nEpoch 2/5\n1719/1719 [==============================] - 7s 4ms/step - loss: 0.2756 - rounded_accuracy: 0.9414 - val_loss: 0.2729 - val_rounded_accuracy: 0.9456\nEpoch 3/5\n1719/1719 [==============================] - 7s 4ms/step - loss: 0.2708 - rounded_accuracy: 0.9462 - val_loss: 0.2696 - val_rounded_accuracy: 0.9497\nEpoch 4/5\n1719/1719 [==============================] - 7s 4ms/step - loss: 0.2682 - rounded_accuracy: 0.9490 - val_loss: 0.2685 - val_rounded_accuracy: 0.9492\nEpoch 5/5\n1719/1719 [==============================] - 7s 4ms/step - loss: 0.2664 - rounded_accuracy: 0.9510 - val_loss: 0.2671 - val_rounded_accuracy: 0.9509\n\n\n\nconv_encoder.summary()\nconv_decoder.summary()\n\nModel: \"sequential_10\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nreshape_2 (Reshape)          (None, 28, 28, 1)         0         \n_________________________________________________________________\nconv2d (Conv2D)              (None, 28, 28, 16)        160       \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 14, 14, 16)        0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 14, 14, 32)        4640      \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 7, 7, 32)          0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 7, 7, 64)          18496     \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 3, 3, 64)          0         \n=================================================================\nTotal params: 23,296\nTrainable params: 23,296\nNon-trainable params: 0\n_________________________________________________________________\nModel: \"sequential_11\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_transpose (Conv2DTran (None, 7, 7, 32)          18464     \n_________________________________________________________________\nconv2d_transpose_1 (Conv2DTr (None, 14, 14, 16)        4624      \n_________________________________________________________________\nconv2d_transpose_2 (Conv2DTr (None, 28, 28, 1)         145       \n_________________________________________________________________\nreshape_3 (Reshape)          (None, 28, 28)            0         \n=================================================================\nTotal params: 23,233\nTrainable params: 23,233\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nshow_reconstructions(conv_ae)\nplt.show()\n\nWARNING:tensorflow:5 out of the last 161 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7fe6d6d61170&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details."
  },
  {
    "objectID": "Machine_Learning/17_autoencoders_and_gans.html#패션-이미지-생성하기",
    "href": "Machine_Learning/17_autoencoders_and_gans.html#패션-이미지-생성하기",
    "title": "17_autoencoders_and_gans",
    "section": "패션 이미지 생성하기",
    "text": "패션 이미지 생성하기\n\ndef plot_multiple_images(images, n_cols=None):\n    n_cols = n_cols or len(images)\n    n_rows = (len(images) - 1) // n_cols + 1\n    if images.shape[-1] == 1:\n        images = np.squeeze(images, axis=-1)\n    plt.figure(figsize=(n_cols, n_rows))\n    for index, image in enumerate(images):\n        plt.subplot(n_rows, n_cols, index + 1)\n        plt.imshow(image, cmap=\"binary\")\n        plt.axis(\"off\")\n\n몇 개의 랜덤한 코딩을 생성하고 이를 디코딩하여 결과 이미지를 출력합니다:\n\ntf.random.set_seed(42)\n\ncodings = tf.random.normal(shape=[12, codings_size])\nimages = variational_decoder(codings).numpy()\nplot_multiple_images(images, 4)\nsave_fig(\"vae_generated_images_plot\", tight_layout=False)\n\nSaving figure vae_generated_images_plot\n\n\n\n\n\n이미지 사이에서 의미 보간을 수행해 보죠:\n\ntf.random.set_seed(42)\nnp.random.seed(42)\n\ncodings_grid = tf.reshape(codings, [1, 3, 4, codings_size])\nlarger_grid = tf.image.resize(codings_grid, size=[5, 7])\ninterpolated_codings = tf.reshape(larger_grid, [-1, codings_size])\nimages = variational_decoder(interpolated_codings).numpy()\n\nplt.figure(figsize=(7, 5))\nfor index, image in enumerate(images):\n    plt.subplot(5, 7, index + 1)\n    if index%7%2==0 and index//7%2==0:\n        plt.gca().get_xaxis().set_visible(False)\n        plt.gca().get_yaxis().set_visible(False)\n    else:\n        plt.axis(\"off\")\n    plt.imshow(image, cmap=\"binary\")\nsave_fig(\"semantic_interpolation_plot\", tight_layout=False)\n\nSaving figure semantic_interpolation_plot"
  },
  {
    "objectID": "Machine_Learning/17_autoencoders_and_gans.html#이진-오토인코더를-사용한-해싱",
    "href": "Machine_Learning/17_autoencoders_and_gans.html#이진-오토인코더를-사용한-해싱",
    "title": "17_autoencoders_and_gans",
    "section": "이진 오토인코더를 사용한 해싱",
    "text": "이진 오토인코더를 사용한 해싱\n패션 MNIST 데이터셋을 다시 로드합니다:\n\n(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\nX_train_full = X_train_full.astype(np.float32) / 255\nX_test = X_test.astype(np.float32) / 255\nX_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\ny_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n\n인코더의 출력 층이 시그모이드 활성화 함수와 16개의 뉴런을 가지고 있고 가우시안 잡음이 그 앞에 놓인 오토인코더를 훈련해 보죠. 훈련하는 동안 잡음 층이 이전 층이 큰 값을 출력하도록 학습시킵니다. 작은 값은 잡음에 묻히기 때문입니다. 결국 시그모이드 활성화 함수 덕분에 출력층은 0~1 사이의 값을 출력합니다. 출력값을 0과 1로 반올림하면 16비트 “시맨틱” 해시를 얻습니다. 모든 것이 잘 수행되면 비슷한 이미지는 같은 해시를 가지게 될 것입니다. 이는 검색 엔진에 매우 유용합니다. 예를 들어, 이미지의 시맨틱 해시에 따라 서버에 이미지를 저장하면 같은 서버에는 모두 비슷한 이미지가 저장될 것입니다. 검색 엔진 사용자가 탐색할 이미지를 전달하면, 검색 엔진이 인코더를 사용해 이 이미지의 해시를 계산하고, 이 해시에 해당하는 서버의 모든 이미지를 빠르게 반환할 수 있습니다.\n\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nhashing_encoder = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(100, activation=\"selu\"),\n    keras.layers.GaussianNoise(15.),\n    keras.layers.Dense(16, activation=\"sigmoid\"),\n])\nhashing_decoder = keras.models.Sequential([\n    keras.layers.Dense(100, activation=\"selu\", input_shape=[16]),\n    keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n    keras.layers.Reshape([28, 28])\n])\nhashing_ae = keras.models.Sequential([hashing_encoder, hashing_decoder])\nhashing_ae.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.Nadam(),\n                   metrics=[rounded_accuracy])\nhistory = hashing_ae.fit(X_train, X_train, epochs=10,\n                         validation_data=(X_valid, X_valid))\n\nEpoch 1/10\n1719/1719 [==============================] - 7s 4ms/step - loss: 0.4066 - rounded_accuracy: 0.8160 - val_loss: 0.3898 - val_rounded_accuracy: 0.8263\nEpoch 2/10\n1719/1719 [==============================] - 6s 4ms/step - loss: 0.3688 - rounded_accuracy: 0.8495 - val_loss: 0.3764 - val_rounded_accuracy: 0.8361\nEpoch 3/10\n1719/1719 [==============================] - 6s 4ms/step - loss: 0.3585 - rounded_accuracy: 0.8589 - val_loss: 0.3664 - val_rounded_accuracy: 0.8499\nEpoch 4/10\n1719/1719 [==============================] - 6s 4ms/step - loss: 0.3530 - rounded_accuracy: 0.8641 - val_loss: 0.3624 - val_rounded_accuracy: 0.8523\nEpoch 5/10\n1719/1719 [==============================] - 6s 4ms/step - loss: 0.3497 - rounded_accuracy: 0.8677 - val_loss: 0.3535 - val_rounded_accuracy: 0.8652\nEpoch 6/10\n1719/1719 [==============================] - 6s 4ms/step - loss: 0.3472 - rounded_accuracy: 0.8698 - val_loss: 0.3519 - val_rounded_accuracy: 0.8645\nEpoch 7/10\n1719/1719 [==============================] - 6s 4ms/step - loss: 0.3447 - rounded_accuracy: 0.8721 - val_loss: 0.3493 - val_rounded_accuracy: 0.8698\nEpoch 8/10\n1719/1719 [==============================] - 6s 4ms/step - loss: 0.3432 - rounded_accuracy: 0.8744 - val_loss: 0.3460 - val_rounded_accuracy: 0.8729\nEpoch 9/10\n1719/1719 [==============================] - 6s 4ms/step - loss: 0.3415 - rounded_accuracy: 0.8763 - val_loss: 0.3429 - val_rounded_accuracy: 0.8752\nEpoch 10/10\n1719/1719 [==============================] - 6s 4ms/step - loss: 0.3404 - rounded_accuracy: 0.8776 - val_loss: 0.3433 - val_rounded_accuracy: 0.8778\n\n\n오토인코더는 정보를 매우 크게 (16비트로!) 압축하기 때문에 손실이 많지만 이미지를 재구성하려는 것이 아니라 시맨틱 해시를 생성하는 것이 목적이므로 괜찮습니다:\n\nshow_reconstructions(hashing_ae)\nplt.show()\n\n\n\n\n출력은 0과 1에 매우 가깝습니다(왼쪽 그래프):\n\nplot_activations_histogram(hashing_encoder)\nplt.show()\n\n\n\n\n검증 세트에 있는 처음 몇 개의 이미지에 대한 해시를 확인해 보죠:\n\nhashes = np.round(hashing_encoder.predict(X_valid)).astype(np.int32)\nhashes *= np.array([[2**bit for bit in range(16)]])\nhashes = hashes.sum(axis=1)\nfor h in hashes[:5]:\n    print(\"{:016b}\".format(h))\nprint(\"...\")\n\n0000100101011011\n0000100100111011\n0100100100011011\n0001100101001010\n0001000100111100\n...\n\n\n이제 검증 세트에 있는 이미지 중 가장 많이 등장하는 해시를 찾아 보죠. 해시마다 몇 개의 샘플을 출력합니다. 다음 이미지에서 한 행에 있는 이미지는 모두 같은 해시를 가집니다:\n\nfrom collections import Counter\n\nn_hashes = 10\nn_images = 8\n\ntop_hashes = Counter(hashes).most_common(n_hashes)\n\nplt.figure(figsize=(n_images, n_hashes))\nfor hash_index, (image_hash, hash_count) in enumerate(top_hashes):\n    indices = (hashes == image_hash)\n    for index, image in enumerate(X_valid[indices][:n_images]):\n        plt.subplot(n_hashes, n_images, hash_index * n_images + index + 1)\n        plt.imshow(image, cmap=\"binary\")\n        plt.axis(\"off\")"
  },
  {
    "objectID": "Machine_Learning/17_autoencoders_and_gans.html#to-8.",
    "href": "Machine_Learning/17_autoencoders_and_gans.html#to-8.",
    "title": "17_autoencoders_and_gans",
    "section": "1. to 8.",
    "text": "1. to 8.\n부록 A 참조"
  },
  {
    "objectID": "Machine_Learning/17_autoencoders_and_gans.html#section",
    "href": "Machine_Learning/17_autoencoders_and_gans.html#section",
    "title": "17_autoencoders_and_gans",
    "section": "9.",
    "text": "9.\n연습문제: 잡음 제거 오토인코더를 사용해 이미지 분류기를 사전훈련해보세요. (간단하게) MNIST를 사용하거나 도전적인 문제를 원한다면 CIFAR10 같은 좀 더 복잡한 이미지 데이터셋을 사용할 수 있습니다. 어떤 데이터셋을 사용하던지 다음 단계를 따르세요.\n\n데이터셋을 훈련 세트와 테스트 세트로 나눕니다. 전체 훈련 세트에서 심층 잡음 제거 오토인코더를 훈련합니다.\n이미지가 잘 재구성되는 지 확인하세요. 코딩 층의 각 뉴런을 가장 크게 활성화하는 이미지를 시각화해보세요.\n이 오토인코더의 아래 층을 재사용해 분류 DNN을 만드세요. 훈련 세트에서 이미지 500개만 사용해 훈련합니다. 사전훈련을 사용하는 것이 더 나은가요? 사용하지 않는 것이 더 나은가요?\n\n\n[X_train, y_train], [X_test, y_test] = keras.datasets.cifar10.load_data()\nX_train = X_train / 255\nX_test = X_test / 255\n\nDownloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n170500096/170498071 [==============================] - 3s 0us/step\n170508288/170498071 [==============================] - 3s 0us/step\n\n\n\ntf.random.set_seed(42)\nnp.random.seed(42)\n\ndenoising_encoder = keras.models.Sequential([\n    keras.layers.GaussianNoise(0.1, input_shape=[32, 32, 3]),\n    keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n    keras.layers.MaxPool2D(),\n    keras.layers.Flatten(),\n    keras.layers.Dense(512, activation=\"relu\"),\n])\n\n\ndenoising_encoder.summary()\n\nModel: \"sequential_40\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ngaussian_noise_3 (GaussianNo (None, 32, 32, 3)         0         \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 32, 32, 32)        896       \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 16, 16, 32)        0         \n_________________________________________________________________\nflatten_11 (Flatten)         (None, 8192)              0         \n_________________________________________________________________\ndense_46 (Dense)             (None, 512)               4194816   \n=================================================================\nTotal params: 4,195,712\nTrainable params: 4,195,712\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\ndenoising_decoder = keras.models.Sequential([\n    keras.layers.Dense(16 * 16 * 32, activation=\"relu\", input_shape=[512]),\n    keras.layers.Reshape([16, 16, 32]),\n    keras.layers.Conv2DTranspose(filters=3, kernel_size=3, strides=2,\n                                 padding=\"same\", activation=\"sigmoid\")\n])\n\n\ndenoising_decoder.summary()\n\nModel: \"sequential_41\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_47 (Dense)             (None, 8192)              4202496   \n_________________________________________________________________\nreshape_13 (Reshape)         (None, 16, 16, 32)        0         \n_________________________________________________________________\nconv2d_transpose_5 (Conv2DTr (None, 32, 32, 3)         867       \n=================================================================\nTotal params: 4,203,363\nTrainable params: 4,203,363\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\ndenoising_ae = keras.models.Sequential([denoising_encoder, denoising_decoder])\ndenoising_ae.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.Nadam(),\n                     metrics=[\"mse\"])\nhistory = denoising_ae.fit(X_train, X_train, epochs=10,\n                           validation_data=(X_test, X_test))\n\nEpoch 1/10\n1563/1563 [==============================] - 18s 11ms/step - loss: 0.5934 - mse: 0.0186 - val_loss: 0.6037 - val_mse: 0.0217\nEpoch 2/10\n1563/1563 [==============================] - 16s 10ms/step - loss: 0.5726 - mse: 0.0100 - val_loss: 0.5762 - val_mse: 0.0109\nEpoch 3/10\n1563/1563 [==============================] - 16s 10ms/step - loss: 0.5676 - mse: 0.0080 - val_loss: 0.5725 - val_mse: 0.0095\nEpoch 4/10\n1563/1563 [==============================] - 16s 10ms/step - loss: 0.5655 - mse: 0.0072 - val_loss: 0.5724 - val_mse: 0.0094\nEpoch 5/10\n1563/1563 [==============================] - 16s 10ms/step - loss: 0.5640 - mse: 0.0067 - val_loss: 0.5678 - val_mse: 0.0075\nEpoch 6/10\n1563/1563 [==============================] - 16s 10ms/step - loss: 0.5631 - mse: 0.0063 - val_loss: 0.5667 - val_mse: 0.0072\nEpoch 7/10\n1563/1563 [==============================] - 16s 10ms/step - loss: 0.5624 - mse: 0.0060 - val_loss: 0.5655 - val_mse: 0.0067\nEpoch 8/10\n1563/1563 [==============================] - 17s 11ms/step - loss: 0.5620 - mse: 0.0059 - val_loss: 0.5652 - val_mse: 0.0066\nEpoch 9/10\n1563/1563 [==============================] - 16s 10ms/step - loss: 0.5616 - mse: 0.0058 - val_loss: 0.5647 - val_mse: 0.0064\nEpoch 10/10\n1563/1563 [==============================] - 16s 10ms/step - loss: 0.5613 - mse: 0.0056 - val_loss: 0.5639 - val_mse: 0.0061\n\n\n\nn_images = 5\nnew_images = X_test[:n_images]\nnew_images_noisy = new_images + np.random.randn(n_images, 32, 32, 3) * 0.1\nnew_images_denoised = denoising_ae.predict(new_images_noisy)\n\nplt.figure(figsize=(6, n_images * 2))\nfor index in range(n_images):\n    plt.subplot(n_images, 3, index * 3 + 1)\n    plt.imshow(new_images[index])\n    plt.axis('off')\n    if index == 0:\n        plt.title(\"Original\")\n    plt.subplot(n_images, 3, index * 3 + 2)\n    plt.imshow(np.clip(new_images_noisy[index], 0., 1.))\n    plt.axis('off')\n    if index == 0:\n        plt.title(\"Noisy\")\n    plt.subplot(n_images, 3, index * 3 + 3)\n    plt.imshow(new_images_denoised[index])\n    plt.axis('off')\n    if index == 0:\n        plt.title(\"Denoised\")\nplt.show()"
  },
  {
    "objectID": "Machine_Learning/17_autoencoders_and_gans.html#section-1",
    "href": "Machine_Learning/17_autoencoders_and_gans.html#section-1",
    "title": "17_autoencoders_and_gans",
    "section": "10.",
    "text": "10.\n연습문제: 이미지 데이터셋을 하나 선택해 변이형 오토인코더를 훈련하고 이미지를 생성해보세요. 또는 관심있는 레이블이 없는 데이터셋을 찾아서 새로운 샘플을 생성할 수 있는지 확인해 보세요."
  },
  {
    "objectID": "Machine_Learning/17_autoencoders_and_gans.html#section-2",
    "href": "Machine_Learning/17_autoencoders_and_gans.html#section-2",
    "title": "17_autoencoders_and_gans",
    "section": "11.",
    "text": "11.\n연습문제: 이미지 데이터셋을 처리하는 DCGAN을 훈련하고 이를 사용해 이미지를 생성해보세요. 경험 재생을 추가하고 도움이 되는지 확인하세요. 생성된 클래스를 제어할 수 있는 조건 GAN으로 바꾸어 시도해보세요."
  },
  {
    "objectID": "Machine_Learning/15_processing_sequences_using_rnns_and_cnns.html",
    "href": "Machine_Learning/15_processing_sequences_using_rnns_and_cnns.html",
    "title": "15_processing_sequences_using_rnns_and_cnns",
    "section": "",
    "text": "15장 – RNN과 CNN을 사용해 시퀀스 처리하기\n이 노트북은 15장에 있는 모든 샘플 코드와 연습문제 해답을 가지고 있습니다."
  },
  {
    "objectID": "Machine_Learning/15_processing_sequences_using_rnns_and_cnns.html#데이터셋-생성",
    "href": "Machine_Learning/15_processing_sequences_using_rnns_and_cnns.html#데이터셋-생성",
    "title": "15_processing_sequences_using_rnns_and_cnns",
    "section": "데이터셋 생성",
    "text": "데이터셋 생성\n\ndef generate_time_series(batch_size, n_steps):\n    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n    time = np.linspace(0, 1, n_steps)\n    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))  #   웨이브 1\n    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + 웨이브 2\n    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)   # + 잡음\n    return series[..., np.newaxis].astype(np.float32)\n\n\nnp.random.seed(42)\n\nn_steps = 50\nseries = generate_time_series(10000, n_steps + 1)\nX_train, y_train = series[:7000, :n_steps], series[:7000, -1]\nX_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\nX_test, y_test = series[9000:, :n_steps], series[9000:, -1]\n\n\nX_train.shape, y_train.shape\n\n((7000, 50, 1), (7000, 1))\n\n\n\ndef plot_series(series, y=None, y_pred=None, x_label=\"$t$\", y_label=\"$x(t)$\", legend=True):\n    plt.plot(series, \".-\")\n    if y is not None:\n        plt.plot(n_steps, y, \"bo\", label=\"Target\")\n    if y_pred is not None:\n        plt.plot(n_steps, y_pred, \"rx\", markersize=10, label=\"Prediction\")\n    plt.grid(True)\n    if x_label:\n        plt.xlabel(x_label, fontsize=16)\n    if y_label:\n        plt.ylabel(y_label, fontsize=16, rotation=0)\n    plt.hlines(0, 0, 100, linewidth=1)\n    plt.axis([0, n_steps + 1, -1, 1])\n    if legend and (y or y_pred):\n        plt.legend(fontsize=14, loc=\"upper left\")\n\nfig, axes = plt.subplots(nrows=1, ncols=3, sharey=True, figsize=(12, 4))\nfor col in range(3):\n    plt.sca(axes[col])\n    plot_series(X_valid[col, :, 0], y_valid[col, 0],\n                y_label=(\"$x(t)$\" if col==0 else None),\n                legend=(col == 0))\nsave_fig(\"time_series_plot\")\nplt.show()\n\nSaving figure time_series_plot\n\n\n\n\n\n노트: 이 노트북에서 파란 점은 타깃을 나타내고 빨강 X 표시는 예측을 나타냅니다. 처음에 책에서 파란 X 표시를 타깃에 사용하고 빨강 점을 예측에 사용했다가 나중에 바꾸었습니다. 혼동을 드려 죄송합니다."
  },
  {
    "objectID": "Machine_Learning/15_processing_sequences_using_rnns_and_cnns.html#기준-성능-계산하기",
    "href": "Machine_Learning/15_processing_sequences_using_rnns_and_cnns.html#기준-성능-계산하기",
    "title": "15_processing_sequences_using_rnns_and_cnns",
    "section": "기준 성능 계산하기",
    "text": "기준 성능 계산하기\n단순한 예측 (마지막 관측값을 사용해 예측합니다):\n\ny_pred = X_valid[:, -1]\nnp.mean(keras.losses.mean_squared_error(y_valid, y_pred))\n\n0.020211367\n\n\n\nplot_series(X_valid[0, :, 0], y_valid[0, 0], y_pred[0, 0])\nplt.show()\n\n\n\n\n선형 예측:\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[50, 1]),\n    keras.layers.Dense(1)\n])\n\nmodel.compile(loss=\"mse\", optimizer=\"adam\")\nhistory = model.fit(X_train, y_train, epochs=20,\n                    validation_data=(X_valid, y_valid))\n\nEpoch 1/20\n219/219 [==============================] - 1s 3ms/step - loss: 0.1398 - val_loss: 0.0545\nEpoch 2/20\n219/219 [==============================] - 0s 690us/step - loss: 0.0443 - val_loss: 0.0266\nEpoch 3/20\n219/219 [==============================] - 0s 631us/step - loss: 0.0237 - val_loss: 0.0157\nEpoch 4/20\n219/219 [==============================] - 0s 738us/step - loss: 0.0142 - val_loss: 0.0116\nEpoch 5/20\n219/219 [==============================] - 0s 740us/step - loss: 0.0110 - val_loss: 0.0098\nEpoch 6/20\n219/219 [==============================] - 0s 615us/step - loss: 0.0093 - val_loss: 0.0087\nEpoch 7/20\n219/219 [==============================] - 0s 590us/step - loss: 0.0083 - val_loss: 0.0079\nEpoch 8/20\n219/219 [==============================] - 0s 581us/step - loss: 0.0074 - val_loss: 0.0071\nEpoch 9/20\n219/219 [==============================] - 0s 562us/step - loss: 0.0064 - val_loss: 0.0066\nEpoch 10/20\n219/219 [==============================] - 0s 570us/step - loss: 0.0063 - val_loss: 0.0062\nEpoch 11/20\n219/219 [==============================] - 0s 576us/step - loss: 0.0059 - val_loss: 0.0057\nEpoch 12/20\n219/219 [==============================] - 0s 645us/step - loss: 0.0054 - val_loss: 0.0055\nEpoch 13/20\n219/219 [==============================] - 0s 578us/step - loss: 0.0052 - val_loss: 0.0052\nEpoch 14/20\n219/219 [==============================] - 0s 596us/step - loss: 0.0050 - val_loss: 0.0049\nEpoch 15/20\n219/219 [==============================] - 0s 707us/step - loss: 0.0048 - val_loss: 0.0048\nEpoch 16/20\n219/219 [==============================] - 0s 635us/step - loss: 0.0046 - val_loss: 0.0048\nEpoch 17/20\n219/219 [==============================] - 0s 604us/step - loss: 0.0046 - val_loss: 0.0045\nEpoch 18/20\n219/219 [==============================] - 0s 647us/step - loss: 0.0043 - val_loss: 0.0044\nEpoch 19/20\n219/219 [==============================] - 0s 659us/step - loss: 0.0042 - val_loss: 0.0043\nEpoch 20/20\n219/219 [==============================] - 0s 769us/step - loss: 0.0043 - val_loss: 0.0042\n\n\n\nmodel.evaluate(X_valid, y_valid)\n\n63/63 [==============================] - 0s 414us/step - loss: 0.0042\n\n\n0.004168087150901556\n\n\n\ndef plot_learning_curves(loss, val_loss):\n    plt.plot(np.arange(len(loss)) + 0.5, loss, \"b.-\", label=\"Training loss\")\n    plt.plot(np.arange(len(val_loss)) + 1, val_loss, \"r.-\", label=\"Validation loss\")\n    plt.gca().xaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True))\n    plt.axis([1, 20, 0, 0.05])\n    plt.legend(fontsize=14)\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.grid(True)\n\nplot_learning_curves(history.history[\"loss\"], history.history[\"val_loss\"])\nplt.show()\n\n\n\n\n\ny_pred = model.predict(X_valid)\nplot_series(X_valid[0, :, 0], y_valid[0, 0], y_pred[0, 0])\nplt.show()"
  },
  {
    "objectID": "Machine_Learning/15_processing_sequences_using_rnns_and_cnns.html#간단한-rnn-사용하기",
    "href": "Machine_Learning/15_processing_sequences_using_rnns_and_cnns.html#간단한-rnn-사용하기",
    "title": "15_processing_sequences_using_rnns_and_cnns",
    "section": "간단한 RNN 사용하기",
    "text": "간단한 RNN 사용하기\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nmodel = keras.models.Sequential([\n    keras.layers.SimpleRNN(1, input_shape=[None, 1])\n])\n\noptimizer = keras.optimizers.Adam(learning_rate=0.005)\nmodel.compile(loss=\"mse\", optimizer=optimizer)\nhistory = model.fit(X_train, y_train, epochs=20,\n                    validation_data=(X_valid, y_valid))\n\nEpoch 1/20\n219/219 [==============================] - 2s 5ms/step - loss: 0.1554 - val_loss: 0.0489\nEpoch 2/20\n219/219 [==============================] - 1s 4ms/step - loss: 0.0409 - val_loss: 0.0296\nEpoch 3/20\n219/219 [==============================] - 1s 4ms/step - loss: 0.0277 - val_loss: 0.0218\nEpoch 4/20\n219/219 [==============================] - 1s 4ms/step - loss: 0.0208 - val_loss: 0.0177\nEpoch 5/20\n219/219 [==============================] - 1s 4ms/step - loss: 0.0174 - val_loss: 0.0151\nEpoch 6/20\n219/219 [==============================] - 1s 4ms/step - loss: 0.0146 - val_loss: 0.0134\nEpoch 7/20\n219/219 [==============================] - 1s 4ms/step - loss: 0.0138 - val_loss: 0.0123\nEpoch 8/20\n219/219 [==============================] - 1s 4ms/step - loss: 0.0128 - val_loss: 0.0116\nEpoch 9/20\n219/219 [==============================] - 1s 4ms/step - loss: 0.0118 - val_loss: 0.0112\nEpoch 10/20\n219/219 [==============================] - 1s 4ms/step - loss: 0.0117 - val_loss: 0.0110\nEpoch 11/20\n219/219 [==============================] - 1s 4ms/step - loss: 0.0112 - val_loss: 0.0109\nEpoch 12/20\n219/219 [==============================] - 1s 4ms/step - loss: 0.0115 - val_loss: 0.0109\nEpoch 13/20\n219/219 [==============================] - 1s 4ms/step - loss: 0.0114 - val_loss: 0.0109\nEpoch 14/20\n219/219 [==============================] - 1s 4ms/step - loss: 0.0114 - val_loss: 0.0109\nEpoch 15/20\n219/219 [==============================] - 1s 4ms/step - loss: 0.0113 - val_loss: 0.0109\nEpoch 16/20\n219/219 [==============================] - 1s 4ms/step - loss: 0.0114 - val_loss: 0.0109\nEpoch 17/20\n219/219 [==============================] - 1s 4ms/step - loss: 0.0114 - val_loss: 0.0109\nEpoch 18/20\n219/219 [==============================] - 1s 4ms/step - loss: 0.0115 - val_loss: 0.0109\nEpoch 19/20\n219/219 [==============================] - 1s 5ms/step - loss: 0.0115 - val_loss: 0.0109\nEpoch 20/20\n219/219 [==============================] - 1s 4ms/step - loss: 0.0116 - val_loss: 0.0109\n\n\n\nmodel.evaluate(X_valid, y_valid)\n\n63/63 [==============================] - 0s 2ms/step - loss: 0.0109\n\n\n0.010881561785936356\n\n\n\nplot_learning_curves(history.history[\"loss\"], history.history[\"val_loss\"])\nplt.show()\n\n\n\n\n\ny_pred = model.predict(X_valid)\nplot_series(X_valid[0, :, 0], y_valid[0, 0], y_pred[0, 0])\nplt.show()"
  },
  {
    "objectID": "Machine_Learning/15_processing_sequences_using_rnns_and_cnns.html#심층-rnn",
    "href": "Machine_Learning/15_processing_sequences_using_rnns_and_cnns.html#심층-rnn",
    "title": "15_processing_sequences_using_rnns_and_cnns",
    "section": "심층 RNN",
    "text": "심층 RNN\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nmodel = keras.models.Sequential([\n    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n    keras.layers.SimpleRNN(20, return_sequences=True),\n    keras.layers.SimpleRNN(1)\n])\n\nmodel.compile(loss=\"mse\", optimizer=\"adam\")\nhistory = model.fit(X_train, y_train, epochs=20,\n                    validation_data=(X_valid, y_valid))\n\nEpoch 1/20\n219/219 [==============================] - 5s 17ms/step - loss: 0.1324 - val_loss: 0.0090\nEpoch 2/20\n219/219 [==============================] - 3s 15ms/step - loss: 0.0078 - val_loss: 0.0065\nEpoch 3/20\n219/219 [==============================] - 3s 15ms/step - loss: 0.0057 - val_loss: 0.0045\nEpoch 4/20\n219/219 [==============================] - 3s 15ms/step - loss: 0.0045 - val_loss: 0.0040\nEpoch 5/20\n219/219 [==============================] - 3s 15ms/step - loss: 0.0044 - val_loss: 0.0040\nEpoch 6/20\n219/219 [==============================] - 3s 15ms/step - loss: 0.0038 - val_loss: 0.0036\nEpoch 7/20\n219/219 [==============================] - 3s 15ms/step - loss: 0.0036 - val_loss: 0.0040\nEpoch 8/20\n219/219 [==============================] - 3s 15ms/step - loss: 0.0038 - val_loss: 0.0033\nEpoch 9/20\n219/219 [==============================] - 3s 15ms/step - loss: 0.0037 - val_loss: 0.0032\nEpoch 10/20\n219/219 [==============================] - 3s 15ms/step - loss: 0.0035 - val_loss: 0.0031\nEpoch 11/20\n219/219 [==============================] - 3s 15ms/step - loss: 0.0034 - val_loss: 0.0030\nEpoch 12/20\n219/219 [==============================] - 3s 15ms/step - loss: 0.0033 - val_loss: 0.0031\nEpoch 13/20\n219/219 [==============================] - 3s 15ms/step - loss: 0.0034 - val_loss: 0.0031\nEpoch 14/20\n219/219 [==============================] - 3s 15ms/step - loss: 0.0034 - val_loss: 0.0032\nEpoch 15/20\n219/219 [==============================] - 3s 15ms/step - loss: 0.0034 - val_loss: 0.0033\nEpoch 16/20\n219/219 [==============================] - 3s 15ms/step - loss: 0.0037 - val_loss: 0.0030\nEpoch 17/20\n219/219 [==============================] - 3s 14ms/step - loss: 0.0034 - val_loss: 0.0029\nEpoch 18/20\n219/219 [==============================] - 3s 14ms/step - loss: 0.0031 - val_loss: 0.0030\nEpoch 19/20\n219/219 [==============================] - 3s 14ms/step - loss: 0.0032 - val_loss: 0.0029\nEpoch 20/20\n219/219 [==============================] - 3s 14ms/step - loss: 0.0033 - val_loss: 0.0029\n\n\n\nmodel.evaluate(X_valid, y_valid)\n\n63/63 [==============================] - 0s 3ms/step - loss: 0.0029\n\n\n0.002910564187914133\n\n\n\nplot_learning_curves(history.history[\"loss\"], history.history[\"val_loss\"])\nplt.show()\n\n\n\n\n\ny_pred = model.predict(X_valid)\nplot_series(X_valid[0, :, 0], y_valid[0, 0], y_pred[0, 0])\nplt.show()\n\n\n\n\n두 번째 SimpleRNN 층은 마지막 출력만 반환합니다:\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nmodel = keras.models.Sequential([\n    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n    keras.layers.SimpleRNN(20),\n    keras.layers.Dense(1)\n])\n\nmodel.compile(loss=\"mse\", optimizer=\"adam\")\nhistory = model.fit(X_train, y_train, epochs=20,\n                    validation_data=(X_valid, y_valid))\n\nEpoch 1/20\n219/219 [==============================] - 3s 12ms/step - loss: 0.0566 - val_loss: 0.0052\nEpoch 2/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0048 - val_loss: 0.0036\nEpoch 3/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0036 - val_loss: 0.0031\nEpoch 4/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0033 - val_loss: 0.0033\nEpoch 5/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0033 - val_loss: 0.0034\nEpoch 6/20\n219/219 [==============================] - 3s 11ms/step - loss: 0.0031 - val_loss: 0.0029\nEpoch 7/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0030 - val_loss: 0.0034\nEpoch 8/20\n219/219 [==============================] - 3s 12ms/step - loss: 0.0033 - val_loss: 0.0028\nEpoch 9/20\n219/219 [==============================] - 3s 12ms/step - loss: 0.0031 - val_loss: 0.0028\nEpoch 10/20\n219/219 [==============================] - 3s 12ms/step - loss: 0.0029 - val_loss: 0.0029\nEpoch 11/20\n219/219 [==============================] - 3s 12ms/step - loss: 0.0029 - val_loss: 0.0027\nEpoch 12/20\n219/219 [==============================] - 3s 12ms/step - loss: 0.0029 - val_loss: 0.0031\nEpoch 13/20\n219/219 [==============================] - 3s 12ms/step - loss: 0.0029 - val_loss: 0.0031\nEpoch 14/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0031 - val_loss: 0.0030\nEpoch 15/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0030 - val_loss: 0.0030\nEpoch 16/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0030 - val_loss: 0.0027\nEpoch 17/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0030 - val_loss: 0.0028\nEpoch 18/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0029 - val_loss: 0.0027\nEpoch 19/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0029 - val_loss: 0.0028\nEpoch 20/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0030 - val_loss: 0.0026\n\n\n\nmodel.evaluate(X_valid, y_valid)\n\n63/63 [==============================] - 0s 3ms/step - loss: 0.0026\n\n\n0.002623623702675104\n\n\n\nplot_learning_curves(history.history[\"loss\"], history.history[\"val_loss\"])\nplt.show()\n\n\n\n\n\ny_pred = model.predict(X_valid)\nplot_series(X_valid[0, :, 0], y_valid[0, 0], y_pred[0, 0])\nplt.show()"
  },
  {
    "objectID": "Machine_Learning/15_processing_sequences_using_rnns_and_cnns.html#여러-타임-스텝-앞을-예측하기",
    "href": "Machine_Learning/15_processing_sequences_using_rnns_and_cnns.html#여러-타임-스텝-앞을-예측하기",
    "title": "15_processing_sequences_using_rnns_and_cnns",
    "section": "여러 타임 스텝 앞을 예측하기",
    "text": "여러 타임 스텝 앞을 예측하기\n\nnp.random.seed(43) # 42는 훈련 세트에 있는 첫 번째 시리즈를 반환하기 때문에 다른 값으로 지정합니다\n\nseries = generate_time_series(1, n_steps + 10)\nX_new, Y_new = series[:, :n_steps], series[:, n_steps:]\nX = X_new\nfor step_ahead in range(10):\n    y_pred_one = model.predict(X[:, step_ahead:])[:, np.newaxis, :]\n    X = np.concatenate([X, y_pred_one], axis=1)\n\nY_pred = X[:, n_steps:]\n\n\nY_pred.shape\n\n(1, 10, 1)\n\n\n\ndef plot_multiple_forecasts(X, Y, Y_pred):\n    n_steps = X.shape[1]\n    ahead = Y.shape[1]\n    plot_series(X[0, :, 0])\n    plt.plot(np.arange(n_steps, n_steps + ahead), Y[0, :, 0], \"bo-\", label=\"Actual\")\n    plt.plot(np.arange(n_steps, n_steps + ahead), Y_pred[0, :, 0], \"rx-\", label=\"Forecast\", markersize=10)\n    plt.axis([0, n_steps + ahead, -1, 1])\n    plt.legend(fontsize=14)\n\nplot_multiple_forecasts(X_new, Y_new, Y_pred)\nsave_fig(\"forecast_ahead_plot\")\nplt.show()\n\nSaving figure forecast_ahead_plot\n\n\n\n\n\n이 모델을 사용해 다음 10개의 값을 예측해 보겠습니다. 먼저 아홉 개의 타임 스텝을 더 가진 시퀀스를 다시 생성해야 합니다.\n\nnp.random.seed(42)\n\nn_steps = 50\nseries = generate_time_series(10000, n_steps + 10)\nX_train, Y_train = series[:7000, :n_steps], series[:7000, -10:, 0]\nX_valid, Y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0]\nX_test, Y_test = series[9000:, :n_steps], series[9000:, -10:, 0]\n\n이제 하나씩 다음 10개의 값을 예측합니다:\n\nX = X_valid\nfor step_ahead in range(10):\n    y_pred_one = model.predict(X)[:, np.newaxis, :]\n    X = np.concatenate([X, y_pred_one], axis=1)\n\nY_pred = X[:, n_steps:, 0]\n\n\nY_pred.shape\n\n(2000, 10)\n\n\n\nnp.mean(keras.metrics.mean_squared_error(Y_valid, Y_pred))\n\n0.027510857\n\n\n이 성능을 단순한 예측이나 간단한 선형 모델과 비교해 보죠:\n\nY_naive_pred = np.tile(X_valid[:, -1], 10) # 마지막 타임 스텝 값을 선택해 10번 반복합니다\nnp.mean(keras.metrics.mean_squared_error(Y_valid, Y_naive_pred))\n\n0.25697407\n\n\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[50, 1]),\n    keras.layers.Dense(10)\n])\n\nmodel.compile(loss=\"mse\", optimizer=\"adam\")\nhistory = model.fit(X_train, Y_train, epochs=20,\n                    validation_data=(X_valid, Y_valid))\n\nEpoch 1/20\n219/219 [==============================] - 0s 1ms/step - loss: 0.2186 - val_loss: 0.0606\nEpoch 2/20\n219/219 [==============================] - 0s 743us/step - loss: 0.0535 - val_loss: 0.0425\nEpoch 3/20\n219/219 [==============================] - 0s 727us/step - loss: 0.0406 - val_loss: 0.0353\nEpoch 4/20\n219/219 [==============================] - 0s 731us/step - loss: 0.0343 - val_loss: 0.0311\nEpoch 5/20\n219/219 [==============================] - 0s 743us/step - loss: 0.0300 - val_loss: 0.0283\nEpoch 6/20\n219/219 [==============================] - 0s 721us/step - loss: 0.0278 - val_loss: 0.0264\nEpoch 7/20\n219/219 [==============================] - 0s 722us/step - loss: 0.0262 - val_loss: 0.0249\nEpoch 8/20\n219/219 [==============================] - 0s 731us/step - loss: 0.0246 - val_loss: 0.0237\nEpoch 9/20\n219/219 [==============================] - 0s 725us/step - loss: 0.0236 - val_loss: 0.0229\nEpoch 10/20\n219/219 [==============================] - 0s 735us/step - loss: 0.0228 - val_loss: 0.0222\nEpoch 11/20\n219/219 [==============================] - 0s 743us/step - loss: 0.0220 - val_loss: 0.0216\nEpoch 12/20\n219/219 [==============================] - 0s 733us/step - loss: 0.0214 - val_loss: 0.0212\nEpoch 13/20\n219/219 [==============================] - 0s 714us/step - loss: 0.0212 - val_loss: 0.0208\nEpoch 14/20\n219/219 [==============================] - 0s 739us/step - loss: 0.0207 - val_loss: 0.0207\nEpoch 15/20\n219/219 [==============================] - 0s 712us/step - loss: 0.0207 - val_loss: 0.0202\nEpoch 16/20\n219/219 [==============================] - 0s 723us/step - loss: 0.0199 - val_loss: 0.0199\nEpoch 17/20\n219/219 [==============================] - 0s 738us/step - loss: 0.0197 - val_loss: 0.0195\nEpoch 18/20\n219/219 [==============================] - 0s 715us/step - loss: 0.0190 - val_loss: 0.0192\nEpoch 19/20\n219/219 [==============================] - 0s 719us/step - loss: 0.0189 - val_loss: 0.0189\nEpoch 20/20\n219/219 [==============================] - 0s 726us/step - loss: 0.0188 - val_loss: 0.0187\n\n\n이제 동시에 다음 10개의 값을 모두 예측하는 RNN을 만들어 보겠습니다:\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nmodel = keras.models.Sequential([\n    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n    keras.layers.SimpleRNN(20),\n    keras.layers.Dense(10)\n])\n\nmodel.compile(loss=\"mse\", optimizer=\"adam\")\nhistory = model.fit(X_train, Y_train, epochs=20,\n                    validation_data=(X_valid, Y_valid))\n\nEpoch 1/20\n219/219 [==============================] - 3s 12ms/step - loss: 0.1216 - val_loss: 0.0317\nEpoch 2/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0294 - val_loss: 0.0200\nEpoch 3/20\n219/219 [==============================] - 3s 11ms/step - loss: 0.0198 - val_loss: 0.0160\nEpoch 4/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0162 - val_loss: 0.0144\nEpoch 5/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0144 - val_loss: 0.0118\nEpoch 6/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0127 - val_loss: 0.0112\nEpoch 7/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0119 - val_loss: 0.0110\nEpoch 8/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0114 - val_loss: 0.0103\nEpoch 9/20\n219/219 [==============================] - 3s 12ms/step - loss: 0.0110 - val_loss: 0.0112\nEpoch 10/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0118 - val_loss: 0.0100\nEpoch 11/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0109 - val_loss: 0.0103\nEpoch 12/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0104 - val_loss: 0.0096\nEpoch 13/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0103 - val_loss: 0.0100\nEpoch 14/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0101 - val_loss: 0.0103\nEpoch 15/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0095 - val_loss: 0.0107\nEpoch 16/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0095 - val_loss: 0.0089\nEpoch 17/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0092 - val_loss: 0.0111\nEpoch 18/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0098 - val_loss: 0.0094\nEpoch 19/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0090 - val_loss: 0.0083\nEpoch 20/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0092 - val_loss: 0.0085\n\n\n\nnp.random.seed(43)\n\nseries = generate_time_series(1, 50 + 10)\nX_new, Y_new = series[:, :50, :], series[:, -10:, :]\nY_pred = model.predict(X_new)[..., np.newaxis]\n\n\nplot_multiple_forecasts(X_new, Y_new, Y_pred)\nplt.show()\n\n\n\n\n이제 타임 스텝마다 다음 10 스텝을 예측하는 RNN을 만들어 보겠습니다. 즉 타임 스텝 0에서 49까지를 기반으로 타임 스텝 50에서 59를 예측하는 것이 아니라, 타임 스텝 0에서 타임 스텝 1에서 10까지 예측하고 그다음 타임 스텝 1에서 타임 스텝 2에서 11까지 예측합니다. 마지막 타임 스텝에서는 타임 스텝 50에서 59까지 예측합니다. 이 모델은 인과 모델입니다. 어떤 타임 스텝에서 예측을 만들 때 과거 타임 스텝만 볼 수 있습니다.\n\nnp.random.seed(42)\n\nn_steps = 50\nseries = generate_time_series(10000, n_steps + 10)\nX_train = series[:7000, :n_steps]\nX_valid = series[7000:9000, :n_steps]\nX_test = series[9000:, :n_steps]\nY = np.empty((10000, n_steps, 10))\nfor step_ahead in range(1, 10 + 1):\n    Y[..., step_ahead - 1] = series[..., step_ahead:step_ahead + n_steps, 0]\nY_train = Y[:7000]\nY_valid = Y[7000:9000]\nY_test = Y[9000:]\n\n\nX_train.shape, Y_train.shape\n\n((7000, 50, 1), (7000, 50, 10))\n\n\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nmodel = keras.models.Sequential([\n    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n    keras.layers.SimpleRNN(20, return_sequences=True),\n    keras.layers.TimeDistributed(keras.layers.Dense(10))\n])\n\ndef last_time_step_mse(Y_true, Y_pred):\n    return keras.metrics.mean_squared_error(Y_true[:, -1], Y_pred[:, -1])\n\nmodel.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(learning_rate=0.01), metrics=[last_time_step_mse])\nhistory = model.fit(X_train, Y_train, epochs=20,\n                    validation_data=(X_valid, Y_valid))\n\nEpoch 1/20\n219/219 [==============================] - 4s 12ms/step - loss: 0.0705 - last_time_step_mse: 0.0621 - val_loss: 0.0429 - val_last_time_step_mse: 0.0324\nEpoch 2/20\n219/219 [==============================] - 3s 12ms/step - loss: 0.0413 - last_time_step_mse: 0.0301 - val_loss: 0.0366 - val_last_time_step_mse: 0.0264\nEpoch 3/20\n219/219 [==============================] - 3s 11ms/step - loss: 0.0333 - last_time_step_mse: 0.0223 - val_loss: 0.0343 - val_last_time_step_mse: 0.0244\nEpoch 4/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0306 - last_time_step_mse: 0.0200 - val_loss: 0.0284 - val_last_time_step_mse: 0.0164\nEpoch 5/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0281 - last_time_step_mse: 0.0167 - val_loss: 0.0282 - val_last_time_step_mse: 0.0196\nEpoch 6/20\n219/219 [==============================] - 3s 11ms/step - loss: 0.0259 - last_time_step_mse: 0.0137 - val_loss: 0.0215 - val_last_time_step_mse: 0.0081\nEpoch 7/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0234 - last_time_step_mse: 0.0105 - val_loss: 0.0220 - val_last_time_step_mse: 0.0089\nEpoch 8/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0216 - last_time_step_mse: 0.0085 - val_loss: 0.0217 - val_last_time_step_mse: 0.0091\nEpoch 9/20\n219/219 [==============================] - 3s 12ms/step - loss: 0.0214 - last_time_step_mse: 0.0089 - val_loss: 0.0202 - val_last_time_step_mse: 0.0074\nEpoch 10/20\n219/219 [==============================] - 3s 12ms/step - loss: 0.0210 - last_time_step_mse: 0.0085 - val_loss: 0.0211 - val_last_time_step_mse: 0.0086\nEpoch 11/20\n219/219 [==============================] - 3s 11ms/step - loss: 0.0203 - last_time_step_mse: 0.0078 - val_loss: 0.0201 - val_last_time_step_mse: 0.0078\nEpoch 12/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0203 - last_time_step_mse: 0.0079 - val_loss: 0.0194 - val_last_time_step_mse: 0.0073\nEpoch 13/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0198 - last_time_step_mse: 0.0074 - val_loss: 0.0209 - val_last_time_step_mse: 0.0085\nEpoch 14/20\n219/219 [==============================] - 3s 12ms/step - loss: 0.0197 - last_time_step_mse: 0.0073 - val_loss: 0.0189 - val_last_time_step_mse: 0.0067\nEpoch 15/20\n219/219 [==============================] - 3s 12ms/step - loss: 0.0192 - last_time_step_mse: 0.0072 - val_loss: 0.0182 - val_last_time_step_mse: 0.0066\nEpoch 16/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0187 - last_time_step_mse: 0.0066 - val_loss: 0.0196 - val_last_time_step_mse: 0.0095\nEpoch 17/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0187 - last_time_step_mse: 0.0067 - val_loss: 0.0185 - val_last_time_step_mse: 0.0072\nEpoch 18/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0186 - last_time_step_mse: 0.0067 - val_loss: 0.0179 - val_last_time_step_mse: 0.0064\nEpoch 19/20\n219/219 [==============================] - 3s 11ms/step - loss: 0.0185 - last_time_step_mse: 0.0068 - val_loss: 0.0172 - val_last_time_step_mse: 0.0058\nEpoch 20/20\n219/219 [==============================] - 2s 11ms/step - loss: 0.0181 - last_time_step_mse: 0.0066 - val_loss: 0.0205 - val_last_time_step_mse: 0.0096\n\n\n\nnp.random.seed(43)\n\nseries = generate_time_series(1, 50 + 10)\nX_new, Y_new = series[:, :50, :], series[:, 50:, :]\nY_pred = model.predict(X_new)[:, -1][..., np.newaxis]\n\n\nplot_multiple_forecasts(X_new, Y_new, Y_pred)\nplt.show()"
  },
  {
    "objectID": "Machine_Learning/15_processing_sequences_using_rnns_and_cnns.html#d-합성곱-층을-사용해-시퀀스-처리하기",
    "href": "Machine_Learning/15_processing_sequences_using_rnns_and_cnns.html#d-합성곱-층을-사용해-시퀀스-처리하기",
    "title": "15_processing_sequences_using_rnns_and_cnns",
    "section": "1D 합성곱 층을 사용해 시퀀스 처리하기",
    "text": "1D 합성곱 층을 사용해 시퀀스 처리하기\n1D conv layer with kernel size 4, stride 2, VALID padding:\n\n              |-----2-----|     |-----5---...------|     |-----23----|\n        |-----1-----|     |-----4-----|   ...      |-----22----|\n  |-----0----|      |-----3-----|     |---...|-----21----|\nX: 0  1  2  3  4  5  6  7  8  9  10 11 12 ... 42 43 44 45 46 47 48 49\nY: 1  2  3  4  5  6  7  8  9  10 11 12 13 ... 43 44 45 46 47 48 49 50\n  /10 11 12 13 14 15 16 17 18 19 20 21 22 ... 52 53 54 55 56 57 58 59\n\nOutput:\n\nX:     0/3   2/5   4/7   6/9   8/11 10/13 .../43 42/45 44/47 46/49\nY:     4/13  6/15  8/17 10/19 12/21 14/23 .../53 46/55 48/57 50/59\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nmodel = keras.models.Sequential([\n    keras.layers.Conv1D(filters=20, kernel_size=4, strides=2, padding=\"valid\",\n                        input_shape=[None, 1]),\n    keras.layers.GRU(20, return_sequences=True),\n    keras.layers.GRU(20, return_sequences=True),\n    keras.layers.TimeDistributed(keras.layers.Dense(10))\n])\n\nmodel.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\nhistory = model.fit(X_train, Y_train[:, 3::2], epochs=20,\n                    validation_data=(X_valid, Y_valid[:, 3::2]))\n\nEpoch 1/20\n219/219 [==============================] - 6s 16ms/step - loss: 0.0908 - last_time_step_mse: 0.0845 - val_loss: 0.0477 - val_last_time_step_mse: 0.0396\nEpoch 2/20\n219/219 [==============================] - 3s 14ms/step - loss: 0.0437 - last_time_step_mse: 0.0357 - val_loss: 0.0367 - val_last_time_step_mse: 0.0285\nEpoch 3/20\n219/219 [==============================] - 3s 14ms/step - loss: 0.0356 - last_time_step_mse: 0.0282 - val_loss: 0.0307 - val_last_time_step_mse: 0.0218\nEpoch 4/20\n219/219 [==============================] - 3s 13ms/step - loss: 0.0293 - last_time_step_mse: 0.0201 - val_loss: 0.0259 - val_last_time_step_mse: 0.0152\nEpoch 5/20\n219/219 [==============================] - 3s 13ms/step - loss: 0.0256 - last_time_step_mse: 0.0152 - val_loss: 0.0246 - val_last_time_step_mse: 0.0141\nEpoch 6/20\n219/219 [==============================] - 3s 13ms/step - loss: 0.0239 - last_time_step_mse: 0.0129 - val_loss: 0.0227 - val_last_time_step_mse: 0.0115\nEpoch 7/20\n219/219 [==============================] - 3s 13ms/step - loss: 0.0228 - last_time_step_mse: 0.0116 - val_loss: 0.0225 - val_last_time_step_mse: 0.0116\nEpoch 8/20\n219/219 [==============================] - 3s 13ms/step - loss: 0.0222 - last_time_step_mse: 0.0111 - val_loss: 0.0216 - val_last_time_step_mse: 0.0105\nEpoch 9/20\n219/219 [==============================] - 3s 13ms/step - loss: 0.0215 - last_time_step_mse: 0.0109 - val_loss: 0.0217 - val_last_time_step_mse: 0.0109\nEpoch 10/20\n219/219 [==============================] - 3s 13ms/step - loss: 0.0216 - last_time_step_mse: 0.0107 - val_loss: 0.0210 - val_last_time_step_mse: 0.0102\nEpoch 11/20\n219/219 [==============================] - 3s 13ms/step - loss: 0.0210 - last_time_step_mse: 0.0103 - val_loss: 0.0208 - val_last_time_step_mse: 0.0100\nEpoch 12/20\n219/219 [==============================] - 3s 13ms/step - loss: 0.0209 - last_time_step_mse: 0.0102 - val_loss: 0.0208 - val_last_time_step_mse: 0.0102\nEpoch 13/20\n219/219 [==============================] - 3s 13ms/step - loss: 0.0206 - last_time_step_mse: 0.0098 - val_loss: 0.0206 - val_last_time_step_mse: 0.0101\nEpoch 14/20\n219/219 [==============================] - 3s 13ms/step - loss: 0.0205 - last_time_step_mse: 0.0100 - val_loss: 0.0204 - val_last_time_step_mse: 0.0099\nEpoch 15/20\n219/219 [==============================] - 3s 13ms/step - loss: 0.0202 - last_time_step_mse: 0.0099 - val_loss: 0.0199 - val_last_time_step_mse: 0.0093\nEpoch 16/20\n219/219 [==============================] - 3s 13ms/step - loss: 0.0202 - last_time_step_mse: 0.0097 - val_loss: 0.0201 - val_last_time_step_mse: 0.0095\nEpoch 17/20\n219/219 [==============================] - 3s 13ms/step - loss: 0.0197 - last_time_step_mse: 0.0094 - val_loss: 0.0197 - val_last_time_step_mse: 0.0091\nEpoch 18/20\n219/219 [==============================] - 3s 13ms/step - loss: 0.0195 - last_time_step_mse: 0.0090 - val_loss: 0.0192 - val_last_time_step_mse: 0.0086\nEpoch 19/20\n219/219 [==============================] - 3s 13ms/step - loss: 0.0190 - last_time_step_mse: 0.0088 - val_loss: 0.0188 - val_last_time_step_mse: 0.0084\nEpoch 20/20\n219/219 [==============================] - 3s 13ms/step - loss: 0.0186 - last_time_step_mse: 0.0084 - val_loss: 0.0184 - val_last_time_step_mse: 0.0080"
  },
  {
    "objectID": "Machine_Learning/15_processing_sequences_using_rnns_and_cnns.html#wavenet",
    "href": "Machine_Learning/15_processing_sequences_using_rnns_and_cnns.html#wavenet",
    "title": "15_processing_sequences_using_rnns_and_cnns",
    "section": "WaveNet",
    "text": "WaveNet\nC2  /\\ /\\ /\\ /\\ /\\ /\\ /\\ /\\ /\\ /\\ /\\ /\\ /\\.../\\ /\\ /\\ /\\ /\\ /\\\n   \\  /  \\  /  \\  /  \\  /  \\  /  \\  /  \\       /  \\  /  \\  /  \\\n     /    \\      /    \\      /    \\                 /    \\\nC1  /\\ /\\ /\\ /\\ /\\ /\\ /\\ /\\ /\\ /\\ /\\  /\\ /.../\\ /\\ /\\ /\\ /\\ /\\ /\\\nX: 0  1  2  3  4  5  6  7  8  9  10 11 12 ... 43 44 45 46 47 48 49\nY: 1  2  3  4  5  6  7  8  9  10 11 12 13 ... 44 45 46 47 48 49 50\n  /10 11 12 13 14 15 16 17 18 19 20 21 22 ... 53 54 55 56 57 58 59\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.InputLayer(input_shape=[None, 1]))\nfor rate in (1, 2, 4, 8) * 2:\n    model.add(keras.layers.Conv1D(filters=20, kernel_size=2, padding=\"causal\",\n                                  activation=\"relu\", dilation_rate=rate))\nmodel.add(keras.layers.Conv1D(filters=10, kernel_size=1))\nmodel.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\nhistory = model.fit(X_train, Y_train, epochs=20,\n                    validation_data=(X_valid, Y_valid))\n\nEpoch 1/20\n219/219 [==============================] - 2s 7ms/step - loss: 0.0981 - last_time_step_mse: 0.0891 - val_loss: 0.0365 - val_last_time_step_mse: 0.0231\nEpoch 2/20\n219/219 [==============================] - 1s 7ms/step - loss: 0.0340 - last_time_step_mse: 0.0212 - val_loss: 0.0294 - val_last_time_step_mse: 0.0166\nEpoch 3/20\n219/219 [==============================] - 1s 7ms/step - loss: 0.0291 - last_time_step_mse: 0.0163 - val_loss: 0.0269 - val_last_time_step_mse: 0.0145\nEpoch 4/20\n219/219 [==============================] - 1s 6ms/step - loss: 0.0265 - last_time_step_mse: 0.0141 - val_loss: 0.0254 - val_last_time_step_mse: 0.0130\nEpoch 5/20\n219/219 [==============================] - 1s 6ms/step - loss: 0.0251 - last_time_step_mse: 0.0129 - val_loss: 0.0244 - val_last_time_step_mse: 0.0122\nEpoch 6/20\n219/219 [==============================] - 2s 7ms/step - loss: 0.0242 - last_time_step_mse: 0.0121 - val_loss: 0.0233 - val_last_time_step_mse: 0.0108\nEpoch 7/20\n219/219 [==============================] - 1s 6ms/step - loss: 0.0234 - last_time_step_mse: 0.0112 - val_loss: 0.0230 - val_last_time_step_mse: 0.0109\nEpoch 8/20\n219/219 [==============================] - 1s 7ms/step - loss: 0.0228 - last_time_step_mse: 0.0105 - val_loss: 0.0228 - val_last_time_step_mse: 0.0105\nEpoch 9/20\n219/219 [==============================] - 1s 6ms/step - loss: 0.0222 - last_time_step_mse: 0.0105 - val_loss: 0.0225 - val_last_time_step_mse: 0.0107\nEpoch 10/20\n219/219 [==============================] - 2s 7ms/step - loss: 0.0221 - last_time_step_mse: 0.0102 - val_loss: 0.0214 - val_last_time_step_mse: 0.0092\nEpoch 11/20\n219/219 [==============================] - 1s 7ms/step - loss: 0.0214 - last_time_step_mse: 0.0095 - val_loss: 0.0211 - val_last_time_step_mse: 0.0091\nEpoch 12/20\n219/219 [==============================] - 1s 7ms/step - loss: 0.0212 - last_time_step_mse: 0.0092 - val_loss: 0.0214 - val_last_time_step_mse: 0.0099\nEpoch 13/20\n219/219 [==============================] - 1s 7ms/step - loss: 0.0209 - last_time_step_mse: 0.0090 - val_loss: 0.0204 - val_last_time_step_mse: 0.0084\nEpoch 14/20\n219/219 [==============================] - 1s 6ms/step - loss: 0.0207 - last_time_step_mse: 0.0088 - val_loss: 0.0202 - val_last_time_step_mse: 0.0084\nEpoch 15/20\n219/219 [==============================] - 2s 7ms/step - loss: 0.0202 - last_time_step_mse: 0.0085 - val_loss: 0.0198 - val_last_time_step_mse: 0.0079\nEpoch 16/20\n219/219 [==============================] - 1s 7ms/step - loss: 0.0205 - last_time_step_mse: 0.0086 - val_loss: 0.0197 - val_last_time_step_mse: 0.0080\nEpoch 17/20\n219/219 [==============================] - 1s 6ms/step - loss: 0.0196 - last_time_step_mse: 0.0078 - val_loss: 0.0194 - val_last_time_step_mse: 0.0077\nEpoch 18/20\n219/219 [==============================] - 1s 7ms/step - loss: 0.0194 - last_time_step_mse: 0.0074 - val_loss: 0.0192 - val_last_time_step_mse: 0.0076\nEpoch 19/20\n219/219 [==============================] - 2s 7ms/step - loss: 0.0193 - last_time_step_mse: 0.0077 - val_loss: 0.0188 - val_last_time_step_mse: 0.0072\nEpoch 20/20\n219/219 [==============================] - 2s 7ms/step - loss: 0.0190 - last_time_step_mse: 0.0073 - val_loss: 0.0188 - val_last_time_step_mse: 0.0072\n\n\n다음은 논문에 정의된 원본 WaveNet입니다: ReLU 대신에 GatedActivationUnit과 스킵 연결을 사용합니다. 또한 점점 더 시퀀스가 짧아지는 것을 피하기 위해 왼쪽에 0으로 패딩합니다:\n\nclass GatedActivationUnit(keras.layers.Layer):\n    def __init__(self, activation=\"tanh\", **kwargs):\n        super().__init__(**kwargs)\n        self.activation = keras.activations.get(activation)\n    def call(self, inputs):\n        n_filters = inputs.shape[-1] // 2\n        linear_output = self.activation(inputs[..., :n_filters])\n        gate = keras.activations.sigmoid(inputs[..., n_filters:])\n        return self.activation(linear_output) * gate\n\n\ndef wavenet_residual_block(inputs, n_filters, dilation_rate):\n    z = keras.layers.Conv1D(2 * n_filters, kernel_size=2, padding=\"causal\",\n                            dilation_rate=dilation_rate)(inputs)\n    z = GatedActivationUnit()(z)\n    z = keras.layers.Conv1D(n_filters, kernel_size=1)(z)\n    return keras.layers.Add()([z, inputs]), z\n\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nn_layers_per_block = 3 # 10 in the paper\nn_blocks = 1 # 3 in the paper\nn_filters = 32 # 128 in the paper\nn_outputs = 10 # 256 in the paper\n\ninputs = keras.layers.Input(shape=[None, 1])\nz = keras.layers.Conv1D(n_filters, kernel_size=2, padding=\"causal\")(inputs)\nskip_to_last = []\nfor dilation_rate in [2**i for i in range(n_layers_per_block)] * n_blocks:\n    z, skip = wavenet_residual_block(z, n_filters, dilation_rate)\n    skip_to_last.append(skip)\nz = keras.activations.relu(keras.layers.Add()(skip_to_last))\nz = keras.layers.Conv1D(n_filters, kernel_size=1, activation=\"relu\")(z)\nY_proba = keras.layers.Conv1D(n_outputs, kernel_size=1, activation=\"softmax\")(z)\n\nmodel = keras.models.Model(inputs=[inputs], outputs=[Y_proba])\n\n\nmodel.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\nhistory = model.fit(X_train, Y_train, epochs=2,\n                    validation_data=(X_valid, Y_valid))\n\nEpoch 1/2\n219/219 [==============================] - 3s 9ms/step - loss: 0.1387 - last_time_step_mse: 0.1347 - val_loss: 0.1229 - val_last_time_step_mse: 0.1199\nEpoch 2/2\n219/219 [==============================] - 2s 8ms/step - loss: 0.1222 - last_time_step_mse: 0.1161 - val_loss: 0.1217 - val_last_time_step_mse: 0.1189\n\n\n이 장에서 RNN의 기초 사항을 살펴 보았고 RNN을 사용해 시퀀스(소위 시계열)을 처리했습니다. CNN을 포함하여 시퀀스를 처리하는 다른 방법도 알아 보았습니다. 다음 장에서는 RNN을 자연어 처리에 적용해 보겠습니다. 그리고 RNN에 대해 더 자세히 배워 보겠습니다(양방향 RNN, 상태가 있는 RNN과 상태가 없는 RNN, 인코더-디코더, 어텐션을 사용한 인코더-디코더). 또한 어텐션만 사용하는 구조인 트랜스포머도 살펴 보겠습니다."
  },
  {
    "objectID": "Machine_Learning/15_processing_sequences_using_rnns_and_cnns.html#to-8.",
    "href": "Machine_Learning/15_processing_sequences_using_rnns_and_cnns.html#to-8.",
    "title": "15_processing_sequences_using_rnns_and_cnns",
    "section": "1. to 8.",
    "text": "1. to 8.\n부록 A 참조."
  },
  {
    "objectID": "Machine_Learning/15_processing_sequences_using_rnns_and_cnns.html#sketchrnn-데이터셋-다루기",
    "href": "Machine_Learning/15_processing_sequences_using_rnns_and_cnns.html#sketchrnn-데이터셋-다루기",
    "title": "15_processing_sequences_using_rnns_and_cnns",
    "section": "9. SketchRNN 데이터셋 다루기",
    "text": "9. SketchRNN 데이터셋 다루기\n연습문제: 텐서플로 데이터셋에서 제공하는 SketchRNN 데이터셋으로 분류 모델을 훈련해보세요.\n이 데이터셋은 아직 TFDS에서 제공하지 않습니다. 아직 풀 리퀘스트가 진행 중입니다. 다행히 이 데이터는 TFRecord로 제공되므로 다운로드해보죠(3,450,000 훈련 스케치와 345,000 테스트 스케치가 포함된 이 데이터셋은 1GB 정도되기 때문에 다운로드 시간이 조금 걸립니다):\n\nDOWNLOAD_ROOT = \"http://download.tensorflow.org/data/\"\nFILENAME = \"quickdraw_tutorial_dataset_v1.tar.gz\"\nfilepath = keras.utils.get_file(FILENAME,\n                                DOWNLOAD_ROOT + FILENAME,\n                                cache_subdir=\"datasets/quickdraw\",\n                                extract=True)\n\n\nquickdraw_dir = Path(filepath).parent\ntrain_files = sorted([str(path) for path in quickdraw_dir.glob(\"training.tfrecord-*\")])\neval_files = sorted([str(path) for path in quickdraw_dir.glob(\"eval.tfrecord-*\")])\n\n\ntrain_files\n\n['/Users/ageron/.keras/datasets/quickdraw/training.tfrecord-00000-of-00010',\n '/Users/ageron/.keras/datasets/quickdraw/training.tfrecord-00001-of-00010',\n '/Users/ageron/.keras/datasets/quickdraw/training.tfrecord-00002-of-00010',\n '/Users/ageron/.keras/datasets/quickdraw/training.tfrecord-00003-of-00010',\n '/Users/ageron/.keras/datasets/quickdraw/training.tfrecord-00004-of-00010',\n '/Users/ageron/.keras/datasets/quickdraw/training.tfrecord-00005-of-00010',\n '/Users/ageron/.keras/datasets/quickdraw/training.tfrecord-00006-of-00010',\n '/Users/ageron/.keras/datasets/quickdraw/training.tfrecord-00007-of-00010',\n '/Users/ageron/.keras/datasets/quickdraw/training.tfrecord-00008-of-00010',\n '/Users/ageron/.keras/datasets/quickdraw/training.tfrecord-00009-of-00010']\n\n\n\neval_files\n\n['/Users/ageron/.keras/datasets/quickdraw/eval.tfrecord-00000-of-00010',\n '/Users/ageron/.keras/datasets/quickdraw/eval.tfrecord-00001-of-00010',\n '/Users/ageron/.keras/datasets/quickdraw/eval.tfrecord-00002-of-00010',\n '/Users/ageron/.keras/datasets/quickdraw/eval.tfrecord-00003-of-00010',\n '/Users/ageron/.keras/datasets/quickdraw/eval.tfrecord-00004-of-00010',\n '/Users/ageron/.keras/datasets/quickdraw/eval.tfrecord-00005-of-00010',\n '/Users/ageron/.keras/datasets/quickdraw/eval.tfrecord-00006-of-00010',\n '/Users/ageron/.keras/datasets/quickdraw/eval.tfrecord-00007-of-00010',\n '/Users/ageron/.keras/datasets/quickdraw/eval.tfrecord-00008-of-00010',\n '/Users/ageron/.keras/datasets/quickdraw/eval.tfrecord-00009-of-00010']\n\n\n\nwith open(quickdraw_dir / \"eval.tfrecord.classes\") as test_classes_file:\n    test_classes = test_classes_file.readlines()\n    \nwith open(quickdraw_dir / \"training.tfrecord.classes\") as train_classes_file:\n    train_classes = train_classes_file.readlines()\n\n\nassert train_classes == test_classes\nclass_names = [name.strip().lower() for name in train_classes]\n\n\nsorted(class_names)\n\n['aircraft carrier',\n 'airplane',\n 'alarm clock',\n 'ambulance',\n 'angel',\n 'animal migration',\n 'ant',\n 'anvil',\n 'apple',\n 'arm',\n 'asparagus',\n 'axe',\n 'backpack',\n 'banana',\n 'bandage',\n 'barn',\n 'baseball',\n 'baseball bat',\n 'basket',\n 'basketball',\n 'bat',\n 'bathtub',\n 'beach',\n 'bear',\n 'beard',\n 'bed',\n 'bee',\n 'belt',\n 'bench',\n 'bicycle',\n 'binoculars',\n 'bird',\n 'birthday cake',\n 'blackberry',\n 'blueberry',\n 'book',\n 'boomerang',\n 'bottlecap',\n 'bowtie',\n 'bracelet',\n 'brain',\n 'bread',\n 'bridge',\n 'broccoli',\n 'broom',\n 'bucket',\n 'bulldozer',\n 'bus',\n 'bush',\n 'butterfly',\n 'cactus',\n 'cake',\n 'calculator',\n 'calendar',\n 'camel',\n 'camera',\n 'camouflage',\n 'campfire',\n 'candle',\n 'cannon',\n 'canoe',\n 'car',\n 'carrot',\n 'castle',\n 'cat',\n 'ceiling fan',\n 'cell phone',\n 'cello',\n 'chair',\n 'chandelier',\n 'church',\n 'circle',\n 'clarinet',\n 'clock',\n 'cloud',\n 'coffee cup',\n 'compass',\n 'computer',\n 'cookie',\n 'cooler',\n 'couch',\n 'cow',\n 'crab',\n 'crayon',\n 'crocodile',\n 'crown',\n 'cruise ship',\n 'cup',\n 'diamond',\n 'dishwasher',\n 'diving board',\n 'dog',\n 'dolphin',\n 'donut',\n 'door',\n 'dragon',\n 'dresser',\n 'drill',\n 'drums',\n 'duck',\n 'dumbbell',\n 'ear',\n 'elbow',\n 'elephant',\n 'envelope',\n 'eraser',\n 'eye',\n 'eyeglasses',\n 'face',\n 'fan',\n 'feather',\n 'fence',\n 'finger',\n 'fire hydrant',\n 'fireplace',\n 'firetruck',\n 'fish',\n 'flamingo',\n 'flashlight',\n 'flip flops',\n 'floor lamp',\n 'flower',\n 'flying saucer',\n 'foot',\n 'fork',\n 'frog',\n 'frying pan',\n 'garden',\n 'garden hose',\n 'giraffe',\n 'goatee',\n 'golf club',\n 'grapes',\n 'grass',\n 'guitar',\n 'hamburger',\n 'hammer',\n 'hand',\n 'harp',\n 'hat',\n 'headphones',\n 'hedgehog',\n 'helicopter',\n 'helmet',\n 'hexagon',\n 'hockey puck',\n 'hockey stick',\n 'horse',\n 'hospital',\n 'hot air balloon',\n 'hot dog',\n 'hot tub',\n 'hourglass',\n 'house',\n 'house plant',\n 'hurricane',\n 'ice cream',\n 'jacket',\n 'jail',\n 'kangaroo',\n 'key',\n 'keyboard',\n 'knee',\n 'knife',\n 'ladder',\n 'lantern',\n 'laptop',\n 'leaf',\n 'leg',\n 'light bulb',\n 'lighter',\n 'lighthouse',\n 'lightning',\n 'line',\n 'lion',\n 'lipstick',\n 'lobster',\n 'lollipop',\n 'mailbox',\n 'map',\n 'marker',\n 'matches',\n 'megaphone',\n 'mermaid',\n 'microphone',\n 'microwave',\n 'monkey',\n 'moon',\n 'mosquito',\n 'motorbike',\n 'mountain',\n 'mouse',\n 'moustache',\n 'mouth',\n 'mug',\n 'mushroom',\n 'nail',\n 'necklace',\n 'nose',\n 'ocean',\n 'octagon',\n 'octopus',\n 'onion',\n 'oven',\n 'owl',\n 'paint can',\n 'paintbrush',\n 'palm tree',\n 'panda',\n 'pants',\n 'paper clip',\n 'parachute',\n 'parrot',\n 'passport',\n 'peanut',\n 'pear',\n 'peas',\n 'pencil',\n 'penguin',\n 'piano',\n 'pickup truck',\n 'picture frame',\n 'pig',\n 'pillow',\n 'pineapple',\n 'pizza',\n 'pliers',\n 'police car',\n 'pond',\n 'pool',\n 'popsicle',\n 'postcard',\n 'potato',\n 'power outlet',\n 'purse',\n 'rabbit',\n 'raccoon',\n 'radio',\n 'rain',\n 'rainbow',\n 'rake',\n 'remote control',\n 'rhinoceros',\n 'rifle',\n 'river',\n 'roller coaster',\n 'rollerskates',\n 'sailboat',\n 'sandwich',\n 'saw',\n 'saxophone',\n 'school bus',\n 'scissors',\n 'scorpion',\n 'screwdriver',\n 'sea turtle',\n 'see saw',\n 'shark',\n 'sheep',\n 'shoe',\n 'shorts',\n 'shovel',\n 'sink',\n 'skateboard',\n 'skull',\n 'skyscraper',\n 'sleeping bag',\n 'smiley face',\n 'snail',\n 'snake',\n 'snorkel',\n 'snowflake',\n 'snowman',\n 'soccer ball',\n 'sock',\n 'speedboat',\n 'spider',\n 'spoon',\n 'spreadsheet',\n 'square',\n 'squiggle',\n 'squirrel',\n 'stairs',\n 'star',\n 'steak',\n 'stereo',\n 'stethoscope',\n 'stitches',\n 'stop sign',\n 'stove',\n 'strawberry',\n 'streetlight',\n 'string bean',\n 'submarine',\n 'suitcase',\n 'sun',\n 'swan',\n 'sweater',\n 'swing set',\n 'sword',\n 'syringe',\n 't-shirt',\n 'table',\n 'teapot',\n 'teddy-bear',\n 'telephone',\n 'television',\n 'tennis racquet',\n 'tent',\n 'the eiffel tower',\n 'the great wall of china',\n 'the mona lisa',\n 'tiger',\n 'toaster',\n 'toe',\n 'toilet',\n 'tooth',\n 'toothbrush',\n 'toothpaste',\n 'tornado',\n 'tractor',\n 'traffic light',\n 'train',\n 'tree',\n 'triangle',\n 'trombone',\n 'truck',\n 'trumpet',\n 'umbrella',\n 'underwear',\n 'van',\n 'vase',\n 'violin',\n 'washing machine',\n 'watermelon',\n 'waterslide',\n 'whale',\n 'wheel',\n 'windmill',\n 'wine bottle',\n 'wine glass',\n 'wristwatch',\n 'yoga',\n 'zebra',\n 'zigzag']\n\n\n\ndef parse(data_batch):\n    feature_descriptions = {\n        \"ink\": tf.io.VarLenFeature(dtype=tf.float32),\n        \"shape\": tf.io.FixedLenFeature([2], dtype=tf.int64),\n        \"class_index\": tf.io.FixedLenFeature([1], dtype=tf.int64)\n    }\n    examples = tf.io.parse_example(data_batch, feature_descriptions)\n    flat_sketches = tf.sparse.to_dense(examples[\"ink\"])\n    sketches = tf.reshape(flat_sketches, shape=[tf.size(data_batch), -1, 3])\n    lengths = examples[\"shape\"][:, 0]\n    labels = examples[\"class_index\"][:, 0]\n    return sketches, lengths, labels\n\n\ndef quickdraw_dataset(filepaths, batch_size=32, shuffle_buffer_size=None,\n                      n_parse_threads=5, n_read_threads=5, cache=False):\n    dataset = tf.data.TFRecordDataset(filepaths,\n                                      num_parallel_reads=n_read_threads)\n    if cache:\n        dataset = dataset.cache()\n    if shuffle_buffer_size:\n        dataset = dataset.shuffle(shuffle_buffer_size)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.map(parse, num_parallel_calls=n_parse_threads)\n    return dataset.prefetch(1)\n\n\ntrain_set = quickdraw_dataset(train_files, shuffle_buffer_size=10000)\nvalid_set = quickdraw_dataset(eval_files[:5])\ntest_set = quickdraw_dataset(eval_files[5:])\n\n\nfor sketches, lengths, labels in train_set.take(1):\n    print(\"sketches =\", sketches)\n    print(\"lengths =\", lengths)\n    print(\"labels =\", labels)\n\nsketches = tf.Tensor(\n[[[-0.07058823  0.04255319  0.        ]\n  [-0.01568627  0.0425532   0.        ]\n  [-0.09803921  0.03191489  0.        ]\n  ...\n  [ 0.          0.          0.        ]\n  [ 0.          0.          0.        ]\n  [ 0.          0.          0.        ]]\n\n [[ 0.07058824  0.27741933  0.        ]\n  [-0.02745098  0.06451613  0.        ]\n  [-0.02352941  0.          0.        ]\n  ...\n  [ 0.          0.          0.        ]\n  [ 0.          0.          0.        ]\n  [ 0.          0.          0.        ]]\n\n [[-0.17857143  0.06666667  0.        ]\n  [-0.26020408  0.15294117  0.        ]\n  [-0.01020408  0.01568627  0.        ]\n  ...\n  [ 0.          0.          0.        ]\n  [ 0.          0.          0.        ]\n  [ 0.          0.          0.        ]]\n\n ...\n\n [[ 0.03056769 -0.01176471  0.        ]\n  [ 0.29694325  0.          0.        ]\n  [ 0.38864627  0.04705882  0.        ]\n  ...\n  [ 0.          0.          0.        ]\n  [ 0.          0.          0.        ]\n  [ 0.          0.          0.        ]]\n\n [[ 0.34901962  0.02985072  0.        ]\n  [ 0.10588235  0.07462686  0.        ]\n  [ 0.01176471 -0.35820895  0.        ]\n  ...\n  [ 0.          0.          0.        ]\n  [ 0.          0.          0.        ]\n  [ 0.          0.          0.        ]]\n\n [[ 0.01176471  0.          0.        ]\n  [ 0.00392157  0.03448276  0.        ]\n  [ 0.00784314  0.21551724  0.        ]\n  ...\n  [ 0.          0.          0.        ]\n  [ 0.          0.          0.        ]\n  [ 0.          0.          0.        ]]], shape=(32, 195, 3), dtype=float32)\nlengths = tf.Tensor(\n[ 44  30  18  44  20  21  26  44  17  43  47  44  34  39  50  28  24  29\n  37  17 195  64  78  49  45  33  28  19  17  56  12  30], shape=(32,), dtype=int64)\nlabels = tf.Tensor(\n[ 70 247 266  10 149 170 268 252  53 121  11   5 116 209 199  50 244  32\n 327 140  22  58   8 151 204 167  39 275 143 333 152  71], shape=(32,), dtype=int64)\n\n\n\ndef draw_sketch(sketch, label=None):\n    origin = np.array([[0., 0., 0.]])\n    sketch = np.r_[origin, sketch]\n    stroke_end_indices = np.argwhere(sketch[:, -1]==1.)[:, 0]\n    coordinates = np.cumsum(sketch[:, :2], axis=0)\n    strokes = np.split(coordinates, stroke_end_indices + 1)\n    title = class_names[label.numpy()] if label is not None else \"Try to guess\"\n    plt.title(title)\n    plt.plot(coordinates[:, 0], -coordinates[:, 1], \"y:\")\n    for stroke in strokes:\n        plt.plot(stroke[:, 0], -stroke[:, 1], \".-\")\n    plt.axis(\"off\")\n\ndef draw_sketches(sketches, lengths, labels):\n    n_sketches = len(sketches)\n    n_cols = 4\n    n_rows = (n_sketches - 1) // n_cols + 1\n    plt.figure(figsize=(n_cols * 3, n_rows * 3.5))\n    for index, sketch, length, label in zip(range(n_sketches), sketches, lengths, labels):\n        plt.subplot(n_rows, n_cols, index + 1)\n        draw_sketch(sketch[:length], label)\n    plt.show()\n\nfor sketches, lengths, labels in train_set.take(1):\n    draw_sketches(sketches, lengths, labels)\n\n\n\n\n대부분의 스케치는 100개 포인트 이하로 구성되어 있습니다:\n\nlengths = np.concatenate([lengths for _, lengths, _ in train_set.take(1000)])\nplt.hist(lengths, bins=150, density=True)\nplt.axis([0, 200, 0, 0.03])\nplt.xlabel(\"length\")\nplt.ylabel(\"density\")\nplt.show()\n\n\n\n\n\ndef crop_long_sketches(dataset, max_length=100):\n    return dataset.map(lambda inks, lengths, labels: (inks[:, :max_length], labels))\n\ncropped_train_set = crop_long_sketches(train_set)\ncropped_valid_set = crop_long_sketches(valid_set)\ncropped_test_set = crop_long_sketches(test_set)\n\n\nmodel = keras.models.Sequential([\n    keras.layers.Conv1D(32, kernel_size=5, strides=2, activation=\"relu\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv1D(64, kernel_size=5, strides=2, activation=\"relu\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv1D(128, kernel_size=3, strides=2, activation=\"relu\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.LSTM(128, return_sequences=True),\n    keras.layers.LSTM(128),\n    keras.layers.Dense(len(class_names), activation=\"softmax\")\n])\noptimizer = keras.optimizers.SGD(learning_rate=1e-2, clipnorm=1.)\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=optimizer,\n              metrics=[\"accuracy\", \"sparse_top_k_categorical_accuracy\"])\nhistory = model.fit(cropped_train_set, epochs=2,\n                    validation_data=cropped_valid_set)\n\nEpoch 1/2\n107813/107813 [==============================] - 2182s 20ms/step - loss: 3.8473 - accuracy: 0.2086 - sparse_top_k_categorical_accuracy: 0.4242 - val_loss: 2.6672 - val_accuracy: 0.3872 - val_sparse_top_k_categorical_accuracy: 0.6771\nEpoch 2/2\n107813/107813 [==============================] - 2049s 19ms/step - loss: 2.3393 - accuracy: 0.4502 - sparse_top_k_categorical_accuracy: 0.7367 - val_loss: 2.1072 - val_accuracy: 0.4968 - val_sparse_top_k_categorical_accuracy: 0.7759\n\n\n\ny_test = np.concatenate([labels for _, _, labels in test_set])\ny_probas = model.predict(test_set)\n\n\nnp.mean(keras.metrics.sparse_top_k_categorical_accuracy(y_test, y_probas))\n\n0.6899671\n\n\n\nn_new = 10\nY_probas = model.predict(sketches)\ntop_k = tf.nn.top_k(Y_probas, k=5)\nfor index in range(n_new):\n    plt.figure(figsize=(3, 3.5))\n    draw_sketch(sketches[index])\n    plt.show()\n    print(\"Top-5 predictions:\".format(index + 1))\n    for k in range(5):\n        class_name = class_names[top_k.indices[index, k]]\n        proba = 100 * top_k.values[index, k]\n        print(\"  {}. {} {:.3f}%\".format(k + 1, class_name, proba))\n    print(\"Answer: {}\".format(class_names[labels[index].numpy()]))\n\n\n\n\nTop-5 predictions:\n  1. firetruck 46.565%\n  2. police car 30.455%\n  3. ambulance 3.810%\n  4. car 3.695%\n  5. cannon 3.371%\nAnswer: firetruck\nTop-5 predictions:\n  1. mouth 23.162%\n  2. pond 14.151%\n  3. pool 12.582%\n  4. beard 11.375%\n  5. goatee 9.808%\nAnswer: mouth\nTop-5 predictions:\n  1. jail 71.532%\n  2. fence 6.519%\n  3. swing set 5.708%\n  4. grass 3.302%\n  5. rain 3.023%\nAnswer: jail\nTop-5 predictions:\n  1. baseball 79.233%\n  2. watermelon 7.687%\n  3. basketball 5.259%\n  4. clock 1.659%\n  5. compass 1.101%\nAnswer: baseball\nTop-5 predictions:\n  1. basketball 51.888%\n  2. baseball 17.328%\n  3. onion 12.688%\n  4. watermelon 9.989%\n  5. brain 2.216%\nAnswer: baseball\nTop-5 predictions:\n  1. lantern 7.235%\n  2. toothpaste 6.845%\n  3. drill 6.254%\n  4. lighthouse 4.624%\n  5. crayon 3.566%\nAnswer: brain\nTop-5 predictions:\n  1. animal migration 8.771%\n  2. blackberry 7.932%\n  3. blueberry 6.413%\n  4. peas 5.549%\n  5. bracelet 3.623%\nAnswer: helicopter\nTop-5 predictions:\n  1. vase 42.793%\n  2. wine glass 13.744%\n  3. shovel 8.136%\n  4. house plant 5.144%\n  5. sailboat 4.850%\nAnswer: vase\nTop-5 predictions:\n  1. anvil 25.870%\n  2. drill 9.670%\n  3. nail 7.246%\n  4. screwdriver 5.611%\n  5. knee 4.355%\nAnswer: anvil\nTop-5 predictions:\n  1. hurricane 34.674%\n  2. tornado 16.056%\n  3. blackberry 7.664%\n  4. squiggle 5.489%\n  5. zigzag 4.906%\nAnswer: pillow\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel.save(\"my_sketchrnn\")\n\nWARNING:tensorflow:From /Users/ageron/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nINFO:tensorflow:Assets written to: my_sketchrnn/assets"
  },
  {
    "objectID": "Machine_Learning/15_processing_sequences_using_rnns_and_cnns.html#바흐-합창곡",
    "href": "Machine_Learning/15_processing_sequences_using_rnns_and_cnns.html#바흐-합창곡",
    "title": "15_processing_sequences_using_rnns_and_cnns",
    "section": "10. 바흐 합창곡",
    "text": "10. 바흐 합창곡\n연습문제: 바흐 합창곡 데이터셋을 다운로드하여 압축을 풉니다. 이 데이터셋은 요한 제바스티안 바흐가 작곡한 382개의 합창곡으로 구성되어 있습니다. 각 곡은 100에서 640까지 타임 스텝 길이입니다. 각 타임 스텝은 4개의 정수를 담고 있습니다. 각 정수는 피아노 음표의 인덱스에 해당합니다(연주되는 음표가 없다는 것을 의미하는 0은 제외). 코랄의 타임 스텝 시퀀스가 주어지면 다음 타임 스텝(4개의 음표)을 예측할 수 있는 순환 모델, 합성곱 모델 또는 두 가지를 합친 모델을 훈련하세요. 그 다음 이 모델을 사용해 한 번에 하나의 음표씩 바흐와 같은 음악을 생성하세요. 코랄의 시작 부분을 모델에 주입하고 다음 타임 스텝을 예측합니다. 이 타임 스텝을 입력 시퀀스에 추가하여 모델이 다음 음표를 예측하게 만드는 식입니다. 또 바흐를 위한 구글 두들에 사용한 구글의 Coconet 모델을 확인해보세요.\n\nDOWNLOAD_ROOT = \"https://github.com/ageron/handson-ml2/raw/master/datasets/jsb_chorales/\"\nFILENAME = \"jsb_chorales.tgz\"\nfilepath = keras.utils.get_file(FILENAME,\n                                DOWNLOAD_ROOT + FILENAME,\n                                cache_subdir=\"datasets/jsb_chorales\",\n                                extract=True)\n\n\njsb_chorales_dir = Path(filepath).parent\ntrain_files = sorted(jsb_chorales_dir.glob(\"train/chorale_*.csv\"))\nvalid_files = sorted(jsb_chorales_dir.glob(\"valid/chorale_*.csv\"))\ntest_files = sorted(jsb_chorales_dir.glob(\"test/chorale_*.csv\"))\n\n\nimport pandas as pd\n\ndef load_chorales(filepaths):\n    return [pd.read_csv(filepath).values.tolist() for filepath in filepaths]\n\ntrain_chorales = load_chorales(train_files)\nvalid_chorales = load_chorales(valid_files)\ntest_chorales = load_chorales(test_files)\n\n\ntrain_chorales[0]\n\n[[74, 70, 65, 58],\n [74, 70, 65, 58],\n [74, 70, 65, 58],\n [74, 70, 65, 58],\n [75, 70, 58, 55],\n [75, 70, 58, 55],\n [75, 70, 60, 55],\n [75, 70, 60, 55],\n [77, 69, 62, 50],\n [77, 69, 62, 50],\n [77, 69, 62, 50],\n [77, 69, 62, 50],\n [77, 70, 62, 55],\n [77, 70, 62, 55],\n [77, 69, 62, 55],\n [77, 69, 62, 55],\n [75, 67, 63, 48],\n [75, 67, 63, 48],\n [75, 69, 63, 48],\n [75, 69, 63, 48],\n [74, 70, 65, 46],\n [74, 70, 65, 46],\n [74, 70, 65, 46],\n [74, 70, 65, 46],\n [72, 69, 65, 53],\n [72, 69, 65, 53],\n [72, 69, 65, 53],\n [72, 69, 65, 53],\n [72, 69, 65, 53],\n [72, 69, 65, 53],\n [72, 69, 65, 53],\n [72, 69, 65, 53],\n [74, 70, 65, 46],\n [74, 70, 65, 46],\n [74, 70, 65, 46],\n [74, 70, 65, 46],\n [75, 69, 63, 48],\n [75, 69, 63, 48],\n [75, 67, 63, 48],\n [75, 67, 63, 48],\n [77, 65, 62, 50],\n [77, 65, 62, 50],\n [77, 65, 60, 50],\n [77, 65, 60, 50],\n [74, 67, 58, 55],\n [74, 67, 58, 55],\n [74, 67, 58, 53],\n [74, 67, 58, 53],\n [72, 67, 58, 51],\n [72, 67, 58, 51],\n [72, 67, 58, 51],\n [72, 67, 58, 51],\n [72, 65, 57, 53],\n [72, 65, 57, 53],\n [72, 65, 57, 53],\n [72, 65, 57, 53],\n [70, 65, 62, 46],\n [70, 65, 62, 46],\n [70, 65, 62, 46],\n [70, 65, 62, 46],\n [70, 65, 62, 46],\n [70, 65, 62, 46],\n [70, 65, 62, 46],\n [70, 65, 62, 46],\n [72, 69, 65, 53],\n [72, 69, 65, 53],\n [72, 69, 65, 53],\n [72, 69, 65, 53],\n [74, 71, 53, 50],\n [74, 71, 53, 50],\n [74, 71, 53, 50],\n [74, 71, 53, 50],\n [75, 72, 55, 48],\n [75, 72, 55, 48],\n [75, 72, 55, 50],\n [75, 72, 55, 50],\n [75, 67, 60, 51],\n [75, 67, 60, 51],\n [75, 67, 60, 53],\n [75, 67, 60, 53],\n [74, 67, 60, 55],\n [74, 67, 60, 55],\n [74, 67, 57, 55],\n [74, 67, 57, 55],\n [74, 65, 59, 43],\n [74, 65, 59, 43],\n [72, 63, 59, 43],\n [72, 63, 59, 43],\n [72, 63, 55, 48],\n [72, 63, 55, 48],\n [72, 63, 55, 48],\n [72, 63, 55, 48],\n [72, 63, 55, 48],\n [72, 63, 55, 48],\n [72, 63, 55, 48],\n [72, 63, 55, 48],\n [75, 67, 60, 60],\n [75, 67, 60, 60],\n [75, 67, 60, 60],\n [75, 67, 60, 60],\n [77, 70, 62, 58],\n [77, 70, 62, 58],\n [77, 70, 62, 56],\n [77, 70, 62, 56],\n [79, 70, 62, 55],\n [79, 70, 62, 55],\n [79, 70, 62, 53],\n [79, 70, 62, 53],\n [79, 70, 63, 51],\n [79, 70, 63, 51],\n [79, 70, 63, 51],\n [79, 70, 63, 51],\n [77, 70, 63, 58],\n [77, 70, 63, 58],\n [77, 70, 60, 58],\n [77, 70, 60, 58],\n [77, 70, 62, 46],\n [77, 70, 62, 46],\n [77, 68, 62, 46],\n [75, 68, 62, 46],\n [75, 67, 58, 51],\n [75, 67, 58, 51],\n [75, 67, 58, 51],\n [75, 67, 58, 51],\n [75, 67, 58, 51],\n [75, 67, 58, 51],\n [75, 67, 58, 51],\n [75, 67, 58, 51],\n [74, 67, 58, 55],\n [74, 67, 58, 55],\n [74, 67, 58, 55],\n [74, 67, 58, 55],\n [75, 67, 58, 53],\n [75, 67, 58, 53],\n [75, 67, 58, 51],\n [75, 67, 58, 51],\n [77, 65, 58, 50],\n [77, 65, 58, 50],\n [77, 65, 56, 50],\n [77, 65, 56, 50],\n [70, 63, 55, 51],\n [70, 63, 55, 51],\n [70, 63, 55, 51],\n [70, 63, 55, 51],\n [75, 65, 60, 45],\n [75, 65, 60, 45],\n [75, 65, 60, 45],\n [75, 65, 60, 45],\n [74, 65, 58, 46],\n [74, 65, 58, 46],\n [74, 65, 58, 46],\n [74, 65, 58, 46],\n [72, 65, 57, 53],\n [72, 65, 57, 53],\n [72, 65, 57, 53],\n [72, 65, 57, 53],\n [72, 65, 57, 53],\n [72, 65, 57, 53],\n [72, 65, 57, 53],\n [72, 65, 57, 53],\n [74, 65, 58, 58],\n [74, 65, 58, 58],\n [74, 65, 58, 58],\n [74, 65, 58, 58],\n [75, 67, 58, 57],\n [75, 67, 58, 57],\n [75, 67, 58, 55],\n [75, 67, 58, 55],\n [77, 65, 60, 57],\n [77, 65, 60, 57],\n [77, 65, 60, 53],\n [77, 65, 60, 53],\n [74, 65, 58, 58],\n [74, 65, 58, 58],\n [74, 65, 58, 58],\n [74, 65, 58, 58],\n [72, 67, 58, 51],\n [72, 67, 58, 51],\n [72, 67, 58, 51],\n [72, 67, 58, 51],\n [72, 65, 57, 53],\n [72, 65, 57, 53],\n [72, 65, 57, 53],\n [72, 65, 57, 53],\n [70, 65, 62, 46],\n [70, 65, 62, 46],\n [70, 65, 62, 46],\n [70, 65, 62, 46],\n [70, 65, 62, 46],\n [70, 65, 62, 46],\n [70, 65, 62, 46],\n [70, 65, 62, 46]]\n\n\n음표의 범위는 36(C1 = 옥타브 1의 C)에서 81(A5 = 옥타브 5의 A)까지이고 무음을 위해 0을 추가합니다:\n\nnotes = set()\nfor chorales in (train_chorales, valid_chorales, test_chorales):\n    for chorale in chorales:\n        for chord in chorale:\n            notes |= set(chord)\n\nn_notes = len(notes)\nmin_note = min(notes - {0})\nmax_note = max(notes)\n\nassert min_note == 36\nassert max_note == 81\n\n이 코랄을 듣기 위한 몇 개의 함수를 만들어 보죠(자세한 내용을 이해할 필요는 없습니다. 사실 MIDI 플레이어처럼 더 간단한 방법이 있지만 그냥 하나의 합성기(synthesizer)를 만들어 보고 싶었습니다):\n\nfrom IPython.display import Audio\n\ndef notes_to_frequencies(notes):\n    # 한 옥타브 올라갈 때 주파수는 두배가 됩니다; 옥타브마다 12개의 반음이 있습니다;\n    # 옥타브 4의 A는 440Hz이고 음표 번호는 69입니다.\n    return 2 ** ((np.array(notes) - 69) / 12) * 440\n\ndef frequencies_to_samples(frequencies, tempo, sample_rate):\n    note_duration = 60 / tempo # tempo는 분당 박자 수로 측정합니다\n    # 매 박자마다 딸깍거리는 소리를 줄이기 위해 주파수를 반올림하여 각 음의 끝에서 샘플을 0에 가깝게 만듭니다.\n    frequencies = np.round(note_duration * frequencies) / note_duration\n    n_samples = int(note_duration * sample_rate)\n    time = np.linspace(0, note_duration, n_samples)\n    sine_waves = np.sin(2 * np.pi * frequencies.reshape(-1, 1) * time)\n    # (음표 0 = 무음을 포함해) 9Hz 이하인 주파수는 모두 삭제합니다\n    sine_waves *= (frequencies &gt; 9.).reshape(-1, 1)\n    return sine_waves.reshape(-1)\n\ndef chords_to_samples(chords, tempo, sample_rate):\n    freqs = notes_to_frequencies(chords)\n    freqs = np.r_[freqs, freqs[-1:]] # 마지막 음표를 조금 더 길게합니다\n    merged = np.mean([frequencies_to_samples(melody, tempo, sample_rate)\n                     for melody in freqs.T], axis=0)\n    n_fade_out_samples = sample_rate * 60 // tempo # 마지막 음을 희미하게 합니다\n    fade_out = np.linspace(1., 0., n_fade_out_samples)**2\n    merged[-n_fade_out_samples:] *= fade_out\n    return merged\n\ndef play_chords(chords, tempo=160, amplitude=0.1, sample_rate=44100, filepath=None):\n    samples = amplitude * chords_to_samples(chords, tempo, sample_rate)\n    if filepath:\n        from scipy.io import wavfile\n        samples = (2**15 * samples).astype(np.int16)\n        wavfile.write(filepath, sample_rate, samples)\n        return display(Audio(filepath))\n    else:\n        return display(Audio(samples, rate=sample_rate))\n\n이제 몇 개의 코랄을 들어 보죠:\n\nfor index in range(3):\n    play_chords(train_chorales[index])\n\n멋지네요! :)\n새로운 코랄을 생성하기 위해서는 이전의 화음이 주어졌을 때 다음 화음을 예측할 수 있는 모델을 훈련해야 합니다. 한 번에 4개의 음표를 예측하는 식으로 다음 화음을 예측한다면 잘 어울리지 않는 음표를 얻게 됩니다(믿으세요. 제가 해 보았습니다). 한 번에 하나의 음표를 예측하는 것이 간단하고 더 낫습니다. 따라서 모든 코랄을 전처리하여 각 화음을 아르페지오로 바꾸어야 합니다(즉, 동시에 연주되는 음표가 아니라 음표의 시퀀스). 그다음 이전의 모든 음표가 주어졌을 때 다음 음표를 예측하는 모델을 훈련할 수 있습니다. 시퀀스-투-시퀀스 방식을 사용하겠습니다. 신경망에 한 윈도를 주입하고 한 타임 스텝 미래로 이동한 윈도를 예측합니다.\n또한 0에서 46까지 범위를 갖도록 값을 이동시키겠습니다. 여기에서 0은 무음을 나타내고 1에서 46까지는 36(C1)에서 81(A5)까지를 나타냅니다.\n128 음표(즉, 32개 화음)의 윈도에서 모델을 훈련하겠습니다.\n이 데이터셋은 메모리에 올라갈 수 있기 때문에 파이썬 코드를 사용해 RAM에서 코랄을 전처리할 수 있지만 여기에서는 tf.data를 사용해 전처리하는 방법을 시연하겠습니다(다음 장에서 tf.data를 사용해 윈도를 만드는 과정을 자세히 설명하겠습니다).\n\ndef create_target(batch):\n    X = batch[:, :-1]\n    Y = batch[:, 1:] # 각 스텝에서 아르페지오에 있는 다음 음표를 예측합니다\n    return X, Y\n\ndef preprocess(window):\n    window = tf.where(window == 0, window, window - min_note + 1) # 값 이동\n    return tf.reshape(window, [-1]) # 아르페지오로 변환\n\ndef bach_dataset(chorales, batch_size=32, shuffle_buffer_size=None,\n                 window_size=32, window_shift=16, cache=True):\n    def batch_window(window):\n        return window.batch(window_size + 1)\n\n    def to_windows(chorale):\n        dataset = tf.data.Dataset.from_tensor_slices(chorale)\n        dataset = dataset.window(window_size + 1, window_shift, drop_remainder=True)\n        return dataset.flat_map(batch_window)\n\n    chorales = tf.ragged.constant(chorales, ragged_rank=1)\n    dataset = tf.data.Dataset.from_tensor_slices(chorales)\n    dataset = dataset.flat_map(to_windows).map(preprocess)\n    if cache:\n        dataset = dataset.cache()\n    if shuffle_buffer_size:\n        dataset = dataset.shuffle(shuffle_buffer_size)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.map(create_target)\n    return dataset.prefetch(1)\n\n훈련 세트, 검증 세트, 테스트 세트를 만듭니다:\n\ntrain_set = bach_dataset(train_chorales, shuffle_buffer_size=1000)\nvalid_set = bach_dataset(valid_chorales)\ntest_set = bach_dataset(test_chorales)\n\n이제 모델을 만듭니다:\n\n음표를 실수 값으로 모델에 직접 주입할 수 있지만 좋은 결과를 얻지 못할 것입니다. 음표 간의 관계는 단순하지 않습니다. 예를 들어 C3을 C4로 바꾼다면 두 음표 사이에 반음이 12개(즉 한 옥타브) 떨어져 있음에도 멜로디는 여전히 괜찮게 들립니다. 반대로 C3을 C#3으로 바꾼다면 바로 다음 음표임에도 화음이 매우 좋지 않습니다. 따라서 Embedding 층을 사용해 음표를 작은 벡터 표현으로 바꾸겠습니다(임베딩에 대해서는 16장을 참고하세요). 5-차원 임베딩을 사용하므로 첫 번째 층의 출력은 [batch_size, window_size, 5] 크기가 됩니다.\n그 다음 이 데이터를 4개의 Conv1D 층을 쌓고 dilation 비율을 두 배씩 늘린 작은 WeveNet 신경망에 주입합니다. 빠른 수렴을 위해 이 층 다음에 BatchNormalization 층을 배치합니다.\n그다음 하나의 LSTM 층이 장기 패턴을 감지합니다.\n마지막으로 Dense 층이 최종 음표 확률을 생성합니다. 타임 스텝과 (무음을 포함해) 가능한 음표 마다 배치에 있는 각 코랄에 대해 하나의 확률을 예측합니다. 따라서 출력 크기는 [batch_size, window_size, 47]가 됩니다.\n\n\nn_embedding_dims = 5\n\nmodel = keras.models.Sequential([\n    keras.layers.Embedding(input_dim=n_notes, output_dim=n_embedding_dims,\n                           input_shape=[None]),\n    keras.layers.Conv1D(32, kernel_size=2, padding=\"causal\", activation=\"relu\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv1D(48, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=2),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv1D(64, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=4),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv1D(96, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=8),\n    keras.layers.BatchNormalization(),\n    keras.layers.LSTM(256, return_sequences=True),\n    keras.layers.Dense(n_notes, activation=\"softmax\")\n])\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, None, 5)           235       \n_________________________________________________________________\nconv1d (Conv1D)              (None, None, 32)          352       \n_________________________________________________________________\nbatch_normalization (BatchNo (None, None, 32)          128       \n_________________________________________________________________\nconv1d_1 (Conv1D)            (None, None, 48)          3120      \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, None, 48)          192       \n_________________________________________________________________\nconv1d_2 (Conv1D)            (None, None, 64)          6208      \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, None, 64)          256       \n_________________________________________________________________\nconv1d_3 (Conv1D)            (None, None, 96)          12384     \n_________________________________________________________________\nbatch_normalization_3 (Batch (None, None, 96)          384       \n_________________________________________________________________\nlstm (LSTM)                  (None, None, 256)         361472    \n_________________________________________________________________\ndense (Dense)                (None, None, 47)          12079     \n=================================================================\nTotal params: 396,810\nTrainable params: 396,330\nNon-trainable params: 480\n_________________________________________________________________\n\n\n이제 모델을 컴파일하고 훈련할 준비가 되었습니다!\n\noptimizer = keras.optimizers.Nadam(learning_rate=1e-3)\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n              metrics=[\"accuracy\"])\nmodel.fit(train_set, epochs=20, validation_data=valid_set)\n\nEpoch 1/20\n98/98 [==============================] - 17s 171ms/step - loss: 1.8198 - accuracy: 0.5358 - val_loss: 3.7675 - val_accuracy: 0.0428\nEpoch 2/20\n98/98 [==============================] - 15s 152ms/step - loss: 0.8885 - accuracy: 0.7641 - val_loss: 4.1054 - val_accuracy: 0.0470\nEpoch 3/20\n98/98 [==============================] - 16s 165ms/step - loss: 0.7471 - accuracy: 0.7930 - val_loss: 3.8600 - val_accuracy: 0.0368\nEpoch 4/20\n98/98 [==============================] - 16s 165ms/step - loss: 0.6749 - accuracy: 0.8083 - val_loss: 3.0490 - val_accuracy: 0.2196\nEpoch 5/20\n98/98 [==============================] - 15s 157ms/step - loss: 0.6221 - accuracy: 0.8188 - val_loss: 1.7138 - val_accuracy: 0.5153\nEpoch 6/20\n98/98 [==============================] - 16s 163ms/step - loss: 0.5833 - accuracy: 0.8283 - val_loss: 1.9068 - val_accuracy: 0.4570\nEpoch 7/20\n98/98 [==============================] - 16s 165ms/step - loss: 0.5484 - accuracy: 0.8362 - val_loss: 0.7930 - val_accuracy: 0.7678\nEpoch 8/20\n98/98 [==============================] - 16s 159ms/step - loss: 0.5163 - accuracy: 0.8447 - val_loss: 0.6577 - val_accuracy: 0.8091\nEpoch 9/20\n98/98 [==============================] - 15s 158ms/step - loss: 0.4877 - accuracy: 0.8519 - val_loss: 0.6239 - val_accuracy: 0.8180\nEpoch 10/20\n98/98 [==============================] - 17s 171ms/step - loss: 0.4607 - accuracy: 0.8595 - val_loss: 0.6330 - val_accuracy: 0.8151\nEpoch 11/20\n98/98 [==============================] - 15s 156ms/step - loss: 0.4369 - accuracy: 0.8657 - val_loss: 0.6248 - val_accuracy: 0.8179\nEpoch 12/20\n98/98 [==============================] - 16s 167ms/step - loss: 0.4125 - accuracy: 0.8726 - val_loss: 0.6046 - val_accuracy: 0.8248\nEpoch 13/20\n98/98 [==============================] - 16s 162ms/step - loss: 0.3924 - accuracy: 0.8784 - val_loss: 0.6618 - val_accuracy: 0.8096\nEpoch 14/20\n98/98 [==============================] - 16s 159ms/step - loss: 0.3713 - accuracy: 0.8847 - val_loss: 0.6919 - val_accuracy: 0.8067\nEpoch 15/20\n98/98 [==============================] - 17s 176ms/step - loss: 0.3562 - accuracy: 0.8889 - val_loss: 0.6123 - val_accuracy: 0.8236\nEpoch 16/20\n98/98 [==============================] - 16s 165ms/step - loss: 0.3328 - accuracy: 0.8969 - val_loss: 0.6547 - val_accuracy: 0.8133\nEpoch 17/20\n98/98 [==============================] - 15s 156ms/step - loss: 0.3182 - accuracy: 0.9011 - val_loss: 0.6322 - val_accuracy: 0.8202\nEpoch 18/20\n98/98 [==============================] - 16s 167ms/step - loss: 0.3007 - accuracy: 0.9069 - val_loss: 0.6929 - val_accuracy: 0.8037\nEpoch 19/20\n98/98 [==============================] - 16s 168ms/step - loss: 0.2869 - accuracy: 0.9103 - val_loss: 0.6446 - val_accuracy: 0.8220\nEpoch 20/20\n98/98 [==============================] - 17s 173ms/step - loss: 0.2703 - accuracy: 0.9158 - val_loss: 0.6439 - val_accuracy: 0.8189\n\n\n&lt;tensorflow.python.keras.callbacks.History at 0x7fee205ff490&gt;\n\n\n여기서는 하이퍼파라미터 탐색을 많이 수행하지 않았습니다. 자유롭게 이 모델을 사용해 하이퍼파라미터를 탐색하고 최적화해 보세요. 예를 들어 LSTM 층을 제거하고 Conv1D 층으로 바꿀 수 있습니다. 층의 개수, 학습률, 옵티마이저 등을 실험해 볼 수 있습니다.\n검증 세트에 대한 모델의 성능이 만족스럽다면 모델을 저장하고 테스트 세트에서 마지막으로 평가합니다:\n\nmodel.save(\"my_bach_model.h5\")\nmodel.evaluate(test_set)\n\n     34/Unknown - 2s 66ms/step - loss: 0.6557 - accuracy: 0.8164\n\n\n[0.6556663916391485, 0.8164004]\n\n\n노트: 이 예제에서는 테스트 세트가 진짜로 필요하지 않습니다. 모델이 생성한 음악을 듣는 것이 최종 평가가 되기 때문입니다. 따라서 필요하다면 테스트 세트를 훈련 세트에 넣고 모델을 다시 훈련하여 조금 더 나은 모델을 얻을 수 있습니다.\n이제 새로운 코럴을 생성하는 함수를 만들어 보죠. 몇 개의 시드 화음을 주고 이를 (모델이 기대하는 포맷인) 아르페지오로 변환합니다. 그다음 모델을 사용해 다음 음표을 예측합니다. 마지막에 4개씩 음표를 모아서 다시 화음을 만들고 최종 코럴을 반환합니다.\n경고: model.predict_classes(X)는 deprecated 되었습니다. 대신 np.argmax(model.predict(X), axis=-1)을 사용하세요.\n\ndef generate_chorale(model, seed_chords, length):\n    arpegio = preprocess(tf.constant(seed_chords, dtype=tf.int64))\n    arpegio = tf.reshape(arpegio, [1, -1])\n    for chord in range(length):\n        for note in range(4):\n            #next_note = model.predict_classes(arpegio)[:1, -1:]\n            next_note = np.argmax(model.predict(arpegio), axis=-1)[:1, -1:]\n            arpegio = tf.concat([arpegio, next_note], axis=1)\n    arpegio = tf.where(arpegio == 0, arpegio, arpegio + min_note - 1)\n    return tf.reshape(arpegio, shape=[-1, 4])\n\n이 함수를 테스트하려면 시드 화음이 필요합니다. 테스트 코럴 중 하나에 있는 처음 8개의 화음을 사용해 보죠(실제로 이는 4번 반복되는 2개의 화음입니다):\n\nseed_chords = test_chorales[2][:8]\nplay_chords(seed_chords, amplitude=0.2)\n\n첫 번째 코럴을 생성할 준비를 마쳤습니다! 56개의 화음을 생성하여 총 64개의 화음, 즉 4 소절(소절마다 4개의 화음인 4/4박으로 가정합니다)을 만들어 보겠습니다:\n\nnew_chorale = generate_chorale(model, seed_chords, 56)\nplay_chords(new_chorale)\n\n이 방식에는 한가지 단점이 있습니다: 너무 보수적인 경우가 많습니다. 실제로 이 모델은 모험을 하지 않아 항상 가장 높은 확률의 음표를 선택합니다. 이전 음표를 반복하면 충분히 듣기 좋고 가장 덜 위험하기 때문에 이 알고리즘은 마지막 음표를 오래 지속시키는 경향이 있습니다. 상당히 지루합니다. 또한 이 모델을 여러 번 실행하면 항상 같은 멜로디를 생성할 것입니다.\n조금 더 신나게 만들어 보죠! 항상 가장 높은 점수의 음표를 선택하는 대신, 예측된 확률을 기반으로 랜덤하게 다음 음표를 선택하겠습니다. 예를 들어, 모델이 75% 확률로 C3를 예측하고 25% 확률로 G3를 예측했다면 이 확률대로 랜덤하게 두 음표 중 하나를 선택하겠습니다. 또한 temperature 매개변수를 추가하여 시스템의 온도(즉 대담성)를 제어하겠습니다. 높은 온도는 예측 확률을 비슷하게 만들어 가능성이 높은 음표의 확률을 줄이고 가능성이 낮은 음표의 확률을 높입니다.\n\ndef generate_chorale_v2(model, seed_chords, length, temperature=1):\n    arpegio = preprocess(tf.constant(seed_chords, dtype=tf.int64))\n    arpegio = tf.reshape(arpegio, [1, -1])\n    for chord in range(length):\n        for note in range(4):\n            next_note_probas = model.predict(arpegio)[0, -1:]\n            rescaled_logits = tf.math.log(next_note_probas) / temperature\n            next_note = tf.random.categorical(rescaled_logits, num_samples=1)\n            arpegio = tf.concat([arpegio, next_note], axis=1)\n    arpegio = tf.where(arpegio == 0, arpegio, arpegio + min_note - 1)\n    return tf.reshape(arpegio, shape=[-1, 4])\n\n이 함수로 3개의 코랄을 생성해 보겠습니다. 하나는 차갑게, 하나는 중간으로, 하나는 뜨겁게 만듭니다(시드, 길이, 온도를 사용해 자유롭게 실험해 보세요). 다음 코드는 각 코랄을 별개의 파일에 저장합니다. 마음에 드는 음악을 만날 때까지 이 셀을 반복해서 실행할 수 있습니다!\n가장 아름다운 코럴을 트위터 @aureliengeron로 공유해 주시면 정말 감사하겠습니다! :))\n\nnew_chorale_v2_cold = generate_chorale_v2(model, seed_chords, 56, temperature=0.8)\nplay_chords(new_chorale_v2_cold, filepath=\"bach_cold.wav\")\n\n\nnew_chorale_v2_medium = generate_chorale_v2(model, seed_chords, 56, temperature=1.0)\nplay_chords(new_chorale_v2_medium, filepath=\"bach_medium.wav\")\n\n\nnew_chorale_v2_hot = generate_chorale_v2(model, seed_chords, 56, temperature=1.5)\nplay_chords(new_chorale_v2_hot, filepath=\"bach_hot.wav\")\n\n마지막으로 재미있는 실험을 해 볼 수 있습니다: 친구에게 마음에 드는 코럴 몇 개와 진짜 코럴을 보내고 어떤 것이 진짜인지 물어 보세요!\n\nplay_chords(test_chorales[2][:64], filepath=\"bach_test_4.wav\")"
  },
  {
    "objectID": "Machine_Learning/13_loading_and_preprocessing_data.html",
    "href": "Machine_Learning/13_loading_and_preprocessing_data.html",
    "title": "13_loading_and_preprocessing_data",
    "section": "",
    "text": "13장 – 텐서플로에서 데이터 적재와 전처리하기\n이 노트북은 13장에 있는 모든 샘플 코드와 연습문제 해답을 가지고 있습니다."
  },
  {
    "objectID": "Machine_Learning/13_loading_and_preprocessing_data.html#데이터셋",
    "href": "Machine_Learning/13_loading_and_preprocessing_data.html#데이터셋",
    "title": "13_loading_and_preprocessing_data",
    "section": "데이터셋",
    "text": "데이터셋\n\nX = tf.range(10)\ndataset = tf.data.Dataset.from_tensor_slices(X)\ndataset\n\n&lt;TensorSliceDataset shapes: (), types: tf.int32&gt;\n\n\n다음과 동일합니다:\n\ndataset = tf.data.Dataset.range(10)\n\n\nfor item in dataset:\n    print(item)\n\ntf.Tensor(0, shape=(), dtype=int64)\ntf.Tensor(1, shape=(), dtype=int64)\ntf.Tensor(2, shape=(), dtype=int64)\ntf.Tensor(3, shape=(), dtype=int64)\ntf.Tensor(4, shape=(), dtype=int64)\ntf.Tensor(5, shape=(), dtype=int64)\ntf.Tensor(6, shape=(), dtype=int64)\ntf.Tensor(7, shape=(), dtype=int64)\ntf.Tensor(8, shape=(), dtype=int64)\ntf.Tensor(9, shape=(), dtype=int64)\n\n\n\ndataset = dataset.repeat(3).batch(7)\nfor item in dataset:\n    print(item)\n\ntf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int64)\ntf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int64)\ntf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int64)\ntf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int64)\ntf.Tensor([8 9], shape=(2,), dtype=int64)\n\n\n\ndataset = dataset.map(lambda x: x * 2)\n\n\nfor item in dataset:\n    print(item)\n\ntf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int64)\ntf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int64)\ntf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int64)\ntf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int64)\ntf.Tensor([16 18], shape=(2,), dtype=int64)\n\n\n\n#dataset = dataset.apply(tf.data.experimental.unbatch()) # Now deprecated\ndataset = dataset.unbatch()\n\n\ndataset = dataset.filter(lambda x: x &lt; 10)  # keep only items &lt; 10\n\n\nfor item in dataset.take(3):\n    print(item)\n\ntf.Tensor(0, shape=(), dtype=int64)\ntf.Tensor(2, shape=(), dtype=int64)\ntf.Tensor(4, shape=(), dtype=int64)\n\n\n\ntf.random.set_seed(42)\n\ndataset = tf.data.Dataset.range(10).repeat(3)\ndataset = dataset.shuffle(buffer_size=3, seed=42).batch(7)\nfor item in dataset:\n    print(item)\n\ntf.Tensor([1 3 0 4 2 5 6], shape=(7,), dtype=int64)\ntf.Tensor([8 7 1 0 3 2 5], shape=(7,), dtype=int64)\ntf.Tensor([4 6 9 8 9 7 0], shape=(7,), dtype=int64)\ntf.Tensor([3 1 4 5 2 8 7], shape=(7,), dtype=int64)\ntf.Tensor([6 9], shape=(2,), dtype=int64)"
  },
  {
    "objectID": "Machine_Learning/13_loading_and_preprocessing_data.html#캘리포니아-주택-데이터셋을-여러-개의-csv로-나누기",
    "href": "Machine_Learning/13_loading_and_preprocessing_data.html#캘리포니아-주택-데이터셋을-여러-개의-csv로-나누기",
    "title": "13_loading_and_preprocessing_data",
    "section": "캘리포니아 주택 데이터셋을 여러 개의 CSV로 나누기",
    "text": "캘리포니아 주택 데이터셋을 여러 개의 CSV로 나누기\n캘리포니아 주택 데이터셋을 로드하고 준비해 보죠. 먼저 로드한 다음 훈련 세트, 검증 세트, 테스트 세트로 나눕니다. 마지막으로 스케일을 조정합니다:\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nhousing = fetch_california_housing()\nX_train_full, X_test, y_train_full, y_test = train_test_split(\n    housing.data, housing.target.reshape(-1, 1), random_state=42)\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X_train_full, y_train_full, random_state=42)\n\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_mean = scaler.mean_\nX_std = scaler.scale_\n\n메모리에 맞지 않는 매우 큰 데이터셋인 경우 일반적으로 먼저 여러 개의 파일로 나누고 텐서플로에서 이 파일들을 병렬로 읽게합니다. 데모를 위해 주택 데이터셋을 20개의 CSV 파일로 나누어 보죠:\n\ndef save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n    housing_dir = os.path.join(\"datasets\", \"housing\")\n    os.makedirs(housing_dir, exist_ok=True)\n    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n\n    filepaths = []\n    m = len(data)\n    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n        part_csv = path_format.format(name_prefix, file_idx)\n        filepaths.append(part_csv)\n        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n            if header is not None:\n                f.write(header)\n                f.write(\"\\n\")\n            for row_idx in row_indices:\n                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n                f.write(\"\\n\")\n    return filepaths\n\n\ntrain_data = np.c_[X_train, y_train]\nvalid_data = np.c_[X_valid, y_valid]\ntest_data = np.c_[X_test, y_test]\nheader_cols = housing.feature_names + [\"MedianHouseValue\"]\nheader = \",\".join(header_cols)\n\ntrain_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\nvalid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\ntest_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)\n\n좋습니다. 이 CSV 파일 중 하나에서 몇 줄을 출력해 보죠:\n\nimport pandas as pd\n\npd.read_csv(train_filepaths[0]).head()\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\nMedianHouseValue\n\n\n\n\n0\n3.5214\n15.0\n3.049945\n1.106548\n1447.0\n1.605993\n37.63\n-122.43\n1.442\n\n\n1\n5.3275\n5.0\n6.490060\n0.991054\n3464.0\n3.443340\n33.69\n-117.39\n1.687\n\n\n2\n3.1000\n29.0\n7.542373\n1.591525\n1328.0\n2.250847\n38.44\n-122.98\n1.621\n\n\n3\n7.1736\n12.0\n6.289003\n0.997442\n1054.0\n2.695652\n33.55\n-117.70\n2.621\n\n\n4\n2.0549\n13.0\n5.312457\n1.085092\n3297.0\n2.244384\n33.93\n-116.93\n0.956\n\n\n\n\n\n\n\n텍스트 파일로 읽으면 다음과 같습니다:\n\nwith open(train_filepaths[0]) as f:\n    for i in range(5):\n        print(f.readline(), end=\"\")\n\nMedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442\n5.3275,5.0,6.490059642147117,0.9910536779324056,3464.0,3.4433399602385686,33.69,-117.39,1.687\n3.1,29.0,7.5423728813559325,1.5915254237288134,1328.0,2.2508474576271187,38.44,-122.98,1.621\n7.1736,12.0,6.289002557544757,0.9974424552429667,1054.0,2.6956521739130435,33.55,-117.7,2.621\n\n\n\ntrain_filepaths\n\n['datasets/housing/my_train_00.csv',\n 'datasets/housing/my_train_01.csv',\n 'datasets/housing/my_train_02.csv',\n 'datasets/housing/my_train_03.csv',\n 'datasets/housing/my_train_04.csv',\n 'datasets/housing/my_train_05.csv',\n 'datasets/housing/my_train_06.csv',\n 'datasets/housing/my_train_07.csv',\n 'datasets/housing/my_train_08.csv',\n 'datasets/housing/my_train_09.csv',\n 'datasets/housing/my_train_10.csv',\n 'datasets/housing/my_train_11.csv',\n 'datasets/housing/my_train_12.csv',\n 'datasets/housing/my_train_13.csv',\n 'datasets/housing/my_train_14.csv',\n 'datasets/housing/my_train_15.csv',\n 'datasets/housing/my_train_16.csv',\n 'datasets/housing/my_train_17.csv',\n 'datasets/housing/my_train_18.csv',\n 'datasets/housing/my_train_19.csv']"
  },
  {
    "objectID": "Machine_Learning/13_loading_and_preprocessing_data.html#입력-파이프라인-만들기",
    "href": "Machine_Learning/13_loading_and_preprocessing_data.html#입력-파이프라인-만들기",
    "title": "13_loading_and_preprocessing_data",
    "section": "입력 파이프라인 만들기",
    "text": "입력 파이프라인 만들기\n\nfilepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)\n\n\nfor filepath in filepath_dataset:\n    print(filepath)\n\ntf.Tensor(b'datasets/housing/my_train_15.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets/housing/my_train_08.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets/housing/my_train_03.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets/housing/my_train_01.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets/housing/my_train_10.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets/housing/my_train_05.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets/housing/my_train_19.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets/housing/my_train_16.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets/housing/my_train_02.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets/housing/my_train_09.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets/housing/my_train_00.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets/housing/my_train_07.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets/housing/my_train_12.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets/housing/my_train_04.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets/housing/my_train_17.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets/housing/my_train_11.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets/housing/my_train_14.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets/housing/my_train_18.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets/housing/my_train_06.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets/housing/my_train_13.csv', shape=(), dtype=string)\n\n\n\nn_readers = 5\ndataset = filepath_dataset.interleave(\n    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n    cycle_length=n_readers)\n\n\nfor line in dataset.take(5):\n    print(line.numpy())\n\nb'4.6477,38.0,5.03728813559322,0.911864406779661,745.0,2.5254237288135593,32.64,-117.07,1.504'\nb'8.72,44.0,6.163179916317992,1.0460251046025104,668.0,2.794979079497908,34.2,-118.18,4.159'\nb'3.8456,35.0,5.461346633416459,0.9576059850374065,1154.0,2.8778054862842892,37.96,-122.05,1.598'\nb'3.3456,37.0,4.514084507042254,0.9084507042253521,458.0,3.2253521126760565,36.67,-121.7,2.526'\nb'3.6875,44.0,4.524475524475524,0.993006993006993,457.0,3.195804195804196,34.04,-118.15,1.625'\n\n\n네 번째 필드의 4는 문자열로 해석됩니다.\n\nrecord_defaults=[0, np.nan, tf.constant(np.nan, dtype=tf.float64), \"Hello\", tf.constant([])]\nparsed_fields = tf.io.decode_csv('1,2,3,4,5', record_defaults)\nparsed_fields\n\n[&lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt;,\n &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.0&gt;,\n &lt;tf.Tensor: shape=(), dtype=float64, numpy=3.0&gt;,\n &lt;tf.Tensor: shape=(), dtype=string, numpy=b'4'&gt;,\n &lt;tf.Tensor: shape=(), dtype=float32, numpy=5.0&gt;]\n\n\n누락된 값은 제공된 기본값으로 대체됩니다:\n\nparsed_fields = tf.io.decode_csv(',,,,5', record_defaults)\nparsed_fields\n\n[&lt;tf.Tensor: shape=(), dtype=int32, numpy=0&gt;,\n &lt;tf.Tensor: shape=(), dtype=float32, numpy=nan&gt;,\n &lt;tf.Tensor: shape=(), dtype=float64, numpy=nan&gt;,\n &lt;tf.Tensor: shape=(), dtype=string, numpy=b'Hello'&gt;,\n &lt;tf.Tensor: shape=(), dtype=float32, numpy=5.0&gt;]\n\n\n다섯 번째 필드는 필수입니니다(기본값을 tf.constant([])로 지정했기 때문에). 따라서 값을 전달하지 않으면 예외가 발생합니다:\n\ntry:\n    parsed_fields = tf.io.decode_csv(',,,,', record_defaults)\nexcept tf.errors.InvalidArgumentError as ex:\n    print(ex)\n\nField 4 is required but missing in record 0! [Op:DecodeCSV]\n\n\n필드 개수는 record_defaults에 있는 필드 개수와 정확히 맞아야 합니다:\n\ntry:\n    parsed_fields = tf.io.decode_csv('1,2,3,4,5,6,7', record_defaults)\nexcept tf.errors.InvalidArgumentError as ex:\n    print(ex)\n\nExpect 5 fields but have 7 in record 0 [Op:DecodeCSV]\n\n\n\nn_inputs = 8 # X_train.shape[-1]\n\n@tf.function\ndef preprocess(line):\n    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n    fields = tf.io.decode_csv(line, record_defaults=defs)\n    x = tf.stack(fields[:-1])\n    y = tf.stack(fields[-1:])\n    return (x - X_mean) / X_std, y\n\n\npreprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')\n\n(&lt;tf.Tensor: shape=(8,), dtype=float32, numpy=\n array([ 0.16579157,  1.216324  , -0.05204565, -0.39215982, -0.5277444 ,\n        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)&gt;,\n &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)&gt;)\n\n\n\ndef csv_reader_dataset(filepaths, repeat=1, n_readers=5,\n                       n_read_threads=None, shuffle_buffer_size=10000,\n                       n_parse_threads=5, batch_size=32):\n    dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)\n    dataset = dataset.interleave(\n        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n    dataset = dataset.shuffle(shuffle_buffer_size)\n    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n    dataset = dataset.batch(batch_size)\n    return dataset.prefetch(1)\n\n\ntf.random.set_seed(42)\n\ntrain_set = csv_reader_dataset(train_filepaths, batch_size=3)\nfor X_batch, y_batch in train_set.take(2):\n    print(\"X =\", X_batch)\n    print(\"y =\", y_batch)\n    print()\n\nX = tf.Tensor(\n[[ 0.5804519  -0.20762321  0.05616303 -0.15191229  0.01343246  0.00604472\n   1.2525111  -1.3671792 ]\n [ 5.818099    1.8491895   1.1784915   0.28173092 -1.2496178  -0.3571987\n   0.7231292  -1.0023477 ]\n [-0.9253566   0.5834586  -0.7807257  -0.28213993 -0.36530012  0.27389365\n  -0.76194876  0.72684526]], shape=(3, 8), dtype=float32)\ny = tf.Tensor(\n[[1.752]\n [1.313]\n [1.535]], shape=(3, 1), dtype=float32)\n\nX = tf.Tensor(\n[[-0.8324941   0.6625668  -0.20741376 -0.18699841 -0.14536144  0.09635526\n   0.9807942  -0.67250353]\n [-0.62183803  0.5834586  -0.19862501 -0.3500319  -1.1437552  -0.3363751\n   1.107282   -0.8674123 ]\n [ 0.8683102   0.02970133  0.3427381  -0.29872298  0.7124906   0.28026953\n  -0.72915536  0.86178064]], shape=(3, 8), dtype=float32)\ny = tf.Tensor(\n[[0.919]\n [1.028]\n [2.182]], shape=(3, 1), dtype=float32)\n\n\n\n\ntrain_set = csv_reader_dataset(train_filepaths, repeat=None)\nvalid_set = csv_reader_dataset(valid_filepaths)\ntest_set = csv_reader_dataset(test_filepaths)\n\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nmodel = keras.models.Sequential([\n    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n    keras.layers.Dense(1),\n])\n\n\nmodel.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n\n\n\nbatch_size = 32\nmodel.fit(train_set, steps_per_epoch=len(X_train) // batch_size, epochs=10,\n          validation_data=valid_set)\n\nEpoch 1/10\n362/362 [==============================] - 2s 3ms/step - loss: 1.4679 - val_loss: 21.5124\nEpoch 2/10\n362/362 [==============================] - 1s 3ms/step - loss: 0.8735 - val_loss: 0.6648\nEpoch 3/10\n362/362 [==============================] - 1s 3ms/step - loss: 0.6317 - val_loss: 0.6196\nEpoch 4/10\n362/362 [==============================] - 1s 3ms/step - loss: 0.5933 - val_loss: 0.5669\nEpoch 5/10\n362/362 [==============================] - 1s 3ms/step - loss: 0.5629 - val_loss: 0.5402\nEpoch 6/10\n362/362 [==============================] - 1s 3ms/step - loss: 0.5693 - val_loss: 0.5209\nEpoch 7/10\n362/362 [==============================] - 1s 3ms/step - loss: 0.5231 - val_loss: 0.6130\nEpoch 8/10\n362/362 [==============================] - 1s 3ms/step - loss: 0.5074 - val_loss: 0.4818\nEpoch 9/10\n362/362 [==============================] - 1s 3ms/step - loss: 0.4963 - val_loss: 0.4904\nEpoch 10/10\n362/362 [==============================] - 1s 3ms/step - loss: 0.5023 - val_loss: 0.4585\n\n\n&lt;tensorflow.python.keras.callbacks.History at 0x7fb90e831550&gt;\n\n\n\nmodel.evaluate(test_set, steps=len(X_test) // batch_size)\n\n161/161 [==============================] - 0s 2ms/step - loss: 0.4788\n\n\n0.4787752032279968\n\n\n\nnew_set = test_set.map(lambda X, y: X) # we could instead just pass test_set, Keras would ignore the labels\nX_new = X_test\nmodel.predict(new_set, steps=len(X_new) // batch_size)\n\narray([[2.3576405],\n       [2.255291 ],\n       [1.4437605],\n       ...,\n       [0.5654392],\n       [3.9442453],\n       [1.0232248]], dtype=float32)\n\n\n\noptimizer = keras.optimizers.Nadam(learning_rate=0.01)\nloss_fn = keras.losses.mean_squared_error\n\nn_epochs = 5\nbatch_size = 32\nn_steps_per_epoch = len(X_train) // batch_size\ntotal_steps = n_epochs * n_steps_per_epoch\nglobal_step = 0\nfor X_batch, y_batch in train_set.take(total_steps):\n    global_step += 1\n    print(\"\\rGlobal step {}/{}\".format(global_step, total_steps), end=\"\")\n    with tf.GradientTape() as tape:\n        y_pred = model(X_batch)\n        main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n        loss = tf.add_n([main_loss] + model.losses)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n\n\nGlobal step 1810/1810\n\n\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n\noptimizer = keras.optimizers.Nadam(learning_rate=0.01)\nloss_fn = keras.losses.mean_squared_error\n\n@tf.function\ndef train(model, n_epochs, batch_size=32,\n          n_readers=5, n_read_threads=5, shuffle_buffer_size=10000, n_parse_threads=5):\n    train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, n_readers=n_readers,\n                       n_read_threads=n_read_threads, shuffle_buffer_size=shuffle_buffer_size,\n                       n_parse_threads=n_parse_threads, batch_size=batch_size)\n    for X_batch, y_batch in train_set:\n        with tf.GradientTape() as tape:\n            y_pred = model(X_batch)\n            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n            loss = tf.add_n([main_loss] + model.losses)\n        gradients = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\ntrain(model, 5)\n\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n\n\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n\noptimizer = keras.optimizers.Nadam(learning_rate=0.01)\nloss_fn = keras.losses.mean_squared_error\n\n@tf.function\ndef train(model, n_epochs, batch_size=32,\n          n_readers=5, n_read_threads=5, shuffle_buffer_size=10000, n_parse_threads=5):\n    train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, n_readers=n_readers,\n                       n_read_threads=n_read_threads, shuffle_buffer_size=shuffle_buffer_size,\n                       n_parse_threads=n_parse_threads, batch_size=batch_size)\n    n_steps_per_epoch = len(X_train) // batch_size\n    total_steps = n_epochs * n_steps_per_epoch\n    global_step = 0\n    for X_batch, y_batch in train_set.take(total_steps):\n        global_step += 1\n        if tf.equal(global_step % 100, 0):\n            tf.print(\"\\rGlobal step\", global_step, \"/\", total_steps)\n        with tf.GradientTape() as tape:\n            y_pred = model(X_batch)\n            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n            loss = tf.add_n([main_loss] + model.losses)\n        gradients = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\ntrain(model, 5)\n\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n\n\nGlobal step 100 / 1810\nGlobal step 200 / 1810\nGlobal step 300 / 1810\nGlobal step 400 / 1810\nGlobal step 500 / 1810\nGlobal step 600 / 1810\nGlobal step 700 / 1810\nGlobal step 800 / 1810\nGlobal step 900 / 1810\nGlobal step 1000 / 1810\nGlobal step 1100 / 1810\nGlobal step 1200 / 1810\nGlobal step 1300 / 1810\nGlobal step 1400 / 1810\nGlobal step 1500 / 1810\nGlobal step 1600 / 1810\nGlobal step 1700 / 1810\nGlobal step 1800 / 1810\n\n\nDataset 클래스에 있는 메서드의 간략한 설명입니다:\n\nfor m in dir(tf.data.Dataset):\n    if not (m.startswith(\"_\") or m.endswith(\"_\")):\n        func = getattr(tf.data.Dataset, m)\n        if hasattr(func, \"__doc__\"):\n            print(\"● {:21s}{}\".format(m + \"()\", func.__doc__.split(\"\\n\")[0]))\n\n● apply()              Applies a transformation function to this dataset.\n● as_numpy_iterator()  Returns an iterator which converts all elements of the dataset to numpy.\n● batch()              Combines consecutive elements of this dataset into batches.\n● cache()              Caches the elements in this dataset.\n● cardinality()        Returns the cardinality of the dataset, if known.\n● concatenate()        Creates a `Dataset` by concatenating the given dataset with this dataset.\n● element_spec()       The type specification of an element of this dataset.\n● enumerate()          Enumerates the elements of this dataset.\n● filter()             Filters this dataset according to `predicate`.\n● flat_map()           Maps `map_func` across this dataset and flattens the result.\n● from_generator()     Creates a `Dataset` whose elements are generated by `generator`. (deprecated arguments)\n● from_tensor_slices() Creates a `Dataset` whose elements are slices of the given tensors.\n● from_tensors()       Creates a `Dataset` with a single element, comprising the given tensors.\n● interleave()         Maps `map_func` across this dataset, and interleaves the results.\n● list_files()         A dataset of all files matching one or more glob patterns.\n● map()                Maps `map_func` across the elements of this dataset.\n● options()            Returns the options for this dataset and its inputs.\n● padded_batch()       Combines consecutive elements of this dataset into padded batches.\n● prefetch()           Creates a `Dataset` that prefetches elements from this dataset.\n● range()              Creates a `Dataset` of a step-separated range of values.\n● reduce()             Reduces the input dataset to a single element.\n● repeat()             Repeats this dataset so each original value is seen `count` times.\n● shard()              Creates a `Dataset` that includes only 1/`num_shards` of this dataset.\n● shuffle()            Randomly shuffles the elements of this dataset.\n● skip()               Creates a `Dataset` that skips `count` elements from this dataset.\n● take()               Creates a `Dataset` with at most `count` elements from this dataset.\n● unbatch()            Splits elements of a dataset into multiple elements.\n● window()             Combines (nests of) input elements into a dataset of (nests of) windows.\n● with_options()       Returns a new `tf.data.Dataset` with the given options set.\n● zip()                Creates a `Dataset` by zipping together the given datasets."
  },
  {
    "objectID": "Machine_Learning/13_loading_and_preprocessing_data.html#tfrecord-이진-포맷",
    "href": "Machine_Learning/13_loading_and_preprocessing_data.html#tfrecord-이진-포맷",
    "title": "13_loading_and_preprocessing_data",
    "section": "TFRecord 이진 포맷",
    "text": "TFRecord 이진 포맷\nTFRecord 파일은 단순히 이진 레코드의 리스트입니다. tf.io.TFRecordWriter를 사용해 만들 수 있습니다:\n\nwith tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n    f.write(b\"This is the first record\")\n    f.write(b\"And this is the second record\")\n\n그리고 tf.data.TFRecordDataset 사용해 읽을 수 있습니다.:\n\nfilepaths = [\"my_data.tfrecord\"]\ndataset = tf.data.TFRecordDataset(filepaths)\nfor item in dataset:\n    print(item)\n\ntf.Tensor(b'This is the first record', shape=(), dtype=string)\ntf.Tensor(b'And this is the second record', shape=(), dtype=string)\n\n\n하나의 TFRecordDataset로 여러 개의 TFRecord 파일을 읽을 수 있습니다. 기본적으로 한 번에 하나의 파일만 읽지만 num_parallel_reads=3와 같이 지정하면 동시에 3개를 읽고 레코드를 번갈아 반환합니다:\n\nfilepaths = [\"my_test_{}.tfrecord\".format(i) for i in range(5)]\nfor i, filepath in enumerate(filepaths):\n    with tf.io.TFRecordWriter(filepath) as f:\n        for j in range(3):\n            f.write(\"File {} record {}\".format(i, j).encode(\"utf-8\"))\n\ndataset = tf.data.TFRecordDataset(filepaths, num_parallel_reads=3)\nfor item in dataset:\n    print(item)\n\ntf.Tensor(b'File 0 record 0', shape=(), dtype=string)\ntf.Tensor(b'File 1 record 0', shape=(), dtype=string)\ntf.Tensor(b'File 2 record 0', shape=(), dtype=string)\ntf.Tensor(b'File 0 record 1', shape=(), dtype=string)\ntf.Tensor(b'File 1 record 1', shape=(), dtype=string)\ntf.Tensor(b'File 2 record 1', shape=(), dtype=string)\ntf.Tensor(b'File 0 record 2', shape=(), dtype=string)\ntf.Tensor(b'File 1 record 2', shape=(), dtype=string)\ntf.Tensor(b'File 2 record 2', shape=(), dtype=string)\ntf.Tensor(b'File 3 record 0', shape=(), dtype=string)\ntf.Tensor(b'File 4 record 0', shape=(), dtype=string)\ntf.Tensor(b'File 3 record 1', shape=(), dtype=string)\ntf.Tensor(b'File 4 record 1', shape=(), dtype=string)\ntf.Tensor(b'File 3 record 2', shape=(), dtype=string)\ntf.Tensor(b'File 4 record 2', shape=(), dtype=string)\n\n\n\noptions = tf.io.TFRecordOptions(compression_type=\"GZIP\")\nwith tf.io.TFRecordWriter(\"my_compressed.tfrecord\", options) as f:\n    f.write(b\"This is the first record\")\n    f.write(b\"And this is the second record\")\n\n\ndataset = tf.data.TFRecordDataset([\"my_compressed.tfrecord\"],\n                                  compression_type=\"GZIP\")\nfor item in dataset:\n    print(item)\n\ntf.Tensor(b'This is the first record', shape=(), dtype=string)\ntf.Tensor(b'And this is the second record', shape=(), dtype=string)\n\n\n\n프로토콜 버퍼 개요\n이 절을 위해서는 프로토콜 버퍼를 설치해야 합니다. 일반적으로 텐서플로를 사용할 때 프로토콜 버퍼를 설치할 필요는 없습니다. 텐서플로는 tf.train.Example 타입의 프로토콜 버퍼를 만들고 파싱할 수 있는 함수를 제공하며 보통의 경우 충분합니다. 하지만 이 절에서는 자체적인 프로토콜 버퍼를 간단히 만들어 보겠습니다. 따라서 프로토콜 버퍼 컴파일러(protoc)가 필요합니다. 이를 사용해 프로토콜 버퍼 정의를 컴파일하여 코드에서 사용할 수 있는 파이썬 모듈을 만들겠습니다.\n먼저 간단한 프로토콜 버퍼 정의를 작성해 보죠:\n\n%%writefile person.proto\nsyntax = \"proto3\";\nmessage Person {\n  string name = 1;\n  int32 id = 2;\n  repeated string email = 3;\n}\n\nOverwriting person.proto\n\n\n이 정의를 컴파일합니다(--descriptor_set_out와 --include_imports 옵션은 아래 tf.io.decode_proto() 예제를 위해서 필요합니다):\n\n!protoc person.proto --python_out=. --descriptor_set_out=person.desc --include_imports\n\n\n!ls person*\n\nperson.desc  person_pb2.py  person.proto\n\n\n\nfrom person_pb2 import Person\n\nperson = Person(name=\"Al\", id=123, email=[\"a@b.com\"])  # Person 생성\nprint(person)  # Person 출력\n\nname: \"Al\"\nid: 123\nemail: \"a@b.com\"\n\n\n\n\nperson.name  # 필드 읽기\n\n'Al'\n\n\n\nperson.name = \"Alice\"  # 필드 수정\n\n\nperson.email[0]  # 배열처럼 사용할 수 있는 반복 필드\n\n'a@b.com'\n\n\n\nperson.email.append(\"c@d.com\")  # 이메일 추가\n\n\ns = person.SerializeToString()  # 바이트 문자열로 직렬화\ns\n\nb'\\n\\x05Alice\\x10{\\x1a\\x07a@b.com\\x1a\\x07c@d.com'\n\n\n\nperson2 = Person()  # 새로운 Person 생성\nperson2.ParseFromString(s)  # 바이트 문자열 파싱 (27 바이트)\n\n27\n\n\n\nperson == person2  # 동일\n\nTrue\n\n\n\n사용자 정의 protobuf\n드문 경우에 텐서플로에서 (앞서 우리가 만든 것처럼) 사용자 정의 프로토콜 버퍼를 파싱해야 합니다. 이를 위해 tf.io.decode_proto() 함수를 사용할 수 있습니다:\n\nperson_tf = tf.io.decode_proto(\n    bytes=s,\n    message_type=\"Person\",\n    field_names=[\"name\", \"id\", \"email\"],\n    output_types=[tf.string, tf.int32, tf.string],\n    descriptor_source=\"person.desc\")\n\nperson_tf.values\n\n[&lt;tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Alice'], dtype=object)&gt;,\n &lt;tf.Tensor: shape=(1,), dtype=int32, numpy=array([123], dtype=int32)&gt;,\n &lt;tf.Tensor: shape=(2,), dtype=string, numpy=array([b'a@b.com', b'c@d.com'], dtype=object)&gt;]\n\n\n더 자세한 내용은 tf.io.decode_proto() 문서를 참고하세요.\n\n\n\n텐서플로 프로토콜 버퍼\n다음이 tf.train.Example 프로토콜 버퍼의 정의입니다.:\nsyntax = \"proto3\";\n\nmessage BytesList { repeated bytes value = 1; }\nmessage FloatList { repeated float value = 1 [packed = true]; }\nmessage Int64List { repeated int64 value = 1 [packed = true]; }\nmessage Feature {\n    oneof kind {\n        BytesList bytes_list = 1;\n        FloatList float_list = 2;\n        Int64List int64_list = 3;\n    }\n};\nmessage Features { map&lt;string, Feature&gt; feature = 1; };\nmessage Example { Features features = 1; };\n경고: 텐서플로 2.0과 2.1에서 from tensorflow.train import X와 같이 사용하지 못하는 버그가 있기 때문에 X = tf.train.X로 씁니다. 자세한 내용은 https://github.com/tensorflow/tensorflow/issues/33289을 참고하세요.\n\nfrom tensorflow.train import BytesList, FloatList, Int64List\nfrom tensorflow.train import Feature, Features, Example\n\nperson_example = Example(\n    features=Features(\n        feature={\n            \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])),\n            \"id\": Feature(int64_list=Int64List(value=[123])),\n            \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\", b\"c@d.com\"]))\n        }))\n\nwith tf.io.TFRecordWriter(\"my_contacts.tfrecord\") as f:\n    f.write(person_example.SerializeToString())\n\n\nfeature_description = {\n    \"name\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n    \"id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n    \"emails\": tf.io.VarLenFeature(tf.string),\n}\nfor serialized_example in tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]):\n    parsed_example = tf.io.parse_single_example(serialized_example,\n                                                feature_description)\n\n\nparsed_example\n\n{'emails': &lt;tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7fb9109be650&gt;,\n 'id': &lt;tf.Tensor: shape=(), dtype=int64, numpy=123&gt;,\n 'name': &lt;tf.Tensor: shape=(), dtype=string, numpy=b'Alice'&gt;}\n\n\n\nparsed_example\n\n{'emails': &lt;tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7fb9109be650&gt;,\n 'id': &lt;tf.Tensor: shape=(), dtype=int64, numpy=123&gt;,\n 'name': &lt;tf.Tensor: shape=(), dtype=string, numpy=b'Alice'&gt;}\n\n\n\nparsed_example[\"emails\"].values[0]\n\n&lt;tf.Tensor: shape=(), dtype=string, numpy=b'a@b.com'&gt;\n\n\n\ntf.sparse.to_dense(parsed_example[\"emails\"], default_value=b\"\")\n\n&lt;tf.Tensor: shape=(2,), dtype=string, numpy=array([b'a@b.com', b'c@d.com'], dtype=object)&gt;\n\n\n\nparsed_example[\"emails\"].values\n\n&lt;tf.Tensor: shape=(2,), dtype=string, numpy=array([b'a@b.com', b'c@d.com'], dtype=object)&gt;\n\n\n\n\nTFRecord에 이미지 넣기\n\nfrom sklearn.datasets import load_sample_images\n\nimg = load_sample_images()[\"images\"][0]\nplt.imshow(img)\nplt.axis(\"off\")\nplt.title(\"Original Image\")\nplt.show()\n\n\n\n\n\ndata = tf.io.encode_jpeg(img)\nexample_with_image = Example(features=Features(feature={\n    \"image\": Feature(bytes_list=BytesList(value=[data.numpy()]))}))\nserialized_example = example_with_image.SerializeToString()\n# then save to TFRecord\n\n\nfeature_description = { \"image\": tf.io.VarLenFeature(tf.string) }\nexample_with_image = tf.io.parse_single_example(serialized_example, feature_description)\ndecoded_img = tf.io.decode_jpeg(example_with_image[\"image\"].values[0])\n\n또는 decode_image()를 사용합니다. 이 함수는 BMP, GIF, JPEG, PNG 포맷을 지원합니다:\n\ndecoded_img = tf.io.decode_image(example_with_image[\"image\"].values[0])\n\n\nplt.imshow(decoded_img)\nplt.title(\"Decoded Image\")\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\nTFRecord에 텐서와 희소 텐서 넣기\ntf.io.serialize_tensor()와 tf.io.parse_tensor()를 사용해 텐서를 쉽게 직렬화하고 파싱할 수 있습니다:\n\nt = tf.constant([[0., 1.], [2., 3.], [4., 5.]])\ns = tf.io.serialize_tensor(t)\ns\n\n&lt;tf.Tensor: shape=(), dtype=string, numpy=b'\\x08\\x01\\x12\\x08\\x12\\x02\\x08\\x03\\x12\\x02\\x08\\x02\"\\x18\\x00\\x00\\x00\\x00\\x00\\x00\\x80?\\x00\\x00\\x00@\\x00\\x00@@\\x00\\x00\\x80@\\x00\\x00\\xa0@'&gt;\n\n\n\ntf.io.parse_tensor(s, out_type=tf.float32)\n\n&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=\narray([[0., 1.],\n       [2., 3.],\n       [4., 5.]], dtype=float32)&gt;\n\n\n\nserialized_sparse = tf.io.serialize_sparse(parsed_example[\"emails\"])\nserialized_sparse\n\n&lt;tf.Tensor: shape=(3,), dtype=string, numpy=\narray([b'\\x08\\t\\x12\\x08\\x12\\x02\\x08\\x02\\x12\\x02\\x08\\x01\"\\x10\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00',\n       b'\\x08\\x07\\x12\\x04\\x12\\x02\\x08\\x02\"\\x10\\x07\\x07a@b.comc@d.com',\n       b'\\x08\\t\\x12\\x04\\x12\\x02\\x08\\x01\"\\x08\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00'],\n      dtype=object)&gt;\n\n\n\nBytesList(value=serialized_sparse.numpy())\n\nvalue: \"\\010\\t\\022\\010\\022\\002\\010\\002\\022\\002\\010\\001\\\"\\020\\000\\000\\000\\000\\000\\000\\000\\000\\001\\000\\000\\000\\000\\000\\000\\000\"\nvalue: \"\\010\\007\\022\\004\\022\\002\\010\\002\\\"\\020\\007\\007a@b.comc@d.com\"\nvalue: \"\\010\\t\\022\\004\\022\\002\\010\\001\\\"\\010\\002\\000\\000\\000\\000\\000\\000\\000\"\n\n\n\ndataset = tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]).batch(10)\nfor serialized_examples in dataset:\n    parsed_examples = tf.io.parse_example(serialized_examples,\n                                          feature_description)\n\n\nparsed_examples\n\n{'image': &lt;tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7fb90c85b2d0&gt;}"
  },
  {
    "objectID": "Machine_Learning/13_loading_and_preprocessing_data.html#sequenceexample를-사용해-순차-데이터-다루기",
    "href": "Machine_Learning/13_loading_and_preprocessing_data.html#sequenceexample를-사용해-순차-데이터-다루기",
    "title": "13_loading_and_preprocessing_data",
    "section": "SequenceExample를 사용해 순차 데이터 다루기",
    "text": "SequenceExample를 사용해 순차 데이터 다루기\nsyntax = \"proto3\";\n\nmessage FeatureList { repeated Feature feature = 1; };\nmessage FeatureLists { map&lt;string, FeatureList&gt; feature_list = 1; };\nmessage SequenceExample {\n  Features context = 1;\n  FeatureLists feature_lists = 2;\n};\n경고: 텐서플로 2.0과 2.1에서 from tensorflow.train import X와 같이 사용하지 못하는 버그가 있기 때문에 X = tf.train.X로 씁니다. 자세한 내용은 https://github.com/tensorflow/tensorflow/issues/33289을 참고하세요.\n\nfrom tensorflow.train import FeatureList, FeatureLists, SequenceExample\n\ncontext = Features(feature={\n    \"author_id\": Feature(int64_list=Int64List(value=[123])),\n    \"title\": Feature(bytes_list=BytesList(value=[b\"A\", b\"desert\", b\"place\", b\".\"])),\n    \"pub_date\": Feature(int64_list=Int64List(value=[1623, 12, 25]))\n})\n\ncontent = [[\"When\", \"shall\", \"we\", \"three\", \"meet\", \"again\", \"?\"],\n           [\"In\", \"thunder\", \",\", \"lightning\", \",\", \"or\", \"in\", \"rain\", \"?\"]]\ncomments = [[\"When\", \"the\", \"hurlyburly\", \"'s\", \"done\", \".\"],\n            [\"When\", \"the\", \"battle\", \"'s\", \"lost\", \"and\", \"won\", \".\"]]\n\ndef words_to_feature(words):\n    return Feature(bytes_list=BytesList(value=[word.encode(\"utf-8\")\n                                               for word in words]))\n\ncontent_features = [words_to_feature(sentence) for sentence in content]\ncomments_features = [words_to_feature(comment) for comment in comments]\n            \nsequence_example = SequenceExample(\n    context=context,\n    feature_lists=FeatureLists(feature_list={\n        \"content\": FeatureList(feature=content_features),\n        \"comments\": FeatureList(feature=comments_features)\n    }))\n\n\nsequence_example\n\ncontext {\n  feature {\n    key: \"author_id\"\n    value {\n      int64_list {\n        value: 123\n      }\n    }\n  }\n  feature {\n    key: \"pub_date\"\n    value {\n      int64_list {\n        value: 1623\n        value: 12\n        value: 25\n      }\n    }\n  }\n  feature {\n    key: \"title\"\n    value {\n      bytes_list {\n        value: \"A\"\n        value: \"desert\"\n        value: \"place\"\n        value: \".\"\n      }\n    }\n  }\n}\nfeature_lists {\n  feature_list {\n    key: \"comments\"\n    value {\n      feature {\n        bytes_list {\n          value: \"When\"\n          value: \"the\"\n          value: \"hurlyburly\"\n          value: \"\\'s\"\n          value: \"done\"\n          value: \".\"\n        }\n      }\n      feature {\n        bytes_list {\n          value: \"When\"\n          value: \"the\"\n          value: \"battle\"\n          value: \"\\'s\"\n          value: \"lost\"\n          value: \"and\"\n          value: \"won\"\n          value: \".\"\n        }\n      }\n    }\n  }\n  feature_list {\n    key: \"content\"\n    value {\n      feature {\n        bytes_list {\n          value: \"When\"\n          value: \"shall\"\n          value: \"we\"\n          value: \"three\"\n          value: \"meet\"\n          value: \"again\"\n          value: \"?\"\n        }\n      }\n      feature {\n        bytes_list {\n          value: \"In\"\n          value: \"thunder\"\n          value: \",\"\n          value: \"lightning\"\n          value: \",\"\n          value: \"or\"\n          value: \"in\"\n          value: \"rain\"\n          value: \"?\"\n        }\n      }\n    }\n  }\n}\n\n\n\nserialized_sequence_example = sequence_example.SerializeToString()\n\n\ncontext_feature_descriptions = {\n    \"author_id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n    \"title\": tf.io.VarLenFeature(tf.string),\n    \"pub_date\": tf.io.FixedLenFeature([3], tf.int64, default_value=[0, 0, 0]),\n}\nsequence_feature_descriptions = {\n    \"content\": tf.io.VarLenFeature(tf.string),\n    \"comments\": tf.io.VarLenFeature(tf.string),\n}\nparsed_context, parsed_feature_lists = tf.io.parse_single_sequence_example(\n    serialized_sequence_example, context_feature_descriptions,\n    sequence_feature_descriptions)\n\n\nparsed_context\n\n{'author_id': &lt;tf.Tensor: shape=(), dtype=int64, numpy=123&gt;,\n 'pub_date': &lt;tf.Tensor: shape=(3,), dtype=int64, numpy=array([1623,   12,   25])&gt;,\n 'title': &lt;tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7fb910db4b10&gt;}\n\n\n\nparsed_context[\"title\"].values\n\n&lt;tf.Tensor: shape=(4,), dtype=string, numpy=array([b'A', b'desert', b'place', b'.'], dtype=object)&gt;\n\n\n\nparsed_feature_lists\n\n{'comments': &lt;tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7fb910db4b50&gt;,\n 'content': &lt;tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7fb910db48d0&gt;}\n\n\n\nprint(tf.RaggedTensor.from_sparse(parsed_feature_lists[\"content\"]))\n\n&lt;tf.RaggedTensor [[b'When', b'shall', b'we', b'three', b'meet', b'again', b'?'], [b'In', b'thunder', b',', b'lightning', b',', b'or', b'in', b'rain', b'?']]&gt;"
  },
  {
    "objectID": "Machine_Learning/13_loading_and_preprocessing_data.html#feature_column을-사용해-파싱하기",
    "href": "Machine_Learning/13_loading_and_preprocessing_data.html#feature_column을-사용해-파싱하기",
    "title": "13_loading_and_preprocessing_data",
    "section": "feature_column을 사용해 파싱하기",
    "text": "feature_column을 사용해 파싱하기\n\nmedian_house_value = tf.feature_column.numeric_column(\"median_house_value\")\n\n\ncolumns = [housing_median_age, median_house_value]\nfeature_descriptions = tf.feature_column.make_parse_example_spec(columns)\nfeature_descriptions\n\n{'housing_median_age': FixedLenFeature(shape=(1,), dtype=tf.float32, default_value=None),\n 'median_house_value': FixedLenFeature(shape=(1,), dtype=tf.float32, default_value=None)}\n\n\n\nwith tf.io.TFRecordWriter(\"my_data_with_features.tfrecords\") as f:\n    for x, y in zip(X_train[:, 1:2], y_train):\n        example = Example(features=Features(feature={\n            \"housing_median_age\": Feature(float_list=FloatList(value=[x])),\n            \"median_house_value\": Feature(float_list=FloatList(value=[y]))\n        }))\n        f.write(example.SerializeToString())\n\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n\ndef parse_examples(serialized_examples):\n    examples = tf.io.parse_example(serialized_examples, feature_descriptions)\n    targets = examples.pop(\"median_house_value\") # separate the targets\n    return examples, targets\n\nbatch_size = 32\ndataset = tf.data.TFRecordDataset([\"my_data_with_features.tfrecords\"])\ndataset = dataset.repeat().shuffle(10000).batch(batch_size).map(parse_examples)\n\nWarning: the DenseFeatures layer currently does not work with the Functional API, see TF issue #27416. Hopefully this will be resolved before the final release of TF 2.0.\n\ncolumns_without_target = columns[:-1]\nmodel = keras.models.Sequential([\n    keras.layers.DenseFeatures(feature_columns=columns_without_target),\n    keras.layers.Dense(1)\n])\nmodel.compile(loss=\"mse\",\n              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n              metrics=[\"accuracy\"])\nmodel.fit(dataset, steps_per_epoch=len(X_train) // batch_size, epochs=5)\n\nEpoch 1/5\nWARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a &lt;class 'dict'&gt; input: {'housing_median_age': &lt;tf.Tensor 'IteratorGetNext:0' shape=(None, 1) dtype=float32&gt;}\nConsider rewriting this model with the Functional API.\nWARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a &lt;class 'dict'&gt; input: {'housing_median_age': &lt;tf.Tensor 'IteratorGetNext:0' shape=(None, 1) dtype=float32&gt;}\nConsider rewriting this model with the Functional API.\n362/362 [==============================] - 1s 2ms/step - loss: 3.7619 - accuracy: 0.0016\nEpoch 2/5\n362/362 [==============================] - 0s 1ms/step - loss: 1.9311 - accuracy: 0.0027\nEpoch 3/5\n362/362 [==============================] - 0s 1ms/step - loss: 1.4434 - accuracy: 0.0026\nEpoch 4/5\n362/362 [==============================] - 0s 1ms/step - loss: 1.3579 - accuracy: 0.0030\nEpoch 5/5\n362/362 [==============================] - 0s 1ms/step - loss: 1.3473 - accuracy: 0.0038\n\n\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n\n\n&lt;tensorflow.python.keras.callbacks.History at 0x7fb90c8900d0&gt;\n\n\n\nsome_columns = [ocean_proximity_embed, bucketized_income]\ndense_features = keras.layers.DenseFeatures(some_columns)\ndense_features({\n    \"ocean_proximity\": [[\"NEAR OCEAN\"], [\"INLAND\"], [\"INLAND\"]],\n    \"median_income\": [[3.], [7.2], [1.]]\n})\n\n&lt;tf.Tensor: shape=(3, 7), dtype=float32, numpy=\narray([[ 0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n        -0.14504611,  0.7563394 ],\n       [ 0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n        -1.1119912 ,  0.56957847],\n       [ 1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        -1.1119912 ,  0.56957847]], dtype=float32)&gt;"
  },
  {
    "objectID": "Machine_Learning/13_loading_and_preprocessing_data.html#to-8.",
    "href": "Machine_Learning/13_loading_and_preprocessing_data.html#to-8.",
    "title": "13_loading_and_preprocessing_data",
    "section": "1. to 8.",
    "text": "1. to 8.\n부록 A 참고"
  },
  {
    "objectID": "Machine_Learning/13_loading_and_preprocessing_data.html#section",
    "href": "Machine_Learning/13_loading_and_preprocessing_data.html#section",
    "title": "13_loading_and_preprocessing_data",
    "section": "9.",
    "text": "9.\n\na.\n문제: (10장에서 소개한) 패션 MNIST 데이터셋을 적재하고 훈련 세트, 검증 세트, 테스트 세트로 나눕니다. 훈련 세트를 섞은 다음 각 데이터셋을 TFRecord 파일로 저장합니 다. 각 레코드는 두 개의 특성을 가진 Example 프로토콜 버퍼, 즉 직렬화된 이미지(tf.io.serialize_tensor()를 사용해 이미지를 직렬화하세요)와 레이블입니다. 참고: 용량이 큰 이미지일 경우 tf.io.encode_jpeg()를 사용할 수 있습니다. 많은 공간을 절약할 수 있지만 이미지 품질이 손해를 봅니다.\n\n(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\nX_valid, X_train = X_train_full[:5000], X_train_full[5000:]\ny_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n\ntrain_set = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train))\nvalid_set = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\ntest_set = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n\n\ndef create_example(image, label):\n    image_data = tf.io.serialize_tensor(image)\n    #image_data = tf.io.encode_jpeg(image[..., np.newaxis])\n    return Example(\n        features=Features(\n            feature={\n                \"image\": Feature(bytes_list=BytesList(value=[image_data.numpy()])),\n                \"label\": Feature(int64_list=Int64List(value=[label])),\n            }))\n\n\nfor image, label in valid_set.take(1):\n    print(create_example(image, label))\n\nfeatures {\n  feature {\n    key: \"image\"\n    value {\n      bytes_list {\n        value: \"\\010\\004\\022\\010\\022\\002\\010\\034\\022\\002\\010\\034\\\"\\220\\006\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\001\\000\\000\\rI\\000\\000\\001\\004\\000\\000\\000\\000\\001\\001\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\003\\000$\\210\\177&gt;6\\000\\000\\000\\001\\003\\004\\000\\000\\003\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\006\\000f\\314\\260\\206\\220{\\027\\000\\000\\000\\000\\014\\n\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\233\\354\\317\\262k\\234\\241m@\\027M\\202H\\017\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\001\\000E\\317\\337\\332\\330\\330\\243\\177yz\\222\\215X\\254B\\000\\000\\000\\000\\000\\000\\000\\000\\000\\001\\001\\001\\000\\310\\350\\350\\351\\345\\337\\337\\327\\325\\244\\177{\\304\\345\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\267\\341\\330\\337\\344\\353\\343\\340\\336\\340\\335\\337\\365\\255\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\301\\344\\332\\325\\306\\264\\324\\322\\323\\325\\337\\334\\363\\312\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\001\\003\\000\\014\\333\\334\\324\\332\\300\\251\\343\\320\\332\\340\\324\\342\\305\\3214\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\006\\000c\\364\\336\\334\\332\\313\\306\\335\\327\\325\\336\\334\\365w\\2478\\000\\000\\000\\000\\000\\000\\000\\000\\000\\004\\000\\0007\\354\\344\\346\\344\\360\\350\\325\\332\\337\\352\\331\\331\\321\\\\\\000\\000\\000\\001\\004\\006\\007\\002\\000\\000\\000\\000\\000\\355\\342\\331\\337\\336\\333\\336\\335\\330\\337\\345\\327\\332\\377M\\000\\000\\003\\000\\000\\000\\000\\000\\000\\000&gt;\\221\\314\\344\\317\\325\\335\\332\\320\\323\\332\\340\\337\\333\\327\\340\\364\\237\\000\\000\\000\\000\\000\\022,Rk\\275\\344\\334\\336\\331\\342\\310\\315\\323\\346\\340\\352\\260\\274\\372\\370\\351\\356\\327\\000\\0009\\273\\320\\340\\335\\340\\320\\314\\326\\320\\321\\310\\237\\365\\301\\316\\337\\377\\377\\335\\352\\335\\323\\334\\350\\366\\000\\003\\312\\344\\340\\335\\323\\323\\326\\315\\315\\315\\334\\360P\\226\\377\\345\\335\\274\\232\\277\\322\\314\\321\\336\\344\\341\\000b\\351\\306\\322\\336\\345\\345\\352\\371\\334\\302\\327\\331\\361AIju\\250\\333\\335\\327\\331\\337\\337\\340\\345\\035K\\314\\324\\314\\301\\315\\323\\341\\330\\271\\305\\316\\306\\325\\360\\303\\343\\365\\357\\337\\332\\324\\321\\336\\334\\335\\346C0\\313\\267\\302\\325\\305\\271\\276\\302\\300\\312\\326\\333\\335\\334\\354\\341\\330\\307\\316\\272\\265\\261\\254\\265\\315\\316s\\000z\\333\\301\\263\\253\\267\\304\\314\\322\\325\\317\\323\\322\\310\\304\\302\\277\\303\\277\\306\\300\\260\\234\\247\\261\\322\\\\\\000\\000J\\275\\324\\277\\257\\254\\257\\265\\271\\274\\275\\274\\301\\306\\314\\321\\322\\322\\323\\274\\274\\302\\300\\330\\252\\000\\002\\000\\000\\000B\\310\\336\\355\\357\\362\\366\\363\\364\\335\\334\\301\\277\\263\\266\\266\\265\\260\\246\\250c:\\000\\000\\000\\000\\000\\000\\000\\000\\000(=,H)#\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\"\n      }\n    }\n  }\n  feature {\n    key: \"label\"\n    value {\n      int64_list {\n        value: 9\n      }\n    }\n  }\n}\n\n\n\n다음 함수는 주어진 데이터셋을 일련의 TFRecord 파일로 저장합니다. 이 예제는 라운드-로빈 방식으로 파일에 저장합니다. 이를 위해 dataset.enumerate() 메서드로 모든 샘플을 순회하고 저장할 파일을 겨정하기 위해 index % n_shards를 계산합니다. 표준 contextlib.ExitStack 클래스를 사용해 쓰는 동안 I/O 에러의 발생 여부에 상관없이 모든 writer가 적절히 종료되었는지 확인합니다.\n\nfrom contextlib import ExitStack\n\ndef write_tfrecords(name, dataset, n_shards=10):\n    paths = [\"{}.tfrecord-{:05d}-of-{:05d}\".format(name, index, n_shards)\n             for index in range(n_shards)]\n    with ExitStack() as stack:\n        writers = [stack.enter_context(tf.io.TFRecordWriter(path))\n                   for path in paths]\n        for index, (image, label) in dataset.enumerate():\n            shard = index % n_shards\n            example = create_example(image, label)\n            writers[shard].write(example.SerializeToString())\n    return paths\n\n\ntrain_filepaths = write_tfrecords(\"my_fashion_mnist.train\", train_set)\nvalid_filepaths = write_tfrecords(\"my_fashion_mnist.valid\", valid_set)\ntest_filepaths = write_tfrecords(\"my_fashion_mnist.test\", test_set)\n\n\n\nb.\n문제: tf.data로 각 세트를 위한 효율적인 데이터셋을 만듭니다. 마지막으로 이 데이터셋으로 입력 특성을 표준화하는 전처리 층을 포함한 케라스 모델을 훈련합니다. 텐서보드로 프 로파일 데이터를 시각화하여 가능한 한 입력 파이프라인을 효율적으로 만들어보세요.\n\ndef preprocess(tfrecord):\n    feature_descriptions = {\n        \"image\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n        \"label\": tf.io.FixedLenFeature([], tf.int64, default_value=-1)\n    }\n    example = tf.io.parse_single_example(tfrecord, feature_descriptions)\n    image = tf.io.parse_tensor(example[\"image\"], out_type=tf.uint8)\n    #image = tf.io.decode_jpeg(example[\"image\"])\n    image = tf.reshape(image, shape=[28, 28])\n    return image, example[\"label\"]\n\ndef mnist_dataset(filepaths, n_read_threads=5, shuffle_buffer_size=None,\n                  n_parse_threads=5, batch_size=32, cache=True):\n    dataset = tf.data.TFRecordDataset(filepaths,\n                                      num_parallel_reads=n_read_threads)\n    if cache:\n        dataset = dataset.cache()\n    if shuffle_buffer_size:\n        dataset = dataset.shuffle(shuffle_buffer_size)\n    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n    dataset = dataset.batch(batch_size)\n    return dataset.prefetch(1)\n\n\ntrain_set = mnist_dataset(train_filepaths, shuffle_buffer_size=60000)\nvalid_set = mnist_dataset(valid_filepaths)\ntest_set = mnist_dataset(test_filepaths)\n\n\nfor X, y in train_set.take(1):\n    for i in range(5):\n        plt.subplot(1, 5, i + 1)\n        plt.imshow(X[i].numpy(), cmap=\"binary\")\n        plt.axis(\"off\")\n        plt.title(str(y[i].numpy()))\n\n\n\n\n\nkeras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nclass Standardization(keras.layers.Layer):\n    def adapt(self, data_sample):\n        self.means_ = np.mean(data_sample, axis=0, keepdims=True)\n        self.stds_ = np.std(data_sample, axis=0, keepdims=True)\n    def call(self, inputs):\n        return (inputs - self.means_) / (self.stds_ + keras.backend.epsilon())\n\nstandardization = Standardization(input_shape=[28, 28])\n# or perhaps soon:\n#standardization = keras.layers.Normalization()\n\nsample_image_batches = train_set.take(100).map(lambda image, label: image)\nsample_images = np.concatenate(list(sample_image_batches.as_numpy_iterator()),\n                               axis=0).astype(np.float32)\nstandardization.adapt(sample_images)\n\nmodel = keras.models.Sequential([\n    standardization,\n    keras.layers.Flatten(),\n    keras.layers.Dense(100, activation=\"relu\"),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=\"nadam\", metrics=[\"accuracy\"])\n\n\nfrom datetime import datetime\nlogs = os.path.join(os.curdir, \"my_logs\",\n                    \"run_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n\ntensorboard_cb = tf.keras.callbacks.TensorBoard(\n    log_dir=logs, histogram_freq=1, profile_batch=10)\n\nmodel.fit(train_set, epochs=5, validation_data=valid_set,\n          callbacks=[tensorboard_cb])\n\nEpoch 1/5\n1719/1719 [==============================] - 10s 6ms/step - loss: 570.1453 - accuracy: 0.8415 - val_loss: 149.1351 - val_accuracy: 0.8670\nEpoch 2/5\n1719/1719 [==============================] - 9s 5ms/step - loss: 639.2134 - accuracy: 0.8783 - val_loss: 282.9144 - val_accuracy: 0.8746\nEpoch 3/5\n1719/1719 [==============================] - 9s 5ms/step - loss: 98.1983 - accuracy: 0.8907 - val_loss: 0.3328 - val_accuracy: 0.8808\nEpoch 4/5\n1719/1719 [==============================] - 8s 5ms/step - loss: 437.1278 - accuracy: 0.9007 - val_loss: 151.8366 - val_accuracy: 0.8798\nEpoch 5/5\n1719/1719 [==============================] - 9s 5ms/step - loss: 198.3806 - accuracy: 0.9077 - val_loss: 87.7543 - val_accuracy: 0.8816\n\n\n&lt;tensorflow.python.keras.callbacks.History at 0x7fb9119e7850&gt;\n\n\nWarning: The profiling tab in TensorBoard works if you use TensorFlow 2.2+. You also need to make sure tensorboard_plugin_profile is installed (and restart Jupyter if necessary).\n\n%load_ext tensorboard\n%tensorboard --logdir=./my_logs --port=6006\n\nReusing TensorBoard on port 6006 (pid 702), started 1:15:59 ago. (Use '!kill 702' to kill it.)"
  },
  {
    "objectID": "Machine_Learning/13_loading_and_preprocessing_data.html#section-1",
    "href": "Machine_Learning/13_loading_and_preprocessing_data.html#section-1",
    "title": "13_loading_and_preprocessing_data",
    "section": "10.",
    "text": "10.\n문제: 이 연습문제에서 데이터셋을 다운로드 및 분할하고 tf.data.Dataset 객체를 만들어 데이터를 적재하고 효율적으로 전처리하겠습니다. 그다음 Embedding 층을 포함한 이진 분류 모델을 만들고 훈련시킵니다.\n\na.\n문제: 인터넷 영화 데이터베이스의 영화 리뷰 50,000개를 담은 영화 리뷰 데이터셋을 다운로드합니다. 이 데이터는 train과 test라는 두 개의 디렉터리로 구성되어 있습니다. 각 디렉터리에는 12,500개의 긍정 리뷰를 담은 pos 서브디렉터리와 12,500개의 부정 리뷰를 담은 neg 서브디렉터리가 있습니다. 리뷰는 각각 별도의 텍스트 파일에 저장되어 있습니다. (전처리된 BOW를 포함해) 다른 파일과 디렉터리가 있지만 이 연습문제에서는 무시합니다.\n\nfrom pathlib import Path\n\nDOWNLOAD_ROOT = \"http://ai.stanford.edu/~amaas/data/sentiment/\"\nFILENAME = \"aclImdb_v1.tar.gz\"\nfilepath = keras.utils.get_file(FILENAME, DOWNLOAD_ROOT + FILENAME, extract=True)\npath = Path(filepath).parent / \"aclImdb\"\npath\n\nPosixPath('/root/.keras/datasets/aclImdb')\n\n\n\nfor name, subdirs, files in os.walk(path):\n    indent = len(Path(name).parts) - len(path.parts)\n    print(\"    \" * indent + Path(name).parts[-1] + os.sep)\n    for index, filename in enumerate(sorted(files)):\n        if index == 3:\n            print(\"    \" * (indent + 1) + \"...\")\n            break\n        print(\"    \" * (indent + 1) + filename)\n\naclImdb/\n    README\n    imdb.vocab\n    imdbEr.txt\n    test/\n        labeledBow.feat\n        urls_neg.txt\n        urls_pos.txt\n        neg/\n            0_2.txt\n            10000_4.txt\n            10001_1.txt\n            ...\n        pos/\n            0_10.txt\n            10000_7.txt\n            10001_9.txt\n            ...\n    train/\n        labeledBow.feat\n        unsupBow.feat\n        urls_neg.txt\n        ...\n        neg/\n            0_3.txt\n            10000_4.txt\n            10001_4.txt\n            ...\n        pos/\n            0_9.txt\n            10000_8.txt\n            10001_10.txt\n            ...\n        unsup/\n            0_0.txt\n            10000_0.txt\n            10001_0.txt\n            ...\n\n\n\ndef review_paths(dirpath):\n    return [str(path) for path in dirpath.glob(\"*.txt\")]\n\ntrain_pos = review_paths(path / \"train\" / \"pos\")\ntrain_neg = review_paths(path / \"train\" / \"neg\")\ntest_valid_pos = review_paths(path / \"test\" / \"pos\")\ntest_valid_neg = review_paths(path / \"test\" / \"neg\")\n\nlen(train_pos), len(train_neg), len(test_valid_pos), len(test_valid_neg)\n\n(12500, 12500, 12500, 12500)\n\n\n\n\nb.\n문제: 테스트 세트를 검증 세트(15,000개)와 테스트 세트(10,000개)로 나눕니다.\n\nnp.random.shuffle(test_valid_pos)\n\ntest_pos = test_valid_pos[:5000]\ntest_neg = test_valid_neg[:5000]\nvalid_pos = test_valid_pos[5000:]\nvalid_neg = test_valid_neg[5000:]\n\n\n\nc.\n문제: tf.data를 사용해 각 세트에 대한 효율적인 데이터셋을 만듭니다.\n이 데이터셋을 메모리에 적재할 수 있으므로 파이썬 코드와 tf.data.Dataset.from_tensor_slices()를 사용해 모든 데이터를 적재합니다:\n\ndef imdb_dataset(filepaths_positive, filepaths_negative):\n    reviews = []\n    labels = []\n    for filepaths, label in ((filepaths_negative, 0), (filepaths_positive, 1)):\n        for filepath in filepaths:\n            with open(filepath) as review_file:\n                reviews.append(review_file.read())\n            labels.append(label)\n    return tf.data.Dataset.from_tensor_slices(\n        (tf.constant(reviews), tf.constant(labels)))\n\n\nfor X, y in imdb_dataset(train_pos, train_neg).take(3):\n    print(X)\n    print(y)\n    print()\n\ntf.Tensor(b'Positively awful George Sanders vehicle where he goes from being a thief to police czar.&lt;br /&gt;&lt;br /&gt;While Sanders was an excellent character actor, he was certainly no leading man and this film proves it.&lt;br /&gt;&lt;br /&gt;It is absolutely beyond stupidity. Gene Lockhart did provide some comic relief until a moment of anger led him to fire his gun with tragedy resulting.&lt;br /&gt;&lt;br /&gt;Sadly, George Sanders and co-star Carol Landis committed suicide in real life. After making a film as deplorable as this, it is not shocking.&lt;br /&gt;&lt;br /&gt;The usual appealing Signe Hasso is really nothing here.', shape=(), dtype=string)\ntf.Tensor(0, shape=(), dtype=int32)\n\ntf.Tensor(b\"Ooof! This one was a stinker. It does not fall 'somewhere in between Star Wars and Thriller', thats for sure. In all actuality, it falls somewhere between the cracks of a Wham! video and Captain EO, only with not as big of a budget, and a lot more close ups of ugly teenagers crying. Simon Le Bon preens front and center, while the rest of the band gamely tries to hide the fact that they stole their whole career from Roxy Music's last 3 albums. Brief clips from Barbarella add nothing. Avoid at all costs. (However, I liked the part when they played 'Hungry Like The Wolf' but why was there a tiger lurking in the audience changing into a woman painted with tiger stripes? I mean, they aren't singing 'Eye of the Tiger' or 'Hungry like the Tiger' it's a Wolf! Whatever.) A DVD of Duran Duran's '80s videos is probably worth a look for nostalgia's sake\", shape=(), dtype=string)\ntf.Tensor(0, shape=(), dtype=int32)\n\ntf.Tensor(b\"It's rare that I feel a need to write a review on this site, but this film is very deserving because of how poorly it was created, and how bias its product was.&lt;br /&gt;&lt;br /&gt;I felt a distinct attempt on the part of the film-makers to display the Palestinian family as boorish and untrustworthy. We hear them discuss the sadness that they feel from oppression, yet the film is shot and arranged in a way that we feel the politically oppressed population is the Jewish Israeli population. We see no evidence that parallels the position of the Palestinian teenager. We only hear from other Palestinians in prison. I understand restrictions are in place, but the political nature of the restrictions are designed to prevent peace.&lt;br /&gt;&lt;br /&gt;I came out of the film feeling that the mother of the victim was selfish in her mourning and completely closed minded due to her side of the fence, so to speak. She continued to be unwilling to see the hurt of the bomber's parents, and her angry and closed-minded words caused the final meeting to spiral out of control. It is more realistic, in my mind, to see the Israeli mindset to be a root of the problem; ignored pleas for understanding and freedom, ignored requests for acknowledgment for the process by which the Jewish population acquired the land.&lt;br /&gt;&lt;br /&gt;I have given this a two because of these selfish weaknesses of the mother, which normally would be admirable in a documentary, however in the light of the lack of impartiality, it all seems exploitative. Also for the poor edits, lack of background in the actual instance, and finally the lack of proper representation of the Palestinian side. Ultimately, it is a poor documentary and a poor film. I acknowledge this is partially the result of the political situation, but am obliged to note the flaws in direction regardless of the heart-wrenching and sad subject matter.\", shape=(), dtype=string)\ntf.Tensor(0, shape=(), dtype=int32)\n\n\n\n\n%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).repeat(10): pass\n\n1 loop, best of 1: 23.8 s per loop\n\n\n이 데이터셋을 적재하고 10회 반복하는데 약 17초가 걸립니다.\n하지만 이 데이터셋이 메모리에 맞지 않는다고 가정하고 좀 더 재미있는 것을 만들어 보죠. 다행히 각 리뷰는 한 줄로 되어 있기 때문에(&lt;br /&gt;로 줄바꿈됩니다) TextLineDataset를 사용해 리뷰를 읽을 수 있습니다. 그렇지 않으면 입력 파일을 전처리해야 합니다(예를 들어, TFRecord로 바꿉니다). 매우 큰 데이터셋의 경우 아파치 빔(Apache Beam) 같은 도구를 사용하는 것이 합리적입니다.\n\ndef imdb_dataset(filepaths_positive, filepaths_negative, n_read_threads=5):\n    dataset_neg = tf.data.TextLineDataset(filepaths_negative,\n                                          num_parallel_reads=n_read_threads)\n    dataset_neg = dataset_neg.map(lambda review: (review, 0))\n    dataset_pos = tf.data.TextLineDataset(filepaths_positive,\n                                          num_parallel_reads=n_read_threads)\n    dataset_pos = dataset_pos.map(lambda review: (review, 1))\n    return tf.data.Dataset.concatenate(dataset_pos, dataset_neg)\n\n\n%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).repeat(10): pass\n\n1 loop, best of 1: 2min 22s per loop\n\n\n이 데이터셋을 10회 반복하는데 33초 걸립니다. 데이터셋이 RAM에 캐싱되지 않고 에포크마다 다시 로드되기 때문에 매우 느립니다. .repeat(10) 전에 .cache()를 추가하면 이전만큼 빨라지는 것을 확인할 수 있습니다.\n\n%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).cache().repeat(10): pass\n\n1 loop, best of 1: 29.2 s per loop\n\n\n\nbatch_size = 32\n\ntrain_set = imdb_dataset(train_pos, train_neg).shuffle(25000).batch(batch_size).prefetch(1)\nvalid_set = imdb_dataset(valid_pos, valid_neg).batch(batch_size).prefetch(1)\ntest_set = imdb_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)\n\n\n\nd.\n문제: 리뷰를 전처리하기 위해 TextVectorization 층을 사용한 이진 분류 모델을 만드세요. TextVectorization 층을 아직 사용할 수 없다면 (또는 도전을 좋아한다면) 사용자 전처리 층을 만들어보세요. tf.strings 패키지에 있는 함수를 사용할 수 있습니다. 예를 들어 lower()로 소문자로 만들거나 regex_replace()로 구두점을 공백으로 바꾸고 split()로 공백을 기준으로 단어를 나눌 수 있습니다. 룩업 테이블을 사용해 단어 인덱스를 출력하세요. adapt() 메서드로 미리 층을 적응시켜야 합니다.\n먼저 리뷰를 전처리하는 함수를 만듭니다. 이 함수는 리뷰를 300자로 자르고 소문자로 변환합니다. 그다음 &lt;br /&gt;와 글자가 아닌 모든 문자를 공백으로 바꾸고 리뷰를 단어로 분할해 마지막으로 각 리뷰가 n_words 개수의 토큰이 되도록 패딩하거나 잘라냅니다:\n\ndef preprocess(X_batch, n_words=50):\n    shape = tf.shape(X_batch) * tf.constant([1, 0]) + tf.constant([0, n_words])\n    Z = tf.strings.substr(X_batch, 0, 300)\n    Z = tf.strings.lower(Z)\n    Z = tf.strings.regex_replace(Z, b\"&lt;br\\\\s*/?&gt;\", b\" \")\n    Z = tf.strings.regex_replace(Z, b\"[^a-z]\", b\" \")\n    Z = tf.strings.split(Z)\n    return Z.to_tensor(shape=shape, default_value=b\"&lt;pad&gt;\")\n\nX_example = tf.constant([\"It's a great, great movie! I loved it.\", \"It was terrible, run away!!!\"])\npreprocess(X_example)\n\n&lt;tf.Tensor: shape=(2, 50), dtype=string, numpy=\narray([[b'it', b's', b'a', b'great', b'great', b'movie', b'i', b'loved',\n        b'it', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;',\n        b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;',\n        b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;',\n        b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;',\n        b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;',\n        b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;',\n        b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;'],\n       [b'it', b'was', b'terrible', b'run', b'away', b'&lt;pad&gt;', b'&lt;pad&gt;',\n        b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;',\n        b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;',\n        b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;',\n        b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;',\n        b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;',\n        b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;',\n        b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;', b'&lt;pad&gt;',\n        b'&lt;pad&gt;']], dtype=object)&gt;\n\n\n이제 preprocess() 함수의 출력과 동일한 포맷의 데이터 샘플을 입력받는 두 번째 유틸리티 함수를 만듭니다. 이 함수는 가장 빈번한 max_size 개수의 단어로 된 리스트를 출력합니다. 가장 흔한 단어는 패딩 토큰입니다.\n\nfrom collections import Counter\n\ndef get_vocabulary(data_sample, max_size=1000):\n    preprocessed_reviews = preprocess(data_sample).numpy()\n    counter = Counter()\n    for words in preprocessed_reviews:\n        for word in words:\n            if word != b\"&lt;pad&gt;\":\n                counter[word] += 1\n    return [b\"&lt;pad&gt;\"] + [word for word, count in counter.most_common(max_size)]\n\nget_vocabulary(X_example)\n\n[b'&lt;pad&gt;',\n b'it',\n b'great',\n b's',\n b'a',\n b'movie',\n b'i',\n b'loved',\n b'was',\n b'terrible',\n b'run',\n b'away']\n\n\n이제 TextVectorization 층을 만들 준비가 되었습니다. 이 층의 생성자는 단순하게 하이퍼파라미터(max_vocabulary_size와 n_oov_buckets)를 저장하는 역할만 수행합니다. adapt() 메서드는 get_vocabulary() 함수를 사용해 어휘 사전을 계산합니다. 그다음 StaticVocabularyTable를 만듭니다(16장에서 자세히 설명합니다). call() 메서드는 각 리뷰의 단어 리스트를 패딩합니다. 그다음 StaticVocabularyTable를 사용해 어휘 사전에 있는 단어의 인덱스를 조회합니다:\n\nclass TextVectorization(keras.layers.Layer):\n    def __init__(self, max_vocabulary_size=1000, n_oov_buckets=100, dtype=tf.string, **kwargs):\n        super().__init__(dtype=dtype, **kwargs)\n        self.max_vocabulary_size = max_vocabulary_size\n        self.n_oov_buckets = n_oov_buckets\n\n    def adapt(self, data_sample):\n        self.vocab = get_vocabulary(data_sample, self.max_vocabulary_size)\n        words = tf.constant(self.vocab)\n        word_ids = tf.range(len(self.vocab), dtype=tf.int64)\n        vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n        self.table = tf.lookup.StaticVocabularyTable(vocab_init, self.n_oov_buckets)\n        \n    def call(self, inputs):\n        preprocessed_inputs = preprocess(inputs)\n        return self.table.lookup(preprocessed_inputs)\n\n앞서 정의한 X_example로 테스트해 보죠:\n\ntext_vectorization = TextVectorization()\n\ntext_vectorization.adapt(X_example)\ntext_vectorization(X_example)\n\n&lt;tf.Tensor: shape=(2, 50), dtype=int64, numpy=\narray([[ 1,  3,  4,  2,  2,  5,  6,  7,  1,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0],\n       [ 1,  8,  9, 10, 11,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0]])&gt;\n\n\n좋습니다! 여기에서 볼 수 있듯이 각 리뷰는 정제되고 토큰화되었습니다. 각 단어는 어휘 사전의 인덱스로 인코딩됩니다(0은 &lt;pad&gt; 토큰입니다).\n이제 또 다른 TextVectorization 층을 만들고 전체 IMDB 훈련 세트에 적용해 보겠습니다(훈련 세트가 메모리에 맞지 않으면 train_set.take(500)처럼 일부 데이터만 사용할 수 있습니다):\n\nmax_vocabulary_size = 1000\nn_oov_buckets = 100\n\nsample_review_batches = train_set.map(lambda review, label: review)\nsample_reviews = np.concatenate(list(sample_review_batches.as_numpy_iterator()),\n                                axis=0)\n\ntext_vectorization = TextVectorization(max_vocabulary_size, n_oov_buckets,\n                                       input_shape=[])\ntext_vectorization.adapt(sample_reviews)\n\n동일하게 X_example로 실행해 보죠. 어휘 사전이 크기 때문에 단어의 ID가 큽니다:\n\ntext_vectorization(X_example)\n\n&lt;tf.Tensor: shape=(2, 50), dtype=int64, numpy=\narray([[  9,  14,   2,  64,  64,  12,   5, 256,   9,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  9,  13, 269, 531, 334,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])&gt;\n\n\n좋습니다. 그럼 어휘 사전에서 처음 10개 단어를 확인해 보죠:\n\ntext_vectorization.vocab[:10]\n\n[b'&lt;pad&gt;', b'the', b'a', b'of', b'and', b'i', b'to', b'is', b'this', b'it']\n\n\n이 단어가 리뷰에서 가장 많이 등장하는 단어입니다.\n이제 모델을 만들기 위해 모든 단어 ID를 어떤 식으로 인코딩해야 합니다. 한가지 방법은 BoW(bag of words)입니다. 어휘 사전에 있는 각 단어에 대해 리뷰에 단어가 등장하는 횟수를 카운트합니다. 예를 들면 다음과 같습니다:\n\nsimple_example = tf.constant([[1, 3, 1, 0, 0], [2, 2, 0, 0, 0]])\ntf.reduce_sum(tf.one_hot(simple_example, 4), axis=1)\n\n&lt;tf.Tensor: shape=(2, 4), dtype=float32, numpy=\narray([[2., 2., 0., 1.],\n       [3., 0., 2., 0.]], dtype=float32)&gt;\n\n\n첫 번째 리뷰에는 단어 0이 두 번 등장하고, 단어 1도 두 번, 단어 2는 0번, 단어 3은 한 번 등장합니다. 따라서 BoW 표현은 [2, 2, 0, 1]입니다. 비슷하게 두 번째 리뷰에는 단어 0이 세 번, 단어 1이 0번 등장하는 식입니다. 이 로직을 간단한 사용자 정의 층으로 구현해서 테스트해 보겠습니다. 단어 0은 &lt;pad&gt; 토큰에 해당하므로 카운트하지 않겠습니다.\n\nclass BagOfWords(keras.layers.Layer):\n    def __init__(self, n_tokens, dtype=tf.int32, **kwargs):\n        super().__init__(dtype=dtype, **kwargs)\n        self.n_tokens = n_tokens\n    def call(self, inputs):\n        one_hot = tf.one_hot(inputs, self.n_tokens)\n        return tf.reduce_sum(one_hot, axis=1)[:, 1:]\n\n테스트해 보죠:\n\nbag_of_words = BagOfWords(n_tokens=4)\nbag_of_words(simple_example)\n\n&lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[2., 0., 1.],\n       [0., 2., 0.]], dtype=float32)&gt;\n\n\n잘 동작하네요! 이제 훈련 세트의 어휘 사전 크기를 지정한 BagOfWord 객체를 만듭니다:\n\nn_tokens = max_vocabulary_size + n_oov_buckets + 1 # add 1 for &lt;pad&gt;\nbag_of_words = BagOfWords(n_tokens)\n\n이제 모델을 훈련할 차례입니다!\n\nmodel = keras.models.Sequential([\n    text_vectorization,\n    bag_of_words,\n    keras.layers.Dense(100, activation=\"relu\"),\n    keras.layers.Dense(1, activation=\"sigmoid\"),\n])\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n              metrics=[\"accuracy\"])\nmodel.fit(train_set, epochs=5, validation_data=valid_set)\n\nEpoch 1/5\n782/782 [==============================] - 31s 21ms/step - loss: 0.5428 - accuracy: 0.7189 - val_loss: 0.5081 - val_accuracy: 0.7399\nEpoch 2/5\n782/782 [==============================] - 21s 23ms/step - loss: 0.4719 - accuracy: 0.7692 - val_loss: 0.5037 - val_accuracy: 0.7473\nEpoch 3/5\n782/782 [==============================] - 20s 21ms/step - loss: 0.4205 - accuracy: 0.8057 - val_loss: 0.5102 - val_accuracy: 0.7431\nEpoch 4/5\n782/782 [==============================] - 20s 21ms/step - loss: 0.3504 - accuracy: 0.8502 - val_loss: 0.5317 - val_accuracy: 0.7397\nEpoch 5/5\n782/782 [==============================] - 20s 21ms/step - loss: 0.2690 - accuracy: 0.9022 - val_loss: 0.5723 - val_accuracy: 0.7355\n\n\n&lt;tensorflow.python.keras.callbacks.History at 0x7fb9014bde10&gt;\n\n\n첫 번째 에포크에서 검증 세트에 대해 73.5% 정확도를 얻었습니다. 하지만 더 진전이 없습니다. 16장에서 이를 더 개선해 보겠습니다. 지금은 tf.data와 케라스 전처리 층으로 효율적인 전처리를 수행하는 것에만 초점을 맞추었습니다.\n\n\ne.\n문제: Embedding 층을 추가하고 단어 개수의 제곱근을 곱하여 리뷰마다 평균 임베딩을 계산하세요(16장 참조). 이제 스케일이 조정된 이 평균 임베딩을 모델의 다음 부분으로 전달할 수 있습니다.\n각 리뷰의 평균 임베딩을 계산하고 리뷰에 있는 단어 개수의 제곱근을 곱하기 위해 간단한 함수를 정의합니다. 각 문장에 대해서 이 함수는 \\(M \\times \\sqrt N\\)을 계산합니다. 여기에서 \\(M\\)은 (패딩 토큰을 제외하고) 문장에 있는 모든 단어 임베딩의 평균입니다. \\(N\\)은 (패딩 토큰을 제외한) 문장에 있는 단어의 개수입니다. \\(M\\)을 \\(\\dfrac{S}{N}\\)로 다시 쓸 수 있습니다. 여기에서 \\(S\\)는 모든 단어 임베딩의 합입니다(패딩 토큰은 0 벡터이므로 합에서는 패딩 토큰을 포함했는지 여부가 문제가 안됩니다). 따라서 이 함수는 \\(M \\times \\sqrt N = \\dfrac{S}{N} \\times \\sqrt N = \\dfrac{S}{\\sqrt N \\times \\sqrt N} \\times \\sqrt N= \\dfrac{S}{\\sqrt N}\\)를 반환해야 합니다.\n각 리뷰의 평균 임베딩을 계산하고 리뷰의 단어 개수의 제곱근을 곱하기 위해 간단한 함수를 정의합니다:\n\ndef compute_mean_embedding(inputs):\n    not_pad = tf.math.count_nonzero(inputs, axis=-1)\n    n_words = tf.math.count_nonzero(not_pad, axis=-1, keepdims=True)    \n    sqrt_n_words = tf.math.sqrt(tf.cast(n_words, tf.float32))\n    return tf.reduce_sum(inputs, axis=1) / sqrt_n_words\n\nanother_example = tf.constant([[[1., 2., 3.], [4., 5., 0.], [0., 0., 0.]],\n                               [[6., 0., 0.], [0., 0., 0.], [0., 0., 0.]]])\ncompute_mean_embedding(another_example)\n\n&lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[3.535534 , 4.9497476, 2.1213205],\n       [6.       , 0.       , 0.       ]], dtype=float32)&gt;\n\n\n결과가 올바른지 확인해 보죠. 첫 번째 리뷰에는 2개의 단어가 있습니다(마지막 토큰은 &lt;pad&gt; 토큰을 나타내는 0벡터입니다).\n이 두 단어의 평균 임베딩을 계산하고 그 결과에 2의 제곱근을 곱해 보겠습니다:\n\ntf.reduce_mean(another_example[0:1, :2], axis=1) * tf.sqrt(2.)\n\n&lt;tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[3.535534 , 4.9497476, 2.1213202]], dtype=float32)&gt;\n\n\n좋습니다. 두 번째 리뷰를 확인해 보죠. 이 리뷰는 하나의 단어만 가지고 있습니다(두 개의 패딩 토큰은 무시합니다):\n\ntf.reduce_mean(another_example[1:2, :1], axis=1) * tf.sqrt(1.)\n\n&lt;tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[6., 0., 0.]], dtype=float32)&gt;\n\n\n완벽하군요. 이제 최종 모델을 훈련할 차례입니다. 이전과 동이하지만 BagOfWords 층을 Embedding 층과 compute_mean_embedding을 호출하는 Lambda 층으로 바꿉니다:\n\nembedding_size = 20\n\nmodel = keras.models.Sequential([\n    text_vectorization,\n    keras.layers.Embedding(input_dim=n_tokens,\n                           output_dim=embedding_size,\n                           mask_zero=True), # &lt;pad&gt; tokens =&gt; zero vectors\n    keras.layers.Lambda(compute_mean_embedding),\n    keras.layers.Dense(100, activation=\"relu\"),\n    keras.layers.Dense(1, activation=\"sigmoid\"),\n])\n\n\n\nf.\n문제: 모델을 훈련하고 얼마의 정확도가 나오는지 확인해보세요. 가능한 한 훈련 속도를 빠르게 하기 위해 파이프라인을 최적화해보세요.\n\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\nmodel.fit(train_set, epochs=5, validation_data=valid_set)\n\nEpoch 1/5\n782/782 [==============================] - 15s 12ms/step - loss: 0.5545 - accuracy: 0.7062 - val_loss: 0.5130 - val_accuracy: 0.7380\nEpoch 2/5\n782/782 [==============================] - 11s 9ms/step - loss: 0.4953 - accuracy: 0.7540 - val_loss: 0.5057 - val_accuracy: 0.7453\nEpoch 3/5\n782/782 [==============================] - 25s 15ms/step - loss: 0.4845 - accuracy: 0.7594 - val_loss: 0.5057 - val_accuracy: 0.7412\nEpoch 4/5\n782/782 [==============================] - 11s 10ms/step - loss: 0.4764 - accuracy: 0.7627 - val_loss: 0.5086 - val_accuracy: 0.7413\nEpoch 5/5\n782/782 [==============================] - 30s 9ms/step - loss: 0.4698 - accuracy: 0.7638 - val_loss: 0.5074 - val_accuracy: 0.7374\n\n\n&lt;tensorflow.python.keras.callbacks.History at 0x7fb8fe0cb790&gt;\n\n\n임베딩을 사용해서 더 나아지지 않았습니다(16장에서 이를 개선해 보겠습니다). 파이프라인은 충분히 빨라 보입니다(앞서 최적화했습니다).\n\n\ng.\n문제: tfds.load(\"imdb_reviews\")와 같이 TFDS를 사용해 동일한 데이터셋을 간단하게 적재해보세요.\n\nimport tensorflow_datasets as tfds\n\ndatasets = tfds.load(name=\"imdb_reviews\")\ntrain_set, test_set = datasets[\"train\"], datasets[\"test\"]\n\n\nfor example in train_set.take(1):\n    print(example[\"text\"])\n    print(example[\"label\"])\n\ntf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string)\ntf.Tensor(0, shape=(), dtype=int64)"
  },
  {
    "objectID": "Machine_Learning/11_training_deep_neural_networks.html",
    "href": "Machine_Learning/11_training_deep_neural_networks.html",
    "title": "11_training_deep_neural_networks",
    "section": "",
    "text": "11장 – 심층 신경망 훈련하기\n이 노트북은 11장에 있는 모든 샘플 코드와 연습문제 해답을 가지고 있습니다."
  },
  {
    "objectID": "Machine_Learning/11_training_deep_neural_networks.html#xavier-초기화와-he-초기화",
    "href": "Machine_Learning/11_training_deep_neural_networks.html#xavier-초기화와-he-초기화",
    "title": "11_training_deep_neural_networks",
    "section": "Xavier 초기화와 He 초기화",
    "text": "Xavier 초기화와 He 초기화\n\n[name for name in dir(keras.initializers) if not name.startswith(\"_\")]\n\n['Constant',\n 'GlorotNormal',\n 'GlorotUniform',\n 'HeNormal',\n 'HeUniform',\n 'Identity',\n 'Initializer',\n 'LecunNormal',\n 'LecunUniform',\n 'Ones',\n 'Orthogonal',\n 'RandomNormal',\n 'RandomUniform',\n 'TruncatedNormal',\n 'VarianceScaling',\n 'Zeros',\n 'constant',\n 'deserialize',\n 'get',\n 'glorot_normal',\n 'glorot_uniform',\n 'he_normal',\n 'he_uniform',\n 'identity',\n 'lecun_normal',\n 'lecun_uniform',\n 'ones',\n 'orthogonal',\n 'random_normal',\n 'random_uniform',\n 'serialize',\n 'truncated_normal',\n 'variance_scaling',\n 'zeros']\n\n\n\nkeras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")\n\n&lt;keras.layers.core.Dense at 0x7f98d0126828&gt;\n\n\n\ninit = keras.initializers.VarianceScaling(scale=2., mode='fan_avg',\n                                          distribution='uniform')\nkeras.layers.Dense(10, activation=\"relu\", kernel_initializer=init)\n\n&lt;keras.layers.core.Dense at 0x7f98207e8240&gt;"
  },
  {
    "objectID": "Machine_Learning/11_training_deep_neural_networks.html#수렴하지-않는-활성화-함수",
    "href": "Machine_Learning/11_training_deep_neural_networks.html#수렴하지-않는-활성화-함수",
    "title": "11_training_deep_neural_networks",
    "section": "수렴하지 않는 활성화 함수",
    "text": "수렴하지 않는 활성화 함수\n\nLeakyReLU\n\ndef leaky_relu(z, alpha=0.01):\n    return np.maximum(alpha*z, z)\n\n\nplt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\nplt.plot([-5, 5], [0, 0], 'k-')\nplt.plot([0, 0], [-0.5, 4.2], 'k-')\nplt.grid(True)\nprops = dict(facecolor='black', shrink=0.1)\nplt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\nplt.title(\"Leaky ReLU activation function\", fontsize=14)\nplt.axis([-5, 5, -0.5, 4.2])\n\nsave_fig(\"leaky_relu_plot\")\nplt.show()\n\n그림 저장: leaky_relu_plot\n\n\n\n\n\n\n[m for m in dir(keras.activations) if not m.startswith(\"_\")]\n\n['deserialize',\n 'elu',\n 'exponential',\n 'gelu',\n 'get',\n 'hard_sigmoid',\n 'linear',\n 'relu',\n 'selu',\n 'serialize',\n 'sigmoid',\n 'softmax',\n 'softplus',\n 'softsign',\n 'swish',\n 'tanh']\n\n\n\n[m for m in dir(keras.layers) if \"relu\" in m.lower()]\n\n['LeakyReLU', 'PReLU', 'ReLU', 'ThresholdedReLU']\n\n\nLeakyReLU를 사용해 패션 MNIST에서 신경망을 훈련해 보죠:\n\n(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\nX_train_full = X_train_full / 255.0\nX_test = X_test / 255.0\nX_valid, X_train = X_train_full[:5000], X_train_full[5000:]\ny_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n\n\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n    keras.layers.LeakyReLU(),\n    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n    keras.layers.LeakyReLU(),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\n\n2021-10-10 01:38:52.146735: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n              metrics=[\"accuracy\"])\n\n\nhistory = model.fit(X_train, y_train, epochs=10,\n                    validation_data=(X_valid, y_valid))\n\n2021-10-10 01:38:52.524612: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n\n\nEpoch 1/10\n1719/1719 [==============================] - 4s 2ms/step - loss: 1.2819 - accuracy: 0.6229 - val_loss: 0.8886 - val_accuracy: 0.7160\nEpoch 2/10\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.7955 - accuracy: 0.7362 - val_loss: 0.7130 - val_accuracy: 0.7658\nEpoch 3/10\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.6816 - accuracy: 0.7720 - val_loss: 0.6427 - val_accuracy: 0.7898\nEpoch 4/10\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.6217 - accuracy: 0.7944 - val_loss: 0.5900 - val_accuracy: 0.8064\nEpoch 5/10\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.5832 - accuracy: 0.8074 - val_loss: 0.5582 - val_accuracy: 0.8202\nEpoch 6/10\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.5553 - accuracy: 0.8156 - val_loss: 0.5350 - val_accuracy: 0.8238\nEpoch 7/10\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.5338 - accuracy: 0.8225 - val_loss: 0.5157 - val_accuracy: 0.8306\nEpoch 8/10\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.5173 - accuracy: 0.8272 - val_loss: 0.5079 - val_accuracy: 0.8286\nEpoch 9/10\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.5040 - accuracy: 0.8289 - val_loss: 0.4895 - val_accuracy: 0.8388\nEpoch 10/10\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.4924 - accuracy: 0.8321 - val_loss: 0.4817 - val_accuracy: 0.8398\n\n\nPReLU를 테스트해 보죠:\n\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n    keras.layers.PReLU(),\n    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n    keras.layers.PReLU(),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\n\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n              metrics=[\"accuracy\"])\n\n\nhistory = model.fit(X_train, y_train, epochs=10,\n                    validation_data=(X_valid, y_valid))\n\nEpoch 1/10\n1719/1719 [==============================] - 4s 2ms/step - loss: 1.3461 - accuracy: 0.6209 - val_loss: 0.9255 - val_accuracy: 0.7184\nEpoch 2/10\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.8197 - accuracy: 0.7355 - val_loss: 0.7305 - val_accuracy: 0.7632\nEpoch 3/10\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.6966 - accuracy: 0.7693 - val_loss: 0.6565 - val_accuracy: 0.7884\nEpoch 4/10\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.6331 - accuracy: 0.7909 - val_loss: 0.6003 - val_accuracy: 0.8046\nEpoch 5/10\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.5917 - accuracy: 0.8057 - val_loss: 0.5656 - val_accuracy: 0.8184\nEpoch 6/10\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.5618 - accuracy: 0.8135 - val_loss: 0.5406 - val_accuracy: 0.8238\nEpoch 7/10\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.5390 - accuracy: 0.8205 - val_loss: 0.5196 - val_accuracy: 0.8312\nEpoch 8/10\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.5213 - accuracy: 0.8257 - val_loss: 0.5113 - val_accuracy: 0.8312\nEpoch 9/10\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.5070 - accuracy: 0.8289 - val_loss: 0.4917 - val_accuracy: 0.8380\nEpoch 10/10\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.4945 - accuracy: 0.8315 - val_loss: 0.4826 - val_accuracy: 0.8396\n\n\n\n\nELU\n\ndef elu(z, alpha=1):\n    return np.where(z &lt; 0, alpha * (np.exp(z) - 1), z)\n\n\nplt.plot(z, elu(z), \"b-\", linewidth=2)\nplt.plot([-5, 5], [0, 0], 'k-')\nplt.plot([-5, 5], [-1, -1], 'k--')\nplt.plot([0, 0], [-2.2, 3.2], 'k-')\nplt.grid(True)\nplt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\nplt.axis([-5, 5, -2.2, 3.2])\n\nsave_fig(\"elu_plot\")\nplt.show()\n\n그림 저장: elu_plot\n\n\n\n\n\n텐서플로에서 쉽게 ELU를 적용할 수 있습니다. 층을 만들 때 활성화 함수로 지정하면 됩니다:\n\nkeras.layers.Dense(10, activation=\"elu\")\n\n&lt;keras.layers.core.Dense at 0x7f97aa71d3c8&gt;\n\n\n\n\nSELU\nGünter Klambauer, Thomas Unterthiner, Andreas Mayr는 2017년 한 훌륭한 논문에서 SELU 활성화 함수를 소개했습니다. 훈련하는 동안 완전 연결 층만 쌓아서 신경망을 만들고 SELU 활성화 함수와 LeCun 초기화를 사용한다면 자기 정규화됩니다. 각 층의 출력이 평균과 표준편차를 보존하는 경향이 있습니다. 이는 그레이디언트 소실과 폭주 문제를 막아줍니다. 그 결과로 SELU 활성화 함수는 이런 종류의 네트워크(특히 아주 깊은 네트워크)에서 다른 활성화 함수보다 뛰어난 성능을 종종 냅니다. 따라서 꼭 시도해 봐야 합니다. 하지만 SELU 활성화 함수의 자기 정규화 특징은 쉽게 깨집니다. ℓ1나 ℓ2 정규화, 드롭아웃, 맥스 노름, 스킵 연결이나 시퀀셜하지 않은 다른 토폴로지를 사용할 수 없습니다(즉 순환 신경망은 자기 정규화되지 않습니다). 하지만 실전에서 시퀀셜 CNN과 잘 동작합니다. 자기 정규화가 깨지면 SELU가 다른 활성화 함수보다 더 나은 성능을 내지 않을 것입니다.\n\nfrom scipy.special import erfc\n\n# alpha와 scale은 평균 0과 표준 편차 1로 자기 정규화합니다\n# (논문에 있는 식 14 참조):\nalpha_0_1 = -np.sqrt(2 / np.pi) / (erfc(1/np.sqrt(2)) * np.exp(1/2) - 1)\nscale_0_1 = (1 - erfc(1 / np.sqrt(2)) * np.sqrt(np.e)) * np.sqrt(2 * np.pi) * (2 * erfc(np.sqrt(2))*np.e**2 + np.pi*erfc(1/np.sqrt(2))**2*np.e - 2*(2+np.pi)*erfc(1/np.sqrt(2))*np.sqrt(np.e)+np.pi+2)**(-1/2)\n\n\ndef selu(z, scale=scale_0_1, alpha=alpha_0_1):\n    return scale * elu(z, alpha)\n\n\nplt.plot(z, selu(z), \"b-\", linewidth=2)\nplt.plot([-5, 5], [0, 0], 'k-')\nplt.plot([-5, 5], [-1.758, -1.758], 'k--')\nplt.plot([0, 0], [-2.2, 3.2], 'k-')\nplt.grid(True)\nplt.title(\"SELU activation function\", fontsize=14)\nplt.axis([-5, 5, -2.2, 3.2])\n\nsave_fig(\"selu_plot\")\nplt.show()\n\n그림 저장: selu_plot\n\n\n\n\n\n기본적으로 SELU 하이퍼파라미터(scale과 alpha)는 각 뉴런의 평균 출력이 0에 가깝고 표준 편차는 1에 가깝도록 조정됩니다(입력은 평균이 0이고 표준 편차 1로 표준화되었다고 가정합니다). 이 활성화 함수를 사용하면 1,000개의 층이 있는 심층 신경망도 모든 층에 걸쳐 거의 평균이 0이고 표준 편차를 1로 유지합니다. 이를 통해 그레이디언트 폭주와 소실 문제를 피할 수 있습니다:\n\nnp.random.seed(42)\nZ = np.random.normal(size=(500, 100)) # 표준화된 입력\nfor layer in range(1000):\n    W = np.random.normal(size=(100, 100), scale=np.sqrt(1 / 100)) # LeCun 초기화\n    Z = selu(np.dot(Z, W))\n    means = np.mean(Z, axis=0).mean()\n    stds = np.std(Z, axis=0).mean()\n    if layer % 100 == 0:\n        print(\"Layer {}: mean {:.2f}, std deviation {:.2f}\".format(layer, means, stds))\n\nLayer 0: mean -0.00, std deviation 1.00\nLayer 100: mean 0.02, std deviation 0.96\nLayer 200: mean 0.01, std deviation 0.90\nLayer 300: mean -0.02, std deviation 0.92\nLayer 400: mean 0.05, std deviation 0.89\nLayer 500: mean 0.01, std deviation 0.93\nLayer 600: mean 0.02, std deviation 0.92\nLayer 700: mean -0.02, std deviation 0.90\nLayer 800: mean 0.05, std deviation 0.83\nLayer 900: mean 0.02, std deviation 1.00\n\n\n쉽게 SELU를 사용할 수 있습니다:\n\nkeras.layers.Dense(10, activation=\"selu\",\n                   kernel_initializer=\"lecun_normal\")\n\n&lt;keras.layers.core.Dense at 0x7f97aa71dc50&gt;\n\n\n100개의 은닉층과 SELU 활성화 함수를 사용한 패션 MNIST를 위한 신경망을 만들어 보죠:\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Flatten(input_shape=[28, 28]))\nmodel.add(keras.layers.Dense(300, activation=\"selu\",\n                             kernel_initializer=\"lecun_normal\"))\nfor layer in range(99):\n    model.add(keras.layers.Dense(100, activation=\"selu\",\n                                 kernel_initializer=\"lecun_normal\"))\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))\n\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n              metrics=[\"accuracy\"])\n\n이제 훈련해 보죠. 입력을 평균 0과 표준 편차 1로 바꾸어야 한다는 것을 잊지 마세요:\n\npixel_means = X_train.mean(axis=0, keepdims=True)\npixel_stds = X_train.std(axis=0, keepdims=True)\nX_train_scaled = (X_train - pixel_means) / pixel_stds\nX_valid_scaled = (X_valid - pixel_means) / pixel_stds\nX_test_scaled = (X_test - pixel_means) / pixel_stds\n\n\nhistory = model.fit(X_train_scaled, y_train, epochs=5,\n                    validation_data=(X_valid_scaled, y_valid))\n\nEpoch 1/5\n1719/1719 [==============================] - 27s 14ms/step - loss: 1.2359 - accuracy: 0.5200 - val_loss: 0.8552 - val_accuracy: 0.6756\nEpoch 2/5\n1719/1719 [==============================] - 23s 13ms/step - loss: 0.7186 - accuracy: 0.7408 - val_loss: 0.6080 - val_accuracy: 0.7830\nEpoch 3/5\n1719/1719 [==============================] - 24s 14ms/step - loss: 0.6907 - accuracy: 0.7527 - val_loss: 0.6446 - val_accuracy: 0.7532\nEpoch 4/5\n1719/1719 [==============================] - 23s 13ms/step - loss: 0.6234 - accuracy: 0.7805 - val_loss: 0.6986 - val_accuracy: 0.7346\nEpoch 5/5\n1719/1719 [==============================] - 24s 14ms/step - loss: 0.6030 - accuracy: 0.7859 - val_loss: 0.7600 - val_accuracy: 0.7390\n\n\n대신 ReLU 활성화 함수를 사용하면 어떤 일이 일어나는지 확인해 보죠:\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Flatten(input_shape=[28, 28]))\nmodel.add(keras.layers.Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"))\nfor layer in range(99):\n    model.add(keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"))\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))\n\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n              metrics=[\"accuracy\"])\n\n\nhistory = model.fit(X_train_scaled, y_train, epochs=5,\n                    validation_data=(X_valid_scaled, y_valid))\n\nEpoch 1/5\n1719/1719 [==============================] - 23s 12ms/step - loss: 1.8369 - accuracy: 0.2603 - val_loss: 1.3222 - val_accuracy: 0.4058\nEpoch 2/5\n1719/1719 [==============================] - 20s 12ms/step - loss: 1.2342 - accuracy: 0.4627 - val_loss: 1.0143 - val_accuracy: 0.5800\nEpoch 3/5\n1719/1719 [==============================] - 20s 12ms/step - loss: 0.9480 - accuracy: 0.5912 - val_loss: 0.8867 - val_accuracy: 0.5914\nEpoch 4/5\n1719/1719 [==============================] - 20s 12ms/step - loss: 1.0154 - accuracy: 0.5803 - val_loss: 0.9080 - val_accuracy: 0.6020\nEpoch 5/5\n1719/1719 [==============================] - 20s 11ms/step - loss: 0.8391 - accuracy: 0.6571 - val_loss: 0.7974 - val_accuracy: 0.6784\n\n\n좋지 않군요. 그레이디언트 폭주나 소실 문제가 발생한 것입니다."
  },
  {
    "objectID": "Machine_Learning/11_training_deep_neural_networks.html#그레이디언트-클리핑",
    "href": "Machine_Learning/11_training_deep_neural_networks.html#그레이디언트-클리핑",
    "title": "11_training_deep_neural_networks",
    "section": "그레이디언트 클리핑",
    "text": "그레이디언트 클리핑\n모든 케라스 옵티마이저는 clipnorm이나 clipvalue 매개변수를 지원합니다:\n\noptimizer = keras.optimizers.SGD(clipvalue=1.0)\n\n\noptimizer = keras.optimizers.SGD(clipnorm=1.0)"
  },
  {
    "objectID": "Machine_Learning/11_training_deep_neural_networks.html#사전-훈련된-층-재사용하기",
    "href": "Machine_Learning/11_training_deep_neural_networks.html#사전-훈련된-층-재사용하기",
    "title": "11_training_deep_neural_networks",
    "section": "사전 훈련된 층 재사용하기",
    "text": "사전 훈련된 층 재사용하기\n\n케라스 모델 재사용하기\n패션 MNIST 훈련 세트를 두 개로 나누어 보죠: * X_train_A: 샌달과 셔츠(클래스 5와 6)을 제외한 모든 이미지 * X_train_B: 샌달과 셔츠 이미지 중 처음 200개만 가진 작은 훈련 세트\n검증 세트와 테스트 세트도 이렇게 나눕니다. 하지만 이미지 개수는 제한하지 않습니다.\nA 세트(8개의 클래스를 가진 분류 문제)에서 모델을 훈련하고 이를 재사용하여 B 세트(이진 분류)를 해결해 보겠습니다. A 작업에서 B 작업으로 약간의 지식이 전달되기를 기대합니다. 왜냐하면 A 세트의 클래스(스니커즈, 앵클 부츠, 코트, 티셔츠 등)가 B 세트에 있는 클래스(샌달과 셔츠)와 조금 비슷하기 때문입니다. 하지만 Dense 층을 사용하기 때문에 동일한 위치에 나타난 패턴만 재사용할 수 있습니다(반대로 합성곱 층은 훨씬 많은 정보를 전송합니다. 학습한 패턴을 이미지의 어느 위치에서나 감지할 수 있기 때문입니다. CNN 장에서 자세히 알아 보겠습니다).\n\ndef split_dataset(X, y):\n    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n    y_A = y[~y_5_or_6]\n    y_A[y_A &gt; 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n    return ((X[~y_5_or_6], y_A),\n            (X[y_5_or_6], y_B))\n\n(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\nX_train_B = X_train_B[:200]\ny_train_B = y_train_B[:200]\n\n\nX_train_A.shape\n\n(43986, 28, 28)\n\n\n\nX_train_B.shape\n\n(200, 28, 28)\n\n\n\ny_train_A[:30]\n\narray([4, 0, 5, 7, 7, 7, 4, 4, 3, 4, 0, 1, 6, 3, 4, 3, 2, 6, 5, 3, 4, 5,\n       1, 3, 4, 2, 0, 6, 7, 1], dtype=uint8)\n\n\n\ny_train_B[:30]\n\narray([1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n       0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1.], dtype=float32)\n\n\n\ntf.random.set_seed(42)\nnp.random.seed(42)\n\n\nmodel_A = keras.models.Sequential()\nmodel_A.add(keras.layers.Flatten(input_shape=[28, 28]))\nfor n_hidden in (300, 100, 50, 50, 50):\n    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\nmodel_A.add(keras.layers.Dense(8, activation=\"softmax\"))\n\n\nmodel_A.compile(loss=\"sparse_categorical_crossentropy\",\n                optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n                metrics=[\"accuracy\"])\n\n\nhistory = model_A.fit(X_train_A, y_train_A, epochs=20,\n                    validation_data=(X_valid_A, y_valid_A))\n\nEpoch 1/20\n1375/1375 [==============================] - 3s 2ms/step - loss: 0.5926 - accuracy: 0.8104 - val_loss: 0.3894 - val_accuracy: 0.8665\nEpoch 2/20\n1375/1375 [==============================] - 3s 2ms/step - loss: 0.3523 - accuracy: 0.8788 - val_loss: 0.3287 - val_accuracy: 0.8822\nEpoch 3/20\n1375/1375 [==============================] - 3s 2ms/step - loss: 0.3170 - accuracy: 0.8895 - val_loss: 0.3010 - val_accuracy: 0.8994\nEpoch 4/20\n1375/1375 [==============================] - 3s 2ms/step - loss: 0.2973 - accuracy: 0.8976 - val_loss: 0.2894 - val_accuracy: 0.9023\nEpoch 5/20\n1375/1375 [==============================] - 3s 2ms/step - loss: 0.2835 - accuracy: 0.9020 - val_loss: 0.2770 - val_accuracy: 0.9068\nEpoch 6/20\n1375/1375 [==============================] - 3s 2ms/step - loss: 0.2730 - accuracy: 0.9060 - val_loss: 0.2731 - val_accuracy: 0.9068\nEpoch 7/20\n1375/1375 [==============================] - 3s 2ms/step - loss: 0.2641 - accuracy: 0.9090 - val_loss: 0.2719 - val_accuracy: 0.9081\nEpoch 8/20\n1375/1375 [==============================] - 3s 2ms/step - loss: 0.2573 - accuracy: 0.9125 - val_loss: 0.2587 - val_accuracy: 0.9141\nEpoch 9/20\n1375/1375 [==============================] - 3s 2ms/step - loss: 0.2519 - accuracy: 0.9133 - val_loss: 0.2565 - val_accuracy: 0.9141\nEpoch 10/20\n1375/1375 [==============================] - 3s 2ms/step - loss: 0.2469 - accuracy: 0.9154 - val_loss: 0.2541 - val_accuracy: 0.9158\nEpoch 11/20\n1375/1375 [==============================] - 3s 2ms/step - loss: 0.2423 - accuracy: 0.9176 - val_loss: 0.2495 - val_accuracy: 0.9153\nEpoch 12/20\n1375/1375 [==============================] - 3s 2ms/step - loss: 0.2382 - accuracy: 0.9189 - val_loss: 0.2510 - val_accuracy: 0.9131\nEpoch 13/20\n1375/1375 [==============================] - 3s 2ms/step - loss: 0.2351 - accuracy: 0.9200 - val_loss: 0.2444 - val_accuracy: 0.9158\nEpoch 14/20\n1375/1375 [==============================] - 3s 2ms/step - loss: 0.2315 - accuracy: 0.9213 - val_loss: 0.2414 - val_accuracy: 0.9175\nEpoch 15/20\n1375/1375 [==============================] - 3s 2ms/step - loss: 0.2287 - accuracy: 0.9214 - val_loss: 0.2448 - val_accuracy: 0.9185\nEpoch 16/20\n1375/1375 [==============================] - 3s 2ms/step - loss: 0.2255 - accuracy: 0.9225 - val_loss: 0.2384 - val_accuracy: 0.9193\nEpoch 17/20\n1375/1375 [==============================] - 3s 2ms/step - loss: 0.2231 - accuracy: 0.9232 - val_loss: 0.2409 - val_accuracy: 0.9175\nEpoch 18/20\n1375/1375 [==============================] - 3s 2ms/step - loss: 0.2201 - accuracy: 0.9246 - val_loss: 0.2423 - val_accuracy: 0.9145\nEpoch 19/20\n1375/1375 [==============================] - 3s 2ms/step - loss: 0.2178 - accuracy: 0.9256 - val_loss: 0.2328 - val_accuracy: 0.9203\nEpoch 20/20\n1375/1375 [==============================] - 3s 2ms/step - loss: 0.2156 - accuracy: 0.9261 - val_loss: 0.2332 - val_accuracy: 0.9210\n\n\n\nmodel_A.save(\"my_model_A.h5\")\n\n\nmodel_B = keras.models.Sequential()\nmodel_B.add(keras.layers.Flatten(input_shape=[28, 28]))\nfor n_hidden in (300, 100, 50, 50, 50):\n    model_B.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\nmodel_B.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n\n\nmodel_B.compile(loss=\"binary_crossentropy\",\n                optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n                metrics=[\"accuracy\"])\n\n\nhistory = model_B.fit(X_train_B, y_train_B, epochs=20,\n                      validation_data=(X_valid_B, y_valid_B))\n\nEpoch 1/20\n7/7 [==============================] - 0s 28ms/step - loss: 0.9573 - accuracy: 0.4650 - val_loss: 0.6314 - val_accuracy: 0.6004\nEpoch 2/20\n7/7 [==============================] - 0s 10ms/step - loss: 0.5692 - accuracy: 0.7450 - val_loss: 0.4784 - val_accuracy: 0.8529\nEpoch 3/20\n7/7 [==============================] - 0s 10ms/step - loss: 0.4503 - accuracy: 0.8650 - val_loss: 0.4102 - val_accuracy: 0.8945\nEpoch 4/20\n7/7 [==============================] - 0s 11ms/step - loss: 0.3879 - accuracy: 0.8950 - val_loss: 0.3647 - val_accuracy: 0.9178\nEpoch 5/20\n7/7 [==============================] - 0s 10ms/step - loss: 0.3435 - accuracy: 0.9250 - val_loss: 0.3300 - val_accuracy: 0.9320\nEpoch 6/20\n7/7 [==============================] - 0s 10ms/step - loss: 0.3081 - accuracy: 0.9300 - val_loss: 0.3019 - val_accuracy: 0.9402\nEpoch 7/20\n7/7 [==============================] - 0s 10ms/step - loss: 0.2800 - accuracy: 0.9350 - val_loss: 0.2804 - val_accuracy: 0.9422\nEpoch 8/20\n7/7 [==============================] - 0s 11ms/step - loss: 0.2564 - accuracy: 0.9450 - val_loss: 0.2606 - val_accuracy: 0.9473\nEpoch 9/20\n7/7 [==============================] - 0s 10ms/step - loss: 0.2362 - accuracy: 0.9550 - val_loss: 0.2428 - val_accuracy: 0.9523\nEpoch 10/20\n7/7 [==============================] - 0s 11ms/step - loss: 0.2188 - accuracy: 0.9600 - val_loss: 0.2281 - val_accuracy: 0.9544\nEpoch 11/20\n7/7 [==============================] - 0s 10ms/step - loss: 0.2036 - accuracy: 0.9700 - val_loss: 0.2150 - val_accuracy: 0.9584\nEpoch 12/20\n7/7 [==============================] - 0s 10ms/step - loss: 0.1898 - accuracy: 0.9700 - val_loss: 0.2036 - val_accuracy: 0.9584\nEpoch 13/20\n7/7 [==============================] - 0s 10ms/step - loss: 0.1773 - accuracy: 0.9750 - val_loss: 0.1931 - val_accuracy: 0.9615\nEpoch 14/20\n7/7 [==============================] - 0s 10ms/step - loss: 0.1668 - accuracy: 0.9800 - val_loss: 0.1838 - val_accuracy: 0.9635\nEpoch 15/20\n7/7 [==============================] - 0s 10ms/step - loss: 0.1570 - accuracy: 0.9900 - val_loss: 0.1746 - val_accuracy: 0.9686\nEpoch 16/20\n7/7 [==============================] - 0s 10ms/step - loss: 0.1481 - accuracy: 0.9900 - val_loss: 0.1674 - val_accuracy: 0.9686\nEpoch 17/20\n7/7 [==============================] - 0s 10ms/step - loss: 0.1406 - accuracy: 0.9900 - val_loss: 0.1604 - val_accuracy: 0.9706\nEpoch 18/20\n7/7 [==============================] - 0s 11ms/step - loss: 0.1334 - accuracy: 0.9900 - val_loss: 0.1539 - val_accuracy: 0.9706\nEpoch 19/20\n7/7 [==============================] - 0s 10ms/step - loss: 0.1268 - accuracy: 0.9900 - val_loss: 0.1482 - val_accuracy: 0.9716\nEpoch 20/20\n7/7 [==============================] - 0s 14ms/step - loss: 0.1208 - accuracy: 0.9900 - val_loss: 0.1431 - val_accuracy: 0.9716\n\n\n\nmodel_B.summary()\n\nModel: \"sequential_7\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten_7 (Flatten)          (None, 784)               0         \n_________________________________________________________________\ndense_224 (Dense)            (None, 300)               235500    \n_________________________________________________________________\ndense_225 (Dense)            (None, 100)               30100     \n_________________________________________________________________\ndense_226 (Dense)            (None, 50)                5050      \n_________________________________________________________________\ndense_227 (Dense)            (None, 50)                2550      \n_________________________________________________________________\ndense_228 (Dense)            (None, 50)                2550      \n_________________________________________________________________\ndense_229 (Dense)            (None, 1)                 51        \n=================================================================\nTotal params: 275,801\nTrainable params: 275,801\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nmodel_A = keras.models.load_model(\"my_model_A.h5\")\nmodel_B_on_A = keras.models.Sequential(model_A.layers[:-1])\nmodel_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n\nmodel_B_on_A와 model_A는 층을 공유하기 때문에 하나를 훈련하면 두 모델이 업데이트됩니다. 이를 피하려면 model_A를 클론한 것을 사용해 model_B_on_A를 만들어야 합니다:\n\nmodel_A_clone = keras.models.clone_model(model_A)\nmodel_A_clone.set_weights(model_A.get_weights())\nmodel_B_on_A = keras.models.Sequential(model_A_clone.layers[:-1])\nmodel_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n\n\nfor layer in model_B_on_A.layers[:-1]:\n    layer.trainable = False\n\nmodel_B_on_A.compile(loss=\"binary_crossentropy\",\n                     optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n                     metrics=[\"accuracy\"])\n\n\nhistory = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n                           validation_data=(X_valid_B, y_valid_B))\n\nfor layer in model_B_on_A.layers[:-1]:\n    layer.trainable = True\n\nmodel_B_on_A.compile(loss=\"binary_crossentropy\",\n                     optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n                     metrics=[\"accuracy\"])\nhistory = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n                           validation_data=(X_valid_B, y_valid_B))\n\nEpoch 1/4\n7/7 [==============================] - 0s 27ms/step - loss: 0.2649 - accuracy: 0.9400 - val_loss: 0.2792 - val_accuracy: 0.9260\nEpoch 2/4\n7/7 [==============================] - 0s 9ms/step - loss: 0.2552 - accuracy: 0.9400 - val_loss: 0.2697 - val_accuracy: 0.9300\nEpoch 3/4\n7/7 [==============================] - 0s 10ms/step - loss: 0.2459 - accuracy: 0.9400 - val_loss: 0.2610 - val_accuracy: 0.9331\nEpoch 4/4\n7/7 [==============================] - 0s 10ms/step - loss: 0.2374 - accuracy: 0.9450 - val_loss: 0.2528 - val_accuracy: 0.9351\nEpoch 1/16\n7/7 [==============================] - 0s 27ms/step - loss: 0.2124 - accuracy: 0.9500 - val_loss: 0.2046 - val_accuracy: 0.9635\nEpoch 2/16\n7/7 [==============================] - 0s 10ms/step - loss: 0.1699 - accuracy: 0.9550 - val_loss: 0.1722 - val_accuracy: 0.9716\nEpoch 3/16\n7/7 [==============================] - 0s 10ms/step - loss: 0.1409 - accuracy: 0.9700 - val_loss: 0.1495 - val_accuracy: 0.9817\nEpoch 4/16\n7/7 [==============================] - 0s 10ms/step - loss: 0.1199 - accuracy: 0.9800 - val_loss: 0.1327 - val_accuracy: 0.9817\nEpoch 5/16\n7/7 [==============================] - 0s 10ms/step - loss: 0.1047 - accuracy: 0.9900 - val_loss: 0.1203 - val_accuracy: 0.9848\nEpoch 6/16\n7/7 [==============================] - 0s 10ms/step - loss: 0.0930 - accuracy: 0.9950 - val_loss: 0.1103 - val_accuracy: 0.9858\nEpoch 7/16\n7/7 [==============================] - 0s 10ms/step - loss: 0.0838 - accuracy: 0.9950 - val_loss: 0.1022 - val_accuracy: 0.9858\nEpoch 8/16\n7/7 [==============================] - 0s 11ms/step - loss: 0.0763 - accuracy: 0.9950 - val_loss: 0.0955 - val_accuracy: 0.9878\nEpoch 9/16\n7/7 [==============================] - 0s 10ms/step - loss: 0.0699 - accuracy: 0.9950 - val_loss: 0.0894 - val_accuracy: 0.9878\nEpoch 10/16\n7/7 [==============================] - 0s 10ms/step - loss: 0.0641 - accuracy: 0.9950 - val_loss: 0.0845 - val_accuracy: 0.9888\nEpoch 11/16\n7/7 [==============================] - 0s 10ms/step - loss: 0.0596 - accuracy: 0.9950 - val_loss: 0.0801 - val_accuracy: 0.9888\nEpoch 12/16\n7/7 [==============================] - 0s 10ms/step - loss: 0.0554 - accuracy: 1.0000 - val_loss: 0.0763 - val_accuracy: 0.9878\nEpoch 13/16\n7/7 [==============================] - 0s 10ms/step - loss: 0.0517 - accuracy: 1.0000 - val_loss: 0.0730 - val_accuracy: 0.9878\nEpoch 14/16\n7/7 [==============================] - 0s 10ms/step - loss: 0.0485 - accuracy: 1.0000 - val_loss: 0.0702 - val_accuracy: 0.9878\nEpoch 15/16\n7/7 [==============================] - 0s 10ms/step - loss: 0.0459 - accuracy: 1.0000 - val_loss: 0.0677 - val_accuracy: 0.9878\nEpoch 16/16\n7/7 [==============================] - 0s 10ms/step - loss: 0.0435 - accuracy: 1.0000 - val_loss: 0.0653 - val_accuracy: 0.9878\n\n\n마지막 점수는 어떤가요?\n\nmodel_B.evaluate(X_test_B, y_test_B)\n\n63/63 [==============================] - 0s 1ms/step - loss: 0.1408 - accuracy: 0.9705\n\n\n[0.1408407837152481, 0.9704999923706055]\n\n\n\nmodel_B_on_A.evaluate(X_test_B, y_test_B)\n\n63/63 [==============================] - 0s 1ms/step - loss: 0.0563 - accuracy: 0.9940\n\n\n[0.056250184774398804, 0.9940000176429749]\n\n\n훌륭하네요! 꽤 많은 정보를 전달했습니다: 오차율이 4.9배나 줄었네요!\n\n(100 - 97.05) / (100 - 99.40)\n\n4.916666666666718"
  },
  {
    "objectID": "Machine_Learning/11_training_deep_neural_networks.html#모멘텀-옵티마이저",
    "href": "Machine_Learning/11_training_deep_neural_networks.html#모멘텀-옵티마이저",
    "title": "11_training_deep_neural_networks",
    "section": "모멘텀 옵티마이저",
    "text": "모멘텀 옵티마이저\n\noptimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)"
  },
  {
    "objectID": "Machine_Learning/11_training_deep_neural_networks.html#네스테로프-가속-경사",
    "href": "Machine_Learning/11_training_deep_neural_networks.html#네스테로프-가속-경사",
    "title": "11_training_deep_neural_networks",
    "section": "네스테로프 가속 경사",
    "text": "네스테로프 가속 경사\n\noptimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)"
  },
  {
    "objectID": "Machine_Learning/11_training_deep_neural_networks.html#adagrad",
    "href": "Machine_Learning/11_training_deep_neural_networks.html#adagrad",
    "title": "11_training_deep_neural_networks",
    "section": "AdaGrad",
    "text": "AdaGrad\n\noptimizer = keras.optimizers.Adagrad(learning_rate=0.001)"
  },
  {
    "objectID": "Machine_Learning/11_training_deep_neural_networks.html#rmsprop",
    "href": "Machine_Learning/11_training_deep_neural_networks.html#rmsprop",
    "title": "11_training_deep_neural_networks",
    "section": "RMSProp",
    "text": "RMSProp\n\noptimizer = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)"
  },
  {
    "objectID": "Machine_Learning/11_training_deep_neural_networks.html#adam-옵티마이저",
    "href": "Machine_Learning/11_training_deep_neural_networks.html#adam-옵티마이저",
    "title": "11_training_deep_neural_networks",
    "section": "Adam 옵티마이저",
    "text": "Adam 옵티마이저\n\noptimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
  },
  {
    "objectID": "Machine_Learning/11_training_deep_neural_networks.html#adamax-옵티마이저",
    "href": "Machine_Learning/11_training_deep_neural_networks.html#adamax-옵티마이저",
    "title": "11_training_deep_neural_networks",
    "section": "Adamax 옵티마이저",
    "text": "Adamax 옵티마이저\n\noptimizer = keras.optimizers.Adamax(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
  },
  {
    "objectID": "Machine_Learning/11_training_deep_neural_networks.html#nadam-옵티마이저",
    "href": "Machine_Learning/11_training_deep_neural_networks.html#nadam-옵티마이저",
    "title": "11_training_deep_neural_networks",
    "section": "Nadam 옵티마이저",
    "text": "Nadam 옵티마이저\n\noptimizer = keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
  },
  {
    "objectID": "Machine_Learning/11_training_deep_neural_networks.html#학습률-스케줄링",
    "href": "Machine_Learning/11_training_deep_neural_networks.html#학습률-스케줄링",
    "title": "11_training_deep_neural_networks",
    "section": "학습률 스케줄링",
    "text": "학습률 스케줄링\n\n거듭제곱 스케줄링\nlr = lr0 / (1 + steps / s)**c * 케라스는 c=1과 s = 1 / decay을 사용합니다\n\noptimizer = keras.optimizers.SGD(learning_rate=0.01, decay=1e-4)\n\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n\n\nn_epochs = 25\nhistory = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n                    validation_data=(X_valid_scaled, y_valid))\n\nEpoch 1/25\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.4898 - accuracy: 0.8266 - val_loss: 0.4064 - val_accuracy: 0.8608\nEpoch 2/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.3792 - accuracy: 0.8654 - val_loss: 0.3731 - val_accuracy: 0.8720\nEpoch 3/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.3468 - accuracy: 0.8774 - val_loss: 0.3744 - val_accuracy: 0.8728\nEpoch 4/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.3259 - accuracy: 0.8848 - val_loss: 0.3509 - val_accuracy: 0.8792\nEpoch 5/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.3108 - accuracy: 0.8897 - val_loss: 0.3449 - val_accuracy: 0.8778\nEpoch 6/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2973 - accuracy: 0.8941 - val_loss: 0.3417 - val_accuracy: 0.8846\nEpoch 7/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2871 - accuracy: 0.8981 - val_loss: 0.3379 - val_accuracy: 0.8828\nEpoch 8/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2779 - accuracy: 0.9014 - val_loss: 0.3421 - val_accuracy: 0.8798\nEpoch 9/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2697 - accuracy: 0.9030 - val_loss: 0.3289 - val_accuracy: 0.8868\nEpoch 10/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2624 - accuracy: 0.9058 - val_loss: 0.3282 - val_accuracy: 0.8858\nEpoch 11/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2564 - accuracy: 0.9088 - val_loss: 0.3264 - val_accuracy: 0.8876\nEpoch 12/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2504 - accuracy: 0.9113 - val_loss: 0.3337 - val_accuracy: 0.8816\nEpoch 13/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2448 - accuracy: 0.9135 - val_loss: 0.3245 - val_accuracy: 0.8910\nEpoch 14/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2407 - accuracy: 0.9144 - val_loss: 0.3283 - val_accuracy: 0.8858\nEpoch 15/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2354 - accuracy: 0.9166 - val_loss: 0.3225 - val_accuracy: 0.8882\nEpoch 16/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2314 - accuracy: 0.9185 - val_loss: 0.3204 - val_accuracy: 0.8904\nEpoch 17/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2275 - accuracy: 0.9191 - val_loss: 0.3243 - val_accuracy: 0.8888\nEpoch 18/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2235 - accuracy: 0.9212 - val_loss: 0.3189 - val_accuracy: 0.8924\nEpoch 19/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2202 - accuracy: 0.9225 - val_loss: 0.3226 - val_accuracy: 0.8900\nEpoch 20/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2169 - accuracy: 0.9242 - val_loss: 0.3203 - val_accuracy: 0.8906\nEpoch 21/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2132 - accuracy: 0.9255 - val_loss: 0.3201 - val_accuracy: 0.8894\nEpoch 22/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2107 - accuracy: 0.9267 - val_loss: 0.3181 - val_accuracy: 0.8896\nEpoch 23/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2078 - accuracy: 0.9273 - val_loss: 0.3201 - val_accuracy: 0.8926\nEpoch 24/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2049 - accuracy: 0.9292 - val_loss: 0.3202 - val_accuracy: 0.8904\nEpoch 25/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2027 - accuracy: 0.9295 - val_loss: 0.3196 - val_accuracy: 0.8894\n\n\n\nimport math\n\nlearning_rate = 0.01\ndecay = 1e-4\nbatch_size = 32\nn_steps_per_epoch = math.ceil(len(X_train) / batch_size)\nepochs = np.arange(n_epochs)\nlrs = learning_rate / (1 + decay * epochs * n_steps_per_epoch)\n\nplt.plot(epochs, lrs,  \"o-\")\nplt.axis([0, n_epochs - 1, 0, 0.01])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Learning Rate\")\nplt.title(\"Power Scheduling\", fontsize=14)\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n지수 기반 스케줄링\nlr = lr0 * 0.1**(epoch / s)\n\ndef exponential_decay_fn(epoch):\n    return 0.01 * 0.1**(epoch / 20)\n\n\ndef exponential_decay(lr0, s):\n    def exponential_decay_fn(epoch):\n        return lr0 * 0.1**(epoch / s)\n    return exponential_decay_fn\n\nexponential_decay_fn = exponential_decay(lr0=0.01, s=20)\n\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\nn_epochs = 25\n\n\nlr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\nhistory = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n                    validation_data=(X_valid_scaled, y_valid),\n                    callbacks=[lr_scheduler])\n\nEpoch 1/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.8326 - accuracy: 0.7621 - val_loss: 1.0349 - val_accuracy: 0.7256\nEpoch 2/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.7964 - accuracy: 0.7646 - val_loss: 0.6616 - val_accuracy: 0.8154\nEpoch 3/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.6590 - accuracy: 0.8045 - val_loss: 0.9604 - val_accuracy: 0.7426\nEpoch 4/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.5828 - accuracy: 0.8258 - val_loss: 0.5403 - val_accuracy: 0.8402\nEpoch 5/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.5404 - accuracy: 0.8358 - val_loss: 0.6116 - val_accuracy: 0.8410\nEpoch 6/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.5186 - accuracy: 0.8456 - val_loss: 0.5160 - val_accuracy: 0.8506\nEpoch 7/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.4688 - accuracy: 0.8583 - val_loss: 0.5551 - val_accuracy: 0.8354\nEpoch 8/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.4353 - accuracy: 0.8694 - val_loss: 0.5710 - val_accuracy: 0.8452\nEpoch 9/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.3989 - accuracy: 0.8770 - val_loss: 0.5819 - val_accuracy: 0.8142\nEpoch 10/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.3863 - accuracy: 0.8810 - val_loss: 0.4887 - val_accuracy: 0.8764\nEpoch 11/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.3576 - accuracy: 0.8898 - val_loss: 0.4841 - val_accuracy: 0.8690\nEpoch 12/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.3277 - accuracy: 0.8957 - val_loss: 0.5037 - val_accuracy: 0.8642\nEpoch 13/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.3250 - accuracy: 0.9001 - val_loss: 0.4737 - val_accuracy: 0.8728\nEpoch 14/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2895 - accuracy: 0.9073 - val_loss: 0.4703 - val_accuracy: 0.8732\nEpoch 15/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2711 - accuracy: 0.9122 - val_loss: 0.5017 - val_accuracy: 0.8750\nEpoch 16/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2598 - accuracy: 0.9159 - val_loss: 0.4757 - val_accuracy: 0.8800\nEpoch 17/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2483 - accuracy: 0.9202 - val_loss: 0.4981 - val_accuracy: 0.8772\nEpoch 18/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2329 - accuracy: 0.9248 - val_loss: 0.4908 - val_accuracy: 0.8756\nEpoch 19/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2218 - accuracy: 0.9282 - val_loss: 0.5164 - val_accuracy: 0.8840\nEpoch 20/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2108 - accuracy: 0.9311 - val_loss: 0.5471 - val_accuracy: 0.8772\nEpoch 21/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.1995 - accuracy: 0.9352 - val_loss: 0.5695 - val_accuracy: 0.8814\nEpoch 22/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.1921 - accuracy: 0.9369 - val_loss: 0.5697 - val_accuracy: 0.8826\nEpoch 23/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.1832 - accuracy: 0.9413 - val_loss: 0.5817 - val_accuracy: 0.8774\nEpoch 24/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.1763 - accuracy: 0.9431 - val_loss: 0.5968 - val_accuracy: 0.8806\nEpoch 25/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.1683 - accuracy: 0.9457 - val_loss: 0.5972 - val_accuracy: 0.8812\n\n\n\nplt.plot(history.epoch, history.history[\"lr\"], \"o-\")\nplt.axis([0, n_epochs - 1, 0, 0.011])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Learning Rate\")\nplt.title(\"Exponential Scheduling\", fontsize=14)\nplt.grid(True)\nplt.show()\n\n\n\n\n이 스케줄 함수는 두 번째 매개변수로 현재 학습률을 받을 수 있습니다:\n\ndef exponential_decay_fn(epoch, lr):\n    return lr * 0.1**(1 / 20)\n\n에포크가 아니라 반복마다 학습률을 업데이트하려면 사용자 정의 콜백 클래스를 작성해야 합니다:\n\nK = keras.backend\n\nclass ExponentialDecay(keras.callbacks.Callback):\n    def __init__(self, s=40000):\n        super().__init__()\n        self.s = s\n\n    def on_batch_begin(self, batch, logs=None):\n        # 노트: 에포크마다 `batch` 매개변수가 재설정됩니다\n        lr = K.get_value(self.model.optimizer.lr)\n        K.set_value(self.model.optimizer.lr, lr * 0.1**(1 / self.s))\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        logs['lr'] = K.get_value(self.model.optimizer.lr)\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\nlr0 = 0.01\noptimizer = keras.optimizers.Nadam(learning_rate=lr0)\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\nn_epochs = 25\n\ns = 20 * len(X_train) // 32 # 20 에포크 동안 스텝 횟수 (배치 크기 = 32)\nexp_decay = ExponentialDecay(s)\nhistory = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n                    validation_data=(X_valid_scaled, y_valid),\n                    callbacks=[exp_decay])\n\nEpoch 1/25\n1719/1719 [==============================] - 6s 3ms/step - loss: 0.7978 - accuracy: 0.7641 - val_loss: 0.8657 - val_accuracy: 0.7200\nEpoch 2/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.6872 - accuracy: 0.7914 - val_loss: 0.6638 - val_accuracy: 0.8124\nEpoch 3/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.5742 - accuracy: 0.8197 - val_loss: 3.4342 - val_accuracy: 0.6938\nEpoch 4/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.5214 - accuracy: 0.8386 - val_loss: 0.6039 - val_accuracy: 0.8280\nEpoch 5/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.4828 - accuracy: 0.8483 - val_loss: 0.4606 - val_accuracy: 0.8646\nEpoch 6/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.4235 - accuracy: 0.8639 - val_loss: 0.4625 - val_accuracy: 0.8518\nEpoch 7/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.3910 - accuracy: 0.8737 - val_loss: 0.4413 - val_accuracy: 0.8616\nEpoch 8/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.3583 - accuracy: 0.8818 - val_loss: 0.4679 - val_accuracy: 0.8604\nEpoch 9/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.3352 - accuracy: 0.8899 - val_loss: 0.4638 - val_accuracy: 0.8672\nEpoch 10/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.3269 - accuracy: 0.8919 - val_loss: 0.4391 - val_accuracy: 0.8788\nEpoch 11/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2916 - accuracy: 0.9012 - val_loss: 0.4256 - val_accuracy: 0.8782\nEpoch 12/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2686 - accuracy: 0.9071 - val_loss: 0.4297 - val_accuracy: 0.8746\nEpoch 13/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2467 - accuracy: 0.9145 - val_loss: 0.4410 - val_accuracy: 0.8784\nEpoch 14/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2308 - accuracy: 0.9209 - val_loss: 0.4280 - val_accuracy: 0.8788\nEpoch 15/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2147 - accuracy: 0.9267 - val_loss: 0.3936 - val_accuracy: 0.8848\nEpoch 16/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.1965 - accuracy: 0.9324 - val_loss: 0.4200 - val_accuracy: 0.8892\nEpoch 17/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.1856 - accuracy: 0.9365 - val_loss: 0.4689 - val_accuracy: 0.8822\nEpoch 18/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.1709 - accuracy: 0.9419 - val_loss: 0.5048 - val_accuracy: 0.8876\nEpoch 19/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.1573 - accuracy: 0.9469 - val_loss: 0.5060 - val_accuracy: 0.8930\nEpoch 20/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.1475 - accuracy: 0.9497 - val_loss: 0.5078 - val_accuracy: 0.8900\nEpoch 21/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.1358 - accuracy: 0.9539 - val_loss: 0.5430 - val_accuracy: 0.8878\nEpoch 22/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.1288 - accuracy: 0.9567 - val_loss: 0.5410 - val_accuracy: 0.8924\nEpoch 23/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.1220 - accuracy: 0.9590 - val_loss: 0.5399 - val_accuracy: 0.8882\nEpoch 24/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.1141 - accuracy: 0.9626 - val_loss: 0.5942 - val_accuracy: 0.8914\nEpoch 25/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.1076 - accuracy: 0.9643 - val_loss: 0.6073 - val_accuracy: 0.8882\n\n\n\nn_steps = n_epochs * len(X_train) // 32\nsteps = np.arange(n_steps)\nlrs = lr0 * 0.1**(steps / s)\n\n\nplt.plot(steps, lrs, \"-\", linewidth=2)\nplt.axis([0, n_steps - 1, 0, lr0 * 1.1])\nplt.xlabel(\"Batch\")\nplt.ylabel(\"Learning Rate\")\nplt.title(\"Exponential Scheduling (per batch)\", fontsize=14)\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n기간별 고정 스케줄링\n\ndef piecewise_constant_fn(epoch):\n    if epoch &lt; 5:\n        return 0.01\n    elif epoch &lt; 15:\n        return 0.005\n    else:\n        return 0.001\n\n\ndef piecewise_constant(boundaries, values):\n    boundaries = np.array([0] + boundaries)\n    values = np.array(values)\n    def piecewise_constant_fn(epoch):\n        return values[np.argmax(boundaries &gt; epoch) - 1]\n    return piecewise_constant_fn\n\npiecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001])\n\n\nlr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\nn_epochs = 25\nhistory = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n                    validation_data=(X_valid_scaled, y_valid),\n                    callbacks=[lr_scheduler])\n\nEpoch 1/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.8454 - accuracy: 0.7551 - val_loss: 0.9107 - val_accuracy: 0.7354\nEpoch 2/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.8216 - accuracy: 0.7619 - val_loss: 0.6475 - val_accuracy: 0.7948\nEpoch 3/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.8380 - accuracy: 0.7576 - val_loss: 1.1238 - val_accuracy: 0.7248\nEpoch 4/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.8244 - accuracy: 0.7562 - val_loss: 1.2028 - val_accuracy: 0.6642\nEpoch 5/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.9605 - accuracy: 0.7073 - val_loss: 0.8275 - val_accuracy: 0.7606\nEpoch 6/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.6131 - accuracy: 0.8073 - val_loss: 0.6293 - val_accuracy: 0.8160\nEpoch 7/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.5336 - accuracy: 0.8332 - val_loss: 0.6297 - val_accuracy: 0.8248\nEpoch 8/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.5495 - accuracy: 0.8390 - val_loss: 0.6212 - val_accuracy: 0.8224\nEpoch 9/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.4908 - accuracy: 0.8481 - val_loss: 0.6210 - val_accuracy: 0.8492\nEpoch 10/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.4786 - accuracy: 0.8547 - val_loss: 0.6047 - val_accuracy: 0.8458\nEpoch 11/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.4746 - accuracy: 0.8554 - val_loss: 0.7762 - val_accuracy: 0.8494\nEpoch 12/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.4605 - accuracy: 0.8611 - val_loss: 0.5888 - val_accuracy: 0.8454\nEpoch 13/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.4830 - accuracy: 0.8597 - val_loss: 0.9160 - val_accuracy: 0.8108\nEpoch 14/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.4645 - accuracy: 0.8629 - val_loss: 0.7827 - val_accuracy: 0.8480\nEpoch 15/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.4433 - accuracy: 0.8641 - val_loss: 0.6753 - val_accuracy: 0.8426\nEpoch 16/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.3245 - accuracy: 0.8943 - val_loss: 0.5471 - val_accuracy: 0.8680\nEpoch 17/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2980 - accuracy: 0.9018 - val_loss: 0.5652 - val_accuracy: 0.8676\nEpoch 18/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2890 - accuracy: 0.9054 - val_loss: 0.5596 - val_accuracy: 0.8708\nEpoch 19/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2783 - accuracy: 0.9087 - val_loss: 0.6174 - val_accuracy: 0.8742\nEpoch 20/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2711 - accuracy: 0.9113 - val_loss: 0.6321 - val_accuracy: 0.8706\nEpoch 21/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2635 - accuracy: 0.9133 - val_loss: 0.6899 - val_accuracy: 0.8728\nEpoch 22/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2590 - accuracy: 0.9156 - val_loss: 0.6208 - val_accuracy: 0.8774\nEpoch 23/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2516 - accuracy: 0.9180 - val_loss: 0.6517 - val_accuracy: 0.8708\nEpoch 24/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2451 - accuracy: 0.9195 - val_loss: 0.6852 - val_accuracy: 0.8732\nEpoch 25/25\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2421 - accuracy: 0.9206 - val_loss: 0.6683 - val_accuracy: 0.8754\n\n\n\nplt.plot(history.epoch, [piecewise_constant_fn(epoch) for epoch in history.epoch], \"o-\")\nplt.axis([0, n_epochs - 1, 0, 0.011])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Learning Rate\")\nplt.title(\"Piecewise Constant Scheduling\", fontsize=14)\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n성능 기반 스케줄링\n\ntf.random.set_seed(42)\nnp.random.seed(42)\n\n\nlr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\noptimizer = keras.optimizers.SGD(learning_rate=0.02, momentum=0.9)\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\nn_epochs = 25\nhistory = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n                    validation_data=(X_valid_scaled, y_valid),\n                    callbacks=[lr_scheduler])\n\nEpoch 1/25\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.5889 - accuracy: 0.8079 - val_loss: 0.4879 - val_accuracy: 0.8516\nEpoch 2/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.4959 - accuracy: 0.8388 - val_loss: 0.6396 - val_accuracy: 0.8240\nEpoch 3/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.5203 - accuracy: 0.8412 - val_loss: 0.5057 - val_accuracy: 0.8576\nEpoch 4/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.5169 - accuracy: 0.8459 - val_loss: 0.4907 - val_accuracy: 0.8578\nEpoch 5/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.5305 - accuracy: 0.8484 - val_loss: 0.5726 - val_accuracy: 0.8306\nEpoch 6/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.5184 - accuracy: 0.8537 - val_loss: 0.5930 - val_accuracy: 0.8454\nEpoch 7/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.3131 - accuracy: 0.8913 - val_loss: 0.3942 - val_accuracy: 0.8722\nEpoch 8/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2618 - accuracy: 0.9037 - val_loss: 0.3978 - val_accuracy: 0.8732\nEpoch 9/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2385 - accuracy: 0.9118 - val_loss: 0.3797 - val_accuracy: 0.8828\nEpoch 10/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2240 - accuracy: 0.9181 - val_loss: 0.3976 - val_accuracy: 0.8916\nEpoch 11/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2106 - accuracy: 0.9227 - val_loss: 0.3871 - val_accuracy: 0.8896\nEpoch 12/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2015 - accuracy: 0.9250 - val_loss: 0.4809 - val_accuracy: 0.8680\nEpoch 13/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.1890 - accuracy: 0.9299 - val_loss: 0.4645 - val_accuracy: 0.8830\nEpoch 14/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.1777 - accuracy: 0.9339 - val_loss: 0.4485 - val_accuracy: 0.8768\nEpoch 15/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.1216 - accuracy: 0.9527 - val_loss: 0.4029 - val_accuracy: 0.8922\nEpoch 16/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.1057 - accuracy: 0.9593 - val_loss: 0.4127 - val_accuracy: 0.8952\nEpoch 17/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.0984 - accuracy: 0.9618 - val_loss: 0.4463 - val_accuracy: 0.8926\nEpoch 18/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.0895 - accuracy: 0.9653 - val_loss: 0.4632 - val_accuracy: 0.8898\nEpoch 19/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.0854 - accuracy: 0.9671 - val_loss: 0.4770 - val_accuracy: 0.8938\nEpoch 20/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.0636 - accuracy: 0.9765 - val_loss: 0.4725 - val_accuracy: 0.8928\nEpoch 21/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.0572 - accuracy: 0.9797 - val_loss: 0.4881 - val_accuracy: 0.8928\nEpoch 22/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.0540 - accuracy: 0.9807 - val_loss: 0.5021 - val_accuracy: 0.8934\nEpoch 23/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.0506 - accuracy: 0.9826 - val_loss: 0.5114 - val_accuracy: 0.8912\nEpoch 24/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.0472 - accuracy: 0.9838 - val_loss: 0.5236 - val_accuracy: 0.8942\nEpoch 25/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.0393 - accuracy: 0.9875 - val_loss: 0.5303 - val_accuracy: 0.8932\n\n\n\nplt.plot(history.epoch, history.history[\"lr\"], \"bo-\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Learning Rate\", color='b')\nplt.tick_params('y', colors='b')\nplt.gca().set_xlim(0, n_epochs - 1)\nplt.grid(True)\n\nax2 = plt.gca().twinx()\nax2.plot(history.epoch, history.history[\"val_loss\"], \"r^-\")\nax2.set_ylabel('Validation Loss', color='r')\nax2.tick_params('y', colors='r')\n\nplt.title(\"Reduce LR on Plateau\", fontsize=14)\nplt.show()\n\n\n\n\n\n\ntf.keras 스케줄러\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\ns = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\nlearning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\noptimizer = keras.optimizers.SGD(learning_rate)\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\nn_epochs = 25\nhistory = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n                    validation_data=(X_valid_scaled, y_valid))\n\nEpoch 1/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.4894 - accuracy: 0.8277 - val_loss: 0.4096 - val_accuracy: 0.8592\nEpoch 2/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.3820 - accuracy: 0.8652 - val_loss: 0.3740 - val_accuracy: 0.8700\nEpoch 3/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.3487 - accuracy: 0.8766 - val_loss: 0.3735 - val_accuracy: 0.8688\nEpoch 4/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.3264 - accuracy: 0.8837 - val_loss: 0.3494 - val_accuracy: 0.8796\nEpoch 5/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.3104 - accuracy: 0.8896 - val_loss: 0.3431 - val_accuracy: 0.8792\nEpoch 6/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2958 - accuracy: 0.8951 - val_loss: 0.3415 - val_accuracy: 0.8808\nEpoch 7/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2853 - accuracy: 0.8987 - val_loss: 0.3356 - val_accuracy: 0.8814\nEpoch 8/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2760 - accuracy: 0.9016 - val_loss: 0.3368 - val_accuracy: 0.8814\nEpoch 9/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2677 - accuracy: 0.9052 - val_loss: 0.3266 - val_accuracy: 0.8854\nEpoch 10/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2607 - accuracy: 0.9067 - val_loss: 0.3243 - val_accuracy: 0.8862\nEpoch 11/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2550 - accuracy: 0.9086 - val_loss: 0.3253 - val_accuracy: 0.8866\nEpoch 12/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2496 - accuracy: 0.9126 - val_loss: 0.3305 - val_accuracy: 0.8808\nEpoch 13/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2449 - accuracy: 0.9138 - val_loss: 0.3222 - val_accuracy: 0.8864\nEpoch 14/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2415 - accuracy: 0.9148 - val_loss: 0.3225 - val_accuracy: 0.8860\nEpoch 15/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2375 - accuracy: 0.9167 - val_loss: 0.3212 - val_accuracy: 0.8880\nEpoch 16/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2343 - accuracy: 0.9182 - val_loss: 0.3187 - val_accuracy: 0.8884\nEpoch 17/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2316 - accuracy: 0.9183 - val_loss: 0.3201 - val_accuracy: 0.8896\nEpoch 18/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2291 - accuracy: 0.9197 - val_loss: 0.3171 - val_accuracy: 0.8900\nEpoch 19/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2270 - accuracy: 0.9206 - val_loss: 0.3200 - val_accuracy: 0.8898\nEpoch 20/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2250 - accuracy: 0.9220 - val_loss: 0.3173 - val_accuracy: 0.8900\nEpoch 21/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2229 - accuracy: 0.9223 - val_loss: 0.3183 - val_accuracy: 0.8910\nEpoch 22/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2216 - accuracy: 0.9224 - val_loss: 0.3167 - val_accuracy: 0.8912\nEpoch 23/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2201 - accuracy: 0.9234 - val_loss: 0.3175 - val_accuracy: 0.8898\nEpoch 24/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2188 - accuracy: 0.9239 - val_loss: 0.3170 - val_accuracy: 0.8898\nEpoch 25/25\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.2179 - accuracy: 0.9241 - val_loss: 0.3169 - val_accuracy: 0.8908\n\n\n구간별 고정 스케줄링은 다음을 사용하세요:\n\nlearning_rate = keras.optimizers.schedules.PiecewiseConstantDecay(\n    boundaries=[5. * n_steps_per_epoch, 15. * n_steps_per_epoch],\n    values=[0.01, 0.005, 0.001])\n\n\n\n1사이클 스케줄링\n\nK = keras.backend\n\nclass ExponentialLearningRate(keras.callbacks.Callback):\n    def __init__(self, factor):\n        self.factor = factor\n        self.rates = []\n        self.losses = []\n    def on_batch_end(self, batch, logs):\n        self.rates.append(K.get_value(self.model.optimizer.lr))\n        self.losses.append(logs[\"loss\"])\n        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)\n\ndef find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n    init_weights = model.get_weights()\n    iterations = math.ceil(len(X) / batch_size) * epochs\n    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n    init_lr = K.get_value(model.optimizer.lr)\n    K.set_value(model.optimizer.lr, min_rate)\n    exp_lr = ExponentialLearningRate(factor)\n    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n                        callbacks=[exp_lr])\n    K.set_value(model.optimizer.lr, init_lr)\n    model.set_weights(init_weights)\n    return exp_lr.rates, exp_lr.losses\n\ndef plot_lr_vs_loss(rates, losses):\n    plt.plot(rates, losses)\n    plt.gca().set_xscale('log')\n    plt.hlines(min(losses), min(rates), max(rates))\n    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n    plt.xlabel(\"Learning rate\")\n    plt.ylabel(\"Loss\")\n\n경고: on_batch_end() 메서드에서 logs[\"loss\"]로 배치 손실을 모으지만 텐서플로 2.2.0에서 (에포크의) 평균 손실로 바뀌었습니다. (텐서플로 2.2 이상을 사용한다면) 이런 이유로 아래 그래프가 이전보다 훨씬 부드럽습니다. 이는 그래프에서 배치 손실이 폭주하기 시작하는 지점과 그렇지 않은 지점 사이에 지연이 있다는 뜻입니다. 따라서 변동이 심한 그래프에서는 조금 더 작은 학습률을 선택해야 합니다. 또한 ExponentialLearningRate 콜백을 조금 바꾸어 (현재 평균 손실과 이전 평균 손실을 기반으로) 배치 손실을 계산할 수 있습니다:\nclass ExponentialLearningRate(keras.callbacks.Callback):\n    def __init__(self, factor):\n        self.factor = factor\n        self.rates = []\n        self.losses = []\n    def on_epoch_begin(self, epoch, logs=None):\n        self.prev_loss = 0\n    def on_batch_end(self, batch, logs=None):\n        batch_loss = logs[\"loss\"] * (batch + 1) - self.prev_loss * batch\n        self.prev_loss = logs[\"loss\"]\n        self.rates.append(K.get_value(self.model.optimizer.lr))\n        self.losses.append(batch_loss)\n        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)\n\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n              metrics=[\"accuracy\"])\n\n\nbatch_size = 128\nrates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\nplot_lr_vs_loss(rates, losses)\n\n430/430 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.3861\n\n\n\n\n\n\nclass OneCycleScheduler(keras.callbacks.Callback):\n    def __init__(self, iterations, max_rate, start_rate=None,\n                 last_iterations=None, last_rate=None):\n        self.iterations = iterations\n        self.max_rate = max_rate\n        self.start_rate = start_rate or max_rate / 10\n        self.last_iterations = last_iterations or iterations // 10 + 1\n        self.half_iteration = (iterations - self.last_iterations) // 2\n        self.last_rate = last_rate or self.start_rate / 1000\n        self.iteration = 0\n    def _interpolate(self, iter1, iter2, rate1, rate2):\n        return ((rate2 - rate1) * (self.iteration - iter1)\n                / (iter2 - iter1) + rate1)\n    def on_batch_begin(self, batch, logs):\n        if self.iteration &lt; self.half_iteration:\n            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n        elif self.iteration &lt; 2 * self.half_iteration:\n            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n                                     self.max_rate, self.start_rate)\n        else:\n            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n                                     self.start_rate, self.last_rate)\n        self.iteration += 1\n        K.set_value(self.model.optimizer.lr, rate)\n\n\nn_epochs = 25\nonecycle = OneCycleScheduler(math.ceil(len(X_train) / batch_size) * n_epochs, max_rate=0.05)\nhistory = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n                    validation_data=(X_valid_scaled, y_valid),\n                    callbacks=[onecycle])\n\nEpoch 1/25\n430/430 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.7739 - val_loss: 0.4872 - val_accuracy: 0.8336\nEpoch 2/25\n430/430 [==============================] - 1s 3ms/step - loss: 0.4581 - accuracy: 0.8396 - val_loss: 0.4275 - val_accuracy: 0.8520\nEpoch 3/25\n430/430 [==============================] - 1s 3ms/step - loss: 0.4122 - accuracy: 0.8547 - val_loss: 0.4117 - val_accuracy: 0.8582\nEpoch 4/25\n430/430 [==============================] - 1s 3ms/step - loss: 0.3837 - accuracy: 0.8642 - val_loss: 0.3869 - val_accuracy: 0.8682\nEpoch 5/25\n430/430 [==============================] - 1s 3ms/step - loss: 0.3640 - accuracy: 0.8717 - val_loss: 0.3767 - val_accuracy: 0.8678\nEpoch 6/25\n430/430 [==============================] - 1s 3ms/step - loss: 0.3457 - accuracy: 0.8772 - val_loss: 0.3743 - val_accuracy: 0.8712\nEpoch 7/25\n430/430 [==============================] - 1s 3ms/step - loss: 0.3331 - accuracy: 0.8810 - val_loss: 0.3638 - val_accuracy: 0.8712\nEpoch 8/25\n430/430 [==============================] - 1s 3ms/step - loss: 0.3185 - accuracy: 0.8858 - val_loss: 0.3958 - val_accuracy: 0.8606\nEpoch 9/25\n430/430 [==============================] - 1s 3ms/step - loss: 0.3066 - accuracy: 0.8890 - val_loss: 0.3487 - val_accuracy: 0.8764\nEpoch 10/25\n430/430 [==============================] - 1s 3ms/step - loss: 0.2944 - accuracy: 0.8922 - val_loss: 0.3401 - val_accuracy: 0.8802\nEpoch 11/25\n430/430 [==============================] - 1s 3ms/step - loss: 0.2840 - accuracy: 0.8963 - val_loss: 0.3459 - val_accuracy: 0.8808\nEpoch 12/25\n430/430 [==============================] - 1s 3ms/step - loss: 0.2710 - accuracy: 0.9023 - val_loss: 0.3656 - val_accuracy: 0.8704\nEpoch 13/25\n430/430 [==============================] - 1s 3ms/step - loss: 0.2538 - accuracy: 0.9085 - val_loss: 0.3360 - val_accuracy: 0.8834\nEpoch 14/25\n430/430 [==============================] - 1s 3ms/step - loss: 0.2406 - accuracy: 0.9136 - val_loss: 0.3459 - val_accuracy: 0.8808\nEpoch 15/25\n430/430 [==============================] - 1s 3ms/step - loss: 0.2280 - accuracy: 0.9183 - val_loss: 0.3254 - val_accuracy: 0.8852\nEpoch 16/25\n430/430 [==============================] - 1s 3ms/step - loss: 0.2160 - accuracy: 0.9233 - val_loss: 0.3296 - val_accuracy: 0.8832\nEpoch 17/25\n430/430 [==============================] - 1s 3ms/step - loss: 0.2062 - accuracy: 0.9267 - val_loss: 0.3344 - val_accuracy: 0.8858\nEpoch 18/25\n430/430 [==============================] - 1s 3ms/step - loss: 0.1979 - accuracy: 0.9297 - val_loss: 0.3242 - val_accuracy: 0.8904\nEpoch 19/25\n430/430 [==============================] - 1s 3ms/step - loss: 0.1892 - accuracy: 0.9339 - val_loss: 0.3234 - val_accuracy: 0.8896\nEpoch 20/25\n430/430 [==============================] - 1s 3ms/step - loss: 0.1821 - accuracy: 0.9371 - val_loss: 0.3226 - val_accuracy: 0.8924\nEpoch 21/25\n430/430 [==============================] - 1s 3ms/step - loss: 0.1752 - accuracy: 0.9400 - val_loss: 0.3219 - val_accuracy: 0.8912\nEpoch 22/25\n430/430 [==============================] - 1s 3ms/step - loss: 0.1700 - accuracy: 0.9419 - val_loss: 0.3180 - val_accuracy: 0.8954\nEpoch 23/25\n430/430 [==============================] - 1s 3ms/step - loss: 0.1654 - accuracy: 0.9439 - val_loss: 0.3185 - val_accuracy: 0.8940\nEpoch 24/25\n430/430 [==============================] - 1s 3ms/step - loss: 0.1626 - accuracy: 0.9457 - val_loss: 0.3175 - val_accuracy: 0.8938\nEpoch 25/25\n430/430 [==============================] - 1s 3ms/step - loss: 0.1609 - accuracy: 0.9463 - val_loss: 0.3168 - val_accuracy: 0.8952"
  },
  {
    "objectID": "Machine_Learning/11_training_deep_neural_networks.html#ell_1과-ell_2-규제",
    "href": "Machine_Learning/11_training_deep_neural_networks.html#ell_1과-ell_2-규제",
    "title": "11_training_deep_neural_networks",
    "section": "\\(\\ell_1\\)과 \\(\\ell_2\\) 규제",
    "text": "\\(\\ell_1\\)과 \\(\\ell_2\\) 규제\n\nlayer = keras.layers.Dense(100, activation=\"elu\",\n                           kernel_initializer=\"he_normal\",\n                           kernel_regularizer=keras.regularizers.l2(0.01))\n# or l1(0.1) for ℓ1 regularization with a factor of 0.1\n# or l1_l2(0.1, 0.01) for both ℓ1 and ℓ2 regularization, with factors 0.1 and 0.01 respectively\n\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(300, activation=\"elu\",\n                       kernel_initializer=\"he_normal\",\n                       kernel_regularizer=keras.regularizers.l2(0.01)),\n    keras.layers.Dense(100, activation=\"elu\",\n                       kernel_initializer=\"he_normal\",\n                       kernel_regularizer=keras.regularizers.l2(0.01)),\n    keras.layers.Dense(10, activation=\"softmax\",\n                       kernel_regularizer=keras.regularizers.l2(0.01))\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\nn_epochs = 2\nhistory = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n                    validation_data=(X_valid_scaled, y_valid))\n\nEpoch 1/2\n1719/1719 [==============================] - 6s 3ms/step - loss: 1.5956 - accuracy: 0.8124 - val_loss: 0.7169 - val_accuracy: 0.8340\nEpoch 2/2\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.7197 - accuracy: 0.8274 - val_loss: 0.6850 - val_accuracy: 0.8376\n\n\n\nfrom functools import partial\n\nRegularizedDense = partial(keras.layers.Dense,\n                           activation=\"elu\",\n                           kernel_initializer=\"he_normal\",\n                           kernel_regularizer=keras.regularizers.l2(0.01))\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    RegularizedDense(300),\n    RegularizedDense(100),\n    RegularizedDense(10, activation=\"softmax\")\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\nn_epochs = 2\nhistory = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n                    validation_data=(X_valid_scaled, y_valid))\n\nEpoch 1/2\n1719/1719 [==============================] - 5s 3ms/step - loss: 1.6313 - accuracy: 0.8113 - val_loss: 0.7218 - val_accuracy: 0.8310\nEpoch 2/2\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.7187 - accuracy: 0.8273 - val_loss: 0.6826 - val_accuracy: 0.8382"
  },
  {
    "objectID": "Machine_Learning/11_training_deep_neural_networks.html#드롭아웃",
    "href": "Machine_Learning/11_training_deep_neural_networks.html#드롭아웃",
    "title": "11_training_deep_neural_networks",
    "section": "드롭아웃",
    "text": "드롭아웃\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dropout(rate=0.2),\n    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n    keras.layers.Dropout(rate=0.2),\n    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n    keras.layers.Dropout(rate=0.2),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\nn_epochs = 2\nhistory = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n                    validation_data=(X_valid_scaled, y_valid))\n\nEpoch 1/2\n1719/1719 [==============================] - 6s 3ms/step - loss: 0.5838 - accuracy: 0.7997 - val_loss: 0.3730 - val_accuracy: 0.8644\nEpoch 2/2\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.4209 - accuracy: 0.8442 - val_loss: 0.3396 - val_accuracy: 0.8720"
  },
  {
    "objectID": "Machine_Learning/11_training_deep_neural_networks.html#알파-드롭아웃",
    "href": "Machine_Learning/11_training_deep_neural_networks.html#알파-드롭아웃",
    "title": "11_training_deep_neural_networks",
    "section": "알파 드롭아웃",
    "text": "알파 드롭아웃\n\ntf.random.set_seed(42)\nnp.random.seed(42)\n\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.AlphaDropout(rate=0.2),\n    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.AlphaDropout(rate=0.2),\n    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.AlphaDropout(rate=0.2),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\noptimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\nn_epochs = 20\nhistory = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n                    validation_data=(X_valid_scaled, y_valid))\n\nEpoch 1/20\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.6641 - accuracy: 0.7594 - val_loss: 0.5788 - val_accuracy: 0.8446\nEpoch 2/20\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.5565 - accuracy: 0.7947 - val_loss: 0.5192 - val_accuracy: 0.8522\nEpoch 3/20\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.5206 - accuracy: 0.8075 - val_loss: 0.4896 - val_accuracy: 0.8598\nEpoch 4/20\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.5084 - accuracy: 0.8124 - val_loss: 0.4880 - val_accuracy: 0.8596\nEpoch 5/20\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.4949 - accuracy: 0.8188 - val_loss: 0.4237 - val_accuracy: 0.8694\nEpoch 6/20\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.4831 - accuracy: 0.8192 - val_loss: 0.4572 - val_accuracy: 0.8634\nEpoch 7/20\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.4720 - accuracy: 0.8252 - val_loss: 0.4702 - val_accuracy: 0.8632\nEpoch 8/20\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.4610 - accuracy: 0.8273 - val_loss: 0.4265 - val_accuracy: 0.8668\nEpoch 9/20\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.4618 - accuracy: 0.8278 - val_loss: 0.4330 - val_accuracy: 0.8750\nEpoch 10/20\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.4563 - accuracy: 0.8308 - val_loss: 0.4402 - val_accuracy: 0.8616\nEpoch 11/20\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.4491 - accuracy: 0.8323 - val_loss: 0.4245 - val_accuracy: 0.8722\nEpoch 12/20\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.4451 - accuracy: 0.8347 - val_loss: 0.5396 - val_accuracy: 0.8552\nEpoch 13/20\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.4384 - accuracy: 0.8377 - val_loss: 0.4285 - val_accuracy: 0.8770\nEpoch 14/20\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.4364 - accuracy: 0.8393 - val_loss: 0.4391 - val_accuracy: 0.8664\nEpoch 15/20\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.4326 - accuracy: 0.8386 - val_loss: 0.4343 - val_accuracy: 0.8696\nEpoch 16/20\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.4316 - accuracy: 0.8390 - val_loss: 0.4204 - val_accuracy: 0.8776\nEpoch 17/20\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.4279 - accuracy: 0.8400 - val_loss: 0.5390 - val_accuracy: 0.8598\nEpoch 18/20\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.4267 - accuracy: 0.8404 - val_loss: 0.4812 - val_accuracy: 0.8732\nEpoch 19/20\n1719/1719 [==============================] - 4s 3ms/step - loss: 0.4251 - accuracy: 0.8409 - val_loss: 0.4696 - val_accuracy: 0.8738\nEpoch 20/20\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.4184 - accuracy: 0.8431 - val_loss: 0.4388 - val_accuracy: 0.8740\n\n\n\nmodel.evaluate(X_test_scaled, y_test)\n\n313/313 [==============================] - 0s 1ms/step - loss: 0.4765 - accuracy: 0.8596\n\n\n[0.4765377938747406, 0.8596000075340271]\n\n\n\nmodel.evaluate(X_train_scaled, y_train)\n\n1719/1719 [==============================] - 2s 1ms/step - loss: 0.3489 - accuracy: 0.8833\n\n\n[0.3489045798778534, 0.8833272457122803]\n\n\n\nhistory = model.fit(X_train_scaled, y_train)\n\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.4226 - accuracy: 0.8425"
  },
  {
    "objectID": "Machine_Learning/11_training_deep_neural_networks.html#mc-드롭아웃",
    "href": "Machine_Learning/11_training_deep_neural_networks.html#mc-드롭아웃",
    "title": "11_training_deep_neural_networks",
    "section": "MC 드롭아웃",
    "text": "MC 드롭아웃\n\ntf.random.set_seed(42)\nnp.random.seed(42)\n\n\ny_probas = np.stack([model(X_test_scaled, training=True)\n                     for sample in range(100)])\ny_proba = y_probas.mean(axis=0)\ny_std = y_probas.std(axis=0)\n\n\nnp.round(model.predict(X_test_scaled[:1]), 2)\n\narray([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)\n\n\n\nnp.round(y_probas[:, :1], 2)\n\narray([[[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.65, 0.  , 0.33]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.89, 0.  , 0.09]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.99]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.06, 0.  , 0.93]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.27, 0.  , 0.72]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.53, 0.  , 0.46]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.2 , 0.  , 0.31, 0.  , 0.49]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.13, 0.  , 0.31, 0.  , 0.56]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.38, 0.  , 0.07, 0.  , 0.55]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.05, 0.  , 0.94]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.43, 0.  , 0.25, 0.  , 0.31]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.11, 0.  , 0.86]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.1 , 0.  , 0.79]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.31, 0.  , 0.61]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.37, 0.  , 0.08, 0.  , 0.55]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.  , 0.  , 0.96]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.49, 0.  , 0.47]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.15, 0.  , 0.84]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.  , 0.  , 0.96]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.93, 0.01, 0.  , 0.  , 0.06]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.02, 0.  , 0.89]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.91]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.03, 0.  , 0.94]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.79, 0.  , 0.17]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.06, 0.  , 0.91]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.23, 0.  , 0.33, 0.  , 0.44]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.05, 0.  , 0.9 ]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.25, 0.  , 0.5 , 0.  , 0.25]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.65, 0.  , 0.01, 0.  , 0.34]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.18, 0.  , 0.74, 0.  , 0.08]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.08, 0.  , 0.9 ]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.77, 0.  , 0.16]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.13, 0.  , 0.84]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.23, 0.  , 0.69]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.48, 0.  , 0.03, 0.  , 0.49]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.07, 0.  , 0.92]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.5 , 0.  , 0.29, 0.  , 0.21]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.02, 0.  , 0.97]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.12, 0.  , 0.08, 0.  , 0.8 ]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.47, 0.  , 0.53]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.2 , 0.  , 0.78]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.42, 0.  , 0.25, 0.  , 0.33]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.83, 0.  , 0.1 ]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.19, 0.  , 0.04, 0.  , 0.77]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.01, 0.  , 0.88]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.15, 0.  , 0.85]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.15, 0.  , 0.82]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.23, 0.  , 0.72]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.22, 0.  , 0.2 , 0.  , 0.57]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.13, 0.  , 0.02, 0.  , 0.84]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.15, 0.  , 0.84]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.01, 0.  , 0.95]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.12, 0.  , 0.87]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.36, 0.  , 0.55, 0.  , 0.09]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.41, 0.  , 0.43, 0.  , 0.16]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.14, 0.  , 0.47, 0.  , 0.39]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.54, 0.  , 0.37]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.52, 0.  , 0.41]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.08, 0.  , 0.89]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.12, 0.  , 0.81]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.09, 0.  , 0.89]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.12, 0.  , 0.26, 0.  , 0.62]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.26, 0.  , 0.01, 0.  , 0.72]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.07, 0.  , 0.92]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.65, 0.  , 0.29]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.27, 0.  , 0.72]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.08, 0.  , 0.91]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.96]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.01, 0.  , 0.98]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.04, 0.  , 0.94]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.41, 0.  , 0.54]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.96, 0.  , 0.02, 0.  , 0.01]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.13, 0.  , 0.02, 0.  , 0.85]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.97]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.06, 0.  , 0.9 ]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.08, 0.  , 0.87]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.24, 0.  , 0.56, 0.  , 0.2 ]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.31, 0.  , 0.14, 0.  , 0.56]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.08, 0.  , 0.9 ]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.99]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.27, 0.  , 0.72]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.97]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.28, 0.  , 0.19, 0.  , 0.52]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.26, 0.  , 0.17, 0.  , 0.57]]],\n      dtype=float32)\n\n\n\nnp.round(y_proba[:1], 2)\n\narray([[0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.18, 0.  , 0.71]],\n      dtype=float32)\n\n\n\ny_std = y_probas.std(axis=0)\nnp.round(y_std[:1], 2)\n\narray([[0.  , 0.  , 0.  , 0.  , 0.  , 0.18, 0.  , 0.22, 0.  , 0.29]],\n      dtype=float32)\n\n\n\ny_pred = np.argmax(y_proba, axis=1)\n\n\naccuracy = np.sum(y_pred == y_test) / len(y_test)\naccuracy\n\n0.8627\n\n\n\nclass MCDropout(keras.layers.Dropout):\n    def call(self, inputs):\n        return super().call(inputs, training=True)\n\nclass MCAlphaDropout(keras.layers.AlphaDropout):\n    def call(self, inputs):\n        return super().call(inputs, training=True)\n\n\ntf.random.set_seed(42)\nnp.random.seed(42)\n\n\nmc_model = keras.models.Sequential([\n    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n    for layer in model.layers\n])\n\n\nmc_model.summary()\n\nModel: \"sequential_21\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten_18 (Flatten)         (None, 784)               0         \n_________________________________________________________________\nmc_alpha_dropout (MCAlphaDro (None, 784)               0         \n_________________________________________________________________\ndense_263 (Dense)            (None, 300)               235500    \n_________________________________________________________________\nmc_alpha_dropout_1 (MCAlphaD (None, 300)               0         \n_________________________________________________________________\ndense_264 (Dense)            (None, 100)               30100     \n_________________________________________________________________\nmc_alpha_dropout_2 (MCAlphaD (None, 100)               0         \n_________________________________________________________________\ndense_265 (Dense)            (None, 10)                1010      \n=================================================================\nTotal params: 266,610\nTrainable params: 266,610\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\noptimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\nmc_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n\n\nmc_model.set_weights(model.get_weights())\n\n이제 MC 드롭아웃을 모델에 사용할 수 있습니다:\n\nnp.round(np.mean([mc_model.predict(X_test_scaled[:1]) for sample in range(100)], axis=0), 2)\n\narray([[0.  , 0.  , 0.  , 0.  , 0.  , 0.16, 0.  , 0.22, 0.  , 0.62]],\n      dtype=float32)"
  },
  {
    "objectID": "Machine_Learning/11_training_deep_neural_networks.html#맥스-노름",
    "href": "Machine_Learning/11_training_deep_neural_networks.html#맥스-노름",
    "title": "11_training_deep_neural_networks",
    "section": "맥스 노름",
    "text": "맥스 노름\n\nlayer = keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                           kernel_constraint=keras.constraints.max_norm(1.))\n\n\nMaxNormDense = partial(keras.layers.Dense,\n                       activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                       kernel_constraint=keras.constraints.max_norm(1.))\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    MaxNormDense(300),\n    MaxNormDense(100),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\nn_epochs = 2\nhistory = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n                    validation_data=(X_valid_scaled, y_valid))\n\nEpoch 1/2\n1719/1719 [==============================] - 6s 3ms/step - loss: 0.4747 - accuracy: 0.8329 - val_loss: 0.3831 - val_accuracy: 0.8564\nEpoch 2/2\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.3536 - accuracy: 0.8716 - val_loss: 0.3676 - val_accuracy: 0.8670"
  },
  {
    "objectID": "Machine_Learning/11_training_deep_neural_networks.html#to-7.",
    "href": "Machine_Learning/11_training_deep_neural_networks.html#to-7.",
    "title": "11_training_deep_neural_networks",
    "section": "1. to 7.",
    "text": "1. to 7.\n부록 A 참조."
  },
  {
    "objectID": "Machine_Learning/11_training_deep_neural_networks.html#cifar10에서-딥러닝",
    "href": "Machine_Learning/11_training_deep_neural_networks.html#cifar10에서-딥러닝",
    "title": "11_training_deep_neural_networks",
    "section": "8. CIFAR10에서 딥러닝",
    "text": "8. CIFAR10에서 딥러닝\n\na.\n문제: 100개의 뉴런을 가진 은닉층 20개로 심층 신경망을 만들어보세요(너무 많은 것 같지만 이 연습문제의 핵심입니다). He 초기화와 ELU 활성화 함수를 사용하세요.\n\nkeras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\nfor _ in range(20):\n    model.add(keras.layers.Dense(100,\n                                 activation=\"elu\",\n                                 kernel_initializer=\"he_normal\"))\n\n\n\nb.\n문제: Nadam 옵티마이저와 조기 종료를 사용하여 CIFAR10 데이터셋에 이 네트워크를 훈련하세요. keras.datasets.cifar10.load_ data()를 사용하여 데이터를 적재할 수 있습니다. 이 데이터셋은 10개의 클래스와 32×32 크기의 컬러 이미지 60,000개로 구성됩니다(50,000개는 훈련, 10,000개는 테스트). 따라서 10개의 뉴런과 소프트맥스 활성화 함수를 사용하는 출력층이 필요합니다. 모델 구조와 하이퍼파라미터를 바꿀 때마다 적절한 학습률을 찾아야 한다는 것을 기억하세요.\n모델에 출력층을 추가합니다:\n\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))\n\n학습률 5e-5인 Nadam 옵티마이저를 사용해 보죠. 학습률 1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2를 테스트하고 10번의 에포크 동안 (아래 텐서보드 콜백으로) 학습 곡선을 비교해 보았습니다. 학습률 3e-5와 1e-4가 꽤 좋았기 때문에 5e-5를 시도해 보았고 조금 더 나은 결과를 냈습니다.\n\noptimizer = keras.optimizers.Nadam(learning_rate=5e-5)\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=optimizer,\n              metrics=[\"accuracy\"])\n\nCIFAR10 데이터셋을 로드하죠. 조기 종료를 사용하기 때문에 검증 세트가 필요합니다. 원본 훈련 세트에서 처음 5,000개를 검증 세트로 사용하겠습니다:\n\n(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n\nX_train = X_train_full[5000:]\ny_train = y_train_full[5000:]\nX_valid = X_train_full[:5000]\ny_valid = y_train_full[:5000]\n\n이제 콜백을 만들고 모델을 훈련합니다:\n\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\nmodel_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_model.h5\", save_best_only=True)\nrun_index = 1 # 모델을 훈련할 때마다 증가시킴\nrun_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_{:03d}\".format(run_index))\ntensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\ncallbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n\n2021-10-10 02:00:32.263585: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n2021-10-10 02:00:32.263638: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n2021-10-10 02:00:32.263724: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n\n\n코랩에서 아래 셀에 있는 %tensorboard 명령을 실행하면 다음과 같은 텐서보드 화면을 볼 수 있습니다.\n\n\n\n스크린샷 2021-02-17 오후 11.42.41.png\n\n\n\n%tensorboard --logdir=./my_cifar10_logs --port=6006\n\n\n      \n      \n      \n    \n\n\n\nmodel.fit(X_train, y_train, epochs=100,\n          validation_data=(X_valid, y_valid),\n          callbacks=callbacks)\n\nEpoch 1/100\n  22/1407 [..............................] - ETA: 16s - loss: 55.1513 - accuracy: 0.12071407/1407 [==============================] - 12s 6ms/step - loss: 4.2185 - accuracy: 0.1574 - val_loss: 2.1635 - val_accuracy: 0.2170\nEpoch 2/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 2.0720 - accuracy: 0.2463 - val_loss: 2.0470 - val_accuracy: 0.2470\nEpoch 3/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.9429 - accuracy: 0.2905 - val_loss: 1.9534 - val_accuracy: 0.2886\nEpoch 4/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.8597 - accuracy: 0.3232 - val_loss: 1.8771 - val_accuracy: 0.3340\nEpoch 5/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.8019 - accuracy: 0.3426 - val_loss: 1.8094 - val_accuracy: 0.3466\nEpoch 6/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.7501 - accuracy: 0.3665 - val_loss: 1.7618 - val_accuracy: 0.3708\nEpoch 7/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.7086 - accuracy: 0.3802 - val_loss: 1.7529 - val_accuracy: 0.3648\nEpoch 8/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.6742 - accuracy: 0.3963 - val_loss: 1.6654 - val_accuracy: 0.3974\nEpoch 9/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.6430 - accuracy: 0.4063 - val_loss: 1.6337 - val_accuracy: 0.4082\nEpoch 10/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.6197 - accuracy: 0.4140 - val_loss: 1.6689 - val_accuracy: 0.4024\nEpoch 11/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.5987 - accuracy: 0.4226 - val_loss: 1.6639 - val_accuracy: 0.4046\nEpoch 12/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.5758 - accuracy: 0.4297 - val_loss: 1.6391 - val_accuracy: 0.4022\nEpoch 13/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.5559 - accuracy: 0.4358 - val_loss: 1.6196 - val_accuracy: 0.4108\nEpoch 14/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.5429 - accuracy: 0.4430 - val_loss: 1.6304 - val_accuracy: 0.4172\nEpoch 15/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.5268 - accuracy: 0.4497 - val_loss: 1.5864 - val_accuracy: 0.4298\nEpoch 16/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.5095 - accuracy: 0.4554 - val_loss: 1.5616 - val_accuracy: 0.4438\nEpoch 17/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.4956 - accuracy: 0.4614 - val_loss: 1.5776 - val_accuracy: 0.4400\nEpoch 18/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.4803 - accuracy: 0.4684 - val_loss: 1.6018 - val_accuracy: 0.4248\nEpoch 19/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.4687 - accuracy: 0.4696 - val_loss: 1.5597 - val_accuracy: 0.4402\nEpoch 20/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.4562 - accuracy: 0.4734 - val_loss: 1.5343 - val_accuracy: 0.4492\nEpoch 21/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.4422 - accuracy: 0.4806 - val_loss: 1.5665 - val_accuracy: 0.4384\nEpoch 22/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.4297 - accuracy: 0.4844 - val_loss: 1.5450 - val_accuracy: 0.4450\nEpoch 23/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.4207 - accuracy: 0.4896 - val_loss: 1.5538 - val_accuracy: 0.4478\nEpoch 24/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.4067 - accuracy: 0.4929 - val_loss: 1.5521 - val_accuracy: 0.4400\nEpoch 25/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.4008 - accuracy: 0.4956 - val_loss: 1.5262 - val_accuracy: 0.4514\nEpoch 26/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.3853 - accuracy: 0.4983 - val_loss: 1.5717 - val_accuracy: 0.4388\nEpoch 27/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.3736 - accuracy: 0.5059 - val_loss: 1.5212 - val_accuracy: 0.4598\nEpoch 28/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.3686 - accuracy: 0.5076 - val_loss: 1.5759 - val_accuracy: 0.4458\nEpoch 29/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.3565 - accuracy: 0.5109 - val_loss: 1.4968 - val_accuracy: 0.4686\nEpoch 30/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.3459 - accuracy: 0.5141 - val_loss: 1.5707 - val_accuracy: 0.4494\nEpoch 31/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.3368 - accuracy: 0.5203 - val_loss: 1.5303 - val_accuracy: 0.4542\nEpoch 32/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.3293 - accuracy: 0.5209 - val_loss: 1.5137 - val_accuracy: 0.4608\nEpoch 33/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.3219 - accuracy: 0.5219 - val_loss: 1.5409 - val_accuracy: 0.4592\nEpoch 34/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.3153 - accuracy: 0.5272 - val_loss: 1.5710 - val_accuracy: 0.4514\nEpoch 35/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.3043 - accuracy: 0.5298 - val_loss: 1.5353 - val_accuracy: 0.4576\nEpoch 36/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.2980 - accuracy: 0.5328 - val_loss: 1.5232 - val_accuracy: 0.4652\nEpoch 37/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.2887 - accuracy: 0.5342 - val_loss: 1.5187 - val_accuracy: 0.4662\nEpoch 38/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.2783 - accuracy: 0.5399 - val_loss: 1.5217 - val_accuracy: 0.4688\nEpoch 39/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.2728 - accuracy: 0.5413 - val_loss: 1.5294 - val_accuracy: 0.4650\nEpoch 40/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.2647 - accuracy: 0.5430 - val_loss: 1.5035 - val_accuracy: 0.4694\nEpoch 41/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.2586 - accuracy: 0.5484 - val_loss: 1.5070 - val_accuracy: 0.4670\nEpoch 42/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.2520 - accuracy: 0.5488 - val_loss: 1.5238 - val_accuracy: 0.4670\nEpoch 43/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.2403 - accuracy: 0.5514 - val_loss: 1.5230 - val_accuracy: 0.4648\nEpoch 44/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.2356 - accuracy: 0.5552 - val_loss: 1.5581 - val_accuracy: 0.4540\nEpoch 45/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.2231 - accuracy: 0.5590 - val_loss: 1.5044 - val_accuracy: 0.4722\nEpoch 46/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.2191 - accuracy: 0.5606 - val_loss: 1.5136 - val_accuracy: 0.4720\nEpoch 47/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.2086 - accuracy: 0.5625 - val_loss: 1.5134 - val_accuracy: 0.4690\nEpoch 48/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.2042 - accuracy: 0.5661 - val_loss: 1.5057 - val_accuracy: 0.4742\nEpoch 49/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.1936 - accuracy: 0.5680 - val_loss: 1.5303 - val_accuracy: 0.4734\n\n\n2021-10-10 02:00:37.792046: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n2021-10-10 02:00:37.792093: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n2021-10-10 02:00:37.801048: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n2021-10-10 02:00:37.806955: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n2021-10-10 02:00:37.817439: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./my_cifar10_logs/run_001/train/plugins/profile/2021_10_10_02_00_37\n\n2021-10-10 02:00:37.823185: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to ./my_cifar10_logs/run_001/train/plugins/profile/2021_10_10_02_00_37/instance-1.trace.json.gz\n2021-10-10 02:00:37.833949: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./my_cifar10_logs/run_001/train/plugins/profile/2021_10_10_02_00_37\n\n2021-10-10 02:00:37.834168: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to ./my_cifar10_logs/run_001/train/plugins/profile/2021_10_10_02_00_37/instance-1.memory_profile.json.gz\n2021-10-10 02:00:37.834914: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./my_cifar10_logs/run_001/train/plugins/profile/2021_10_10_02_00_37\nDumped tool data for xplane.pb to ./my_cifar10_logs/run_001/train/plugins/profile/2021_10_10_02_00_37/instance-1.xplane.pb\nDumped tool data for overview_page.pb to ./my_cifar10_logs/run_001/train/plugins/profile/2021_10_10_02_00_37/instance-1.overview_page.pb\nDumped tool data for input_pipeline.pb to ./my_cifar10_logs/run_001/train/plugins/profile/2021_10_10_02_00_37/instance-1.input_pipeline.pb\nDumped tool data for tensorflow_stats.pb to ./my_cifar10_logs/run_001/train/plugins/profile/2021_10_10_02_00_37/instance-1.tensorflow_stats.pb\nDumped tool data for kernel_stats.pb to ./my_cifar10_logs/run_001/train/plugins/profile/2021_10_10_02_00_37/instance-1.kernel_stats.pb\n\n\n\n&lt;keras.callbacks.History at 0x7f97b20c00b8&gt;\n\n\n\nmodel = keras.models.load_model(\"my_cifar10_model.h5\")\nmodel.evaluate(X_valid, y_valid)\n\n157/157 [==============================] - 1s 2ms/step - loss: 1.4968 - accuracy: 0.4686\n\n\n[1.4967584609985352, 0.46860000491142273]\n\n\n가장 낮은 검증 손실을 내는 모델은 검증 세트에서 약 47.6% 정확도를 얻었습니다. 이 검증 점수에 도달하는데 27번의 에포크가 걸렸습니다. (GPU가 없는) 제 노트북에서 에포크당 약 8초 정도 걸렸습니다. 배치 정규화를 사용해 성능을 올릴 수 있는지 확인해 보죠.\n\n\nc.\n문제: 배치 정규화를 추가하고 학습 곡선을 비교해보세요. 이전보다 빠르게 수렴하나요? 더 좋은 모델이 만들어지나요? 훈련 속도에는 어떤 영향을 미치나요?\n다음 코드는 위의 코드와 배우 비슷합니다. 몇 가지 다른 점은 아래와 같습니다:\n\n출력층을 제외하고 모든 Dense 층 다음에 (활성화 함수 전에) BN 층을 추가했습니다. 처음 은닉층 전에도 BN 층을 추가했습니다.\n학습률을 5e-4로 바꾸었습니다. 1e-5, 3e-5, 5e-5, 1e-4, 3e-4, 5e-4, 1e-3, 3e-3를 시도해 보고 20번 에포크 후에 검증 세트 성능이 가장 좋은 것을 선택했습니다.\nrun_logdir를 run_bn_* 으로 이름을 바꾸고 모델 파일 이름을 my_cifar10_bn_model.h5로 변경했습니다.\n\n\nkeras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\nmodel.add(keras.layers.BatchNormalization())\nfor _ in range(20):\n    model.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\"))\n    model.add(keras.layers.BatchNormalization())\n    model.add(keras.layers.Activation(\"elu\"))\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))\n\noptimizer = keras.optimizers.Nadam(learning_rate=5e-4)\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=optimizer,\n              metrics=[\"accuracy\"])\n\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\nmodel_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_bn_model.h5\", save_best_only=True)\nrun_index = 1 # 모델을 훈련할 때마다 증가시킴\nrun_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_bn_{:03d}\".format(run_index))\ntensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\ncallbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n\nmodel.fit(X_train, y_train, epochs=100,\n          validation_data=(X_valid, y_valid),\n          callbacks=callbacks)\n\nmodel = keras.models.load_model(\"my_cifar10_bn_model.h5\")\nmodel.evaluate(X_valid, y_valid)\n\n2021-10-10 02:07:32.842926: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n2021-10-10 02:07:32.843104: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n2021-10-10 02:07:32.843403: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n2021-10-10 02:07:40.561943: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n2021-10-10 02:07:40.562214: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n2021-10-10 02:07:41.055928: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n2021-10-10 02:07:41.078270: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n2021-10-10 02:07:41.108257: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./my_cifar10_logs/run_bn_001/train/plugins/profile/2021_10_10_02_07_41\n\n2021-10-10 02:07:41.122373: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to ./my_cifar10_logs/run_bn_001/train/plugins/profile/2021_10_10_02_07_41/instance-1.trace.json.gz\n2021-10-10 02:07:41.155742: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./my_cifar10_logs/run_bn_001/train/plugins/profile/2021_10_10_02_07_41\n\n2021-10-10 02:07:41.156074: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to ./my_cifar10_logs/run_bn_001/train/plugins/profile/2021_10_10_02_07_41/instance-1.memory_profile.json.gz\n2021-10-10 02:07:41.158809: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./my_cifar10_logs/run_bn_001/train/plugins/profile/2021_10_10_02_07_41\nDumped tool data for xplane.pb to ./my_cifar10_logs/run_bn_001/train/plugins/profile/2021_10_10_02_07_41/instance-1.xplane.pb\nDumped tool data for overview_page.pb to ./my_cifar10_logs/run_bn_001/train/plugins/profile/2021_10_10_02_07_41/instance-1.overview_page.pb\nDumped tool data for input_pipeline.pb to ./my_cifar10_logs/run_bn_001/train/plugins/profile/2021_10_10_02_07_41/instance-1.input_pipeline.pb\nDumped tool data for tensorflow_stats.pb to ./my_cifar10_logs/run_bn_001/train/plugins/profile/2021_10_10_02_07_41/instance-1.tensorflow_stats.pb\nDumped tool data for kernel_stats.pb to ./my_cifar10_logs/run_bn_001/train/plugins/profile/2021_10_10_02_07_41/instance-1.kernel_stats.pb\n\n\n\nEpoch 1/100\n   2/1407 [..............................] - ETA: 5:02 - loss: 2.8693 - accuracy: 0.1094      8/1407 [..............................] - ETA: 2:55 - loss: 2.7177 - accuracy: 0.10161407/1407 [==============================] - 25s 13ms/step - loss: 1.8375 - accuracy: 0.3419 - val_loss: 1.6449 - val_accuracy: 0.4120\nEpoch 2/100\n1407/1407 [==============================] - 17s 12ms/step - loss: 1.6605 - accuracy: 0.4096 - val_loss: 1.6076 - val_accuracy: 0.4172\nEpoch 3/100\n1407/1407 [==============================] - 16s 12ms/step - loss: 1.5923 - accuracy: 0.4328 - val_loss: 1.5143 - val_accuracy: 0.4638\nEpoch 4/100\n1407/1407 [==============================] - 16s 12ms/step - loss: 1.5420 - accuracy: 0.4536 - val_loss: 1.5096 - val_accuracy: 0.4654\nEpoch 5/100\n1407/1407 [==============================] - 16s 11ms/step - loss: 1.4995 - accuracy: 0.4678 - val_loss: 1.4309 - val_accuracy: 0.4936\nEpoch 6/100\n1407/1407 [==============================] - 16s 11ms/step - loss: 1.4651 - accuracy: 0.4808 - val_loss: 1.4100 - val_accuracy: 0.4954\nEpoch 7/100\n1407/1407 [==============================] - 16s 12ms/step - loss: 1.4308 - accuracy: 0.4934 - val_loss: 1.4097 - val_accuracy: 0.4982\nEpoch 8/100\n1407/1407 [==============================] - 16s 11ms/step - loss: 1.4024 - accuracy: 0.5018 - val_loss: 1.3888 - val_accuracy: 0.5028\nEpoch 9/100\n1407/1407 [==============================] - 16s 11ms/step - loss: 1.3789 - accuracy: 0.5106 - val_loss: 1.3670 - val_accuracy: 0.5172\nEpoch 10/100\n1407/1407 [==============================] - 16s 12ms/step - loss: 1.3578 - accuracy: 0.5190 - val_loss: 1.3578 - val_accuracy: 0.5166\nEpoch 11/100\n1407/1407 [==============================] - 17s 12ms/step - loss: 1.3384 - accuracy: 0.5264 - val_loss: 1.3728 - val_accuracy: 0.5106\nEpoch 12/100\n1407/1407 [==============================] - 16s 12ms/step - loss: 1.3142 - accuracy: 0.5364 - val_loss: 1.3836 - val_accuracy: 0.5076\nEpoch 13/100\n1407/1407 [==============================] - 16s 12ms/step - loss: 1.2976 - accuracy: 0.5418 - val_loss: 1.3877 - val_accuracy: 0.5080\nEpoch 14/100\n1407/1407 [==============================] - 17s 12ms/step - loss: 1.2772 - accuracy: 0.5473 - val_loss: 1.3546 - val_accuracy: 0.5262\nEpoch 15/100\n1407/1407 [==============================] - 16s 12ms/step - loss: 1.2586 - accuracy: 0.5564 - val_loss: 1.3646 - val_accuracy: 0.5232\nEpoch 16/100\n1407/1407 [==============================] - 17s 12ms/step - loss: 1.2498 - accuracy: 0.5575 - val_loss: 1.3733 - val_accuracy: 0.5278\nEpoch 17/100\n1407/1407 [==============================] - 16s 12ms/step - loss: 1.2277 - accuracy: 0.5647 - val_loss: 1.3282 - val_accuracy: 0.5286\nEpoch 18/100\n1407/1407 [==============================] - 16s 12ms/step - loss: 1.2111 - accuracy: 0.5727 - val_loss: 1.3356 - val_accuracy: 0.5336\nEpoch 19/100\n1407/1407 [==============================] - 16s 12ms/step - loss: 1.1970 - accuracy: 0.5799 - val_loss: 1.3403 - val_accuracy: 0.5324\nEpoch 20/100\n1407/1407 [==============================] - 16s 12ms/step - loss: 1.1867 - accuracy: 0.5828 - val_loss: 1.3695 - val_accuracy: 0.5220\nEpoch 21/100\n1407/1407 [==============================] - 16s 12ms/step - loss: 1.1739 - accuracy: 0.5862 - val_loss: 1.3694 - val_accuracy: 0.5206\nEpoch 22/100\n1407/1407 [==============================] - 17s 12ms/step - loss: 1.1538 - accuracy: 0.5933 - val_loss: 1.3414 - val_accuracy: 0.5270\nEpoch 23/100\n1407/1407 [==============================] - 16s 12ms/step - loss: 1.1470 - accuracy: 0.5964 - val_loss: 1.3346 - val_accuracy: 0.5382\nEpoch 24/100\n1407/1407 [==============================] - 16s 12ms/step - loss: 1.1348 - accuracy: 0.6004 - val_loss: 1.3432 - val_accuracy: 0.5392\nEpoch 25/100\n1407/1407 [==============================] - 16s 12ms/step - loss: 1.1244 - accuracy: 0.6039 - val_loss: 1.3435 - val_accuracy: 0.5370\nEpoch 26/100\n1407/1407 [==============================] - 17s 12ms/step - loss: 1.1108 - accuracy: 0.6087 - val_loss: 1.3529 - val_accuracy: 0.5326\nEpoch 27/100\n1407/1407 [==============================] - 17s 12ms/step - loss: 1.0964 - accuracy: 0.6130 - val_loss: 1.3500 - val_accuracy: 0.5292\nEpoch 28/100\n1407/1407 [==============================] - 17s 12ms/step - loss: 1.0934 - accuracy: 0.6170 - val_loss: 1.3525 - val_accuracy: 0.5360\nEpoch 29/100\n1407/1407 [==============================] - 17s 12ms/step - loss: 1.0825 - accuracy: 0.6184 - val_loss: 1.3644 - val_accuracy: 0.5272\nEpoch 30/100\n1407/1407 [==============================] - 17s 12ms/step - loss: 1.0681 - accuracy: 0.6236 - val_loss: 1.3699 - val_accuracy: 0.5306\nEpoch 31/100\n1407/1407 [==============================] - 17s 12ms/step - loss: 1.0545 - accuracy: 0.6249 - val_loss: 1.3717 - val_accuracy: 0.5376\nEpoch 32/100\n1407/1407 [==============================] - 17s 12ms/step - loss: 1.0445 - accuracy: 0.6323 - val_loss: 1.3760 - val_accuracy: 0.5412\nEpoch 33/100\n1407/1407 [==============================] - 16s 12ms/step - loss: 1.0319 - accuracy: 0.6372 - val_loss: 1.3725 - val_accuracy: 0.5388\nEpoch 34/100\n1407/1407 [==============================] - 17s 12ms/step - loss: 1.0282 - accuracy: 0.6355 - val_loss: 1.3553 - val_accuracy: 0.5488\nEpoch 35/100\n1407/1407 [==============================] - 17s 12ms/step - loss: 1.0174 - accuracy: 0.6413 - val_loss: 1.4069 - val_accuracy: 0.5312\nEpoch 36/100\n1407/1407 [==============================] - 17s 12ms/step - loss: 1.0103 - accuracy: 0.6444 - val_loss: 1.3772 - val_accuracy: 0.5446\nEpoch 37/100\n1407/1407 [==============================] - 16s 12ms/step - loss: 0.9954 - accuracy: 0.6481 - val_loss: 1.3570 - val_accuracy: 0.5406\n157/157 [==============================] - 1s 3ms/step - loss: 1.3282 - accuracy: 0.5286\n\n\n[1.328158974647522, 0.5285999774932861]\n\n\n\n이전보다 빠르게 수렴하나요? 훨씬 빠릅니다! 이전 모델은 가장 낮은 검증 손실에 도달하기 위해 27 에포크가 걸렸지만 새 모델은 동일한 손실에 도달하는데 5 에포크가 걸렸고 16 에포크까지 계속 줄어듭니다. 이전 모델보다 두 배 이상 빠릅니다. BN 층은 훈련을 안정적으로 수행하고 더 큰 학습률을 사용할 수 있기 때문에 수렴이 빨라졌습니다.\nBN이 더 좋은 모델을 만드나요? 네! 최종 모델의 성능이 47.6%가 아니라 54.0% 정확도로 더 좋습니다. 이는 아주 좋은 모델이 아니지만 적어도 이전보다는 낫습니다(합성곱 신경망이 더 낫겠지만 이는 다른 주제입니다. 14장을 참고하세요).\nBN이 훈련 속도에 영향을 미치나요? 모델이 훨씬 빠르게 수렴했지만 각 에포크는 8초가 아니라 12초가 걸렸습니다. BN 층에서 추가된 계산 때문입니다. 하지만 전반적인 훈련 시간(탁상 시계 시간)은 크게 줄었습니다!\n\n\n\nd.\n문제: 배치 정규화를 SELU로 바꾸어보세요. 네트워크가 자기 정규화하기 위해 필요한 변경 사항을 적용해보세요(즉, 입력 특성 표준화, 르쿤 정규분포 초기화, 완전 연결 층만 순차적으로 쌓은 심층 신경망 등).\n\nkeras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\nfor _ in range(20):\n    model.add(keras.layers.Dense(100,\n                                 kernel_initializer=\"lecun_normal\",\n                                 activation=\"selu\"))\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))\n\noptimizer = keras.optimizers.Nadam(learning_rate=7e-4)\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=optimizer,\n              metrics=[\"accuracy\"])\n\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\nmodel_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_selu_model.h5\", save_best_only=True)\nrun_index = 1 # 모델을 훈련할 때마다 증가시킴\nrun_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_selu_{:03d}\".format(run_index))\ntensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\ncallbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n\nX_means = X_train.mean(axis=0)\nX_stds = X_train.std(axis=0)\nX_train_scaled = (X_train - X_means) / X_stds\nX_valid_scaled = (X_valid - X_means) / X_stds\nX_test_scaled = (X_test - X_means) / X_stds\n\nmodel.fit(X_train_scaled, y_train, epochs=100,\n          validation_data=(X_valid_scaled, y_valid),\n          callbacks=callbacks)\n\nmodel = keras.models.load_model(\"my_cifar10_selu_model.h5\")\nmodel.evaluate(X_valid_scaled, y_valid)\n\n2021-10-10 02:17:56.621633: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n2021-10-10 02:17:56.621804: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n2021-10-10 02:17:56.622160: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n2021-10-10 02:18:02.441434: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n2021-10-10 02:18:02.441637: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n2021-10-10 02:18:02.724666: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n2021-10-10 02:18:02.732012: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n2021-10-10 02:18:02.743009: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./my_cifar10_logs/run_selu_001/train/plugins/profile/2021_10_10_02_18_02\n\n2021-10-10 02:18:02.748681: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to ./my_cifar10_logs/run_selu_001/train/plugins/profile/2021_10_10_02_18_02/instance-1.trace.json.gz\n2021-10-10 02:18:02.759983: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./my_cifar10_logs/run_selu_001/train/plugins/profile/2021_10_10_02_18_02\n\n2021-10-10 02:18:02.760264: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to ./my_cifar10_logs/run_selu_001/train/plugins/profile/2021_10_10_02_18_02/instance-1.memory_profile.json.gz\n2021-10-10 02:18:02.761347: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./my_cifar10_logs/run_selu_001/train/plugins/profile/2021_10_10_02_18_02\nDumped tool data for xplane.pb to ./my_cifar10_logs/run_selu_001/train/plugins/profile/2021_10_10_02_18_02/instance-1.xplane.pb\nDumped tool data for overview_page.pb to ./my_cifar10_logs/run_selu_001/train/plugins/profile/2021_10_10_02_18_02/instance-1.overview_page.pb\nDumped tool data for input_pipeline.pb to ./my_cifar10_logs/run_selu_001/train/plugins/profile/2021_10_10_02_18_02/instance-1.input_pipeline.pb\nDumped tool data for tensorflow_stats.pb to ./my_cifar10_logs/run_selu_001/train/plugins/profile/2021_10_10_02_18_02/instance-1.tensorflow_stats.pb\nDumped tool data for kernel_stats.pb to ./my_cifar10_logs/run_selu_001/train/plugins/profile/2021_10_10_02_18_02/instance-1.kernel_stats.pb\n\n\n\nEpoch 1/100\n   2/1407 [..............................] - ETA: 1:47 - loss: 3.0440 - accuracy: 0.1094     28/1407 [..............................] - ETA: 28s - loss: 2.4112 - accuracy: 0.18641407/1407 [==============================] - 11s 6ms/step - loss: 1.9366 - accuracy: 0.3096 - val_loss: 1.8654 - val_accuracy: 0.3362\nEpoch 2/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.7306 - accuracy: 0.3857 - val_loss: 1.8635 - val_accuracy: 0.3384\nEpoch 3/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.6275 - accuracy: 0.4276 - val_loss: 1.6944 - val_accuracy: 0.3836\nEpoch 4/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.5621 - accuracy: 0.4501 - val_loss: 1.6325 - val_accuracy: 0.4224\nEpoch 5/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.5064 - accuracy: 0.4725 - val_loss: 1.6295 - val_accuracy: 0.4146\nEpoch 6/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.4631 - accuracy: 0.4894 - val_loss: 1.5299 - val_accuracy: 0.4708\nEpoch 7/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.4156 - accuracy: 0.5050 - val_loss: 1.5704 - val_accuracy: 0.4500\nEpoch 8/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.3811 - accuracy: 0.5176 - val_loss: 1.4958 - val_accuracy: 0.4738\nEpoch 9/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.3505 - accuracy: 0.5323 - val_loss: 1.5240 - val_accuracy: 0.4626\nEpoch 10/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.3216 - accuracy: 0.5419 - val_loss: 1.5021 - val_accuracy: 0.4892\nEpoch 11/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.2884 - accuracy: 0.5514 - val_loss: 1.5091 - val_accuracy: 0.4750\nEpoch 12/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.2561 - accuracy: 0.5650 - val_loss: 1.4831 - val_accuracy: 0.4900\nEpoch 13/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.2300 - accuracy: 0.5751 - val_loss: 1.5019 - val_accuracy: 0.4966\nEpoch 14/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.2116 - accuracy: 0.5787 - val_loss: 1.5095 - val_accuracy: 0.4994\nEpoch 15/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.1869 - accuracy: 0.5916 - val_loss: 1.5340 - val_accuracy: 0.4886\nEpoch 16/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.1557 - accuracy: 0.6028 - val_loss: 1.5245 - val_accuracy: 0.5026\nEpoch 17/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.1434 - accuracy: 0.6089 - val_loss: 1.4797 - val_accuracy: 0.5054\nEpoch 18/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.1211 - accuracy: 0.6151 - val_loss: 1.4863 - val_accuracy: 0.4960\nEpoch 19/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.1024 - accuracy: 0.6194 - val_loss: 1.5406 - val_accuracy: 0.5066\nEpoch 20/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.0803 - accuracy: 0.6310 - val_loss: 1.5287 - val_accuracy: 0.5106\nEpoch 21/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.0606 - accuracy: 0.6384 - val_loss: 1.5305 - val_accuracy: 0.5068\nEpoch 22/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.0349 - accuracy: 0.6486 - val_loss: 1.5436 - val_accuracy: 0.4980\nEpoch 23/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.0217 - accuracy: 0.6529 - val_loss: 1.5507 - val_accuracy: 0.4948\nEpoch 24/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.0082 - accuracy: 0.6585 - val_loss: 1.5921 - val_accuracy: 0.5016\nEpoch 25/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 0.9880 - accuracy: 0.6668 - val_loss: 1.5627 - val_accuracy: 0.5180\nEpoch 26/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 0.9745 - accuracy: 0.6697 - val_loss: 1.5463 - val_accuracy: 0.5080\nEpoch 27/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 0.9713 - accuracy: 0.6698 - val_loss: 1.5078 - val_accuracy: 0.5074\nEpoch 28/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 0.9500 - accuracy: 0.6792 - val_loss: 1.5613 - val_accuracy: 0.5008\nEpoch 29/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 45.0468 - accuracy: 0.6433 - val_loss: 1.6315 - val_accuracy: 0.4506\nEpoch 30/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.3280 - accuracy: 0.5452 - val_loss: 1.5685 - val_accuracy: 0.4696\nEpoch 31/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.2027 - accuracy: 0.5827 - val_loss: 1.5454 - val_accuracy: 0.4902\nEpoch 32/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.1413 - accuracy: 0.6045 - val_loss: 1.5691 - val_accuracy: 0.4882\nEpoch 33/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.1030 - accuracy: 0.6172 - val_loss: 1.5414 - val_accuracy: 0.5010\nEpoch 34/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.0693 - accuracy: 0.6291 - val_loss: 1.5601 - val_accuracy: 0.4992\nEpoch 35/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.0419 - accuracy: 0.6390 - val_loss: 1.6308 - val_accuracy: 0.4934\nEpoch 36/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.0214 - accuracy: 0.6486 - val_loss: 1.6348 - val_accuracy: 0.4984\nEpoch 37/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 0.9975 - accuracy: 0.6578 - val_loss: 1.5899 - val_accuracy: 0.4958\n157/157 [==============================] - 0s 2ms/step - loss: 1.4797 - accuracy: 0.5054\n\n\n[1.4796942472457886, 0.5054000020027161]\n\n\n\nmodel = keras.models.load_model(\"my_cifar10_selu_model.h5\")\nmodel.evaluate(X_valid_scaled, y_valid)\n\n157/157 [==============================] - 0s 2ms/step - loss: 1.4797 - accuracy: 0.5054\n\n\n[1.4796942472457886, 0.5054000020027161]\n\n\n47.9% 정확도를 얻었습니다. 원래 모델(47.6%)보다 크게 높지 않습니다. 배치 정규화를 사용한 모델(54.0%)만큼 좋지도 않습니다. 하지만 BN 모델만큼 빠르게 수렴했습니다. 각 에포크는 7초만 걸렸습니다. 따라서 이 모델이 지금까지 가장 빠른 모델입니다.\n\n\ne.\n문제: 알파 드롭아웃으로 모델에 규제를 적용해보세요. 그다음 모델을 다시 훈련하지 않고 MC 드롭아웃으로 더 높은 정확도를 얻을 수 있는지 확인해보세요.\n\nkeras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\nfor _ in range(20):\n    model.add(keras.layers.Dense(100,\n                                 kernel_initializer=\"lecun_normal\",\n                                 activation=\"selu\"))\n\nmodel.add(keras.layers.AlphaDropout(rate=0.1))\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))\n\noptimizer = keras.optimizers.Nadam(learning_rate=5e-4)\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=optimizer,\n              metrics=[\"accuracy\"])\n\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\nmodel_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_alpha_dropout_model.h5\", save_best_only=True)\nrun_index = 1 # 모델을 훈련할 때마다 증가시킴\nrun_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_alpha_dropout_{:03d}\".format(run_index))\ntensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\ncallbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n\nX_means = X_train.mean(axis=0)\nX_stds = X_train.std(axis=0)\nX_train_scaled = (X_train - X_means) / X_stds\nX_valid_scaled = (X_valid - X_means) / X_stds\nX_test_scaled = (X_test - X_means) / X_stds\n\nmodel.fit(X_train_scaled, y_train, epochs=100,\n          validation_data=(X_valid_scaled, y_valid),\n          callbacks=callbacks)\n\nmodel = keras.models.load_model(\"my_cifar10_alpha_dropout_model.h5\")\nmodel.evaluate(X_valid_scaled, y_valid)\n\n2021-10-10 02:23:24.536681: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n2021-10-10 02:23:24.536860: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n2021-10-10 02:23:24.537406: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n2021-10-10 02:23:30.647672: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n2021-10-10 02:23:30.647844: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n2021-10-10 02:23:30.881209: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n2021-10-10 02:23:30.888528: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n2021-10-10 02:23:30.900150: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./my_cifar10_logs/run_alpha_dropout_001/train/plugins/profile/2021_10_10_02_23_30\n\n2021-10-10 02:23:30.906102: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to ./my_cifar10_logs/run_alpha_dropout_001/train/plugins/profile/2021_10_10_02_23_30/instance-1.trace.json.gz\n2021-10-10 02:23:30.918678: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./my_cifar10_logs/run_alpha_dropout_001/train/plugins/profile/2021_10_10_02_23_30\n\n2021-10-10 02:23:30.918960: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to ./my_cifar10_logs/run_alpha_dropout_001/train/plugins/profile/2021_10_10_02_23_30/instance-1.memory_profile.json.gz\n2021-10-10 02:23:30.919972: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./my_cifar10_logs/run_alpha_dropout_001/train/plugins/profile/2021_10_10_02_23_30\nDumped tool data for xplane.pb to ./my_cifar10_logs/run_alpha_dropout_001/train/plugins/profile/2021_10_10_02_23_30/instance-1.xplane.pb\nDumped tool data for overview_page.pb to ./my_cifar10_logs/run_alpha_dropout_001/train/plugins/profile/2021_10_10_02_23_30/instance-1.overview_page.pb\nDumped tool data for input_pipeline.pb to ./my_cifar10_logs/run_alpha_dropout_001/train/plugins/profile/2021_10_10_02_23_30/instance-1.input_pipeline.pb\nDumped tool data for tensorflow_stats.pb to ./my_cifar10_logs/run_alpha_dropout_001/train/plugins/profile/2021_10_10_02_23_30/instance-1.tensorflow_stats.pb\nDumped tool data for kernel_stats.pb to ./my_cifar10_logs/run_alpha_dropout_001/train/plugins/profile/2021_10_10_02_23_30/instance-1.kernel_stats.pb\n\n\n\nEpoch 1/100\n   2/1407 [..............................] - ETA: 1:41 - loss: 3.0759 - accuracy: 0.1094   19/1407 [..............................] - ETA: 34s - loss: 2.5471 - accuracy: 0.15621407/1407 [==============================] - 12s 7ms/step - loss: 1.8827 - accuracy: 0.3335 - val_loss: 1.8141 - val_accuracy: 0.3422\nEpoch 2/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.6604 - accuracy: 0.4151 - val_loss: 1.6295 - val_accuracy: 0.4204\nEpoch 3/100\n1407/1407 [==============================] - 9s 7ms/step - loss: 1.5713 - accuracy: 0.4498 - val_loss: 1.6646 - val_accuracy: 0.4162\nEpoch 4/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.5043 - accuracy: 0.4716 - val_loss: 1.6436 - val_accuracy: 0.4452\nEpoch 5/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.4497 - accuracy: 0.4917 - val_loss: 1.5975 - val_accuracy: 0.4644\nEpoch 6/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.3983 - accuracy: 0.5140 - val_loss: 1.4979 - val_accuracy: 0.4880\nEpoch 7/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.3535 - accuracy: 0.5312 - val_loss: 1.5254 - val_accuracy: 0.4744\nEpoch 8/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.3149 - accuracy: 0.5426 - val_loss: 1.4812 - val_accuracy: 0.5004\nEpoch 9/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.2799 - accuracy: 0.5561 - val_loss: 1.5204 - val_accuracy: 0.4882\nEpoch 10/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.2509 - accuracy: 0.5674 - val_loss: 1.4942 - val_accuracy: 0.5012\nEpoch 11/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.2206 - accuracy: 0.5800 - val_loss: 1.5644 - val_accuracy: 0.4970\nEpoch 12/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.1914 - accuracy: 0.5904 - val_loss: 1.5452 - val_accuracy: 0.5000\nEpoch 13/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.1565 - accuracy: 0.5999 - val_loss: 1.6069 - val_accuracy: 0.5040\nEpoch 14/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.1325 - accuracy: 0.6092 - val_loss: 1.5100 - val_accuracy: 0.5094\nEpoch 15/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.1061 - accuracy: 0.6182 - val_loss: 1.6162 - val_accuracy: 0.5102\nEpoch 16/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.0858 - accuracy: 0.6301 - val_loss: 1.6036 - val_accuracy: 0.5164\nEpoch 17/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.0617 - accuracy: 0.6362 - val_loss: 1.6463 - val_accuracy: 0.5018\nEpoch 18/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 1.0394 - accuracy: 0.6424 - val_loss: 1.6183 - val_accuracy: 0.5084\nEpoch 19/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 1.0227 - accuracy: 0.6516 - val_loss: 1.6803 - val_accuracy: 0.5202\nEpoch 20/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 0.9968 - accuracy: 0.6591 - val_loss: 1.6436 - val_accuracy: 0.5026\nEpoch 21/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 0.9775 - accuracy: 0.6673 - val_loss: 1.7502 - val_accuracy: 0.5114\nEpoch 22/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 0.9560 - accuracy: 0.6764 - val_loss: 1.7188 - val_accuracy: 0.5170\nEpoch 23/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 0.9427 - accuracy: 0.6808 - val_loss: 1.7112 - val_accuracy: 0.5120\nEpoch 24/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 0.9330 - accuracy: 0.6839 - val_loss: 1.6890 - val_accuracy: 0.5194\nEpoch 25/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 0.9064 - accuracy: 0.6920 - val_loss: 1.7430 - val_accuracy: 0.5184\nEpoch 26/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 0.8920 - accuracy: 0.6994 - val_loss: 1.7455 - val_accuracy: 0.5002\nEpoch 27/100\n1407/1407 [==============================] - 9s 6ms/step - loss: 0.8743 - accuracy: 0.7047 - val_loss: 1.8365 - val_accuracy: 0.5138\nEpoch 28/100\n1407/1407 [==============================] - 8s 6ms/step - loss: 0.8566 - accuracy: 0.7108 - val_loss: 1.7643 - val_accuracy: 0.5056\n157/157 [==============================] - 0s 2ms/step - loss: 1.4812 - accuracy: 0.5004\n\n\n[1.481205701828003, 0.5004000067710876]\n\n\n이 모델은 검증 세트에서 48.9% 정확도에 도달합니다. 드롭아웃이 없을 때보다(47.6%) 조금 더 좋습니다. 하이퍼파라미터 탐색을 좀 많이 수행해 보면 더 나아 질 수 있습니다(드롭아웃 비율 5%, 10%, 20%, 40%과 학습률 1e-4, 3e-4, 5e-4, 1e-3을 시도했습니다). 하지만 이 경우에는 크지 않을 것 같습니다.\n이제 MC 드롭아웃을 사용해 보죠. 앞서 사용한 MCAlphaDropout 클래스를 복사해 사용하겠습니다:\n\nclass MCAlphaDropout(keras.layers.AlphaDropout):\n    def call(self, inputs):\n        return super().call(inputs, training=True)\n\n방금 훈련했던 모델과 (같은 가중치를 가진) 동일한 새로운 모델을 만들어 보죠. 하지만 AlphaDropout 층 대신 MCAlphaDropout 드롭아웃 층을 사용합니다:\n\nmc_model = keras.models.Sequential([\n    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n    for layer in model.layers\n])\n\n그다음 몇 가지 유틸리티 함수를 추가합니다. 첫 번째 함수는 모델을 여러 번 실행합니다(기본적으로 10번). 그다음 평균한 예측 클래스 확률을 반환합니다. 두 번째 함수는 이 평균 확률을 사용해 각 샘플의 클래스를 예측합니다:\n\ndef mc_dropout_predict_probas(mc_model, X, n_samples=10):\n    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n    return np.mean(Y_probas, axis=0)\n\ndef mc_dropout_predict_classes(mc_model, X, n_samples=10):\n    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n    return np.argmax(Y_probas, axis=1)\n\n이제 검증 세트의 모든 샘플에 대해 예측을 만들고 정확도를 계산해 보죠:\n\nkeras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\ny_pred = mc_dropout_predict_classes(mc_model, X_valid_scaled)\naccuracy = np.mean(y_pred == y_valid[:, 0])\naccuracy\n\n0.5008\n\n\n이 경우에는 정확도 향상이 없습니다(여전히 정확도는 48.9%입니다).\n따라서 이 연습문에서 얻은 최상의 모델은 배치 정규화 모델입니다.\n\n\nf.\n문제: 1사이클 스케줄링으로 모델을 다시 훈련하고 훈련 속도와 모델 정확도가 향상되는지 확인해보세요.\n\nkeras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\nfor _ in range(20):\n    model.add(keras.layers.Dense(100,\n                                 kernel_initializer=\"lecun_normal\",\n                                 activation=\"selu\"))\n\nmodel.add(keras.layers.AlphaDropout(rate=0.1))\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))\n\noptimizer = keras.optimizers.SGD(learning_rate=1e-3)\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=optimizer,\n              metrics=[\"accuracy\"])\n\n\nbatch_size = 128\nrates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\nplot_lr_vs_loss(rates, losses)\nplt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 1.4])\n\n352/352 [==============================] - 3s 8ms/step - loss: nan - accuracy: 0.1378\n\n\n(9.999999747378752e-06,\n 9.615227699279785,\n 2.6294026374816895,\n 3.9444747992924283)\n\n\n\n\n\n\nkeras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\nfor _ in range(20):\n    model.add(keras.layers.Dense(100,\n                                 kernel_initializer=\"lecun_normal\",\n                                 activation=\"selu\"))\n\nmodel.add(keras.layers.AlphaDropout(rate=0.1))\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))\n\noptimizer = keras.optimizers.SGD(learning_rate=1e-2)\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=optimizer,\n              metrics=[\"accuracy\"])\n\n\nn_epochs = 15\nonecycle = OneCycleScheduler(len(X_train_scaled) // batch_size * n_epochs, max_rate=0.05)\nhistory = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n                    validation_data=(X_valid_scaled, y_valid),\n                    callbacks=[onecycle])\n\nEpoch 1/15\n352/352 [==============================] - 4s 9ms/step - loss: 2.0630 - accuracy: 0.2825 - val_loss: 1.8195 - val_accuracy: 0.3674\nEpoch 2/15\n352/352 [==============================] - 3s 8ms/step - loss: 1.7607 - accuracy: 0.3766 - val_loss: 1.6499 - val_accuracy: 0.4174\nEpoch 3/15\n352/352 [==============================] - 3s 8ms/step - loss: 1.6213 - accuracy: 0.4257 - val_loss: 1.6189 - val_accuracy: 0.4332\nEpoch 4/15\n352/352 [==============================] - 3s 8ms/step - loss: 1.5391 - accuracy: 0.4540 - val_loss: 1.6567 - val_accuracy: 0.4244\nEpoch 5/15\n352/352 [==============================] - 3s 8ms/step - loss: 1.4839 - accuracy: 0.4726 - val_loss: 1.6156 - val_accuracy: 0.4482\nEpoch 6/15\n352/352 [==============================] - 3s 8ms/step - loss: 1.4409 - accuracy: 0.4889 - val_loss: 1.5545 - val_accuracy: 0.4616\nEpoch 7/15\n352/352 [==============================] - 3s 8ms/step - loss: 1.4074 - accuracy: 0.5002 - val_loss: 1.5639 - val_accuracy: 0.4598\nEpoch 8/15\n352/352 [==============================] - 3s 8ms/step - loss: 1.3381 - accuracy: 0.5242 - val_loss: 1.4700 - val_accuracy: 0.4948\nEpoch 9/15\n352/352 [==============================] - 3s 8ms/step - loss: 1.2623 - accuracy: 0.5527 - val_loss: 1.5123 - val_accuracy: 0.4796\nEpoch 10/15\n352/352 [==============================] - 3s 8ms/step - loss: 1.1919 - accuracy: 0.5756 - val_loss: 1.5519 - val_accuracy: 0.4826\nEpoch 11/15\n352/352 [==============================] - 3s 8ms/step - loss: 1.1223 - accuracy: 0.6001 - val_loss: 1.5353 - val_accuracy: 0.4968\nEpoch 12/15\n352/352 [==============================] - 3s 8ms/step - loss: 1.0540 - accuracy: 0.6239 - val_loss: 1.5265 - val_accuracy: 0.5006\nEpoch 13/15\n352/352 [==============================] - 3s 8ms/step - loss: 0.9837 - accuracy: 0.6476 - val_loss: 1.5636 - val_accuracy: 0.5146\nEpoch 14/15\n352/352 [==============================] - 3s 8ms/step - loss: 0.9180 - accuracy: 0.6714 - val_loss: 1.5781 - val_accuracy: 0.5114\nEpoch 15/15\n352/352 [==============================] - 3s 8ms/step - loss: 0.8779 - accuracy: 0.6862 - val_loss: 1.6093 - val_accuracy: 0.5112\n\n\n1사이클 방식을 사용해 모델을 15에포크 동안 훈련했습니다. (큰 배치 크기 덕분에) 각 에포크는 2초만 걸렸습니다. 이는 지금까지 훈련한 가장 빠른 모델보다 몇 배 더 빠릅니다. 또한 모델 성능도 올라갔습니다(47.6%에서 52.0%). 배치 정규화 모델이 조금 더 성능(54%)이 높지만 훈련 속도가 더 느립니다."
  },
  {
    "objectID": "Machine_Learning/09_unsupervised_learning.html",
    "href": "Machine_Learning/09_unsupervised_learning.html",
    "title": "09_unsupervised_learning",
    "section": "",
    "text": "9장 – 비지도 학습\n이 노트북은 9장에 있는 모든 샘플 코드와 연습문제 해답을 가지고 있습니다."
  },
  {
    "objectID": "Machine_Learning/09_unsupervised_learning.html#k-평균",
    "href": "Machine_Learning/09_unsupervised_learning.html#k-평균",
    "title": "09_unsupervised_learning",
    "section": "K-평균",
    "text": "K-평균\n먼저 예제 데이터를 생성해 보죠:\n\nfrom sklearn.datasets import make_blobs\n\n\nblob_centers = np.array(\n    [[ 0.2,  2.3],\n     [-1.5 ,  2.3],\n     [-2.8,  1.8],\n     [-2.8,  2.8],\n     [-2.8,  1.3]])\nblob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\n\n\nX, y = make_blobs(n_samples=2000, centers=blob_centers,\n                  cluster_std=blob_std, random_state=7)\n\n데이터를 그래프로 그립니다:\n\ndef plot_clusters(X, y=None):\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n    plt.xlabel(\"$x_1$\", fontsize=14)\n    plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n\n\nplt.figure(figsize=(8, 4))\nplot_clusters(X)\nsave_fig(\"blobs_plot\")\nplt.show()\n\n그림 저장: blobs_plot\n\n\n\n\n\n훈련과 예측\n이 데이터셋에 K-평균 군집 알고리즘을 훈련해 보겠습니다. 이 알고리즘은 클러스터 중심을 찾고 각 샘플을 가까운 클러스터에 할당합니다:\n\nfrom sklearn.cluster import KMeans\n\n\nk = 5\nkmeans = KMeans(n_clusters=k, random_state=42)\ny_pred = kmeans.fit_predict(X)\n\n각 샘플은 5개의 클러스터 중 하나에 할당됩니다:\n\ny_pred\n\narray([4, 0, 1, ..., 2, 1, 0], dtype=int32)\n\n\n\ny_pred is kmeans.labels_\n\nTrue\n\n\n5개의 센트로이드 (즉 클러스터 중심)을 찾았습니다:\n\nkmeans.cluster_centers_\n\narray([[-2.80389616,  1.80117999],\n       [ 0.20876306,  2.25551336],\n       [-2.79290307,  2.79641063],\n       [-1.46679593,  2.28585348],\n       [-2.80037642,  1.30082566]])\n\n\nKMeans 객체는 훈련한 샘플의 레이블을 가지고 있습니다. 조금 혼동스럽지만 여기에서 샘플의 레이블 은 샘플에 할당한 클러스터의 인덱스입니다:\n\nkmeans.labels_\n\narray([4, 0, 1, ..., 2, 1, 0], dtype=int32)\n\n\n물론 새로운 샘플의 레이블을 예측할 수 있습니다:\n\nX_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\nkmeans.predict(X_new)\n\narray([1, 1, 2, 2], dtype=int32)\n\n\n결정 경계\n이 모델의 결정 경계를 그려 보죠. 이 그림은 보로노이 다이어그램 이 됩니다:\n\ndef plot_data(X):\n    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n\ndef plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n    if weights is not None:\n        centroids = centroids[weights &gt; weights.max() / 10]\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='o', s=35, linewidths=8,\n                color=circle_color, zorder=10, alpha=0.9)\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='x', s=2, linewidths=12, \n                color=cross_color, zorder=11, alpha=1)\n\ndef plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,\n                             show_xlabels=True, show_ylabels=True):\n    mins = X.min(axis=0) - 0.1\n    maxs = X.max(axis=0) + 0.1\n    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n                         np.linspace(mins[1], maxs[1], resolution))\n    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n                cmap=\"Pastel2\")\n    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n                linewidths=1, colors='k')\n    plot_data(X)\n    if show_centroids:\n        plot_centroids(clusterer.cluster_centers_)\n\n    if show_xlabels:\n        plt.xlabel(\"$x_1$\", fontsize=14)\n    else:\n        plt.tick_params(labelbottom=False)\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n\n\nplt.figure(figsize=(8, 4))\nplot_decision_boundaries(kmeans, X)\nsave_fig(\"voronoi_plot\")\nplt.show()\n\n그림 저장: voronoi_plot\n\n\n\n\n\n나쁘지 않군요! 경계 근처에 있는 일부 샘플은 아마도 잘못 클러스터에 할당된 것 같습니다. 하지만 전반적으로 아주 좋은 것 같습니다.\n하드 군집 vs 소프트 군집\n하드 군집 은 각 샘플에 대해 가장 가까운 클러스터를 선택합니다. 이 대신 샘플에서 5개의 센트로이드까지 거리를 측정하는 것이 나을 수 있습니다. transform() 메서드에서 이 거리를 계산합니다:\n\nkmeans.transform(X_new)\n\narray([[2.81093633, 0.32995317, 2.9042344 , 1.49439034, 2.88633901],\n       [5.80730058, 2.80290755, 5.84739223, 4.4759332 , 5.84236351],\n       [1.21475352, 3.29399768, 0.29040966, 1.69136631, 1.71086031],\n       [0.72581411, 3.21806371, 0.36159148, 1.54808703, 1.21567622]])\n\n\n이 거리가 샘플과 센트로이드 사이의 유클리드 거리인지 확인할 수 있습니다:\n\nnp.linalg.norm(np.tile(X_new, (1, k)).reshape(-1, k, 2) - kmeans.cluster_centers_, axis=2)\n\narray([[2.81093633, 0.32995317, 2.9042344 , 1.49439034, 2.88633901],\n       [5.80730058, 2.80290755, 5.84739223, 4.4759332 , 5.84236351],\n       [1.21475352, 3.29399768, 0.29040966, 1.69136631, 1.71086031],\n       [0.72581411, 3.21806371, 0.36159148, 1.54808703, 1.21567622]])\n\n\n\nK-평균 알고리즘\nK-평균 알고리즘은 가장 빠르고 가장 간단한 군집 알고리즘 중 하나입니다: * 먼저 \\(k\\) 개의 센트로이드를 랜덤하게 초기화합니다: 데이터셋에서 \\(k\\) 개의 샘플을 랜덤하게 선택하고 센트로이드를 그 위치에 놓습니다. * 수렴할 때까지 다음을 반복합니다(즉, 센트로이드가 더이상 이동하지 않을 때까지): * 각 샘플을 가장 가까운 센트로이드에 할당합니다. * 센트로이드에 할당된 샘플의 평균으로 센트로이드를 업데이트합니다.\nKMeans 클래스는 기본적으로 최적화된 알고리즘을 적용합니다. (교육적인 목적으로) 원래 K-평균 알고리즘을 사용하려면 init=\"randome\", n_init=1, algorithm=\"full\"로 설정해야 합니다. 이 매개변수들은 아래에서 설명하겠습니다.\nK-평균 알고리즘을 1, 2, 3회 반복하고 센트로이드가 어떻게 움직이는지 확인해 보겠습니다:\n\nkmeans_iter1 = KMeans(n_clusters=5, init=\"random\", n_init=1,\n                      algorithm=\"full\", max_iter=1, random_state=0)\nkmeans_iter2 = KMeans(n_clusters=5, init=\"random\", n_init=1,\n                      algorithm=\"full\", max_iter=2, random_state=0)\nkmeans_iter3 = KMeans(n_clusters=5, init=\"random\", n_init=1,\n                      algorithm=\"full\", max_iter=3, random_state=0)\nkmeans_iter1.fit(X)\nkmeans_iter2.fit(X)\nkmeans_iter3.fit(X)\n\nKMeans(algorithm='full', init='random', max_iter=3, n_clusters=5, n_init=1,\n       random_state=0)\n\n\n그래프를 그려 보죠:\n\nplt.figure(figsize=(10, 8))\n\nplt.subplot(321)\nplot_data(X)\nplot_centroids(kmeans_iter1.cluster_centers_, circle_color='r', cross_color='w')\nplt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\nplt.tick_params(labelbottom=False)\nplt.title(\"Update the centroids (initially randomly)\", fontsize=14)\n\nplt.subplot(322)\nplot_decision_boundaries(kmeans_iter1, X, show_xlabels=False, show_ylabels=False)\nplt.title(\"Label the instances\", fontsize=14)\n\nplt.subplot(323)\nplot_decision_boundaries(kmeans_iter1, X, show_centroids=False, show_xlabels=False)\nplot_centroids(kmeans_iter2.cluster_centers_)\n\nplt.subplot(324)\nplot_decision_boundaries(kmeans_iter2, X, show_xlabels=False, show_ylabels=False)\n\nplt.subplot(325)\nplot_decision_boundaries(kmeans_iter2, X, show_centroids=False)\nplot_centroids(kmeans_iter3.cluster_centers_)\n\nplt.subplot(326)\nplot_decision_boundaries(kmeans_iter3, X, show_ylabels=False)\n\nsave_fig(\"kmeans_algorithm_plot\")\nplt.show()\n\n그림 저장: kmeans_algorithm_plot\n\n\n\n\n\nK-평균의 변동성\n원래 K-평균 알고리즘에서는 센트로이가 그냥 랜덤하게 초기되고 알고리즘은 단순히 한번씩 반복하여 앞서 본 것처럼 점차 센트로이드를 개선시킵니다.\n하지만 이 방식의 문제점은 K-평균을 여러번 (또는 다른 랜덤 시드로) 실행하면 아래에서 보듯이 매우 다른 결과를 얻게됩니다:\n\ndef plot_clusterer_comparison(clusterer1, clusterer2, X, title1=None, title2=None):\n    clusterer1.fit(X)\n    clusterer2.fit(X)\n\n    plt.figure(figsize=(10, 3.2))\n\n    plt.subplot(121)\n    plot_decision_boundaries(clusterer1, X)\n    if title1:\n        plt.title(title1, fontsize=14)\n\n    plt.subplot(122)\n    plot_decision_boundaries(clusterer2, X, show_ylabels=False)\n    if title2:\n        plt.title(title2, fontsize=14)\n\n\nkmeans_rnd_init1 = KMeans(n_clusters=5, init=\"random\", n_init=1,\n                          algorithm=\"full\", random_state=2)\nkmeans_rnd_init2 = KMeans(n_clusters=5, init=\"random\", n_init=1,\n                          algorithm=\"full\", random_state=5)\n\nplot_clusterer_comparison(kmeans_rnd_init1, kmeans_rnd_init2, X,\n                          \"Solution 1\", \"Solution 2 (with a different random init)\")\n\nsave_fig(\"kmeans_variability_plot\")\nplt.show()\n\n그림 저장: kmeans_variability_plot\n\n\n\n\n\n\n\n이너셔\n최선의 모델을 선택하려면 K-평균 모델의 성능을 평가할 방법이 있어야 합니다. 안타깝지만 군집은 비지도 학습이기 때문에 타깃이 없습니다. 하지만 적어도 각 샘플과 센트로이드 사이의 거리는 측정할 수 있습니다. 이것이 이너셔 지표의 아이디어입니다:\n\nkmeans.inertia_\n\n211.5985372581684\n\n\n이너셔는 각 훈련 샘플과 가장 가까운 센트로이드 사이의 제곱 거리의 합으로 쉽게 검증할 수 있습니다:\n\nX_dist = kmeans.transform(X)\nnp.sum(X_dist[np.arange(len(X_dist)), kmeans.labels_]**2)\n\n211.59853725816856\n\n\nscore() 메서드는 음의 이너셔를 반환합니다. 왜 음수일까요? 사이킷런의 score() 메서드는 항상 ” 큰 값이 좋은 것 ” 규칙을 따라야 하기 때문입니다.\n\nkmeans.score(X)\n\n-211.59853725816836\n\n\n\n\n다중 초기화\n변동성 이슈를 해결하는 한 방법은 단순히 K-평균 알고리즘을 랜덤 초기화를 다르게 하여 여러 번 실행하고 가장 작은 이너셔를 만드는 솔루션을 선택합니다. 예를 들어 앞선 그림에 있는 엉터리 모델 두 개의 이너셔는 다음과 같습니다.\n\nkmeans_rnd_init1.inertia_\n\n219.43539442771402\n\n\n\nkmeans_rnd_init2.inertia_\n\n211.5985372581684\n\n\n여기서 볼 수 있듯이 앞서 훈련한 “좋은” 모델보다 이너셔가 더 높습니다. 즉 더 나쁘다는 것을 의미합니다.\nn_init 매개변수를 지정하면 사이킷런은 원래 알고리즘을 n_init 번 실행하고 이너셔가 가장 작은 솔루션을 선택합니다. 이 매개변수의 기본값은 n_init=10입니다.\n\nkmeans_rnd_10_inits = KMeans(n_clusters=5, init=\"random\", n_init=10,\n                             algorithm=\"full\", random_state=2)\nkmeans_rnd_10_inits.fit(X)\n\nKMeans(algorithm='full', init='random', n_clusters=5, random_state=2)\n\n\n여기에서 볼 수 있듯이 결국 처음 만들었던 모델을 얻었습니다. 이 모델이 최적의 K-평균 결과로 보입니다(\\(k=5\\)라고 가정하고 이너셔를 기준으로 했을 때입니다).\n\nplt.figure(figsize=(8, 4))\nplot_decision_boundaries(kmeans_rnd_10_inits, X)\nplt.show()\n\n\n\n\n\n\n센트로이드 초기화 방법\n센트로이드를 완전히 랜덤하게 초기화하는 대신 David Arthur와 Sergei Vassilvitskii가 2006년 논문에서 제안한 다음 알고리즘을 사용해 초기화하는 것이 더 좋습니다: * 데이터셋에서 무작위로 균등하게 하나의 센트로이드 \\(c_1\\)을 선택합니다. * \\(D(\\mathbf{x}_i)^2\\) / \\(\\sum\\limits_{j=1}^{m}{D(\\mathbf{x}_j)}^2\\)의 확률로 샘플 \\(\\mathbf{x}_i\\)를 새로운 센트로이드 \\(c_i\\)로 선택합니다. 여기에서 \\(D(\\mathbf{x}_i)\\)는 샘플 \\(\\mathbf{x}_i\\)에서 이미 선택된 가장 가까운 센트로이드까지 거리입니다. 이 확률 분포는 이미 선택한 센트로이드에서 멀리 떨어진 샘플을 센트로이드로 선택할 가능성을 높입니다. * \\(k\\) 개의 센트로이드를 선택할 때까지 이전 단계를 반복합니다.\nK-평균++ 알고리즘의 나머지는 일반 K-평균과 같습니다. 이 초기화 방식을 사용하면 K-평균 알고리즘이 최적의 솔루션에 수렴할 가능성이 훨씬 높아집니다. 따라서 n_init 값을 상당히 줄일 수 있습니다. 대부분의 경우 n_init를 줄이는 것이 초기화 과정에 추가된 복잡도를 보상합니다.\nK-평균++ 초기화를 사용하려면 간단하게 init=\"k-means++\"로 지정하면 됩니다(사실 이 값이 기본값입니다):\n\nKMeans()\n\nKMeans()\n\n\n\ngood_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])\nkmeans = KMeans(n_clusters=5, init=good_init, n_init=1, random_state=42)\nkmeans.fit(X)\nkmeans.inertia_\n\n211.59853725816836\n\n\n\n\nK-평균 속도 개선\nK-평균 알고리즘은 불필요한 거리 계산을 많이 피하는 식으로 속도를 크게 높일 수 있습니다. 이를 위해 삼각 부등식을 사용합니다(3개의 포인트 A, B, C가 있을 때, 거리 AC는 항상 AC ≤ AB + BC를 만족합니다). 그리고 샘플과 센트로이드 사이 거리의 최솟값과 최댓값을 유지합니다(더 자세한 내용은 Charles Elkan의 2003년 논문을 참고하세요).\nElkan의 K-평균 방식을 사용하려면 algorithm=\"elkan\"으로 설정하세요. 이 방법은 희소 행렬을 지원하지 않습니다. 따라서 사이킷런은 밀집 배열에는 \"elkan\"을 사용하고 희소 행렬에는 (기본 K-평균 알고리즘인) \"full을 사용합니다.\n\n%timeit -n 50 KMeans(algorithm=\"elkan\", random_state=42).fit(X)\n\n1.41 s ± 25.6 ms per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\n\n%timeit -n 50 KMeans(algorithm=\"full\", random_state=42).fit(X)\n\n1.46 s ± 23.1 ms per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\n데이터셋이 작기 때문에 여기에서는 큰 차이가 없습니다.\n\n\n미니배치 K-평균\n사이킷런은 미니배치를 지원하는 K-평균 방식도 제공합니다(이 논문 참조):\n\nfrom sklearn.cluster import MiniBatchKMeans\n\n\nminibatch_kmeans = MiniBatchKMeans(n_clusters=5, random_state=42)\nminibatch_kmeans.fit(X)\n\nMiniBatchKMeans(n_clusters=5, random_state=42)\n\n\n\nminibatch_kmeans.inertia_\n\n211.652398504332\n\n\n데이터셋이 메모리에 다 들어가지 못하면 가장 간단한 방법은 이전 장의 점진적 PCA에서 했던 것처럼 memmap 클래스를 사용하는 것입니다. 먼저 MNIST 데이터를 로드합니다:\n\nimport urllib.request\nfrom sklearn.datasets import fetch_openml\n\nmnist = fetch_openml('mnist_784', version=1)\nmnist.target = mnist.target.astype(np.int64)\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    mnist[\"data\"], mnist[\"target\"], random_state=42)\n\nmemmap에 데이터를 기록합니다:\n\nfilename = \"my_mnist.data\"\nX_mm = np.memmap(filename, dtype='float32', mode='write', shape=X_train.shape)\nX_mm[:] = X_train\n\n\nminibatch_kmeans = MiniBatchKMeans(n_clusters=10, batch_size=10, random_state=42)\nminibatch_kmeans.fit(X_mm)\n\nMiniBatchKMeans(batch_size=10, n_clusters=10, random_state=42)\n\n\n데이터가 너무 커서 memmap을 사용할 수 없다면 문제는 더 복잡해집니다. 배치를 로드하는 함수를 먼저 만들어 보죠(실전에서는 디스크에서 데이터를 로드합니다):\n\ndef load_next_batch(batch_size):\n    return X[np.random.choice(len(X), batch_size, replace=False)]\n\n한 번에 하나의 배치를 모델에 주입하여 훈련할 수 있습니다. 또한 여러 번 초기화를 수행하고 이너셔가 가장 낮은 모델을 선택합니다:\n\nnp.random.seed(42)\n\n\nk = 5\nn_init = 10\nn_iterations = 100\nbatch_size = 100\ninit_size = 500  # K-Means++ 초기화를 위해 충분한 데이터 전달\nevaluate_on_last_n_iters = 10\n\nbest_kmeans = None\n\nfor init in range(n_init):\n    minibatch_kmeans = MiniBatchKMeans(n_clusters=k, init_size=init_size)\n    X_init = load_next_batch(init_size)\n    minibatch_kmeans.partial_fit(X_init)\n\n    minibatch_kmeans.sum_inertia_ = 0\n    for iteration in range(n_iterations):\n        X_batch = load_next_batch(batch_size)\n        minibatch_kmeans.partial_fit(X_batch)\n        if iteration &gt;= n_iterations - evaluate_on_last_n_iters:\n            minibatch_kmeans.sum_inertia_ += minibatch_kmeans.inertia_\n\n    if (best_kmeans is None or\n        minibatch_kmeans.sum_inertia_ &lt; best_kmeans.sum_inertia_):\n        best_kmeans = minibatch_kmeans\n\n\nbest_kmeans.score(X)\n\n-211.62571878891143\n\n\n미니배치 K-평균이 일반 K-평균보다 훨씬 빠릅니다:\n\n%timeit KMeans(n_clusters=5, random_state=42).fit(X)\n\n882 ms ± 108 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\n%timeit MiniBatchKMeans(n_clusters=5, random_state=42).fit(X)\n\n53.8 ms ± 5.44 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n훨씬 빠르군요! 하지만 성능이 낮은 (즉 이너셔가 높은) 경우가 많습니다. k 가 증가할수록 더 그렇습니다. 미니배치 K-평균과 일반 K-평균 사이에 이너셔와 훈련 시간을 그래프로 나타내 보겠습니다:\n\nfrom timeit import timeit\n\n\ntimes = np.empty((100, 2))\ninertias = np.empty((100, 2))\nfor k in range(1, 101):\n    kmeans_ = KMeans(n_clusters=k, random_state=42)\n    minibatch_kmeans = MiniBatchKMeans(n_clusters=k, random_state=42)\n    print(\"\\r{}/{}\".format(k, 100), end=\"\")\n    times[k-1, 0] = timeit(\"kmeans_.fit(X)\", number=10, globals=globals())\n    times[k-1, 1]  = timeit(\"minibatch_kmeans.fit(X)\", number=10, globals=globals())\n    inertias[k-1, 0] = kmeans_.inertia_\n    inertias[k-1, 1] = minibatch_kmeans.inertia_\n\n100/100\n\n\n\nplt.figure(figsize=(10,4))\n\nplt.subplot(121)\nplt.plot(range(1, 101), inertias[:, 0], \"r--\", label=\"K-Means\")\nplt.plot(range(1, 101), inertias[:, 1], \"b.-\", label=\"Mini-batch K-Means\")\nplt.xlabel(\"$k$\", fontsize=16)\nplt.title(\"Inertia\", fontsize=14)\nplt.legend(fontsize=14)\nplt.axis([1, 100, 0, 100])\n\nplt.subplot(122)\nplt.plot(range(1, 101), times[:, 0], \"r--\", label=\"K-Means\")\nplt.plot(range(1, 101), times[:, 1], \"b.-\", label=\"Mini-batch K-Means\")\nplt.xlabel(\"$k$\", fontsize=16)\nplt.title(\"Training time (seconds)\", fontsize=14)\nplt.axis([1, 100, 0, 6])\n\nsave_fig(\"minibatch_kmeans_vs_kmeans\")\nplt.show()\n\n그림 저장: minibatch_kmeans_vs_kmeans\n\n\n\n\n\n\n\n최적의 클러스터 개수 찾기\n클러스터 개수가 5보다 작거나 크게 지정하면 어떨까요?\n\nkmeans_k3 = KMeans(n_clusters=3, random_state=42)\nkmeans_k8 = KMeans(n_clusters=8, random_state=42)\n\nplot_clusterer_comparison(kmeans_k3, kmeans_k8, X, \"$k=3$\", \"$k=8$\")\nsave_fig(\"bad_n_clusters_plot\")\nplt.show()\n\n그림 저장: bad_n_clusters_plot\n\n\n\n\n\n두 모델 모두 좋아 보이지 않는군요. 이너셔는 어떨까요?\n\nkmeans_k3.inertia_\n\n653.2167190021553\n\n\n\nkmeans_k8.inertia_\n\n119.11983416102879\n\n\n\\(k\\)가 증가할수록 이너셔가 줄어들기 때문에 단순히 이너셔가 작은 \\(k\\)를 선택할 수 없군요. 실제 클러스터가 많을수록 샘플은 인접한 센트로이드에 더 가깝습니다. 따라서 이너셔가 더 작습니다. 하지만 \\(k\\)에 대한 이너셔를 그래프로 그리고 결과 그래프를 조사해 볼 수 있습니다:\n\nkmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(X)\n                for k in range(1, 10)]\ninertias = [model.inertia_ for model in kmeans_per_k]\n\n\nplt.figure(figsize=(8, 3.5))\nplt.plot(range(1, 10), inertias, \"bo-\")\nplt.xlabel(\"$k$\", fontsize=14)\nplt.ylabel(\"Inertia\", fontsize=14)\nplt.annotate('Elbow',\n             xy=(4, inertias[3]),\n             xytext=(0.55, 0.55),\n             textcoords='figure fraction',\n             fontsize=16,\n             arrowprops=dict(facecolor='black', shrink=0.1)\n            )\nplt.axis([1, 8.5, 0, 1300])\nsave_fig(\"inertia_vs_k_plot\")\nplt.show()\n\n그림 저장: inertia_vs_k_plot\n\n\n\n\n\n여기에서 볼 수 있듯이 \\(k=4\\)에서 엘보우가 있습니다. 이 값보다 클러스터가 작으면 나쁘다는 뜻입니다. 이보다 더 많으면 크게 도움이 되지 않고 배로 늘려도 그렇습니다. 따라서 \\(k=4\\)가 아주 좋은 선택입니다. 물론 이 예제에서는 이 값이 완벽하지 않습니다. 왼쪽 아래 두 클러스터가 하나의 클러스터로 간주되었지만 꽤 좋은 군집 결과입니다.\n\nplot_decision_boundaries(kmeans_per_k[4-1], X)\nplt.show()\n\n\n\n\n또 다른 방법은 모든 샘플에 대한 실루엣 계수 의 평균인 실루엣 점수 입니다. 한 샘플의 실루엣 계수는 \\((b - a)/\\max(a, b)\\)입니다. 여기에서 \\(a\\)는 같은 클러스터에 있는 다른 샘플까지의 평균 거리입니다(이를 클러스터 내부 평균 거리 라고 합니다). \\(b\\)는 가장 가까운 클러스터까지 평균 거리입니다. 즉 가장 가까운 클러스터(샘플 자신의 클러스터를 제외하고 \\(b\\)를 최소화하는 클러스터)의 샘플까지 평균 거리입니다. 실루엣 계수는 -1에서 +1 사이 값을 가집니다. +1에 가까우면 샘플이 다른 클러스터로부터 떨어져 자신의 클러스터 안에 잘 있다는 것을 의미합니다.0에 가까우면 클러스터 경계에 가깝다는 의미입니다. -1에 가까우면 샘플이 잘못된 클러스터에 할당되었을지 모릅니다.\n\\(k\\)에 대한 실루엣 점수를 그래프로 그려보죠:\n\nfrom sklearn.metrics import silhouette_score\n\n\nsilhouette_score(X, kmeans.labels_)\n\n0.655517642572828\n\n\n\nsilhouette_scores = [silhouette_score(X, model.labels_)\n                     for model in kmeans_per_k[1:]]\n\n\nplt.figure(figsize=(8, 3))\nplt.plot(range(2, 10), silhouette_scores, \"bo-\")\nplt.xlabel(\"$k$\", fontsize=14)\nplt.ylabel(\"Silhouette score\", fontsize=14)\nplt.axis([1.8, 8.5, 0.55, 0.7])\nsave_fig(\"silhouette_score_vs_k_plot\")\nplt.show()\n\n그림 저장: silhouette_score_vs_k_plot\n\n\n\n\n\n여기에서 볼 수 있듯이 이 그래프는 이전보다 정보가 더 풍부합니다. 특히 \\(k=4\\)가 매우 좋은 선택이지만 \\(k=5\\)도 꽤 괜찮은 선택이라는 것을 보여줍니다.\n모든 샘플의 실루엣 계수를 할당된 클러스터와 실루엣 값으로 정렬하여 그리면 훨씬 많은 정보를 얻을 수 있습니다. 이를 실루엣 다이어그램 이라고 합니다:\n\nfrom sklearn.metrics import silhouette_samples\nfrom matplotlib.ticker import FixedLocator, FixedFormatter\n\nplt.figure(figsize=(11, 9))\n\nfor k in (3, 4, 5, 6):\n    plt.subplot(2, 2, k - 2)\n    \n    y_pred = kmeans_per_k[k - 1].labels_\n    silhouette_coefficients = silhouette_samples(X, y_pred)\n\n    padding = len(X) // 30\n    pos = padding\n    ticks = []\n    for i in range(k):\n        coeffs = silhouette_coefficients[y_pred == i]\n        coeffs.sort()\n\n        color = mpl.cm.Spectral(i / k)\n        plt.fill_betweenx(np.arange(pos, pos + len(coeffs)), 0, coeffs,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n        ticks.append(pos + len(coeffs) // 2)\n        pos += len(coeffs) + padding\n\n    plt.gca().yaxis.set_major_locator(FixedLocator(ticks))\n    plt.gca().yaxis.set_major_formatter(FixedFormatter(range(k)))\n    if k in (3, 5):\n        plt.ylabel(\"Cluster\")\n    \n    if k in (5, 6):\n        plt.gca().set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n        plt.xlabel(\"Silhouette Coefficient\")\n    else:\n        plt.tick_params(labelbottom=False)\n\n    plt.axvline(x=silhouette_scores[k - 2], color=\"red\", linestyle=\"--\")\n    plt.title(\"$k={}$\".format(k), fontsize=16)\n\nsave_fig(\"silhouette_analysis_plot\")\nplt.show()\n\n그림 저장: silhouette_analysis_plot\n\n\n\n\n\n여기에서 볼 수 있듯이 \\(k=5\\)가 가장 좋은 선택입니다. 모든 클러스터의 크기가 거의 동일하고 평균 실루엣 점수를 나타내는 파선을 모두 넘었습니다."
  },
  {
    "objectID": "Machine_Learning/09_unsupervised_learning.html#k-평균의-한계",
    "href": "Machine_Learning/09_unsupervised_learning.html#k-평균의-한계",
    "title": "09_unsupervised_learning",
    "section": "K-평균의 한계",
    "text": "K-평균의 한계\n\nX1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)\nX1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))\nX2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)\nX2 = X2 + [6, -8]\nX = np.r_[X1, X2]\ny = np.r_[y1, y2]\n\n\nplot_clusters(X)\n\n\n\n\n\nkmeans_good = KMeans(n_clusters=3, init=np.array([[-1.5, 2.5], [0.5, 0], [4, 0]]), n_init=1, random_state=42)\nkmeans_bad = KMeans(n_clusters=3, random_state=42)\nkmeans_good.fit(X)\nkmeans_bad.fit(X)\n\nKMeans(n_clusters=3, random_state=42)\n\n\n\nplt.figure(figsize=(10, 3.2))\n\nplt.subplot(121)\nplot_decision_boundaries(kmeans_good, X)\nplt.title(\"Inertia = {:.1f}\".format(kmeans_good.inertia_), fontsize=14)\n\nplt.subplot(122)\nplot_decision_boundaries(kmeans_bad, X, show_ylabels=False)\nplt.title(\"Inertia = {:.1f}\".format(kmeans_bad.inertia_), fontsize=14)\n\nsave_fig(\"bad_kmeans_plot\")\nplt.show()\n\n그림 저장: bad_kmeans_plot"
  },
  {
    "objectID": "Machine_Learning/09_unsupervised_learning.html#군집을-사용한-이미지-분할",
    "href": "Machine_Learning/09_unsupervised_learning.html#군집을-사용한-이미지-분할",
    "title": "09_unsupervised_learning",
    "section": "군집을 사용한 이미지 분할",
    "text": "군집을 사용한 이미지 분할\n\n# 무당벌레 이미지를 다운로드합니다\nimages_path = os.path.join(PROJECT_ROOT_DIR, \"images\", \"unsupervised_learning\")\nos.makedirs(images_path, exist_ok=True)\nDOWNLOAD_ROOT = \"https://raw.githubusercontent.com/rickiepark/handson-ml2/master/\"\nfilename = \"ladybug.png\"\nprint(\"Downloading\", filename)\nurl = DOWNLOAD_ROOT + \"images/unsupervised_learning/\" + filename\nurllib.request.urlretrieve(url, os.path.join(images_path, filename))\n\nDownloading ladybug.png\n\n\n('./images/unsupervised_learning/ladybug.png',\n &lt;http.client.HTTPMessage at 0x7efda93af390&gt;)\n\n\n\nfrom matplotlib.image import imread\nimage = imread(os.path.join(images_path, filename))\nimage.shape\n\n(533, 800, 3)\n\n\n\nX = image.reshape(-1, 3)\nkmeans = KMeans(n_clusters=8, random_state=42).fit(X)\nsegmented_img = kmeans.cluster_centers_[kmeans.labels_]\nsegmented_img = segmented_img.reshape(image.shape)\n\n\nsegmented_imgs = []\nn_colors = (10, 8, 6, 4, 2)\nfor n_clusters in n_colors:\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(X)\n    segmented_img = kmeans.cluster_centers_[kmeans.labels_]\n    segmented_imgs.append(segmented_img.reshape(image.shape))\n\n\nplt.figure(figsize=(10,5))\nplt.subplots_adjust(wspace=0.05, hspace=0.1)\n\nplt.subplot(231)\nplt.imshow(image)\nplt.title(\"Original image\")\nplt.axis('off')\n\nfor idx, n_clusters in enumerate(n_colors):\n    plt.subplot(232 + idx)\n    plt.imshow(segmented_imgs[idx])\n    plt.title(\"{} colors\".format(n_clusters))\n    plt.axis('off')\n\nsave_fig('image_segmentation_diagram', tight_layout=False)\nplt.show()\n\n그림 저장: image_segmentation_diagram"
  },
  {
    "objectID": "Machine_Learning/09_unsupervised_learning.html#군집을-사용한-전처리",
    "href": "Machine_Learning/09_unsupervised_learning.html#군집을-사용한-전처리",
    "title": "09_unsupervised_learning",
    "section": "군집을 사용한 전처리",
    "text": "군집을 사용한 전처리\nMNIST와 유사하게 숫자 0에서 9까지 8x8 흑백 이미지 1,797개로 이루어진 숫자 데이터셋 을 다루어 보겠습니다.\n\nfrom sklearn.datasets import load_digits\n\n\nX_digits, y_digits = load_digits(return_X_y=True)\n\n훈련 세트와 테스트 세트로 나눕니다:\n\nfrom sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, random_state=42)\n\n로지스틱 회귀 모델을 훈련하고 테스트 세트에서 평가합니다:\n\nfrom sklearn.linear_model import LogisticRegression\n\n\nlog_reg = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)\nlog_reg.fit(X_train, y_train)\n\nLogisticRegression(max_iter=5000, multi_class='ovr', random_state=42)\n\n\n\nlog_reg_score = log_reg.score(X_test, y_test)\nlog_reg_score\n\n0.9688888888888889\n\n\n좋습니다. 기본 모델의 정확도는 96.89%입니다. 이제 K-평균을 전처리 단계로 사용해 더 향상할 수 있는지 알아 보죠. 훈련 세트를 50개의 클러스터로 만들고 이미지를 이 클러스터까지 거리로 바꾼 다음 로지스틱 회귀 모델을 적용하는 파이프라인을 만듭니다:\n\nfrom sklearn.pipeline import Pipeline\n\n\npipeline = Pipeline([\n    (\"kmeans\", KMeans(n_clusters=50, random_state=42)),\n    (\"log_reg\", LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)),\n])\npipeline.fit(X_train, y_train)\n\nPipeline(steps=[('kmeans', KMeans(n_clusters=50, random_state=42)),\n                ('log_reg',\n                 LogisticRegression(max_iter=5000, multi_class='ovr',\n                                    random_state=42))])\n\n\n\npipeline_score = pipeline.score(X_test, y_test)\npipeline_score\n\n0.9777777777777777\n\n\n얼마나 오차가 감소했나요?\n\n1 - (1 - pipeline_score) / (1 - log_reg_score)\n\n0.28571428571428414\n\n\n어떤가요? 오차율을 35%나 줄였습니다! 하지만 클러스터 개수 \\(k\\)를 임의로 결정했습니다. 이 보다 더 나은 방법이 있겠죠. K-평균을 분류 파이프라인에서 전처리 단계로 사용했기 때문에 좋은 \\(k\\) 값을 찾는 것은 이전보다 더 쉽습니다. 실루엣 분석을 수행하거나 이너셔를 최소화할 필요가 없습니다. 가장 좋은 \\(k\\) 값은 가장 좋은 분류 성능을 내는 것입니다.\n\nfrom sklearn.model_selection import GridSearchCV\n\n경고: 사용하는 하드웨어에 따라 다음 셀을 실행하는데 20분 또는 그 이상 걸릴 수 있습니다.\n\nparam_grid = dict(kmeans__n_clusters=range(2, 100))\ngrid_clf = GridSearchCV(pipeline, param_grid, cv=3, verbose=2)\ngrid_clf.fit(X_train, y_train)\n\nFitting 3 folds for each of 98 candidates, totalling 294 fits\n[CV] END ...............................kmeans__n_clusters=2; total time=   1.4s\n[CV] END ...............................kmeans__n_clusters=2; total time=   1.4s\n[CV] END ...............................kmeans__n_clusters=2; total time=   1.4s\n[CV] END ...............................kmeans__n_clusters=3; total time=   1.4s\n[CV] END ...............................kmeans__n_clusters=3; total time=   1.4s\n[CV] END ...............................kmeans__n_clusters=3; total time=   1.4s\n[CV] END ...............................kmeans__n_clusters=4; total time=   1.4s\n[CV] END ...............................kmeans__n_clusters=4; total time=   1.4s\n[CV] END ...............................kmeans__n_clusters=4; total time=   1.4s\n[CV] END ...............................kmeans__n_clusters=5; total time=   1.5s\n[CV] END ...............................kmeans__n_clusters=5; total time=   1.5s\n[CV] END ...............................kmeans__n_clusters=5; total time=   1.5s\n[CV] END ...............................kmeans__n_clusters=6; total time=   1.5s\n[CV] END ...............................kmeans__n_clusters=6; total time=   1.5s\n[CV] END ...............................kmeans__n_clusters=6; total time=   1.5s\n[CV] END ...............................kmeans__n_clusters=7; total time=   1.6s\n[CV] END ...............................kmeans__n_clusters=7; total time=   1.6s\n[CV] END ...............................kmeans__n_clusters=7; total time=   1.6s\n[CV] END ...............................kmeans__n_clusters=8; total time=   1.6s\n[CV] END ...............................kmeans__n_clusters=8; total time=   1.7s\n[CV] END ...............................kmeans__n_clusters=8; total time=   1.7s\n[CV] END ...............................kmeans__n_clusters=9; total time=   1.8s\n[CV] END ...............................kmeans__n_clusters=9; total time=   1.8s\n[CV] END ...............................kmeans__n_clusters=9; total time=   1.8s\n[CV] END ..............................kmeans__n_clusters=10; total time=   1.9s\n[CV] END ..............................kmeans__n_clusters=10; total time=   1.9s\n[CV] END ..............................kmeans__n_clusters=10; total time=   1.9s\n[CV] END ..............................kmeans__n_clusters=11; total time=   2.5s\n[CV] END ..............................kmeans__n_clusters=11; total time=   2.8s\n[CV] END ..............................kmeans__n_clusters=11; total time=   2.4s\n[CV] END ..............................kmeans__n_clusters=12; total time=   2.7s\n[CV] END ..............................kmeans__n_clusters=12; total time=   2.9s\n[CV] END ..............................kmeans__n_clusters=12; total time=   2.8s\n[CV] END ..............................kmeans__n_clusters=13; total time=   3.1s\n[CV] END ..............................kmeans__n_clusters=13; total time=   3.1s\n[CV] END ..............................kmeans__n_clusters=13; total time=   3.2s\n[CV] END ..............................kmeans__n_clusters=14; total time=   3.4s\n[CV] END ..............................kmeans__n_clusters=14; total time=   3.5s\n[CV] END ..............................kmeans__n_clusters=14; total time=   3.4s\n[CV] END ..............................kmeans__n_clusters=15; total time=   3.7s\n[CV] END ..............................kmeans__n_clusters=15; total time=   3.4s\n[CV] END ..............................kmeans__n_clusters=15; total time=   3.7s\n[CV] END ..............................kmeans__n_clusters=16; total time=   4.0s\n[CV] END ..............................kmeans__n_clusters=16; total time=   3.7s\n[CV] END ..............................kmeans__n_clusters=16; total time=   3.8s\n[CV] END ..............................kmeans__n_clusters=17; total time=   4.1s\n[CV] END ..............................kmeans__n_clusters=17; total time=   4.1s\n[CV] END ..............................kmeans__n_clusters=17; total time=   4.1s\n[CV] END ..............................kmeans__n_clusters=18; total time=   4.5s\n[CV] END ..............................kmeans__n_clusters=18; total time=   4.1s\n[CV] END ..............................kmeans__n_clusters=18; total time=   4.2s\n[CV] END ..............................kmeans__n_clusters=19; total time=   4.7s\n[CV] END ..............................kmeans__n_clusters=19; total time=   4.6s\n[CV] END ..............................kmeans__n_clusters=19; total time=   4.5s\n[CV] END ..............................kmeans__n_clusters=20; total time=   4.9s\n[CV] END ..............................kmeans__n_clusters=20; total time=   4.6s\n[CV] END ..............................kmeans__n_clusters=20; total time=   4.3s\n[CV] END ..............................kmeans__n_clusters=21; total time=   5.1s\n[CV] END ..............................kmeans__n_clusters=21; total time=   5.1s\n[CV] END ..............................kmeans__n_clusters=21; total time=   5.0s\n[CV] END ..............................kmeans__n_clusters=22; total time=   5.4s\n[CV] END ..............................kmeans__n_clusters=22; total time=   5.9s\n[CV] END ..............................kmeans__n_clusters=22; total time=   5.1s\n[CV] END ..............................kmeans__n_clusters=23; total time=   5.4s\n[CV] END ..............................kmeans__n_clusters=23; total time=   5.5s\n[CV] END ..............................kmeans__n_clusters=23; total time=   5.2s\n[CV] END ..............................kmeans__n_clusters=24; total time=   5.7s\n[CV] END ..............................kmeans__n_clusters=24; total time=   5.6s\n[CV] END ..............................kmeans__n_clusters=24; total time=   5.4s\n[CV] END ..............................kmeans__n_clusters=25; total time=   5.5s\n[CV] END ..............................kmeans__n_clusters=25; total time=   5.6s\n[CV] END ..............................kmeans__n_clusters=25; total time=   6.0s\n[CV] END ..............................kmeans__n_clusters=26; total time=   6.0s\n[CV] END ..............................kmeans__n_clusters=26; total time=   6.1s\n[CV] END ..............................kmeans__n_clusters=26; total time=   5.6s\n[CV] END ..............................kmeans__n_clusters=27; total time=   5.6s\n[CV] END ..............................kmeans__n_clusters=27; total time=   6.2s\n[CV] END ..............................kmeans__n_clusters=27; total time=   5.5s\n[CV] END ..............................kmeans__n_clusters=28; total time=   5.9s\n[CV] END ..............................kmeans__n_clusters=28; total time=   6.3s\n[CV] END ..............................kmeans__n_clusters=28; total time=   5.5s\n[CV] END ..............................kmeans__n_clusters=29; total time=   5.9s\n[CV] END ..............................kmeans__n_clusters=29; total time=   7.0s\n[CV] END ..............................kmeans__n_clusters=29; total time=   6.2s\n[CV] END ..............................kmeans__n_clusters=30; total time=   6.4s\n[CV] END ..............................kmeans__n_clusters=30; total time=   6.5s\n[CV] END ..............................kmeans__n_clusters=30; total time=   6.2s\n[CV] END ..............................kmeans__n_clusters=31; total time=   6.8s\n[CV] END ..............................kmeans__n_clusters=31; total time=   5.5s\n[CV] END ..............................kmeans__n_clusters=31; total time=   6.1s\n[CV] END ..............................kmeans__n_clusters=32; total time=   6.4s\n[CV] END ..............................kmeans__n_clusters=32; total time=   6.9s\n[CV] END ..............................kmeans__n_clusters=32; total time=   6.6s\n[CV] END ..............................kmeans__n_clusters=33; total time=   7.0s\n[CV] END ..............................kmeans__n_clusters=33; total time=   7.2s\n[CV] END ..............................kmeans__n_clusters=33; total time=   6.4s\n[CV] END ..............................kmeans__n_clusters=34; total time=   7.6s\n[CV] END ..............................kmeans__n_clusters=34; total time=   7.1s\n[CV] END ..............................kmeans__n_clusters=34; total time=   6.6s\n[CV] END ..............................kmeans__n_clusters=35; total time=   6.7s\n[CV] END ..............................kmeans__n_clusters=35; total time=   7.4s\n[CV] END ..............................kmeans__n_clusters=35; total time=   6.3s\n[CV] END ..............................kmeans__n_clusters=36; total time=   7.1s\n[CV] END ..............................kmeans__n_clusters=36; total time=   7.1s\n[CV] END ..............................kmeans__n_clusters=36; total time=   6.4s\n[CV] END ..............................kmeans__n_clusters=37; total time=   7.5s\n[CV] END ..............................kmeans__n_clusters=37; total time=   7.0s\n[CV] END ..............................kmeans__n_clusters=37; total time=   5.8s\n[CV] END ..............................kmeans__n_clusters=38; total time=   7.1s\n[CV] END ..............................kmeans__n_clusters=38; total time=   6.8s\n[CV] END ..............................kmeans__n_clusters=38; total time=   7.1s\n[CV] END ..............................kmeans__n_clusters=39; total time=   7.1s\n[CV] END ..............................kmeans__n_clusters=39; total time=   8.3s\n[CV] END ..............................kmeans__n_clusters=39; total time=   6.9s\n[CV] END ..............................kmeans__n_clusters=40; total time=   7.3s\n[CV] END ..............................kmeans__n_clusters=40; total time=   7.0s\n[CV] END ..............................kmeans__n_clusters=40; total time=   7.2s\n[CV] END ..............................kmeans__n_clusters=41; total time=   7.7s\n[CV] END ..............................kmeans__n_clusters=41; total time=   7.2s\n[CV] END ..............................kmeans__n_clusters=41; total time=   7.6s\n[CV] END ..............................kmeans__n_clusters=42; total time=   7.1s\n[CV] END ..............................kmeans__n_clusters=42; total time=   7.4s\n[CV] END ..............................kmeans__n_clusters=42; total time=   7.2s\n[CV] END ..............................kmeans__n_clusters=43; total time=   7.4s\n[CV] END ..............................kmeans__n_clusters=43; total time=   7.3s\n[CV] END ..............................kmeans__n_clusters=43; total time=   7.1s\n[CV] END ..............................kmeans__n_clusters=44; total time=   7.1s\n[CV] END ..............................kmeans__n_clusters=44; total time=   6.9s\n[CV] END ..............................kmeans__n_clusters=44; total time=   6.4s\n[CV] END ..............................kmeans__n_clusters=45; total time=   7.1s\n[CV] END ..............................kmeans__n_clusters=45; total time=   7.4s\n[CV] END ..............................kmeans__n_clusters=45; total time=   7.0s\n[CV] END ..............................kmeans__n_clusters=46; total time=   6.8s\n[CV] END ..............................kmeans__n_clusters=46; total time=   7.7s\n[CV] END ..............................kmeans__n_clusters=46; total time=   7.3s\n[CV] END ..............................kmeans__n_clusters=47; total time=   8.1s\n[CV] END ..............................kmeans__n_clusters=47; total time=   7.1s\n[CV] END ..............................kmeans__n_clusters=47; total time=   7.1s\n[CV] END ..............................kmeans__n_clusters=48; total time=   7.8s\n[CV] END ..............................kmeans__n_clusters=48; total time=   7.5s\n[CV] END ..............................kmeans__n_clusters=48; total time=   6.6s\n[CV] END ..............................kmeans__n_clusters=49; total time=   8.4s\n[CV] END ..............................kmeans__n_clusters=49; total time=   6.9s\n[CV] END ..............................kmeans__n_clusters=49; total time=   8.2s\n[CV] END ..............................kmeans__n_clusters=50; total time=   7.4s\n[CV] END ..............................kmeans__n_clusters=50; total time=   7.3s\n[CV] END ..............................kmeans__n_clusters=50; total time=   7.9s\n[CV] END ..............................kmeans__n_clusters=51; total time=   7.7s\n[CV] END ..............................kmeans__n_clusters=51; total time=   8.1s\n[CV] END ..............................kmeans__n_clusters=51; total time=   7.2s\n[CV] END ..............................kmeans__n_clusters=52; total time=   7.0s\n[CV] END ..............................kmeans__n_clusters=52; total time=   7.8s\n[CV] END ..............................kmeans__n_clusters=52; total time=   8.0s\n[CV] END ..............................kmeans__n_clusters=53; total time=   7.4s\n[CV] END ..............................kmeans__n_clusters=53; total time=   7.4s\n[CV] END ..............................kmeans__n_clusters=53; total time=   8.0s\n[CV] END ..............................kmeans__n_clusters=54; total time=   7.9s\n[CV] END ..............................kmeans__n_clusters=54; total time=   7.3s\n[CV] END ..............................kmeans__n_clusters=54; total time=   7.0s\n[CV] END ..............................kmeans__n_clusters=55; total time=   7.4s\n[CV] END ..............................kmeans__n_clusters=55; total time=   8.1s\n[CV] END ..............................kmeans__n_clusters=55; total time=   9.6s\n[CV] END ..............................kmeans__n_clusters=56; total time=   7.2s\n[CV] END ..............................kmeans__n_clusters=56; total time=   6.9s\n[CV] END ..............................kmeans__n_clusters=56; total time=   9.0s\n[CV] END ..............................kmeans__n_clusters=57; total time=   7.7s\n[CV] END ..............................kmeans__n_clusters=57; total time=   8.0s\n[CV] END ..............................kmeans__n_clusters=57; total time=   7.6s\n[CV] END ..............................kmeans__n_clusters=58; total time=   7.8s\n[CV] END ..............................kmeans__n_clusters=58; total time=   8.4s\n[CV] END ..............................kmeans__n_clusters=58; total time=   7.4s\n[CV] END ..............................kmeans__n_clusters=59; total time=   8.4s\n[CV] END ..............................kmeans__n_clusters=59; total time=   7.8s\n[CV] END ..............................kmeans__n_clusters=59; total time=   7.3s\n[CV] END ..............................kmeans__n_clusters=60; total time=   7.6s\n[CV] END ..............................kmeans__n_clusters=60; total time=   8.6s\n[CV] END ..............................kmeans__n_clusters=60; total time=   8.0s\n[CV] END ..............................kmeans__n_clusters=61; total time=   7.2s\n[CV] END ..............................kmeans__n_clusters=61; total time=   7.5s\n[CV] END ..............................kmeans__n_clusters=61; total time=   7.5s\n[CV] END ..............................kmeans__n_clusters=62; total time=   7.8s\n[CV] END ..............................kmeans__n_clusters=62; total time=   8.5s\n[CV] END ..............................kmeans__n_clusters=62; total time=   6.9s\n[CV] END ..............................kmeans__n_clusters=63; total time=   7.5s\n[CV] END ..............................kmeans__n_clusters=63; total time=   7.9s\n[CV] END ..............................kmeans__n_clusters=63; total time=   7.8s\n[CV] END ..............................kmeans__n_clusters=64; total time=   8.1s\n[CV] END ..............................kmeans__n_clusters=64; total time=   8.3s\n[CV] END ..............................kmeans__n_clusters=64; total time=   7.2s\n[CV] END ..............................kmeans__n_clusters=65; total time=   7.6s\n[CV] END ..............................kmeans__n_clusters=65; total time=   8.4s\n[CV] END ..............................kmeans__n_clusters=65; total time=   7.8s\n[CV] END ..............................kmeans__n_clusters=66; total time=   8.6s\n[CV] END ..............................kmeans__n_clusters=66; total time=   8.0s\n[CV] END ..............................kmeans__n_clusters=66; total time=   7.6s\n[CV] END ..............................kmeans__n_clusters=67; total time=   8.3s\n[CV] END ..............................kmeans__n_clusters=67; total time=   8.1s\n[CV] END ..............................kmeans__n_clusters=67; total time=   7.5s\n[CV] END ..............................kmeans__n_clusters=68; total time=   8.2s\n[CV] END ..............................kmeans__n_clusters=68; total time=   8.1s\n[CV] END ..............................kmeans__n_clusters=68; total time=   7.8s\n[CV] END ..............................kmeans__n_clusters=69; total time=   7.6s\n[CV] END ..............................kmeans__n_clusters=69; total time=   8.8s\n[CV] END ..............................kmeans__n_clusters=69; total time=   8.2s\n[CV] END ..............................kmeans__n_clusters=70; total time=   8.1s\n[CV] END ..............................kmeans__n_clusters=70; total time=   8.3s\n[CV] END ..............................kmeans__n_clusters=70; total time=   8.0s\n[CV] END ..............................kmeans__n_clusters=71; total time=   9.2s\n[CV] END ..............................kmeans__n_clusters=71; total time=   8.2s\n[CV] END ..............................kmeans__n_clusters=71; total time=   8.1s\n[CV] END ..............................kmeans__n_clusters=72; total time=   7.6s\n[CV] END ..............................kmeans__n_clusters=72; total time=   8.5s\n[CV] END ..............................kmeans__n_clusters=72; total time=   7.5s\n[CV] END ..............................kmeans__n_clusters=73; total time=   8.0s\n[CV] END ..............................kmeans__n_clusters=73; total time=   8.2s\n[CV] END ..............................kmeans__n_clusters=73; total time=   7.8s\n[CV] END ..............................kmeans__n_clusters=74; total time=   8.4s\n[CV] END ..............................kmeans__n_clusters=74; total time=   8.8s\n[CV] END ..............................kmeans__n_clusters=74; total time=   8.2s\n[CV] END ..............................kmeans__n_clusters=75; total time=   7.9s\n[CV] END ..............................kmeans__n_clusters=75; total time=   8.2s\n[CV] END ..............................kmeans__n_clusters=75; total time=   8.3s\n[CV] END ..............................kmeans__n_clusters=76; total time=   8.5s\n[CV] END ..............................kmeans__n_clusters=76; total time=   7.7s\n[CV] END ..............................kmeans__n_clusters=76; total time=   6.9s\n[CV] END ..............................kmeans__n_clusters=77; total time=   8.6s\n[CV] END ..............................kmeans__n_clusters=77; total time=   8.8s\n[CV] END ..............................kmeans__n_clusters=77; total time=   7.7s\n[CV] END ..............................kmeans__n_clusters=78; total time=   8.1s\n[CV] END ..............................kmeans__n_clusters=78; total time=   8.9s\n[CV] END ..............................kmeans__n_clusters=78; total time=   8.5s\n[CV] END ..............................kmeans__n_clusters=79; total time=   8.4s\n[CV] END ..............................kmeans__n_clusters=79; total time=   8.4s\n[CV] END ..............................kmeans__n_clusters=79; total time=   7.0s\n[CV] END ..............................kmeans__n_clusters=80; total time=   8.3s\n[CV] END ..............................kmeans__n_clusters=80; total time=  10.2s\n[CV] END ..............................kmeans__n_clusters=80; total time=  11.0s\n[CV] END ..............................kmeans__n_clusters=81; total time=   8.4s\n[CV] END ..............................kmeans__n_clusters=81; total time=   7.8s\n[CV] END ..............................kmeans__n_clusters=81; total time=   7.9s\n[CV] END ..............................kmeans__n_clusters=82; total time=   8.2s\n[CV] END ..............................kmeans__n_clusters=82; total time=   9.0s\n[CV] END ..............................kmeans__n_clusters=82; total time=   8.5s\n[CV] END ..............................kmeans__n_clusters=83; total time=   8.1s\n[CV] END ..............................kmeans__n_clusters=83; total time=   8.0s\n[CV] END ..............................kmeans__n_clusters=83; total time=   8.0s\n[CV] END ..............................kmeans__n_clusters=84; total time=   8.2s\n[CV] END ..............................kmeans__n_clusters=84; total time=   8.3s\n[CV] END ..............................kmeans__n_clusters=84; total time=   7.7s\n[CV] END ..............................kmeans__n_clusters=85; total time=   8.9s\n[CV] END ..............................kmeans__n_clusters=85; total time=   8.8s\n[CV] END ..............................kmeans__n_clusters=85; total time=   7.5s\n[CV] END ..............................kmeans__n_clusters=86; total time=   7.9s\n[CV] END ..............................kmeans__n_clusters=86; total time=   8.1s\n[CV] END ..............................kmeans__n_clusters=86; total time=   8.4s\n[CV] END ..............................kmeans__n_clusters=87; total time=   8.2s\n[CV] END ..............................kmeans__n_clusters=87; total time=   8.6s\n[CV] END ..............................kmeans__n_clusters=87; total time=   8.7s\n[CV] END ..............................kmeans__n_clusters=88; total time=   7.8s\n[CV] END ..............................kmeans__n_clusters=88; total time=   8.6s\n[CV] END ..............................kmeans__n_clusters=88; total time=   8.3s\n[CV] END ..............................kmeans__n_clusters=89; total time=   7.8s\n[CV] END ..............................kmeans__n_clusters=89; total time=   8.1s\n[CV] END ..............................kmeans__n_clusters=89; total time=   8.4s\n[CV] END ..............................kmeans__n_clusters=90; total time=   8.2s\n[CV] END ..............................kmeans__n_clusters=90; total time=   8.3s\n[CV] END ..............................kmeans__n_clusters=90; total time=   7.8s\n[CV] END ..............................kmeans__n_clusters=91; total time=   8.1s\n[CV] END ..............................kmeans__n_clusters=91; total time=   7.8s\n[CV] END ..............................kmeans__n_clusters=91; total time=   8.5s\n[CV] END ..............................kmeans__n_clusters=92; total time=   8.5s\n[CV] END ..............................kmeans__n_clusters=92; total time=   8.3s\n[CV] END ..............................kmeans__n_clusters=92; total time=   8.3s\n[CV] END ..............................kmeans__n_clusters=93; total time=   7.9s\n[CV] END ..............................kmeans__n_clusters=93; total time=   9.1s\n[CV] END ..............................kmeans__n_clusters=93; total time=   7.5s\n[CV] END ..............................kmeans__n_clusters=94; total time=   7.8s\n[CV] END ..............................kmeans__n_clusters=94; total time=   9.6s\n[CV] END ..............................kmeans__n_clusters=94; total time=   7.1s\n[CV] END ..............................kmeans__n_clusters=95; total time=   7.8s\n[CV] END ..............................kmeans__n_clusters=95; total time=   7.3s\n[CV] END ..............................kmeans__n_clusters=95; total time=   7.6s\n[CV] END ..............................kmeans__n_clusters=96; total time=   7.9s\n[CV] END ..............................kmeans__n_clusters=96; total time=   8.8s\n[CV] END ..............................kmeans__n_clusters=96; total time=   7.7s\n[CV] END ..............................kmeans__n_clusters=97; total time=   8.0s\n[CV] END ..............................kmeans__n_clusters=97; total time=   9.5s\n[CV] END ..............................kmeans__n_clusters=97; total time=   7.9s\n[CV] END ..............................kmeans__n_clusters=98; total time=   7.4s\n[CV] END ..............................kmeans__n_clusters=98; total time=   8.3s\n[CV] END ..............................kmeans__n_clusters=98; total time=   7.7s\n[CV] END ..............................kmeans__n_clusters=99; total time=   7.7s\n[CV] END ..............................kmeans__n_clusters=99; total time=   7.8s\n[CV] END ..............................kmeans__n_clusters=99; total time=   8.0s\n\n\nGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('kmeans',\n                                        KMeans(n_clusters=50, random_state=42)),\n                                       ('log_reg',\n                                        LogisticRegression(max_iter=5000,\n                                                           multi_class='ovr',\n                                                           random_state=42))]),\n             param_grid={'kmeans__n_clusters': range(2, 100)}, verbose=2)\n\n\n최고의 클러스터 개수를 확인해 보죠:\n\ngrid_clf.best_params_\n\n{'kmeans__n_clusters': 88}\n\n\n\ngrid_clf.score(X_test, y_test)\n\n0.9822222222222222"
  },
  {
    "objectID": "Machine_Learning/09_unsupervised_learning.html#군집을-사용한-준지도-학습",
    "href": "Machine_Learning/09_unsupervised_learning.html#군집을-사용한-준지도-학습",
    "title": "09_unsupervised_learning",
    "section": "군집을 사용한 준지도 학습",
    "text": "군집을 사용한 준지도 학습\n군집의 또 다른 사용처는 레이블이 없는 샘플이 많고 레이블이 있는 샘플이 적을 때 사용하는 준지도 학습입니다.\n레이블을 가진 샘플이 50개만 있을 때 로지스틱 회귀 모델의 성능을 확인해 보죠:\n\nn_labeled = 50\n\n\nlog_reg = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", random_state=42)\nlog_reg.fit(X_train[:n_labeled], y_train[:n_labeled])\nlog_reg.score(X_test, y_test)\n\n0.8333333333333334\n\n\n당연히 이전보다 훨씬 낮네요. 어떻게 더 향상할 수 있는지 알아 보겠습니다. 먼저 훈련 세트를 클러스터 50개로 군집합니다. 그다음 각 클러스터에서 센트로이드에 가장 가까운 이미지를 찾습니다. 이 이미지를 대표 이미지라고 부르겠습니다:\n\nk = 50\n\n\nkmeans = KMeans(n_clusters=k, random_state=42)\nX_digits_dist = kmeans.fit_transform(X_train)\nrepresentative_digit_idx = np.argmin(X_digits_dist, axis=0)\nX_representative_digits = X_train[representative_digit_idx]\n\n대표 이미지를 출력하고 수동으로 레이블을 매겨 보겠습니다:\n\nplt.figure(figsize=(8, 2))\nfor index, X_representative_digit in enumerate(X_representative_digits):\n    plt.subplot(k // 10, 10, index + 1)\n    plt.imshow(X_representative_digit.reshape(8, 8), cmap=\"binary\", interpolation=\"bilinear\")\n    plt.axis('off')\n\nsave_fig(\"representative_images_diagram\", tight_layout=False)\nplt.show()\n\n그림 저장: representative_images_diagram\n\n\n\n\n\n\ny_train[representative_digit_idx]\n\narray([4, 8, 0, 6, 8, 3, 7, 7, 9, 2, 5, 5, 8, 5, 2, 1, 2, 9, 6, 1, 1, 6,\n       9, 0, 8, 3, 0, 7, 4, 1, 6, 5, 2, 4, 1, 8, 6, 3, 9, 2, 4, 2, 9, 4,\n       7, 6, 2, 3, 1, 1])\n\n\n\ny_representative_digits = np.array([\n    0, 1, 3, 2, 7, 6, 4, 6, 9, 5,\n    1, 2, 9, 5, 2, 7, 8, 1, 8, 6,\n    3, 1, 5, 4, 5, 4, 0, 3, 2, 6,\n    1, 7, 7, 9, 1, 8, 6, 5, 4, 8,\n    5, 3, 3, 6, 7, 9, 7, 8, 4, 9])\n\n이 데이터셋은 레이블이 있는 샘플이 50개뿐이지만 완전히 랜덤한 샘플이 아니라 각 샘플은 클러스터의 대표 이미지입니다. 성능이 더 나은지 확인해 보죠:\n\nlog_reg = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)\nlog_reg.fit(X_representative_digits, y_representative_digits)\nlog_reg.score(X_test, y_test)\n\n0.09555555555555556\n\n\n와우! 83.3%에서 91.3%로 확 올랐네요. 여전히 50개의 샘플로만 모델을 훈련했습니다. 샘플에 레이블을 다는 것은 비용이 많이 들고 어려운 작업입니다. 특히 전문가가 수동으로 작업할 때 그렇습니다. 이 때 랜덤한 샘플보다는 대표 샘플에 레이블을 다는 것이 좋은 생각입니다.\n하지만 더 향상시킬 수 있습니다. 이 레이블을 같은 클러스터에 있는 다른 모든 샘플에 전파하면 어떨까요?\n\ny_train_propagated = np.empty(len(X_train), dtype=np.int32)\nfor i in range(k):\n    y_train_propagated[kmeans.labels_==i] = y_representative_digits[i]\n\n\nlog_reg = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)\nlog_reg.fit(X_train, y_train_propagated)\n\nLogisticRegression(max_iter=5000, multi_class='ovr', random_state=42)\n\n\n\nlog_reg.score(X_test, y_test)\n\n0.15333333333333332\n\n\n아주 조금 정확도를 높였습니다. 없는 것보다는 낫지만 센트로이드에 가까운 샘플에만 레이블을 전파하는 것이 나을지 모릅니다. 왜냐하면 전체 클러스터에 전파하면 일부 이상치를 포함하기 때문입니다. 레이블을 센트로이드에 가까운 75번째 백분위수까지만 전파해보죠:\n\npercentile_closest = 75\n\nX_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]\nfor i in range(k):\n    in_cluster = (kmeans.labels_ == i)\n    cluster_dist = X_cluster_dist[in_cluster]\n    cutoff_distance = np.percentile(cluster_dist, percentile_closest)\n    above_cutoff = (X_cluster_dist &gt; cutoff_distance)\n    X_cluster_dist[in_cluster & above_cutoff] = -1\n\n\npartially_propagated = (X_cluster_dist != -1)\nX_train_partially_propagated = X_train[partially_propagated]\ny_train_partially_propagated = y_train_propagated[partially_propagated]\n\n\nlog_reg = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)\nlog_reg.fit(X_train_partially_propagated, y_train_partially_propagated)\n\nLogisticRegression(max_iter=5000, multi_class='ovr', random_state=42)\n\n\n\nlog_reg.score(X_test, y_test)\n\n0.15777777777777777\n\n\n조금 더 낫네요. 레이블된 샘플 50개(클래스당 평균 5개 샘플!)만 가지고 92.7% 성능을 달성했습니다. 레이블된 전체 숫자 데이터셋에서 훈련한 로지스틱 회귀의 성능(96.9%)과 매우 비슷합니다.\n이는 전파된 레이블이 실제로 매우 좋기 때문입니다. 이 정확도는 96%에 가깝습니다:\n\nnp.mean(y_train_partially_propagated == y_train[partially_propagated])\n\n0.19541375872382852\n\n\n능동 학습을 여러 번 반복할 수 있습니다: 1. 분류기의 확신이 부족한 샘플에 수동으로 레이블을 부여합니다. 가능하면 다른 클러스터에서 샘플을 선택합니다. 2. 추가된 레이블을 사용해 새로운 모델을 훈련합니다."
  },
  {
    "objectID": "Machine_Learning/09_unsupervised_learning.html#dbscan",
    "href": "Machine_Learning/09_unsupervised_learning.html#dbscan",
    "title": "09_unsupervised_learning",
    "section": "DBSCAN",
    "text": "DBSCAN\n\nfrom sklearn.datasets import make_moons\n\n\nX, y = make_moons(n_samples=1000, noise=0.05, random_state=42)\n\n\nfrom sklearn.cluster import DBSCAN\n\n\ndbscan = DBSCAN(eps=0.05, min_samples=5)\ndbscan.fit(X)\n\nDBSCAN(eps=0.05)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DBSCANDBSCAN(eps=0.05)\n\n\n\ndbscan.labels_[:10]\n\narray([ 0,  2, -1, -1,  1,  0,  0,  0,  2,  5], dtype=int64)\n\n\n\nlen(dbscan.core_sample_indices_)\n\n808\n\n\n\ndbscan.core_sample_indices_[:10]\n\narray([ 0,  4,  5,  6,  7,  8, 10, 11, 12, 13], dtype=int64)\n\n\n\ndbscan.components_[:3]\n\narray([[-0.02137124,  0.40618608],\n       [-0.84192557,  0.53058695],\n       [ 0.58930337, -0.32137599]])\n\n\n\nnp.unique(dbscan.labels_)\n\narray([-1,  0,  1,  2,  3,  4,  5,  6])\n\n\n\ndbscan2 = DBSCAN(eps=0.2)\ndbscan2.fit(X)\n\nDBSCAN(eps=0.2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DBSCANDBSCAN(eps=0.2)\n\n\n\ndef plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):\n    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n    core_mask[dbscan.core_sample_indices_] = True\n    anomalies_mask = dbscan.labels_ == -1\n    non_core_mask = ~(core_mask | anomalies_mask)\n\n    cores = dbscan.components_\n    anomalies = X[anomalies_mask]\n    non_cores = X[non_core_mask]\n    \n    plt.scatter(cores[:, 0], cores[:, 1],\n                c=dbscan.labels_[core_mask], marker='o', s=size, cmap=\"Paired\")\n    plt.scatter(cores[:, 0], cores[:, 1], marker='*', s=20, c=dbscan.labels_[core_mask])\n    plt.scatter(anomalies[:, 0], anomalies[:, 1],\n                c=\"r\", marker=\"x\", s=100)\n    plt.scatter(non_cores[:, 0], non_cores[:, 1], c=dbscan.labels_[non_core_mask], marker=\".\")\n    if show_xlabels:\n        plt.xlabel(\"$x_1$\", fontsize=14)\n    else:\n        plt.tick_params(labelbottom=False)\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n    plt.title(\"eps={:.2f}, min_samples={}\".format(dbscan.eps, dbscan.min_samples), fontsize=14)\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nplt.figure(figsize=(9, 3.2))\n\nplt.subplot(121)\nplot_dbscan(dbscan, X, size=100)\n\nplt.subplot(122)\nplot_dbscan(dbscan2, X, size=600, show_ylabels=False)\n\n#save_fig(\"dbscan_plot\")\nplt.show()\n\n\n\n\n\ndbscan = dbscan2\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\nknn = KNeighborsClassifier(n_neighbors=50)\nknn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])\n\nKNeighborsClassifier(n_neighbors=50)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier(n_neighbors=50)\n\n\n\nX_new = np.array([[-0.5, 0], [0, 0.5], [1, -0.1], [2, 1]])\nknn.predict(X_new)\n\narray([1, 0, 1, 0], dtype=int64)\n\n\n\nknn.predict_proba(X_new)\n\narray([[0.18, 0.82],\n       [1.  , 0.  ],\n       [0.12, 0.88],\n       [1.  , 0.  ]])\n\n\n\nplt.figure(figsize=(6, 3))\nplot_decision_boundaries(knn, X, show_centroids=False)\nplt.scatter(X_new[:, 0], X_new[:, 1], c=\"b\", marker=\"+\", s=200, zorder=10)\n#save_fig(\"cluster_classification_plot\")\nplt.show()\n\n\ny_dist, y_pred_idx = knn.kneighbors(X_new, n_neighbors=1)\ny_pred = dbscan.labels_[dbscan.core_sample_indices_][y_pred_idx]\ny_pred[y_dist &gt; 0.2] = -1\ny_pred.ravel()\n\narray([-1,  0,  1, -1], dtype=int64)"
  },
  {
    "objectID": "Machine_Learning/09_unsupervised_learning.html#다른-군집-알고리즘",
    "href": "Machine_Learning/09_unsupervised_learning.html#다른-군집-알고리즘",
    "title": "09_unsupervised_learning",
    "section": "다른 군집 알고리즘",
    "text": "다른 군집 알고리즘\n\n스펙트럼 군집\n\nfrom sklearn.cluster import SpectralClustering\n\n\nsc1 = SpectralClustering(n_clusters=2, gamma=100, random_state=42)\nsc1.fit(X)\n\nSpectralClustering(gamma=100, n_clusters=2, random_state=42)\n\n\n\nsc2 = SpectralClustering(n_clusters=2, gamma=1, random_state=42)\nsc2.fit(X)\n\nSpectralClustering(gamma=1, n_clusters=2, random_state=42)\n\n\n\nnp.percentile(sc1.affinity_matrix_, 95)\n\n0.04251990648936265\n\n\n\ndef plot_spectral_clustering(sc, X, size, alpha, show_xlabels=True, show_ylabels=True):\n    plt.scatter(X[:, 0], X[:, 1], marker='o', s=size, c='gray', cmap=\"Paired\", alpha=alpha)\n    plt.scatter(X[:, 0], X[:, 1], marker='o', s=30, c='w')\n    plt.scatter(X[:, 0], X[:, 1], marker='.', s=10, c=sc.labels_, cmap=\"Paired\")\n    \n    if show_xlabels:\n        plt.xlabel(\"$x_1$\", fontsize=14)\n    else:\n        plt.tick_params(labelbottom=False)\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n    plt.title(\"RBF gamma={}\".format(sc.gamma), fontsize=14)\n\n\nplt.figure(figsize=(9, 3.2))\n\nplt.subplot(121)\nplot_spectral_clustering(sc1, X, size=500, alpha=0.1)\n\nplt.subplot(122)\nplot_spectral_clustering(sc2, X, size=4000, alpha=0.01, show_ylabels=False)\n\nplt.show()\n\n\n\n\n\n\n병합 군집\n\nfrom sklearn.cluster import AgglomerativeClustering\n\n\nX = np.array([0, 2, 5, 8.5]).reshape(-1, 1)\nagg = AgglomerativeClustering(linkage=\"complete\").fit(X)\n\n\ndef learned_parameters(estimator):\n    return [attrib for attrib in dir(estimator)\n            if attrib.endswith(\"_\") and not attrib.startswith(\"_\")]\n\n\nlearned_parameters(agg)\n\n['children_',\n 'labels_',\n 'n_clusters_',\n 'n_connected_components_',\n 'n_features_in_',\n 'n_leaves_']\n\n\n\nagg.children_\n\narray([[0, 1],\n       [2, 3],\n       [4, 5]])"
  },
  {
    "objectID": "Machine_Learning/09_unsupervised_learning.html#가우시안-혼합을-사용한-이상치-탐지",
    "href": "Machine_Learning/09_unsupervised_learning.html#가우시안-혼합을-사용한-이상치-탐지",
    "title": "09_unsupervised_learning",
    "section": "가우시안 혼합을 사용한 이상치 탐지",
    "text": "가우시안 혼합을 사용한 이상치 탐지\n가우시안 혼합을 이상치 탐지 에 사용할 수 있습니다: 밀도가 낮은 지역에 있는 샘플을 이상치로 생각할 수 있습니다. 사용할 밀도 임곗값을 결정해야 합니다. 예를 들어 한 제조 회사에서 결함 제품을 감지하려고 합니다. 결함 제품의 비율은 잘 알려져 있습니다. 이 비율이 4%라고 하면 밀도 임곗값을 이 값으로 지정하여 임계 밀도보다 낮은 지역에 있는 샘플을 얻을 수 있습니다:\n\ndensities = gm.score_samples(X)\ndensity_threshold = np.percentile(densities, 4)\nanomalies = X[densities &lt; density_threshold]\n\n\nplt.figure(figsize=(8, 4))\n\nplot_gaussian_mixture(gm, X)\nplt.scatter(anomalies[:, 0], anomalies[:, 1], color='r', marker='*')\nplt.ylim(top=5.1)\n\nsave_fig(\"mixture_anomaly_detection_plot\")\nplt.show()\n\n그림 저장: mixture_anomaly_detection_plot"
  },
  {
    "objectID": "Machine_Learning/09_unsupervised_learning.html#모델-선택",
    "href": "Machine_Learning/09_unsupervised_learning.html#모델-선택",
    "title": "09_unsupervised_learning",
    "section": "모델 선택",
    "text": "모델 선택\n이너셔나 실루엣 점수는 모두 원형 클러스터를 가정하기 때문에 가우시안 혼합 모델에 사용할 수 없습니다. 대신 BIC(Bayesian Information Criterion)나 AIC(Akaike Information Criterion) 같은 이론적 정보 기준을 최소화하는 모델을 찾을 수 있습니다:\n\\({BIC} = {\\log(m)p - 2\\log({\\hat L})}\\)\n\\({AIC} = 2p - 2\\log(\\hat L)\\)\n\n\\(m\\)은 샘플의 개수입니다.\n\\(p\\)는 모델이 학습할 파라미터 개수입니다.\n\\(\\hat L\\)은 모델의 가능도 함수의 최댓값입니다. 이는 모델과 최적의 파라미터가 주어졌을 때 관측 데이터 \\(\\mathbf{X}\\)의 조건부 확률입니다.\n\nBIC와 AIC 모두 모델이 많은 파라미터(예를 들면 많은 클러스터)를 학습하지 못하도록 제한합니다. 그리고 데이터에 잘 맞는 모델(즉, 관측 데이터에 가능도가 높은 모델)에 보상을 줍니다.\n\ngm.bic(X)\n\n8189.747000497186\n\n\n\ngm.aic(X)\n\n8102.521720382148\n\n\n다음과 같이 BIC를 수동으로 계산할 수 있습니다:\n\nn_clusters = 3\nn_dims = 2\nn_params_for_weights = n_clusters - 1\nn_params_for_means = n_clusters * n_dims\nn_params_for_covariance = n_clusters * n_dims * (n_dims + 1) // 2\nn_params = n_params_for_weights + n_params_for_means + n_params_for_covariance\nmax_log_likelihood = gm.score(X) * len(X) # log(L^)\nbic = np.log(len(X)) * n_params - 2 * max_log_likelihood\naic = 2 * n_params - 2 * max_log_likelihood\n\n\nbic, aic\n\n(8189.747000497186, 8102.521720382148)\n\n\n\nn_params\n\n17\n\n\n클러스터마다 하나의 가중치가 있지만 모두 더하면 1이 되어야 합니다. 따라서 자유도는 1이 줄어듭니다. 비슷하게 \\(n \\times n\\) 공분산 행렬의 자유도는 \\(n^2\\)가 아니라 \\(1 + 2 + \\dots + n = \\dfrac{n (n+1)}{2}\\)입니다.\n여러 가지 \\(k\\) 값에 대해 가우시안 혼합 모델을 훈련하고 BIC를 측정해 보죠:\n\ngms_per_k = [GaussianMixture(n_components=k, n_init=10, random_state=42).fit(X)\n             for k in range(1, 11)]\n\n\nbics = [model.bic(X) for model in gms_per_k]\naics = [model.aic(X) for model in gms_per_k]\n\n\nplt.figure(figsize=(8, 3))\nplt.plot(range(1, 11), bics, \"bo-\", label=\"BIC\")\nplt.plot(range(1, 11), aics, \"go--\", label=\"AIC\")\nplt.xlabel(\"$k$\", fontsize=14)\nplt.ylabel(\"Information Criterion\", fontsize=14)\nplt.axis([1, 9.5, np.min(aics) - 50, np.max(aics) + 50])\nplt.annotate('Minimum',\n             xy=(3, bics[2]),\n             xytext=(0.35, 0.6),\n             textcoords='figure fraction',\n             fontsize=14,\n             arrowprops=dict(facecolor='black', shrink=0.1)\n            )\nplt.legend()\nsave_fig(\"aic_bic_vs_k_plot\")\nplt.show()\n\n그림 저장: aic_bic_vs_k_plot\n\n\n\n\n\n클러스터 개수와 covariance_type 하이퍼파라미터의 최적 조합을 찾아 보겠습니다:\n\nmin_bic = np.infty\n\nfor k in range(1, 11):\n    for covariance_type in (\"full\", \"tied\", \"spherical\", \"diag\"):\n        bic = GaussianMixture(n_components=k, n_init=10,\n                              covariance_type=covariance_type,\n                              random_state=42).fit(X).bic(X)\n        if bic &lt; min_bic:\n            min_bic = bic\n            best_k = k\n            best_covariance_type = covariance_type\n\n\nbest_k\n\n3\n\n\n\nbest_covariance_type\n\n'full'"
  },
  {
    "objectID": "Machine_Learning/09_unsupervised_learning.html#베이즈-가우시안-혼합-모델",
    "href": "Machine_Learning/09_unsupervised_learning.html#베이즈-가우시안-혼합-모델",
    "title": "09_unsupervised_learning",
    "section": "베이즈 가우시안 혼합 모델",
    "text": "베이즈 가우시안 혼합 모델\n최적의 클러스터 개수를 수동으로 찾는 대신 BayesianGaussianMixture 클래스를 사용해 불필요한 클러스터의 가중치를 0으로 (또는 0에 가깝게) 만들 수 있습니다. 최적의 클러스터 개수보다 큰 컴포넌트의 개수를 지정하면 됩니다. 이 알고리즘은 자동으로 불필요한 클러스터를 제거합니다:\n\nfrom sklearn.mixture import BayesianGaussianMixture\n\n\nbgm = BayesianGaussianMixture(n_components=10, n_init=10, random_state=42)\nbgm.fit(X)\n\nBayesianGaussianMixture(n_components=10, n_init=10, random_state=42)\n\n\n알고리즘이 자동으로 3개의 컴포넌트가 필요하다는 것을 감지했습니다:\n\nnp.round(bgm.weights_, 2)\n\narray([0.4 , 0.21, 0.4 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ])\n\n\n\nplt.figure(figsize=(8, 5))\nplot_gaussian_mixture(bgm, X)\nplt.show()\n\n\n\n\n\nbgm_low = BayesianGaussianMixture(n_components=10, max_iter=1000, n_init=1,\n                                  weight_concentration_prior=0.01, random_state=42)\nbgm_high = BayesianGaussianMixture(n_components=10, max_iter=1000, n_init=1,\n                                  weight_concentration_prior=10000, random_state=42)\nnn = 73\nbgm_low.fit(X[:nn])\nbgm_high.fit(X[:nn])\n\nBayesianGaussianMixture(max_iter=1000, n_components=10, random_state=42,\n                        weight_concentration_prior=10000)\n\n\n\nnp.round(bgm_low.weights_, 2)\n\narray([0.52, 0.48, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ])\n\n\n\nnp.round(bgm_high.weights_, 2)\n\narray([0.01, 0.18, 0.27, 0.11, 0.01, 0.01, 0.01, 0.01, 0.37, 0.01])\n\n\n\nplt.figure(figsize=(9, 4))\n\nplt.subplot(121)\nplot_gaussian_mixture(bgm_low, X[:nn])\nplt.title(\"weight_concentration_prior = 0.01\", fontsize=14)\n\nplt.subplot(122)\nplot_gaussian_mixture(bgm_high, X[:nn], show_ylabels=False)\nplt.title(\"weight_concentration_prior = 10000\", fontsize=14)\n\nsave_fig(\"mixture_concentration_prior_plot\")\nplt.show()\n\n그림 저장: mixture_concentration_prior_plot\n\n\n\n\n\n\nX_moons, y_moons = make_moons(n_samples=1000, noise=0.05, random_state=42)\n\n\nbgm = BayesianGaussianMixture(n_components=10, n_init=10, random_state=42)\nbgm.fit(X_moons)\n\nBayesianGaussianMixture(n_components=10, n_init=10, random_state=42)\n\n\n\nplt.figure(figsize=(9, 3.2))\n\nplt.subplot(121)\nplot_data(X_moons)\nplt.xlabel(\"$x_1$\", fontsize=14)\nplt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n\nplt.subplot(122)\nplot_gaussian_mixture(bgm, X_moons, show_ylabels=False)\n\nsave_fig(\"moons_vs_bgm_plot\")\nplt.show()\n\n그림 저장: moons_vs_bgm_plot\n\n\n\n\n\n이런 좋지 않군요. 반달 모양 클러스터 2개를 감지하는 대신 이 알고리즘은 8개의 타원 클러스터를 감지했습니다. 하지만 밀도 그래프는 그렇게 나쁘지 않기 때문에 이상치 탐지에 사용할 수 있습니다.\n가능도 함수\n\nfrom scipy.stats import norm\n\n\nxx = np.linspace(-6, 4, 101)\nss = np.linspace(1, 2, 101)\nXX, SS = np.meshgrid(xx, ss)\nZZ = 2 * norm.pdf(XX - 1.0, 0, SS) + norm.pdf(XX + 4.0, 0, SS)\nZZ = ZZ / ZZ.sum(axis=1)[:,np.newaxis] / (xx[1] - xx[0])\n\n\nfrom matplotlib.patches import Polygon\n\nplt.figure(figsize=(8, 4.5))\n\nx_idx = 85\ns_idx = 30\n\nplt.subplot(221)\nplt.contourf(XX, SS, ZZ, cmap=\"GnBu\")\nplt.plot([-6, 4], [ss[s_idx], ss[s_idx]], \"k-\", linewidth=2)\nplt.plot([xx[x_idx], xx[x_idx]], [1, 2], \"b-\", linewidth=2)\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$\\theta$\", fontsize=14, rotation=0)\nplt.title(r\"Model $f(x; \\theta)$\", fontsize=14)\n\nplt.subplot(222)\nplt.plot(ss, ZZ[:, x_idx], \"b-\")\nmax_idx = np.argmax(ZZ[:, x_idx])\nmax_val = np.max(ZZ[:, x_idx])\nplt.plot(ss[max_idx], max_val, \"r.\")\nplt.plot([ss[max_idx], ss[max_idx]], [0, max_val], \"r:\")\nplt.plot([0, ss[max_idx]], [max_val, max_val], \"r:\")\nplt.text(1.01, max_val + 0.005, r\"$\\hat{L}$\", fontsize=14)\nplt.text(ss[max_idx]+ 0.01, 0.055, r\"$\\hat{\\theta}$\", fontsize=14)\nplt.text(ss[max_idx]+ 0.01, max_val - 0.012, r\"$Max$\", fontsize=12)\nplt.axis([1, 2, 0.05, 0.15])\nplt.xlabel(r\"$\\theta$\", fontsize=14)\nplt.grid(True)\nplt.text(1.99, 0.135, r\"$=f(x=2.5; \\theta)$\", fontsize=14, ha=\"right\")\nplt.title(r\"Likelihood function $\\mathcal{L}(\\theta|x=2.5)$\", fontsize=14)\n\nplt.subplot(223)\nplt.plot(xx, ZZ[s_idx], \"k-\")\nplt.axis([-6, 4, 0, 0.25])\nplt.xlabel(r\"$x$\", fontsize=14)\nplt.grid(True)\nplt.title(r\"PDF $f(x; \\theta=1.3)$\", fontsize=14)\nverts = [(xx[41], 0)] + list(zip(xx[41:81], ZZ[s_idx, 41:81])) + [(xx[80], 0)]\npoly = Polygon(verts, facecolor='0.9', edgecolor='0.5')\nplt.gca().add_patch(poly)\n\nplt.subplot(224)\nplt.plot(ss, np.log(ZZ[:, x_idx]), \"b-\")\nmax_idx = np.argmax(np.log(ZZ[:, x_idx]))\nmax_val = np.max(np.log(ZZ[:, x_idx]))\nplt.plot(ss[max_idx], max_val, \"r.\")\nplt.plot([ss[max_idx], ss[max_idx]], [-5, max_val], \"r:\")\nplt.plot([0, ss[max_idx]], [max_val, max_val], \"r:\")\nplt.axis([1, 2, -2.4, -2])\nplt.xlabel(r\"$\\theta$\", fontsize=14)\nplt.text(ss[max_idx]+ 0.01, max_val - 0.05, r\"$Max$\", fontsize=12)\nplt.text(ss[max_idx]+ 0.01, -2.39, r\"$\\hat{\\theta}$\", fontsize=14)\nplt.text(1.01, max_val + 0.02, r\"$\\log \\, \\hat{L}$\", fontsize=14)\nplt.grid(True)\nplt.title(r\"$\\log \\, \\mathcal{L}(\\theta|x=2.5)$\", fontsize=14)\n\nsave_fig(\"likelihood_function_plot\")\nplt.show()\n\n그림 저장: likelihood_function_plot"
  },
  {
    "objectID": "Machine_Learning/09_unsupervised_learning.html#to-9.",
    "href": "Machine_Learning/09_unsupervised_learning.html#to-9.",
    "title": "09_unsupervised_learning",
    "section": "1. to 9.",
    "text": "1. to 9.\n부록 A 참조."
  },
  {
    "objectID": "Machine_Learning/09_unsupervised_learning.html#올리베타-얼굴-데이터셋-군집하기",
    "href": "Machine_Learning/09_unsupervised_learning.html#올리베타-얼굴-데이터셋-군집하기",
    "title": "09_unsupervised_learning",
    "section": "10. 올리베타 얼굴 데이터셋 군집하기",
    "text": "10. 올리베타 얼굴 데이터셋 군집하기\n문제: 전통적인 올리베티 얼굴 데이터셋은 64×64 픽셀 크기의 흑백 얼굴 이미지 400개를 담고 있습니다. 각 이미지는 4,096 크기의 1D 벡터로 펼쳐져 있습니다. 사람 40명의 사진을 10장씩 찍은 것입니다. 어떤 사람의 사진인지 예측하는 모델을 훈련하는 것이 일반적입니다. sklearn.datasets.fetch_olivetti_faces() 함수를 사용해 데이터셋을 불러오세요.\n\nfrom sklearn.datasets import fetch_olivetti_faces\n\nolivetti = fetch_olivetti_faces()\n\ndownloading Olivetti faces from https://ndownloader.figshare.com/files/5976027 to /home/haesun/scikit_learn_data\n\n\n\nprint(olivetti.DESCR)\n\n.. _olivetti_faces_dataset:\n\nThe Olivetti faces dataset\n--------------------------\n\n`This dataset contains a set of face images`_ taken between April 1992 and \nApril 1994 at AT&T Laboratories Cambridge. The\n:func:`sklearn.datasets.fetch_olivetti_faces` function is the data\nfetching / caching function that downloads the data\narchive from AT&T.\n\n.. _This dataset contains a set of face images: http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html\n\nAs described on the original website:\n\n    There are ten different images of each of 40 distinct subjects. For some\n    subjects, the images were taken at different times, varying the lighting,\n    facial expressions (open / closed eyes, smiling / not smiling) and facial\n    details (glasses / no glasses). All the images were taken against a dark\n    homogeneous background with the subjects in an upright, frontal position \n    (with tolerance for some side movement).\n\n**Data Set Characteristics:**\n\n    =================   =====================\n    Classes                                40\n    Samples total                         400\n    Dimensionality                       4096\n    Features            real, between 0 and 1\n    =================   =====================\n\nThe image is quantized to 256 grey levels and stored as unsigned 8-bit \nintegers; the loader will convert these to floating point values on the \ninterval [0, 1], which are easier to work with for many algorithms.\n\nThe \"target\" for this database is an integer from 0 to 39 indicating the\nidentity of the person pictured; however, with only 10 examples per class, this\nrelatively small dataset is more interesting from an unsupervised or\nsemi-supervised perspective.\n\nThe original dataset consisted of 92 x 112, while the version available here\nconsists of 64x64 images.\n\nWhen using these images, please give credit to AT&T Laboratories Cambridge.\n\n\n\n\nolivetti.target\n\narray([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,\n        3,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  5,\n        5,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,\n        6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,\n        8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n       10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n       11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n       13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15,\n       15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18,\n       18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20,\n       20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22,\n       22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23,\n       23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25,\n       25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27,\n       27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n       28, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30, 30, 30,\n       30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 32, 32, 32,\n       32, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n       34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 35, 35, 35, 35, 35, 35, 35,\n       35, 35, 35, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 37, 37, 37, 37,\n       37, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 39,\n       39, 39, 39, 39, 39, 39, 39, 39, 39])\n\n\n문제: 데이터셋을 훈련 세트, 검증 세트, 테스트 세트로 나눕니다(이 데이터셋은 이미 0에서 1 사이로 스케일이 조정되어 있습니다). 이 데이터셋은 매우 작으니 계층적 샘플링을 사용해 각 세트에 동일한 사람의 얼굴이 고루 섞이도록 하는 것이 좋습니다.\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nstrat_split = StratifiedShuffleSplit(n_splits=1, test_size=40, random_state=42)\ntrain_valid_idx, test_idx = next(strat_split.split(olivetti.data, olivetti.target))\nX_train_valid = olivetti.data[train_valid_idx]\ny_train_valid = olivetti.target[train_valid_idx]\nX_test = olivetti.data[test_idx]\ny_test = olivetti.target[test_idx]\n\nstrat_split = StratifiedShuffleSplit(n_splits=1, test_size=80, random_state=43)\ntrain_idx, valid_idx = next(strat_split.split(X_train_valid, y_train_valid))\nX_train = X_train_valid[train_idx]\ny_train = y_train_valid[train_idx]\nX_valid = X_train_valid[valid_idx]\ny_valid = y_train_valid[valid_idx]\n\n\nprint(X_train.shape, y_train.shape)\nprint(X_valid.shape, y_valid.shape)\nprint(X_test.shape, y_test.shape)\n\n(280, 4096) (280,)\n(80, 4096) (80,)\n(40, 4096) (40,)\n\n\n속도를 높이기 위해 PCA로 데이터의 차원을 줄입니다:\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(0.99)\nX_train_pca = pca.fit_transform(X_train)\nX_valid_pca = pca.transform(X_valid)\nX_test_pca = pca.transform(X_test)\n\npca.n_components_\n\n199\n\n\n문제: 그다음 k-평균을 사용해 이미지를 군집해보세요. (이 장에서 소개한 기법 중 하나를 사용해) 적절한 클러스터 개수를 찾아보세요.\n\nfrom sklearn.cluster import KMeans\n\nk_range = range(5, 150, 5)\nkmeans_per_k = []\nfor k in k_range:\n    print(\"k={}\".format(k))\n    kmeans = KMeans(n_clusters=k, random_state=42).fit(X_train_pca)\n    kmeans_per_k.append(kmeans)\n\nk=5\nk=10\nk=15\nk=20\nk=25\nk=30\nk=35\nk=40\nk=45\nk=50\nk=55\nk=60\nk=65\nk=70\nk=75\nk=80\nk=85\nk=90\nk=95\nk=100\nk=105\nk=110\nk=115\nk=120\nk=125\nk=130\nk=135\nk=140\nk=145\n\n\n\nfrom sklearn.metrics import silhouette_score\n\nsilhouette_scores = [silhouette_score(X_train_pca, model.labels_)\n                     for model in kmeans_per_k]\nbest_index = np.argmax(silhouette_scores)\nbest_k = k_range[best_index]\nbest_score = silhouette_scores[best_index]\n\nplt.figure(figsize=(8, 3))\nplt.plot(k_range, silhouette_scores, \"bo-\")\nplt.xlabel(\"$k$\", fontsize=14)\nplt.ylabel(\"Silhouette score\", fontsize=14)\nplt.plot(best_k, best_score, \"rs\")\nplt.show()\n\n\n\n\n\nbest_k\n\n120\n\n\n최적의 클러스터 개수는 120개로 매우 큽니다. 사진 속의 인물이 40명이기 때문에 아마 40 정도로 예상했을 것입니다. 하지만 같은 사람이 사진마다 다르게 보일 수 있습니다(예를 들어, 안경 착용 유무나 서 있는 방향이 다를 수 있습니다).\n\ninertias = [model.inertia_ for model in kmeans_per_k]\nbest_inertia = inertias[best_index]\n\nplt.figure(figsize=(8, 3.5))\nplt.plot(k_range, inertias, \"bo-\")\nplt.xlabel(\"$k$\", fontsize=14)\nplt.ylabel(\"Inertia\", fontsize=14)\nplt.plot(best_k, best_inertia, \"rs\")\nplt.show()\n\n\n\n\n이너셔 그래프에서는 엘보우 지점이 없기 때문에 최적의 클러스터 개수가 명확하지 않습니다. 그냥 \\(k=120\\)을 사용하겠습니다.\n\nbest_model = kmeans_per_k[best_index]\n\n문제: 클러스터를 시각화해보세요. 각 클러스터에 비슷한 얼굴이 들어 있나요?\n\ndef plot_faces(faces, labels, n_cols=5):\n    faces = faces.reshape(-1, 64, 64)\n    n_rows = (len(faces) - 1) // n_cols + 1\n    plt.figure(figsize=(n_cols, n_rows * 1.1))\n    for index, (face, label) in enumerate(zip(faces, labels)):\n        plt.subplot(n_rows, n_cols, index + 1)\n        plt.imshow(face, cmap=\"gray\")\n        plt.axis(\"off\")\n        plt.title(label)\n    plt.show()\n\nfor cluster_id in np.unique(best_model.labels_):\n    print(\"Cluster\", cluster_id)\n    in_cluster = best_model.labels_==cluster_id\n    faces = X_train[in_cluster]\n    labels = y_train[in_cluster]\n    plot_faces(faces, labels)\n\nCluster 0\nCluster 1\nCluster 2\nCluster 3\nCluster 4\nCluster 5\nCluster 6\nCluster 7\nCluster 8\nCluster 9\nCluster 10\nCluster 11\nCluster 12\nCluster 13\nCluster 14\nCluster 15\nCluster 16\nCluster 17\nCluster 18\nCluster 19\nCluster 20\nCluster 21\nCluster 22\nCluster 23\nCluster 24\nCluster 25\nCluster 26\nCluster 27\nCluster 28\nCluster 29\nCluster 30\nCluster 31\nCluster 32\nCluster 33\nCluster 34\nCluster 35\nCluster 36\nCluster 37\nCluster 38\nCluster 39\nCluster 40\nCluster 41\nCluster 42\nCluster 43\nCluster 44\nCluster 45\nCluster 46\nCluster 47\nCluster 48\nCluster 49\nCluster 50\nCluster 51\nCluster 52\nCluster 53\nCluster 54\nCluster 55\nCluster 56\nCluster 57\nCluster 58\nCluster 59\nCluster 60\nCluster 61\nCluster 62\nCluster 63\nCluster 64\nCluster 65\nCluster 66\nCluster 67\nCluster 68\nCluster 69\nCluster 70\nCluster 71\nCluster 72\nCluster 73\nCluster 74\nCluster 75\nCluster 76\nCluster 77\nCluster 78\nCluster 79\nCluster 80\nCluster 81\nCluster 82\nCluster 83\nCluster 84\nCluster 85\nCluster 86\nCluster 87\nCluster 88\nCluster 89\nCluster 90\nCluster 91\nCluster 92\nCluster 93\nCluster 94\nCluster 95\nCluster 96\nCluster 97\nCluster 98\nCluster 99\nCluster 100\nCluster 101\nCluster 102\nCluster 103\nCluster 104\nCluster 105\nCluster 106\nCluster 107\nCluster 108\nCluster 109\nCluster 110\nCluster 111\nCluster 112\nCluster 113\nCluster 114\nCluster 115\nCluster 116\nCluster 117\nCluster 118\nCluster 119\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n약 2/3 정도가 쓸모있어 보입니다. 즉 적어도 2개의 사진을 가지고 있고 모두 동일한 사람입니다. 하지만 나머지 클러스터는 다른 사람 얼굴이 하나 이상 있거나 하나의 사진만 가지고 있습니다.\n이런 방식으로 군집된 이미지는 부정확해서 (아래에서 보겠지만) 모델을 훈련할 때 바로 사용하기 어렵습니다. 하지만 새로운 데이터셋에 이미지를 레이블링할 때는 매우 유용할 수 있습니다. 일반적으로 훨씬 빠르게 레이블을 부여할 수 있습니다."
  },
  {
    "objectID": "Machine_Learning/09_unsupervised_learning.html#분류를-위해-군집으로-전처리하기",
    "href": "Machine_Learning/09_unsupervised_learning.html#분류를-위해-군집으로-전처리하기",
    "title": "09_unsupervised_learning",
    "section": "11. 분류를 위해 군집으로 전처리하기",
    "text": "11. 분류를 위해 군집으로 전처리하기\n문제: 올리베티 얼굴 데이터셋으로 계속해보겠습니다. 사진에 나타난 사람을 예측하는 분류기를 훈련하고 검증 세트에서 평가해보세요.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(n_estimators=150, random_state=42)\nclf.fit(X_train_pca, y_train)\nclf.score(X_valid_pca, y_valid)\n\n0.925\n\n\n문제: 그다음 k-평균을 차원 축소 도구로 사용하여 축소된 세트에서 분류기를 훈련해보세요.\n\nX_train_reduced = best_model.transform(X_train_pca)\nX_valid_reduced = best_model.transform(X_valid_pca)\nX_test_reduced = best_model.transform(X_test_pca)\n\nclf = RandomForestClassifier(n_estimators=150, random_state=42)\nclf.fit(X_train_reduced, y_train)\n    \nclf.score(X_valid_reduced, y_valid)\n\n0.7\n\n\n윽! 전혀 좋아지지 않았군요! 클러스터 개수를 튜닝하면 도움이 되는지 알아 보죠.\n문제: 분류기 성능을 최대로 만드는 클러스터 개수를 찾아보세요. 얼마나 성능이 나오나요?\n앞에서 처럼 GridSearchCV를 사용할 수 있습니다. 하지만 검증 세트가 이미 있기 때문에 K-폴드 교차 검증을 할 필요가 없고 하나의 하이퍼파라미터만 탐색하기 때문에 간단히 직접 반복문을 만들겠습니다:\n\nfrom sklearn.pipeline import Pipeline\n\nfor n_clusters in k_range:\n    pipeline = Pipeline([\n        (\"kmeans\", KMeans(n_clusters=n_clusters, random_state=42)),\n        (\"forest_clf\", RandomForestClassifier(n_estimators=150, random_state=42))\n    ])\n    pipeline.fit(X_train_pca, y_train)\n    print(n_clusters, pipeline.score(X_valid_pca, y_valid))\n\n5 0.3875\n10 0.575\n15 0.6\n20 0.6625\n25 0.6625\n30 0.6625\n35 0.675\n40 0.75\n45 0.7375\n50 0.725\n55 0.7125\n60 0.7125\n65 0.7375\n70 0.7375\n75 0.7375\n80 0.7875\n85 0.7625\n90 0.75\n95 0.7125\n100 0.775\n105 0.725\n110 0.725\n115 0.7125\n120 0.7\n125 0.75\n130 0.725\n135 0.7375\n140 0.7625\n145 0.6875\n\n\n클러스터 개수를 튜닝해도 80% 정확도를 넘지 못하는군요. 클러스터 센트로이드까지 거리는 원본 이미지만큼 유용하지 않은 것 같습니다.\n문제: 축소된 세트에서 추출한 특성을 원본 특성에 추가하면 어떤가요? (여기에서도 최선의 클러스터 개수를 찾아보세요.)\n\nX_train_extended = np.c_[X_train_pca, X_train_reduced]\nX_valid_extended = np.c_[X_valid_pca, X_valid_reduced]\nX_test_extended = np.c_[X_test_pca, X_test_reduced]\n\n\nclf = RandomForestClassifier(n_estimators=150, random_state=42)\nclf.fit(X_train_extended, y_train)\nclf.score(X_valid_extended, y_valid)\n\n0.8125\n\n\n조금 나아졌네요. 하지만 클러스터 특성이 없는 것이 차라리 낫습니다. 이 경우 클러스터는 분류기를 직접 훈련하는데 도움이 되지 않습니다(하지만 새로운 훈련 샘플을 레이블링하는데는 여전히 도움이 될 수 있습니다)."
  },
  {
    "objectID": "Machine_Learning/09_unsupervised_learning.html#올리베티-얼굴-데이터셋을-위한-가우시안-혼합-모델",
    "href": "Machine_Learning/09_unsupervised_learning.html#올리베티-얼굴-데이터셋을-위한-가우시안-혼합-모델",
    "title": "09_unsupervised_learning",
    "section": "12. 올리베티 얼굴 데이터셋을 위한 가우시안 혼합 모델",
    "text": "12. 올리베티 얼굴 데이터셋을 위한 가우시안 혼합 모델\n문제: 올리베티 얼굴 데이터셋에서 가우시안 혼합 모델을 훈련해보세요. 알고리즘의 속도를 높이기 위해 데이터셋의 차원을 감소시켜야 할 것입니다(예를 들면 분산의 99%를 유지하면서 PCA를 사용합니다).\n\nfrom sklearn.mixture import GaussianMixture\n\ngm = GaussianMixture(n_components=40, random_state=42)\ny_pred = gm.fit_predict(X_train_pca)\n\n문제: 이 모델을 사용해 (sample() 메서드로) 새로운 얼굴을 생성하고 시각화해보세요(PCA를 사용했다면 inverse_transform() 메서드를 사용해야 합니다).\n\nn_gen_faces = 20\ngen_faces_reduced, y_gen_faces = gm.sample(n_samples=n_gen_faces)\ngen_faces = pca.inverse_transform(gen_faces_reduced)\n\n\nplot_faces(gen_faces, y_gen_faces)\n\n\n\n\n문제: 일부 이미지를 수정해보세요(예를 들면 회전, 뒤집기, 어둡게 하기). 모델이 이상치를 감지하는지 확인해보세요(즉 정상 샘플과 이상치에 대해 score_samples() 메서드 출력을 비교해보세요).\n\nn_rotated = 4\nrotated = np.transpose(X_train[:n_rotated].reshape(-1, 64, 64), axes=[0, 2, 1])\nrotated = rotated.reshape(-1, 64*64)\ny_rotated = y_train[:n_rotated]\n\nn_flipped = 3\nflipped = X_train[:n_flipped].reshape(-1, 64, 64)[:, ::-1]\nflipped = flipped.reshape(-1, 64*64)\ny_flipped = y_train[:n_flipped]\n\nn_darkened = 3\ndarkened = X_train[:n_darkened].copy()\ndarkened[:, 1:-1] *= 0.3\ny_darkened = y_train[:n_darkened]\n\nX_bad_faces = np.r_[rotated, flipped, darkened]\ny_bad = np.concatenate([y_rotated, y_flipped, y_darkened])\n\nplot_faces(X_bad_faces, y_bad)\n\n\n\n\n\nX_bad_faces_pca = pca.transform(X_bad_faces)\n\n\ngm.score_samples(X_bad_faces_pca)\n\narray([-2.43643110e+07, -1.89785043e+07, -3.78112108e+07, -4.98187684e+07,\n       -3.20479093e+07, -1.37531398e+07, -2.92373880e+07, -1.05488971e+08,\n       -1.19575239e+08, -6.74256356e+07])\n\n\n잘못된 사진은 이 가우시안 혼합 모델에서 등장할 가능성이 매우 낮습니다. 다른 훈련 샘플의 점수와 비교해 보세요:\n\ngm.score_samples(X_train_pca[:10])\n\narray([1163.02020913, 1134.03637757, 1156.3213282 , 1170.67602774,\n       1141.45404771, 1154.35205046, 1091.3289437 , 1111.4114939 ,\n       1096.43048794, 1132.98982592])"
  },
  {
    "objectID": "Machine_Learning/09_unsupervised_learning.html#차원-축소-기법을-사용해-이상치-탐지하기",
    "href": "Machine_Learning/09_unsupervised_learning.html#차원-축소-기법을-사용해-이상치-탐지하기",
    "title": "09_unsupervised_learning",
    "section": "13. 차원 축소 기법을 사용해 이상치 탐지하기",
    "text": "13. 차원 축소 기법을 사용해 이상치 탐지하기\n문제: 일부 차원 축소 기법은 이상치 탐지를 위해서 사용할 수도 있습니다. 예를 들어 올리베티 얼굴 데이터셋을 PCA를 사용해 분산의 99% 유지하도록 축소해보세요. 그다음 각 이미지의 재구성 오차를 계산합니다. 그다음 이전 연습문제에서 만든 수정된 이미지를 선택해 재구성 오차를 확인해보세요. 재구성 오차가 얼마나 커지는지 확인하세요. 재구성 이미지를 출력해보면 이유를 알 수 있습니다. 정상 얼굴을 재구성하기 때문입니다.\n이미 PCA를 사용해 축소된 데이터셋을 가지고 있습니다:\n\nX_train_pca\n\narray([[ 3.78082347e+00, -1.85479248e+00, -5.14403629e+00, ...,\n        -1.35631904e-01, -2.14083940e-01,  6.11892231e-02],\n       [ 1.01488562e+01, -1.52754772e+00, -7.66965568e-01, ...,\n         1.23926833e-01, -1.35262579e-01, -2.32606847e-02],\n       [-1.00152912e+01,  2.87729216e+00, -9.19882417e-01, ...,\n         7.26157725e-02, -2.96338997e-03,  1.24889754e-01],\n       ...,\n       [ 2.47587085e+00,  2.95597053e+00,  1.29985607e+00, ...,\n        -2.09053084e-02,  3.48560028e-02, -1.54327497e-01],\n       [-3.22031403e+00,  5.34898043e+00,  1.39426959e+00, ...,\n         5.75501136e-02, -2.28312939e-01,  1.55562788e-01],\n       [-9.22877252e-01, -3.64703059e+00,  2.26088047e+00, ...,\n         1.36848077e-01, -6.91322759e-02,  6.26963675e-02]], dtype=float32)\n\n\n\ndef reconstruction_errors(pca, X):\n    X_pca = pca.transform(X)\n    X_reconstructed = pca.inverse_transform(X_pca)\n    mse = np.square(X_reconstructed - X).mean(axis=-1)\n    return mse\n\n\nreconstruction_errors(pca, X_train).mean()\n\n0.0001920535\n\n\n\nreconstruction_errors(pca, X_bad_faces).mean()\n\n0.004707354\n\n\n\nplot_faces(X_bad_faces, y_bad)\n\n\n\n\n\nX_bad_faces_reconstructed = pca.inverse_transform(X_bad_faces_pca)\nplot_faces(X_bad_faces_reconstructed, y_bad)"
  },
  {
    "objectID": "Machine_Learning/07_ensemble_learning_and_random_forests.html",
    "href": "Machine_Learning/07_ensemble_learning_and_random_forests.html",
    "title": "07_ensemble_learning_and_random_forests",
    "section": "",
    "text": "7장 – 앙상블 학습과 랜덤 포레스트\n이 노트북은 7장에 있는 모든 샘플 코드와 연습문제 해답을 가지고 있습니다.\n\n\n\n구글 코랩에서 실행하기"
  },
  {
    "objectID": "Machine_Learning/07_ensemble_learning_and_random_forests.html#사이킷런의-배깅과-페이스팅",
    "href": "Machine_Learning/07_ensemble_learning_and_random_forests.html#사이킷런의-배깅과-페이스팅",
    "title": "07_ensemble_learning_and_random_forests",
    "section": "사이킷런의 배깅과 페이스팅",
    "text": "사이킷런의 배깅과 페이스팅\n\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nbag_clf = BaggingClassifier(\n    DecisionTreeClassifier(), n_estimators=500,\n    max_samples=100, bootstrap=True, random_state=42) #max_sample 각 트리마다 100개의 샘플을 / bootstrap 배깅 중복을 허용 false는 페이스팅이 된다 \nbag_clf.fit(X_train, y_train)\ny_pred = bag_clf.predict(X_test)\n\n\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(y_test, y_pred))\n\n0.904\n\n\n\ntree_clf = DecisionTreeClassifier(random_state=42)\ntree_clf.fit(X_train, y_train)\ny_pred_tree = tree_clf.predict(X_test)\nprint(accuracy_score(y_test, y_pred_tree))\n\n0.856\n\n\n\ny_pred_tree = tree_clf.predict(X_train)\nprint(accuracy_score(y_train, y_pred_tree)) #train set을 가지고 정확도를 테스트 // overfitting된 모습을 보인다 \n\n1.0\n\n\n&lt;그림 7-5. 단일 경정 트리(왼쪽)와 500개 트리로 만든 배깅 앙상블(오른쪽) 비교&gt; 생성 코드\n\nfrom matplotlib.colors import ListedColormap\n\ndef plot_decision_boundary(clf, X, y, axes=[-1.5, 2.45, -1, 1.5], alpha=0.5, contour=True):\n    x1s = np.linspace(axes[0], axes[1], 100)\n    x2s = np.linspace(axes[2], axes[3], 100)\n    x1, x2 = np.meshgrid(x1s, x2s)\n    X_new = np.c_[x1.ravel(), x2.ravel()]\n    y_pred = clf.predict(X_new).reshape(x1.shape)\n    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n    if contour:\n        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", alpha=alpha)\n    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", alpha=alpha)\n    plt.axis(axes)\n    plt.xlabel(r\"$x_1$\", fontsize=18)\n    plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)\n\n\nfig, axes = plt.subplots(ncols=2, figsize=(10,4), sharey=True)\nplt.sca(axes[0])\nplot_decision_boundary(tree_clf, X, y)\nplt.title(\"Decision Tree\", fontsize=14)\nplt.sca(axes[1])\nplot_decision_boundary(bag_clf, X, y)\nplt.title(\"Decision Trees with Bagging\", fontsize=14)\nplt.ylabel(\"\")\nsave_fig(\"decision_tree_without_and_with_bagging_plot\")\nplt.show() #왼쪽이 오버피팅이 되어 일반화가 잘 되지 않은 모습 \n\n그림 저장: decision_tree_without_and_with_bagging_plot"
  },
  {
    "objectID": "Machine_Learning/07_ensemble_learning_and_random_forests.html#oobout-of-bag-평가",
    "href": "Machine_Learning/07_ensemble_learning_and_random_forests.html#oobout-of-bag-평가",
    "title": "07_ensemble_learning_and_random_forests",
    "section": "OOB(out of bag) 평가",
    "text": "OOB(out of bag) 평가\n별도의 validation set의 필요나 train test split의 의미가 없음\n\nbag_clf = BaggingClassifier(\n    DecisionTreeClassifier(), n_estimators=500,\n    bootstrap=True, oob_score=True, random_state=40, max_samples=100) #oob_score를 True로 하면서 valid 할 필요 없어짐 \nbag_clf.fit(X_train, y_train)\nbag_clf.oob_score_ #정확도 True를 하면서 생긴 값 \n\n0.9226666666666666\n\n\n\nbag_clf.oob_decision_function_\n\n\nfrom sklearn.metrics import accuracy_score\ny_pred = bag_clf.predict(X_test)\naccuracy_score(y_test, y_pred)\n\n0.92\n\n\n각각의 feature를 모두 사용하지 않고 진행을 하여 모델의 다양성을 준다. 만약 일반적인 데이터에서 원핫인코더를 사용하면 컬럼의 수가 많아져서 학습이 오래걸리거나 하는 이슈 존재"
  },
  {
    "objectID": "Machine_Learning/07_ensemble_learning_and_random_forests.html#특성-중요도",
    "href": "Machine_Learning/07_ensemble_learning_and_random_forests.html#특성-중요도",
    "title": "07_ensemble_learning_and_random_forests",
    "section": "특성 중요도",
    "text": "특성 중요도\n\nfrom sklearn.datasets import load_iris\niris = load_iris()\nrnd_clf = RandomForestClassifier(n_estimators=500, random_state=42)\nrnd_clf.fit(iris[\"data\"], iris[\"target\"])\nfor name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n    print(name, score)\n\n\nrnd_clf.feature_importances_\n\narray([0.11249225, 0.02311929, 0.44103046, 0.423358  ])\n\n\n다음 그림은 15개 결정 트리의 결정 경계를 중첩한 것입니다. 여기서 볼 수 있듯이 개별 결정 트리는 불완전하지만 앙상블되면 매우 좋은 결정 경계를 만듭니다:\n\nplt.figure(figsize=(6, 4))\n\nfor i in range(15):\n    tree_clf = DecisionTreeClassifier(max_leaf_nodes=16, random_state=42 + i)\n    indices_with_replacement = np.random.randint(0, len(X_train), len(X_train))\n    tree_clf.fit(X_train[indices_with_replacement], y_train[indices_with_replacement])\n    plot_decision_boundary(tree_clf, X, y, axes=[-1.5, 2.45, -1, 1.5], alpha=0.02, contour=False)\n\nplt.show()\n\n\n\n\n&lt;그림 7-6. (랜덤 포레스트 분류기에서 얻은) MNIST 픽셀 중요도&gt; 생성 코드"
  },
  {
    "objectID": "Machine_Learning/07_ensemble_learning_and_random_forests.html#특성-중요도-2",
    "href": "Machine_Learning/07_ensemble_learning_and_random_forests.html#특성-중요도-2",
    "title": "07_ensemble_learning_and_random_forests",
    "section": "특성 중요도 (2)",
    "text": "특성 중요도 (2)\n\nfrom sklearn.datasets import fetch_openml\n\nmnist = fetch_openml('mnist_784', version=1)\nmnist.target = mnist.target.astype(np.uint8)\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nrnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\nrnd_clf.fit(mnist[\"data\"], mnist[\"target\"])\n\nRandomForestClassifier(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(random_state=42)\n\n\n\ndef plot_digit(data):\n    image = data.reshape(28, 28)\n    plt.imshow(image, cmap = mpl.cm.hot,\n               interpolation=\"nearest\")\n    plt.axis(\"off\")\n\n\nplot_digit(rnd_clf.feature_importances_)\n\ncbar = plt.colorbar(ticks=[rnd_clf.feature_importances_.min(), rnd_clf.feature_importances_.max()])\ncbar.ax.set_yticklabels(['Not important', 'Very important'])\n\nsave_fig(\"mnist_feature_importance_plot\")\nplt.show()\n\n그림 저장: mnist_feature_importance_plot\n\n\n\n\n\n\nrnd_clf.feature_importances_ #이것도 확인 필요 \n\narray([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 3.13668159e-07, 6.28737024e-07,\n       1.51289846e-06, 2.30265116e-06, 4.11958200e-06, 3.51604270e-06,\n       1.80271386e-06, 2.93191758e-06, 1.52505574e-06, 3.55353595e-06,\n       3.13323836e-06, 3.24188578e-06, 3.13577715e-07, 2.37973461e-06,\n       4.56525367e-07, 2.85803540e-07, 0.00000000e+00, 9.39484674e-07,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.36293599e-07,\n       0.00000000e+00, 0.00000000e+00, 4.74852205e-06, 7.34406411e-06,\n       1.53353626e-05, 6.37689626e-05, 6.41615599e-05, 1.25928559e-04,\n       1.60369396e-04, 2.01891022e-04, 2.64801209e-04, 1.29731345e-04,\n       6.17425007e-05, 5.52776725e-05, 5.79612630e-05, 6.92443891e-05,\n       3.74207685e-05, 1.53321711e-05, 4.47932350e-06, 1.84800230e-06,\n       3.13624560e-07, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 6.35138011e-07,\n       1.73183609e-06, 1.17071615e-06, 5.56543594e-06, 2.96703096e-05,\n       5.82357388e-05, 1.53884756e-04, 1.96166939e-04, 4.42520920e-04,\n       8.25236462e-04, 8.54154692e-04, 1.05039633e-03, 1.64115046e-03,\n       3.02553714e-03, 1.94201042e-03, 8.40838976e-04, 9.44125403e-04,\n       2.94348389e-04, 1.82247111e-04, 6.05463364e-05, 3.00370334e-05,\n       1.32997433e-05, 2.22302610e-06, 1.54083884e-06, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 1.22372059e-06, 8.85694601e-07,\n       7.48034211e-06, 1.04184202e-05, 2.42510892e-05, 6.49438772e-05,\n       1.59295141e-04, 3.75817357e-04, 8.20403732e-04, 1.07276063e-03,\n       1.62001680e-03, 2.52310314e-03, 3.00859499e-03, 1.91864630e-03,\n       1.33858166e-03, 1.20675951e-03, 8.27603369e-04, 5.80457257e-04,\n       3.70065906e-04, 2.13093844e-04, 1.64366218e-04, 8.90264996e-05,\n       3.12638100e-05, 1.90497693e-05, 3.36624496e-06, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 5.29262601e-07, 8.56961487e-07,\n       1.84990746e-05, 4.80700534e-05, 1.14993002e-04, 2.96417779e-04,\n       6.23307125e-04, 1.40926125e-03, 1.37309869e-03, 2.30199575e-03,\n       3.63601811e-03, 4.11177873e-03, 3.99201149e-03, 7.41697633e-03,\n       4.75843970e-03, 3.53655177e-03, 2.11282904e-03, 1.57659269e-03,\n       1.16190834e-03, 7.05633257e-04, 5.90619791e-04, 3.99600405e-04,\n       8.96536926e-05, 2.65851228e-05, 4.38421231e-06, 3.10375643e-07,\n       0.00000000e+00, 0.00000000e+00, 3.00181982e-06, 8.48085207e-06,\n       3.49207647e-05, 9.66171936e-05, 2.86650140e-04, 4.15391901e-04,\n       8.24017978e-04, 2.15116301e-03, 2.67286844e-03, 3.04211689e-03,\n       1.81707256e-03, 3.12497920e-03, 4.97138670e-03, 5.30494681e-03,\n       3.38220371e-03, 2.16900534e-03, 2.19921962e-03, 1.82933892e-03,\n       1.45507121e-03, 1.63634774e-03, 1.28445815e-03, 8.42378332e-04,\n       4.02988409e-04, 6.24993922e-05, 6.03671163e-06, 2.63521072e-06,\n       0.00000000e+00, 6.19563145e-07, 1.20553722e-06, 1.42875550e-05,\n       7.22525995e-05, 1.87593502e-04, 3.36952932e-04, 6.31212424e-04,\n       1.04483075e-03, 2.10126705e-03, 3.26063120e-03, 2.51706556e-03,\n       2.27538670e-03, 3.77641368e-03, 5.51973034e-03, 6.58124076e-03,\n       4.45354137e-03, 3.09743485e-03, 2.83929642e-03, 1.72836178e-03,\n       1.83064289e-03, 1.55810062e-03, 1.51863197e-03, 1.11414299e-03,\n       5.77755931e-04, 2.76307757e-04, 1.45115710e-05, 1.05998264e-06,\n       0.00000000e+00, 9.09347140e-07, 4.67413978e-06, 2.09508160e-05,\n       9.52992079e-05, 1.70081992e-04, 4.33322374e-04, 7.17478065e-04,\n       1.02970941e-03, 1.27612894e-03, 2.71820141e-03, 3.15834700e-03,\n       3.04612746e-03, 4.14180077e-03, 4.65342780e-03, 5.69022837e-03,\n       4.24724367e-03, 3.23224098e-03, 3.36359657e-03, 3.00348752e-03,\n       2.09376577e-03, 1.31704775e-03, 1.05101661e-03, 9.73027739e-04,\n       8.69041137e-04, 1.95120635e-04, 1.45605191e-05, 1.98594621e-06,\n       0.00000000e+00, 1.21799039e-06, 6.07195263e-06, 2.74870044e-05,\n       7.45644367e-05, 2.38837923e-04, 3.58275604e-04, 8.30239058e-04,\n       1.80083693e-03, 2.87743914e-03, 4.94781698e-03, 4.03742080e-03,\n       3.20867913e-03, 3.98982354e-03, 4.01044050e-03, 4.65797033e-03,\n       3.24344046e-03, 3.98607567e-03, 4.26551201e-03, 3.09856688e-03,\n       2.91069664e-03, 1.56083914e-03, 7.78155462e-04, 9.46247516e-04,\n       4.89641442e-04, 9.06909013e-05, 3.20256533e-05, 2.85548544e-06,\n       0.00000000e+00, 0.00000000e+00, 4.45951073e-06, 2.59639936e-05,\n       9.55801927e-05, 2.13706434e-04, 3.87789663e-04, 9.20041177e-04,\n       1.47491409e-03, 2.53714456e-03, 5.40410585e-03, 5.15026514e-03,\n       2.99575331e-03, 2.77069601e-03, 2.90322285e-03, 4.07948219e-03,\n       3.66055425e-03, 3.94820835e-03, 4.52669697e-03, 3.62975680e-03,\n       2.78552144e-03, 2.22700312e-03, 1.05758526e-03, 4.21997130e-04,\n       2.47121784e-04, 7.44669615e-05, 1.49515686e-05, 8.66493507e-07,\n       0.00000000e+00, 1.38343358e-06, 5.15943874e-06, 2.17357856e-05,\n       8.20904555e-05, 2.00038011e-04, 5.77401653e-04, 1.31394594e-03,\n       2.61066843e-03, 3.88242328e-03, 7.20182490e-03, 6.80561847e-03,\n       3.59011322e-03, 3.03953680e-03, 5.30605653e-03, 6.04285896e-03,\n       3.40088810e-03, 3.26517782e-03, 5.22064082e-03, 3.26068535e-03,\n       1.55944737e-03, 1.74641844e-03, 1.85079197e-03, 5.85482642e-04,\n       1.28418632e-04, 3.61382921e-05, 1.13507448e-05, 8.68034400e-07,\n       0.00000000e+00, 7.89085740e-07, 2.79705111e-06, 1.82260583e-05,\n       9.10300509e-05, 3.08058344e-04, 9.64414722e-04, 1.60433949e-03,\n       3.24119867e-03, 3.74768582e-03, 4.21065343e-03, 4.84116039e-03,\n       4.81009281e-03, 4.57805794e-03, 7.23475180e-03, 5.32443670e-03,\n       3.13433620e-03, 3.85129925e-03, 3.75445919e-03, 2.50432434e-03,\n       1.57345226e-03, 2.25827399e-03, 1.72777775e-03, 7.58638142e-04,\n       1.23152470e-04, 2.29306001e-05, 7.33130343e-06, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 3.69597010e-06, 1.49033926e-05,\n       8.63745197e-05, 3.01436178e-04, 9.02771919e-04, 1.97130648e-03,\n       2.64349675e-03, 4.39477086e-03, 6.09825819e-03, 5.27288666e-03,\n       5.30273926e-03, 6.58467783e-03, 9.79148976e-03, 5.18241032e-03,\n       3.39460657e-03, 6.29371788e-03, 3.07728347e-03, 1.78802648e-03,\n       1.11932630e-03, 1.20026131e-03, 2.59784029e-03, 9.02398159e-04,\n       1.25619247e-04, 1.65897614e-05, 8.54095284e-06, 0.00000000e+00,\n       9.06862556e-07, 3.13316993e-07, 2.09209474e-06, 1.66814902e-05,\n       8.85725364e-05, 3.21474651e-04, 9.98247947e-04, 2.14664212e-03,\n       4.73309292e-03, 5.75384603e-03, 4.00927189e-03, 4.66532695e-03,\n       4.32515577e-03, 7.30462193e-03, 6.30990155e-03, 4.21482599e-03,\n       5.10805435e-03, 8.23664985e-03, 3.09430716e-03, 1.57823733e-03,\n       1.11963667e-03, 1.89947460e-03, 1.66986430e-03, 5.87085326e-04,\n       1.48813674e-04, 4.06664614e-05, 5.63437736e-06, 1.58784402e-07,\n       0.00000000e+00, 0.00000000e+00, 1.80399047e-06, 2.45528604e-05,\n       7.08326217e-05, 3.82583394e-04, 1.10246961e-03, 3.25853755e-03,\n       3.15337997e-03, 5.59597619e-03, 4.71789231e-03, 3.84711427e-03,\n       4.81914856e-03, 9.68755124e-03, 6.33656053e-03, 5.01589980e-03,\n       3.60626501e-03, 5.70334764e-03, 2.89518331e-03, 1.82378695e-03,\n       1.63559150e-03, 2.44403824e-03, 8.85571087e-04, 5.16737565e-04,\n       1.89648851e-04, 4.24292508e-05, 1.70753033e-05, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 4.78146060e-06, 1.98915626e-05,\n       9.83136783e-05, 4.15766749e-04, 2.33870331e-03, 3.04887808e-03,\n       2.99359632e-03, 4.99723796e-03, 4.82009038e-03, 4.89960588e-03,\n       6.40454403e-03, 6.94272101e-03, 6.06550474e-03, 2.88392533e-03,\n       3.10527653e-03, 3.55265089e-03, 2.44762699e-03, 1.86648805e-03,\n       1.45193862e-03, 1.41577925e-03, 6.07261670e-04, 3.74407815e-04,\n       1.75130408e-04, 5.66459420e-05, 1.30765069e-05, 1.79396744e-06,\n       0.00000000e+00, 6.18741661e-07, 5.35569977e-06, 3.58106561e-05,\n       1.37228100e-04, 5.44888763e-04, 1.04960614e-03, 2.08874919e-03,\n       3.38881074e-03, 4.78452379e-03, 4.98902732e-03, 5.25686435e-03,\n       4.49580970e-03, 7.40449830e-03, 4.24894505e-03, 2.36066437e-03,\n       1.79890346e-03, 2.52104121e-03, 2.05544191e-03, 1.31289294e-03,\n       1.61457513e-03, 9.90369338e-04, 6.01012383e-04, 3.28568050e-04,\n       1.66816023e-04, 6.95030316e-05, 7.29416781e-06, 5.29844953e-06,\n       0.00000000e+00, 9.09202376e-07, 3.59321647e-06, 4.69835068e-05,\n       1.53439417e-04, 5.25678946e-04, 1.54696317e-03, 2.55343514e-03,\n       2.08525828e-03, 3.15427091e-03, 5.79642194e-03, 5.71033191e-03,\n       4.19779208e-03, 4.29927350e-03, 2.63501766e-03, 1.84028995e-03,\n       1.18841422e-03, 1.64003491e-03, 2.53632914e-03, 1.87976573e-03,\n       1.37501799e-03, 1.13732963e-03, 6.19011347e-04, 4.25934248e-04,\n       2.12134315e-04, 5.81410983e-05, 7.18673246e-06, 1.23251157e-06,\n       0.00000000e+00, 1.33672504e-06, 1.15087585e-05, 7.30802164e-05,\n       2.86972120e-04, 6.18337109e-04, 1.30627304e-03, 3.12289189e-03,\n       5.11524170e-03, 5.45649814e-03, 5.91865676e-03, 7.68226369e-03,\n       3.46216551e-03, 2.56662716e-03, 1.87187679e-03, 1.29707857e-03,\n       1.30736363e-03, 1.93689759e-03, 3.74677501e-03, 2.37527363e-03,\n       1.49164221e-03, 6.69079931e-04, 9.46999650e-04, 3.50574576e-04,\n       1.48070566e-04, 3.37660593e-05, 2.47667569e-06, 6.21857859e-07,\n       0.00000000e+00, 1.81324990e-06, 2.72158032e-06, 7.14240638e-05,\n       2.37139906e-04, 5.58343648e-04, 1.95611607e-03, 3.01200508e-03,\n       5.50041471e-03, 5.36764820e-03, 3.42292200e-03, 3.33212973e-03,\n       3.10919517e-03, 2.00898253e-03, 1.78493453e-03, 1.62441762e-03,\n       1.76544257e-03, 2.04879960e-03, 2.81198578e-03, 1.54546488e-03,\n       1.17872554e-03, 7.72900966e-04, 5.63185098e-04, 3.75088508e-04,\n       1.33264276e-04, 1.95091253e-05, 3.56201479e-06, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 6.70047990e-06, 3.78333984e-05,\n       1.46989402e-04, 5.86924093e-04, 9.51603744e-04, 3.54988806e-03,\n       5.21233301e-03, 3.47271449e-03, 2.70828282e-03, 1.66792871e-03,\n       1.88183793e-03, 1.45177845e-03, 1.32164081e-03, 1.23495301e-03,\n       1.33772942e-03, 1.56676575e-03, 1.24777612e-03, 1.35924838e-03,\n       7.57774432e-04, 4.83781776e-04, 3.01734863e-04, 1.63118241e-04,\n       8.16831023e-05, 1.09631650e-05, 4.08887859e-07, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 4.06245955e-06, 2.91004673e-05,\n       9.12948687e-05, 2.50094044e-04, 5.71980978e-04, 2.01748422e-03,\n       1.89626152e-03, 3.17871687e-03, 3.03672320e-03, 1.96823592e-03,\n       1.99028992e-03, 1.83405565e-03, 1.45329954e-03, 1.54691878e-03,\n       1.05906385e-03, 1.00211709e-03, 8.52807863e-04, 5.47067909e-04,\n       3.46137654e-04, 2.49121504e-04, 1.48156897e-04, 6.34869739e-05,\n       2.53126662e-05, 5.49428043e-06, 5.52277589e-07, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 3.10289465e-07, 1.59027175e-05,\n       5.89928582e-05, 1.56995846e-04, 3.99738585e-04, 8.14814385e-04,\n       2.13573632e-03, 2.23964977e-03, 2.84574278e-03, 3.43319436e-03,\n       5.09038234e-03, 4.04854728e-03, 4.65013423e-03, 2.53004193e-03,\n       1.65127603e-03, 1.01577993e-03, 6.31164672e-04, 3.71476771e-04,\n       3.07491877e-04, 1.37121260e-04, 7.50339134e-05, 3.23272178e-05,\n       1.19055337e-05, 4.92881397e-06, 1.48199573e-06, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 1.46183676e-06, 6.05319574e-06,\n       4.80124710e-05, 1.21200261e-04, 2.17550747e-04, 4.27339605e-04,\n       6.54436719e-04, 7.37536593e-04, 1.52492920e-03, 1.12147638e-03,\n       1.68082909e-03, 1.03707653e-03, 1.12003113e-03, 6.96297928e-04,\n       5.45122159e-04, 4.66650673e-04, 3.45686147e-04, 2.22292945e-04,\n       1.29176488e-04, 7.65058425e-05, 3.26365187e-05, 1.52272453e-05,\n       9.35616934e-06, 1.41516298e-06, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       1.49080975e-05, 5.48881852e-05, 9.43942609e-05, 2.69713038e-04,\n       3.39678788e-04, 6.80276593e-04, 8.35948555e-04, 8.43859083e-04,\n       8.05815243e-04, 6.27156638e-04, 5.73738801e-04, 4.02124871e-04,\n       4.48149754e-04, 4.78261971e-04, 2.66090089e-04, 2.04017010e-04,\n       1.00451685e-04, 3.65383891e-05, 1.00610075e-05, 2.71411579e-06,\n       3.49189127e-06, 4.11711646e-07, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.34442503e-07,\n       4.88692209e-06, 1.08142236e-05, 1.09488350e-05, 2.11124837e-05,\n       6.15701480e-05, 8.48673475e-05, 8.31225000e-05, 2.49260513e-04,\n       2.04042621e-04, 2.85686996e-04, 3.73348368e-04, 1.76741660e-04,\n       1.46988496e-04, 1.75776976e-04, 7.74587688e-05, 3.63925123e-05,\n       2.75731538e-05, 1.32217906e-05, 2.17941859e-06, 1.35162275e-06,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 2.25761580e-06, 2.72192461e-07,\n       7.44840738e-07, 6.51842660e-07, 1.21365249e-06, 4.36559369e-06,\n       3.31814135e-06, 3.68882280e-06, 4.20006225e-06, 3.39559535e-06,\n       5.49059054e-06, 1.99492302e-06, 3.16563593e-06, 3.05374779e-06,\n       3.10365087e-07, 6.64022906e-07, 1.44504661e-06, 7.84278432e-07,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00])"
  },
  {
    "objectID": "Machine_Learning/07_ensemble_learning_and_random_forests.html#에이다부스트",
    "href": "Machine_Learning/07_ensemble_learning_and_random_forests.html#에이다부스트",
    "title": "07_ensemble_learning_and_random_forests",
    "section": "에이다부스트",
    "text": "에이다부스트\n앞선 분류기가 예측을 하지 못한 부분에 가중치를 주고 두번째에는 가중치를 얻는 것으로 분류를 하면서 정확도를 올리는 작업\n부스팅 기반의 학습기는 많이 할수록 정확도 올라가나 오버피팅이 되는 것 따라서 early stopping이 필요!!\n\n#load dataset and split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_moons\n\nX, y = make_moons(n_samples=500, noise=0.30, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nada_clf = AdaBoostClassifier(\n    DecisionTreeClassifier(max_depth=1), n_estimators=200, #max_depth=1가지를 한번만 치기에 성능이 안좋음 그러나 200번을 반복하면 좋아질수도 \n    algorithm=\"SAMME.R\", learning_rate=0.5, random_state=42) #learning_rate를 조절해 보면서 0.1, 1.0 이런식으로 \nada_clf.fit(X_train, y_train)\n\nAdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n                   learning_rate=0.5, n_estimators=200, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AdaBoostClassifierAdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n                   learning_rate=0.5, n_estimators=200, random_state=42)base_estimator: DecisionTreeClassifierDecisionTreeClassifier(max_depth=1)DecisionTreeClassifierDecisionTreeClassifier(max_depth=1)\n\n\n\nplot_decision_boundary(ada_clf, X, y)\n\n\n\n\n\nfrom sklearn.metrics import accuracy_score\n\ny_pred_train = ada_clf.predict(X_train)\naccuracy_score(y_train,y_pred_train)\n\n0.944\n\n\n&lt;그림 7-8. 연속된 예측기의 결정 경계&gt; 생성 코드\n\nfrom sklearn.svm import SVC\nm = len(X_train)\n\nfig, axes = plt.subplots(ncols=2, figsize=(10,4), sharey=True)\nfor subplot, learning_rate in ((0, 1), (1, 0.5)):\n    sample_weights = np.ones(m) / m\n    plt.sca(axes[subplot])\n    for i in range(5):\n        svm_clf = SVC(kernel=\"rbf\", C=0.2, gamma=0.6, random_state=42)\n        svm_clf.fit(X_train, y_train, sample_weight=sample_weights * m)\n        y_pred = svm_clf.predict(X_train)\n        r = sample_weights[y_pred != y_train].sum() / sample_weights.sum() # equation 7-1\n        alpha = learning_rate * np.log((1 - r) / r) # equation 7-2\n        sample_weights[y_pred != y_train] *= np.exp(alpha) # equation 7-3\n        sample_weights /= sample_weights.sum() # normalization step\n        plot_decision_boundary(svm_clf, X, y, alpha=0.2)\n        plt.title(\"learning_rate = {}\".format(learning_rate), fontsize=16)\n    if subplot == 0:\n        plt.text(-0.7, -0.65, \"1\", fontsize=14)\n        plt.text(-0.6, -0.10, \"2\", fontsize=14)\n        plt.text(-0.5,  0.10, \"3\", fontsize=14)\n        plt.text(-0.4,  0.55, \"4\", fontsize=14)\n        plt.text(-0.3,  0.90, \"5\", fontsize=14)\n    else:\n        plt.ylabel(\"\")\n\nsave_fig(\"boosting_plot\")\nplt.show()\n\n그림 저장: boosting_plot"
  },
  {
    "objectID": "Machine_Learning/07_ensemble_learning_and_random_forests.html#to-7.",
    "href": "Machine_Learning/07_ensemble_learning_and_random_forests.html#to-7.",
    "title": "07_ensemble_learning_and_random_forests",
    "section": "1. to 7.",
    "text": "1. to 7.\n부록 A 참조."
  },
  {
    "objectID": "Machine_Learning/07_ensemble_learning_and_random_forests.html#투표-기반-분류기",
    "href": "Machine_Learning/07_ensemble_learning_and_random_forests.html#투표-기반-분류기",
    "title": "07_ensemble_learning_and_random_forests",
    "section": "8. 투표 기반 분류기",
    "text": "8. 투표 기반 분류기\n문제: MNIST 데이터를 불러들여 훈련 세트, 검증 세트, 테스트 세트로 나눕니다(예를 들면 훈련에 40,000개 샘플, 검증에 10,000개 샘플, 테스트에 10,000개 샘플).\nMNIST 데이터셋은 앞에서 로드했습니다.\n\nfrom sklearn.model_selection import train_test_split\n\n\nX_train_val, X_test, y_train_val, y_test = train_test_split(\n    mnist.data, mnist.target, test_size=10000, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train_val, y_train_val, test_size=10000, random_state=42)\n\n문제: 그런 다음 랜덤 포레스트 분류기, 엑스트라 트리 분류기, SVM 같은 여러 종류의 분류기를 훈련시킵니다.\n\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neural_network import MLPClassifier\n\n\nrandom_forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\nextra_trees_clf = ExtraTreesClassifier(n_estimators=100, random_state=42)\nsvm_clf = LinearSVC(max_iter=100, tol=20, random_state=42)\nmlp_clf = MLPClassifier(random_state=42)\n\n\nestimators = [random_forest_clf, extra_trees_clf, svm_clf, mlp_clf]\nfor estimator in estimators:\n    print(\"Training the\", estimator)\n    estimator.fit(X_train, y_train)\n\nTraining the RandomForestClassifier(random_state=42)\nTraining the ExtraTreesClassifier(random_state=42)\nTraining the LinearSVC(max_iter=100, random_state=42, tol=20)\nTraining the MLPClassifier(random_state=42)\n\n\n\n[estimator.score(X_val, y_val) for estimator in estimators]\n\n[0.9692, 0.9715, 0.859, 0.9666]\n\n\n선형 SVM이 다른 분류기보다 성능이 많이 떨어집니다. 그러나 투표 기반 분류기의 성능을 향상시킬 수 있으므로 그대로 두겠습니다.\n문제: 그리고 검증 세트에서 개개의 분류기보다 더 높은 성능을 내도록 이들을 간접 또는 직접 투표 분류기를 사용하는 앙상블로 연결해보세요.\n\nfrom sklearn.ensemble import VotingClassifier\n\n\nnamed_estimators = [\n    (\"random_forest_clf\", random_forest_clf),\n    (\"extra_trees_clf\", extra_trees_clf),\n    (\"svm_clf\", svm_clf),\n    (\"mlp_clf\", mlp_clf),\n]\n\n\nvoting_clf = VotingClassifier(named_estimators)\n\n\nvoting_clf.fit(X_train, y_train)\n\nVotingClassifier(estimators=[('random_forest_clf',\n                              RandomForestClassifier(random_state=42)),\n                             ('extra_trees_clf',\n                              ExtraTreesClassifier(random_state=42)),\n                             ('svm_clf',\n                              LinearSVC(max_iter=100, random_state=42, tol=20)),\n                             ('mlp_clf', MLPClassifier(random_state=42))])\n\n\n\nvoting_clf.score(X_val, y_val)\n\n0.9708\n\n\n\n[estimator.score(X_val, y_val) for estimator in voting_clf.estimators_]\n\n[0.9692, 0.9715, 0.859, 0.9666]\n\n\nSVM 모델을 제거해서 성능이 향상되는지 확인해 보죠. 다음과 같이 set_params()를 사용하여 None으로 지정하면 특정 예측기를 제외시킬 수 있습니다:\n\nvoting_clf.set_params(svm_clf=None)\n\nVotingClassifier(estimators=[('random_forest_clf',\n                              RandomForestClassifier(random_state=42)),\n                             ('extra_trees_clf',\n                              ExtraTreesClassifier(random_state=42)),\n                             ('svm_clf', None),\n                             ('mlp_clf', MLPClassifier(random_state=42))])\n\n\n예측기 목록이 업데이트되었습니다:\n\nvoting_clf.estimators\n\n[('random_forest_clf', RandomForestClassifier(random_state=42)),\n ('extra_trees_clf', ExtraTreesClassifier(random_state=42)),\n ('svm_clf', None),\n ('mlp_clf', MLPClassifier(random_state=42))]\n\n\n하지만 훈련된 예측기 목록은 업데이트되지 않습니다:\n\nvoting_clf.estimators_\n\n[RandomForestClassifier(random_state=42),\n ExtraTreesClassifier(random_state=42),\n LinearSVC(max_iter=100, random_state=42, tol=20),\n MLPClassifier(random_state=42)]\n\n\nVotingClassifier를 다시 훈련시키거나 그냥 훈련된 예측기 목록에서 SVM 모델을 제거할 수 있습니다:\n\ndel voting_clf.estimators_[2]\n\nVotingClassifier를 다시 평가해 보죠:\n\nvoting_clf.score(X_val, y_val)\n\n0.9741\n\n\n훨씬 나아졌네요! SVM 모델이 성능을 저하시켰습니다. 이제 간접 투표 분류기를 사용해 보죠. 분류기를 다시 훈련시킬 필요는 없고 voting을 \"soft\"로 지정하면 됩니다:\n\nvoting_clf.voting = \"soft\"\n\n\nvoting_clf.score(X_val, y_val)\n\n0.972\n\n\n이 경우는 직접 투표 방식이 낫네요.\n앙상블을 얻고 나면 테스트 세트로 확인해보세요. 개개의 분류기와 비교해서 성능이 얼마나 향상되나요?\n\nvoting_clf.voting = \"hard\"\nvoting_clf.score(X_test, y_test)\n\n0.971\n\n\n\n[estimator.score(X_test, y_test) for estimator in voting_clf.estimators_]\n\n[0.9645, 0.9691, 0.9643]\n\n\n여기서는 투표 기반 분류기가 최선의 모델의 오차율을 아주 조금만 감소시킵니다."
  },
  {
    "objectID": "Machine_Learning/07_ensemble_learning_and_random_forests.html#스태킹-앙상블",
    "href": "Machine_Learning/07_ensemble_learning_and_random_forests.html#스태킹-앙상블",
    "title": "07_ensemble_learning_and_random_forests",
    "section": "9. 스태킹 앙상블",
    "text": "9. 스태킹 앙상블\n문제: 이전 연습문제의 각 분류기를 실행해서 검증 세트에서 예측을 만들고 그 결과로 새로운 훈련 세트를 만들어보세요. 각 훈련 샘플은 하나의 이미지에 대한 전체 분류기의 예측을 담은 벡터고 타깃은 이미지의 클래스입니다. 새로운 훈련 세트에 분류기 하나를 훈련시켜 보세요.\n\nX_val_predictions = np.empty((len(X_val), len(estimators)), dtype=np.float32)\n\nfor index, estimator in enumerate(estimators):\n    X_val_predictions[:, index] = estimator.predict(X_val)\n\n\nX_val_predictions\n\narray([[5., 5., 5., 5.],\n       [8., 8., 8., 8.],\n       [2., 2., 3., 2.],\n       ...,\n       [7., 7., 7., 7.],\n       [6., 6., 6., 6.],\n       [7., 7., 7., 7.]], dtype=float32)\n\n\n\nrnd_forest_blender = RandomForestClassifier(n_estimators=200, oob_score=True, random_state=42)\nrnd_forest_blender.fit(X_val_predictions, y_val)\n\nRandomForestClassifier(n_estimators=200, oob_score=True, random_state=42)\n\n\n\nrnd_forest_blender.oob_score_\n\n0.9707\n\n\n이 블렌더를 세밀하게 튜닝하거나 다른 종류의 블렌더(예를 들어, MLPClassifier)를 시도해 볼 수 있습니다. 그런 늘 하던대로 다음 교차 검증을 사용해 가장 좋은 것을 선택합니다.\n문제: 축하합니다. 방금 블렌더를 훈련시켰습니다. 그리고 이 분류기를 모아서 스태킹 앙상블을 구성했습니다. 이제 테스트 세트에 앙상블을 평가해보세요. 테스트 세트의 각 이미지에 대해 모든 분류기로 예측을 만들고 앙상블의 예측 결과를 만들기 위해 블렌더에 그 예측을 주입합니다. 앞서 만든 투표 분류기와 비교하면 어떤가요?\n\nX_test_predictions = np.empty((len(X_test), len(estimators)), dtype=np.float32)\n\nfor index, estimator in enumerate(estimators):\n    X_test_predictions[:, index] = estimator.predict(X_test)\n\n\ny_pred = rnd_forest_blender.predict(X_test_predictions)\n\n\nfrom sklearn.metrics import accuracy_score\n\n\naccuracy_score(y_test, y_pred)\n\n0.9695\n\n\n이 스태킹 앙상블은 앞서 만든 투표 기반 분류기만큼 성능을 내지는 못합니다. 최선의 개별 분류기만큼 뛰어나지는 않습니다."
  },
  {
    "objectID": "Machine_Learning/05_support_vector_machines.html",
    "href": "Machine_Learning/05_support_vector_machines.html",
    "title": "05_support_vector_machines",
    "section": "",
    "text": "5장 – 서포트 벡터 머신\n이 노트북은 5장에 있는 모든 샘플 코드와 연습문제 해답을 가지고 있습니다."
  },
  {
    "objectID": "Machine_Learning/05_support_vector_machines.html#소프트-마진-분류",
    "href": "Machine_Learning/05_support_vector_machines.html#소프트-마진-분류",
    "title": "05_support_vector_machines",
    "section": "소프트 마진 분류",
    "text": "소프트 마진 분류\n&lt;그림 5-3. 이상치에 민감한 하드 마진&gt; 생성 코드\n\nX_outliers = np.array([[3.4, 1.3], [3.2, 0.8]])\ny_outliers = np.array([0, 0])\nXo1 = np.concatenate([X, X_outliers[:1]], axis=0)\nyo1 = np.concatenate([y, y_outliers[:1]], axis=0)\nXo2 = np.concatenate([X, X_outliers[1:]], axis=0)\nyo2 = np.concatenate([y, y_outliers[1:]], axis=0)\n\nsvm_clf2 = SVC(kernel=\"linear\", C=10**9)\nsvm_clf2.fit(Xo2, yo2)\n\nfig, axes = plt.subplots(ncols=2, figsize=(10,2.7), sharey=True)\n\nplt.sca(axes[0])\nplt.plot(Xo1[:, 0][yo1==1], Xo1[:, 1][yo1==1], \"bs\")\nplt.plot(Xo1[:, 0][yo1==0], Xo1[:, 1][yo1==0], \"yo\")\nplt.text(0.3, 1.0, \"Impossible!\", fontsize=24, color=\"red\")\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.ylabel(\"Petal width\", fontsize=14)\nplt.annotate(\"Outlier\",\n             xy=(X_outliers[0][0], X_outliers[0][1]),\n             xytext=(2.5, 1.7),\n             ha=\"center\",\n             arrowprops=dict(facecolor='black', shrink=0.1),\n             fontsize=16,\n            )\nplt.axis([0, 5.5, 0, 2])\n\nplt.sca(axes[1])\nplt.plot(Xo2[:, 0][yo2==1], Xo2[:, 1][yo2==1], \"bs\")\nplt.plot(Xo2[:, 0][yo2==0], Xo2[:, 1][yo2==0], \"yo\")\nplot_svc_decision_boundary(svm_clf2, 0, 5.5)\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.annotate(\"Outlier\",\n             xy=(X_outliers[1][0], X_outliers[1][1]),\n             xytext=(3.2, 0.08),\n             ha=\"center\",\n             arrowprops=dict(facecolor='black', shrink=0.1),\n             fontsize=16,\n            )\nplt.axis([0, 5.5, 0, 2])\n\nsave_fig(\"sensitivity_to_outliers_plot\")\nplt.show()\n\n그림 저장: sensitivity_to_outliers_plot\n\n\n\n\n\n다음이 5장의 첫 번째 코드 예제입니다:\n\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\n\niris = datasets.load_iris()\nX = iris[\"data\"][:, (2, 3)]  # 꽃잎 길이, 꽃잎 너비\ny = (iris[\"target\"] == 2).astype(np.float64)  # Iris virginica\n\nsvm_clf = Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\", random_state=42)),\n    ])\n\nsvm_clf.fit(X, y)\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('linear_svc', LinearSVC(C=1, loss='hinge', random_state=42))])\n\n\n\nsvm_clf.predict([[5.5, 1.7]])\n\narray([1.])\n\n\n&lt;그림 5-4. 넓은 마진(왼쪽) 대 적은 마진 오류(오른쪽)&gt; 생성 코드\n\nscaler = StandardScaler()\nsvm_clf1 = LinearSVC(C=1, loss=\"hinge\", random_state=42)\nsvm_clf2 = LinearSVC(C=100, loss=\"hinge\", random_state=42)\n\nscaled_svm_clf1 = Pipeline([\n        (\"scaler\", scaler),\n        (\"linear_svc\", svm_clf1),\n    ])\nscaled_svm_clf2 = Pipeline([\n        (\"scaler\", scaler),\n        (\"linear_svc\", svm_clf2),\n    ])\n\nscaled_svm_clf1.fit(X, y)\nscaled_svm_clf2.fit(X, y)\n\n/home/haesun/handson-ml2/.env/lib/python3.7/site-packages/sklearn/svm/_base.py:1201: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  ConvergenceWarning,\n\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('linear_svc',\n                 LinearSVC(C=100, loss='hinge', random_state=42))])\n\n\n\n# 스케일되지 않은 파라미터로 변경\nb1 = svm_clf1.decision_function([-scaler.mean_ / scaler.scale_])\nb2 = svm_clf2.decision_function([-scaler.mean_ / scaler.scale_])\nw1 = svm_clf1.coef_[0] / scaler.scale_\nw2 = svm_clf2.coef_[0] / scaler.scale_\nsvm_clf1.intercept_ = np.array([b1])\nsvm_clf2.intercept_ = np.array([b2])\nsvm_clf1.coef_ = np.array([w1])\nsvm_clf2.coef_ = np.array([w2])\n\n# 서포트 벡터 찾기 (libsvm과 달리 liblinear 라이브러리에서 제공하지 않기 때문에 \n# LinearSVC에는 서포트 벡터가 저장되어 있지 않습니다.)\nt = y * 2 - 1\nsupport_vectors_idx1 = (t * (X.dot(w1) + b1) &lt; 1).ravel()\nsupport_vectors_idx2 = (t * (X.dot(w2) + b2) &lt; 1).ravel()\nsvm_clf1.support_vectors_ = X[support_vectors_idx1]\nsvm_clf2.support_vectors_ = X[support_vectors_idx2]\n\n\nfig, axes = plt.subplots(ncols=2, figsize=(10,2.7), sharey=True)\n\nplt.sca(axes[0])\nplt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\", label=\"Iris virginica\")\nplt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\", label=\"Iris versicolor\")\nplot_svc_decision_boundary(svm_clf1, 4, 5.9)\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.ylabel(\"Petal width\", fontsize=14)\nplt.legend(loc=\"upper left\", fontsize=14)\nplt.title(\"$C = {}$\".format(svm_clf1.C), fontsize=16)\nplt.axis([4, 5.9, 0.8, 2.8])\n\nplt.sca(axes[1])\nplt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\nplt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\nplot_svc_decision_boundary(svm_clf2, 4, 5.99)\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.title(\"$C = {}$\".format(svm_clf2.C), fontsize=16)\nplt.axis([4, 5.9, 0.8, 2.8])\n\nsave_fig(\"regularization_plot\")\n\n그림 저장: regularization_plot"
  },
  {
    "objectID": "Machine_Learning/05_support_vector_machines.html#다항식-커널",
    "href": "Machine_Learning/05_support_vector_machines.html#다항식-커널",
    "title": "05_support_vector_machines",
    "section": "다항식 커널",
    "text": "다항식 커널\n다음 코드 에제:\n\nfrom sklearn.svm import SVC\n\npoly_kernel_svm_clf = Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n    ])\npoly_kernel_svm_clf.fit(X, y)\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('svm_clf', SVC(C=5, coef0=1, kernel='poly'))])\n\n\n&lt;그림 5-7. 다항식 커널을 사용한 SVM 분류기&gt; 생성 코드\n\npoly100_kernel_svm_clf = Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"svm_clf\", SVC(kernel=\"poly\", degree=10, coef0=100, C=5))\n    ])\npoly100_kernel_svm_clf.fit(X, y)\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('svm_clf', SVC(C=5, coef0=100, degree=10, kernel='poly'))])\n\n\n\nfig, axes = plt.subplots(ncols=2, figsize=(10.5, 4), sharey=True)\n\nplt.sca(axes[0])\nplot_predictions(poly_kernel_svm_clf, [-1.5, 2.45, -1, 1.5])\nplot_dataset(X, y, [-1.5, 2.4, -1, 1.5])\nplt.title(r\"$d=3, r=1, C=5$\", fontsize=18)\n\nplt.sca(axes[1])\nplot_predictions(poly100_kernel_svm_clf, [-1.5, 2.45, -1, 1.5])\nplot_dataset(X, y, [-1.5, 2.4, -1, 1.5])\nplt.title(r\"$d=10, r=100, C=5$\", fontsize=18)\nplt.ylabel(\"\")\n\nsave_fig(\"moons_kernelized_polynomial_svc_plot\")\nplt.show()\n\n그림 저장: moons_kernelized_polynomial_svc_plot"
  },
  {
    "objectID": "Machine_Learning/05_support_vector_machines.html#유사도-특성",
    "href": "Machine_Learning/05_support_vector_machines.html#유사도-특성",
    "title": "05_support_vector_machines",
    "section": "유사도 특성",
    "text": "유사도 특성\n&lt;그림 5-8. 가우시안 RBF를 사용한 유사도 특성&gt; 생성 코드\n식 5-1: 가우시안 RBF\n$ {_{}(, )} = {({-| - |^2})} $\n\ndef gaussian_rbf(x, landmark, gamma):\n    return np.exp(-gamma * np.linalg.norm(x - landmark, axis=1)**2)\n\ngamma = 0.3\n\nx1s = np.linspace(-4.5, 4.5, 200).reshape(-1, 1)\nx2s = gaussian_rbf(x1s, -2, gamma)\nx3s = gaussian_rbf(x1s, 1, gamma)\n\nXK = np.c_[gaussian_rbf(X1D, -2, gamma), gaussian_rbf(X1D, 1, gamma)]\nyk = np.array([0, 0, 1, 1, 1, 1, 1, 0, 0])\n\nplt.figure(figsize=(10.5, 4))\n\nplt.subplot(121)\nplt.grid(True, which='both')\nplt.axhline(y=0, color='k')\nplt.scatter(x=[-2, 1], y=[0, 0], s=150, alpha=0.5, c=\"red\")\nplt.plot(X1D[:, 0][yk==0], np.zeros(4), \"bs\")\nplt.plot(X1D[:, 0][yk==1], np.zeros(5), \"g^\")\nplt.plot(x1s, x2s, \"g--\")\nplt.plot(x1s, x3s, \"b:\")\nplt.gca().get_yaxis().set_ticks([0, 0.25, 0.5, 0.75, 1])\nplt.xlabel(r\"$x_1$\", fontsize=20)\nplt.ylabel(r\"Similarity\", fontsize=14)\nplt.annotate(r'$\\mathbf{x}$',\n             xy=(X1D[3, 0], 0),\n             xytext=(-0.5, 0.20),\n             ha=\"center\",\n             arrowprops=dict(facecolor='black', shrink=0.1),\n             fontsize=18,\n            )\nplt.text(-2, 0.9, \"$x_2$\", ha=\"center\", fontsize=20)\nplt.text(1, 0.9, \"$x_3$\", ha=\"center\", fontsize=20)\nplt.axis([-4.5, 4.5, -0.1, 1.1])\n\nplt.subplot(122)\nplt.grid(True, which='both')\nplt.axhline(y=0, color='k')\nplt.axvline(x=0, color='k')\nplt.plot(XK[:, 0][yk==0], XK[:, 1][yk==0], \"bs\")\nplt.plot(XK[:, 0][yk==1], XK[:, 1][yk==1], \"g^\")\nplt.xlabel(r\"$x_2$\", fontsize=20)\nplt.ylabel(r\"$x_3$  \", fontsize=20, rotation=0)\nplt.annotate(r'$\\phi\\left(\\mathbf{x}\\right)$',\n             xy=(XK[3, 0], XK[3, 1]),\n             xytext=(0.65, 0.50),\n             ha=\"center\",\n             arrowprops=dict(facecolor='black', shrink=0.1),\n             fontsize=18,\n            )\nplt.plot([-0.1, 1.1], [0.57, -0.1], \"r--\", linewidth=3)\nplt.axis([-0.1, 1.1, -0.1, 1.1])\n    \nplt.subplots_adjust(right=1)\n\nsave_fig(\"kernel_method_plot\")\nplt.show()\n\n그림 저장: kernel_method_plot\n\n\n\n\n\n\nx1_example = X1D[3, 0]\nfor landmark in (-2, 1):\n    k = gaussian_rbf(np.array([[x1_example]]), np.array([[landmark]]), gamma)\n    print(\"Phi({}, {}) = {}\".format(x1_example, landmark, k))\n\nPhi(-1.0, -2) = [0.74081822]\nPhi(-1.0, 1) = [0.30119421]"
  },
  {
    "objectID": "Machine_Learning/05_support_vector_machines.html#가우시안-rbf-커널",
    "href": "Machine_Learning/05_support_vector_machines.html#가우시안-rbf-커널",
    "title": "05_support_vector_machines",
    "section": "가우시안 RBF 커널",
    "text": "가우시안 RBF 커널\n다음 코드 예제:\n\nrbf_kernel_svm_clf = Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n    ])\nrbf_kernel_svm_clf.fit(X, y)\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('svm_clf', SVC(C=0.001, gamma=5))])\n\n\n&lt;그림 5-9. RBF 커널을 사용한 SVM 분류기&gt; 생성 코드\n\nfrom sklearn.svm import SVC\n\ngamma1, gamma2 = 0.1, 5\nC1, C2 = 0.001, 1000\nhyperparams = (gamma1, C1), (gamma1, C2), (gamma2, C1), (gamma2, C2)\n\nsvm_clfs = []\nfor gamma, C in hyperparams:\n    rbf_kernel_svm_clf = Pipeline([\n            (\"scaler\", StandardScaler()),\n            (\"svm_clf\", SVC(kernel=\"rbf\", gamma=gamma, C=C))\n        ])\n    rbf_kernel_svm_clf.fit(X, y)\n    svm_clfs.append(rbf_kernel_svm_clf)\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10.5, 7), sharex=True, sharey=True)\n\nfor i, svm_clf in enumerate(svm_clfs):\n    plt.sca(axes[i // 2, i % 2])\n    plot_predictions(svm_clf, [-1.5, 2.45, -1, 1.5])\n    plot_dataset(X, y, [-1.5, 2.45, -1, 1.5])\n    gamma, C = hyperparams[i]\n    plt.title(r\"$\\gamma = {}, C = {}$\".format(gamma, C), fontsize=16)\n    if i in (0, 1):\n        plt.xlabel(\"\")\n    if i in (1, 3):\n        plt.ylabel(\"\")\n\nsave_fig(\"moons_rbf_svc_plot\")\nplt.show()\n\n그림 저장: moons_rbf_svc_plot"
  },
  {
    "objectID": "Machine_Learning/05_support_vector_machines.html#결정-함수와-예측",
    "href": "Machine_Learning/05_support_vector_machines.html#결정-함수와-예측",
    "title": "05_support_vector_machines",
    "section": "결정 함수와 예측",
    "text": "결정 함수와 예측\n식 5-2: 선형 SVM 분류기의 예측\n$ =\n\\[\\begin{cases}\n0 & \\mathbf{w}^T \\mathbf{x} + b &lt; 0 \\text{ 일 때}, \\\\\n1 & \\mathbf{w}^T \\mathbf{x} + b \\geq 0 \\text{ 일 때}\n\\end{cases}\\]\n$\n&lt;그림 5-12. iris 데이터셋의 결정 함수&gt; 생성 코드\n\niris = datasets.load_iris()\nX = iris[\"data\"][:, (2, 3)]  # 꽃잎 길이, 꽃잎 너비\ny = (iris[\"target\"] == 2).astype(np.float64)  # Iris virginica\n\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef plot_3D_decision_function(ax, w, b, x1_lim=[4, 6], x2_lim=[0.8, 2.8]):\n    x1_in_bounds = (X[:, 0] &gt; x1_lim[0]) & (X[:, 0] &lt; x1_lim[1])\n    X_crop = X[x1_in_bounds]\n    y_crop = y[x1_in_bounds]\n    x1s = np.linspace(x1_lim[0], x1_lim[1], 20)\n    x2s = np.linspace(x2_lim[0], x2_lim[1], 20)\n    x1, x2 = np.meshgrid(x1s, x2s)\n    xs = np.c_[x1.ravel(), x2.ravel()]\n    df = (xs.dot(w) + b).reshape(x1.shape)\n    m = 1 / np.linalg.norm(w)\n    boundary_x2s = -x1s*(w[0]/w[1])-b/w[1]\n    margin_x2s_1 = -x1s*(w[0]/w[1])-(b-1)/w[1]\n    margin_x2s_2 = -x1s*(w[0]/w[1])-(b+1)/w[1]\n    ax.plot_surface(x1s, x2, np.zeros_like(x1),\n                    color=\"b\", alpha=0.2, cstride=100, rstride=100)\n    ax.plot(x1s, boundary_x2s, 0, \"k-\", linewidth=2, label=r\"$h=0$\")\n    ax.plot(x1s, margin_x2s_1, 0, \"k--\", linewidth=2, label=r\"$h=\\pm 1$\")\n    ax.plot(x1s, margin_x2s_2, 0, \"k--\", linewidth=2)\n    ax.plot(X_crop[:, 0][y_crop==1], X_crop[:, 1][y_crop==1], 0, \"g^\")\n    ax.plot_wireframe(x1, x2, df, alpha=0.3, color=\"k\")\n    ax.plot(X_crop[:, 0][y_crop==0], X_crop[:, 1][y_crop==0], 0, \"bs\")\n    ax.axis(x1_lim + x2_lim)\n    ax.text(4.5, 2.5, 3.8, \"Decision function $h$\", fontsize=16)\n    ax.set_xlabel(r\"Petal length\", fontsize=16, labelpad=10)\n    ax.set_ylabel(r\"Petal width\", fontsize=16, labelpad=10)\n    ax.set_zlabel(r\"$h = \\mathbf{w}^T \\mathbf{x} + b$\", fontsize=18, labelpad=5)\n    ax.legend(loc=\"upper left\", fontsize=16)\n\nfig = plt.figure(figsize=(11, 6))\nax1 = fig.add_subplot(111, projection='3d')\nplot_3D_decision_function(ax1, w=svm_clf2.coef_[0], b=svm_clf2.intercept_[0])\n\nsave_fig(\"iris_3D_plot\")\nplt.show()\n\n그림 저장: iris_3D_plot\n\n\n\n\n\n&lt;그림 5-13. 가중치 벡터가 작을수록 마진은 커집니다&gt; 생성 코드\n\ndef plot_2D_decision_function(w, b, ylabel=True, x1_lim=[-3, 3]):\n    x1 = np.linspace(x1_lim[0], x1_lim[1], 200)\n    y = w * x1 + b\n    m = 1 / w\n\n    plt.plot(x1, y)\n    plt.plot(x1_lim, [1, 1], \"k:\")\n    plt.plot(x1_lim, [-1, -1], \"k:\")\n    plt.axhline(y=0, color='k')\n    plt.axvline(x=0, color='k')\n    plt.plot([m, m], [0, 1], \"k--\")\n    plt.plot([-m, -m], [0, -1], \"k--\")\n    plt.plot([-m, m], [0, 0], \"k-o\", linewidth=3)\n    plt.axis(x1_lim + [-2, 2])\n    plt.xlabel(r\"$x_1$\", fontsize=16)\n    if ylabel:\n        plt.ylabel(r\"$w_1 x_1$  \", rotation=0, fontsize=16)\n    plt.title(r\"$w_1 = {}$\".format(w), fontsize=16)\n\nfig, axes = plt.subplots(ncols=2, figsize=(9, 3.2), sharey=True)\nplt.sca(axes[0])\nplot_2D_decision_function(1, 0)\nplt.sca(axes[1])\nplot_2D_decision_function(0.5, 0, ylabel=False)\nsave_fig(\"small_w_large_margin_plot\")\nplt.show()\n\n그림 저장: small_w_large_margin_plot\n\n\n\n\n\n\nfrom sklearn.svm import SVC\nfrom sklearn import datasets\n\niris = datasets.load_iris()\nX = iris[\"data\"][:, (2, 3)] # 꽃잎 길이, 꽃잎 너비\ny = (iris[\"target\"] == 2).astype(np.float64) # Iris virginica\n\nsvm_clf = SVC(kernel=\"linear\", C=1)\nsvm_clf.fit(X, y)\nsvm_clf.predict([[5.3, 1.3]])\n\narray([1.])\n\n\n식 5-3: 하드 마진 선형 SVM 분류기 목적 함수\n$\n\\[\\begin{split}\n&\\underset{\\mathbf{w}, b}{\\operatorname{minimize}}\\quad{\\frac{1}{2}\\mathbf{w}^T \\mathbf{w}} \\\\\n&\\text{subject to} \\quad t^{(i)}(\\mathbf{w}^T \\mathbf{x}^{(i)} + b) \\ge 1 \\quad \\text{for } i = 1, 2, \\dots, m\n\\end{split}\\]\n$\n식 5-4: 소프트 마진 선형 SVM 분류기 목적 함수\n$\n\\[\\begin{split}\n&\\underset{\\mathbf{w}, b, \\mathbf{\\zeta}}{\\operatorname{minimize}}\\quad{\\dfrac{1}{2}\\mathbf{w}^T \\mathbf{w} + C \\sum\\limits_{i=1}^m{\\zeta^{(i)}}}\\\\\n&\\text{subject to} \\quad t^{(i)}(\\mathbf{w}^T \\mathbf{x}^{(i)} + b) \\ge 1 - \\zeta^{(i)} \\quad \\text{and} \\quad \\zeta^{(i)} \\ge 0 \\quad \\text{for } i = 1, 2, \\dots, m\n\\end{split}\\]\n$\n식 5-8: 2차 다항식 매핑\n$ () = (\n\\[\\begin{pmatrix}\n  x_1 \\\\\n  x_2\n\\end{pmatrix}\\]\n) =\n\\[\\begin{pmatrix}\n  {x_1}^2 \\\\\n  \\sqrt{2} \\, x_1 x_2 \\\\\n  {x_2}^2\n\\end{pmatrix}\\]\n$\n식 5-9: 2차 다항식 매핑을 위한 커널 트릭\n$\n\\[\\begin{split}\n\\phi(\\mathbf{a})^T \\phi(\\mathbf{b}) & \\quad = \\begin{pmatrix}\n  {a_1}^2 \\\\\n  \\sqrt{2} \\, a_1 a_2 \\\\\n  {a_2}^2\n  \\end{pmatrix}^T \\begin{pmatrix}\n  {b_1}^2 \\\\\n  \\sqrt{2} \\, b_1 b_2 \\\\\n  {b_2}^2\n\\end{pmatrix} = {a_1}^2 {b_1}^2 + 2 a_1 b_1 a_2 b_2 + {a_2}^2 {b_2}^2 \\\\\n& \\quad = \\left( a_1 b_1 + a_2 b_2 \\right)^2 = \\left( \\begin{pmatrix}\n  a_1 \\\\\n  a_2\n\\end{pmatrix}^T \\begin{pmatrix}\n    b_1 \\\\\n    b_2\n  \\end{pmatrix} \\right)^2 = (\\mathbf{a}^T \\mathbf{b})^2\n\\end{split}\\]\n$\n식 5-10: 일반적인 커널\n$\n\\[\\begin{split}\n\\text{선형:} & \\quad K(\\mathbf{a}, \\mathbf{b}) = \\mathbf{a}^T \\mathbf{b} \\\\\n\\text{다항식:} & \\quad K(\\mathbf{a}, \\mathbf{b}) = \\left(\\gamma \\mathbf{a}^T \\mathbf{b} + r \\right)^d \\\\\n\\text{가우시안 RBF:} & \\quad K(\\mathbf{a}, \\mathbf{b}) = \\exp({\\displaystyle -\\gamma \\left\\| \\mathbf{a} - \\mathbf{b} \\right\\|^2}) \\\\\n\\text{시그모이드:} & \\quad K(\\mathbf{a}, \\mathbf{b}) = \\tanh\\left(\\gamma \\mathbf{a}^T \\mathbf{b} + r\\right)\n\\end{split}\\]\n$\n식 5-13: 선형 SVM 분류기의 비용 함수\n$ J(, b) = ^T ,+, C {_{i=1}^{m}max(0, t^{(i)} - (^T ^{(i)} + b) )} $\n힌지 손실 그림 생성 코드\n\nt = np.linspace(-2, 4, 200)\nh = np.where(1 - t &lt; 0, 0, 1 - t)  # max(0, 1-t)\n\nplt.figure(figsize=(5,2.8))\nplt.plot(t, h, \"b-\", linewidth=2, label=\"$max(0, 1 - t)$\")\nplt.grid(True, which='both')\nplt.axhline(y=0, color='k')\nplt.axvline(x=0, color='k')\nplt.yticks(np.arange(-1, 2.5, 1))\nplt.xlabel(\"$t$\", fontsize=16)\nplt.axis([-2, 4, -1, 2.5])\nplt.legend(loc=\"upper right\", fontsize=16)\nsave_fig(\"hinge_plot\")\nplt.show()\n\n그림 저장: hinge_plot"
  },
  {
    "objectID": "Machine_Learning/05_support_vector_machines.html#훈련-시간",
    "href": "Machine_Learning/05_support_vector_machines.html#훈련-시간",
    "title": "05_support_vector_machines",
    "section": "훈련 시간",
    "text": "훈련 시간\n\nX, y = make_moons(n_samples=1000, noise=0.4, random_state=42)\nplt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\nplt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n\n\n\n\n\nimport time\n\ntol = 0.1\ntols = []\ntimes = []\nfor i in range(10):\n    svm_clf = SVC(kernel=\"poly\", gamma=3, C=10, tol=tol, verbose=1)\n    t1 = time.time()\n    svm_clf.fit(X, y)\n    t2 = time.time()\n    times.append(t2-t1)\n    tols.append(tol)\n    print(i, tol, t2-t1)\n    tol /= 10\nplt.semilogx(tols, times, \"bo-\")\nplt.xlabel(\"Tolerance\", fontsize=16)\nplt.ylabel(\"Time (seconds)\", fontsize=16)\nplt.grid(True)\nplt.show()\n\n[LibSVM]......................................\nWarning: using -h 0 may be faster\n*.......................\nWarning: using -h 0 may be faster\n*..............................................................\nWarning: using -h 0 may be faster\n*...................................*.......................................................*\noptimization finished, #iter = 212105\nobj = -4447.997680, rho = 0.075931\nnSV = 449, nBSV = 441\nTotal nSV = 449\n0 0.1 0.7381584644317627\n[LibSVM]................................................*..........................................................*..............*..............................................................*..................................................................*...........*\noptimization finished, #iter = 258151\nobj = -4448.479655, rho = 0.058653\nnSV = 446, nBSV = 441\nTotal nSV = 446\n1 0.01 0.7186133861541748\n[LibSVM]...................................................*......*............................................................*..............................................................................*..............................................................................*...........................................................................*...................*..............................................................................................*\noptimization finished, #iter = 457597\nobj = -4448.486560, rho = 0.058757\nnSV = 448, nBSV = 442\nTotal nSV = 448\n2 0.001 0.7357399463653564\n[LibSVM]..................................................................*.......*..........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*.............................................*................................................................................................................................................................................................................................................................................................*.........................................................................*...................*........................................................................................................*\noptimization finished, #iter = 2065000\nobj = -4448.486967, rho = 0.058504\nnSV = 447, nBSV = 442\nTotal nSV = 447\n3 0.0001 1.5096161365509033\n[LibSVM]...................................................................*........*..................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*...........................................\nWarning: using -h 0 may be faster\n*.............................................................*.............*...............................*...................................................*.................................................................................*\noptimization finished, #iter = 5714802\nobj = -4448.486967, rho = 0.058489\nnSV = 447, nBSV = 442\nTotal nSV = 447\n4 1e-05 2.542227029800415\n[LibSVM]....................................................................*.........*..................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*..............................*........................................................................*..................................................................*.................................................................................*\noptimization finished, #iter = 5269415\nobj = -4448.486967, rho = 0.058480\nnSV = 447, nBSV = 442\nTotal nSV = 447\n5 1.0000000000000002e-06 2.4116880893707275\n[LibSVM]......................................................................*............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*.........................................*...........................................................................................................................................*..................................................................................................................................*................................................................................................................................................*\noptimization finished, #iter = 44838621\nobj = -4448.486967, rho = 0.058480\nnSV = 447, nBSV = 442\nTotal nSV = 447\n6 1.0000000000000002e-07 18.176522254943848\n[LibSVM].......................................................................*...........*..........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*..............................*........................................................................*...................................................................*.................................................................................*\noptimization finished, #iter = 5795624\nobj = -4448.486967, rho = 0.058480\nnSV = 447, nBSV = 442\nTotal nSV = 447\n7 1.0000000000000002e-08 2.8188459873199463\n[LibSVM]........................................................................*............*..........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*..............................*........................................................................*...................................................................*.................................................................................*\noptimization finished, #iter = 5798063\nobj = -4448.486967, rho = 0.058480\nnSV = 447, nBSV = 442\nTotal nSV = 447\n8 1.0000000000000003e-09 2.4928481578826904\n[LibSVM].........................................................................*.............*..........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*..............................*........................................................................*...................................................................*.................................................................................*\noptimization finished, #iter = 5800419\nobj = -4448.486967, rho = 0.058480\nnSV = 447, nBSV = 442\nTotal nSV = 447\n9 1.0000000000000003e-10 2.57908296585083"
  },
  {
    "objectID": "Machine_Learning/05_support_vector_machines.html#배치-경사-하강법을-사용한-선형-svm-분류기-구현",
    "href": "Machine_Learning/05_support_vector_machines.html#배치-경사-하강법을-사용한-선형-svm-분류기-구현",
    "title": "05_support_vector_machines",
    "section": "배치 경사 하강법을 사용한 선형 SVM 분류기 구현",
    "text": "배치 경사 하강법을 사용한 선형 SVM 분류기 구현\n\n# 훈련 세트\nX = iris[\"data\"][:, (2, 3)] # # 꽃잎 길이, 꽃잎 너비\ny = (iris[\"target\"] == 2).astype(np.float64).reshape(-1, 1) # Iris virginica\n\n\nfrom sklearn.base import BaseEstimator\n\nclass MyLinearSVC(BaseEstimator):\n    def __init__(self, C=1, eta0=1, eta_d=10000, n_epochs=1000, random_state=None):\n        self.C = C\n        self.eta0 = eta0\n        self.n_epochs = n_epochs\n        self.random_state = random_state\n        self.eta_d = eta_d\n\n    def eta(self, epoch):\n        return self.eta0 / (epoch + self.eta_d)\n        \n    def fit(self, X, y):\n        # Random initialization\n        if self.random_state:\n            np.random.seed(self.random_state)\n        w = np.random.randn(X.shape[1], 1) # n feature weights\n        b = 0\n\n        m = len(X)\n        t = y * 2 - 1  # -1 if y==0, +1 if y==1\n        X_t = X * t\n        self.Js=[]\n\n        # Training\n        for epoch in range(self.n_epochs):\n            support_vectors_idx = (X_t.dot(w) + t * b &lt; 1).ravel()\n            X_t_sv = X_t[support_vectors_idx]\n            t_sv = t[support_vectors_idx]\n\n            J = 1/2 * np.sum(w * w) + self.C * (np.sum(1 - X_t_sv.dot(w)) - b * np.sum(t_sv))\n            self.Js.append(J)\n\n            w_gradient_vector = w - self.C * np.sum(X_t_sv, axis=0).reshape(-1, 1)\n            b_derivative = -self.C * np.sum(t_sv)\n                \n            w = w - self.eta(epoch) * w_gradient_vector\n            b = b - self.eta(epoch) * b_derivative\n            \n\n        self.intercept_ = np.array([b])\n        self.coef_ = np.array([w])\n        support_vectors_idx = (X_t.dot(w) + t * b &lt; 1).ravel()\n        self.support_vectors_ = X[support_vectors_idx]\n        return self\n\n    def decision_function(self, X):\n        return X.dot(self.coef_[0]) + self.intercept_[0]\n\n    def predict(self, X):\n        return (self.decision_function(X) &gt;= 0).astype(np.float64)\n\nC=2\nsvm_clf = MyLinearSVC(C=C, eta0 = 10, eta_d = 1000, n_epochs=60000, random_state=2)\nsvm_clf.fit(X, y)\nsvm_clf.predict(np.array([[5, 2], [4, 1]]))\n\narray([[1.],\n       [0.]])\n\n\n\nplt.plot(range(svm_clf.n_epochs), svm_clf.Js)\nplt.axis([0, svm_clf.n_epochs, 0, 100])\n\n(0.0, 60000.0, 0.0, 100.0)\n\n\n\n\n\n\nprint(svm_clf.intercept_, svm_clf.coef_)\n\n[-15.56761653] [[[2.28120287]\n  [2.71621742]]]\n\n\n\nsvm_clf2 = SVC(kernel=\"linear\", C=C)\nsvm_clf2.fit(X, y.ravel())\nprint(svm_clf2.intercept_, svm_clf2.coef_)\n\n[-15.51721253] [[2.27128546 2.71287145]]\n\n\n\nyr = y.ravel()\nfig, axes = plt.subplots(ncols=2, figsize=(11, 3.2), sharey=True)\nplt.sca(axes[0])\nplt.plot(X[:, 0][yr==1], X[:, 1][yr==1], \"g^\", label=\"Iris virginica\")\nplt.plot(X[:, 0][yr==0], X[:, 1][yr==0], \"bs\", label=\"Not Iris virginica\")\nplot_svc_decision_boundary(svm_clf, 4, 6)\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.ylabel(\"Petal width\", fontsize=14)\nplt.title(\"MyLinearSVC\", fontsize=14)\nplt.axis([4, 6, 0.8, 2.8])\nplt.legend(loc=\"upper left\")\n\nplt.sca(axes[1])\nplt.plot(X[:, 0][yr==1], X[:, 1][yr==1], \"g^\")\nplt.plot(X[:, 0][yr==0], X[:, 1][yr==0], \"bs\")\nplot_svc_decision_boundary(svm_clf2, 4, 6)\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.title(\"SVC\", fontsize=14)\nplt.axis([4, 6, 0.8, 2.8])\n\n(4.0, 6.0, 0.8, 2.8)\n\n\n\n\n\n\nfrom sklearn.linear_model import SGDClassifier\n\nsgd_clf = SGDClassifier(loss=\"hinge\", alpha=0.017, max_iter=1000, tol=1e-3, random_state=42)\nsgd_clf.fit(X, y.ravel())\n\nm = len(X)\nt = y * 2 - 1  # y==0이면 -1, y==1이면 +1\nX_b = np.c_[np.ones((m, 1)), X]  # 편향 x0=1을 추가합니다\nX_b_t = X_b * t\nsgd_theta = np.r_[sgd_clf.intercept_[0], sgd_clf.coef_[0]]\nprint(sgd_theta)\nsupport_vectors_idx = (X_b_t.dot(sgd_theta) &lt; 1).ravel()\nsgd_clf.support_vectors_ = X[support_vectors_idx]\nsgd_clf.C = C\n\nplt.figure(figsize=(5.5,3.2))\nplt.plot(X[:, 0][yr==1], X[:, 1][yr==1], \"g^\")\nplt.plot(X[:, 0][yr==0], X[:, 1][yr==0], \"bs\")\nplot_svc_decision_boundary(sgd_clf, 4, 6)\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.ylabel(\"Petal width\", fontsize=14)\nplt.title(\"SGDClassifier\", fontsize=14)\nplt.axis([4, 6, 0.8, 2.8])\n\n[-12.52988101   1.94162342   1.84544824]\n\n\n(4.0, 6.0, 0.8, 2.8)"
  },
  {
    "objectID": "Machine_Learning/05_support_vector_machines.html#to-7.",
    "href": "Machine_Learning/05_support_vector_machines.html#to-7.",
    "title": "05_support_vector_machines",
    "section": "1. to 7.",
    "text": "1. to 7.\n부록 A 참조."
  },
  {
    "objectID": "Machine_Learning/05_support_vector_machines.html#section-2",
    "href": "Machine_Learning/05_support_vector_machines.html#section-2",
    "title": "05_support_vector_machines",
    "section": "10.",
    "text": "10.\n문제: 캘리포니아 주택 가격 데이터셋에 SVM 회귀를 훈련시켜보세요.\n사이킷런의 fetch_california_housing() 함수를 사용해 데이터셋을 로드합니다:\n\nfrom sklearn.datasets import fetch_california_housing\n\nhousing = fetch_california_housing()\nX = housing[\"data\"]\ny = housing[\"target\"]\n\n훈련 세트와 테스트 세트로 나눕니다:\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n데이터의 스케일을 조정하는 것을 잊지 마세요:\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n먼저 간단한 LinearSVR을 훈련시켜 보죠:\n\nfrom sklearn.svm import LinearSVR\n\nlin_svr = LinearSVR(random_state=42)\nlin_svr.fit(X_train_scaled, y_train)\n\n/home/haesun/handson-ml2/.env/lib/python3.7/site-packages/sklearn/svm/_base.py:1201: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  ConvergenceWarning,\n\n\nLinearSVR(random_state=42)\n\n\n훈련 세트에 대한 성능을 확인해 보겠습니다:\n\nfrom sklearn.metrics import mean_squared_error\n\ny_pred = lin_svr.predict(X_train_scaled)\nmse = mean_squared_error(y_train, y_pred)\nmse\n\n0.9641780189948642\n\n\nRMSE를 확인해 보겠습니다:\n\nnp.sqrt(mse)\n\n0.9819256687727764\n\n\n훈련 세트에서 타깃은 만달러 단위입니다. RMSE는 기대할 수 있는 에러의 정도를 대략 가늠하게 도와줍니다(에러가 클수록 큰 폭으로 증가합니다). 이 모델의 에러가 대략 $10,000 정도로 예상할 수 있습니다. 썩 훌륭하지 않네요. RBF 커널이 더 나을지 확인해 보겠습니다. 하이퍼파라미터 C와 gamma의 적절한 값을 찾기 위해 교차 검증을 사용한 랜덤 서치를 적용하겠습니다:\n\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import reciprocal, uniform\n\nparam_distributions = {\"gamma\": reciprocal(0.001, 0.1), \"C\": uniform(1, 10)}\nrnd_search_cv = RandomizedSearchCV(SVR(), param_distributions, n_iter=10, verbose=2, cv=3, random_state=42)\nrnd_search_cv.fit(X_train_scaled, y_train)\n\nFitting 3 folds for each of 10 candidates, totalling 30 fits\n[CV] END .....C=4.745401188473625, gamma=0.07969454818643928; total time=  20.6s\n[CV] END .....C=4.745401188473625, gamma=0.07969454818643928; total time=  19.0s\n[CV] END .....C=4.745401188473625, gamma=0.07969454818643928; total time=  19.1s\n[CV] END .....C=8.31993941811405, gamma=0.015751320499779724; total time=  18.2s\n[CV] END .....C=8.31993941811405, gamma=0.015751320499779724; total time=  17.5s\n[CV] END .....C=8.31993941811405, gamma=0.015751320499779724; total time=  16.8s\n[CV] END ....C=2.560186404424365, gamma=0.002051110418843397; total time=  17.4s\n[CV] END ....C=2.560186404424365, gamma=0.002051110418843397; total time=  16.9s\n[CV] END ....C=2.560186404424365, gamma=0.002051110418843397; total time=  17.4s\n[CV] END ....C=1.5808361216819946, gamma=0.05399484409787431; total time=  16.6s\n[CV] END ....C=1.5808361216819946, gamma=0.05399484409787431; total time=  22.3s\n[CV] END ....C=1.5808361216819946, gamma=0.05399484409787431; total time=  22.8s\n[CV] END ....C=7.011150117432088, gamma=0.026070247583707663; total time=  21.4s\n[CV] END ....C=7.011150117432088, gamma=0.026070247583707663; total time=  18.1s\n[CV] END ....C=7.011150117432088, gamma=0.026070247583707663; total time=  18.5s\n[CV] END .....C=1.2058449429580245, gamma=0.0870602087830485; total time=  22.3s\n[CV] END .....C=1.2058449429580245, gamma=0.0870602087830485; total time=  24.2s\n[CV] END .....C=1.2058449429580245, gamma=0.0870602087830485; total time=  23.2s\n[CV] END ...C=9.324426408004218, gamma=0.0026587543983272693; total time=  24.6s\n[CV] END ...C=9.324426408004218, gamma=0.0026587543983272693; total time=  19.7s\n[CV] END ...C=9.324426408004218, gamma=0.0026587543983272693; total time=  17.3s\n[CV] END ...C=2.818249672071006, gamma=0.0023270677083837795; total time=  21.4s\n[CV] END ...C=2.818249672071006, gamma=0.0023270677083837795; total time=  24.3s\n[CV] END ...C=2.818249672071006, gamma=0.0023270677083837795; total time=  22.5s\n[CV] END ....C=4.042422429595377, gamma=0.011207606211860567; total time=  17.3s\n[CV] END ....C=4.042422429595377, gamma=0.011207606211860567; total time=  16.3s\n[CV] END ....C=4.042422429595377, gamma=0.011207606211860567; total time=  17.2s\n[CV] END ....C=5.319450186421157, gamma=0.003823475224675185; total time=  17.0s\n[CV] END ....C=5.319450186421157, gamma=0.003823475224675185; total time=  17.9s\n[CV] END ....C=5.319450186421157, gamma=0.003823475224675185; total time=  16.9s\n\n\nRandomizedSearchCV(cv=3, estimator=SVR(),\n                   param_distributions={'C': &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7f5ef9023a20&gt;,\n                                        'gamma': &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7f5ef5bce8d0&gt;},\n                   random_state=42, verbose=2)\n\n\n\nrnd_search_cv.best_estimator_\n\nSVR(C=4.745401188473625, gamma=0.07969454818643928)\n\n\n이제 훈련 세트에서 RMSE를 측정해 보겠습니다:\n\ny_pred = rnd_search_cv.best_estimator_.predict(X_train_scaled)\nmse = mean_squared_error(y_train, y_pred)\nnp.sqrt(mse)\n\n0.5727524770785358\n\n\n선형 모델보다 훨씬 나아졌네요. 이 모델을 선택하고 테스트 세트에서 평가해 보겠습니다:\n\ny_pred = rnd_search_cv.best_estimator_.predict(X_test_scaled)\nmse = mean_squared_error(y_test, y_pred)\nnp.sqrt(mse)\n\n0.5929168385528742"
  },
  {
    "objectID": "Machine_Learning/03_classification.html",
    "href": "Machine_Learning/03_classification.html",
    "title": "03_classification",
    "section": "",
    "text": "3장 – 분류\n이 노트북은 3장의 모든 샘플 코드와 연습 문제 정답을 담고 있습니다."
  },
  {
    "objectID": "Machine_Learning/03_classification.html#교차-검증을-사용한-정확도-측정",
    "href": "Machine_Learning/03_classification.html#교차-검증을-사용한-정확도-측정",
    "title": "03_classification",
    "section": "교차 검증을 사용한 정확도 측정",
    "text": "교차 검증을 사용한 정확도 측정\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.base import clone\n\n# shuffle=False가 기본값이기 때문에 random_state를 삭제하던지 shuffle=True로 지정하라는 경고가 발생합니다.\n# 0.24버전부터는 에러가 발생할 예정이므로 향후 버전을 위해 shuffle=True을 지정합니다.\nskfolds = StratifiedKFold(n_splits=3, random_state=42, shuffle=True)\n\nfor train_index, test_index in skfolds.split(X_train, y_train_5):\n    clone_clf = clone(sgd_clf)\n    X_train_folds = X_train[train_index]\n    y_train_folds = y_train_5[train_index]\n    X_test_fold = X_train[test_index]\n    y_test_fold = y_train_5[test_index]\n\n    clone_clf.fit(X_train_folds, y_train_folds)\n    y_pred = clone_clf.predict(X_test_fold)\n    n_correct = sum(y_pred == y_test_fold)\n    print(n_correct / len(y_pred))\n\n0.9669\n0.91625\n0.96785\n\n\n\nfrom sklearn.base import BaseEstimator\nclass Never5Classifier(BaseEstimator):\n    def fit(self, X, y=None):\n        pass\n    def predict(self, X):\n        return np.zeros((len(X), 1), dtype=bool)\n\n\nnever_5_clf = Never5Classifier()\ncross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n\narray([0.91125, 0.90855, 0.90915])\n\n\n노트: 이 출력(그리고 이 노트북과 다른 노트북의 출력)이 책의 내용과 조금 다를 수 있습니다. 걱정할 필요 없습니다. 괜찮습니다! 달라지는 이유가 몇가지 있습니다:\n\n첫째, 사이킷런과 다른 라이브러리들이 발전하면서 알고리즘이 조금씩 변경되기 때문에 얻어지는 결괏값이 바뀔 수 있습니다. 최신 사이킷런 버전을 사용한다면(일반적으로 권장됩니다) 책이나 이 노트북을 만들 때 사용한 버전과 다를 것이므로 차이가 납니다. 노트북은 최신으로 업데이트하려고 노력하지만 책의 내용은 그렇게 할 수 없습니다.\n둘째, 많은 훈련 알고리즘은 확률적입니다. 즉 무작위성에 의존합니다. 이론적으로 의사 난수를 생성하도록 난순 생성기에 시드 값을 지정하여 일관된 결과를 얻을 수 있습니다(random_state=42나 np.random.seed(42)를 종종 보게 되는 이유입니다). 하지만 여기에서 언급한 다른 요인으로 인해 충분하지 않을 때가 있습니다.\n세째, 훈련 알고리즘이 여러 스레드(C로 구현된 알고리즘)나 여러 프로세스(예를 들어 n_jobs 매개변수를 사용할 때)로 실행되면 연산이 실행되는 정확한 순서가 항상 보장되지 않습니다. 따라서 결괏값이 조금 다를 수 있습니다.\n마지막으로, 여러 세션에 결쳐 순서가 보장되지 않는 파이썬 딕셔너리(dict)이나 셋(set) 같은 것은 완벽한 재현성이 불가능합니다. 또한 디렉토리 안에 있는 파일의 순서도 보장되지 않습니다."
  },
  {
    "objectID": "Machine_Learning/03_classification.html#오차-행렬",
    "href": "Machine_Learning/03_classification.html#오차-행렬",
    "title": "03_classification",
    "section": "오차 행렬",
    "text": "오차 행렬\n\nfrom sklearn.model_selection import cross_val_predict\n\ny_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\n\n\nfrom sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_train_5, y_train_pred)\n\narray([[53892,   687],\n       [ 1891,  3530]])\n\n\n\ny_train_perfect_predictions = y_train_5  # 완변한척 하자\nconfusion_matrix(y_train_5, y_train_perfect_predictions)\n\narray([[54579,     0],\n       [    0,  5421]])"
  },
  {
    "objectID": "Machine_Learning/03_classification.html#정밀도와-재현율",
    "href": "Machine_Learning/03_classification.html#정밀도와-재현율",
    "title": "03_classification",
    "section": "정밀도와 재현율",
    "text": "정밀도와 재현율\n\nfrom sklearn.metrics import precision_score, recall_score\n\nprecision_score(y_train_5, y_train_pred)\n\n0.8370879772350012\n\n\n\ncm = confusion_matrix(y_train_5, y_train_pred)\ncm[1, 1] / (cm[0, 1] + cm[1, 1])\n\n0.8370879772350012\n\n\n\nrecall_score(y_train_5, y_train_pred)\n\n0.6511713705958311\n\n\n\ncm[1, 1] / (cm[1, 0] + cm[1, 1])\n\n0.6511713705958311\n\n\n\nfrom sklearn.metrics import f1_score\n\nf1_score(y_train_5, y_train_pred)\n\n0.7325171197343846\n\n\n\ncm[1, 1] / (cm[1, 1] + (cm[1, 0] + cm[0, 1]) / 2)\n\n0.7325171197343847"
  },
  {
    "objectID": "Machine_Learning/03_classification.html#정밀도재현율-트레이드오프",
    "href": "Machine_Learning/03_classification.html#정밀도재현율-트레이드오프",
    "title": "03_classification",
    "section": "정밀도/재현율 트레이드오프",
    "text": "정밀도/재현율 트레이드오프\n\ny_scores = sgd_clf.decision_function([some_digit])\ny_scores\n\narray([2164.22030239])\n\n\n\nthreshold = 0\ny_some_digit_pred = (y_scores &gt; threshold)\n\n\ny_some_digit_pred\n\narray([ True])\n\n\n\nthreshold = 8000\ny_some_digit_pred = (y_scores &gt; threshold)\ny_some_digit_pred\n\narray([False])\n\n\n\ny_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\n                             method=\"decision_function\")\n\n\nfrom sklearn.metrics import precision_recall_curve\n\nprecisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n\n\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n    plt.legend(loc=\"center right\", fontsize=16) # Not shown in the book\n    plt.xlabel(\"Threshold\", fontsize=16)        # Not shown\n    plt.grid(True)                              # Not shown\n    plt.axis([-50000, 50000, 0, 1])             # Not shown\n\n\n\nrecall_90_precision = recalls[np.argmax(precisions &gt;= 0.90)]\nthreshold_90_precision = thresholds[np.argmax(precisions &gt;= 0.90)]\n\n\nplt.figure(figsize=(8, 4))                                                                  # Not shown\nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\nplt.plot([threshold_90_precision, threshold_90_precision], [0., 0.9], \"r:\")                 # Not shown\nplt.plot([-50000, threshold_90_precision], [0.9, 0.9], \"r:\")                                # Not shown\nplt.plot([-50000, threshold_90_precision], [recall_90_precision, recall_90_precision], \"r:\")# Not shown\nplt.plot([threshold_90_precision], [0.9], \"ro\")                                             # Not shown\nplt.plot([threshold_90_precision], [recall_90_precision], \"ro\")                             # Not shown\nsave_fig(\"precision_recall_vs_threshold_plot\")                                              # Not shown\nplt.show()\n\n그림 저장: precision_recall_vs_threshold_plot\n\n\n\n\n\n\n(y_train_pred == (y_scores &gt; 0)).all()\n\nTrue\n\n\n\ndef plot_precision_vs_recall(precisions, recalls):\n    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n    plt.xlabel(\"Recall\", fontsize=16)\n    plt.ylabel(\"Precision\", fontsize=16)\n    plt.axis([0, 1, 0, 1])\n    plt.grid(True)\n\nplt.figure(figsize=(8, 6))\nplot_precision_vs_recall(precisions, recalls)\nplt.plot([recall_90_precision, recall_90_precision], [0., 0.9], \"r:\")\nplt.plot([0.0, recall_90_precision], [0.9, 0.9], \"r:\")\nplt.plot([recall_90_precision], [0.9], \"ro\")\nsave_fig(\"precision_vs_recall_plot\")\nplt.show()\n\n그림 저장: precision_vs_recall_plot\n\n\n\n\n\n\nthreshold_90_precision = thresholds[np.argmax(precisions &gt;= 0.90)]\n\n\nthreshold_90_precision\n\n3370.0194991439557\n\n\n\ny_train_pred_90 = (y_scores &gt;= threshold_90_precision)\n\n\nprecision_score(y_train_5, y_train_pred_90)\n\n0.9000345901072293\n\n\n\nrecall_score(y_train_5, y_train_pred_90)\n\n0.4799852425751706"
  },
  {
    "objectID": "Machine_Learning/03_classification.html#roc-곡선",
    "href": "Machine_Learning/03_classification.html#roc-곡선",
    "title": "03_classification",
    "section": "ROC 곡선",
    "text": "ROC 곡선\n\nfrom sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\n\n\ndef plot_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'k--') # 대각 점선\n    plt.axis([0, 1, 0, 1])                                    # Not shown in the book\n    plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16) # Not shown\n    plt.ylabel('True Positive Rate (Recall)', fontsize=16)    # Not shown\n    plt.grid(True)                                            # Not shown\n\nplt.figure(figsize=(8, 6))                                    # Not shown\nplot_roc_curve(fpr, tpr)\nfpr_90 = fpr[np.argmax(tpr &gt;= recall_90_precision)]           # Not shown\nplt.plot([fpr_90, fpr_90], [0., recall_90_precision], \"r:\")   # Not shown\nplt.plot([0.0, fpr_90], [recall_90_precision, recall_90_precision], \"r:\")  # Not shown\nplt.plot([fpr_90], [recall_90_precision], \"ro\")               # Not shown\nsave_fig(\"roc_curve_plot\")                                    # Not shown\nplt.show()\n\n그림 저장: roc_curve_plot\n\n\n\n\n\n\nfrom sklearn.metrics import roc_auc_score\n\nroc_auc_score(y_train_5, y_scores)\n\n0.9604938554008616\n\n\n노트: 사이킷런 0.22 버전에서 바뀔 기본 값을 사용해 n_estimators=100로 지정합니다.\n\nfrom sklearn.ensemble import RandomForestClassifier\nforest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\ny_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3,\n                                    method=\"predict_proba\")\n\n\ny_scores_forest = y_probas_forest[:, 1] # 점수 = 양성 클래스의 확률\nfpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5,y_scores_forest)\n\n\nrecall_for_forest = tpr_forest[np.argmax(fpr_forest &gt;= fpr_90)]\n\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, \"b:\", linewidth=2, label=\"SGD\")\nplot_roc_curve(fpr_forest, tpr_forest, \"Random Forest\")\nplt.plot([fpr_90, fpr_90], [0., recall_90_precision], \"r:\")\nplt.plot([0.0, fpr_90], [recall_90_precision, recall_90_precision], \"r:\")\nplt.plot([fpr_90], [recall_90_precision], \"ro\")\nplt.plot([fpr_90, fpr_90], [0., recall_for_forest], \"r:\")\nplt.plot([fpr_90], [recall_for_forest], \"ro\")\nplt.grid(True)\nplt.legend(loc=\"lower right\", fontsize=16)\nsave_fig(\"roc_curve_comparison_plot\")\nplt.show()\n\n그림 저장: roc_curve_comparison_plot\n\n\n\n\n\n\nroc_auc_score(y_train_5, y_scores_forest)\n\n0.9983436731328145\n\n\n\ny_train_pred_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3)\nprecision_score(y_train_5, y_train_pred_forest)\n\n0.9905083315756169\n\n\n\nrecall_score(y_train_5, y_train_pred_forest)\n\n0.8662608374838591"
  },
  {
    "objectID": "Machine_Learning/03_classification.html#더미-즉-랜덤-분류기",
    "href": "Machine_Learning/03_classification.html#더미-즉-랜덤-분류기",
    "title": "03_classification",
    "section": "더미 (즉 랜덤) 분류기",
    "text": "더미 (즉 랜덤) 분류기\n\nfrom sklearn.dummy import DummyClassifier\n# 0.24버전부터 strategy의 기본값이 'stratified'에서 'prior'로 바뀌므로 명시적으로 지정합니다.\ndmy_clf = DummyClassifier(strategy='prior')\ny_probas_dmy = cross_val_predict(dmy_clf, X_train, y_train_5, cv=3, method=\"predict_proba\")\ny_scores_dmy = y_probas_dmy[:, 1]\n\n\nfprr, tprr, thresholdsr = roc_curve(y_train_5, y_scores_dmy)\nplot_roc_curve(fprr, tprr)"
  },
  {
    "objectID": "Machine_Learning/03_classification.html#knn-분류기",
    "href": "Machine_Learning/03_classification.html#knn-분류기",
    "title": "03_classification",
    "section": "KNN 분류기",
    "text": "KNN 분류기\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_clf = KNeighborsClassifier(weights='distance', n_neighbors=4)\nknn_clf.fit(X_train, y_train)\n\nKNeighborsClassifier(n_neighbors=4, weights='distance')\n\n\n\ny_knn_pred = knn_clf.predict(X_test)\n\n\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_knn_pred)\n\n0.9714\n\n\n\nfrom scipy.ndimage.interpolation import shift\ndef shift_digit(digit_array, dx, dy, new=0):\n    return shift(digit_array.reshape(28, 28), [dy, dx], cval=new).reshape(784)\n\nplot_digit(shift_digit(some_digit, 5, 1, new=100))\n\n\n\n\n\nX_train_expanded = [X_train]\ny_train_expanded = [y_train]\nfor dx, dy in ((1, 0), (-1, 0), (0, 1), (0, -1)):\n    shifted_images = np.apply_along_axis(shift_digit, axis=1, arr=X_train, dx=dx, dy=dy)\n    X_train_expanded.append(shifted_images)\n    y_train_expanded.append(y_train)\n\nX_train_expanded = np.concatenate(X_train_expanded)\ny_train_expanded = np.concatenate(y_train_expanded)\nX_train_expanded.shape, y_train_expanded.shape\n\n((300000, 784), (300000,))\n\n\n\nknn_clf.fit(X_train_expanded, y_train_expanded)\n\nKNeighborsClassifier(n_neighbors=4, weights='distance')\n\n\n\ny_knn_expanded_pred = knn_clf.predict(X_test)\n\n\naccuracy_score(y_test, y_knn_expanded_pred)\n\n0.9763\n\n\n\nambiguous_digit = X_test[2589]\nknn_clf.predict_proba([ambiguous_digit])\n\narray([[0.24579675, 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.75420325]])\n\n\n\nplot_digit(ambiguous_digit)"
  },
  {
    "objectID": "Machine_Learning/03_classification.html#정확도의-mnist-분류기",
    "href": "Machine_Learning/03_classification.html#정확도의-mnist-분류기",
    "title": "03_classification",
    "section": "1. 97% 정확도의 MNIST 분류기",
    "text": "1. 97% 정확도의 MNIST 분류기\n경고: 사용하는 하드웨어에 따라 다음 셀을 실행하는데 16시간 또는 그 이상 걸릴 수 있습니다.\n\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = [{'weights': [\"uniform\", \"distance\"], 'n_neighbors': [3, 4, 5]}]\n\nknn_clf = KNeighborsClassifier()\ngrid_search = GridSearchCV(knn_clf, param_grid, cv=5, verbose=3)\ngrid_search.fit(X_train, y_train)\n\nFitting 5 folds for each of 6 candidates, totalling 30 fits\n[CV 1/5] END ....n_neighbors=3, weights=uniform;, score=0.972 total time=  17.8s\n[CV 2/5] END ....n_neighbors=3, weights=uniform;, score=0.971 total time=  17.4s\n[CV 3/5] END ....n_neighbors=3, weights=uniform;, score=0.969 total time=  17.9s\n[CV 4/5] END ....n_neighbors=3, weights=uniform;, score=0.969 total time=  17.4s\n[CV 5/5] END ....n_neighbors=3, weights=uniform;, score=0.970 total time=  17.7s\n[CV 1/5] END ...n_neighbors=3, weights=distance;, score=0.972 total time=  17.2s\n[CV 2/5] END ...n_neighbors=3, weights=distance;, score=0.972 total time=  17.6s\n[CV 3/5] END ...n_neighbors=3, weights=distance;, score=0.970 total time=  16.7s\n[CV 4/5] END ...n_neighbors=3, weights=distance;, score=0.970 total time=  17.4s\n[CV 5/5] END ...n_neighbors=3, weights=distance;, score=0.971 total time=  17.1s\n[CV 1/5] END ....n_neighbors=4, weights=uniform;, score=0.969 total time=  20.7s\n[CV 2/5] END ....n_neighbors=4, weights=uniform;, score=0.968 total time=  21.0s\n[CV 3/5] END ....n_neighbors=4, weights=uniform;, score=0.968 total time=  21.0s\n[CV 4/5] END ....n_neighbors=4, weights=uniform;, score=0.967 total time=  21.0s\n[CV 5/5] END ....n_neighbors=4, weights=uniform;, score=0.970 total time=  21.4s\n[CV 1/5] END ...n_neighbors=4, weights=distance;, score=0.973 total time=  20.7s\n[CV 2/5] END ...n_neighbors=4, weights=distance;, score=0.972 total time=  20.8s\n[CV 3/5] END ...n_neighbors=4, weights=distance;, score=0.970 total time=  20.5s\n[CV 4/5] END ...n_neighbors=4, weights=distance;, score=0.971 total time=  21.0s\n[CV 5/5] END ...n_neighbors=4, weights=distance;, score=0.972 total time=  21.3s\n[CV 1/5] END ....n_neighbors=5, weights=uniform;, score=0.970 total time=  20.9s\n[CV 2/5] END ....n_neighbors=5, weights=uniform;, score=0.970 total time=  21.3s\n[CV 3/5] END ....n_neighbors=5, weights=uniform;, score=0.969 total time=  20.6s\n[CV 4/5] END ....n_neighbors=5, weights=uniform;, score=0.968 total time=  20.9s\n[CV 5/5] END ....n_neighbors=5, weights=uniform;, score=0.969 total time=  21.3s\n[CV 1/5] END ...n_neighbors=5, weights=distance;, score=0.970 total time=  20.7s\n[CV 2/5] END ...n_neighbors=5, weights=distance;, score=0.971 total time=  20.9s\n[CV 3/5] END ...n_neighbors=5, weights=distance;, score=0.970 total time=  21.0s\n[CV 4/5] END ...n_neighbors=5, weights=distance;, score=0.969 total time=  20.9s\n[CV 5/5] END ...n_neighbors=5, weights=distance;, score=0.971 total time=  21.2s\n\n\nGridSearchCV(cv=5, estimator=KNeighborsClassifier(),\n             param_grid=[{'n_neighbors': [3, 4, 5],\n                          'weights': ['uniform', 'distance']}],\n             verbose=3)\n\n\n\ngrid_search.best_params_\n\n{'n_neighbors': 4, 'weights': 'distance'}\n\n\n\ngrid_search.best_score_\n\n0.9716166666666666\n\n\n\nfrom sklearn.metrics import accuracy_score\n\ny_pred = grid_search.predict(X_test)\naccuracy_score(y_test, y_pred)\n\n0.9714"
  },
  {
    "objectID": "Machine_Learning/03_classification.html#데이터-증식",
    "href": "Machine_Learning/03_classification.html#데이터-증식",
    "title": "03_classification",
    "section": "2. 데이터 증식",
    "text": "2. 데이터 증식\n\nfrom scipy.ndimage.interpolation import shift\n\n\ndef shift_image(image, dx, dy):\n    image = image.reshape((28, 28))\n    shifted_image = shift(image, [dy, dx], cval=0, mode=\"constant\")\n    return shifted_image.reshape([-1])\n\n\nimage = X_train[1000]\nshifted_image_down = shift_image(image, 0, 5)\nshifted_image_left = shift_image(image, -5, 0)\n\nplt.figure(figsize=(12,3))\nplt.subplot(131)\nplt.title(\"Original\", fontsize=14)\nplt.imshow(image.reshape(28, 28), interpolation=\"nearest\", cmap=\"Greys\")\nplt.subplot(132)\nplt.title(\"Shifted down\", fontsize=14)\nplt.imshow(shifted_image_down.reshape(28, 28), interpolation=\"nearest\", cmap=\"Greys\")\nplt.subplot(133)\nplt.title(\"Shifted left\", fontsize=14)\nplt.imshow(shifted_image_left.reshape(28, 28), interpolation=\"nearest\", cmap=\"Greys\")\nplt.show()\n\n\n\n\n\nX_train_augmented = [image for image in X_train]\ny_train_augmented = [label for label in y_train]\n\nfor dx, dy in ((1, 0), (-1, 0), (0, 1), (0, -1)):\n    for image, label in zip(X_train, y_train):\n        X_train_augmented.append(shift_image(image, dx, dy))\n        y_train_augmented.append(label)\n\nX_train_augmented = np.array(X_train_augmented)\ny_train_augmented = np.array(y_train_augmented)\n\n\nshuffle_idx = np.random.permutation(len(X_train_augmented))\nX_train_augmented = X_train_augmented[shuffle_idx]\ny_train_augmented = y_train_augmented[shuffle_idx]\n\n\nknn_clf = KNeighborsClassifier(**grid_search.best_params_)\n\n\nknn_clf.fit(X_train_augmented, y_train_augmented)\n\nKNeighborsClassifier(n_neighbors=4, weights='distance')\n\n\n경고: 사용하는 하드웨어에 따라 다음 셀을 실행하는데 1시간 또는 그 이상 걸릴 수 있습니다.\n\ny_pred = knn_clf.predict(X_test)\naccuracy_score(y_test, y_pred)\n\n0.9763\n\n\n간단히 데이터를 증식해서 0.5% 정확도를 높였습니다. :)"
  },
  {
    "objectID": "Machine_Learning/03_classification.html#타이타닉-데이터셋-도전",
    "href": "Machine_Learning/03_classification.html#타이타닉-데이터셋-도전",
    "title": "03_classification",
    "section": "3. 타이타닉 데이터셋 도전",
    "text": "3. 타이타닉 데이터셋 도전\n승객의 나이, 성별, 승객 등급, 승선 위치 같은 속성을 기반으로 하여 승객의 생존 여부를 예측하는 것이 목표입니다.\n먼저 데이터를 로드해 보죠:\n\nimport os\nimport urllib.request\n\nTITANIC_PATH = os.path.join(\"datasets\", \"titanic\")\nDOWNLOAD_URL = \"https://raw.githubusercontent.com/rickiepark/handson-ml2/master/datasets/titanic/\"\n\ndef fetch_titanic_data(url=DOWNLOAD_URL, path=TITANIC_PATH):\n    if not os.path.isdir(path):\n        os.makedirs(path)\n    for filename in (\"train.csv\", \"test.csv\"):\n        filepath = os.path.join(path, filename)\n        if not os.path.isfile(filepath):\n            print(\"Downloading\", filename)\n            urllib.request.urlretrieve(url + filename, filepath)\n\nfetch_titanic_data()   \n\n\nimport pandas as pd\n\ndef load_titanic_data(filename, titanic_path=TITANIC_PATH):\n    csv_path = os.path.join(titanic_path, filename)\n    return pd.read_csv(csv_path)\n\n\ntrain_data = load_titanic_data(\"train.csv\")\ntest_data = load_titanic_data(\"test.csv\")\n\n데이터는 이미 훈련 세트와 테스트 세트로 분리되어 있습니다. 그러나 테스트 데이터는 레이블을 가지고 있지 않습니다: 훈련 데이터를 이용하여 가능한 최고의 모델을 만들고 테스트 데이터에 대한 예측을 캐글(Kaggle)에 업로드하여 최종 점수를 확인하는 것이 목표입니다.\n훈련 세트에서 맨 위 몇 개의 열을 살펴 보겠습니다:\n\ntrain_data.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n속성은 다음과 같은 의미를 가집니다: * PassengerId: 각 승객의 고유 식별자. * Survived: 타깃입니다. 0은 생존하지 못한 것이고 1은 생존을 의미합니다. * Pclass: 승객 등급. 1, 2, 3등석. * Name, Sex, Age: 이름 그대로 의미입니다. * SibSp: 함께 탑승한 형제, 배우자의 수. * Parch: 함께 탑승한 자녀, 부모의 수. * Ticket: 티켓 아이디 * Fare: 티켓 요금 (파운드) * Cabin: 객실 번호 * Embarked: 승객이 탑승한 곳. C(Cherbourg), Q(Queenstown), S(Southampton)\nPassengerId 열을 인덱스 열로 지정하겠습니다:\n\ntrain_data = train_data.set_index(\"PassengerId\")\ntest_data = test_data.set_index(\"PassengerId\")\n\n누락된 데이터가 얼마나 되는지 알아보겠습니다:\n\ntrain_data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 891 entries, 1 to 891\nData columns (total 11 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Survived  891 non-null    int64  \n 1   Pclass    891 non-null    int64  \n 2   Name      891 non-null    object \n 3   Sex       891 non-null    object \n 4   Age       714 non-null    float64\n 5   SibSp     891 non-null    int64  \n 6   Parch     891 non-null    int64  \n 7   Ticket    891 non-null    object \n 8   Fare      891 non-null    float64\n 9   Cabin     204 non-null    object \n 10  Embarked  889 non-null    object \ndtypes: float64(2), int64(4), object(5)\nmemory usage: 83.5+ KB\n\n\n\ntrain_data[train_data[\"Sex\"]==\"female\"][\"Age\"].median()\n\n27.0\n\n\n좋습니다. Age, Cabin, Embarked 속성의 일부가 null입니다(891개의 non-null 보다 작습니다). 특히 Cabin은 77%가 null입니다. 일단 Cabin은 무시하고 나머지를 활용하겠습니다. Age는 19%가 null이므로 이를 어떻게 처리할지 결정해야 합니다. null을 중간 나이로 바꾸는 것이 괜찮아 보입니다. 다른 열을 사용하여 나이를 예측하는 것(예를 들어, 1등석의 중간 나이는 37, 2등석은 29, 3등석은 24입니다)이 조금 현명해 보일 수 있지만 단순함을 위해 전체의 중간 나이를 사용하겠습니다.\nName과 Ticket 속성도 값을 가지고 있지만 머신러닝 모델이 사용할 수 있는 숫자로 변환하는 것이 조금 까다롭습니다. 그래서 지금은 이 두 속성을 무시하겠습니다.\n통계치를 살펴 보겠습니다:\n\ntrain_data.describe()\n\n\n\n\n\n\n\n\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\n\n\n\n\ncount\n891.000000\n891.000000\n714.000000\n891.000000\n891.000000\n891.000000\n\n\nmean\n0.383838\n2.308642\n29.699118\n0.523008\n0.381594\n32.204208\n\n\nstd\n0.486592\n0.836071\n14.526497\n1.102743\n0.806057\n49.693429\n\n\nmin\n0.000000\n1.000000\n0.420000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n2.000000\n20.125000\n0.000000\n0.000000\n7.910400\n\n\n50%\n0.000000\n3.000000\n28.000000\n0.000000\n0.000000\n14.454200\n\n\n75%\n1.000000\n3.000000\n38.000000\n1.000000\n0.000000\n31.000000\n\n\nmax\n1.000000\n3.000000\n80.000000\n8.000000\n6.000000\n512.329200\n\n\n\n\n\n\n\n\n이크, 38%만 Survived입니다. 😭 거의 40%에 가까우므로 정확도를 사용해 모델을 평가해도 괜찮을 것 같습니다.\n평균 Fare는 32.20 파운드라 그렇게 비싸보이지는 않습니다(아마 요금을 많이 반환해 주었기 때문일 것입니다)\n평균 Age는 30보다 작습니다.\n\n타깃이 0과 1로 이루어졌는지 확인합니다:\n\ntrain_data[\"Survived\"].value_counts()\n\n0    549\n1    342\nName: Survived, dtype: int64\n\n\n범주형 특성들을 확인해 보겠습니다:\n\ntrain_data[\"Pclass\"].value_counts()\n\n3    491\n1    216\n2    184\nName: Pclass, dtype: int64\n\n\n\ntrain_data[\"Sex\"].value_counts()\n\nmale      577\nfemale    314\nName: Sex, dtype: int64\n\n\n\ntrain_data[\"Embarked\"].value_counts()\n\nS    644\nC    168\nQ     77\nName: Embarked, dtype: int64\n\n\nEmbarked 특성은 승객이 탑승한 곳을 알려 줍니다: C=Cherbourg, Q=Queenstown, S=Southampton.\n수치 특성을 위한 파이프라인부터 시작해서 전처리 파이프라인을 만들어 보죠:\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n        (\"imputer\", SimpleImputer(strategy=\"median\")),\n        (\"scaler\", StandardScaler())\n    ])\n\n이제 범주형 특성을 위한 파이프라인을 만듭니다:\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n\ncat_pipeline = Pipeline([\n        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n        (\"cat_encoder\", OneHotEncoder(sparse=False)),\n    ])\n\n마지막으로 숫자와 범주형 파이프라인을 연결합니다:\n\nfrom sklearn.compose import ColumnTransformer\n\nnum_attribs = [\"Age\", \"SibSp\", \"Parch\", \"Fare\"]\ncat_attribs = [\"Pclass\", \"Sex\", \"Embarked\"]\n\npreprocess_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, num_attribs),\n        (\"cat\", cat_pipeline, cat_attribs),\n    ])\n\n좋습니다! 이제 원본 데이터를 받아 머신러닝 모델에 주입할 숫자 입력 특성을 출력하는 전처리 파이프라인을 만들었습니다.\n\nX_train = preprocess_pipeline.fit_transform(\n    train_data[num_attribs + cat_attribs])\nX_train\n\narray([[-0.56573646,  0.43279337, -0.47367361, ...,  0.        ,\n         0.        ,  1.        ],\n       [ 0.66386103,  0.43279337, -0.47367361, ...,  1.        ,\n         0.        ,  0.        ],\n       [-0.25833709, -0.4745452 , -0.47367361, ...,  0.        ,\n         0.        ,  1.        ],\n       ...,\n       [-0.1046374 ,  0.43279337,  2.00893337, ...,  0.        ,\n         0.        ,  1.        ],\n       [-0.25833709, -0.4745452 , -0.47367361, ...,  1.        ,\n         0.        ,  0.        ],\n       [ 0.20276197, -0.4745452 , -0.47367361, ...,  0.        ,\n         1.        ,  0.        ]])\n\n\n레이블을 가져옵니다:\n\ny_train = train_data[\"Survived\"]\n\n이제 분류기를 훈련시킬 차례입니다. 먼저 RandomForestClassifier를 사용해 보겠습니다:\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nforest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\nforest_clf.fit(X_train, y_train)\n\nRandomForestClassifier(random_state=42)\n\n\n모델이 잘 훈련된 것 같습니다. 이를 사용해서 테스트 세트에 대한 예측을 만듭니다:\n\nX_test = preprocess_pipeline.transform(test_data[num_attribs + cat_attribs])\ny_pred = forest_clf.predict(X_test)\n\n이 예측 결과를 (캐글에서 기대하는 형태인) CSV 파일로 만들어 업로드하고 평가를 받아볼 수 있습니다. 하지만 그냥 좋을거라 기대하는 것보다 교차 검증으로 모델이 얼마나 좋은지 평가하는 것이 좋습니다.\n\nfrom sklearn.model_selection import cross_val_score\n\nforest_scores = cross_val_score(forest_clf, X_train, y_train, cv=10)\nforest_scores.mean()\n\n0.8092759051186016\n\n\n아주 나쁘지 않네요! 캐글에서 타이타닉 경연 대회의 리더보드에서 상위 2% 안에 든 점수를 볼 수 있습니다. 와우! 어떤 사람들은 100%를 달성했습니다. 하지만 타이타닉의 희생자 목록을 쉽게 찾을 수 있으므로 머신러닝을 사용하지 않고도 이런 정확도를 달성할 수 있습니다! 😆\nSVC를 적용해 보겠습니다:\n\nfrom sklearn.svm import SVC\n\nsvm_clf = SVC(gamma=\"auto\")\nsvm_scores = cross_val_score(svm_clf, X_train, y_train, cv=10)\nsvm_scores.mean()\n\n0.8249313358302123\n\n\n좋네요! 이 모델이 훨씬 나아보입니다.\n하지만 10 폴드 교차 검증에 대한 평균 정확도를 보는 대신 모델에서 얻은 10개의 점수를 1사분위, 3사분위를 명료하게 표현해주는 상자 수염 그림(box-and-whisker) 그래프를 만들어 보겠습니다(이 방식을 제안해 준 Nevin Yilmaz에게 감사합니다). boxplot() 함수는 이상치(플라이어(flier)라고 부릅니다)를 감지하고 수염 부분에 이를 포함시키지 않습니다. 1사분위가 \\(Q_1\\)이고 3사분위가 \\(Q_3\\)이라면 사분위수 범위는 \\(IQR = Q_3 - Q_1\\)가 됩니다(이 값이 박스의 높이가 됩니다). \\(Q_1 - 1.5 \\times IQR\\) 보다 낮거나 \\(Q3 + 1.5 \\times IQR\\) 보다 높은 점수는 이상치로 간주됩니다.\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 4))\nplt.plot([1]*10, svm_scores, \".\")\nplt.plot([2]*10, forest_scores, \".\")\nplt.boxplot([svm_scores, forest_scores], labels=(\"SVM\",\"Random Forest\"))\nplt.ylabel(\"Accuracy\", fontsize=14)\nplt.show()\n\n\n\n\n랜덤 포레스트 분류기가 10개의 폴드 중 하나에서 매우 높은 점수를 얻었지만 넓게 퍼져 있기 때문에 전체적인 평균 점수는 낮습니다. 따라서 SVM 분류기가 일반화를 더 잘하는 것 같습니다.\n이 결과를 더 향상시키려면: * 교차 검증과 그리드 탐색을 사용하여 더 많은 모델을 비교하고 하이퍼파라미터를 튜닝하세요. * 특성 공학을 더 시도해 보세요, 예를 들면: * 수치 특성을 범주형 특성으로 바꾸어 보세요: 예를 들어, 나이대가 다른 경우 다른 생존 비율을 가질 수 있습니다(아래 참조). 그러므로 나이 구간을 범주로 만들어 나이 대신 사용하는 것이 도움이 될 수 있스니다. 비슷하게 생존자의 30%가 혼자 여행하는 사람이기 때문에 이들을 위한 특별한 범주를 만드는 것이 도움이 될 수 있습니다(아래 참조). * SibSp와 Parch을 이 두 특성의 합으로 바꿉니다. * Survived 특성과 관련된 이름을 구별해 보세요. * Cabin 열을 사용하세요. 예를 들어 첫 글자를 범주형 속성처럼 다룰 수 있습니다.\n\ntrain_data[\"AgeBucket\"] = train_data[\"Age\"] // 15 * 15\ntrain_data[[\"AgeBucket\", \"Survived\"]].groupby(['AgeBucket']).mean()\n\n\n\n\n\n\n\n\nSurvived\n\n\nAgeBucket\n\n\n\n\n\n0.0\n0.576923\n\n\n15.0\n0.362745\n\n\n30.0\n0.423256\n\n\n45.0\n0.404494\n\n\n60.0\n0.240000\n\n\n75.0\n1.000000\n\n\n\n\n\n\n\n\ntrain_data[\"RelativesOnboard\"] = train_data[\"SibSp\"] + train_data[\"Parch\"]\ntrain_data[[\"RelativesOnboard\", \"Survived\"]].groupby(['RelativesOnboard']).mean()\n\n\n\n\n\n\n\n\nSurvived\n\n\nRelativesOnboard\n\n\n\n\n\n0\n0.303538\n\n\n1\n0.552795\n\n\n2\n0.578431\n\n\n3\n0.724138\n\n\n4\n0.200000\n\n\n5\n0.136364\n\n\n6\n0.333333\n\n\n7\n0.000000\n\n\n10\n0.000000"
  },
  {
    "objectID": "Machine_Learning/03_classification.html#스팸-필터",
    "href": "Machine_Learning/03_classification.html#스팸-필터",
    "title": "03_classification",
    "section": "4. 스팸 필터",
    "text": "4. 스팸 필터\n먼저 데이터를 다운받습니다:\n\nimport os\nimport tarfile\nimport urllib.request\n\nDOWNLOAD_ROOT = \"http://spamassassin.apache.org/old/publiccorpus/\"\nHAM_URL = DOWNLOAD_ROOT + \"20030228_easy_ham.tar.bz2\"\nSPAM_URL = DOWNLOAD_ROOT + \"20030228_spam.tar.bz2\"\nSPAM_PATH = os.path.join(\"datasets\", \"spam\")\n\ndef fetch_spam_data(ham_url=HAM_URL, spam_url=SPAM_URL, spam_path=SPAM_PATH):\n    if not os.path.isdir(spam_path):\n        os.makedirs(spam_path)\n    for filename, url in ((\"ham.tar.bz2\", ham_url), (\"spam.tar.bz2\", spam_url)):\n        path = os.path.join(spam_path, filename)\n        if not os.path.isfile(path):\n            urllib.request.urlretrieve(url, path)\n        tar_bz2_file = tarfile.open(path)\n        tar_bz2_file.extractall(path=spam_path)\n        tar_bz2_file.close()\n\n\nfetch_spam_data()\n\n다음, 모든 이메일을 읽어 들입니다:\n\nHAM_DIR = os.path.join(SPAM_PATH, \"easy_ham\")\nSPAM_DIR = os.path.join(SPAM_PATH, \"spam\")\nham_filenames = [name for name in sorted(os.listdir(HAM_DIR)) if len(name) &gt; 20]\nspam_filenames = [name for name in sorted(os.listdir(SPAM_DIR)) if len(name) &gt; 20]\n\n\nlen(ham_filenames)\n\n2500\n\n\n\nlen(spam_filenames)\n\n500\n\n\n파이썬의 email 모듈을 사용해 이메일을 파싱합니다(헤더, 인코딩 등을 처리합니다):\n\nimport email\nimport email.policy\n\ndef load_email(is_spam, filename, spam_path=SPAM_PATH):\n    directory = \"spam\" if is_spam else \"easy_ham\"\n    with open(os.path.join(spam_path, directory, filename), \"rb\") as f:\n        return email.parser.BytesParser(policy=email.policy.default).parse(f)\n\n\nham_emails = [load_email(is_spam=False, filename=name) for name in ham_filenames]\nspam_emails = [load_email(is_spam=True, filename=name) for name in spam_filenames]\n\n데이터가 어떻게 구성되어 있는지 감을 잡기 위해 햄 메일과 스팸 메일을 하나씩 보겠습니다:\n\nprint(ham_emails[1].get_content().strip())\n\nMartin A posted:\nTassos Papadopoulos, the Greek sculptor behind the plan, judged that the\n limestone of Mount Kerdylio, 70 miles east of Salonika and not far from the\n Mount Athos monastic community, was ideal for the patriotic sculpture. \n \n As well as Alexander's granite features, 240 ft high and 170 ft wide, a\n museum, a restored amphitheatre and car park for admiring crowds are\nplanned\n---------------------\nSo is this mountain limestone or granite?\nIf it's limestone, it'll weather pretty fast.\n\n------------------------ Yahoo! Groups Sponsor ---------------------~--&gt;\n4 DVDs Free +s&p Join Now\nhttp://us.click.yahoo.com/pt6YBB/NXiEAA/mG3HAA/7gSolB/TM\n---------------------------------------------------------------------~-&gt;\n\nTo unsubscribe from this group, send an email to:\nforteana-unsubscribe@egroups.com\n\n \n\nYour use of Yahoo! Groups is subject to http://docs.yahoo.com/info/terms/\n\n\n\nprint(spam_emails[6].get_content().strip())\n\nHelp wanted.  We are a 14 year old fortune 500 company, that is\ngrowing at a tremendous rate.  We are looking for individuals who\nwant to work from home.\n\nThis is an opportunity to make an excellent income.  No experience\nis required.  We will train you.\n\nSo if you are looking to be employed from home with a career that has\nvast opportunities, then go:\n\nhttp://www.basetel.com/wealthnow\n\nWe are looking for energetic and self motivated people.  If that is you\nthan click on the link and fill out the form, and one of our\nemployement specialist will contact you.\n\nTo be removed from our link simple go to:\n\nhttp://www.basetel.com/remove.html\n\n\n4139vOLW7-758DoDY1425FRhM1-764SMFc8513fCsLl40\n\n\n어떤 이메일은 이미지나 첨부 파일을 가진 멀티파트(multipart)입니다(메일에 포함되어 있을수 있습니다). 어떤 파일들이 있는지 살펴 보겠습니다:\n\ndef get_email_structure(email):\n    if isinstance(email, str):\n        return email\n    payload = email.get_payload()\n    if isinstance(payload, list):\n        return \"multipart({})\".format(\", \".join([\n            get_email_structure(sub_email)\n            for sub_email in payload\n        ]))\n    else:\n        return email.get_content_type()\n\n\nfrom collections import Counter\n\ndef structures_counter(emails):\n    structures = Counter()\n    for email in emails:\n        structure = get_email_structure(email)\n        structures[structure] += 1\n    return structures\n\n\nstructures_counter(ham_emails).most_common()\n\n[('text/plain', 2408),\n ('multipart(text/plain, application/pgp-signature)', 66),\n ('multipart(text/plain, text/html)', 8),\n ('multipart(text/plain, text/plain)', 4),\n ('multipart(text/plain)', 3),\n ('multipart(text/plain, application/octet-stream)', 2),\n ('multipart(text/plain, text/enriched)', 1),\n ('multipart(text/plain, application/ms-tnef, text/plain)', 1),\n ('multipart(multipart(text/plain, text/plain, text/plain), application/pgp-signature)',\n  1),\n ('multipart(text/plain, video/mng)', 1),\n ('multipart(text/plain, multipart(text/plain))', 1),\n ('multipart(text/plain, application/x-pkcs7-signature)', 1),\n ('multipart(text/plain, multipart(text/plain, text/plain), text/rfc822-headers)',\n  1),\n ('multipart(text/plain, multipart(text/plain, text/plain), multipart(multipart(text/plain, application/x-pkcs7-signature)))',\n  1),\n ('multipart(text/plain, application/x-java-applet)', 1)]\n\n\n\nstructures_counter(spam_emails).most_common()\n\n[('text/plain', 218),\n ('text/html', 183),\n ('multipart(text/plain, text/html)', 45),\n ('multipart(text/html)', 20),\n ('multipart(text/plain)', 19),\n ('multipart(multipart(text/html))', 5),\n ('multipart(text/plain, image/jpeg)', 3),\n ('multipart(text/html, application/octet-stream)', 2),\n ('multipart(text/plain, application/octet-stream)', 1),\n ('multipart(text/html, text/plain)', 1),\n ('multipart(multipart(text/html), application/octet-stream, image/jpeg)', 1),\n ('multipart(multipart(text/plain, text/html), image/gif)', 1),\n ('multipart/alternative', 1)]\n\n\n햄 메일은 평범한 텍스트가 많고 스팸은 HTML일 경우가 많습니다. 적은 수의 햄 이메일이 PGP로 서명되어 있지만 스팸 메일에는 없습니다. 요약하면 이메일 구조는 유용한 정보입니다.\n이제 이메일 헤더를 살펴보겠습니다:\n\nfor header, value in spam_emails[0].items():\n    print(header,\":\",value)\n\nReturn-Path : &lt;12a1mailbot1@web.de&gt;\nDelivered-To : zzzz@localhost.spamassassin.taint.org\nReceived : from localhost (localhost [127.0.0.1])   by phobos.labs.spamassassin.taint.org (Postfix) with ESMTP id 136B943C32    for &lt;zzzz@localhost&gt;; Thu, 22 Aug 2002 08:17:21 -0400 (EDT)\nReceived : from mail.webnote.net [193.120.211.219]  by localhost with POP3 (fetchmail-5.9.0)    for zzzz@localhost (single-drop); Thu, 22 Aug 2002 13:17:21 +0100 (IST)\nReceived : from dd_it7 ([210.97.77.167])    by webnote.net (8.9.3/8.9.3) with ESMTP id NAA04623 for &lt;zzzz@spamassassin.taint.org&gt;; Thu, 22 Aug 2002 13:09:41 +0100\nFrom : 12a1mailbot1@web.de\nReceived : from r-smtp.korea.com - 203.122.2.197 by dd_it7  with Microsoft SMTPSVC(5.5.1775.675.6);  Sat, 24 Aug 2002 09:42:10 +0900\nTo : dcek1a1@netsgo.com\nSubject : Life Insurance - Why Pay More?\nDate : Wed, 21 Aug 2002 20:31:57 -1600\nMIME-Version : 1.0\nMessage-ID : &lt;0103c1042001882DD_IT7@dd_it7&gt;\nContent-Type : text/html; charset=\"iso-8859-1\"\nContent-Transfer-Encoding : quoted-printable\n\n\n보낸사람의 이메일 주소와 같이 헤더에는 유용한 정보가 많이 있지만 여기서는 Subject 헤더만 다뤄 보겠습니다:\n\nspam_emails[0][\"Subject\"]\n\n'Life Insurance - Why Pay More?'\n\n\n좋습니다. 데이터에를 더 살펴보기 전에 훈련 세트와 테스트 세트로 나누도록 하겠습니다:\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nX = np.array(ham_emails + spam_emails, dtype=object)\ny = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n이제 전처리 함수를 작성하겠습니다. 먼저 HTML을 일반 텍스트로 변환하는 함수가 필요합니다. 이 작업에는 당연히 BeautifulSoup 라이브러리를 사용하는게 좋지만 의존성을 줄이기 위해서 정규식을 사용하여 대강 만들어 보겠습니다(un̨ho͞ly radiańcé destro҉ying all enli̍̈́̂̈́ghtenment의 위험에도 불구하고). 다음 함수는 &lt;head&gt; 섹션을 삭제하고 모든 &lt;a&gt; 태그를 HYPERLINK 문자로 바꿉니다. 그런 다음 모든 HTML 태그를 제거하고 텍스트만 남깁니다. 보기 편하게 여러개의 개행 문자를 하나로 만들고 (&gt;나 &nbsp; 같은) html 엔티티를 복원합니다:\n\nimport re\nfrom html import unescape\n\ndef html_to_plain_text(html):\n    text = re.sub('&lt;head.*?&gt;.*?&lt;/head&gt;', '', html, flags=re.M | re.S | re.I)\n    text = re.sub('&lt;a\\s.*?&gt;', ' HYPERLINK ', text, flags=re.M | re.S | re.I)\n    text = re.sub('&lt;.*?&gt;', '', text, flags=re.M | re.S)\n    text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\n    return unescape(text)\n\n잘 작동하는지 확인해 보겠습니다. 다음은 HTML 스팸입니다:\n\nhtml_spam_emails = [email for email in X_train[y_train==1]\n                    if get_email_structure(email) == \"text/html\"]\nsample_html_spam = html_spam_emails[7]\nprint(sample_html_spam.get_content().strip()[:1000], \"...\")\n\n&lt;HTML&gt;&lt;HEAD&gt;&lt;TITLE&gt;&lt;/TITLE&gt;&lt;META http-equiv=\"Content-Type\" content=\"text/html; charset=windows-1252\"&gt;&lt;STYLE&gt;A:link {TEX-DECORATION: none}A:active {TEXT-DECORATION: none}A:visited {TEXT-DECORATION: none}A:hover {COLOR: #0033ff; TEXT-DECORATION: underline}&lt;/STYLE&gt;&lt;META content=\"MSHTML 6.00.2713.1100\" name=\"GENERATOR\"&gt;&lt;/HEAD&gt;\n&lt;BODY text=\"#000000\" vLink=\"#0033ff\" link=\"#0033ff\" bgColor=\"#CCCC99\"&gt;&lt;TABLE borderColor=\"#660000\" cellSpacing=\"0\" cellPadding=\"0\" border=\"0\" width=\"100%\"&gt;&lt;TR&gt;&lt;TD bgColor=\"#CCCC99\" valign=\"top\" colspan=\"2\" height=\"27\"&gt;\n&lt;font size=\"6\" face=\"Arial, Helvetica, sans-serif\" color=\"#660000\"&gt;\n&lt;b&gt;OTC&lt;/b&gt;&lt;/font&gt;&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD height=\"2\" bgcolor=\"#6a694f\"&gt;\n&lt;font size=\"5\" face=\"Times New Roman, Times, serif\" color=\"#FFFFFF\"&gt;\n&lt;b&gt;&nbsp;Newsletter&lt;/b&gt;&lt;/font&gt;&lt;/TD&gt;&lt;TD height=\"2\" bgcolor=\"#6a694f\"&gt;&lt;div align=\"right\"&gt;&lt;font color=\"#FFFFFF\"&gt;\n&lt;b&gt;Discover Tomorrow's Winners&nbsp;&lt;/b&gt;&lt;/font&gt;&lt;/div&gt;&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD height=\"25\" colspan=\"2\" bgcolor=\"#CCCC99\"&gt;&lt;table width=\"100%\" border=\"0\"  ...\n\n\n변환된 텍스트입니다:\n\nprint(html_to_plain_text(sample_html_spam.get_content())[:1000], \"...\")\n\n\nOTC\n Newsletter\nDiscover Tomorrow's Winners \nFor Immediate Release\nCal-Bay (Stock Symbol: CBYI)\nWatch for analyst \"Strong Buy Recommendations\" and several advisory newsletters picking CBYI.  CBYI has filed to be traded on the OTCBB, share prices historically INCREASE when companies get listed on this larger trading exchange. CBYI is trading around 25 cents and should skyrocket to $2.66 - $3.25 a share in the near future.\nPut CBYI on your watch list, acquire a position TODAY.\nREASONS TO INVEST IN CBYI\nA profitable company and is on track to beat ALL earnings estimates!\nOne of the FASTEST growing distributors in environmental & safety equipment instruments.\nExcellent management team, several EXCLUSIVE contracts.  IMPRESSIVE client list including the U.S. Air Force, Anheuser-Busch, Chevron Refining and Mitsubishi Heavy Industries, GE-Energy & Environmental Research.\nRAPIDLY GROWING INDUSTRY\nIndustry revenues exceed $900 million, estimates indicate that there could be as much as $25 billi ...\n\n\n아주 좋습니다! 이제 포맷에 상관없이 이메일을 입력으로 받아서 일반 텍스트를 출력하는 함수를 만들겠습니다:\n\ndef email_to_text(email):\n    html = None\n    for part in email.walk():\n        ctype = part.get_content_type()\n        if not ctype in (\"text/plain\", \"text/html\"):\n            continue\n        try:\n            content = part.get_content()\n        except: # in case of encoding issues\n            content = str(part.get_payload())\n        if ctype == \"text/plain\":\n            return content\n        else:\n            html = content\n    if html:\n        return html_to_plain_text(html)\n\n\nprint(email_to_text(sample_html_spam)[:100], \"...\")\n\n\nOTC\n Newsletter\nDiscover Tomorrow's Winners \nFor Immediate Release\nCal-Bay (Stock Symbol: CBYI)\nWat ...\n\n\n어간 추출을 해보죠! 이 작업을 하려면 자연어 처리 툴킷(NLTK)을 설치해야 합니다. 다음 명령으로 간단히 설치할 수 있습니다(먼저 virtualenv 환경을 활성화시켜야 합니다. 별도의 환경이 없다면 어드민 권한이 필요할지 모릅니다. 아니면 --user 옵션을 사용하세요):\n$ pip install nltk\n\ntry:\n    import nltk\n\n    stemmer = nltk.PorterStemmer()\n    for word in (\"Computations\", \"Computation\", \"Computing\", \"Computed\", \"Compute\", \"Compulsive\"):\n        print(word, \"=&gt;\", stemmer.stem(word))\nexcept ImportError:\n    print(\"Error: stemming requires the NLTK module.\")\n    stemmer = None\n\nComputations =&gt; comput\nComputation =&gt; comput\nComputing =&gt; comput\nComputed =&gt; comput\nCompute =&gt; comput\nCompulsive =&gt; compuls\n\n\n인터넷 주소는 “URL” 문자로 바꾸겠습니다. 정규식을 하드 코딩할 수도 있지만 urlextract 라이브러리를 사용하겠습니다. 다음 명령으로 설치합니다(먼저 virtualenv 환경을 활성화시켜야 합니다. 별도의 환경이 없다면 어드민 권한이 필요할지 모릅니다. 아니면 --user 옵션을 사용하세요):\n$ pip install urlextract\n\n# 코랩에서 이 노트북을 실행하려면 먼저 pip install urlextract을 실행합니다\ntry:\n    import google.colab\n    %pip install -q -U urlextract\nexcept ImportError:\n    pass # 코랩에서는 실행되지 않음\n\n노트: 주피터 노트북에서는 항상 !pip 대신 %pip를 사용해야 합니다. !pip는 다른 환경에 라이브러리를 설치할 수 있기 때문입니다. 반면 %pip는 현재 실행 중인 환경에 설치됩니다.\n\ntry:\n    import urlextract # 루트 도메인 이름을 다운로드하기 위해 인터넷 연결이 필요할지 모릅니다\n    \n    url_extractor = urlextract.URLExtract()\n    print(url_extractor.find_urls(\"Will it detect github.com and https://youtu.be/7Pq-S557XQU?t=3m32s\"))\nexcept ImportError:\n    print(\"Error: replacing URLs requires the urlextract module.\")\n    url_extractor = None\n\n['github.com', 'https://youtu.be/7Pq-S557XQU?t=3m32s']\n\n\n이들을 모두 하나의 변환기로 연결하여 이메일을 단어 카운트로 바꿀 것입니다. 파이썬의 split() 메서드를 사용하면 구둣점과 단어 경계를 기준으로 문장을 단어로 바꿉니다. 이 방법이 많은 언어에 통하지만 전부는 아닙니다. 예를 들어 중국어와 일본어는 일반적으로 단어 사이에 공백을 두지 않습니다. 베트남어는 음절 사이에 공백을 두기도 합니다. 여기서는 데이터셋이 (거의) 영어로 되어 있기 때문에 문제없습니다.\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, strip_headers=True, lower_case=True, remove_punctuation=True,\n                 replace_urls=True, replace_numbers=True, stemming=True):\n        self.strip_headers = strip_headers\n        self.lower_case = lower_case\n        self.remove_punctuation = remove_punctuation\n        self.replace_urls = replace_urls\n        self.replace_numbers = replace_numbers\n        self.stemming = stemming\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        X_transformed = []\n        for email in X:\n            text = email_to_text(email) or \"\"\n            if self.lower_case:\n                text = text.lower()\n            if self.replace_urls and url_extractor is not None:\n                urls = list(set(url_extractor.find_urls(text)))\n                urls.sort(key=lambda url: len(url), reverse=True)\n                for url in urls:\n                    text = text.replace(url, \" URL \")\n            if self.replace_numbers:\n                text = re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?', 'NUMBER', text)\n            if self.remove_punctuation:\n                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n            word_counts = Counter(text.split())\n            if self.stemming and stemmer is not None:\n                stemmed_word_counts = Counter()\n                for word, count in word_counts.items():\n                    stemmed_word = stemmer.stem(word)\n                    stemmed_word_counts[stemmed_word] += count\n                word_counts = stemmed_word_counts\n            X_transformed.append(word_counts)\n        return np.array(X_transformed)\n\n이 변환기를 몇 개의 이메일에 적용해 보겠습니다:\n\nX_few = X_train[:3]\nX_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)\nX_few_wordcounts\n\narray([Counter({'chuck': 1, 'murcko': 1, 'wrote': 1, 'stuff': 1, 'yawn': 1, 'r': 1}),\n       Counter({'the': 11, 'of': 9, 'and': 8, 'all': 3, 'christian': 3, 'to': 3, 'by': 3, 'jefferson': 2, 'i': 2, 'have': 2, 'superstit': 2, 'one': 2, 'on': 2, 'been': 2, 'ha': 2, 'half': 2, 'rogueri': 2, 'teach': 2, 'jesu': 2, 'some': 1, 'interest': 1, 'quot': 1, 'url': 1, 'thoma': 1, 'examin': 1, 'known': 1, 'word': 1, 'do': 1, 'not': 1, 'find': 1, 'in': 1, 'our': 1, 'particular': 1, 'redeem': 1, 'featur': 1, 'they': 1, 'are': 1, 'alik': 1, 'found': 1, 'fabl': 1, 'mytholog': 1, 'million': 1, 'innoc': 1, 'men': 1, 'women': 1, 'children': 1, 'sinc': 1, 'introduct': 1, 'burnt': 1, 'tortur': 1, 'fine': 1, 'imprison': 1, 'what': 1, 'effect': 1, 'thi': 1, 'coercion': 1, 'make': 1, 'world': 1, 'fool': 1, 'other': 1, 'hypocrit': 1, 'support': 1, 'error': 1, 'over': 1, 'earth': 1, 'six': 1, 'histor': 1, 'american': 1, 'john': 1, 'e': 1, 'remsburg': 1, 'letter': 1, 'william': 1, 'short': 1, 'again': 1, 'becom': 1, 'most': 1, 'pervert': 1, 'system': 1, 'that': 1, 'ever': 1, 'shone': 1, 'man': 1, 'absurd': 1, 'untruth': 1, 'were': 1, 'perpetr': 1, 'upon': 1, 'a': 1, 'larg': 1, 'band': 1, 'dupe': 1, 'import': 1, 'led': 1, 'paul': 1, 'first': 1, 'great': 1, 'corrupt': 1}),\n       Counter({'url': 4, 's': 3, 'group': 3, 'to': 3, 'in': 2, 'forteana': 2, 'martin': 2, 'an': 2, 'and': 2, 'we': 2, 'is': 2, 'yahoo': 2, 'unsubscrib': 2, 'y': 1, 'adamson': 1, 'wrote': 1, 'for': 1, 'altern': 1, 'rather': 1, 'more': 1, 'factual': 1, 'base': 1, 'rundown': 1, 'on': 1, 'hamza': 1, 'career': 1, 'includ': 1, 'hi': 1, 'belief': 1, 'that': 1, 'all': 1, 'non': 1, 'muslim': 1, 'yemen': 1, 'should': 1, 'be': 1, 'murder': 1, 'outright': 1, 'know': 1, 'how': 1, 'unbias': 1, 'memri': 1, 'don': 1, 't': 1, 'html': 1, 'rob': 1, 'sponsor': 1, 'number': 1, 'dvd': 1, 'free': 1, 'p': 1, 'join': 1, 'now': 1, 'from': 1, 'thi': 1, 'send': 1, 'email': 1, 'egroup': 1, 'com': 1, 'your': 1, 'use': 1, 'of': 1, 'subject': 1})],\n      dtype=object)\n\n\n제대로 작동하는 것 같네요!\n이제 단어 카운트를 벡터로 변환해야 합니다. 이를 위해서 또 다른 변환기를 만들겠습니다. 이 변환기는 (자주 나타나는 단어 순으로 정렬된) 어휘 목록을 구축하는 fit() 메서드와 어휘 목록을 사용해 단어를 벡터로 바꾸는 transform() 메서드를 가집니다. 출력은 희소 행렬이 됩니다.\n\nfrom scipy.sparse import csr_matrix\n\nclass WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, vocabulary_size=1000):\n        self.vocabulary_size = vocabulary_size\n    def fit(self, X, y=None):\n        total_count = Counter()\n        for word_count in X:\n            for word, count in word_count.items():\n                total_count[word] += min(count, 10)\n        most_common = total_count.most_common()[:self.vocabulary_size]\n        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n        return self\n    def transform(self, X, y=None):\n        rows = []\n        cols = []\n        data = []\n        for row, word_count in enumerate(X):\n            for word, count in word_count.items():\n                rows.append(row)\n                cols.append(self.vocabulary_.get(word, 0))\n                data.append(count)\n        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))\n\n\nvocab_transformer = WordCounterToVectorTransformer(vocabulary_size=10)\nX_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)\nX_few_vectors\n\n&lt;3x11 sparse matrix of type '&lt;class 'numpy.int64'&gt;'\n    with 20 stored elements in Compressed Sparse Row format&gt;\n\n\n\nX_few_vectors.toarray()\n\narray([[ 6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n       [99, 11,  9,  8,  3,  1,  3,  1,  3,  2,  3],\n       [67,  0,  1,  2,  3,  4,  1,  2,  0,  1,  0]])\n\n\n이 행렬은 무엇을 의미하나요? 세 번째 행의 첫 번째 열의 65는 세 번째 이메일이 어휘 목록에 없는 단어를 65개 가지고 있다는 뜻입니다. 그 다음의 0은 어휘 목록에 있는 첫 번째 단어가 한 번도 등장하지 않는다는 뜻이고 그 다음의 1은 한 번 나타난다는 뜻입니다. 이 단어들이 무엇인지 확인하려면 어휘 목록을 보면 됩니다. 첫 번째 단어는 “the”이고 두 번째 단어는 “of”입니다.\n\nvocab_transformer.vocabulary_\n\n{'the': 1,\n 'of': 2,\n 'and': 3,\n 'to': 4,\n 'url': 5,\n 'all': 6,\n 'in': 7,\n 'christian': 8,\n 'on': 9,\n 'by': 10}\n\n\n이제 스팸 분류기를 훈련시킬 준비를 마쳤습니다! 전체 데이터셋을 변환시켜보죠:\n\nfrom sklearn.pipeline import Pipeline\n\npreprocess_pipeline = Pipeline([\n    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n])\n\nX_train_transformed = preprocess_pipeline.fit_transform(X_train)\n\nNote: to be future-proof, we set solver=\"lbfgs\" since this will be the default value in Scikit-Learn 0.22.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\nlog_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42)\nscore = cross_val_score(log_clf, X_train_transformed, y_train, cv=3, verbose=3)\nscore.mean()\n\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.5s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.9s finished\n\n\n[CV] END ................................ score: (test=0.981) total time=   0.2s\n[CV] END ................................ score: (test=0.984) total time=   0.3s\n[CV] END ................................ score: (test=0.991) total time=   0.4s\n\n\n0.9854166666666666\n\n\n98.5%가 넘네요. 첫 번째 시도치고 나쁘지 않습니다! :) 그러나 이 데이터셋은 비교적 쉬운 문제입니다. 더 어려운 데이터셋에 적용해 보면 결과가 그리 높지 않을 것입니다. 여러개의 모델을 시도해 보고 제일 좋은 것을 골라 교차 검증으로 세밀하게 튜닝해 보세요.\n하지만 전체 내용을 파악했으므로 여기서 멈추겠습니다. 테스트 세트에서 정밀도/재현율을 출력해 보겠습니다:\n\nfrom sklearn.metrics import precision_score, recall_score\n\nX_test_transformed = preprocess_pipeline.transform(X_test)\n\nlog_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42)\nlog_clf.fit(X_train_transformed, y_train)\n\ny_pred = log_clf.predict(X_test_transformed)\n\nprint(\"정밀도: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\nprint(\"재현율: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))\n\n정밀도: 95.88%\n재현율: 97.89%"
  },
  {
    "objectID": "Machine_Learning/01_the_machine_learning_landscape.html",
    "href": "Machine_Learning/01_the_machine_learning_landscape.html",
    "title": "01_the_machine_learning_landscape",
    "section": "",
    "text": "1장 – 한 눈에 보는 머신러닝\n이 노트북은 1장의 그림을 만들기 위한 것입니다.\n\n\n\n구글 코랩에서 실행하기\n\n\n\n\n예제 1-1\n파이썬 2.x도 사용할 수 있지만 향후 이 버전은 더이상 지원되지 않습니다. 대신 파이썬 3을 사용세요.\n번역서의 깃허브는 파이썬 3.7, 사이킷런 0.22.2에서 테스트했습니다.\n\n# Python ≥3.5 이상이 권장됩니다\nimport sys\nassert sys.version_info &gt;= (3, 5)\n\n\n# Scikit-Learn ≥0.20 이상이 권장됩니다\nimport sklearn\nassert sklearn.__version__ &gt;= \"0.20\"\n\n이 함수는 OECD의 삶의 만족도(life satisfaction) 데이터와 IMF의 1인당 GDP(GDP per capita) 데이터를 합칩니다. 이는 번거로운 작업이고 머신러닝과는 관계가 없기 때문에 책 안에 포함시키지 않았습니다.\n\ndef prepare_country_stats(oecd_bli, gdp_per_capita):\n    oecd_bli = oecd_bli[oecd_bli[\"INEQUALITY\"]==\"TOT\"]\n    oecd_bli = oecd_bli.pivot(index=\"Country\", columns=\"Indicator\", values=\"Value\")\n    gdp_per_capita.rename(columns={\"2015\": \"GDP per capita\"}, inplace=True)\n    gdp_per_capita.set_index(\"Country\", inplace=True)\n    full_country_stats = pd.merge(left=oecd_bli, right=gdp_per_capita,\n                                  left_index=True, right_index=True)\n    full_country_stats.sort_values(by=\"GDP per capita\", inplace=True)\n    remove_indices = [0, 1, 6, 8, 33, 34, 35]\n    keep_indices = list(set(range(36)) - set(remove_indices))\n    return full_country_stats[[\"GDP per capita\", 'Life satisfaction']].iloc[keep_indices]\n\n책에 있는 코드는 데이터 파일이 현재 디렉토리에 있다고 가정합니다. 여기에서는 datasets/lifesat 안에서 파일을 읽어 들입니다.\n\nimport os\ndatapath = os.path.join(\"datasets\", \"lifesat\", \"\")\n\n\n# 주피터에 그래프를 깔끔하게 그리기 위해서\n%matplotlib inline\nimport matplotlib as mpl\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\n\n# 데이터 다운로드\nimport urllib.request\nDOWNLOAD_ROOT = \"https://raw.githubusercontent.com/rickiepark/handson-ml2/master/\"\nos.makedirs(datapath, exist_ok=True)\nfor filename in (\"oecd_bli_2015.csv\", \"gdp_per_capita.csv\"):\n    print(\"Downloading\", filename)\n    url = DOWNLOAD_ROOT + \"datasets/lifesat/\" + filename\n    urllib.request.urlretrieve(url, datapath + filename)\n\nDownloading oecd_bli_2015.csv\nDownloading gdp_per_capita.csv\n\n\n\n# 예제 코드\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport sklearn.linear_model\n\n# 데이터 적재\noecd_bli = pd.read_csv(datapath + \"oecd_bli_2015.csv\", thousands=',')\ngdp_per_capita = pd.read_csv(datapath + \"gdp_per_capita.csv\",thousands=',',delimiter='\\t',\n                             encoding='latin1', na_values=\"n/a\")\n\n# 데이터 준비\ncountry_stats = prepare_country_stats(oecd_bli, gdp_per_capita)\nX = np.c_[country_stats[\"GDP per capita\"]]\ny = np.c_[country_stats[\"Life satisfaction\"]]\n\n# 데이터 시각화\ncountry_stats.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction')\nplt.show()\n\n# 선형 모델 선택\nmodel = sklearn.linear_model.LinearRegression()\n\n# 모델 훈련\nmodel.fit(X, y)\n\n# 키프로스에 대한 예측\nX_new = [[22587]]  # 키프로스 1인당 GDP\nprint(model.predict(X_new)) # 출력 [[ 5.96242338]]\n\n\n\n\n[[5.96242338]]\n\n\n이전 코드에서 선형 회귀 모델을 k-최근접 이웃 회귀(이 경우 k=3)로 바꾸려면 간단하게 다음 두 라인만 바꾸면 됩니다:\nimport sklearn.linear_model\nmodel = sklearn.linear_model.LinearRegression()\n아래와 같이 바꿉니다:\nimport sklearn.neighbors\nmodel = sklearn.neighbors.KNeighborsRegressor(n_neighbors=3)\n\n# 3-최근접 이웃 회귀 모델로 바꿉니다.\nimport sklearn.neighbors\nmodel1 = sklearn.neighbors.KNeighborsRegressor(n_neighbors=3)\n\n# 모델을 훈련합니다.\nmodel1.fit(X,y)\n\n# 키프로스에 대한 예측을 만듭니다.\nprint(model1.predict(X_new)) # 출력 [[5.76666667]]\n\n[[5.76666667]]\n\n\n\n\n알림: 이후의 코드는 무시해도 좋습니다. 이 코드는 1장에 있는 그림을 생성하기 위한 것입니다.\n그림을 저장하기 위함 함수\n\n# Where to save the figures\nPROJECT_ROOT_DIR = \".\"\nCHAPTER_ID = \"fundamentals\"\nIMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\nos.makedirs(IMAGES_PATH, exist_ok=True)\n\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n    print(\"Saving figure\", fig_id)\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)\n\n노트북 출력을 동일하게 만들기 위해서\n\nnp.random.seed(42)\n\n\n\n삶의 만족도 데이터 적재와 준비\n필요하면 OECE 웹사이트에서 새 데이터를 받을 수 있습니다. http://stats.oecd.org/index.aspx?DataSetCode=BLI 에서 CSV 파일을 다운받아 datasets/lifesat/에 저장합니다.\n\noecd_bli = pd.read_csv(datapath + \"oecd_bli_2015.csv\", thousands=',')\noecd_bli = oecd_bli[oecd_bli[\"INEQUALITY\"]==\"TOT\"]\noecd_bli = oecd_bli.pivot(index=\"Country\", columns=\"Indicator\", values=\"Value\")\noecd_bli.head(2)\n\n\n\n\n\n\n\nIndicator\nAir pollution\nAssault rate\nConsultation on rule-making\nDwellings without basic facilities\nEducational attainment\nEmployees working very long hours\nEmployment rate\nHomicide rate\nHousehold net adjusted disposable income\nHousehold net financial wealth\n...\nLong-term unemployment rate\nPersonal earnings\nQuality of support network\nRooms per person\nSelf-reported health\nStudent skills\nTime devoted to leisure and personal care\nVoter turnout\nWater quality\nYears in education\n\n\nCountry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAustralia\n13.0\n2.1\n10.5\n1.1\n76.0\n14.02\n72.0\n0.8\n31588.0\n47657.0\n...\n1.08\n50449.0\n92.0\n2.3\n85.0\n512.0\n14.41\n93.0\n91.0\n19.4\n\n\nAustria\n27.0\n3.4\n7.1\n1.0\n83.0\n7.61\n72.0\n0.4\n31173.0\n49887.0\n...\n1.19\n45199.0\n89.0\n1.6\n69.0\n500.0\n14.46\n75.0\n94.0\n17.0\n\n\n\n\n2 rows × 24 columns\n\n\n\n\noecd_bli[\"Life satisfaction\"].head()\n\nCountry\nAustralia    7.3\nAustria      6.9\nBelgium      6.9\nBrazil       7.0\nCanada       7.3\nName: Life satisfaction, dtype: float64\n\n\n\n\n1인당 GDP 데이터 적재와 준비\n위와 마찬가지로 필요하면 1인당 GDP 데이터를 새로 받을 수 있습니다. http://goo.gl/j1MSKe (=&gt; imf.org)에서 다운받아 datasets/lifesat/에 저장하면 됩니다.\n\ngdp_per_capita = pd.read_csv(datapath+\"gdp_per_capita.csv\", thousands=',', delimiter='\\t',\n                             encoding='latin1', na_values=\"n/a\")\ngdp_per_capita.rename(columns={\"2015\": \"GDP per capita\"}, inplace=True)\ngdp_per_capita.set_index(\"Country\", inplace=True)\ngdp_per_capita.head(2)\n\n\n\n\n\n\n\n\nSubject Descriptor\nUnits\nScale\nCountry/Series-specific Notes\nGDP per capita\nEstimates Start After\n\n\nCountry\n\n\n\n\n\n\n\n\n\n\nAfghanistan\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n599.994\n2013.0\n\n\nAlbania\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n3995.383\n2010.0\n\n\n\n\n\n\n\n\nfull_country_stats = pd.merge(left=oecd_bli, right=gdp_per_capita, left_index=True, right_index=True)\nfull_country_stats.sort_values(by=\"GDP per capita\", inplace=True)\nfull_country_stats\n\n\n\n\n\n\n\n\nAir pollution\nAssault rate\nConsultation on rule-making\nDwellings without basic facilities\nEducational attainment\nEmployees working very long hours\nEmployment rate\nHomicide rate\nHousehold net adjusted disposable income\nHousehold net financial wealth\n...\nTime devoted to leisure and personal care\nVoter turnout\nWater quality\nYears in education\nSubject Descriptor\nUnits\nScale\nCountry/Series-specific Notes\nGDP per capita\nEstimates Start After\n\n\nCountry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBrazil\n18.0\n7.9\n4.0\n6.7\n45.0\n10.41\n67.0\n25.5\n11664.0\n6844.0\n...\n14.97\n79.0\n72.0\n16.3\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n8669.998\n2014.0\n\n\nMexico\n30.0\n12.8\n9.0\n4.2\n37.0\n28.83\n61.0\n23.4\n13085.0\n9056.0\n...\n13.89\n63.0\n67.0\n14.4\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n9009.280\n2015.0\n\n\nRussia\n15.0\n3.8\n2.5\n15.1\n94.0\n0.16\n69.0\n12.8\n19292.0\n3412.0\n...\n14.97\n65.0\n56.0\n16.0\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n9054.914\n2015.0\n\n\nTurkey\n35.0\n5.0\n5.5\n12.7\n34.0\n40.86\n50.0\n1.2\n14095.0\n3251.0\n...\n13.42\n88.0\n62.0\n16.4\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n9437.372\n2013.0\n\n\nHungary\n15.0\n3.6\n7.9\n4.8\n82.0\n3.19\n58.0\n1.3\n15442.0\n13277.0\n...\n15.04\n62.0\n77.0\n17.6\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n12239.894\n2015.0\n\n\nPoland\n33.0\n1.4\n10.8\n3.2\n90.0\n7.41\n60.0\n0.9\n17852.0\n10919.0\n...\n14.20\n55.0\n79.0\n18.4\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n12495.334\n2014.0\n\n\nChile\n46.0\n6.9\n2.0\n9.4\n57.0\n15.42\n62.0\n4.4\n14533.0\n17733.0\n...\n14.41\n49.0\n73.0\n16.5\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n13340.905\n2014.0\n\n\nSlovak Republic\n13.0\n3.0\n6.6\n0.6\n92.0\n7.02\n60.0\n1.2\n17503.0\n8663.0\n...\n14.99\n59.0\n81.0\n16.3\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n15991.736\n2015.0\n\n\nCzech Republic\n16.0\n2.8\n6.8\n0.9\n92.0\n6.98\n68.0\n0.8\n18404.0\n17299.0\n...\n14.98\n59.0\n85.0\n18.1\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n17256.918\n2015.0\n\n\nEstonia\n9.0\n5.5\n3.3\n8.1\n90.0\n3.30\n68.0\n4.8\n15167.0\n7680.0\n...\n14.90\n64.0\n79.0\n17.5\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n17288.083\n2014.0\n\n\nGreece\n27.0\n3.7\n6.5\n0.7\n68.0\n6.16\n49.0\n1.6\n18575.0\n14579.0\n...\n14.91\n64.0\n69.0\n18.6\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n18064.288\n2014.0\n\n\nPortugal\n18.0\n5.7\n6.5\n0.9\n38.0\n9.62\n61.0\n1.1\n20086.0\n31245.0\n...\n14.95\n58.0\n86.0\n17.6\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n19121.592\n2014.0\n\n\nSlovenia\n26.0\n3.9\n10.3\n0.5\n85.0\n5.63\n63.0\n0.4\n19326.0\n18465.0\n...\n14.62\n52.0\n88.0\n18.4\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n20732.482\n2015.0\n\n\nSpain\n24.0\n4.2\n7.3\n0.1\n55.0\n5.89\n56.0\n0.6\n22477.0\n24774.0\n...\n16.06\n69.0\n71.0\n17.6\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n25864.721\n2014.0\n\n\nKorea\n30.0\n2.1\n10.4\n4.2\n82.0\n18.72\n64.0\n1.1\n19510.0\n29091.0\n...\n14.63\n76.0\n78.0\n17.5\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n27195.197\n2014.0\n\n\nItaly\n21.0\n4.7\n5.0\n1.1\n57.0\n3.66\n56.0\n0.7\n25166.0\n54987.0\n...\n14.98\n75.0\n71.0\n16.8\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n29866.581\n2015.0\n\n\nJapan\n24.0\n1.4\n7.3\n6.4\n94.0\n22.26\n72.0\n0.3\n26111.0\n86764.0\n...\n14.93\n53.0\n85.0\n16.3\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n32485.545\n2015.0\n\n\nIsrael\n21.0\n6.4\n2.5\n3.7\n85.0\n16.03\n67.0\n2.3\n22104.0\n52933.0\n...\n14.48\n68.0\n68.0\n15.8\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n35343.336\n2015.0\n\n\nNew Zealand\n11.0\n2.2\n10.3\n0.2\n74.0\n13.87\n73.0\n1.2\n23815.0\n28290.0\n...\n14.87\n77.0\n89.0\n18.1\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n37044.891\n2015.0\n\n\nFrance\n12.0\n5.0\n3.5\n0.5\n73.0\n8.15\n64.0\n0.6\n28799.0\n48741.0\n...\n15.33\n80.0\n82.0\n16.4\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n37675.006\n2015.0\n\n\nBelgium\n21.0\n6.6\n4.5\n2.0\n72.0\n4.57\n62.0\n1.1\n28307.0\n83876.0\n...\n15.71\n89.0\n87.0\n18.9\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n40106.632\n2014.0\n\n\nGermany\n16.0\n3.6\n4.5\n0.1\n86.0\n5.25\n73.0\n0.5\n31252.0\n50394.0\n...\n15.31\n72.0\n95.0\n18.2\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n40996.511\n2014.0\n\n\nFinland\n15.0\n2.4\n9.0\n0.6\n85.0\n3.58\n69.0\n1.4\n27927.0\n18761.0\n...\n14.89\n69.0\n94.0\n19.7\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n41973.988\n2014.0\n\n\nCanada\n15.0\n1.3\n10.5\n0.2\n89.0\n3.94\n72.0\n1.5\n29365.0\n67913.0\n...\n14.25\n61.0\n91.0\n17.2\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n43331.961\n2015.0\n\n\nNetherlands\n30.0\n4.9\n6.1\n0.0\n73.0\n0.45\n74.0\n0.9\n27888.0\n77961.0\n...\n15.44\n75.0\n92.0\n18.7\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n43603.115\n2014.0\n\n\nAustria\n27.0\n3.4\n7.1\n1.0\n83.0\n7.61\n72.0\n0.4\n31173.0\n49887.0\n...\n14.46\n75.0\n94.0\n17.0\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n43724.031\n2015.0\n\n\nUnited Kingdom\n13.0\n1.9\n11.5\n0.2\n78.0\n12.70\n71.0\n0.3\n27029.0\n60778.0\n...\n14.83\n66.0\n88.0\n16.4\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n43770.688\n2015.0\n\n\nSweden\n10.0\n5.1\n10.9\n0.0\n88.0\n1.13\n74.0\n0.7\n29185.0\n60328.0\n...\n15.11\n86.0\n95.0\n19.3\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n49866.266\n2014.0\n\n\nIceland\n18.0\n2.7\n5.1\n0.4\n71.0\n12.25\n82.0\n0.3\n23965.0\n43045.0\n...\n14.61\n81.0\n97.0\n19.8\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n50854.583\n2014.0\n\n\nAustralia\n13.0\n2.1\n10.5\n1.1\n76.0\n14.02\n72.0\n0.8\n31588.0\n47657.0\n...\n14.41\n93.0\n91.0\n19.4\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n50961.865\n2014.0\n\n\nIreland\n13.0\n2.6\n9.0\n0.2\n75.0\n4.20\n60.0\n0.8\n23917.0\n31580.0\n...\n15.19\n70.0\n80.0\n17.6\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n51350.744\n2014.0\n\n\nDenmark\n15.0\n3.9\n7.0\n0.9\n78.0\n2.03\n73.0\n0.3\n26491.0\n44488.0\n...\n16.06\n88.0\n94.0\n19.4\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n52114.165\n2015.0\n\n\nUnited States\n18.0\n1.5\n8.3\n0.1\n89.0\n11.30\n67.0\n5.2\n41355.0\n145769.0\n...\n14.27\n68.0\n85.0\n17.2\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n55805.204\n2015.0\n\n\nNorway\n16.0\n3.3\n8.1\n0.3\n82.0\n2.82\n75.0\n0.6\n33492.0\n8797.0\n...\n15.56\n78.0\n94.0\n17.9\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n74822.106\n2015.0\n\n\nSwitzerland\n20.0\n4.2\n8.4\n0.0\n86.0\n6.72\n80.0\n0.5\n33491.0\n108823.0\n...\n14.98\n49.0\n96.0\n17.3\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n80675.308\n2015.0\n\n\nLuxembourg\n12.0\n4.3\n6.0\n0.1\n78.0\n3.47\n66.0\n0.4\n38951.0\n61765.0\n...\n15.12\n91.0\n86.0\n15.1\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n101994.093\n2014.0\n\n\n\n\n36 rows × 30 columns\n\n\n\n\nfull_country_stats[[\"GDP per capita\", 'Life satisfaction']].loc[\"United States\"]\n\nGDP per capita       55805.204\nLife satisfaction        7.200\nName: United States, dtype: float64\n\n\n\nremove_indices = [0, 1, 6, 8, 33, 34, 35]\nkeep_indices = list(set(range(36)) - set(remove_indices))\n\nsample_data = full_country_stats[[\"GDP per capita\", 'Life satisfaction']].iloc[keep_indices]\nmissing_data = full_country_stats[[\"GDP per capita\", 'Life satisfaction']].iloc[remove_indices]\n\n\nsample_data.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction', figsize=(5,3))\nplt.axis([0, 60000, 0, 10])\nposition_text = {\n    \"Hungary\": (5000, 1),\n    \"Korea\": (18000, 1.7),\n    \"France\": (29000, 2.4),\n    \"Australia\": (40000, 3.0),\n    \"United States\": (52000, 3.8),\n}\nfor country, pos_text in position_text.items():\n    pos_data_x, pos_data_y = sample_data.loc[country]\n    country = \"U.S.\" if country == \"United States\" else country\n    plt.annotate(country, xy=(pos_data_x, pos_data_y), xytext=pos_text,\n            arrowprops=dict(facecolor='black', width=0.5, shrink=0.1, headwidth=5))\n    plt.plot(pos_data_x, pos_data_y, \"ro\")\nplt.xlabel(\"GDP per capita (USD)\")\nsave_fig('money_happy_scatterplot')\nplt.show()\n\nSaving figure money_happy_scatterplot\n\n\n\n\n\n\nsample_data.to_csv(os.path.join(\"datasets\", \"lifesat\", \"lifesat.csv\"))\n\n\nsample_data.loc[list(position_text.keys())]\n\n\n\n\n\n\n\n\nGDP per capita\nLife satisfaction\n\n\nCountry\n\n\n\n\n\n\nHungary\n12239.894\n4.9\n\n\nKorea\n27195.197\n5.8\n\n\nFrance\n37675.006\n6.5\n\n\nAustralia\n50961.865\n7.3\n\n\nUnited States\n55805.204\n7.2\n\n\n\n\n\n\n\n\nimport numpy as np\n\nsample_data.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction', figsize=(5,3))\nplt.xlabel(\"GDP per capita (USD)\")\nplt.axis([0, 60000, 0, 10])\nX=np.linspace(0, 60000, 1000)\nplt.plot(X, 2*X/100000, \"r\")\nplt.text(40000, 2.7, r\"$\\theta_0 = 0$\", fontsize=14, color=\"r\")\nplt.text(40000, 1.8, r\"$\\theta_1 = 2 \\times 10^{-5}$\", fontsize=14, color=\"r\")\nplt.plot(X, 8 - 5*X/100000, \"g\")\nplt.text(5000, 9.1, r\"$\\theta_0 = 8$\", fontsize=14, color=\"g\")\nplt.text(5000, 8.2, r\"$\\theta_1 = -5 \\times 10^{-5}$\", fontsize=14, color=\"g\")\nplt.plot(X, 4 + 5*X/100000, \"b\")\nplt.text(5000, 3.5, r\"$\\theta_0 = 4$\", fontsize=14, color=\"b\")\nplt.text(5000, 2.6, r\"$\\theta_1 = 5 \\times 10^{-5}$\", fontsize=14, color=\"b\")\nsave_fig('tweaking_model_params_plot')\nplt.show()\n\nSaving figure tweaking_model_params_plot\n\n\n\n\n\n\nfrom sklearn import linear_model\nlin1 = linear_model.LinearRegression()\nXsample = np.c_[sample_data[\"GDP per capita\"]]\nysample = np.c_[sample_data[\"Life satisfaction\"]]\nlin1.fit(Xsample, ysample)\nt0, t1 = lin1.intercept_[0], lin1.coef_[0][0]\nt0, t1\n\n(4.853052800266436, 4.911544589158484e-05)\n\n\n\nsample_data.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction', figsize=(5,3))\nplt.xlabel(\"GDP per capita (USD)\")\nplt.axis([0, 60000, 0, 10])\nX=np.linspace(0, 60000, 1000)\nplt.plot(X, t0 + t1*X, \"b\")\nplt.text(5000, 3.1, r\"$\\theta_0 = 4.85$\", fontsize=14, color=\"b\")\nplt.text(5000, 2.2, r\"$\\theta_1 = 4.91 \\times 10^{-5}$\", fontsize=14, color=\"b\")\nsave_fig('best_fit_model_plot')\nplt.show()\n\nSaving figure best_fit_model_plot\n\n\n\n\n\n\ncyprus_gdp_per_capita = gdp_per_capita.loc[\"Cyprus\"][\"GDP per capita\"]\nprint(cyprus_gdp_per_capita)\ncyprus_predicted_life_satisfaction = lin1.predict([[cyprus_gdp_per_capita]])[0][0]\ncyprus_predicted_life_satisfaction\n\n22587.49\n\n\n5.96244744318815\n\n\n\nsample_data.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction', figsize=(5,3), s=1)\nplt.xlabel(\"GDP per capita (USD)\")\nX=np.linspace(0, 60000, 1000)\nplt.plot(X, t0 + t1*X, \"b\")\nplt.axis([0, 60000, 0, 10])\nplt.text(5000, 7.5, r\"$\\theta_0 = 4.85$\", fontsize=14, color=\"b\")\nplt.text(5000, 6.6, r\"$\\theta_1 = 4.91 \\times 10^{-5}$\", fontsize=14, color=\"b\")\nplt.plot([cyprus_gdp_per_capita, cyprus_gdp_per_capita], [0, cyprus_predicted_life_satisfaction], \"r--\")\nplt.text(25000, 5.0, r\"Prediction = 5.96\", fontsize=14, color=\"b\")\nplt.plot(cyprus_gdp_per_capita, cyprus_predicted_life_satisfaction, \"ro\")\nsave_fig('cyprus_prediction_plot')\nplt.show()\n\nSaving figure cyprus_prediction_plot\n\n\n\n\n\n\nsample_data[7:10]\n\n\n\n\n\n\n\n\nGDP per capita\nLife satisfaction\n\n\nCountry\n\n\n\n\n\n\nPortugal\n19121.592\n5.1\n\n\nSlovenia\n20732.482\n5.7\n\n\nSpain\n25864.721\n6.5\n\n\n\n\n\n\n\n\n(5.1+5.7+6.5)/3\n\n5.766666666666667\n\n\n\nbackup = oecd_bli, gdp_per_capita\n\ndef prepare_country_stats(oecd_bli, gdp_per_capita):\n    oecd_bli = oecd_bli[oecd_bli[\"INEQUALITY\"]==\"TOT\"]\n    oecd_bli = oecd_bli.pivot(index=\"Country\", columns=\"Indicator\", values=\"Value\")\n    gdp_per_capita.rename(columns={\"2015\": \"GDP per capita\"}, inplace=True)\n    gdp_per_capita.set_index(\"Country\", inplace=True)\n    full_country_stats = pd.merge(left=oecd_bli, right=gdp_per_capita,\n                                  left_index=True, right_index=True)\n    full_country_stats.sort_values(by=\"GDP per capita\", inplace=True)\n    remove_indices = [0, 1, 6, 8, 33, 34, 35]\n    keep_indices = list(set(range(36)) - set(remove_indices))\n    return full_country_stats[[\"GDP per capita\", 'Life satisfaction']].iloc[keep_indices]\n\n\n# Code example\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport sklearn.linear_model\n\n# Load the data\noecd_bli = pd.read_csv(datapath + \"oecd_bli_2015.csv\", thousands=',')\ngdp_per_capita = pd.read_csv(datapath + \"gdp_per_capita.csv\",thousands=',',delimiter='\\t',\n                             encoding='latin1', na_values=\"n/a\")\n\n# Prepare the data\ncountry_stats = prepare_country_stats(oecd_bli, gdp_per_capita)\nX = np.c_[country_stats[\"GDP per capita\"]]\ny = np.c_[country_stats[\"Life satisfaction\"]]\n\n# Visualize the data\ncountry_stats.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction')\nplt.show()\n\n# Select a linear model\nmodel = sklearn.linear_model.LinearRegression()\n\n# Train the model\nmodel.fit(X, y)\n\n# Make a prediction for Cyprus\nX_new = [[22587]]  # Cyprus' GDP per capita\nprint(model.predict(X_new)) # outputs [[ 5.96242338]]\n\n\n\n\n[[5.96242338]]\n\n\n\noecd_bli, gdp_per_capita = backup\n\n\nmissing_data\n\n\n\n\n\n\n\n\nGDP per capita\nLife satisfaction\n\n\nCountry\n\n\n\n\n\n\nBrazil\n8669.998\n7.0\n\n\nMexico\n9009.280\n6.7\n\n\nChile\n13340.905\n6.7\n\n\nCzech Republic\n17256.918\n6.5\n\n\nNorway\n74822.106\n7.4\n\n\nSwitzerland\n80675.308\n7.5\n\n\nLuxembourg\n101994.093\n6.9\n\n\n\n\n\n\n\n\nposition_text2 = {\n    \"Brazil\": (1000, 9.0),\n    \"Mexico\": (11000, 9.0),\n    \"Chile\": (25000, 9.0),\n    \"Czech Republic\": (35000, 9.0),\n    \"Norway\": (60000, 3),\n    \"Switzerland\": (72000, 3.0),\n    \"Luxembourg\": (90000, 3.0),\n}\n\n\nsample_data.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction', figsize=(8,3))\nplt.axis([0, 110000, 0, 10])\n\nfor country, pos_text in position_text2.items():\n    pos_data_x, pos_data_y = missing_data.loc[country]\n    plt.annotate(country, xy=(pos_data_x, pos_data_y), xytext=pos_text,\n            arrowprops=dict(facecolor='black', width=0.5, shrink=0.1, headwidth=5))\n    plt.plot(pos_data_x, pos_data_y, \"rs\")\n\nX=np.linspace(0, 110000, 1000)\nplt.plot(X, t0 + t1*X, \"b:\")\n\nlin_reg_full = linear_model.LinearRegression()\nXfull = np.c_[full_country_stats[\"GDP per capita\"]]\nyfull = np.c_[full_country_stats[\"Life satisfaction\"]]\nlin_reg_full.fit(Xfull, yfull)\n\nt0full, t1full = lin_reg_full.intercept_[0], lin_reg_full.coef_[0][0]\nX = np.linspace(0, 110000, 1000)\nplt.plot(X, t0full + t1full * X, \"k\")\nplt.xlabel(\"GDP per capita (USD)\")\n\nsave_fig('representative_training_data_scatterplot')\nplt.show()\n\nSaving figure representative_training_data_scatterplot\n\n\n\n\n\n\nfull_country_stats.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction', figsize=(8,3))\nplt.axis([0, 110000, 0, 10])\n\nfrom sklearn import preprocessing\nfrom sklearn import pipeline\n\npoly = preprocessing.PolynomialFeatures(degree=30, include_bias=False)\nscaler = preprocessing.StandardScaler()\nlin_reg2 = linear_model.LinearRegression()\n\npipeline_reg = pipeline.Pipeline([('poly', poly), ('scal', scaler), ('lin', lin_reg2)])\npipeline_reg.fit(Xfull, yfull)\ncurve = pipeline_reg.predict(X[:, np.newaxis])\nplt.plot(X, curve)\nplt.xlabel(\"GDP per capita (USD)\")\nsave_fig('overfitting_model_plot')\nplt.show()\n\nSaving figure overfitting_model_plot\n\n\n\n\n\n\nfull_country_stats.loc[[c for c in full_country_stats.index if \"W\" in c.upper()]][\"Life satisfaction\"]\n\nCountry\nNew Zealand    7.3\nSweden         7.2\nNorway         7.4\nSwitzerland    7.5\nName: Life satisfaction, dtype: float64\n\n\n\ngdp_per_capita.loc[[c for c in gdp_per_capita.index if \"W\" in c.upper()]].head()\n\n\n\n\n\n\n\n\nSubject Descriptor\nUnits\nScale\nCountry/Series-specific Notes\nGDP per capita\nEstimates Start After\n\n\nCountry\n\n\n\n\n\n\n\n\n\n\nBotswana\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n6040.957\n2008.0\n\n\nKuwait\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n29363.027\n2014.0\n\n\nMalawi\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n354.275\n2011.0\n\n\nNew Zealand\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n37044.891\n2015.0\n\n\nNorway\nGross domestic product per capita, current prices\nU.S. dollars\nUnits\nSee notes for: Gross domestic product, curren...\n74822.106\n2015.0\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8,3))\n\nplt.xlabel(\"GDP per capita\")\nplt.ylabel('Life satisfaction')\n\nplt.plot(list(sample_data[\"GDP per capita\"]), list(sample_data[\"Life satisfaction\"]), \"bo\")\nplt.plot(list(missing_data[\"GDP per capita\"]), list(missing_data[\"Life satisfaction\"]), \"rs\")\n\nX = np.linspace(0, 110000, 1000)\nplt.plot(X, t0full + t1full * X, \"r--\", label=\"Linear model on all data\")\nplt.plot(X, t0 + t1*X, \"b:\", label=\"Linear model on partial data\")\n\nridge = linear_model.Ridge(alpha=10**9.5)\nXsample = np.c_[sample_data[\"GDP per capita\"]]\nysample = np.c_[sample_data[\"Life satisfaction\"]]\nridge.fit(Xsample, ysample)\nt0ridge, t1ridge = ridge.intercept_[0], ridge.coef_[0][0]\nplt.plot(X, t0ridge + t1ridge * X, \"b\", label=\"Regularized linear model on partial data\")\n\nplt.legend(loc=\"lower right\")\nplt.axis([0, 110000, 0, 10])\nplt.xlabel(\"GDP per capita (USD)\")\nsave_fig('ridge_model_plot')\nplt.show()\n\nSaving figure ridge_model_plot"
  },
  {
    "objectID": "Data_Visualization.html",
    "href": "Data_Visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nggplot2 Tutorial\n\n\n\n\n\n\n\nVisualization\n\n\nCode\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMar 8, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Data_Mining.html",
    "href": "Data_Mining.html",
    "title": "Data Mining",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nCOVID-19 Analysis & Visualization\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\n\n\n\n\n\n\n\n\n\nJun 7, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\nManipulating Geospatial Data\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nFolium\n\n\nGeopandas\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\nProximity Analysis\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nFolium\n\n\nGeopandas\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\nCoordinate Refrence Systems\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nGeopandas\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\nInteractive Maps\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nGeopandas\n\n\nFolium\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\nYour First Map\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nGeopandas\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\nNYC taxi\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nScipy\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\nVisualization_with_seaborn_and_matplotlib\n\n\n\n\n\n\n\nPython\n\n\nSeaborn\n\n\nMatplotlib\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\nPython 기초\n\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nMar 18, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\nNumpy 기본\n\n\n\n\n\n\n\nPython\n\n\nNumpy\n\n\n\n\n\n\n\n\n\n\n\nMar 4, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Data_Mining/2022-05-26-exercise-proximity-analysis.html",
    "href": "Data_Mining/2022-05-26-exercise-proximity-analysis.html",
    "title": "Proximity Analysis",
    "section": "",
    "text": "Proximity Analysis\n\nThis notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link.\n\n\nimport math\nimport geopandas as gpd\nimport pandas as pd\nfrom shapely.geometry import MultiPolygon\n\nimport folium\nfrom folium import Choropleth, Marker\nfrom folium.plugins import HeatMap, MarkerCluster\n\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.geospatial.ex5 import *\n\nYou’ll use the embed_map() function to visualize your maps.\n\ndef embed_map(m, file_name):\n    from IPython.display import IFrame\n    m.save(file_name)\n    return IFrame(file_name, width='100%', height='500px')\n\n\nExercises\n\n1) Visualize the collision data.\nRun the code cell below to load a GeoDataFrame collisions tracking major motor vehicle collisions in 2013-2018.\n\ncollisions = gpd.read_file(\"../input/geospatial-learn-course-data/NYPD_Motor_Vehicle_Collisions/NYPD_Motor_Vehicle_Collisions/NYPD_Motor_Vehicle_Collisions.shp\")\ncollisions.head()\n\nUse the “LATITUDE” and “LONGITUDE” columns to create an interactive map to visualize the collision data. What type of map do you think is most effective?\n“LATITUDE” 및 “LONGITUDE” 열을 사용하여 충돌 데이터를 시각화하는 대화형 맵을 만듭니다. 어떤 유형의 지도가 가장 효과적이라고 생각하십니까?\n\nm_1 = folium.Map(location=[40.7, -74], zoom_start=11) \n\n# Your code here: Visualize the collision data\nHeatMap(data=collisions[['LATITUDE', 'LONGITUDE']], radius=9).add_to(m_1)\n\n# Uncomment to see a hint\n#q_1.hint()\n\n# Show the map\nembed_map(m_1, \"q_1.html\")\n\n\n# Get credit for your work after you have created a map\n#q_1.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_1.solution()\n\n\n\n2) Understand hospital coverage.\nRun the next code cell to load the hospital data.\n\nhospitals = gpd.read_file(\"../input/geospatial-learn-course-data/nyu_2451_34494/nyu_2451_34494/nyu_2451_34494.shp\")\nhospitals.head()\n\nUse the “latitude” and “longitude” columns to visualize the hospital locations.\n“위도” 및 “경도” 열을 사용하여 병원 위치를 시각화합니다.\n\nm_2 = folium.Map(location=[40.7, -74], zoom_start=11) \n\n# Your code here: Visualize the hospital locations\nfor idx, row in hospitals.iterrows():\n    Marker([row['latitude'], row['longitude']], popup=row['name']).add_to(m_2)\n    \n# Uncomment to see a hint\n#q_2.hint()\n        \n# Show the map\nembed_map(m_2, \"q_2.html\")\n\n\n# Get credit for your work after you have created a map\nq_2.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_2.solution()\n\n\n\n3) When was the closest hospital more than 10 kilometers away?\nCreate a DataFrame outside_range containing all rows from collisions with crashes that occurred more than 10 kilometers from the closest hospital.\n가장 가까운 병원에서 10km 이상 떨어진 곳에서 발생한 충돌이 있는 ’충돌’의 모든 행을 포함하는 DataFrame ’outside_range’를 만듭니다.\nNote that both hospitals and collisions have EPSG 2263 as the coordinate reference system, and EPSG 2263 has units of meters.\n‘병원’과 ’충돌’ 모두 좌표 참조 시스템으로 EPSG 2263을 사용하고 EPSG 2263은 미터 단위를 사용합니다.\n\n# Your code here\ncoverage = gpd.GeoDataFrame(geometry=hospitals.geometry).buffer(10000)\nmy_union = coverage.geometry.unary_union\noutside_range = collisions.loc[~collisions[\"geometry\"].apply(lambda x: my_union.contains(x))]\n\n# Check your answer\nq_3.check()\n\n\n# Lines below will give you a hint or solution code\n#q_3.hint()\n#q_3.solution()\n\nThe next code cell calculates the percentage of collisions that occurred more than 10 kilometers away from the closest hospital.\n\npercentage = round(100*len(outside_range)/len(collisions), 2)\nprint(\"Percentage of collisions more than 10 km away from the closest hospital: {}%\".format(percentage))\n\n\n\n4) Make a recommender.\nWhen collisions occur in distant locations, it becomes even more vital that injured persons are transported to the nearest available hospital.\n멀리 떨어진 곳에서 충돌이 발생하면 부상자를 가장 가까운 병원으로 이송하는 것이 더욱 중요해집니다.\nWith this in mind, you decide to create a recommender that: - takes the location of the crash (in EPSG 2263) as input, - finds the closest hospital (where distance calculations are done in EPSG 2263), and - returns the name of the closest hospital.\n\ndef best_hospital(collision_location):\n    idx_min = hospitals.geometry.distance(collision_location).idxmin()\n    my_hospital = hospitals.iloc[idx_min]\n    # Your code here\n    name = my_hospital[\"name\"]\n    return name\n\n# Test your function: this should suggest CALVARY HOSPITAL INC\nprint(best_hospital(outside_range.geometry.iloc[0]))\n\n# Check your answer\nq_4.check()\n\n\n# Lines below will give you a hint or solution code\n#q_4.hint()\n#q_4.solution()\n\n\n\n5) Which hospital is under the highest demand?\nConsidering only collisions in the outside_range DataFrame, which hospital is most recommended?\noutside_range DataFrame에서 충돌만 고려한다면 어느 병원을 가장 추천하는가?\nYour answer should be a Python string that exactly matches the name of the hospital returned by the function you created in 4).\n\n# Your code here\nhighest_demand = outside_range.geometry.apply(best_hospital).value_counts().idxmax()\n\n# Check your answer\nq_5.check()\n\n\n# Lines below will give you a hint or solution code\n#q_5.hint()\n#q_5.solution()\n\n\n\n6) Where should the city construct new hospitals?\nRun the next code cell (without changes) to visualize hospital locations, in addition to collisions that occurred more than 10 kilometers away from the closest hospital.\n다음 코드 셀(변경 없이)을 실행하여 가장 가까운 병원에서 10km 이상 떨어진 곳에서 발생한 충돌 외에도 병원 위치를 시각화합니다.\n\nm_6 = folium.Map(location=[40.7, -74], zoom_start=11) \n\ncoverage = gpd.GeoDataFrame(geometry=hospitals.geometry).buffer(10000)\nfolium.GeoJson(coverage.geometry.to_crs(epsg=4326)).add_to(m_6)\nHeatMap(data=outside_range[['LATITUDE', 'LONGITUDE']], radius=9).add_to(m_6)\nfolium.LatLngPopup().add_to(m_6)\n\nembed_map(m_6, 'm_6.html')\n\nClick anywhere on the map to see a pop-up with the corresponding location in latitude and longitude.\nThe city of New York reaches out to you for help with deciding locations for two brand new hospitals. They specifically want your help with identifying locations to bring the calculated percentage from step 3) to less than ten percent. Using the map (and without worrying about zoning laws or what potential buildings would have to be removed in order to build the hospitals), can you identify two locations that would help the city accomplish this goal?\nPut the proposed latitude and longitude for hospital 1 in lat_1 and long_1, respectively. (Likewise for hospital 2.)\n병원 1에 대해 제안된 위도와 경도를 각각 lat_1과 long_1에 넣습니다. (병원2도 마찬가지)\nThen, run the rest of the cell as-is to see the effect of the new hospitals. Your answer will be marked correct, if the two new hospitals bring the percentage to less than ten percent.\n그런 다음 나머지 셀을 그대로 실행하여 새 병원의 효과를 확인하십시오. 두 개의 새로운 병원에서 백분율을 10% 미만으로 낮추면 답이 정답으로 표시됩니다.\n\n# Your answer here: proposed location of hospital 1\nlat_1 = 40.6714\nlong_1 = -73.8492\n\n# Your answer here: proposed location of hospital 2\nlat_2 = 40.6702\nlong_2 = -73.7612\n\n# Do not modify the code below this line\ntry:\n    new_df = pd.DataFrame(\n        {'Latitude': [lat_1, lat_2],\n         'Longitude': [long_1, long_2]})\n    new_gdf = gpd.GeoDataFrame(new_df, geometry=gpd.points_from_xy(new_df.Longitude, new_df.Latitude))\n    new_gdf.crs = {'init' :'epsg:4326'}\n    new_gdf = new_gdf.to_crs(epsg=2263)\n    # get new percentage\n    new_coverage = gpd.GeoDataFrame(geometry=new_gdf.geometry).buffer(10000)\n    new_my_union = new_coverage.geometry.unary_union\n    new_outside_range = outside_range.loc[~outside_range[\"geometry\"].apply(lambda x: new_my_union.contains(x))]\n    new_percentage = round(100*len(new_outside_range)/len(collisions), 2)\n    print(\"(NEW) Percentage of collisions more than 10 km away from the closest hospital: {}%\".format(new_percentage))\n    # Did you help the city to meet its goal?\n    q_6.check()\n    # make the map\n    m = folium.Map(location=[40.7, -74], zoom_start=11) \n    folium.GeoJson(coverage.geometry.to_crs(epsg=4326)).add_to(m)\n    folium.GeoJson(new_coverage.geometry.to_crs(epsg=4326)).add_to(m)\n    for idx, row in new_gdf.iterrows():\n        Marker([row['Latitude'], row['Longitude']]).add_to(m)\n    HeatMap(data=new_outside_range[['LATITUDE', 'LONGITUDE']], radius=9).add_to(m)\n    folium.LatLngPopup().add_to(m)\n    display(embed_map(m, 'q_6.html'))\nexcept:\n    q_6.hint()\n\n\n# Uncomment to see one potential answer \n#q_6.solution()\n\n\n\n\nCongratulations!\nYou have just completed the Geospatial Analysis micro-course! Great job!\n\nHave questions or comments? Visit the course discussion forum to chat with other learners."
  },
  {
    "objectID": "Data_Mining/2022-05-25-newyorktaxi.html",
    "href": "Data_Mining/2022-05-25-newyorktaxi.html",
    "title": "NYC taxi",
    "section": "",
    "text": "NYC taxi"
  },
  {
    "objectID": "Data_Mining/2022-05-25-newyorktaxi.html#visual-analytics-with-python",
    "href": "Data_Mining/2022-05-25-newyorktaxi.html#visual-analytics-with-python",
    "title": "NYC taxi",
    "section": "Visual Analytics with Python",
    "text": "Visual Analytics with Python\n강의자료 출처 - kaggle.com\nIn this script we will explore the spatial and temporal behavior of the people of New York as can be inferred by examining their cab usage.\nThe main fields of this dataset are taxi pickup time and location, as well as dropoff location and trip duration. There is a total of around 1.4 Million trips in the dataset that took place during the first half of 2016.\nWe will study how the patterns of cab usage change throughout the year, throughout the week and throughout the day, and we will focus on difference between weekdays and weekends.\n\n%matplotlib inline\n\nfrom sklearn import decomposition\nfrom scipy import stats\nfrom sklearn import cluster\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nmatplotlib.style.use('fivethirtyeight')\nmatplotlib.rcParams['font.size'] = 12\nmatplotlib.rcParams['figure.figsize'] = (10,10)\n\n\nLoad data and preprocess measurements to sensible units\n\ndata_frame = pd.read_csv('train.csv')\n\n\ndata_frame.describe()\n\n\n\n\n\n\n\n\nvendor_id\npassenger_count\npickup_longitude\npickup_latitude\ndropoff_longitude\ndropoff_latitude\ntrip_duration\n\n\n\n\ncount\n1.458644e+06\n1.458644e+06\n1.458644e+06\n1.458644e+06\n1.458644e+06\n1.458644e+06\n1.458644e+06\n\n\nmean\n1.534950e+00\n1.664530e+00\n-7.397349e+01\n4.075092e+01\n-7.397342e+01\n4.075180e+01\n9.594923e+02\n\n\nstd\n4.987772e-01\n1.314242e+00\n7.090186e-02\n3.288119e-02\n7.064327e-02\n3.589056e-02\n5.237432e+03\n\n\nmin\n1.000000e+00\n0.000000e+00\n-1.219333e+02\n3.435970e+01\n-1.219333e+02\n3.218114e+01\n1.000000e+00\n\n\n25%\n1.000000e+00\n1.000000e+00\n-7.399187e+01\n4.073735e+01\n-7.399133e+01\n4.073588e+01\n3.970000e+02\n\n\n50%\n2.000000e+00\n1.000000e+00\n-7.398174e+01\n4.075410e+01\n-7.397975e+01\n4.075452e+01\n6.620000e+02\n\n\n75%\n2.000000e+00\n2.000000e+00\n-7.396733e+01\n4.076836e+01\n-7.396301e+01\n4.076981e+01\n1.075000e+03\n\n\nmax\n2.000000e+00\n9.000000e+00\n-6.133553e+01\n5.188108e+01\n-6.133553e+01\n4.392103e+01\n3.526282e+06\n\n\n\n\n\n\n\n\ndata_frame.shape\n\n(1458644, 11)\n\n\n\nnp.max(data_frame['trip_duration']) #이렇게 보면서 이상치가 있는 것을 눈으로 봄 3526282(전처리전) --&gt; 4764(전처리후)\n\n3526282\n\n\n\n# remove obvious outliers / 위경도 좌표가 튀는 것을 제거 \nallLat  = np.array(list(data_frame['pickup_latitude'])  + list(data_frame['dropoff_latitude'])) #두개의 컬럼을 리스트로 합쳐줌 \nallLong = np.array(list(data_frame['pickup_longitude']) + list(data_frame['dropoff_longitude']))\n\nlongLimits = [np.percentile(allLong, 0.3), np.percentile(allLong, 99.7)]\nlatLimits  = [np.percentile(allLat , 0.3), np.percentile(allLat , 99.7)]\ndurLimits  = [np.percentile(data_frame['trip_duration'], 0.4), np.percentile(data_frame['trip_duration'], 99.7)]\n\ndata_frame = data_frame[(data_frame['pickup_latitude']   &gt;= latLimits[0] ) & (data_frame['pickup_latitude']   &lt;= latLimits[1]) ] #이상치 제거\ndata_frame = data_frame[(data_frame['dropoff_latitude']  &gt;= latLimits[0] ) & (data_frame['dropoff_latitude']  &lt;= latLimits[1]) ]\ndata_frame = data_frame[(data_frame['pickup_longitude']  &gt;= longLimits[0]) & (data_frame['pickup_longitude']  &lt;= longLimits[1])]\ndata_frame = data_frame[(data_frame['dropoff_longitude'] &gt;= longLimits[0]) & (data_frame['dropoff_longitude'] &lt;= longLimits[1])]\ndata_frame = data_frame[(data_frame['trip_duration']     &gt;= durLimits[0] ) & (data_frame['trip_duration']     &lt;= durLimits[1]) ]\ndata_frame = data_frame.reset_index(drop=True)\n\n\n# convert fields to sensible units\nmedianLat  = np.percentile(allLat,50)\nmedianLong = np.percentile(allLong,50)\n\nlatMultiplier  = 111.32\nlongMultiplier = np.cos(medianLat*(np.pi/180.0)) * 111.32\n\ndata_frame['duration [min]'] = data_frame['trip_duration']/60.0\ndata_frame['src lat [km]']   = latMultiplier  * (data_frame['pickup_latitude']   - medianLat)\ndata_frame['src long [km]']  = longMultiplier * (data_frame['pickup_longitude']  - medianLong)\ndata_frame['dst lat [km]']   = latMultiplier  * (data_frame['dropoff_latitude']  - medianLat)\ndata_frame['dst long [km]']  = longMultiplier * (data_frame['dropoff_longitude'] - medianLong)\n\nallLat  = np.array(list(data_frame['src lat [km]'])  + list(data_frame['dst lat [km]']))\nallLong = np.array(list(data_frame['src long [km]']) + list(data_frame['dst long [km]']))\n\n\n\nPlot the histograms of trip duration, latitude and longitude\n\nfig, axArray = plt.subplots(nrows=1,ncols=3,figsize=(13,4))\naxArray[0].hist(data_frame['duration [min]'],80);\naxArray[0].set_xlabel('trip duration [min]'); axArray[0].set_ylabel('counts')\naxArray[1].hist(allLat ,80); axArray[1].set_xlabel('latitude [km]')\naxArray[2].hist(allLong,80); axArray[2].set_xlabel('longitude [km]')\n#위경도는 큰의미는 없지만 플랏으로 이상치가 있는지 시각적으로 확인 \n\nText(0.5, 0, 'longitude [km]')\n\n\n\n\n\n\n\nPlot the trip Duration vs. the Aerial Distance between pickup and dropoff\n\ndata_frame['log duration']       = np.log1p(data_frame['duration [min]'])\ndata_frame['euclidian distance'] = np.sqrt((data_frame['src lat [km]']  - data_frame['dst lat [km]'] )**2 +\n                                       (data_frame['src long [km]'] - data_frame['dst long [km]'])**2)\n\nfig, axArray = plt.subplots(nrows=1,ncols=2,figsize=(13,6))\naxArray[0].scatter(data_frame['euclidian distance'], data_frame['duration [min]'],c='b',s=5,alpha=0.01);\naxArray[0].set_xlabel('Aerial Euclidian Distance [km]'); axArray[0].set_ylabel('Duration [min]')\naxArray[0].set_xlim(data_frame['euclidian distance'].min(),data_frame['euclidian distance'].max())\naxArray[0].set_ylim(data_frame['duration [min]'].min(),data_frame['duration [min]'].max())\naxArray[0].set_title('trip Duration vs Aerial trip Distance')\n\naxArray[1].scatter(data_frame['euclidian distance'], data_frame['log duration'],c='b',s=5,alpha=0.01);\naxArray[1].set_xlabel('Aerial Euclidian Distance [km]'); axArray[1].set_ylabel('log(1+Duration) [log(min)]')\naxArray[1].set_xlim(data_frame['euclidian distance'].min(),data_frame['euclidian distance'].max())\naxArray[1].set_ylim(data_frame['log duration'].min(),data_frame['log duration'].max())\naxArray[1].set_title('log of trip Duration vs Aerial trip Distance')\n\nText(0.5, 1.0, 'log of trip Duration vs Aerial trip Distance')\n\n\n\n\n\n\n\nExercise 1\n위의 Scatter plot은 Point가 너무 많이 존재하여 좋은 시각화가 아닐 수 있습니다. 보다 효율적인 시각화 방안을 제시해 보세요.\nmatplotlib, seaborn, plotly 등 다양한 패키지를 활용할 수 있습니다\n\nimport seaborn as sns\n\nsns.jointplot(x=data_frame['euclidian distance'], \n              y=data_frame['duration [min]'], \n              kind=\"hex\", \n              color=\"#4CB391\",\n              xlim = (0,30),\n              ylim = (0,70))\n\n\n\n\n\np = sns.jointplot(x=np.log1p(data_frame['euclidian distance']),\n              y=data_frame['log duration'], \n              kind=\"hex\", \n              color=\"#4CB391\") #log를 취해 주어서 히트맵을 변환시켜 보여줌 \n\np.ax_joint.set_xlabel('log(1+Aerial Euclidian Distance) [log(km)]')\np.ax_joint.set_ylabel('log(1+Duration) [log(min)]')\n\nText(8.060000000000002, 0.5, 'log(1+Duration) [log(min)]')\n\n\n\n\n\nWe can see that the trip distance defines the lower bound on trip duration, as one would expect.\n\n\nPlot spatial density plot of the pickup and dropoff locations\n\nimageSize = (700,700)\nlongRange = [-5,19]\nlatRange = [-13,11]\n\nallLatInds  = imageSize[0] - (imageSize[0] * (allLat  - latRange[0])  / (latRange[1]  - latRange[0]) ).astype(int)\nallLongInds =                (imageSize[1] * (allLong - longRange[0]) / (longRange[1] - longRange[0])).astype(int)\n\nlocationDensityImage = np.zeros(imageSize)\nfor latInd, longInd in zip(allLatInds,allLongInds):\n    locationDensityImage[latInd,longInd] += 1\n\nfig, ax = plt.subplots(nrows=1,ncols=1,figsize=(12,12))\nax.imshow(np.log(locationDensityImage+1),cmap='inferno')\nax.set_axis_off()\n\n\n\n\n\n\nExercise 2\nfolium이나 pydeck을 사용하여 택시의 승차지점, 하차지점을 시각화 해봅시다. 디서 승객이 많이 타고 내리는지를 확인할 수 있습니까?\n\ndata_frame.head()\n\n\n\n\n\n\n\n\nid\nvendor_id\npickup_datetime\ndropoff_datetime\npassenger_count\npickup_longitude\npickup_latitude\ndropoff_longitude\ndropoff_latitude\nstore_and_fwd_flag\ntrip_duration\nduration [min]\nsrc lat [km]\nsrc long [km]\ndst lat [km]\ndst long [km]\nlog duration\neuclidian distance\n\n\n\n\n0\nid2875421\n2\n2016-03-14 17:24:55\n2016-03-14 17:32:30\n1\n-73.982155\n40.767937\n-73.964630\n40.765602\nN\n455\n7.583333\n1.516008\n-0.110015\n1.256121\n1.367786\n2.149822\n1.500479\n\n\n1\nid2377394\n1\n2016-06-12 00:43:35\n2016-06-12 00:54:38\n1\n-73.980415\n40.738564\n-73.999481\n40.731152\nN\n663\n11.050000\n-1.753813\n0.036672\n-2.578912\n-1.571088\n2.489065\n1.807119\n\n\n2\nid3858529\n2\n2016-01-19 11:35:24\n2016-01-19 12:10:48\n1\n-73.979027\n40.763939\n-74.005333\n40.710087\nN\n2124\n35.400000\n1.070973\n0.153763\n-4.923841\n-2.064547\n3.594569\n6.392080\n\n\n3\nid3504673\n2\n2016-04-06 19:32:31\n2016-04-06 19:39:40\n1\n-74.010040\n40.719971\n-74.012268\n40.706718\nN\n429\n7.150000\n-3.823568\n-2.461500\n-5.298809\n-2.649362\n2.098018\n1.487155\n\n\n4\nid2181028\n2\n2016-03-26 13:30:55\n2016-03-26 13:38:10\n1\n-73.973053\n40.793209\n-73.972923\n40.782520\nN\n435\n7.250000\n4.329328\n0.657515\n3.139453\n0.668452\n2.110213\n1.189925\n\n\n\n\n\n\n\n\ndata_frame.columns\n\nIndex(['id', 'vendor_id', 'pickup_datetime', 'dropoff_datetime',\n       'passenger_count', 'pickup_longitude', 'pickup_latitude',\n       'dropoff_longitude', 'dropoff_latitude', 'store_and_fwd_flag',\n       'trip_duration', 'duration [min]', 'src lat [km]', 'src long [km]',\n       'dst lat [km]', 'dst long [km]', 'log duration', 'euclidian distance'],\n      dtype='object')\n\n\n\nimport pydeck as pdk\n\n\n# Define a layer to display on a map\nlayer = pdk.Layer(\n    \"HexagonLayer\",\n    data_frame[:1000],\n    get_position=[\"pickup_longitude\", \"pickup_latitude\"],\n    auto_highlight=True,\n    elevation_scale=50,\n    pickable=True,\n    elevation_range=[0, 50],\n    extruded=True,\n    coverage=1,\n)\n\n# Set the viewport location\nview_lon = np.mean(data_frame[\"pickup_longitude\"])\nview_lat = np.mean(data_frame[\"pickup_latitude\"])\n\nview_state = pdk.ViewState(\n    longitude=view_lon, latitude=view_lat, zoom=10, min_zoom=5, max_zoom=15, pitch=40.5, bearing=-27.36,\n)\n\n# Render\nr = pdk.Deck(layers=[layer], initial_view_state=view_state)\nr.show()\n#r.to_html(\"hexagon_ny_taxi.html\")\n\n\n\n\n\nlayer = pdk.Layer(\n    \"HeatmapLayer\",\n    data_frame[:1000],\n    get_position=[\"pickup_longitude\", \"pickup_latitude\"],\n    auto_highlight=True,\n    elevation_scale=50,\n    pickable=True,\n    elevation_range=[0, 50],\n    extruded=True,\n    coverage=1,\n)\n\n# Set the viewport location\nview_lon = np.mean(data_frame[\"pickup_longitude\"])\nview_lat = np.mean(data_frame[\"pickup_latitude\"])\n\nview_state = pdk.ViewState(\n    longitude=view_lon, latitude=view_lat, zoom=10, min_zoom=5, max_zoom=15, pitch=40.5, bearing=-27.36,\n)\n\n# Render\nr = pdk.Deck(layers=[layer], initial_view_state=view_state)\nr.show()\n\n\n\n\n\n#folium 을 이용해서도 예제로 해보기 \n\n\n\nClosing in on Manhattan\n\nimageSizeMan = (720,480)\nlatRangeMan = [-8,10]\nlongRangeMan = [-5,7]\n\nindToKeep  = np.logical_and(allLat &gt; latRangeMan[0], allLat &lt; latRangeMan[1])\nindToKeep  = np.logical_and(indToKeep, np.logical_and(allLong &gt; longRangeMan[0], allLong &lt; longRangeMan[1]))\nallLatMan  = allLat[indToKeep]\nallLongMan = allLong[indToKeep]\n\nallLatIndsMan  = (imageSizeMan[0]-1) - (imageSizeMan[0] * (allLatMan  - latRangeMan[0])\n                                                        / (latRangeMan[1] - latRangeMan[0])).astype(int)\nallLongIndsMan =                       (imageSizeMan[1] * (allLongMan - longRangeMan[0])\n                                                        / (longRangeMan[1] - longRangeMan[0])).astype(int)\n\nlocationDensityImageMan = np.zeros(imageSizeMan)\nfor latInd, longInd in zip(allLatIndsMan,allLongIndsMan):\n    locationDensityImageMan[latInd,longInd] += 1\n\nfig, ax = plt.subplots(nrows=1,ncols=1,figsize=(12,18))\nax.imshow(np.log(locationDensityImageMan+1),cmap='inferno')\nax.set_axis_off()\n\n\n\n\n\n\nCluster the Trips and Look at their distribution\n\npickupTime = pd.to_datetime(data_frame['pickup_datetime'])\n\ndata_frame['src hourOfDay'] = (pickupTime.dt.hour*60.0 + pickupTime.dt.minute)   / 60.0\ndata_frame['dst hourOfDay'] = data_frame['src hourOfDay'] + data_frame['duration [min]'] / 60.0\n\ndata_frame['dayOfWeek']     = pickupTime.dt.weekday\ndata_frame['hourOfWeek']    = data_frame['dayOfWeek']*24.0 + data_frame['src hourOfDay']\n\ndata_frame['monthOfYear']   = pickupTime.dt.month\ndata_frame['dayOfYear']     = pickupTime.dt.dayofyear\ndata_frame['weekOfYear']    = pickupTime.dt.weekofyear\ndata_frame['hourOfYear']    = data_frame['dayOfYear']*24.0 + data_frame['src hourOfDay']\n\nFutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n  data_frame['weekOfYear']    = pickupTime.dt.weekofyear\n\n\n\ntripAttributes = np.array(data_frame.loc[:,['src lat [km]','src long [km]','dst lat [km]','dst long [km]','duration [min]']])\nmeanTripAttr = tripAttributes.mean(axis=0)\nstdTripAttr  = tripAttributes.std(axis=0)\ntripAttributes = stats.zscore(tripAttributes, axis=0)\n\nnumClusters = 40\nTripKmeansModel = cluster.MiniBatchKMeans(n_clusters=numClusters, batch_size=120000, n_init=100, random_state=1)\nclusterInds = TripKmeansModel.fit_predict(tripAttributes)\n\nclusterTotalCounts, _ = np.histogram(clusterInds, bins=numClusters)\nsortedClusterInds = np.flipud(np.argsort(clusterTotalCounts))\n\nplt.figure(figsize=(12,4)); plt.title('Cluster Histogram of all trip')\nplt.bar(range(1,numClusters+1),clusterTotalCounts[sortedClusterInds])\nplt.ylabel('Frequency [counts]'); plt.xlabel('Cluster index (sorted by cluster frequency)')\nplt.xlim(0,numClusters+1)\n\n(0.0, 41.0)\n\n\n\n\n\n\n\nPlot typical Trips on the Map\n\ndef ConvertToImageCoords(latCoord, longCoord, latRange, longRange, imageSize):\n    latInds  = imageSize[0] - (imageSize[0] * (latCoord  - latRange[0])  / (latRange[1]  - latRange[0]) ).astype(int)\n    longInds =                (imageSize[1] * (longCoord - longRange[0]) / (longRange[1] - longRange[0])).astype(int)\n\n    return latInds, longInds\n\ntemplateTrips = TripKmeansModel.cluster_centers_ * np.tile(stdTripAttr,(numClusters,1)) + np.tile(meanTripAttr,(numClusters,1))\n\nsrcCoords = templateTrips[:,:2]\ndstCoords = templateTrips[:,2:4]\n\nsrcImCoords = ConvertToImageCoords(srcCoords[:,0],srcCoords[:,1], latRange, longRange, imageSize)\ndstImCoords = ConvertToImageCoords(dstCoords[:,0],dstCoords[:,1], latRange, longRange, imageSize)\n\nplt.figure(figsize=(12,12))\nplt.imshow(np.log(locationDensityImage+1),cmap='inferno'); plt.grid('off')\nplt.scatter(srcImCoords[1],srcImCoords[0],c='m',s=200,alpha=0.8)\nplt.scatter(dstImCoords[1],dstImCoords[0],c='g',s=200,alpha=0.8)\n\nfor i in range(len(srcImCoords[0])):\n    plt.arrow(srcImCoords[1][i],srcImCoords[0][i], dstImCoords[1][i]-srcImCoords[1][i], dstImCoords[0][i]-srcImCoords[0][i],\n              edgecolor='c', facecolor='c', width=0.8,alpha=0.4,head_width=10.0,head_length=10.0,length\n\nSyntaxError: unexpected EOF while parsing (&lt;ipython-input-22-39197d099a2f&gt;, line 22)\n\n\n\n\nCalculate the trip distribution for different hours of the weekday\n\nhoursOfDay = np.sort(data_frame['src hourOfDay'].astype(int).unique())\nclusterDistributionHourOfDay_weekday = np.zeros((len(hoursOfDay),numClusters))\nfor k, hour in enumerate(hoursOfDay):\n    slectedInds = (data_frame['src hourOfDay'].astype(int) == hour) & (data_frame['dayOfWeek'] &lt;= 4)\n    currDistribution, _ = np.histogram(clusterInds[slectedInds], bins=numClusters)\n    clusterDistributionHourOfDay_weekday[k,:] = currDistribution[sortedClusterInds]\n\nfig, ax = plt.subplots(nrows=1,ncols=1,figsize=(12,6))\nax.set_title('Trip Distribution during Weekdays', fontsize=12)\nax.imshow(clusterDistributionHourOfDay_weekday); ax.grid('off')\nax.set_xlabel('Trip Cluster'); ax.set_ylabel('Hour of Day')\n\n\n\nCalculate the trip distribution for different hours of the weekend\n\nhoursOfDay = np.sort(data_frame['src hourOfDay'].astype(int).unique())\nclusterDistributionHourOfDay_weekend = np.zeros((len(hoursOfDay),numClusters))\nfor k, hour in enumerate(hoursOfDay):\n    slectedInds = (data_frame['src hourOfDay'].astype(int) == hour) & (data_frame['dayOfWeek'] &gt;= 5)\n    currDistribution, _ = np.histogram(clusterInds[slectedInds], bins=numClusters)\n    clusterDistributionHourOfDay_weekend[k,:] = currDistribution[sortedClusterInds]\n\nfig, ax = plt.subplots(nrows=1,ncols=1,figsize=(12,6))\nax.set_title('Trip Distribution during Weekends', fontsize=12)\nax.imshow(clusterDistributionHourOfDay_weekend); ax.grid('off')\nax.set_xlabel('Trip Cluster'); ax.set_ylabel('Hour of Day')\n\n\n\nCalculate the trip distribution for day of week\n\ndaysOfWeek = np.sort(data_frame['dayOfWeek'].unique())\nclusterDistributionDayOfWeek = np.zeros((len(daysOfWeek),numClusters))\nfor k, day in enumerate(daysOfWeek):\n    slectedInds = data_frame['dayOfWeek'] == day\n    currDistribution, _ = np.histogram(clusterInds[slectedInds], bins=numClusters)\n    clusterDistributionDayOfWeek[k,:] = currDistribution[sortedClusterInds]\n\nplt.figure(figsize=(12,5)); plt.title('Trip Distribution throughout the Week')\nplt.imshow(clusterDistributionDayOfWeek); plt.grid('off')\nplt.xlabel('Trip Cluster'); plt.ylabel('Day of Week')\n\n\n\nCalculate the trip distribution for day of year\n\ndaysOfYear = data_frame['dayOfYear'].unique()\ndaysOfYear = np.sort(daysOfYear)\nclusterDistributionDayOfYear = np.zeros((len(daysOfYear),numClusters))\nfor k, day in enumerate(daysOfYear):\n    slectedInds = data_frame['dayOfYear'] == day\n    currDistribution, _ = np.histogram(clusterInds[slectedInds], bins=numClusters)\n    clusterDistributionDayOfYear[k,:] = currDistribution[sortedClusterInds]\n\nfig, ax = plt.subplots(nrows=1,ncols=1,figsize=(10,16))\nax.set_title('Trip Distribution throughout the Year', fontsize=12)\nax.imshow(clusterDistributionDayOfYear); ax.grid('off')\nax.set_xlabel('Trip Cluster'); ax.set_ylabel('Day of Year')\nax.annotate('Large Snowstorm', color='r', fontsize=15 ,xy=(5, 21), xytext=(20, 17),\n            arrowprops=dict(facecolor='red', shrink=0.03))\nax.annotate('Memorial Day', color='r', fontsize=15, xy=(5, 151), xytext=(20, 157),\n            arrowprops=dict(facecolor='red', shrink=0.03))\n\n\n\nComputing PCA coefficients\n\nhoursOfYear = np.sort(data_frame['hourOfYear'].astype(int).unique())\nclusterDistributionHourOfYear = np.zeros((len(range(hoursOfYear[0],hoursOfYear[-1])),numClusters))\ndayOfYearVec  = np.zeros(clusterDistributionHourOfYear.shape[0])\nweekdayVec    = np.zeros(clusterDistributionHourOfYear.shape[0])\nweekOfYearVec = np.zeros(clusterDistributionHourOfYear.shape[0])\nfor k, hour in enumerate(hoursOfYear):\n    slectedInds = data_frame['hourOfYear'].astype(int) == hour\n    currDistribution, _ = np.histogram(clusterInds[slectedInds], bins=numClusters)\n    clusterDistributionHourOfYear[k,:] = currDistribution[sortedClusterInds]\n\n    dayOfYearVec[k]  = data_frame[slectedInds]['dayOfYear'].mean()\n    weekdayVec[k]    = data_frame[slectedInds]['dayOfWeek'].mean()\n    weekOfYearVec[k] = data_frame[slectedInds]['weekOfYear'].mean()\n\nnumComponents = 3\nTripDistributionPCAModel = decomposition.PCA(n_components=numComponents,whiten=True, random_state=1)\ncompactClusterDistributionHourOfYear = TripDistributionPCAModel.fit_transform(clusterDistributionHourOfYear)\n\n\n\nCollect traces for all weeks of year\n\nlistOfFullWeeks = []\nfor uniqueVal in np.unique(weekOfYearVec):\n    if (weekOfYearVec == uniqueVal).sum() == 24*7:\n        listOfFullWeeks.append(uniqueVal)\n\nweeklyTraces = np.zeros((24*7,numComponents,len(listOfFullWeeks)))\nfor k, weekInd in enumerate(listOfFullWeeks):\n    weeklyTraces[:,:,k] = compactClusterDistributionHourOfYear[weekOfYearVec == weekInd,:]\n\nfig, axArray = plt.subplots(nrows=numComponents,ncols=1,sharex=True, figsize=(10,10))\nfig.suptitle('PCA coefficients during the Week', fontsize=25)\nfor PC_coeff in range(numComponents):\n    meanTrace = weeklyTraces[:,PC_coeff,:].mean(axis=1)\n    axArray[PC_coeff].plot(weeklyTraces[:,PC_coeff,:],'red',linewidth=1.5)\n    axArray[PC_coeff].plot(meanTrace,'k',linewidth=2.5)\n    axArray[PC_coeff].set_ylabel('PC %d coeff' %(PC_coeff+1))\n    axArray[PC_coeff].vlines([0,23,47,71,95,119,143,167], weeklyTraces[:,PC_coeff,:].min(), weeklyTraces[:,PC_coeff,:].max(), colors='black', lw=2)\n\naxArray[PC_coeff].set_xlabel('hours since start of week')\naxArray[PC_coeff].set_xlim(-0.9,24*7-0.1)\n\n\n\nExamine what different PC coefficients mean by looking at their trip template distributions\n\nfig, axArray = plt.subplots(nrows=numComponents,ncols=1,sharex=True, figsize=(12,11))\nfig.suptitle('Trip Distribution PCA Components', fontsize=25)\nfor PC_coeff in range(numComponents):\n    tripTemplateDistributionDifference = TripDistributionPCAModel.components_[PC_coeff,:] * \\\n                                         TripDistributionPCAModel.explained_variance_[PC_coeff]\n    axArray[PC_coeff].bar(range(1,numClusters+1),tripTemplateDistributionDifference)\n    axArray[PC_coeff].set_title('PCA %d component' %(PC_coeff+1))\n    axArray[PC_coeff].set_ylabel('delta frequency [counts]')\n    \naxArray[PC_coeff].set_xlabel('cluster index (sorted by cluster frequency)')\naxArray[PC_coeff].set_xlim(0,numClusters+0.5)\n\naxArray[1].hlines([-25,25], 0, numClusters+0.5, colors='r', lw=0.7)\naxArray[2].hlines([-11,11], 0, numClusters+0.5, colors='r', lw=0.7)\n\nWe can see that the first PCA component looks very similar to the overall trip distribution, suggesting that it’s mainly a “gain” component that controls just the number of total trips in that period of time."
  },
  {
    "objectID": "Data_Mining/2022-05-24-exercise-interactive-maps.html",
    "href": "Data_Mining/2022-05-24-exercise-interactive-maps.html",
    "title": "Interactive Maps",
    "section": "",
    "text": "Interactive Maps\n\nThis notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link.\n\n\nimport pandas as pd\nimport geopandas as gpd\n\nimport folium\nfrom folium import Choropleth\nfrom folium.plugins import HeatMap\n\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.geospatial.ex3 import *\n\nWe define a function embed_map() for displaying interactive maps. It accepts two arguments: the variable containing the map, and the name of the HTML file where the map will be saved.\nThis function ensures that the maps are visible in all web browsers.\n\ndef embed_map(m, file_name):\n    from IPython.display import IFrame\n    m.save(file_name)\n    return IFrame(file_name, width='100%', height='500px')\n\n\nExercises\n\n1) Do earthquakes coincide with plate boundaries?\nRun the code cell below to create a DataFrame plate_boundaries that shows global plate boundaries. The “coordinates” column is a list of (latitude, longitude) locations along the boundaries.\n아래 코드 셀을 실행하여 전역 플레이트 경계를 표시하는 DataFrame plate_boundaries를 만듭니다. “좌표” 열은 경계를 따라 (위도, 경도) 위치의 목록입니다.\n\nplate_boundaries = gpd.read_file(\"../input/geospatial-learn-course-data/Plate_Boundaries/Plate_Boundaries/Plate_Boundaries.shp\")\nplate_boundaries['coordinates'] = plate_boundaries.apply(lambda x: [(b,a) for (a,b) in list(x.geometry.coords)], axis='columns')\nplate_boundaries.drop('geometry', axis=1, inplace=True)\n\nplate_boundaries.head()\n\nNext, run the code cell below without changes to load the historical earthquake data into a DataFrame earthquakes.\n\n# Load the data and print the first 5 rows\nearthquakes = pd.read_csv(\"../input/geospatial-learn-course-data/earthquakes1970-2014.csv\", parse_dates=[\"DateTime\"])\nearthquakes.head()\n\nThe code cell below visualizes the plate boundaries on a map. Use all of the earthquake data to add a heatmap to the same map, to determine whether earthquakes coincide with plate boundaries.\n아래 코드 셀은 지도에서 판 경계를 시각화합니다. 모든 지진 데이터를 사용하여 동일한 지도에 히트맵을 추가하여 지진이 판 경계와 일치하는지 확인합니다.\n\n# Create a base map with plate boundaries\nm_1 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)\nfor i in range(len(plate_boundaries)):\n    folium.PolyLine(locations=plate_boundaries.coordinates.iloc[i], weight=2, color='black').add_to(m_1)\n\n# Your code here: Add a heatmap to the map\nHeatMap(data=earthquakes[['Latitude', 'Longitude']], radius=10).add_to(m_1)\n\n# Uncomment to see a hint\n#q_1.a.hint()\n\n# Show the map\nembed_map(m_1, 'q_1.html')\n\n\n# Get credit for your work after you have created a map\nq_1.a.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_1.a.solution()\n\nSo, given the map above, do earthquakes coincide with plate boundaries?\n\n# View the solution (Run this code cell to receive credit!)\nq_1.b.solution()\n\n\n\n2) Is there a relationship between earthquake depth and proximity to a plate boundary in Japan?\nYou recently read that the depth of earthquakes tells us important information about the structure of the earth. You’re interested to see if there are any intereresting global patterns, and you’d also like to understand how depth varies in Japan.\n최근에 지진의 깊이가 중요 정보를 알려준다는 내용을 읽었습니다. -news_science_products) 지구의 구조에 대해. 흥미로운 글로벌 패턴이 있는지 확인하고 싶고 일본의 깊이가 어떻게 다른지 이해하고 싶습니다.\n\n# Create a base map with plate boundaries\nm_2 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)\nfor i in range(len(plate_boundaries)):\n    folium.PolyLine(locations=plate_boundaries.coordinates.iloc[i], weight=2, color='black').add_to(m_2)\n    \n# Your code here: Add a map to visualize earthquake depth\ndef color_producer(val):\n    if val &lt; 50:\n        return 'forestgreen'\n    else:\n        return 'darkred'\n\n\nfor i in range(0,len(earthquakes)):\n    folium.Circle(\n        location=[earthquakes.iloc[i]['Latitude'], earthquakes.iloc[i]['Longitude']],\n        radius=2000,\n        color=color_producer(earthquakes.iloc[i]['Depth'])).add_to(m_2)\n    \n# Uncomment to see a hint\nq_2.a.hint()\n\n# View the map\nembed_map(m_2, 'q_2.html')\n\n\n# Get credit for your work after you have created a map\nq_2.a.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_2.a.solution()\n\nCan you detect a relationship between proximity to a plate boundary and earthquake depth? Does this pattern hold globally? In Japan?\n\n# View the solution (Run this code cell to receive credit!)\nq_2.b.solution()\n\n\n\n3) Which prefectures have high population density?\nRun the next code cell (without changes) to create a GeoDataFrame prefectures that contains the geographical boundaries of Japanese prefectures.\n다음 코드 셀을 변경 없이 실행하여 일본 현의 지리적 경계를 포함하는 GeoDataFrame ’현’을 만듭니다.\n\n# GeoDataFrame with prefecture boundaries\nprefectures = gpd.read_file(\"../input/geospatial-learn-course-data/japan-prefecture-boundaries/japan-prefecture-boundaries/japan-prefecture-boundaries.shp\")\nprefectures.set_index('prefecture', inplace=True)\nprefectures.head()\n\nThe next code cell creates a DataFrame stats containing the population, area (in square kilometers), and population density (per square kilometer) for each Japanese prefecture. Run the code cell without changes.\n다음 코드 셀은 각 일본 현에 대한 인구, 면적(제곱 킬로미터 단위) 및 인구 밀도(제곱 킬로미터당)를 포함하는 DataFrame ’통계’를 생성합니다. 변경 없이 코드 셀을 실행합니다.\n\n# DataFrame containing population of each prefecture\npopulation = pd.read_csv(\"../input/geospatial-learn-course-data/japan-prefecture-population.csv\")\npopulation.set_index('prefecture', inplace=True)\n\n# Calculate area (in square kilometers) of each prefecture\narea_sqkm = pd.Series(prefectures.geometry.to_crs(epsg=32654).area / 10**6, name='area_sqkm')\nstats = population.join(area_sqkm)\n\n# Add density (per square kilometer) of each prefecture\nstats['density'] = stats[\"population\"] / stats[\"area_sqkm\"]\nstats.head()\n\nUse the next code cell to create a choropleth map to visualize population density.\n다음 코드 셀을 사용하여 등치 지도를 만들어 인구 밀도를 시각화합니다.\n\n# Create a base map\nm_3 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)\n\n# Your code here: create a choropleth map to visualize population density\nChoropleth(geo_data=prefectures['geometry'].__geo_interface__, \n           data=stats['density'], \n           key_on=\"feature.id\", \n           fill_color='YlGnBu', \n           legend_name='population density'\n          ).add_to(m_3)\n\n# Uncomment to see a hint\n#q_3.a.hint()\n\n# View the map\nembed_map(m_3, 'q_3.html')\n\n\n# Get credit for your work after you have created a map\nq_3.a.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_3.a.solution()\n\nWhich three prefectures have relatively higher density than the others? Are they spread throughout the country, or all located in roughly the same geographical region? (If you’re unfamiliar with Japanese geography, you might find this map useful to answer the questions.)\n\n# View the solution (Run this code cell to receive credit!)\nq_3.b.solution()\n\n\n\n4) Which high-density prefecture is prone to high-magnitude earthquakes?\nCreate a map to suggest one prefecture that might benefit from earthquake reinforcement. Your map should visualize both density and earthquake magnitude.\n지진 보강의 혜택을 받을 수 있는 한 현을 제안하는 지도를 만듭니다. 지도는 밀도와 지진 규모를 모두 시각화해야 합니다.\n\n# Create a base map\nm_4 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)\n\n# Your code here: create a map\ndef color_producer(magnitude):\n    if magnitude &gt; 6.5:\n        return 'red'\n    else:\n        return 'green'\n\nChoropleth(\n    geo_data=prefectures['geometry'].__geo_interface__,\n    data=stats['density'],\n    key_on=\"feature.id\",\n    fill_color='BuPu',\n    legend_name='Population density (per square kilometer)').add_to(m_4)\n\nfor i in range(0,len(earthquakes)):\n    folium.Circle(\n        location=[earthquakes.iloc[i]['Latitude'], earthquakes.iloc[i]['Longitude']],\n        popup=(\"{} ({})\").format(\n            earthquakes.iloc[i]['Magnitude'],\n            earthquakes.iloc[i]['DateTime'].year),\n        radius=earthquakes.iloc[i]['Magnitude']**5.5,\n        color=color_producer(earthquakes.iloc[i]['Magnitude'])).add_to(m_4)\n# Uncomment to see a hint\n#q_4.a.hint()\n\n# View the map\nembed_map(m_4, 'q_4.html')\n\n\n# Get credit for your work after you have created a map\nq_4.a.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_4.a.solution()\n\nWhich prefecture do you recommend for extra earthquake reinforcement?\n\n# View the solution (Run this code cell to receive credit!)\nq_4.b.solution()\n\n\n\n\nKeep going\nLearn how to convert names of places to geographic coordinates with geocoding. You’ll also explore special ways to join information from multiple GeoDataFrames.\n\nHave questions or comments? Visit the course discussion forum to chat with other learners."
  },
  {
    "objectID": "Data_Mining/2022-05-07-matplotlib and seaborn.html",
    "href": "Data_Mining/2022-05-07-matplotlib and seaborn.html",
    "title": "Visualization_with_seaborn_and_matplotlib",
    "section": "",
    "text": "seaborn과 matplotlib의 시각화\n코드 출처 - 이제현님 블로그"
  },
  {
    "objectID": "Data_Mining/2022-05-07-matplotlib and seaborn.html#visualization-with-seaborn",
    "href": "Data_Mining/2022-05-07-matplotlib and seaborn.html#visualization-with-seaborn",
    "title": "Visualization_with_seaborn_and_matplotlib",
    "section": "Visualization with seaborn",
    "text": "Visualization with seaborn\nseaborn은 python의 시각화 라이브러리인 matplolib를 기반으로 제작된 라이브러리입니다.\n\nimport seaborn as sns\nsns.set()\nsns.set(style=\"darkgrid\")\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nplt.rcParams['figure.figsize']=(5,5) #figure의 사이즈를 설정이 가능하다 \n\n\nLoding dataset\n\n# 사용할 데이터 불러오기 \ndata_BM = pd.read_csv('bigmart_data.csv')\ndata_BM = data_BM.dropna(how=\"any\") #NA값 제거\ndata_BM[\"Visibility_Scaled\"] = data_BM[\"Item_Visibility\"] * 100 #Visibility_Scaled 컬럼의 값들에 100 곱해줌\ndata_BM.head()\n\n\n\n\n\n\n\n\nItem_Identifier\nItem_Weight\nItem_Fat_Content\nItem_Visibility\nItem_Type\nItem_MRP\nOutlet_Identifier\nOutlet_Establishment_Year\nOutlet_Size\nOutlet_Location_Type\nOutlet_Type\nItem_Outlet_Sales\nVisibility_Scaled\n\n\n\n\n0\nFDA15\n9.300\nLow Fat\n0.016047\nDairy\n249.8092\nOUT049\n1999\nMedium\nTier 1\nSupermarket Type1\n3735.1380\n1.604730\n\n\n1\nDRC01\n5.920\nRegular\n0.019278\nSoft Drinks\n48.2692\nOUT018\n2009\nMedium\nTier 3\nSupermarket Type2\n443.4228\n1.927822\n\n\n2\nFDN15\n17.500\nLow Fat\n0.016760\nMeat\n141.6180\nOUT049\n1999\nMedium\nTier 1\nSupermarket Type1\n2097.2700\n1.676007\n\n\n4\nNCD19\n8.930\nLow Fat\n0.000000\nHousehold\n53.8614\nOUT013\n1987\nHigh\nTier 3\nSupermarket Type1\n994.7052\n0.000000\n\n\n5\nFDP36\n10.395\nRegular\n0.000000\nBaking Goods\n51.4008\nOUT018\n2009\nMedium\nTier 3\nSupermarket Type2\n556.6088\n0.000000\n\n\n\n\n\n\n\n\ndata_BM.describe() #이상치가 있는지 확인\n\n\n\n\n\n\n\n\nItem_Weight\nItem_Visibility\nItem_MRP\nOutlet_Establishment_Year\nItem_Outlet_Sales\nVisibility_Scaled\n\n\n\n\ncount\n4650.000000\n4650.000000\n4650.000000\n4650.000000\n4650.000000\n4650.000000\n\n\nmean\n12.898675\n0.060700\n141.716328\n1999.190538\n2272.037489\n6.070048\n\n\nstd\n4.670973\n0.044607\n62.420534\n7.388800\n1497.964740\n4.460652\n\n\nmin\n4.555000\n0.000000\n31.490000\n1987.000000\n69.243200\n0.000000\n\n\n25%\n8.770000\n0.025968\n94.409400\n1997.000000\n1125.202000\n2.596789\n\n\n50%\n12.650000\n0.049655\n142.979900\n1999.000000\n1939.808300\n4.965549\n\n\n75%\n17.000000\n0.088736\n186.614150\n2004.000000\n3111.616300\n8.873565\n\n\nmax\n21.350000\n0.188323\n266.888400\n2009.000000\n10256.649000\n18.832266"
  },
  {
    "objectID": "Data_Mining/2022-05-07-matplotlib and seaborn.html#creating-basic-plots",
    "href": "Data_Mining/2022-05-07-matplotlib and seaborn.html#creating-basic-plots",
    "title": "Visualization_with_seaborn_and_matplotlib",
    "section": "1. Creating basic plots",
    "text": "1. Creating basic plots\nmatplotlib에서 여러 줄이 필요한 한 줄로 seaborn에서 몇 가지 기본 플롯을 만드는 방법을 살펴보겠습니다.\n\nLine Chart\n\n일부 데이터 세트의 경우 한 변수의 변화를 시간의 함수로 이해하거나 이와 유사한 연속 변수를 이해하고자 할 수 있습니다.\nseaborn에서 이는 lineplot() 함수로 직접 또는 kind=\"line\":을 설정하여 relplot()으로 수행할 수 있습니다.\n\n\nsns.lineplot(x=\"Item_Weight\", y=\"Item_MRP\",data=data_BM[:50]); #처음부터 50번째 까지의 데이터만 사용한다\n\n\n\n\n\n\nBar Chart\n\nSeaborn에서는 barplot 기능을 사용하여 간단하게 막대 차트를 생성할 수 있습니다.\nmatplotlib에서 동일한 결과를 얻으려면 데이터 범주를 현명하게 그룹화하기 위해 추가 코드를 작성해야 했습니다.\n그리고 나서 플롯이 올바르게 나오도록 훨씬 더 많은 코드를 작성해야 했습니다.\n\n\nsns.barplot(x=\"Item_Type\", y=\"Item_MRP\", data=data_BM[:5])\n\n&lt;AxesSubplot:xlabel='Item_Type', ylabel='Item_MRP'&gt;\n\n\n\n\n\n\n\nHistogram\n\ndistplot()을 사용하여 seaborn에서 히스토그램을 만들 수 있습니다. 사용할 수 있는 여러 옵션이 있으며 노트북에서 더 자세히 살펴보겠습니다.\n\n\nsns.distplot(data_BM['Item_MRP'])\n\n&lt;AxesSubplot:xlabel='Item_MRP', ylabel='Density'&gt;\n\n\n\n\n\n\n\nBox plots\n\nSeaborn에서 boxplot을 생성하기 위해 boxplot()을 사용할 수 있습니다.\n\n\nsns.boxplot(data_BM['Item_Outlet_Sales'], orient='vertical') \n\n&lt;AxesSubplot:xlabel='Item_Outlet_Sales'&gt;\n\n\n\n\n\n\n\nViolin plot\n\n바이올린 플롯은 상자 및 수염 플롯과 유사한 역할을 합니다.\n이는 하나(또는 그 이상) 범주형 변수의 여러 수준에 걸친 정량적 데이터의 분포를 보여줌으로써 해당 분포를 비교할 수 있습니다.\n모든 플롯 구성 요소가 실제 데이터 포인트에 해당하는 상자 플롯과 달리 바이올린 플롯은 기본 분포의 커널 밀도 추정을 특징으로 합니다.\nseaborn에서 violinplot()을 사용하여 바이올린 플롯을 만들 수 있습니다.\n\n\nsns.violinplot(data_BM['Item_Outlet_Sales'], orient='vertical', color='skyblue')\n\n&lt;AxesSubplot:xlabel='Item_Outlet_Sales'&gt;\n\n\n\n\n\n\n\nScatter plot\n\n각 포인트는 데이터 세트의 관찰을 나타내는 포인트 클라우드를 사용하여 두 변수의 분포를 나타냅니다.\n이 묘사를 통해 눈은 그들 사이에 의미 있는 관계가 있는지 여부에 대한 상당한 양의 정보를 추론할 수 있습니다.\nrelplot()을 kind=scatter 옵션과 함께 사용하여 seaborn에서 산점도를 그릴 수 있습니다.\n\n\nsns.relplot(x=\"Item_MRP\", y=\"Item_Outlet_Sales\", data=data_BM[:200], kind=\"scatter\"); \n#sns.relplot(x=\"Item_MRP\", y=\"Item_Outlet_Sales\", data=data_BM[:200], kind=\"line\"); #kind의 설정으로 표현 설정을 바꿈\n\n\n\n\n\n\nHue semantic\n세 번째 변수에 따라 점을 색칠하여 플롯에 다른 차원을 추가할 수도 있습니다. Seaborn에서는 이것을 “hue semantic” 사용이라고 합니다.\n\nsns.relplot(x=\"Item_MRP\", y=\"Item_Outlet_Sales\", hue=\"Item_Type\",data=data_BM[:200]); #hue로 item_type에 대한 플롯을 그려줌 \n\n\n\n\n\nsns.lineplot(x=\"Item_Weight\", y=\"Item_MRP\",hue='Outlet_Size',data=data_BM[:150]);\n\n\n\n\n\n\nBubble plot\n\nhue semantic 활용하여 Item_Visibility별로 거품을 색칠함과 동시에 개별 거품의 크기로 사용합니다.\n\n\n# Bubble plot\nsns.relplot(x=\"Item_MRP\", y=\"Item_Outlet_Sales\", data=data_BM[:200], kind=\"scatter\", size=\"Visibility_Scaled\", hue=\"Visibility_Scaled\");\n\n\n\n\n\n\nCategory wise sub plot\n\nSeaborn에서 카테고리를 기반으로 플롯을 만들 수도 있습니다.\n각 Outlet_Size에 대한 산점도를 만들었습니다.\n\n\nsns.relplot(x=\"Item_Weight\", y=\"Item_Visibility\",\n            hue='Outlet_Size',style='Outlet_Size',\n            col='Outlet_Size',data=data_BM[:100]);"
  },
  {
    "objectID": "Data_Mining/2022-05-07-matplotlib and seaborn.html#advance-categorical-plots-in-seaborn",
    "href": "Data_Mining/2022-05-07-matplotlib and seaborn.html#advance-categorical-plots-in-seaborn",
    "title": "Visualization_with_seaborn_and_matplotlib",
    "section": "2. Advance categorical plots in seaborn",
    "text": "2. Advance categorical plots in seaborn\n범주형 변수의 경우 seaborn에 세 가지 다른 패밀리가 있습니다.\ncatplot()에서 데이터의 기본 표현은 산점도를 사용합니다.\n\na. Categorical scatterplots\n\n\nStrip plot\n\n하나의 변수가 범주형인 산점도를 그립니다.\ncatplot()에서 kind=strip을 전달하여 생성할 수 있습니다.\n\n\nsns.catplot(x=\"Outlet_Size\", y=\"Item_Outlet_Sales\", kind='strip',data=data_BM[:250]);\n\n\n\n\n\n\nSwarm plot\n\n이 함수는 stripplot()과 유사하지만 점이 겹치지 않도록 조정됩니다(범주형 축을 따라만).\n이렇게 하면 값 분포를 더 잘 표현할 수 있지만 많은 수의 관측치에 대해서는 잘 확장되지 않습니다. 이러한 스타일의 플롯은 때때로 “beeswarm”이라고 불립니다.\ncatplot()에서 kind=swarm을 전달하여 이를 생성할 수 있습니다.\n\n\nsns.catplot(x=\"Outlet_Size\", y=\"Item_Outlet_Sales\", kind='swarm',data=data_BM[:250]);\n\n\n\n\n\n\nb. Categorical distribution plots\n\n\nBox Plots\n\n상자 그림은 극단값과 함께 분포의 3사분위수 값을 보여줍니다.\n“whiskers”은 하위 및 상위 사분위수의 1.5 IQR 내에 있는 점으로 확장되고 이 범위를 벗어나는 관찰은 독립적으로 표시됩니다.\n즉, 상자 그림의 각 값은 데이터의 실제 관측값에 해당합니다.\n\n\nsns.catplot(x=\"Outlet_Size\", y=\"Item_Outlet_Sales\",kind=\"box\",data=data_BM);\n\n\n\n\n\n\nViolin Plots\n\nsns.catplot(x=\"Outlet_Size\", y=\"Item_Outlet_Sales\",kind=\"violin\",data=data_BM);\n\n\n\n\n\n\nPoint Plot\n\nsns.catplot(x=\"Outlet_Size\", y=\"Item_Outlet_Sales\",kind=\"point\",data=data_BM); #y축은 연속형 플랏 x축은 범주형 \n\n\n\n\n\n\nBar plots\n\nsns.catplot(x=\"Outlet_Size\", y=\"Item_Outlet_Sales\",kind=\"bar\",data=data_BM);"
  },
  {
    "objectID": "Data_Mining/2022-05-07-matplotlib and seaborn.html#density-plots",
    "href": "Data_Mining/2022-05-07-matplotlib and seaborn.html#density-plots",
    "title": "Visualization_with_seaborn_and_matplotlib",
    "section": "3. Density Plots",
    "text": "3. Density Plots\n히스토그램 대신 Seaborn이 sn.kdeplot으로 수행하는 커널 밀도 추정을 사용하여 분포의 부드러운 추정치를 얻을 수 있습니다.:\n\n# distribution of Item Visibility\nplt.figure(figsize=(5,5))\nsns.kdeplot(data_BM['Item_Visibility'], shade=True);\n\n\n\n\n\nHistogram and Density Plot\ndistplot을 사용하여 히스토그램과 KDE를 결합할 수 있습니다.:\n\nplt.figure(figsize=(10,10))\nsns.distplot(data_BM['Item_Outlet_Sales']);"
  },
  {
    "objectID": "Data_Mining/2022-05-07-matplotlib and seaborn.html#pair-plots",
    "href": "Data_Mining/2022-05-07-matplotlib and seaborn.html#pair-plots",
    "title": "Visualization_with_seaborn_and_matplotlib",
    "section": "4. Pair plots",
    "text": "4. Pair plots\n\n조인트 플롯을 더 큰 차원의 데이터세트로 일반화하면 쌍 플롯으로 끝납니다. 이것은 모든 값 쌍을 서로에 대해 플롯하려는 경우 다차원 데이터 간의 상관 관계를 탐색하는 데 매우 유용합니다.\n세 가지 붓꽃 종의 꽃잎과 꽃받침 측정값을 나열하는 잘 알려진 Iris 데이터 세트를 사용하여 이것을 시연할 것입니다.\n\n\niris = sns.load_dataset(\"iris\")\nsns.pairplot(iris, hue='species', height=2.5); #둘다 연속형이면 scatter플랏으로 보여줌"
  },
  {
    "objectID": "Data_Mining/2022-05-07-matplotlib and seaborn.html#seaborn-and-matplotlib",
    "href": "Data_Mining/2022-05-07-matplotlib and seaborn.html#seaborn-and-matplotlib",
    "title": "Visualization_with_seaborn_and_matplotlib",
    "section": "Seaborn and Matplotlib",
    "text": "Seaborn and Matplotlib\n\n1.1 Load data\n\n예제로 사용할 펭귄 데이터를 불러옵니다.\nseaborn에 내장되어 있습니다.\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npenguins = sns.load_dataset(\"penguins\")\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n\n\n\n\n\n\n\n1.2 Figure and Axes\n\nmatplotlib으로 도화지figure를 깔고 축공간axes를 만듭니다.\n1 x 2 축공간을 구성합니다.\n\n\nfig, axes = plt.subplots(ncols=2, figsize=(8,4))\n\nfig.tight_layout()\n\n\n\n\n\n\n1.3 plot with matplotlib\n\nmatplotlib 기능을 이용해서 산점도를 그립니다.\n\nx축은 부리 길이 bill length\ny축은 부리 위 아래 두께 bill depth\n색상은 종species로 합니다.\nAdelie, Chinstrap, Gentoo이 있습니다.\n\n두 축공간 중 왼쪽에만 그립니다.\n\n\nfig, axes = plt.subplots(ncols=2,figsize=(8,4))\n\nspecies_u = penguins[\"species\"].unique()\n\nfor i, s in enumerate(species_u):\n    axes[0].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s],\n                    penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s],\n                    c=f\"C{i}\", label=s, alpha=0.3)\n    \naxes[0].legend(species_u, title=\"species\")\naxes[0].set_xlabel(\"Bill Length (mm)\")\naxes[0].set_ylabel(\"Bill Depth (mm)\")\n\n# plt.show()\nfig.tight_layout()\n\n\n\n\n\n\n1.4 Plot with seaborn\n\n단 세 줄로 거의 동일한 그림이 나왔습니다.\n\nscatter plot의 점 크기만 살짝 작습니다.\nlabel의 투명도만 살짝 다릅니다.\n\nseaborn 명령 scatterplot()을 그대로 사용했습니다.\nx축과 y축 label도 바꾸었습니다.\n\nax=axes[1] 인자에서 볼 수 있듯, 존재하는 axes에 그림만 얹었습니다.\nmatplotlib 틀 + seaborn 그림 이므로, matplotlib 명령이 모두 통합니다.\n\n\n\nfig, axes = plt.subplots(ncols=2,figsize=(8,4))\n\nspecies_u = penguins[\"species\"].unique()\n\n# plot 0 : matplotlib\n\nfor i, s in enumerate(species_u):\n    axes[0].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s],\n                    penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s],\n                    c=f\"C{i}\", label=s, alpha=0.3)\n    \naxes[0].legend(species_u, title=\"species\")\naxes[0].set_xlabel(\"Bill Length (mm)\")\naxes[0].set_ylabel(\"Bill Depth (mm)\")\n\n\n# plot 1 : seaborn\nsns.scatterplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", data=penguins, alpha=0.3, ax=axes[1])\naxes[1].set_xlabel(\"Bill Length (mm)\")\naxes[1].set_ylabel(\"Bill Depth (mm)\")\n\nfig.tight_layout()\n\n\n\n\n\n\n1.5 matplotlib + seaborn & seaborn + matplotlib\n\nmatplotlib과 seaborn이 자유롭게 섞일 수 있습니다.\n\nmatplotlib 산점도 위에 seaborn 추세선을 얹을 수 있고,\nseaborn 산점도 위에 matplotlib 중심점을 얹을 수 있습니다.\n\n\n\nfig, axes = plt.subplots(ncols=2, figsize=(8, 4))\n\nspecies_u = penguins[\"species\"].unique()\n\n# plot 0 : matplotlib + seaborn\nfor i, s in enumerate(species_u):\n    # matplotlib 산점도\n    axes[0].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s],\n                   penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s],\n                   c=f\"C{i}\", label=s, alpha=0.3\n                  )\n                  \n    # seaborn 추세선\n    sns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=penguins.loc[penguins[\"species\"]==s], \n                scatter=False, ax=axes[0])\n    \naxes[0].legend(species_u, title=\"species\")\naxes[0].set_xlabel(\"Bill Length (mm)\")\naxes[0].set_ylabel(\"Bill Depth (mm)\")\n\n# plot 1 : seaborn + matplotlib\n# seaborn 산점도\nsns.scatterplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", data=penguins, alpha=0.3, ax=axes[1])\naxes[1].set_xlabel(\"Bill Length (mm)\")\naxes[1].set_ylabel(\"Bill Depth (mm)\")\n\nfor i, s in enumerate(species_u):\n    # matplotlib 중심점\n    axes[1].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s].mean(),\n                   penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s].mean(),\n                   c=f\"C{i}\", alpha=1, marker=\"x\", s=100\n                  )\n\nfig.tight_layout()\n\n\n\n\n\n\n1.6 seaborn + seaborn + matplotlib\n\nseaborn scatterplot + seaborn kdeplot + matplotlib text입니다\n\n\nfig, ax = plt.subplots(figsize=(6,5))\n\n# plot 0: scatter plot\nsns.scatterplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", color=\"k\", data=penguins, alpha=0.3, ax=ax, legend=False)\n\n# plot 1: kde plot\nsns.kdeplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", data=penguins, alpha=0.5, ax=ax, legend=False)\n\n# text:\nspecies_u = penguins[\"species\"].unique()\nfor i, s in enumerate(species_u):\n    ax.text(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s].mean(),\n            penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s].mean(),\n            s = s, fontdict={\"fontsize\":14, \"fontweight\":\"bold\",\"color\":\"k\"}\n            )\n\nax.set_xlabel(\"Bill Length (mm)\")\nax.set_ylabel(\"Bill Depth (mm)\")\n\nfig.tight_layout()"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html",
    "href": "Data_Mining/2022-03-04-numpy.html",
    "title": "Numpy 기본",
    "section": "",
    "text": "numpy 기본 코드 실습\n도구 - 넘파이(NumPy)\n*넘파이(NumPy)는 파이썬의 과학 컴퓨팅을 위한 기본 라이브러리입니다. 넘파이의 핵심은 강력한 N-차원 배열 객체입니다. 또한 선형 대수, 푸리에(Fourier) 변환, 유사 난수 생성과 같은 유용한 함수들도 제공합니다.”"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.zeros",
    "href": "Data_Mining/2022-03-04-numpy.html#np.zeros",
    "title": "Numpy 기본",
    "section": "np.zeros",
    "text": "np.zeros\nzeros 함수는 0으로 채워진 배열을 만듭니다:\n\nnp.zeros(5)\n\narray([0., 0., 0., 0., 0.])\n\n\n2D 배열(즉, 행렬)을 만들려면 원하는 행과 열의 크기를 튜플로 전달합니다. 예를 들어 다음은 \\(3 \\times 4\\) 크기의 행렬입니다:\n\nnp.zeros((3,4))\n\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#용어",
    "href": "Data_Mining/2022-03-04-numpy.html#용어",
    "title": "Numpy 기본",
    "section": "용어",
    "text": "용어\n\n넘파이에서 각 차원을 축(axis) 이라고 합니다\n축의 개수를 랭크(rank) 라고 합니다.\n\n예를 들어, 위의 \\(3 \\times 4\\) 행렬은 랭크 2인 배열입니다(즉 2차원입니다).\n첫 번째 축의 길이는 3이고 두 번째 축의 길이는 4입니다.\n\n배열의 축 길이를 배열의 크기(shape)라고 합니다.\n\n예를 들어, 위 행렬의 크기는 (3, 4)입니다.\n랭크는 크기의 길이와 같습니다.\n\n배열의 사이즈(size)는 전체 원소의 개수입니다. 축의 길이를 모두 곱해서 구할 수 있습니다(가령, \\(3 \\times 4=12\\)).\n\n\na = np.zeros((3,4))\na\n\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])\n\n\n\na.shape\n\n(3, 4)\n\n\n\na.ndim  # len(a.shape)와 같습니다\n\n2\n\n\n\na.size\n\n12"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#n-차원-배열",
    "href": "Data_Mining/2022-03-04-numpy.html#n-차원-배열",
    "title": "Numpy 기본",
    "section": "N-차원 배열",
    "text": "N-차원 배열\n임의의 랭크 수를 가진 N-차원 배열을 만들 수 있습니다. 예를 들어, 다음은 크기가 (2,3,4)인 3D 배열(랭크=3)입니다:\n\nnp.zeros((2,2,5))\n\narray([[[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]],\n\n       [[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#배열-타입",
    "href": "Data_Mining/2022-03-04-numpy.html#배열-타입",
    "title": "Numpy 기본",
    "section": "배열 타입",
    "text": "배열 타입\n넘파이 배열의 타입은 ndarray입니다:\n\ntype(np.zeros((3,4)))\n\nnumpy.ndarray"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.ones",
    "href": "Data_Mining/2022-03-04-numpy.html#np.ones",
    "title": "Numpy 기본",
    "section": "np.ones",
    "text": "np.ones\nndarray를 만들 수 있는 넘파이 함수가 많습니다.\n다음은 1로 채워진 \\(3 \\times 4\\) 크기의 행렬입니다:\n\nnp.ones((3,4))\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.full",
    "href": "Data_Mining/2022-03-04-numpy.html#np.full",
    "title": "Numpy 기본",
    "section": "np.full",
    "text": "np.full\n주어진 값으로 지정된 크기의 배열을 초기화합니다. 다음은 π로 채워진 \\(3 \\times 4\\) 크기의 행렬입니다.\n\nnp.full((3,4), np.pi)\n\narray([[3.14159265, 3.14159265, 3.14159265, 3.14159265],\n       [3.14159265, 3.14159265, 3.14159265, 3.14159265],\n       [3.14159265, 3.14159265, 3.14159265, 3.14159265]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.empty",
    "href": "Data_Mining/2022-03-04-numpy.html#np.empty",
    "title": "Numpy 기본",
    "section": "np.empty",
    "text": "np.empty\n초기화되지 않은 \\(2 \\times 3\\) 크기의 배열을 만듭니다(배열의 내용은 예측이 불가능하며 메모리 상황에 따라 달라집니다):\n\nnp.empty((2,3))\n\narray([[9.6677106e-317, 0.0000000e+000, 0.0000000e+000],\n       [0.0000000e+000, 0.0000000e+000, 0.0000000e+000]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.array",
    "href": "Data_Mining/2022-03-04-numpy.html#np.array",
    "title": "Numpy 기본",
    "section": "np.array",
    "text": "np.array\narray 함수는 파이썬 리스트를 사용하여 ndarray를 초기화합니다:\n\nnp.array([[1,2,3,4], [10, 20, 30, 40]])\n\narray([[ 1,  2,  3,  4],\n       [10, 20, 30, 40]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.arange",
    "href": "Data_Mining/2022-03-04-numpy.html#np.arange",
    "title": "Numpy 기본",
    "section": "np.arange",
    "text": "np.arange\n파이썬의 기본 range 함수와 비슷한 넘파이 arange 함수를 사용하여 ndarray를 만들 수 있습니다:\n\nnp.arange(1, 5)\n\narray([1, 2, 3, 4])\n\n\n부동 소수도 가능합니다:\n\nnp.arange(1.0, 5.0)\n\narray([1., 2., 3., 4.])\n\n\n파이썬의 기본 range 함수처럼 건너 뛰는 정도를 지정할 수 있습니다:\n\nnp.arange(1, 5, 0.5)\n\narray([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5])\n\n\n부동 소수를 사용하면 원소의 개수가 일정하지 않을 수 있습니다. 예를 들면 다음과 같습니다:\n\nprint(np.arange(0, 5/3, 1/3)) # 부동 소수 오차 때문에, 최댓값은 4/3 또는 5/3이 됩니다.\nprint(np.arange(0, 5/3, 0.333333333))\nprint(np.arange(0, 5/3, 0.333333334))\n\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]\n[0.         0.33333333 0.66666667 1.         1.33333334]"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.linspace",
    "href": "Data_Mining/2022-03-04-numpy.html#np.linspace",
    "title": "Numpy 기본",
    "section": "np.linspace",
    "text": "np.linspace\n이런 이유로 부동 소수를 사용할 땐 arange 대신에 linspace 함수를 사용하는 것이 좋습니다. linspace 함수는 지정된 개수만큼 두 값 사이를 나눈 배열을 반환합니다(arange와는 다르게 최댓값이 포함됩니다):\n\nprint(np.linspace(0, 5/3, 6))\n\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.rand와-np.randn",
    "href": "Data_Mining/2022-03-04-numpy.html#np.rand와-np.randn",
    "title": "Numpy 기본",
    "section": "np.rand와 np.randn",
    "text": "np.rand와 np.randn\n넘파이의 random 모듈에는 ndarray를 랜덤한 값으로 초기화할 수 있는 함수들이 많이 있습니다. 예를 들어, 다음은 (균등 분포인) 0과 1사이의 랜덤한 부동 소수로 \\(3 \\times 4\\) 행렬을 초기화합니다:\n\nnp.random.rand(3,4)\n\narray([[0.37892456, 0.17966937, 0.38206837, 0.34922123],\n       [0.80462136, 0.9845914 , 0.9416127 , 0.28305275],\n       [0.21201033, 0.54891417, 0.03781613, 0.4369229 ]])\n\n\n다음은 평균이 0이고 분산이 1인 일변량 정규 분포(가우시안 분포)에서 샘플링한 랜덤한 부동 소수를 담은 \\(3 \\times 4\\) 행렬입니다:\n\nnp.random.randn(3,4)\n\narray([[ 0.83811287, -0.57131751, -0.4381827 ,  1.1485899 ],\n       [ 1.45316084, -0.47259181, -1.23426057, -0.0669813 ],\n       [ 1.01003549,  1.04381736, -0.93060038,  2.39043293]])\n\n\n이 분포의 모양을 알려면 맷플롯립을 사용해 그려보는 것이 좋습니다(더 자세한 것은 맷플롯립 튜토리얼을 참고하세요):\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\n\nplt.hist(np.random.rand(100000), density=True, bins=100, histtype=\"step\", color=\"blue\", label=\"rand\")\nplt.hist(np.random.randn(100000), density=True, bins=100, histtype=\"step\", color=\"red\", label=\"randn\")\nplt.axis([-2.5, 2.5, 0, 1.1])\nplt.legend(loc = \"upper left\")\nplt.title(\"Random distributions\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Density\")\nplt.show()"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.fromfunction",
    "href": "Data_Mining/2022-03-04-numpy.html#np.fromfunction",
    "title": "Numpy 기본",
    "section": "np.fromfunction",
    "text": "np.fromfunction\n함수를 사용하여 ndarray를 초기화할 수도 있습니다:\n\ndef my_function(z, y, x):\n    return x + 10 * y + 100 * z\n\nnp.fromfunction(my_function, (3, 2, 10))\n\narray([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.],\n        [ 10.,  11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.]],\n\n       [[100., 101., 102., 103., 104., 105., 106., 107., 108., 109.],\n        [110., 111., 112., 113., 114., 115., 116., 117., 118., 119.]],\n\n       [[200., 201., 202., 203., 204., 205., 206., 207., 208., 209.],\n        [210., 211., 212., 213., 214., 215., 216., 217., 218., 219.]]])\n\n\n넘파이는 먼저 크기가 (3, 2, 10)인 세 개의 ndarray(차원마다 하나씩)를 만듭니다. 각 배열은 축을 따라 좌표 값과 같은 값을 가집니다. 예를 들어, z 축에 있는 배열의 모든 원소는 z-축의 값과 같습니다:\n[[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n\n [[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]]\n\n [[ 2.  2.  2.  2.  2.  2.  2.  2.  2.  2.]\n  [ 2.  2.  2.  2.  2.  2.  2.  2.  2.  2.]]]\n위의 식 x + 10 * y + 100 * z에서 x, y, z는 사실 ndarray입니다(배열의 산술 연산에 대해서는 아래에서 설명합니다). 중요한 점은 함수 my_function이 원소마다 호출되는 것이 아니고 딱 한 번 호출된다는 점입니다. 그래서 매우 효율적으로 초기화할 수 있습니다."
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#dtype",
    "href": "Data_Mining/2022-03-04-numpy.html#dtype",
    "title": "Numpy 기본",
    "section": "dtype",
    "text": "dtype\n넘파이의 ndarray는 모든 원소가 동일한 타입(보통 숫자)을 가지기 때문에 효율적입니다. dtype 속성으로 쉽게 데이터 타입을 확인할 수 있습니다:\n\nc = np.arange(1, 5)\nprint(c.dtype, c)\n\nint64 [1 2 3 4]\n\n\n\nc = np.arange(1.0, 5.0)\nprint(c.dtype, c)\n\nfloat64 [1. 2. 3. 4.]\n\n\n넘파이가 데이터 타입을 결정하도록 내버려 두는 대신 dtype 매개변수를 사용해서 배열을 만들 때 명시적으로 지정할 수 있습니다:\n\nd = np.arange(1, 5, dtype=np.complex64)\nprint(d.dtype, d)\n\ncomplex64 [1.+0.j 2.+0.j 3.+0.j 4.+0.j]\n\n\n가능한 데이터 타입은 int8, int16, int32, int64, uint8|16|32|64, float16|32|64, complex64|128가 있습니다. 전체 리스트는 온라인 문서를 참고하세요."
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#itemsize",
    "href": "Data_Mining/2022-03-04-numpy.html#itemsize",
    "title": "Numpy 기본",
    "section": "itemsize",
    "text": "itemsize\nitemsize 속성은 각 아이템의 크기(바이트)를 반환합니다:\n\ne = np.arange(1, 5, dtype=np.complex64)\ne.itemsize\n\n8"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#data-버퍼",
    "href": "Data_Mining/2022-03-04-numpy.html#data-버퍼",
    "title": "Numpy 기본",
    "section": "data 버퍼",
    "text": "data 버퍼\n배열의 데이터는 1차원 바이트 버퍼로 메모리에 저장됩니다. data 속성을 사용해 참조할 수 있습니다(사용할 일은 거의 없겠지만요).\n\nf = np.array([[1,2],[1000, 2000]], dtype=np.int32)\nf.data\n\n&lt;memory at 0x7f97929dd790&gt;\n\n\n파이썬 2에서는 f.data가 버퍼이고 파이썬 3에서는 memoryview입니다.\n\nif (hasattr(f.data, \"tobytes\")):\n    data_bytes = f.data.tobytes() # python 3\nelse:\n    data_bytes = memoryview(f.data).tobytes() # python 2\n\ndata_bytes\n\nb'\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\xe8\\x03\\x00\\x00\\xd0\\x07\\x00\\x00'\n\n\n여러 개의 ndarray가 데이터 버퍼를 공유할 수 있습니다. 하나를 수정하면 다른 것도 바뀝니다. 잠시 후에 예를 살펴 보겠습니다."
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#자신을-변경",
    "href": "Data_Mining/2022-03-04-numpy.html#자신을-변경",
    "title": "Numpy 기본",
    "section": "자신을 변경",
    "text": "자신을 변경\nndarray의 shape 속성을 지정하면 간단히 크기를 바꿀 수 있습니다. 배열의 원소 개수는 동일하게 유지됩니다.\n\ng = np.arange(24)\nprint(g)\nprint(\"랭크:\", g.ndim)\n\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n랭크: 1\n\n\n\ng.shape = (6, 4)\nprint(g)\nprint(\"랭크:\", g.ndim)\n\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]\n [12 13 14 15]\n [16 17 18 19]\n [20 21 22 23]]\n랭크: 2\n\n\n\ng.shape = (2, 3, 4)\nprint(g)\nprint(\"랭크:\", g.ndim)\n\n[[[ 0  1  2  3]\n  [ 4  5  6  7]\n  [ 8  9 10 11]]\n\n [[12 13 14 15]\n  [16 17 18 19]\n  [20 21 22 23]]]\n랭크: 3"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#reshape",
    "href": "Data_Mining/2022-03-04-numpy.html#reshape",
    "title": "Numpy 기본",
    "section": "reshape",
    "text": "reshape\nreshape 함수는 동일한 데이터를 가리키는 새로운 ndarray 객체를 반환합니다. 한 배열을 수정하면 다른 것도 함께 바뀝니다.\n\ng2 = g.reshape(4,6)\nprint(g2)\nprint(\"랭크:\", g2.ndim)\n\n[[ 0  1  2  3  4  5]\n [ 6  7  8  9 10 11]\n [12 13 14 15 16 17]\n [18 19 20 21 22 23]]\n랭크: 2\n\n\n행 1, 열 2의 원소를 999로 설정합니다(인덱싱 방식은 아래를 참고하세요).\n\ng2[1, 2] = 999\ng2\n\narray([[  0,   1,   2,   3,   4,   5],\n       [  6,   7, 999,   9,  10,  11],\n       [ 12,  13,  14,  15,  16,  17],\n       [ 18,  19,  20,  21,  22,  23]])\n\n\n이에 상응하는 g의 원소도 수정됩니다.\n\ng\n\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [999,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#ravel",
    "href": "Data_Mining/2022-03-04-numpy.html#ravel",
    "title": "Numpy 기본",
    "section": "ravel",
    "text": "ravel\n마지막으로 ravel 함수는 동일한 데이터를 가리키는 새로운 1차원 ndarray를 반환합니다:\n\ng.ravel()\n\narray([  0,   1,   2,   3,   4,   5,   6,   7, 999,   9,  10,  11,  12,\n        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#규칙-1",
    "href": "Data_Mining/2022-03-04-numpy.html#규칙-1",
    "title": "Numpy 기본",
    "section": "규칙 1",
    "text": "규칙 1\n배열의 랭크가 동일하지 않으면 랭크가 맞을 때까지 랭크가 작은 배열 앞에 1을 추가합니다.\n\nh = np.arange(5).reshape(1, 1, 5)\nh\n\narray([[[0, 1, 2, 3, 4]]])\n\n\n여기에 (1,1,5) 크기의 3D 배열에 (5,) 크기의 1D 배열을 더해 보죠. 브로드캐스팅의 규칙 1이 적용됩니다!\n\nh + [10, 20, 30, 40, 50]  # 다음과 동일합니다: h + [[[10, 20, 30, 40, 50]]]\n\narray([[[10, 21, 32, 43, 54]]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#규칙-2",
    "href": "Data_Mining/2022-03-04-numpy.html#규칙-2",
    "title": "Numpy 기본",
    "section": "규칙 2",
    "text": "규칙 2\n특정 차원이 1인 배열은 그 차원에서 크기가 가장 큰 배열의 크기에 맞춰 동작합니다. 배열의 원소가 차원을 따라 반복됩니다.\n\nk = np.arange(6).reshape(2, 3)\nk\n\narray([[0, 1, 2],\n       [3, 4, 5]])\n\n\n(2,3) 크기의 2D ndarray에 (2,1) 크기의 2D 배열을 더해 보죠. 넘파이는 브로드캐스팅 규칙 2를 적용합니다:\n\nk + [[100], [200]]  # 다음과 같습니다: k + [[100, 100, 100], [200, 200, 200]]\n\narray([[100, 101, 102],\n       [203, 204, 205]])\n\n\n규칙 1과 2를 합치면 다음과 같이 동작합니다:\n\nk + [100, 200, 300]  # 규칙 1 적용: [[100, 200, 300]], 규칙 2 적용: [[100, 200, 300], [100, 200, 300]]\n\narray([[100, 201, 302],\n       [103, 204, 305]])\n\n\n또 매우 간단히 다음 처럼 해도 됩니다:\n\nk + 1000  # 다음과 같습니다: k + [[1000, 1000, 1000], [1000, 1000, 1000]]\n\narray([[1000, 1001, 1002],\n       [1003, 1004, 1005]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#규칙-3",
    "href": "Data_Mining/2022-03-04-numpy.html#규칙-3",
    "title": "Numpy 기본",
    "section": "규칙 3",
    "text": "규칙 3\n규칙 1 & 2을 적용했을 때 모든 배열의 크기가 맞아야 합니다.\n\ntry:\n    k + [33, 44]\nexcept ValueError as e:\n    print(e)\n\noperands could not be broadcast together with shapes (2,3) (2,) \n\n\n브로드캐스팅 규칙은 산술 연산 뿐만 아니라 넘파이 연산에서 많이 사용됩니다. 아래에서 더 보도록 하죠. 브로드캐스팅에 관한 더 자세한 정보는 온라인 문서를 참고하세요."
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#업캐스팅",
    "href": "Data_Mining/2022-03-04-numpy.html#업캐스팅",
    "title": "Numpy 기본",
    "section": "업캐스팅",
    "text": "업캐스팅\ndtype이 다른 배열을 합칠 때 넘파이는 (실제 값에 상관없이) 모든 값을 다룰 수 있는 타입으로 업캐스팅합니다.\n\nk1 = np.arange(0, 5, dtype=np.uint8)\nprint(k1.dtype, k1)\n\nuint8 [0 1 2 3 4]\n\n\n\nk2 = k1 + np.array([5, 6, 7, 8, 9], dtype=np.int8)\nprint(k2.dtype, k2)\n\nint16 [ 5  7  9 11 13]\n\n\n모든 int8과 uint8 값(-128에서 255까지)을 표현하기 위해 int16이 필요합니다. 이 코드에서는 uint8이면 충분하지만 업캐스팅되었습니다.\n\nk3 = k1 + 1.5\nprint(k3.dtype, k3)\n\nfloat64 [1.5 2.5 3.5 4.5 5.5]"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#ndarray-메서드",
    "href": "Data_Mining/2022-03-04-numpy.html#ndarray-메서드",
    "title": "Numpy 기본",
    "section": "ndarray 메서드",
    "text": "ndarray 메서드\n일부 함수는 ndarray 메서드로 제공됩니다. 예를 들면:\n\na = np.array([[-2.5, 3.1, 7], [10, 11, 12]])\nprint(a)\nprint(\"평균 =\", a.mean())\n\n[[-2.5  3.1  7. ]\n [10.  11.  12. ]]\n평균 = 6.766666666666667\n\n\n이 명령은 크기에 상관없이 ndarray에 있는 모든 원소의 평균을 계산합니다.\n다음은 유용한 ndarray 메서드입니다:\n\nfor func in (a.min, a.max, a.sum, a.prod, a.std, a.var):\n    print(func.__name__, \"=\", func())\n\nmin = -2.5\nmax = 12.0\nsum = 40.6\nprod = -71610.0\nstd = 5.084835843520964\nvar = 25.855555555555554\n\n\n이 함수들은 선택적으로 매개변수 axis를 사용합니다. 지정된 축을 따라 원소에 연산을 적용하는데 사용합니다. 예를 들면:\n\nc=np.arange(24).reshape(2,3,4)\nc\n\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])\n\n\n\nc.sum(axis=0)  # 첫 번째 축을 따라 더함, 결과는 3x4 배열\n\narray([[12, 14, 16, 18],\n       [20, 22, 24, 26],\n       [28, 30, 32, 34]])\n\n\n\nc.sum(axis=1)  # 두 번째 축을 따라 더함, 결과는 2x4 배열\n\narray([[12, 15, 18, 21],\n       [48, 51, 54, 57]])\n\n\n여러 축에 대해서 더할 수도 있습니다:\n\nc.sum(axis=(0,2))  # 첫 번째 축과 세 번째 축을 따라 더함, 결과는 (3,) 배열\n\narray([ 60,  92, 124])\n\n\n\n0+1+2+3 + 12+13+14+15, 4+5+6+7 + 16+17+18+19, 8+9+10+11 + 20+21+22+23\n\n(60, 92, 124)"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#일반-함수",
    "href": "Data_Mining/2022-03-04-numpy.html#일반-함수",
    "title": "Numpy 기본",
    "section": "일반 함수",
    "text": "일반 함수\n넘파이는 일반 함수(universal function) 또는 ufunc라고 부르는 원소별 함수를 제공합니다. 예를 들면 square 함수는 원본 ndarray를 복사하여 각 원소를 제곱한 새로운 ndarray 객체를 반환합니다:\n\na = np.array([[-2.5, 3.1, 7], [10, 11, 12]])\nnp.square(a)\n\narray([[  6.25,   9.61,  49.  ],\n       [100.  , 121.  , 144.  ]])\n\n\n다음은 유용한 단항 일반 함수들입니다:\n\nprint(\"원본 ndarray\")\nprint(a)\nfor func in (np.abs, np.sqrt, np.exp, np.log, np.sign, np.ceil, np.modf, np.isnan, np.cos):\n    print(\"\\n\", func.__name__)\n    print(func(a))\n\n원본 ndarray\n[[-2.5  3.1  7. ]\n [10.  11.  12. ]]\n\n absolute\n[[ 2.5  3.1  7. ]\n [10.  11.  12. ]]\n\n sqrt\n[[       nan 1.76068169 2.64575131]\n [3.16227766 3.31662479 3.46410162]]\n\n exp\n[[8.20849986e-02 2.21979513e+01 1.09663316e+03]\n [2.20264658e+04 5.98741417e+04 1.62754791e+05]]\n\n log\n[[       nan 1.13140211 1.94591015]\n [2.30258509 2.39789527 2.48490665]]\n\n sign\n[[-1.  1.  1.]\n [ 1.  1.  1.]]\n\n ceil\n[[-2.  4.  7.]\n [10. 11. 12.]]\n\n modf\n(array([[-0.5,  0.1,  0. ],\n       [ 0. ,  0. ,  0. ]]), array([[-2.,  3.,  7.],\n       [10., 11., 12.]]))\n\n isnan\n[[False False False]\n [False False False]]\n\n cos\n[[-0.80114362 -0.99913515  0.75390225]\n [-0.83907153  0.0044257   0.84385396]]\n\n\nRuntimeWarning: invalid value encountered in sqrt\n  print(func(a))\n&lt;ipython-input-59-d791c8e37e6f&gt;:5: RuntimeWarning: invalid value encountered in log\n  print(func(a))"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#이항-일반-함수",
    "href": "Data_Mining/2022-03-04-numpy.html#이항-일반-함수",
    "title": "Numpy 기본",
    "section": "이항 일반 함수",
    "text": "이항 일반 함수\n두 개의 ndarray에 원소별로 적용되는 이항 함수도 많습니다. 두 배열이 동일한 크기가 아니면 브로드캐스팅 규칙이 적용됩니다:\n\na = np.array([1, -2, 3, 4])\nb = np.array([2, 8, -1, 7])\nnp.add(a, b)  # a + b 와 동일\n\narray([ 3,  6,  2, 11])\n\n\n\nnp.greater(a, b)  # a &gt; b 와 동일\n\narray([False, False,  True, False])\n\n\n\nnp.maximum(a, b)\n\narray([2, 8, 3, 7])\n\n\n\nnp.copysign(a, b)\n\narray([ 1.,  2., -3.,  4.])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#차원-배열",
    "href": "Data_Mining/2022-03-04-numpy.html#차원-배열",
    "title": "Numpy 기본",
    "section": "1차원 배열",
    "text": "1차원 배열\n1차원 넘파이 배열은 보통의 파이썬 배열과 비슷하게 사용할 수 있습니다:\n\na = np.array([1, 5, 3, 19, 13, 7, 3])\na[3]\n\n19\n\n\n\na[2:5]\n\narray([ 3, 19, 13])\n\n\n\na[2:-1]\n\narray([ 3, 19, 13,  7])\n\n\n\na[:2]\n\narray([1, 5])\n\n\n\na[2::2]\n\narray([ 3, 13,  3])\n\n\n\na[::-1]\n\narray([ 3,  7, 13, 19,  3,  5,  1])\n\n\n물론 원소를 수정할 수 있죠:\n\na[3]=999\na\n\narray([  1,   5,   3, 999,  13,   7,   3])\n\n\n슬라이싱을 사용해 ndarray를 수정할 수 있습니다:\n\na[2:5] = [997, 998, 999]\na\n\narray([  1,   5, 997, 998, 999,   7,   3])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#보통의-파이썬-배열과-차이점",
    "href": "Data_Mining/2022-03-04-numpy.html#보통의-파이썬-배열과-차이점",
    "title": "Numpy 기본",
    "section": "보통의 파이썬 배열과 차이점",
    "text": "보통의 파이썬 배열과 차이점\n보통의 파이썬 배열과 대조적으로 ndarray 슬라이싱에 하나의 값을 할당하면 슬라이싱 전체에 복사됩니다. 위에서 언급한 브로드캐스팅 덕택입니다.\n\na[2:5] = -1\na\n\narray([ 1,  5, -1, -1, -1,  7,  3])\n\n\n또한 이런 식으로 ndarray 크기를 늘리거나 줄일 수 없습니다:\n\ntry:\n    a[2:5] = [1,2,3,4,5,6]  # 너무 길어요\nexcept ValueError as e:\n    print(e)\n\ncannot copy sequence with size 6 to array axis with dimension 3\n\n\n원소를 삭제할 수도 없습니다:\n\ntry:\n    del a[2:5]\nexcept ValueError as e:\n    print(e)\n\ncannot delete array elements\n\n\n중요한 점은 ndarray의 슬라이싱은 같은 데이터 버퍼를 바라보는 뷰(view)입니다. 슬라이싱된 객체를 수정하면 실제 원본 ndarray가 수정됩니다!\n\na_slice = a[2:6]\na_slice[1] = 1000\na  # 원본 배열이 수정됩니다!\n\narray([   1,    5,   -1, 1000,   -1,    7,    3])\n\n\n\na[3] = 2000\na_slice  # 비슷하게 원본 배열을 수정하면 슬라이싱 객체에도 반영됩니다!\n\narray([  -1, 2000,   -1,    7])\n\n\n데이터를 복사하려면 copy 메서드를 사용해야 합니다:\n\nanother_slice = a[2:6].copy()\nanother_slice[1] = 3000\na  # 원본 배열이 수정되지 않습니다\n\narray([   1,    5,   -1, 2000,   -1,    7,    3])\n\n\n\na[3] = 4000\nanother_slice  # 마찬가지로 원본 배열을 수정해도 복사된 배열은 바뀌지 않습니다\n\narray([  -1, 3000,   -1,    7])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#다차원-배열",
    "href": "Data_Mining/2022-03-04-numpy.html#다차원-배열",
    "title": "Numpy 기본",
    "section": "다차원 배열",
    "text": "다차원 배열\n다차원 배열은 비슷한 방식으로 각 축을 따라 인덱싱 또는 슬라이싱해서 사용합니다. 콤마로 구분합니다:\n\nb = np.arange(48).reshape(4, 12)\nb\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\n\n\n\nb[1, 2]  # 행 1, 열 2\n\n14\n\n\n\nb[1, :]  # 행 1, 모든 열\n\narray([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\n\n\n\nb[:, 1]  # 모든 행, 열 1\n\narray([ 1, 13, 25, 37])\n\n\n주의: 다음 두 표현에는 미묘한 차이가 있습니다:\n\nb[1, :]\n\narray([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\n\n\n\nb[1:2, :]\n\narray([[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]])\n\n\n첫 번째 표현식은 (12,) 크기인 1D 배열로 행이 하나입니다. 두 번째는 (1, 12) 크기인 2D 배열로 같은 행을 반환합니다."
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#팬시-인덱싱fancy-indexing",
    "href": "Data_Mining/2022-03-04-numpy.html#팬시-인덱싱fancy-indexing",
    "title": "Numpy 기본",
    "section": "팬시 인덱싱(Fancy indexing)",
    "text": "팬시 인덱싱(Fancy indexing)\n관심 대상의 인덱스 리스트를 지정할 수도 있습니다. 이를 팬시 인덱싱이라고 부릅니다.\n\nb[(0,2), 2:5]  # 행 0과 2, 열 2에서 4(5-1)까지\n\narray([[ 2,  3,  4],\n       [26, 27, 28]])\n\n\n\nb[:, (-1, 2, -1)]  # 모든 행, 열 -1 (마지막), 2와 -1 (다시 반대 방향으로)\n\narray([[11,  2, 11],\n       [23, 14, 23],\n       [35, 26, 35],\n       [47, 38, 47]])\n\n\n여러 개의 인덱스 리스트를 지정하면 인덱스에 맞는 값이 포함된 1D ndarray를 반환됩니다.\n\nb[(-1, 2, -1, 2), (5, 9, 1, 9)]  # returns a 1D array with b[-1, 5], b[2, 9], b[-1, 1] and b[2, 9] (again)\n\narray([41, 33, 37, 33])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#고차원",
    "href": "Data_Mining/2022-03-04-numpy.html#고차원",
    "title": "Numpy 기본",
    "section": "고차원",
    "text": "고차원\n고차원에서도 동일한 방식이 적용됩니다. 몇 가지 예를 살펴 보겠습니다:\n\nc = b.reshape(4,2,6)\nc\n\narray([[[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11]],\n\n       [[12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23]],\n\n       [[24, 25, 26, 27, 28, 29],\n        [30, 31, 32, 33, 34, 35]],\n\n       [[36, 37, 38, 39, 40, 41],\n        [42, 43, 44, 45, 46, 47]]])\n\n\n\nc[2, 1, 4]  # 행렬 2, 행 1, 열 4\n\n34\n\n\n\nc[2, :, 3]  # 행렬 2, 모든 행, 열 3\n\narray([27, 33])\n\n\n어떤 축에 대한 인덱스를 지정하지 않으면 이 축의 모든 원소가 반환됩니다:\n\nc[2, 1]  # 행렬 2, 행 1, 모든 열이 반환됩니다. c[2, 1, :]와 동일합니다.\n\narray([30, 31, 32, 33, 34, 35])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#생략-부호-...",
    "href": "Data_Mining/2022-03-04-numpy.html#생략-부호-...",
    "title": "Numpy 기본",
    "section": "생략 부호 (...)",
    "text": "생략 부호 (...)\n생략 부호(...)를 쓰면 모든 지정하지 않은 축의 원소를 포함합니다.\n\nc[2, ...]  #  행렬 2, 모든 행, 모든 열. c[2, :, :]와 동일\n\narray([[24, 25, 26, 27, 28, 29],\n       [30, 31, 32, 33, 34, 35]])\n\n\n\nc[2, 1, ...]  # 행렬 2, 행 1, 모든 열. c[2, 1, :]와 동일\n\narray([30, 31, 32, 33, 34, 35])\n\n\n\nc[2, ..., 3]  # 행렬 2, 모든 행, 열 3. c[2, :, 3]와 동일\n\narray([27, 33])\n\n\n\nc[..., 3]  # 모든 행렬, 모든 행, 열 3. c[:, :, 3]와 동일\n\narray([[ 3,  9],\n       [15, 21],\n       [27, 33],\n       [39, 45]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#불리언-인덱싱",
    "href": "Data_Mining/2022-03-04-numpy.html#불리언-인덱싱",
    "title": "Numpy 기본",
    "section": "불리언 인덱싱",
    "text": "불리언 인덱싱\n불리언 값을 가진 ndarray를 사용해 축의 인덱스를 지정할 수 있습니다.\n\nb = np.arange(48).reshape(4, 12)\nb\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\n\n\n\nrows_on = np.array([True, False, True, False])\nb[rows_on, :]  # 행 0과 2, 모든 열. b[(0, 2), :]와 동일\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]])\n\n\n\ncols_on = np.array([False, True, False] * 4)\nb[:, cols_on]  # 모든 행, 열 1, 4, 7, 10\n\narray([[ 1,  4,  7, 10],\n       [13, 16, 19, 22],\n       [25, 28, 31, 34],\n       [37, 40, 43, 46]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.ix_",
    "href": "Data_Mining/2022-03-04-numpy.html#np.ix_",
    "title": "Numpy 기본",
    "section": "np.ix_",
    "text": "np.ix_\n여러 축에 걸쳐서는 불리언 인덱싱을 사용할 수 없고 ix_ 함수를 사용합니다:\n\nb[np.ix_(rows_on, cols_on)]\n\narray([[ 1,  4,  7, 10],\n       [25, 28, 31, 34]])\n\n\n\nnp.ix_(rows_on, cols_on)\n\n(array([[0],\n        [2]]),\n array([[ 1,  4,  7, 10]]))\n\n\nndarray와 같은 크기의 불리언 배열을 사용하면 해당 위치가 True인 모든 원소를 담은 1D 배열이 반환됩니다. 일반적으로 조건 연산자와 함께 사용합니다:\n\nb[b % 3 == 1]\n\narray([ 1,  4,  7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#vstack",
    "href": "Data_Mining/2022-03-04-numpy.html#vstack",
    "title": "Numpy 기본",
    "section": "vstack",
    "text": "vstack\nvstack 함수를 사용하여 수직으로 쌓아보죠:\n\nq4 = np.vstack((q1, q2, q3))\nq4\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.]])\n\n\n\nq4.shape\n\n(10, 4)\n\n\nq1, q2, q3가 모두 같은 크기이므로 가능합니다(수직으로 쌓기 때문에 수직 축은 크기가 달라도 됩니다)."
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#hstack",
    "href": "Data_Mining/2022-03-04-numpy.html#hstack",
    "title": "Numpy 기본",
    "section": "hstack",
    "text": "hstack\nhstack을 사용해 수평으로도 쌓을 수 있습니다:\n\nq5 = np.hstack((q1, q3))\nq5\n\narray([[1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.]])\n\n\n\nq5.shape\n\n(3, 8)\n\n\nq1과 q3가 모두 3개의 행을 가지고 있기 때문에 가능합니다. q2는 4개의 행을 가지고 있기 때문에 q1, q3와 수평으로 쌓을 수 없습니다:\n\ntry:\n    q5 = np.hstack((q1, q2, q3))\nexcept ValueError as e:\n    print(e)\n\nall the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 3 and the array at index 1 has size 4"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#concatenate",
    "href": "Data_Mining/2022-03-04-numpy.html#concatenate",
    "title": "Numpy 기본",
    "section": "concatenate",
    "text": "concatenate\nconcatenate 함수는 지정한 축으로도 배열을 쌓습니다.\n\nq7 = np.concatenate((q1, q2, q3), axis=0)  # vstack과 동일\nq7\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.]])\n\n\n\nq7.shape\n\n(10, 4)\n\n\n예상했겠지만 hstack은 axis=1으로 concatenate를 호출하는 것과 같습니다."
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#stack",
    "href": "Data_Mining/2022-03-04-numpy.html#stack",
    "title": "Numpy 기본",
    "section": "stack",
    "text": "stack\nstack 함수는 새로운 축을 따라 배열을 쌓습니다. 모든 배열은 같은 크기를 가져야 합니다.\n\nq8 = np.stack((q1, q3))\nq8\n\narray([[[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]],\n\n       [[3., 3., 3., 3.],\n        [3., 3., 3., 3.],\n        [3., 3., 3., 3.]]])\n\n\n\nq8.shape\n\n(2, 3, 4)"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#행렬-전치",
    "href": "Data_Mining/2022-03-04-numpy.html#행렬-전치",
    "title": "Numpy 기본",
    "section": "행렬 전치",
    "text": "행렬 전치\nT 속성은 랭크가 2보다 크거나 같을 때 transpose()를 호출하는 것과 같습니다:\n\nm1 = np.arange(10).reshape(2,5)\nm1\n\narray([[0, 1, 2, 3, 4],\n       [5, 6, 7, 8, 9]])\n\n\n\nm1.T\n\narray([[0, 5],\n       [1, 6],\n       [2, 7],\n       [3, 8],\n       [4, 9]])\n\n\nT 속성은 랭크가 0이거나 1인 배열에는 아무런 영향을 미치지 않습니다:\n\nm2 = np.arange(5)\nm2\n\narray([0, 1, 2, 3, 4])\n\n\n\nm2.T\n\narray([0, 1, 2, 3, 4])\n\n\n먼저 1D 배열을 하나의 행이 있는 행렬(2D)로 바꾼다음 전치를 수행할 수 있습니다:\n\nm2r = m2.reshape(1,5)\nm2r\n\narray([[0, 1, 2, 3, 4]])\n\n\n\nm2r.T\n\narray([[0],\n       [1],\n       [2],\n       [3],\n       [4]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#행렬-곱셈",
    "href": "Data_Mining/2022-03-04-numpy.html#행렬-곱셈",
    "title": "Numpy 기본",
    "section": "행렬 곱셈",
    "text": "행렬 곱셈\n두 개의 행렬을 만들어 dot 메서드로 행렬 곱셈을 실행해 보죠.\n\nn1 = np.arange(10).reshape(2, 5)\nn1\n\narray([[0, 1, 2, 3, 4],\n       [5, 6, 7, 8, 9]])\n\n\n\nn2 = np.arange(15).reshape(5,3)\nn2\n\narray([[ 0,  1,  2],\n       [ 3,  4,  5],\n       [ 6,  7,  8],\n       [ 9, 10, 11],\n       [12, 13, 14]])\n\n\n\nn1.dot(n2)\n\narray([[ 90, 100, 110],\n       [240, 275, 310]])\n\n\n주의: 앞서 언급한 것처럼 n1*n2는 행렬 곱셈이 아니라 원소별 곱셈(또는 아다마르 곱이라 부릅니다)입니다."
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#역행렬과-유사-역행렬",
    "href": "Data_Mining/2022-03-04-numpy.html#역행렬과-유사-역행렬",
    "title": "Numpy 기본",
    "section": "역행렬과 유사 역행렬",
    "text": "역행렬과 유사 역행렬\nnumpy.linalg 모듈 안에 많은 선형 대수 함수들이 있습니다. 특히 inv 함수는 정방 행렬의 역행렬을 계산합니다:\n\nimport numpy.linalg as linalg\n\nm3 = np.array([[1,2,3],[5,7,11],[21,29,31]])\nm3\n\narray([[ 1,  2,  3],\n       [ 5,  7, 11],\n       [21, 29, 31]])\n\n\n\nlinalg.inv(m3)\n\narray([[-2.31818182,  0.56818182,  0.02272727],\n       [ 1.72727273, -0.72727273,  0.09090909],\n       [-0.04545455,  0.29545455, -0.06818182]])\n\n\npinv 함수를 사용하여 유사 역행렬을 계산할 수도 있습니다:\n\nlinalg.pinv(m3)\n\narray([[-2.31818182,  0.56818182,  0.02272727],\n       [ 1.72727273, -0.72727273,  0.09090909],\n       [-0.04545455,  0.29545455, -0.06818182]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#단위-행렬",
    "href": "Data_Mining/2022-03-04-numpy.html#단위-행렬",
    "title": "Numpy 기본",
    "section": "단위 행렬",
    "text": "단위 행렬\n행렬과 그 행렬의 역행렬을 곱하면 단위 행렬이 됩니다(작은 소숫점 오차가 있습니다):\n\nm3.dot(linalg.inv(m3))\n\narray([[ 1.00000000e+00, -1.66533454e-16,  0.00000000e+00],\n       [ 6.31439345e-16,  1.00000000e+00, -1.38777878e-16],\n       [ 5.21110932e-15, -2.38697950e-15,  1.00000000e+00]])\n\n\neye 함수는 NxN 크기의 단위 행렬을 만듭니다:\n\nnp.eye(3)\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#qr-분해",
    "href": "Data_Mining/2022-03-04-numpy.html#qr-분해",
    "title": "Numpy 기본",
    "section": "QR 분해",
    "text": "QR 분해\nqr 함수는 행렬을 QR 분해합니다:\n\nq, r = linalg.qr(m3)\nq\n\narray([[-0.04627448,  0.98786672,  0.14824986],\n       [-0.23137241,  0.13377362, -0.96362411],\n       [-0.97176411, -0.07889213,  0.22237479]])\n\n\n\nr\n\narray([[-21.61018278, -29.89331494, -32.80860727],\n       [  0.        ,   0.62427688,   1.9894538 ],\n       [  0.        ,   0.        ,  -3.26149699]])\n\n\n\nq.dot(r)  # q.r는 m3와 같습니다\n\narray([[ 1.,  2.,  3.],\n       [ 5.,  7., 11.],\n       [21., 29., 31.]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#행렬식",
    "href": "Data_Mining/2022-03-04-numpy.html#행렬식",
    "title": "Numpy 기본",
    "section": "행렬식",
    "text": "행렬식\ndet 함수는 행렬식을 계산합니다:\n\nlinalg.det(m3)  # 행렬식 계산\n\n43.99999999999997"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#고윳값과-고유벡터",
    "href": "Data_Mining/2022-03-04-numpy.html#고윳값과-고유벡터",
    "title": "Numpy 기본",
    "section": "고윳값과 고유벡터",
    "text": "고윳값과 고유벡터\neig 함수는 정방 행렬의 고윳값과 고유벡터를 계산합니다:\n\neigenvalues, eigenvectors = linalg.eig(m3)\neigenvalues # λ\n\narray([42.26600592, -0.35798416, -2.90802176])\n\n\n\neigenvectors # v\n\narray([[-0.08381182, -0.76283526, -0.18913107],\n       [-0.3075286 ,  0.64133975, -0.6853186 ],\n       [-0.94784057, -0.08225377,  0.70325518]])\n\n\n\nm3.dot(eigenvectors) - eigenvalues * eigenvectors  # m3.v - λ*v = 0\n\narray([[ 8.88178420e-15,  2.22044605e-16, -3.10862447e-15],\n       [ 3.55271368e-15,  2.02615702e-15, -1.11022302e-15],\n       [ 3.55271368e-14,  3.33413852e-15, -8.43769499e-15]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#특잇값-분해",
    "href": "Data_Mining/2022-03-04-numpy.html#특잇값-분해",
    "title": "Numpy 기본",
    "section": "특잇값 분해",
    "text": "특잇값 분해\nsvd 함수는 행렬을 입력으로 받아 그 행렬의 특잇값 분해를 반환합니다:\n\nm4 = np.array([[1,0,0,0,2], [0,0,3,0,0], [0,0,0,0,0], [0,2,0,0,0]])\nm4\n\narray([[1, 0, 0, 0, 2],\n       [0, 0, 3, 0, 0],\n       [0, 0, 0, 0, 0],\n       [0, 2, 0, 0, 0]])\n\n\n\nU, S_diag, V = linalg.svd(m4)\nU\n\narray([[ 0.,  1.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0., -1.],\n       [ 0.,  0.,  1.,  0.]])\n\n\n\nS_diag\n\narray([3.        , 2.23606798, 2.        , 0.        ])\n\n\nsvd 함수는 Σ의 대각 원소 값만 반환합니다. 전체 Σ 행렬은 다음과 같이 만듭니다:\n\nS = np.zeros((4, 5))\nS[np.diag_indices(4)] = S_diag\nS  # Σ\n\narray([[3.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 2.23606798, 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 2.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ]])\n\n\n\nV\n\narray([[-0.        ,  0.        ,  1.        , -0.        ,  0.        ],\n       [ 0.4472136 ,  0.        ,  0.        ,  0.        ,  0.89442719],\n       [-0.        ,  1.        ,  0.        , -0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ],\n       [-0.89442719,  0.        ,  0.        ,  0.        ,  0.4472136 ]])\n\n\n\nU.dot(S).dot(V) # U.Σ.V == m4\n\narray([[1., 0., 0., 0., 2.],\n       [0., 0., 3., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 2., 0., 0., 0.]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#대각원소와-대각합",
    "href": "Data_Mining/2022-03-04-numpy.html#대각원소와-대각합",
    "title": "Numpy 기본",
    "section": "대각원소와 대각합",
    "text": "대각원소와 대각합\n\nnp.diag(m3)  # m3의 대각 원소입니다(왼쪽 위에서 오른쪽 아래)\n\narray([ 1,  7, 31])\n\n\n\nnp.trace(m3)  # np.diag(m3).sum()와 같습니다\n\n39"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#선형-방정식-풀기",
    "href": "Data_Mining/2022-03-04-numpy.html#선형-방정식-풀기",
    "title": "Numpy 기본",
    "section": "선형 방정식 풀기",
    "text": "선형 방정식 풀기\nsolve 함수는 다음과 같은 선형 방정식을 풉니다:\n\n\\(2x + 6y = 6\\)\n\\(5x + 3y = -9\\)\n\n\ncoeffs  = np.array([[2, 6], [5, 3]])\ndepvars = np.array([6, -9])\nsolution = linalg.solve(coeffs, depvars)\nsolution\n\narray([-3.,  2.])\n\n\nsolution을 확인해 보죠:\n\ncoeffs.dot(solution), depvars  # 네 같네요\n\n(array([ 6., -9.]), array([ 6, -9]))\n\n\n좋습니다! 다른 방식으로도 solution을 확인해 보죠:\n\nnp.allclose(coeffs.dot(solution), depvars)\n\nTrue"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#바이너리-.npy-포맷",
    "href": "Data_Mining/2022-03-04-numpy.html#바이너리-.npy-포맷",
    "title": "Numpy 기본",
    "section": "바이너리 .npy 포맷",
    "text": "바이너리 .npy 포맷\n랜덤 배열을 만들고 저장해 보죠.\n\na = np.random.rand(2,3)\na\n\narray([[0.5435938 , 0.92886307, 0.01535158],\n       [0.4157283 , 0.9102127 , 0.55129708]])\n\n\n\nnp.save(\"my_array\", a)\n\n끝입니다! 파일 이름의 확장자를 지정하지 않았기 때문에 넘파이는 자동으로 .npy를 붙입니다. 파일 내용을 확인해 보겠습니다:\n\nwith open(\"my_array.npy\", \"rb\") as f:\n    content = f.read()\n\ncontent\n\nb\"\\x93NUMPY\\x01\\x00v\\x00{'descr': '&lt;f8', 'fortran_order': False, 'shape': (2, 3), }                                                          \\nY\\xc1\\xfc\\xd0\\x1ee\\xe1?\\xde{3\\t?\\xb9\\xed?\\x80V\\x08\\xef\\xa5p\\x8f?\\x96I}\\xe0J\\x9b\\xda?\\xe0U\\xfaav \\xed?\\xd8\\xe50\\xc59\\xa4\\xe1?\"\n\n\n이 파일을 넘파이 배열로 로드하려면 load 함수를 사용합니다:\n\na_loaded = np.load(\"my_array.npy\")\na_loaded\n\narray([[0.5435938 , 0.92886307, 0.01535158],\n       [0.4157283 , 0.9102127 , 0.55129708]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#텍스트-포맷",
    "href": "Data_Mining/2022-03-04-numpy.html#텍스트-포맷",
    "title": "Numpy 기본",
    "section": "텍스트 포맷",
    "text": "텍스트 포맷\n배열을 텍스트 포맷으로 저장해 보죠:\n\nnp.savetxt(\"my_array.csv\", a)\n\n파일 내용을 확인해 보겠습니다:\n\nwith open(\"my_array.csv\", \"rt\") as f:\n    print(f.read())\n\n5.435937959464737235e-01 9.288630656918674955e-01 1.535157809943688001e-02\n4.157283012656532994e-01 9.102126992826775620e-01 5.512970782648904944e-01\n\n\n\n이 파일은 탭으로 구분된 CSV 파일입니다. 다른 구분자를 지정할 수도 있습니다:\n\nnp.savetxt(\"my_array.csv\", a, delimiter=\",\")\n\n이 파일을 로드하려면 loadtxt 함수를 사용합니다:\n\na_loaded = np.loadtxt(\"my_array.csv\", delimiter=\",\")\na_loaded\n\narray([[0.5435938 , 0.92886307, 0.01535158],\n       [0.4157283 , 0.9102127 , 0.55129708]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#압축된-.npz-포맷",
    "href": "Data_Mining/2022-03-04-numpy.html#압축된-.npz-포맷",
    "title": "Numpy 기본",
    "section": "압축된 .npz 포맷",
    "text": "압축된 .npz 포맷\n여러 개의 배열을 압축된 한 파일로 저장하는 것도 가능합니다:\n\nb = np.arange(24, dtype=np.uint8).reshape(2, 3, 4)\nb\n\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]], dtype=uint8)\n\n\n\nnp.savez(\"my_arrays\", my_a=a, my_b=b)\n\n파일 내용을 확인해 보죠. .npz 파일 확장자가 자동으로 추가되었습니다.\n\nwith open(\"my_arrays.npz\", \"rb\") as f:\n    content = f.read()\n\nrepr(content)[:180] + \"[...]\"\n\n'b\"PK\\\\x03\\\\x04\\\\x14\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00!\\\\x00\\\\x063\\\\xcf\\\\xb9\\\\xb0\\\\x00\\\\x00\\\\x00\\\\xb0\\\\x00\\\\x00\\\\x00\\\\x08\\\\x00\\\\x14\\\\x00my_a.npy\\\\x01\\\\x00\\\\x10\\\\x00\\\\xb0\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\xb0\\\\x00\\\\x00\\\\x[...]'\n\n\n다음과 같이 이 파일을 로드할 수 있습니다:\n\nmy_arrays = np.load(\"my_arrays.npz\")\nmy_arrays\n\n&lt;numpy.lib.npyio.NpzFile at 0x7f9791c73d60&gt;\n\n\n게으른 로딩을 수행하는 딕셔너리와 유사한 객체입니다:\n\nmy_arrays.keys()\n\nKeysView(&lt;numpy.lib.npyio.NpzFile object at 0x7f9791c73d60&gt;)\n\n\n\nmy_arrays[\"my_a\"]\n\narray([[0.5435938 , 0.92886307, 0.01535158],\n       [0.4157283 , 0.9102127 , 0.55129708]])"
  },
  {
    "objectID": "BigData_Analysis/실기_파이썬기초.html",
    "href": "BigData_Analysis/실기_파이썬기초.html",
    "title": "빅분기 실기 - 파이썬 기초",
    "section": "",
    "text": "파이썬 기초"
  },
  {
    "objectID": "BigData_Analysis/실기_파이썬기초.html#데이터-타입object-int-float-bool-등",
    "href": "BigData_Analysis/실기_파이썬기초.html#데이터-타입object-int-float-bool-등",
    "title": "빅분기 실기 - 파이썬 기초",
    "section": "1.데이터 타입(object, int, float, bool 등)",
    "text": "1.데이터 타입(object, int, float, bool 등)\n\n# 데이터 타입 확인\ndf.dtypes\n\ncar      object\nmpg     float64\ncyl       int64\ndisp    float64\nhp        int64\ndrat    float64\nwt      float64\nqsec    float64\nvs        int64\nam        int64\ngear      int64\ncarb      int64\ndtype: object\n\n\n\n# 데이터 타입 변경 (1개)\ndf1 = df.copy()\ndf1 = df1.astype({'cyl':'object'})\nprint(df1.dtypes)\n\ncar      object\nmpg     float64\ncyl      object\ndisp    float64\nhp        int64\ndrat    float64\nwt      float64\nqsec    float64\nvs        int64\nam        int64\ngear      int64\ncarb      int64\ndtype: object\n\n\n\n# 데이터 타입 변경 (2개 이상)\ndf1 = df.copy()\ndf1 = df1.astype({'cyl':'int', 'gear':'object'})\nprint(df1.dtypes)\n\ncar      object\nmpg     float64\ncyl       int32\ndisp    float64\nhp        int64\ndrat    float64\nwt      float64\nqsec    float64\nvs        int64\nam        int64\ngear     object\ncarb      int64\ndtype: object\n\n\n\n#df1['cyl']\n\n\ndf1['cyl'].value_counts()\n\n8    14\n4    11\n6     7\nName: cyl, dtype: int64"
  },
  {
    "objectID": "BigData_Analysis/실기_파이썬기초.html#기초통계량평균-중앙값-사분위수-iqr-표준편차-등",
    "href": "BigData_Analysis/실기_파이썬기초.html#기초통계량평균-중앙값-사분위수-iqr-표준편차-등",
    "title": "빅분기 실기 - 파이썬 기초",
    "section": "2. 기초통계량(평균, 중앙값, 사분위수, IQR, 표준편차 등)",
    "text": "2. 기초통계량(평균, 중앙값, 사분위수, IQR, 표준편차 등)\n\n# Import CSV mtcars\ndf = pd.read_csv('mtcars.txt')\ndf.head()\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\nMazda RX4\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\nHornet 4 Drive\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n4\nHornet Sportabout\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n\n\n\n\ndf.shape # (행, 열)\n\n(32, 12)\n\n\n\n# 평균값 구하기\nmpg_mean = df['mpg'].mean()\nprint(mpg_mean)\n\n20.090624999999996\n\n\n\n# 중앙값 구하기\nmpg_median = df['mpg'].median()\nprint(mpg_median)\n\n19.2\n\n\n\n# 최빈값 구하기\ncyl_mode = df['cyl'].mode()\nprint(cyl_mode)\n\n0    8\nName: cyl, dtype: int64\n\n\n\ndf['cyl'].value_counts()\n\n8    14\n4    11\n6     7\nName: cyl, dtype: int64\n\n\n\n# 분산\nmpg_var = df['mpg'].var()\nprint(mpg_var)\n\n36.32410282258065\n\n\n\n# 표준편차\nmpg_std = df['mpg'].std()\nprint(mpg_std)\n\n6.026948052089105\n\n\n\n# IQR (Q3 - Q1)\nQ1 = df['mpg'].quantile(.25)\nprint(Q1)\n\n15.425\n\n\n\nQ3 = df['mpg'].quantile(.75)\nprint(Q3)\n\n22.8\n\n\n\nIQR = Q3 - Q1\nprint(IQR)\n\n7.375\n\n\n\nQ2 = df['mpg'].quantile(.5)\nprint(Q2)\nprint(df['mpg'].median()) # 2사분위수와 중앙값은 동일한 값을 출력 \n\n19.2\n19.2\n\n\n\n# 범위(Range) = 최대값 - 최소값\nmpg_max = df['mpg'].max()\nprint(mpg_max)\n\n33.9\n\n\n\nmpg_min = df['mpg'].min()\nprint(mpg_min)\n\n10.4\n\n\n\nmpg_range = mpg_max - mpg_min\nprint(mpg_range)\n\n23.5\n\n\n\n1) 분포의 비대칭도\n\n# 왜도\nmpg_skew = df['mpg'].skew()\nprint(mpg_skew)\n\n0.6723771376290805\n\n\n\n# 첨도\nmpg_kurt = df['mpg'].kurt()\nprint(mpg_kurt)\n\n-0.0220062914240855\n\n\n\n\n2) 기타(합계, 절대값, 데이터 수 등)\n\n# 합계\nmpg_sum = df['mpg'].sum()\nprint(mpg_sum)\n\n642.9000000000001\n\n\n\n# 절대값\nIQR2 = Q1 - Q3\nprint(IQR2)\nprint(abs(IQR2))\n\n-7.375\n7.375\n\n\n\n# 데이터 수\nprint(len(df['mpg']))\n\n32\n\n\n\n\n3) 그룹화하여 계산하기 (groupby 활용)\n\nimport seaborn as sns\ndf = sns.load_dataset('iris')\ndf.head()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\n# species 별로 각 변수의 평균 구해보기\ndf.groupby('species').mean()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\nspecies\n\n\n\n\n\n\n\n\nsetosa\n5.006\n3.428\n1.462\n0.246\n\n\nversicolor\n5.936\n2.770\n4.260\n1.326\n\n\nvirginica\n6.588\n2.974\n5.552\n2.026\n\n\n\n\n\n\n\n\ndf.groupby('species').median()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\nspecies\n\n\n\n\n\n\n\n\nsetosa\n5.0\n3.4\n1.50\n0.2\n\n\nversicolor\n5.9\n2.8\n4.35\n1.3\n\n\nvirginica\n6.5\n3.0\n5.55\n2.0"
  },
  {
    "objectID": "BigData_Analysis/실기_파이썬기초.html#데이터-인덱싱-필터링-정렬-변경-등",
    "href": "BigData_Analysis/실기_파이썬기초.html#데이터-인덱싱-필터링-정렬-변경-등",
    "title": "빅분기 실기 - 파이썬 기초",
    "section": "3. 데이터 인덱싱, 필터링, 정렬, 변경 등",
    "text": "3. 데이터 인덱싱, 필터링, 정렬, 변경 등\n\ndf = pd.read_csv('mtcars.txt')\ndf.head()\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\nMazda RX4\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\nHornet 4 Drive\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n4\nHornet Sportabout\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n\n\n\n\n1) 데이터 인덱싱\n인덱싱 - 행, 열을 기준으로 데이터값을 기준으로 뽑는 것\n\n# 행/열 인덱싱 : df.loc['행', '열']\ndf.loc[3, 'mpg'] # 인덱싱은 0부터 시작임 \n\n21.4\n\n\n\n# 열만 인덱싱\ndf.loc[:, 'mpg'].head() # ':' 는 전체를 가지고 올 때(빈칸으로 두면 에러)\n\n0    21.0\n1    21.0\n2    22.8\n3    21.4\n4    18.7\nName: mpg, dtype: float64\n\n\n\ndf.loc[0:3, ['mpg', 'cyl', 'disp']] # 행에서 0~3번 사이의 인덱스를 가지고 와라\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\n\n\n\n\n0\n21.0\n6\n160.0\n\n\n1\n21.0\n6\n160.0\n\n\n2\n22.8\n4\n108.0\n\n\n3\n21.4\n6\n258.0\n\n\n\n\n\n\n\n\ndf.loc[0:3, 'mpg':'disp']  # 열에서 mpg와 disp 사이의 변수를 가지고 와라\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\n\n\n\n\n0\n21.0\n6\n160.0\n\n\n1\n21.0\n6\n160.0\n\n\n2\n22.8\n4\n108.0\n\n\n3\n21.4\n6\n258.0\n\n\n\n\n\n\n\n\n# 앞에서 n행 인덱싱\ndf.head(2)\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\nMazda RX4\n21.0\n6\n160.0\n110\n3.9\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.9\n2.875\n17.02\n0\n1\n4\n4\n\n\n\n\n\n\n\n\n# 뒤에서 n행 인덱싱\ndf.tail(3)\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n29\nFerrari Dino\n19.7\n6\n145.0\n175\n3.62\n2.77\n15.5\n0\n1\n5\n6\n\n\n30\nMaserati Bora\n15.0\n8\n301.0\n335\n3.54\n3.57\n14.6\n0\n1\n5\n8\n\n\n31\nVolvo 142E\n21.4\n4\n121.0\n109\n4.11\n2.78\n18.6\n1\n1\n4\n2\n\n\n\n\n\n\n\n\n\n2) 열(Columns) 추가/제거\n\n# 열 선택\ndf_cyl = df['cyl']\ndf_cyl.head(3) # df.cyl.head(3) 같은 결과 비추\n\n0    6\n1    6\n2    4\nName: cyl, dtype: int64\n\n\n\ndf_new = df[['cyl','mpg']]\ndf_new.head(3)\n\n\n\n\n\n\n\n\ncyl\nmpg\n\n\n\n\n0\n6\n21.0\n\n\n1\n6\n21.0\n\n\n2\n4\n22.8\n\n\n\n\n\n\n\n\n# 열 제거\ndf.drop(columns = ['car','mpg','cyl']).head(3)\n\n\n\n\n\n\n\n\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n\n\n\n\n\n\n# 열 추가\ndf2 = df.copy()\ndf2['new'] = df['mpg'] + 10 # 새로운 컬럼으로 생김\ndf2.head(3)\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\nnew\n\n\n\n\n0\nMazda RX4\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n31.0\n\n\n1\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n31.0\n\n\n2\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n32.8\n\n\n\n\n\n\n\n\n\n3) 데이터 필터링\n\n# 1개 조건 필터링\n# cyl = 4 인 데이터의 수 \ncond1 = (df['cyl'] == 4)\nlen(df[cond1])\n\n# cyl_4 = df[df['cyl']==4]\n# print(len(cyl_4))\n\n11\n\n\n\n# mpg 가 22 이상인 데이터 수\ncond2 = (df['mpg'] &gt;= 22)\nlen(df[cond2])\n\n9\n\n\n\n# 2개 조건 필터링 \ndf[cond1 & cond2]\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n2\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n7\nMerc 240D\n24.4\n4\n146.7\n62\n3.69\n3.190\n20.00\n1\n0\n4\n2\n\n\n8\nMerc 230\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2\n\n\n17\nFiat 128\n32.4\n4\n78.7\n66\n4.08\n2.200\n19.47\n1\n1\n4\n1\n\n\n18\nHonda Civic\n30.4\n4\n75.7\n52\n4.93\n1.615\n18.52\n1\n1\n4\n2\n\n\n19\nToyota Corolla\n33.9\n4\n71.1\n65\n4.22\n1.835\n19.90\n1\n1\n4\n1\n\n\n25\nFiat X1-9\n27.3\n4\n79.0\n66\n4.08\n1.935\n18.90\n1\n1\n4\n1\n\n\n26\nPorsche 914-2\n26.0\n4\n120.3\n91\n4.43\n2.140\n16.70\n0\n1\n5\n2\n\n\n27\nLotus Europa\n30.4\n4\n95.1\n113\n3.77\n1.513\n16.90\n1\n1\n5\n2\n\n\n\n\n\n\n\n\n# 2개 조건 필터링 후 데이터 개수 (and)\nprint(len(df[cond1 & cond2])) # print() 함수를 이용해서 개수를 출력해야 인정됨 \n\n9\n\n\n\n# 2개 조건 필터링 후 데이터 개수 (or)\ndf[cond1 | cond2]\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n2\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n7\nMerc 240D\n24.4\n4\n146.7\n62\n3.69\n3.190\n20.00\n1\n0\n4\n2\n\n\n8\nMerc 230\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2\n\n\n17\nFiat 128\n32.4\n4\n78.7\n66\n4.08\n2.200\n19.47\n1\n1\n4\n1\n\n\n18\nHonda Civic\n30.4\n4\n75.7\n52\n4.93\n1.615\n18.52\n1\n1\n4\n2\n\n\n19\nToyota Corolla\n33.9\n4\n71.1\n65\n4.22\n1.835\n19.90\n1\n1\n4\n1\n\n\n20\nToyota Corona\n21.5\n4\n120.1\n97\n3.70\n2.465\n20.01\n1\n0\n3\n1\n\n\n25\nFiat X1-9\n27.3\n4\n79.0\n66\n4.08\n1.935\n18.90\n1\n1\n4\n1\n\n\n26\nPorsche 914-2\n26.0\n4\n120.3\n91\n4.43\n2.140\n16.70\n0\n1\n5\n2\n\n\n27\nLotus Europa\n30.4\n4\n95.1\n113\n3.77\n1.513\n16.90\n1\n1\n5\n2\n\n\n31\nVolvo 142E\n21.4\n4\n121.0\n109\n4.11\n2.780\n18.60\n1\n1\n4\n2\n\n\n\n\n\n\n\n\nprint(len(df[cond1 | cond2]))\n\n11\n\n\n\n# 한번에 코딩할 경우\nprint(len(df[(df['cyl'] == 4) & (df['mpg'] &gt;= 22)]))\nprint(len(df[(df['cyl'] == 4) | (df['mpg'] &gt;= 22)])) # 이렇게 한번에 하면 실수할 가능성 존재 cond1,2를 만들어서 하는 것 추천\n\n9\n11\n\n\n\n\n4) 데이터 정렬\n\n# 내림차순 정렬 (위에서부터 내려간다)\ndf.sort_values('mpg', ascending=False).head()\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n19\nToyota Corolla\n33.9\n4\n71.1\n65\n4.22\n1.835\n19.90\n1\n1\n4\n1\n\n\n17\nFiat 128\n32.4\n4\n78.7\n66\n4.08\n2.200\n19.47\n1\n1\n4\n1\n\n\n27\nLotus Europa\n30.4\n4\n95.1\n113\n3.77\n1.513\n16.90\n1\n1\n5\n2\n\n\n18\nHonda Civic\n30.4\n4\n75.7\n52\n4.93\n1.615\n18.52\n1\n1\n4\n2\n\n\n25\nFiat X1-9\n27.3\n4\n79.0\n66\n4.08\n1.935\n18.90\n1\n1\n4\n1\n\n\n\n\n\n\n\n\n# 오름차순 정렬 (아래에서부터 올라간다)\ndf.sort_values('mpg', ascending=True).head()\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n15\nLincoln Continental\n10.4\n8\n460.0\n215\n3.00\n5.424\n17.82\n0\n0\n3\n4\n\n\n14\nCadillac Fleetwood\n10.4\n8\n472.0\n205\n2.93\n5.250\n17.98\n0\n0\n3\n4\n\n\n23\nCamaro Z28\n13.3\n8\n350.0\n245\n3.73\n3.840\n15.41\n0\n0\n3\n4\n\n\n6\nDuster 360\n14.3\n8\n360.0\n245\n3.21\n3.570\n15.84\n0\n0\n3\n4\n\n\n16\nChrysler Imperial\n14.7\n8\n440.0\n230\n3.23\n5.345\n17.42\n0\n0\n3\n4\n\n\n\n\n\n\n\n\n\n5) 데이터 변경 (조건문)\n\nimport numpy as np \ndf = pd.read_csv(\"mtcars.txt\")\n# np.where 활용\n# hp 변수 값 중에서 205가 넘는 값은 205로 처리하고, 나머지는 그대로 유지\ndf['hp'] = np.where(df['hp'] &gt; 205, 205, df['hp'])\n\n# 내림차순 정렬 (위에서부터 내려간다)\ndf.sort_values('hp', ascending=False).head(10)\n\n# 활용 : 이상치를 Max 값이나 Min 값으로 대체할 경우 조건문 활용\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n16\nChrysler Imperial\n14.7\n8\n440.0\n205\n3.23\n5.345\n17.42\n0\n0\n3\n4\n\n\n30\nMaserati Bora\n15.0\n8\n301.0\n205\n3.54\n3.570\n14.60\n0\n1\n5\n8\n\n\n28\nFord Pantera L\n15.8\n8\n351.0\n205\n4.22\n3.170\n14.50\n0\n1\n5\n4\n\n\n6\nDuster 360\n14.3\n8\n360.0\n205\n3.21\n3.570\n15.84\n0\n0\n3\n4\n\n\n23\nCamaro Z28\n13.3\n8\n350.0\n205\n3.73\n3.840\n15.41\n0\n0\n3\n4\n\n\n15\nLincoln Continental\n10.4\n8\n460.0\n205\n3.00\n5.424\n17.82\n0\n0\n3\n4\n\n\n14\nCadillac Fleetwood\n10.4\n8\n472.0\n205\n2.93\n5.250\n17.98\n0\n0\n3\n4\n\n\n13\nMerc 450SLC\n15.2\n8\n275.8\n180\n3.07\n3.780\n18.00\n0\n0\n3\n3\n\n\n11\nMerc 450SE\n16.4\n8\n275.8\n180\n3.07\n4.070\n17.40\n0\n0\n3\n3\n\n\n12\nMerc 450SL\n17.3\n8\n275.8\n180\n3.07\n3.730\n17.60\n0\n0\n3\n3"
  },
  {
    "objectID": "BigData_Analysis/실기_파이썬기초.html#결측치-이상치-중복값-처리제거-or-대체",
    "href": "BigData_Analysis/실기_파이썬기초.html#결측치-이상치-중복값-처리제거-or-대체",
    "title": "빅분기 실기 - 파이썬 기초",
    "section": "4. 결측치, 이상치, 중복값 처리(제거 or 대체)",
    "text": "4. 결측치, 이상치, 중복값 처리(제거 or 대체)\n\n🚢 데이터 불러오기 (타이타닉 데이터셋)\n\n종속변수(y) : 생존 여부 (0 사망, 1 생존)\n\n\n독립변수(x) : pclass, sex, age 등의 탑승자 정보(변수)\n\n\nimport seaborn as sns\n# 데이터셋 목록 : sns.get_dataset_names()\n# 타이타닉 데이터 불러오기\ndf = sns.load_dataset('titanic')\n\n\ndf.head()\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n\n\n\n\n\n\ndf.shape\n\n(891, 15)\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 15 columns):\n #   Column       Non-Null Count  Dtype   \n---  ------       --------------  -----   \n 0   survived     891 non-null    int64   \n 1   pclass       891 non-null    int64   \n 2   sex          891 non-null    object  \n 3   age          714 non-null    float64 \n 4   sibsp        891 non-null    int64   \n 5   parch        891 non-null    int64   \n 6   fare         891 non-null    float64 \n 7   embarked     889 non-null    object  \n 8   class        891 non-null    category\n 9   who          891 non-null    object  \n 10  adult_male   891 non-null    bool    \n 11  deck         203 non-null    category\n 12  embark_town  889 non-null    object  \n 13  alive        891 non-null    object  \n 14  alone        891 non-null    bool    \ndtypes: bool(2), category(2), float64(2), int64(4), object(5)\nmemory usage: 80.7+ KB\n\n\n\n\n1) 결측치 확인 및 처리\n\n# 결측치 확인 \ndf.isnull().sum()\n\nsurvived         0\npclass           0\nsex              0\nage            177\nsibsp            0\nparch            0\nfare             0\nembarked         2\nclass            0\nwho              0\nadult_male       0\ndeck           688\nembark_town      2\nalive            0\nalone            0\ndtype: int64\n\n\n\n# 결측치 제거\nprint(df.dropna(axis=0).shape) # 행 기준, default는 axis = 0 따로 설정 안해도 됨\nprint(df.dropna(axis=1).shape) # 열 기준\n\n(182, 15)\n(891, 11)\n\n\n\n# 결측치 대체\n# 데이터 복사 \ndf2 = df.copy()\ndf2 = pd.DataFrame(df) # df를 데이터 프레임 형태로 변환\n\n\n# 1. 중앙값/평균값 등으로 대체\n\n# 먼저 중앙값을 구합니다\nmedian_age = df2['age'].median()\nprint(median_age)\n\n# 평균으로 대체할 경우 \n# mean_age = df2['age'].mean()\n\n28.0\n\n\n\n# 구한 중앙값으로 결측치를 대체합니다.\ndf2['age'] = df['age'].fillna(median_age)\n\n\n# 결측치가 잘 대체되었는지 확인합니다\ndf2.isnull().sum()\n\nsurvived         0\npclass           0\nsex              0\nage              0\nsibsp            0\nparch            0\nfare             0\nembarked         2\nclass            0\nwho              0\nadult_male       0\ndeck           688\nembark_town      2\nalive            0\nalone            0\ndtype: int64\n\n\n\nprint(df['age'].mean()) # 원본 데이터\nprint(df2['age'].mean()) # 중앙값으로 대체한 데이터 \n\n29.69911764705882\n29.36158249158249\n\n\n\n# 중복값 확인\ndf.drop_duplicates().shape\n\n(784, 15)\n\n\n\n\n2) 이상치 확인 및 처리\n\n\n✔ 상자그림 활용 (이상치: Q1, Q3로부터 1.5*IQR을 초과하는 값)\n\n# 타이타닉 데이터 불러오기\ndf = sns.load_dataset('titanic')\n\n# (참고) 상자그림\nsns.boxplot(df['age'])\n\nC:\\Users\\Hyunsoo Kim\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n  warnings.warn(\n\n\n&lt;AxesSubplot:xlabel='age'&gt;\n\n\n\n\n\n\n# Q1, Q3, IQR 구하기\nQ1 = df['age'].quantile(.25)\nQ3 = df['age'].quantile(.75)\nIQR = Q3 - Q1\nprint(Q1, Q3, IQR)\n\n20.125 38.0 17.875\n\n\n\nupper = Q3 + 1.5*IQR\nlower = Q1 - 1.5*IQR\nprint(upper, lower)\n\n64.8125 -6.6875\n\n\n\n# 문제 : age 변수의 이상치를 제외한 데이터 수는? (상자그림 기준)\ncond1 = (df['age'] &lt;= upper)\ncond2 = (df['age'] &gt;= lower)\nprint(len(df[cond1 & cond2]))\nprint(len(df[cond1]))\nprint(len(df))\n\n703\n703\n891\n\n\n\n# 문제 : age 변수의 이상치를 제외한 데이터셋 확인(상자그림 기준)\ndf_new = df[cond1 & cond2]\ndf_new\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n885\n0\n3\nfemale\n39.0\n0\n5\n29.1250\nQ\nThird\nwoman\nFalse\nNaN\nQueenstown\nno\nFalse\n\n\n886\n0\n2\nmale\n27.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n887\n1\n1\nfemale\n19.0\n0\n0\n30.0000\nS\nFirst\nwoman\nFalse\nB\nSouthampton\nyes\nTrue\n\n\n889\n1\n1\nmale\n26.0\n0\n0\n30.0000\nC\nFirst\nman\nTrue\nC\nCherbourg\nyes\nTrue\n\n\n890\n0\n3\nmale\n32.0\n0\n0\n7.7500\nQ\nThird\nman\nTrue\nNaN\nQueenstown\nno\nTrue\n\n\n\n\n703 rows × 15 columns\n\n\n\n\n\n✔ 표준정규분포 활용(이상치 : \\(\\pm\\) 3Z 값을 넘어가는 값)\n\n# 데이터 표준화, Z = (개별값 -  평균) / 표준편차\n\n\nmean_age = df['age'].mean()\nstd_age = df['age'].std()\nprint(mean_age)\nprint(std_age)\n\n29.69911764705882\n14.526497332334044\n\n\n\nznorm = (df['age']-mean_age) / std_age\nznorm\n\n0     -0.530005\n1      0.571430\n2     -0.254646\n3      0.364911\n4      0.364911\n         ...   \n886   -0.185807\n887   -0.736524\n888         NaN\n889   -0.254646\n890    0.158392\nName: age, Length: 891, dtype: float64\n\n\n\n# 문제 : 이상치의 개수는 몇개인가? (: ±3Z 기준)\n\n\ncond1 = (znorm &gt; 3)\nlen(df[cond1])\n\n2\n\n\n\ncond2 = (znorm &lt; -3)\nlen(df[cond2])\n\n0\n\n\n\nprint(len(df[cond1]) + len(df[cond2]))\n\n2\n\n\n\n\n3) 중복값 제거\n\n# 데이터 불러오기\ndf = sns.load_dataset('titanic')\n\n\ndf.shape\n\n(891, 15)\n\n\n\ndf1 = df.copy()\ndf1 = df1.drop_duplicates()\nprint(df1.shape)\n# (주의) 예제에서는 중복값이 있어서 제거했지만,\n# 중복값이 나올 수 있는 상황이변 제거할 필요없음\n\n(784, 15)"
  },
  {
    "objectID": "BigData_Analysis/실기_파이썬기초.html#데이터-scaling데이터-표준화-정규화",
    "href": "BigData_Analysis/실기_파이썬기초.html#데이터-scaling데이터-표준화-정규화",
    "title": "빅분기 실기 - 파이썬 기초",
    "section": "✅ 5. 데이터 scaling(데이터 표준화, 정규화)",
    "text": "✅ 5. 데이터 scaling(데이터 표준화, 정규화)\n\n1) 데이터 표준화(Z-score normalization)\n\ndf = pd.read_csv(\"mtcars.txt\")\ndf.head()\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\nMazda RX4\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\nHornet 4 Drive\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n4\nHornet Sportabout\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\nzscaler = StandardScaler() # 변수명은 사용하기 편한 변수명으로 사용\ndf['mpg'] = zscaler.fit_transform(df[['mpg']])\ndf.head()\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\nMazda RX4\n0.153299\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\nMazda RX4 Wag\n0.153299\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\nDatsun 710\n0.456737\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\nHornet 4 Drive\n0.220730\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n4\nHornet Sportabout\n-0.234427\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n\n\n\n\n# 확인\nprint(df['mpg'].mean(), df['mpg'].std())\n\n-5.48172618408671e-16 1.016001016001524\n\n\n\n\n2) 데이터 정규화(min-max normalization)\n\ndf = pd.read_csv(\"mtcars.txt\")\ndf.head\n\nfrom sklearn.preprocessing import MinMaxScaler\nmscaler = MinMaxScaler()\ndf['mpg'] = mscaler.fit_transform(df[['mpg']])\ndf.head()\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\nMazda RX4\n0.451064\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\nMazda RX4 Wag\n0.451064\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\nDatsun 710\n0.527660\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\nHornet 4 Drive\n0.468085\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n4\nHornet Sportabout\n0.353191\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n\n\n\n\n# 확인\nprint(df['mpg'].min(), df['mpg'].max())\n\n0.0 1.0"
  },
  {
    "objectID": "BigData_Analysis/실기_파이썬기초.html#데이터-합치기",
    "href": "BigData_Analysis/실기_파이썬기초.html#데이터-합치기",
    "title": "빅분기 실기 - 파이썬 기초",
    "section": "6. 데이터 합치기",
    "text": "6. 데이터 합치기\n\n# 행, 열 방향으로 데이터 합치기\ndf = sns.load_dataset('iris')\ndf.head()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\n# 데이터 2개로 분리\ndf1 = df.loc[0:30, ] # 0~30행 데이터\ndf2 = df.loc[31:60, ] # 31~60행 데이터 \n\n\ndf1.head()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\ndf2.head()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n31\n5.4\n3.4\n1.5\n0.4\nsetosa\n\n\n32\n5.2\n4.1\n1.5\n0.1\nsetosa\n\n\n33\n5.5\n4.2\n1.4\n0.2\nsetosa\n\n\n34\n4.9\n3.1\n1.5\n0.2\nsetosa\n\n\n35\n5.0\n3.2\n1.2\n0.2\nsetosa\n\n\n\n\n\n\n\n\ndf_sum = pd.concat([df1, df2], axis=0) # 행 방향으로 결합 (위, 아래)\nprint(df_sum.head())\nprint(df_sum.shape)\n\n   sepal_length  sepal_width  petal_length  petal_width species\n0           5.1          3.5           1.4          0.2  setosa\n1           4.9          3.0           1.4          0.2  setosa\n2           4.7          3.2           1.3          0.2  setosa\n3           4.6          3.1           1.5          0.2  setosa\n4           5.0          3.6           1.4          0.2  setosa\n(61, 5)\n\n\n\n# 데이터 2개로 나누기\ndf1 = df.loc[:, 'sepal_length':'petal_length'] # 1~3열 추출 데이터\ndf2 = df.loc[:, ['petal_width','species']] # 4~5열 추출 데이터\n\n\ndf_sum = pd.concat([df1, df2], axis=1) # 열 방향으로 결합 (좌, 우)\ndf_sum.head()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa"
  },
  {
    "objectID": "BigData_Analysis/실기_파이썬기초.html#날짜시간-데이터-index-다루기",
    "href": "BigData_Analysis/실기_파이썬기초.html#날짜시간-데이터-index-다루기",
    "title": "빅분기 실기 - 파이썬 기초",
    "section": "7. 날짜/시간 데이터, index 다루기",
    "text": "7. 날짜/시간 데이터, index 다루기\n\n1) 날짜 다루기\n\n# 데이터 만들기\ndf = pd.DataFrame({\n    '날짜' : ['20230105','20230105','20230223','20230223',\n            '20230312','20230422','20230511'],\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600] })\ndf\n\n\n\n\n\n\n\n\n날짜\n물품\n판매수\n개당수익\n\n\n\n\n0\n20230105\nA\n5\n500\n\n\n1\n20230105\nB\n10\n600\n\n\n2\n20230223\nA\n15\n500\n\n\n3\n20230223\nB\n15\n600\n\n\n4\n20230312\nA\n20\n600\n\n\n5\n20230422\nB\n25\n700\n\n\n6\n20230511\nA\n40\n600\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 7 entries, 0 to 6\nData columns (total 4 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   날짜      7 non-null      object\n 1   물품      7 non-null      object\n 2   판매수     7 non-null      int64 \n 3   개당수익    7 non-null      int64 \ndtypes: int64(2), object(2)\nmemory usage: 352.0+ bytes\n\n\n\n# 데이터 타입 datetime으로 변경\ndf['날짜'] = pd.to_datetime(df['날짜'])\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 7 entries, 0 to 6\nData columns (total 4 columns):\n #   Column  Non-Null Count  Dtype         \n---  ------  --------------  -----         \n 0   날짜      7 non-null      datetime64[ns]\n 1   물품      7 non-null      object        \n 2   판매수     7 non-null      int64         \n 3   개당수익    7 non-null      int64         \ndtypes: datetime64[ns](1), int64(2), object(1)\nmemory usage: 352.0+ bytes\n\n\n\n# 년, 월, 일 변수(열) 추가하기\ndf['year'] = df['날짜'].dt.year\ndf['month'] = df['날짜'].dt.month\ndf['day'] = df['날짜'].dt.day\ndf\n\n\n\n\n\n\n\n\n날짜\n물품\n판매수\n개당수익\nyear\nmonth\nday\n\n\n\n\n0\n2023-01-05\nA\n5\n500\n2023\n1\n5\n\n\n1\n2023-01-05\nB\n10\n600\n2023\n1\n5\n\n\n2\n2023-02-23\nA\n15\n500\n2023\n2\n23\n\n\n3\n2023-02-23\nB\n15\n600\n2023\n2\n23\n\n\n4\n2023-03-12\nA\n20\n600\n2023\n3\n12\n\n\n5\n2023-04-22\nB\n25\n700\n2023\n4\n22\n\n\n6\n2023-05-11\nA\n40\n600\n2023\n5\n11\n\n\n\n\n\n\n\n\n# 날짜 구간 필터링\ndf[df['날짜'].between('2023-01-01', '2023-01-31')] # 1월 31일은 미포함\n\n\n\n\n\n\n\n\n날짜\n물품\n판매수\n개당수익\nyear\nmonth\nday\n\n\n\n\n0\n2023-01-05\nA\n5\n500\n2023\n1\n5\n\n\n1\n2023-01-05\nB\n10\n600\n2023\n1\n5\n\n\n\n\n\n\n\n\n# 날짜를 인덱스로 설정후 loc 함수 사용\n# 데이터 만들기 \ndf = pd.DataFrame({\n    '날짜' : ['20230105','20230105','20230223','20230223',\n            '20230312','20230422','20230511'],\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600] })\n\n# 데이터 타입 datetime으로 변경(필수)\ndf['날짜'] = pd.to_datetime(df['날짜'])\n\ndf = df.set_index('날짜', drop=True) # drop=True(디폴트) or False\ndf.head(3)\n\n\n\n\n\n\n\n\n물품\n판매수\n개당수익\n\n\n날짜\n\n\n\n\n\n\n\n2023-01-05\nA\n5\n500\n\n\n2023-01-05\nB\n10\n600\n\n\n2023-02-23\nA\n15\n500\n\n\n\n\n\n\n\n\nprint(df.loc['2023-01-05':'2023-02-23']) # 둘다 기간 포함\nprint(df.loc[ (df.index&gt;='2023-01-05') & (df.index&lt;='2023-02-23') ])\n\n           물품  판매수  개당수익\n날짜                      \n2023-01-05  A    5   500\n2023-01-05  B   10   600\n2023-02-23  A   15   500\n2023-02-23  B   15   600\n           물품  판매수  개당수익\n날짜                      \n2023-01-05  A    5   500\n2023-01-05  B   10   600\n2023-02-23  A   15   500\n2023-02-23  B   15   600\n\n\n\n\n2) 시간 다루기\n\n# 시간 데이터 만들기 \ndf = pd.DataFrame({\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600] })\ntime = pd.date_range('2023-09-24 12:25:00', '2023-09-25 14:45:30', periods=7)\ndf['time'] = time\ndf = df[['time','물품','판매수','개당수익']]\ndf\n\n\n\n\n\n\n\n\ntime\n물품\n판매수\n개당수익\n\n\n\n\n0\n2023-09-24 12:25:00\nA\n5\n500\n\n\n1\n2023-09-24 16:48:25\nB\n10\n600\n\n\n2\n2023-09-24 21:11:50\nA\n15\n500\n\n\n3\n2023-09-25 01:35:15\nB\n15\n600\n\n\n4\n2023-09-25 05:58:40\nA\n20\n600\n\n\n5\n2023-09-25 10:22:05\nB\n25\n700\n\n\n6\n2023-09-25 14:45:30\nA\n40\n600\n\n\n\n\n\n\n\n\n# index 초기화 (인덱스를 컬럼으로)\n# df = df.reset_index()\n# df\n\n\n# index 새로 지정\ndf = df.set_index('time')\ndf\n\n\n\n\n\n\n\n\n물품\n판매수\n개당수익\n\n\ntime\n\n\n\n\n\n\n\n2023-09-24 12:25:00\nA\n5\n500\n\n\n2023-09-24 16:48:25\nB\n10\n600\n\n\n2023-09-24 21:11:50\nA\n15\n500\n\n\n2023-09-25 01:35:15\nB\n15\n600\n\n\n2023-09-25 05:58:40\nA\n20\n600\n\n\n2023-09-25 10:22:05\nB\n25\n700\n\n\n2023-09-25 14:45:30\nA\n40\n600\n\n\n\n\n\n\n\n\n# 시간 데이터 다루기(주의: 시간이 index에 위치해야 함)\ndf.between_time(start_time='12:25', end_time='21:00') #시간 시작, 끝 모두 포함\n# include_start=False, include_end=False 옵션을 시작, 끝 시간 제외 가능\n\n\n\n\n\n\n\n\n물품\n판매수\n개당수익\n\n\ntime\n\n\n\n\n\n\n\n2023-09-24 12:25:00\nA\n5\n500\n\n\n2023-09-24 16:48:25\nB\n10\n600\n\n\n2023-09-25 14:45:30\nA\n40\n600\n\n\n\n\n\n\n\n\n# 날짜를 인덱스로 설정후 loc 함수 사용\nprint(df.loc['2023-09-24 12:25:00':'2023-09-24 21:11:50']) # 둘다 포함\nprint(df.loc[ (df.index&gt;='2023-09-24 12:25:00') & (df.index&lt;='2023-09-24 21:11:50') ])\n\n                    물품  판매수  개당수익\ntime                             \n2023-09-24 12:25:00  A    5   500\n2023-09-24 16:48:25  B   10   600\n2023-09-24 21:11:50  A   15   500\n                    물품  판매수  개당수익\ntime                             \n2023-09-24 12:25:00  A    5   500\n2023-09-24 16:48:25  B   10   600\n2023-09-24 21:11:50  A   15   500"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_5.html",
    "href": "BigData_Analysis/실기_3유형_5.html",
    "title": "빅분기 실기 - 3유형 문제풀이(5)",
    "section": "",
    "text": "실기 3 유형(5)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_5.html#다중회귀분석",
    "href": "BigData_Analysis/실기_3유형_5.html#다중회귀분석",
    "title": "빅분기 실기 - 3유형 문제풀이(5)",
    "section": "✅ 다중회귀분석",
    "text": "✅ 다중회귀분석\n\nimport pandas as pd\nimport numpy as np\n\n\n당뇨병 환자의 질병 진행정도 데이터셋\n\n###############  실기환경 복사 영역  ###############\nimport pandas as pd\nimport numpy as np\n# 실기 시험 데이터셋으로 셋팅하기 (수정금지)\nfrom sklearn.datasets import load_diabetes\n# diabetes 데이터셋 로드\ndiabetes = load_diabetes()\nx = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\ny = pd.DataFrame(diabetes.target)\ny.columns = ['target'] \n###############  실기환경 복사 영역  ###############\n\n\n# 데이터 설명\nprint(diabetes.DESCR)\n\n.. _diabetes_dataset:\n\nDiabetes dataset\n----------------\n\nTen baseline variables, age, sex, body mass index, average blood\npressure, and six blood serum measurements were obtained for each of n =\n442 diabetes patients, as well as the response of interest, a\nquantitative measure of disease progression one year after baseline.\n\n**Data Set Characteristics:**\n\n  :Number of Instances: 442\n\n  :Number of Attributes: First 10 columns are numeric predictive values\n\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n\n  :Attribute Information:\n      - age     age in years\n      - sex\n      - bmi     body mass index\n      - bp      average blood pressure\n      - s1      tc, total serum cholesterol\n      - s2      ldl, low-density lipoproteins\n      - s3      hdl, high-density lipoproteins\n      - s4      tch, total cholesterol / HDL\n      - s5      ltg, possibly log of serum triglycerides level\n      - s6      glu, blood sugar level\n\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n\nSource URL:\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n\nFor more information see:\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n\n\n\n\n1. sklearn 라이브러리 활용\n\n# sklearn 라이브러리 활용\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\n\n# 독립변수와 종속변수 설정\nx = x[['age','sex','bmi']]\nprint(x.head())\nprint(y.head())\n\n        age       sex       bmi\n0  0.038076  0.050680  0.061696\n1 -0.001882 -0.044642 -0.051474\n2  0.085299  0.050680  0.044451\n3 -0.089063 -0.044642 -0.011595\n4  0.005383 -0.044642 -0.036385\n----------------------------------\n   target\n0   151.0\n1    75.0\n2   141.0\n3   206.0\n4   135.0\n\n\n\n회귀식 : y = b0 + b1x1 + b2x2 + b3x3\n(x1=age, x2=sex, x3=bmi)\n\n\n# 모델링 \nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(x, y)\n\nLinearRegression()\n\n\n\n# 회귀분석 관련 지표 출력\n\n# 1. Rsq(결정계수) : model.score(x,y)\nmodel.score(x, y)\nprint(round(model.score(x,y),2))\n\n0.35\n\n\n\n# 2. 회귀계수 출력 : model.coef_\nprint(np.round(model.coef_, 2))        # 전체 회귀계수\nprint(np.round(model.coef_[0,0], 2))   # x1의 회귀계수\nprint(np.round(model.coef_[0,1], 2))   # x2의 회귀계수\nprint(np.round(model.coef_[0,2], 2))   # x3의 회귀계수\n\n[[138.9  -36.14 926.91]]\n138.9\n-36.14\n926.91\n\n\n\n# 3. 회귀계수(절편) : model.intercept_\nprint(np.round(model.intercept_, 2))\n\n[152.13]\n\n\n\n회귀식 : y = b0 + b1x1 + b2x2 + b3x3\n(x1=age, x2=sex, x3=bmi)\n\n\n\n결과 : 152.13 + 138.9age -36.14sex + 926.91bmi"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_5.html#statsmodels-라이브러리-사용",
    "href": "BigData_Analysis/실기_3유형_5.html#statsmodels-라이브러리-사용",
    "title": "빅분기 실기 - 3유형 문제풀이(5)",
    "section": "2. statsmodels 라이브러리 사용",
    "text": "2. statsmodels 라이브러리 사용\n\n###############  실기환경 복사 영역  ###############\nimport pandas as pd\nimport numpy as np\n# 실기 시험 데이터셋으로 셋팅하기 (수정금지)\nfrom sklearn.datasets import load_diabetes\n# diabetes 데이터셋 로드\ndiabetes = load_diabetes()\nx = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\ny = pd.DataFrame(diabetes.target)\ny.columns = ['target'] \n###############  실기환경 복사 영역  ###############\n\n\n# statsmodel.formula 활용\nimport statsmodels.api as sm\n# 독립변수와 종속변수 설정\nx = x[['age','sex','bmi']]\ny = y[['target']]\nprint(x.head())\nprint(y.head())\n\n        age       sex       bmi\n0  0.038076  0.050680  0.061696\n1 -0.001882 -0.044642 -0.051474\n2  0.085299  0.050680  0.044451\n3 -0.089063 -0.044642 -0.011595\n4  0.005383 -0.044642 -0.036385\n   target\n0   151.0\n1    75.0\n2   141.0\n3   206.0\n4   135.0\n\n\n\n# 모델링\nimport statsmodels.api as sm\n\nx = sm.add_constant(x)        # 주의 : 상수항 추가해줘야 함\nmodel = sm.OLS(y, x).fit()\n# y_pred = model.predict(x)\nsummary = model.summary()\nprint(summary)\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 target   R-squared:                       0.351\nModel:                            OLS   Adj. R-squared:                  0.346\nMethod:                 Least Squares   F-statistic:                     78.94\nDate:                Wed, 29 Nov 2023   Prob (F-statistic):           7.77e-41\nTime:                        14:29:14   Log-Likelihood:                -2451.6\nNo. Observations:                 442   AIC:                             4911.\nDf Residuals:                     438   BIC:                             4928.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        152.1335      2.964     51.321      0.000     146.307     157.960\nage          138.9039     64.254      2.162      0.031      12.618     265.189\nsex          -36.1353     63.391     -0.570      0.569    -160.724      88.453\nbmi          926.9120     63.525     14.591      0.000     802.061    1051.763\n==============================================================================\nOmnibus:                       14.687   Durbin-Watson:                   1.851\nProb(Omnibus):                  0.001   Jarque-Bera (JB):                8.290\nSkew:                           0.150   Prob(JB):                       0.0158\nKurtosis:                       2.400   Cond. No.                         23.7\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# 1. Rsq(결정계수)\n# r2 = 0.351\n\n# 2. 회귀계수\n# age = 138.9039\n# sex = -36.1353\n# bmi = 926.9120\n\n# 3. 회귀계수(절편)\n# const = 152.1335\n\n# 4. 회귀식 p-value\n# pvalue = 7.77e-41 # 0에 가까운 작은 값"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_5.html#결과-비교해보기-두-라이브러리-모두-같은-결과값을-출력",
    "href": "BigData_Analysis/실기_3유형_5.html#결과-비교해보기-두-라이브러리-모두-같은-결과값을-출력",
    "title": "빅분기 실기 - 3유형 문제풀이(5)",
    "section": "(결과 비교해보기) 두 라이브러리 모두 같은 결과값을 출력",
    "text": "(결과 비교해보기) 두 라이브러리 모두 같은 결과값을 출력\n\n회귀식 : y = b0 + b1x1 + b2x2 + b3x3\n(x1=age, x2=sex, x3=bmi)\n\n\n1. sklearn : y = 152.13 + 138.9age -36.14sex + 926.91bmi\n\n\n2. statsmodel : y = 152.13 + 138.9age -36.14sex + 926.91bmi"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_5.html#상관분석",
    "href": "BigData_Analysis/실기_3유형_5.html#상관분석",
    "title": "빅분기 실기 - 3유형 문제풀이(5)",
    "section": "✅ 상관분석",
    "text": "✅ 상관분석\n\n###############  실기환경 복사 영역  ###############\nimport pandas as pd\nimport numpy as np\n# 실기 시험 데이터셋으로 셋팅하기 (수정금지)\nfrom sklearn.datasets import load_diabetes\n# diabetes 데이터셋 로드\ndiabetes = load_diabetes()\nx = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\ny = pd.DataFrame(diabetes.target)\ny.columns = ['target'] \n###############  실기환경 복사 영역  ###############\n\n\n# 상관분석을 할 2가지 변수 설정\nx = x['bmi']\ny = y['target']\nprint(x.head())\nprint(y.head())\n\n0    0.061696\n1   -0.051474\n2    0.044451\n3   -0.011595\n4   -0.036385\nName: bmi, dtype: float64\n0    151.0\n1     75.0\n2    141.0\n3    206.0\n4    135.0\nName: target, dtype: float64\n\n\n\n# 라이브러리 불러오기\nfrom scipy.stats import pearsonr\n\n# 상관계수에 대한 검정실시\nr, pvalue = pearsonr(x, y)\n\n# 가설검정\n# H0 : 두 변수간 선형관계가 존재하지 않는다 (p=0)\n# H1 : 두 변수간 선형관계가 존재한다 (p!=0)\n\n# 1. 상관계수\nprint(round(r,2))\n\n# 2. p-value\nprint(round(pvalue,2))\n\n# 3. 검정통계량\n# 통계량은 별로돌 구해야 함 (T = r*root(n-2) / root(1-2r))\n# r = 상관계수\n# n = 데이터의 개수\n\nn = len(x) # 데이터 수\nr2 = r**2  # 상관꼐수의 제곱  \nstatistic = r * ((n-2)**0.5) / ((1-r2)**0.5)\n\nprint(round(statistic,2)) # 통계량값이 크면 p-value 값이 작아진다\n\n# 4. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값이 0.05보다 작기 떄문에 귀무가설을 기각한다.\n# 즉, 두 변수간 선형관계가 존재한다고 할 수 있다.(상관계수가 0이 아니다)\n\n# 답 : 기각\n\n0.59\n0.0\n15.19"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_3.html",
    "href": "BigData_Analysis/실기_3유형_3.html",
    "title": "빅분기 실기 - 3유형 문제풀이(3)",
    "section": "",
    "text": "실기 3 유형(3)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_3.html#검정방법",
    "href": "BigData_Analysis/실기_3유형_3.html#검정방법",
    "title": "빅분기 실기 - 3유형 문제풀이(3)",
    "section": "✅ 검정방법",
    "text": "✅ 검정방법"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_3.html#분산분석anova-a집단-vs-b집단-vs-c집단-vs",
    "href": "BigData_Analysis/실기_3유형_3.html#분산분석anova-a집단-vs-b집단-vs-c집단-vs",
    "title": "빅분기 실기 - 3유형 문제풀이(3)",
    "section": "1. 분산분석(ANOVA) : A집단 vs B집단 vs C집단 vs …",
    "text": "1. 분산분석(ANOVA) : A집단 vs B집단 vs C집단 vs …\n\n(정규성o) ANOVA 분석\n(정규성x) 크루스칼-왈리스 검정(kruskal-wallis test)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_3.html#가설검정-순서중요",
    "href": "BigData_Analysis/실기_3유형_3.html#가설검정-순서중요",
    "title": "빅분기 실기 - 3유형 문제풀이(3)",
    "section": "✅ 가설검정 순서(중요!!)",
    "text": "✅ 가설검정 순서(중요!!)\n\n\n가설검정\n유의수준 확인\n정규성 검정 (주의) 집단 모두 정규성을 따를 경우!\n등분산 검정\n검정실시(통계량, p-value 확인) (주의) 등분산여부 확인\n귀무가설 기각여부 결정(채택/기각)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_3.html#예제문제",
    "href": "BigData_Analysis/실기_3유형_3.html#예제문제",
    "title": "빅분기 실기 - 3유형 문제풀이(3)",
    "section": "✅ 예제문제",
    "text": "✅ 예제문제\n\n문제 1-1\n다음은 A, B, C 그룹 인원 성적 데이터이다.\n세 그룹의 성적 평균이 같다고 할 수 있는지 ANOVA 분석을 실시하시오.\n(유의수준 5%)\n\nA, B, C : 각 그룹 인원의 성적\nH0(귀무가설) : A(평균) = B(평균) = C(평균)\nH1(대립가설) : Not H0 (적어도 하나는 같지 않다)\n\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom scipy.stats import shapiro\n\n\n# 데이터 만들기\ndf = pd.DataFrame( {\n    'A': [120, 135, 122, 124, 135, 122, 145, 160, 155, 142, 144, 135, 167],\n    'B': [110, 132, 123, 119, 123, 115, 140, 162, 142, 138, 135, 142, 160],\n    'C': [130, 120, 115, 122, 133, 144, 122, 120, 110, 134, 125, 122, 122]})\nprint(df.head(3))\n\n     A    B    C\n0  120  110  130\n1  135  132  120\n2  122  123  115\n\n\n\n# 1. 가설검정\n# H0 : 세 그룹 성적의 평균값이 같다. (A(평균) = B(평균) = C(평균))\n# H1 : 세 그룹의 성적 평균값이 적어도 하나는 같지 않다. (not H0)\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3. 정규성 검정\nprint(stats.shapiro(df['A']))\nprint(stats.shapiro(df['B']))\nprint(stats.shapiro(df['C']))\n\n# statistic, pvalue = stats.shapiro(df['A'])\n# print(round(statistic,4), round(pvalue,4))\n\nShapiroResult(statistic=0.9314376711845398, pvalue=0.35585272312164307)\nShapiroResult(statistic=0.9498201012611389, pvalue=0.5955665707588196)\nShapiroResult(statistic=0.9396706223487854, pvalue=0.45265132188796997)\n\n\n\n세 집단 모두 p-value 값이 유의수준(0.05)보다 크다\n귀무가설(H0) 채택 =&gt; 정규분포를 따른다고 할 수 있다.\n만약 하나라도 정규분포를 따르지 않는다면 비모수 검정방법을 써야함\n(크루스칼-왈리스 검정)\n\n\n# 4. 등분산성 검정\n# H0(귀무가설) : 등분산 한다\n# H1(대립가설) : 등분산 하지 않는다\nprint(stats.bartlett(df['A'], df['B'], df['C']))\n\nBartlettResult(statistic=4.222248448848066, pvalue=0.12110174433684852)\n\n\n\np-value 값이 유의수준(0.05)보다 크다\n귀무가설(H0) 채택 =&gt; 등분산 한다고 할 수 있다.\n\n\n# 5.1 (정규성o, 등분산성 o) 분산분석(F_oneway)\nimport scipy.stats as stats\nstatistic, pvalue = stats.f_oneway(df['A'], df['B'], df['C'])\n# 주의 : 데이터가 각각 들어가야 함(밑에 예제와 비교해볼 것)\n\nprint(round(statistic,4), round(pvalue,4))\n\n3.6971 0.0346\n\n\n\n# 5.2 (정규성o, 등분산성 x) Welch_ANOVA 분석\n# import pingouin as pg     # pingouin 패키지 미지원\n# pg.welch_anova(dv = \"그룹변수명\", between=\"성적데이터\", data=데이터)\n# pg.welch_anova(df['A'], df['B'], df['C']) 형태로 분석불가\n\n\n# 5.3 (정규성x, 등분산성 x) 크루스 왈리스 검정\nimport scipy.stats as stats\nstatistic, pvalue = stats.kruskal(df['A'], df['B'], df['C'])\n\nprint(round(statistic,4), round(pvalue,4))\n\n6.897 0.0318\n\n\n\n# 6. 귀무가설 기각 여부 결정(채택/기각)\n# p-value 값(0.0346)이 0.05보다 작기 떄문에 귀무가설을 기각한다\n# 즉, A, B, C 그룹의 성적 평균이 같다고 할 수 없다.\n\n# 답 : 기각(H1)\n\n\n\n문제 1-2 데이터 형태가 다른 경우\n\n# 데이터 만들기\ndf2 = pd.DataFrame( {\n    '항목': ['A','A','A','A','A','A','A','A','A','A','A','A','A',\n           'B','B','B','B','B','B','B','B','B','B','B','B','B',\n           'C','C','C','C','C','C','C','C','C','C','C','C','C',],\n    'value': [120, 135, 122, 124, 135, 122, 145, 160, 155, 142, 144, 135, 167,\n             110, 132, 123, 119, 123, 115, 140, 162, 142, 138, 135, 142, 160,\n             130, 120, 115, 122, 133, 144, 122, 120, 110, 134, 125, 122, 122]\n    })\nprint(df2.head(3))\n\n  항목  value\n0  A    120\n1  A    135\n2  A    122\n\n\n\n# 각각 필터링해서 변수명에 저장하고 분석 진행\na = df2[ df2['항목']=='A' ]['value']\nb = df2[ df2['항목']=='B' ]['value']\nc = df2[ df2['항목']=='C' ]['value']\n\n\n# 분산분석(F_oneway)\nimport scipy.stats as stats\nstatistic, pvalue = stats.f_oneway(a, b, c)\nprint(round(statistic,4), round(pvalue,4))\n\n3.6971 0.0346"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_1.html",
    "href": "BigData_Analysis/실기_3유형_1.html",
    "title": "빅분기 실기 - 3유형 문제풀이(1)",
    "section": "",
    "text": "실기 3 유형(1)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_1.html#검정방법",
    "href": "BigData_Analysis/실기_3유형_1.html#검정방법",
    "title": "빅분기 실기 - 3유형 문제풀이(1)",
    "section": "✅ 검정방법",
    "text": "✅ 검정방법\n\n1) (정규성o) 단일표본 t검정(1sample t-test)\n\n\n2) (정규성x) 윌콕슨 부호순위 검정"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_1.html#가설검정-순서중요",
    "href": "BigData_Analysis/실기_3유형_1.html#가설검정-순서중요",
    "title": "빅분기 실기 - 3유형 문제풀이(1)",
    "section": "✅ 가설검정 순서(중요!!)",
    "text": "✅ 가설검정 순서(중요!!)\n\n\n가설검정\n유의수준 확인\n정규성 검정\n검정실시(통계량, p-value 확인)\n귀무가설 기각여부 결정(채택/기각)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_1.html#데이터-불러오기",
    "href": "BigData_Analysis/실기_3유형_1.html#데이터-불러오기",
    "title": "빅분기 실기 - 3유형 문제풀이(1)",
    "section": "✅ 데이터 불러오기",
    "text": "✅ 데이터 불러오기\n\nimport pandas as pd\nimport numpy as np\n\n\n# 데이터 불러오기 mtcars\ndf = pd.read_csv('mtcars.txt')\ndf.head(3)\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\nMazda RX4\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n\n\n\n\n\n\n예제문제\n\n\n1. mtcars 데이터셋의 mpg열 데이터의 평균이 20과 같다고 할 수 있는지 검정하시오. (유의수준 5%)\n\nimport scipy.stats as stats\nfrom scipy.stats import shapiro\n\n\n# 1. 가설검정\n# H_0 : mpg 열의 평균이 20과 같다\n# H_1 : mpg 열의 평균이 20과 같지 않다\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3. 정규성 검정\n# H_0(귀무가설) : 정규분포를 따른다\n# H_1(대립가설) : 정규분포를 따르지 않는다\nstatistic, pvalue = stats.shapiro(df['mpg'])\nprint(round(statistic, 4), round(pvalue, 4))\n\nresult = stats.shapiro(df['mpg'])\nprint(result)\n\n0.9476 0.1229\nShapiroResult(statistic=0.9475648403167725, pvalue=0.1228824257850647)\n\n\n\np-value 값이 유의수준(0.05) 보다 크다. -&gt; 귀무가설(H_0) 채택\n(만약 정규분포를 따르지 않는다면 비모수 검정방법을 써야 함(윌콕슨의 부호순위 검정)\n\n\n# 4.1 (정규성 만족 o) t-검정 실시\nstatistic, pvalue = stats.ttest_1samp(df['mpg'], popmean=20,\n                                      alternative='two-sided') # default: two-sided\n                                      # H_1 : 왼쪽값이 오른쪽 값과 같지 않다\nprint(round(statistic, 4), round(pvalue, 4))\n# alternative (대립가설: H_1) 옵션 : 'two-sided', 'greater', 'less'\n\n0.0851 0.9328\n\n\n\n# 4.2 (정규성 만족 x) Wilcoxon 부호순위 검정\nstatistic, pvalue = stats.wilcoxon(df['mpg']-20,alternative='two-sided') \nprint(round(statistic, 4), round(pvalue, 4))\n\n249.0 0.7891\n\n\n\n# 5. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값이 0.05보다 크기 때문에(0.9328) 귀무가설을 채택한다\n# 즉, mpg 열의 평균이 20과 같다고 할 수 있다\n\n# 답 : 채택\n\n\n# 실제로 평균을 구해보면\ndf['mpg'].mean()\n\n20.090624999999996\n\n\n\n\n2. mtcars 데이터셋의 mpg 열 데이터의 평균이 17보다 크다고 할 수 있는지 검정하시오. (유의수준 5%)\n\n# 1. 가설검정\n# H_0 : mpg 열의 평균이 17보다 작거나 같다(mpg mean &lt;= 17)\n# H_1 : mpg 열의 평균이 17보다 크다(mpg mean &gt;17)\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3. 정규성 검정\n# H_0(귀무가설) : 정규분포를 따른다\n# H_1(대립가설) : 정규분포를 따르지 않는다\nstatistic, pvalue = stats.shapiro(df['mpg'])\nprint(round(statistic, 4), round(pvalue, 4))\n\n0.9476 0.1229\n\n\n\n# 4.1 (정규성 만족 o) t-검정 실시\nstatistic, pvalue = stats.ttest_1samp(df['mpg'], popmean=17,\n                                      alternative='greater') # alternative(대립가설)\n# H_1: 왼쪽값(df['mpg'].mean)이 오른쪽값(popmean)보다 크다\nprint(round(statistic, 4), round(pvalue, 4))\n\n2.9008 0.0034\n\n\n\n# 4.2 (정규성 만족 x) Wilcoxon 부호순위 검정\nstatistic, pvalue = stats.wilcoxon(df['mpg']-17,alternative='greater') \nprint(round(statistic, 4), round(pvalue, 4))\n\n395.5 0.0066\n\n\n\n# 5. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값이 0.05보다 작기 때문에(0.0034) 귀무가설을 기각한다(대립가설 채택)\n# 즉, mpg 열의 평균이 17보다 크다고 할 수 있다\n\n# 답 : 기각\n\n\n\n3. mtcars 데이터셋의 mpg 열 데이터의 평균이 17보다 작다고 할 수 있는지 검정하시오. (유의수준 5%)\n\n# 1. 가설검정\n# H_0 : mpg 열의 평균이 17보다 크거나 같다(mpg mean &gt;= 17)\n# H_1 : mpg 열의 평균이 17보다 작다(mpg mean &lt;17)\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3. 정규성 검정\nstatistic, pvalue = stats.shapiro(df['mpg'])\nprint(round(statistic, 4), round(pvalue, 4))\n\n0.9476 0.1229\n\n\n\n# 4.1 (정규성 만족 o) t-검정 실시\nstatistic, pvalue = stats.ttest_1samp(df['mpg'], popmean=17,\n                                      alternative='less') # alternative(대립가설)\n# H_1: 왼쪽값(df['mpg'].mean)이 오른쪽값(popmean)보다 작다\nprint(round(statistic, 4), round(pvalue, 4))\n\n2.9008 0.9966\n\n\n\n# 4.2 (정규성 만족 x) Wilcoxon 부호순위 검정\nstatistic, pvalue = stats.wilcoxon(df['mpg']-17,alternative='less') \nprint(round(statistic, 4), round(pvalue, 4))\n\n395.5 0.9938\n\n\n\n# 5. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값이 0.05보다 크기 때문에(0.9966) 귀무가설을 채택한다\n# 즉, mpg 열의 평균이 17보다 크거나 같다고 할 수 있다\n\n# 답 : 채택"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html",
    "href": "BigData_Analysis/실기_2유형_4.html",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "",
    "text": "실기 2 유형(4)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html#데이터-분석-순서",
    "href": "BigData_Analysis/실기_2유형_4.html#데이터-분석-순서",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "✅ 데이터 분석 순서",
    "text": "✅ 데이터 분석 순서\n\n1. 라이브러리 및 데이터 확인\n\n\n2. 데이터 탐색(EDA)\n\n\n3. 데이터 전처리 및 분리\n\n\n4. 모델링 및 성능평가\n\n\n5. 예측값 제출"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html#라이브러리-및-데이터-확인-1",
    "href": "BigData_Analysis/실기_2유형_4.html#라이브러리-및-데이터-확인-1",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "✅ 1. 라이브러리 및 데이터 확인",
    "text": "✅ 1. 라이브러리 및 데이터 확인\n\nimport pandas as pd\nimport numpy as np\n\n\n###############  실기환경 복사 영역  ###############\nimport pandas as pd\nimport numpy as np\n# 실기 시험 데이터셋으로 셋팅하기 (수정금지)\nfrom sklearn.datasets import load_iris\n# Iris 데이터셋을 로드\niris = load_iris()\nx = pd.DataFrame(iris.data, columns=['sepal_length', 'sepal_width',\n                                     'petal_length', 'petal_width'])\ny = iris.target   # 'setosa'=0, 'versicolor'=1, 'virginica'=2\ny = np.where(y&gt;0, 1, 0) # setosa 종은 0, 나머지 종은 1로 변경 // 이진분류\n\n# 실기 시험 데이터셋으로 셋팅하기 (수정금지)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, \n                                                    stratify=y,\n                                                    random_state=2023)\n\nx_test = pd.DataFrame(x_test)\nx_train = pd.DataFrame(x_train)\ny_train = pd.DataFrame(y_train)\ny_train.columns = ['species']\n\n# 결측치 삽입\nx_test['sepal_length'].iloc[0] = None  \nx_train['sepal_length'].iloc[0] = None\n# 이상치 삽입\nx_train['sepal_width'].iloc[0] = 150\n###############  실기환경 복사 영역  ###############"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html#붓꽃iris의-종species을-분류해보자",
    "href": "BigData_Analysis/실기_2유형_4.html#붓꽃iris의-종species을-분류해보자",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "붓꽃(iris)의 종(Species)을 분류해보자",
    "text": "붓꽃(iris)의 종(Species)을 분류해보자"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html#데이터의-결측치-이상치에-대해-처리하고",
    "href": "BigData_Analysis/실기_2유형_4.html#데이터의-결측치-이상치에-대해-처리하고",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "- 데이터의 결측치, 이상치에 대해 처리하고",
    "text": "- 데이터의 결측치, 이상치에 대해 처리하고"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html#분류모델을-사용하여-정확도-f1-score-auc-값을-산출하시오",
    "href": "BigData_Analysis/실기_2유형_4.html#분류모델을-사용하여-정확도-f1-score-auc-값을-산출하시오",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "- 분류모델을 사용하여 정확도, F1 score, AUC 값을 산출하시오",
    "text": "- 분류모델을 사용하여 정확도, F1 score, AUC 값을 산출하시오"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html#제출은-result-변수에-담아-양식에-맞게-제출하시오",
    "href": "BigData_Analysis/실기_2유형_4.html#제출은-result-변수에-담아-양식에-맞게-제출하시오",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "- 제출은 result 변수에 담아 양식에 맞게 제출하시오",
    "text": "- 제출은 result 변수에 담아 양식에 맞게 제출하시오\n\n# 데이터 설명\nprint(iris.DESCR)\n\n.. _iris_dataset:\n\nIris plants dataset\n--------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 150 (50 in each of three classes)\n    :Number of Attributes: 4 numeric, predictive attributes and the class\n    :Attribute Information:\n        - sepal length in cm\n        - sepal width in cm\n        - petal length in cm\n        - petal width in cm\n        - class:\n                - Iris-Setosa\n                - Iris-Versicolour\n                - Iris-Virginica\n                \n    :Summary Statistics:\n\n    ============== ==== ==== ======= ===== ====================\n                    Min  Max   Mean    SD   Class Correlation\n    ============== ==== ==== ======= ===== ====================\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n    ============== ==== ==== ======= ===== ====================\n\n    :Missing Attribute Values: None\n    :Class Distribution: 33.3% for each of 3 classes.\n    :Creator: R.A. Fisher\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n    :Date: July, 1988\n\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\nfrom Fisher's paper. Note that it's the same as in R, but not as in the UCI\nMachine Learning Repository, which has two wrong data points.\n\nThis is perhaps the best known database to be found in the\npattern recognition literature.  Fisher's paper is a classic in the field and\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\ndata set contains 3 classes of 50 instances each, where each class refers to a\ntype of iris plant.  One class is linearly separable from the other 2; the\nlatter are NOT linearly separable from each other.\n\n.. topic:: References\n\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n     Mathematical Statistics\" (John Wiley, NY, 1950).\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n     Structure and Classification Rule for Recognition in Partially Exposed\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n     on Information Theory, May 1972, 431-433.\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n     conceptual clustering system finds 3 classes in the data.\n   - Many, many more ..."
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html#데이터-탐색eda-1",
    "href": "BigData_Analysis/실기_2유형_4.html#데이터-탐색eda-1",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "✅ 2. 데이터 탐색(EDA)",
    "text": "✅ 2. 데이터 탐색(EDA)\n\n# 데이터의 행/열 확인\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\n\n(120, 4)\n(30, 4)\n(120, 1)\n\n\n\n# 초기 데이터 확인\n\nprint(x_train.head(3))\nprint(x_test.head(3))\nprint(y_train.head(3))\n\n    sepal_length  sepal_width  petal_length  petal_width\n2            NaN        150.0           1.3          0.2\n49           5.0          3.3           1.4          0.2\n66           5.6          3.0           4.5          1.5\n     sepal_length  sepal_width  petal_length  petal_width\n93            NaN          2.3           3.3          1.0\n69            5.6          2.5           3.9          1.1\n137           6.4          3.1           5.5          1.8\n   species\n0        0\n1        0\n2        1\n\n\n\n# 변수명과 데이터 타입이 매칭이 되는지, 결측치가 있는지 확인\n\nprint(x_train.info())\nprint(x_test.info())\nprint(y_train.info()) # 현재 train, test에 결측치를 하나씩 넣었기에 확인 가능\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 120 entries, 2 to 44\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   sepal_length  119 non-null    float64\n 1   sepal_width   120 non-null    float64\n 2   petal_length  120 non-null    float64\n 3   petal_width   120 non-null    float64\ndtypes: float64(4)\nmemory usage: 4.7 KB\nNone\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 30 entries, 93 to 55\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   sepal_length  29 non-null     float64\n 1   sepal_width   30 non-null     float64\n 2   petal_length  30 non-null     float64\n 3   petal_width   30 non-null     float64\ndtypes: float64(4)\nmemory usage: 1.2 KB\nNone\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 120 entries, 0 to 119\nData columns (total 1 columns):\n #   Column   Non-Null Count  Dtype\n---  ------   --------------  -----\n 0   species  120 non-null    int32\ndtypes: int32(1)\nmemory usage: 608.0 bytes\nNone\n\n\n\n# x_train 과 x_test 데이터의 기초통계량을 잘 비교해보세요\n\nprint(x_train.describe())\nprint(x_test.describe())\nprint(y_train.describe())\n\n       sepal_length  sepal_width  petal_length  petal_width\ncount    119.000000     120.0000    120.000000   120.000000\nmean       5.920168       4.2950      3.816667     1.226667\nstd        0.841667      13.4191      1.798848     0.780512\nmin        4.300000       2.2000      1.100000     0.100000\n25%        5.150000       2.8000      1.575000     0.300000\n50%        6.000000       3.0000      4.400000     1.350000\n75%        6.500000       3.4000      5.225000     1.800000\nmax        7.900000     150.0000      6.900000     2.500000\n       sepal_length  sepal_width  petal_length  petal_width\ncount     29.000000    30.000000     30.000000     30.00000\nmean       5.596552     3.000000      3.523333      1.09000\nstd        0.709367     0.522593      1.631518      0.68549\nmin        4.600000     2.000000      1.000000      0.10000\n25%        5.000000     2.625000      1.600000      0.35000\n50%        5.500000     3.000000      4.050000      1.15000\n75%        5.900000     3.300000      4.925000      1.57500\nmax        7.600000     4.200000      6.600000      2.30000\n          species\ncount  120.000000\nmean     0.666667\nstd      0.473381\nmin      0.000000\n25%      0.000000\n50%      1.000000\n75%      1.000000\nmax      1.000000\n\n\n\n# y 데이터도 구체적으로 살펴보세요\nprint(y_train.head())\n\n   species\n0        0\n1        0\n2        1\n3        1\n4        1\n\n\n\n# y 데이터도 구체적으로 살펴보세요\nprint(y_train.value_counts())\n\nspecies\n1          80\n0          40\ndtype: int64"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html#데이터-전처리-및-분리-1",
    "href": "BigData_Analysis/실기_2유형_4.html#데이터-전처리-및-분리-1",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "✅ 3. 데이터 전처리 및 분리",
    "text": "✅ 3. 데이터 전처리 및 분리\n\n1) 결측치, 2) 이상치, 3) 변수 처리하기\n\n# 결측치 확인\nprint(x_train.isnull().sum())\nprint(x_test.isnull().sum())\nprint(y_train.isnull().sum())\n\nsepal_length    1\nsepal_width     0\npetal_length    0\npetal_width     0\ndtype: int64\nsepal_length    1\nsepal_width     0\npetal_length    0\npetal_width     0\ndtype: int64\nspecies    0\ndtype: int64\n\n\n\n# 결측치 제거 // 데이터의 수가 많은 경우\n# df = df.dropna()\n# print(df)\n\n# 참고사항\n# print(df.dropna().shape) # 행 기준으로 삭제\n\n\n# 결측치 대체(평균값, 중앙값, 최빈값)\n\n# 연속형 변수 : 중앙값, 평균값\n#  - df['변수명'].median()\n#  - df['변수명'].mean()\n\n# 범수형 변수 :  최빈값\n\n# df['변수명'] = df['변수명'].fillna(대체할 값)\n\n\n# 결측치 대체(중앙값)\n# ** 주의사항 : train 데이터의 중앙값으로 test 데이터도 변경해줘야 함 **\n\nmedian = x_train['sepal_length'].median()\nx_train['sepal_length'] = x_train['sepal_length'].fillna(median)\nx_test['sepal_length'] = x_test['sepal_length'].fillna(median)\n\n\n# 이상치 확인\ncond1 = (x_train['sepal_width']&gt;=10)\nprint(len(x_train[cond1]))\n\n1\n\n\n\n# 이상치 대체\n# (참고) df['변수명'] = np.where( df['변수명'] &gt;= 5, 대체할 값, df['변수명'])\n\n# 예를 들어 'sepal_width' 값이 10이 넘으면 이상치라고 가정해본다명\n# 이상치를 제외한 Max 값을 구해서 대체해보자\ncond1 = (x_train['sepal_width'] &lt;= 10)\nmax_sw = x_train[cond1]['sepal_width'].max()\nprint(max_sw)\n\nx_train['sepal_width'] = np.where(x_train['sepal_width'] &gt;=10, max_sw,\n                                 x_train['sepal_width'])\nprint(x_train.describe())\n\n4.4\n       sepal_length  sepal_width  petal_length  petal_width\ncount    120.000000   120.000000    120.000000   120.000000\nmean       5.920833     3.081667      3.816667     1.226667\nstd        0.838155     0.429966      1.798848     0.780512\nmin        4.300000     2.200000      1.100000     0.100000\n25%        5.175000     2.800000      1.575000     0.300000\n50%        6.000000     3.000000      4.400000     1.350000\n75%        6.500000     3.400000      5.225000     1.800000\nmax        7.900000     4.400000      6.900000     2.500000\n\n\n\n# 변수처리\n\n# 불필요한 변수 제거\n# df = df.drop(columns = ['변수1', '변수2'])\n# df = df.drop(['변수1', '변수2'], axis = 1)\n\n# 필요시 변수 추가(파생변수 생성)\n# df['파생변수명'] = df['A'] * df['B'] (파생변수 생성 수식)\n\n# 원핫인코딩(가변수 처리)\n# x_train = pd.get_dummies(x_train)\n# x_test = pd.get_dummies(x_test)\n# print(x_train.info())\n# print(x_test.info())\n\n\n\n데이터 분리\n\n#  데이터를 훈련 세트와 검증용 세트로 분할 (80% 훈련, 20% 검증)\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_train,\n                                                  y_train['species'],\n                                                  test_size=0.2,\n                                                  stratify = y_train['species'],\n                                                  random_state = 2023)\nprint(x_train.shape)\nprint(x_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)\n\n(96, 4)\n(24, 4)\n(96,)\n(24,)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html#모델링-및-성능평가-1",
    "href": "BigData_Analysis/실기_2유형_4.html#모델링-및-성능평가-1",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "✅ 4. 모델링 및 성능평가",
    "text": "✅ 4. 모델링 및 성능평가\n\n# 랜덤포레스트 모델 사용 (참고 : 회귀모델은 RandomForestRegressor)\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nmodel.fit(x_train, y_train)\n\nRandomForestClassifier()\n\n\n\n# 모델을 사용하여 테스트 데이터 예측\ny_pred = model.predict(x_val)\n\n\n# 모델 성능 평가 (accuracy, f1 score, AUC 등)\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score\nacc = accuracy_score(y_val, y_pred)  # (실제값, 예측값)\nf1 = f1_score(y_val, y_pred)         # (실제값, 예측값)\n# 다중분류인 경우 f1  f1_score(y_val, y_pred, average = 'macro')\nauc = roc_auc_score(y_val, y_pred)   # (실제값, 예측값)\n\n\n# 정확도(Accuracy)\nprint(acc)\n\n1.0\n\n\n\n# F1 Score\nprint(f1)\n\n1.0\n\n\n\n# AUC\nprint(auc)\n\n1.0\n\n\n\n# 참고사항\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_val, y_pred)\nprint(cm)\n\n# #####  예측\n# #####  0  1\n# 실제 0 TN FP\n# 실제 1 FN TP\n\n[[ 8  0]\n [ 0 16]]"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_4.html#예측값-제출-1",
    "href": "BigData_Analysis/실기_2유형_4.html#예측값-제출-1",
    "title": "빅분기 실기 - 2유형 문제풀이(4)",
    "section": "✅ 5. 예측값 제출",
    "text": "✅ 5. 예측값 제출\n\n(주의) x_test 를 모델에 넣어 나온 예측값을 제출해야 함\n\n#(실기시험 안내사항)\n# 아래 코드 예측변수와 수험번호를 개인별로 변경하여 활용\n# pd.DataFrame({'cust_id':cust_id, 'target':y_result}).to_csv('0030000.csv', index=False)\n\n# 모델을 사용하여 테스트 데이터 예측\n\n# 1. 특정 클래스로 분류할 경우 (predict)\ny_result = model.predict(x_test)\nprint(y_result[:5])\n\n# 2. 특정 클래스로 분류될 확률을 구할 경우 (predict_proba)\ny_result_prob = model.predict_proba(x_test)\nprint(y_result_prob[:5])\n\n# 이해해보기\nresult_prob = pd.DataFrame({\n    'result' : y_result,\n    'prob_0' : y_result_prob[:,0]\n})\n# setosa 일 확률 : y_result_prob[:, 0]\n# 그 외 종일 확률 : y_result_prob[:, 1]\n\nprint(result_prob[:5])\n\n[1 1 1 0 1]\n[[0.   1.  ]\n [0.   1.  ]\n [0.   1.  ]\n [1.   0.  ]\n [0.04 0.96]]\n   result  prob_0\n0       1    0.00\n1       1    0.00\n2       1    0.00\n3       0    1.00\n4       1    0.04\n\n\n\n# ★tip : 데이터를 저장한 다음 불러와서 제대로 제출했는지 확인해보자\n# pd.DataFrame({'result':y_result}).to_csv('수험번호.csv', index=False)\n# df2 = pd.read_csv(\"수험번호.csv\")\n# print(df2.head())"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_2.html",
    "href": "BigData_Analysis/실기_2유형_2.html",
    "title": "빅분기 실기 - 2유형 문제풀이(2)",
    "section": "",
    "text": "실기 2 유형(2)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_2.html#데이터-분석-순서",
    "href": "BigData_Analysis/실기_2유형_2.html#데이터-분석-순서",
    "title": "빅분기 실기 - 2유형 문제풀이(2)",
    "section": "✅ 데이터 분석 순서",
    "text": "✅ 데이터 분석 순서\n\n1. 라이브러리 및 데이터 확인\n\n\n2. 데이터 탐색(EDA)\n\n\n3. 데이터 전처리 및 분리\n\n\n4. 모델링 및 성능평가\n\n\n5. 예측값 제출"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_2.html#라이브러리-및-데이터-확인-1",
    "href": "BigData_Analysis/실기_2유형_2.html#라이브러리-및-데이터-확인-1",
    "title": "빅분기 실기 - 2유형 문제풀이(2)",
    "section": "✅ 1. 라이브러리 및 데이터 확인",
    "text": "✅ 1. 라이브러리 및 데이터 확인\n\nimport pandas as pd\nimport numpy as np\n\n\n###### 실기환경 복사 영역 ######\nimport pandas as pd\nimport numpy as np\n# 실기 시험 데이터셋으로 셋팅하기 (수정금지)\nfrom sklearn.datasets import load_diabetes\n# diabetes 데이터셋 로드\ndiabetes = load_diabetes()\nx = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\ny = pd.DataFrame(diabetes.target)\n\n# 실시 시험 데이터셋으로 셋팅하기 (수정금지)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2,\n                                                   random_state = 2023)\n\nx_test = pd.DataFrame(x_test.reset_index())\nx_train = pd.DataFrame(x_train.reset_index())\ny_train = pd.DataFrame(y_train.reset_index())\n\nx_test.rename(columns = {'index':'cust_id'}, inplace=True)\nx_train.rename(columns = {'index':'cust_id'}, inplace=True)\ny_train.columns = ['cust_id', 'target']\n###### 실기환경 복사 영역 ######\n\n\n# from sklearn.datasets import load_diabetes\n# diabetes = load_diabetes()\n# x = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n# y = pd.DataFrame(diabetes.target)\n\n# from sklearn.model_selection import train_test_split\n# x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,\n#                                                 random_state=2023)\n\n# x_test = pd.DataFrame(x_test.reset_index())\n# x_train = pd.DataFrame(x_train.reset_index())\n# y_train = pd.DataFrame(y_train.reset_index())\n\n# x_test.rename(columns={'index':'cust_id'},inplace=True)\n# x_train.rename(columns={'index':'cust_id'},inplace=True)\n# y_train.columns=['cust']\n\n\n🏥 당뇨병 환자의 질병 진행정도를 예측해보자\n\n\n- 데이터의 결측치, 이상치, 변수들에 대해 전처리하고\n\n\n- 회귀모델을 사용하여 Rsq, MSE 값을 산출하시오\n\n\n- 제출은 cust_id, target 변수를 가진 dataframe 형태로 제출하시오\n\n# 데이터 설명\nprint(diabetes.DESCR)\n\n.. _diabetes_dataset:\n\nDiabetes dataset\n----------------\n\nTen baseline variables, age, sex, body mass index, average blood\npressure, and six blood serum measurements were obtained for each of n =\n442 diabetes patients, as well as the response of interest, a\nquantitative measure of disease progression one year after baseline.\n\n**Data Set Characteristics:**\n\n  :Number of Instances: 442\n\n  :Number of Attributes: First 10 columns are numeric predictive values\n\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n\n  :Attribute Information:\n      - age     age in years\n      - sex\n      - bmi     body mass index\n      - bp      average blood pressure\n      - s1      tc, total serum cholesterol\n      - s2      ldl, low-density lipoproteins\n      - s3      hdl, high-density lipoproteins\n      - s4      tch, total cholesterol / HDL\n      - s5      ltg, possibly log of serum triglycerides level\n      - s6      glu, blood sugar level\n\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n\nSource URL:\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n\nFor more information see:\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_2.html#데이터-탐색eda-1",
    "href": "BigData_Analysis/실기_2유형_2.html#데이터-탐색eda-1",
    "title": "빅분기 실기 - 2유형 문제풀이(2)",
    "section": "✅ 2. 데이터 탐색(EDA)",
    "text": "✅ 2. 데이터 탐색(EDA)\n\n# 데이터의 행/열 확인\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\n\n(353, 11)\n(89, 11)\n(353, 2)\n\n\n\n# 초기 데이터 확인\n\nprint(x_train.head(3))\nprint(x_test.head(3))\nprint(y_train.head(3))\n\n   cust_id       age       sex       bmi        bp        s1        s2  \\\n0        4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596   \n1      318  0.088931 -0.044642  0.006728  0.025315  0.030078  0.008707   \n2      301 -0.001882  0.050680 -0.024529  0.052858  0.027326  0.030001   \n\n         s3        s4        s5        s6  \n0  0.008142 -0.002592 -0.031991 -0.046641  \n1  0.063367 -0.039493  0.009436  0.032059  \n2  0.030232 -0.002592 -0.021394  0.036201  \n   cust_id       age       sex       bmi        bp        s1        s2  \\\n0      280  0.009016  0.050680  0.018584  0.039087  0.017694  0.010586   \n1      412  0.074401 -0.044642  0.085408  0.063187  0.014942  0.013091   \n2       68  0.038076  0.050680 -0.029918 -0.040099 -0.033216 -0.024174   \n\n         s3        s4        s5        s6  \n0  0.019187 -0.002592  0.016305 -0.017646  \n1  0.015505 -0.002592  0.006209  0.085907  \n2 -0.010266 -0.002592 -0.012908  0.003064  \n   cust_id  target\n0        4   135.0\n1      318   109.0\n2      301    65.0\n\n\n\n# 변수명과 데이터 타입이 매칭이 되는지, 결측치가 있는지 확인해보세요\n\nprint(x_train.info())\nprint(x_test.info())\nprint(y_train.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 353 entries, 0 to 352\nData columns (total 11 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   cust_id  353 non-null    int64  \n 1   age      353 non-null    float64\n 2   sex      353 non-null    float64\n 3   bmi      353 non-null    float64\n 4   bp       353 non-null    float64\n 5   s1       353 non-null    float64\n 6   s2       353 non-null    float64\n 7   s3       353 non-null    float64\n 8   s4       353 non-null    float64\n 9   s5       353 non-null    float64\n 10  s6       353 non-null    float64\ndtypes: float64(10), int64(1)\nmemory usage: 30.5 KB\nNone\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 89 entries, 0 to 88\nData columns (total 11 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   cust_id  89 non-null     int64  \n 1   age      89 non-null     float64\n 2   sex      89 non-null     float64\n 3   bmi      89 non-null     float64\n 4   bp       89 non-null     float64\n 5   s1       89 non-null     float64\n 6   s2       89 non-null     float64\n 7   s3       89 non-null     float64\n 8   s4       89 non-null     float64\n 9   s5       89 non-null     float64\n 10  s6       89 non-null     float64\ndtypes: float64(10), int64(1)\nmemory usage: 7.8 KB\nNone\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 353 entries, 0 to 352\nData columns (total 2 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   cust_id  353 non-null    int64  \n 1   target   353 non-null    float64\ndtypes: float64(1), int64(1)\nmemory usage: 5.6 KB\nNone\n\n\n\n# x_train 과 x_test 데이터의 기초통계량을 잘 비교해보세요\n\nprint(x_train.describe())\nprint(x_test.describe())\nprint(y_train.describe())\n\n          cust_id         age         sex         bmi          bp          s1  \\\ncount  353.000000  353.000000  353.000000  353.000000  353.000000  353.000000   \nmean   212.634561    0.000804    0.000724    0.000640   -0.000326    0.001179   \nstd    126.668903    0.047617    0.047673    0.048141    0.046585    0.047891   \nmin      0.000000   -0.107226   -0.044642   -0.084886   -0.112400   -0.126781   \n25%    105.000000   -0.038207   -0.044642   -0.035307   -0.033214   -0.033216   \n50%    210.000000    0.005383   -0.044642   -0.006206   -0.005671   -0.002945   \n75%    322.000000    0.038076    0.050680    0.030440    0.032201    0.027326   \nmax    441.000000    0.110727    0.050680    0.170555    0.125158    0.153914   \n\n               s2          s3          s4          s5          s6  \ncount  353.000000  353.000000  353.000000  353.000000  353.000000  \nmean     0.001110   -0.000452    0.000901    0.001446    0.000589  \nstd      0.048248    0.048600    0.048045    0.047160    0.048122  \nmin     -0.115613   -0.102307   -0.076395   -0.126097   -0.137767  \n25%     -0.029184   -0.039719   -0.039493   -0.033249   -0.034215  \n50%     -0.001314   -0.006584   -0.002592    0.000271    0.003064  \n75%      0.031567    0.030232    0.034309    0.033657    0.032059  \nmax      0.198788    0.181179    0.185234    0.133599    0.135612  \n          cust_id        age        sex        bmi         bp         s1  \\\ncount   89.000000  89.000000  89.000000  89.000000  89.000000  89.000000   \nmean   251.696629  -0.003188  -0.002871  -0.002537   0.001292  -0.004676   \nstd    127.901365   0.047761   0.047563   0.045665   0.051777   0.046493   \nmin      9.000000  -0.099961  -0.044642  -0.090275  -0.108957  -0.091006   \n25%    148.000000  -0.034575  -0.044642  -0.030996  -0.036656  -0.037344   \n50%    280.000000  -0.001882  -0.044642  -0.009439  -0.005671  -0.009825   \n75%    366.000000   0.030811   0.050680   0.034751   0.042530   0.031454   \nmax    436.000000   0.096197   0.050680   0.137143   0.132044   0.119515   \n\n              s2         s3         s4         s5         s6  \ncount  89.000000  89.000000  89.000000  89.000000  89.000000  \nmean   -0.004401   0.001792  -0.003575  -0.005737  -0.002334  \nstd     0.045030   0.043723   0.045980   0.049252   0.045757  \nmin    -0.089935  -0.080217  -0.076395  -0.104365  -0.129483  \n25%    -0.030437  -0.028674  -0.039493  -0.038459  -0.030072  \n50%    -0.014153  -0.002903  -0.002592  -0.014956  -0.005220  \n75%     0.020607   0.022869   0.003312   0.024053   0.019633  \nmax     0.130208   0.122273   0.141322   0.133599   0.135612  \n          cust_id      target\ncount  353.000000  353.000000\nmean   212.634561  152.943343\nstd    126.668903   75.324692\nmin      0.000000   37.000000\n25%    105.000000   90.000000\n50%    210.000000  141.000000\n75%    322.000000  208.000000\nmax    441.000000  346.000000\n\n\n\n# y 데이터도 구체적으로 살펴보세요\nprint(y_train.head())\n\n   cust_id  target\n0        4   135.0\n1      318   109.0\n2      301    65.0\n3      189    79.0\n4      288    80.0\n\n\n\n# y 데이터도 구체적으로 살펴보세요\nprint(y_train.describe())\n\n          cust_id      target\ncount  353.000000  353.000000\nmean   212.634561  152.943343\nstd    126.668903   75.324692\nmin      0.000000   37.000000\n25%    105.000000   90.000000\n50%    210.000000  141.000000\n75%    322.000000  208.000000\nmax    441.000000  346.000000"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_2.html#데이터-전처리-및-분리-1",
    "href": "BigData_Analysis/실기_2유형_2.html#데이터-전처리-및-분리-1",
    "title": "빅분기 실기 - 2유형 문제풀이(2)",
    "section": "✅ 3. 데이터 전처리 및 분리",
    "text": "✅ 3. 데이터 전처리 및 분리\n\n1) 결측치, 2) 이상치, 3) 변수처리하기\n\n# 결측치 확인\n\nprint(x_train.isnull().sum())\nprint(x_test.isnull().sum())\nprint(y_train.isnull().sum())\n\ncust_id    0\nage        0\nsex        0\nbmi        0\nbp         0\ns1         0\ns2         0\ns3         0\ns4         0\ns5         0\ns6         0\ndtype: int64\ncust_id    0\nage        0\nsex        0\nbmi        0\nbp         0\ns1         0\ns2         0\ns3         0\ns4         0\ns5         0\ns6         0\ndtype: int64\ncust_id    0\ntarget     0\ndtype: int64\n\n\n\n# 결측치 제거\n# df = df.dropna()\n# print(df)\n\n# 참고사항\n# print(df.dropna().shape) # 행 기준으로 삭제\n\n\n# 결측치 대체 (평균값, 중앙값, 최빈값)\n# 연속형 변수 :  중앙값, 평균값\n# df['변수명'].median()\n# df['변수명'].mean()\n\n# 범주형 변수 :  최빈값\n# df['변수명'].mode()\n\n\n# df['변수명'] = df['변수명'].fillna(대체할값)\n\n\n# 이상치 대체\n# (참고) df['변수명'] = np.where(df['변수명'] &gt;= 5, 대체할 값, df['변수명'])\n\n\n# 변수처리\n\n# 불필요한 변수 제거\n# df = df.drop(columns = ['변수1', '변수2'])\n# df = df.drop(['변수1', '변수2'], axis=1)\n\n# 필요시 변수 추가(파생변수 생성)\n# df['파생변수명'] = df['A'] * df['B'] (파생변수 생성 수식)\n\n# 원핫인코딩(가변수 처리)\n# x_train = pd.get_dummies(x_train)\n# x_test = pd.get_dummies(x_test)\n\n# print(x_train.info())\n# print(x_test.info())\n\n\n# 변수처리\n\n# 불필요한 변수(columns) 제거\n# cust_id 는 불필요한 변수이므로 제거합니다.\n# 단, test셋의 cust_id가 나중에 제출이 필요하기 떄문에 별도로 저장\n\ncust_id = x_test['cust_id'].copy()\n\n# 각 데이터에서 cust_id 변수 제거\nx_train = x_train.drop(columns = ['cust_id']) # drop(columns = ['변수1', '변수2'])\nx_test = x_test.drop(columns = ['cust_id'])\n\n\n\n데이터 분리\n\n# 데이터를 훈련 세트와 검증용 세트로 분할 (80% 훈련, 20% 검증용)\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train['target'],\n                                                   test_size = 0.2,\n                                                   random_state = 23)\n# 분류 모델에서는 층화(starify)를 할 필요가 없다. 연속형인 경우에만 사용\n\nprint(x_train.shape)\nprint(x_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)\n\n(282, 10)\n(71, 10)\n(282,)\n(71,)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_2.html#모델링-및-성능평가-1",
    "href": "BigData_Analysis/실기_2유형_2.html#모델링-및-성능평가-1",
    "title": "빅분기 실기 - 2유형 문제풀이(2)",
    "section": "✅ 4. 모델링 및 성능평가",
    "text": "✅ 4. 모델링 및 성능평가\n\n# 랜덤포레스 모델 사용 (참고 : 분류모델은 RandomForestClassifier)\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(random_state = 2023)\nmodel.fit(x_train, y_train)\n\nRandomForestRegressor(random_state=2023)\n\n\n\n# 모델을 사용하여 테스트 데이터 예측\ny_pred = model.predict(x_val)\n\n\n# 모델 성능 평가 (평균 제곱 오차 및 R-squared)\nfrom sklearn.metrics import mean_squared_error, r2_score\nmse = mean_squared_error(y_val, y_pred) # (실제값, 예측값)\nr2 = r2_score(y_val, y_pred) # (실제값, 예측값)\n\n\n# MSE(mean_squared_error, 평균 제곱 오차)\nprint(mse)\n\n2571.6276845070424\n\n\n\n# R2 score(R-squared)\nprint(r2)\n\n0.5235611874726152\n\n\n\n# RMSE (root mean squared error)\nrmse = mse ** 0.5\nprint(rmse)\n\n50.71121852713699"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_2.html#예측값-제출-1",
    "href": "BigData_Analysis/실기_2유형_2.html#예측값-제출-1",
    "title": "빅분기 실기 - 2유형 문제풀이(2)",
    "section": "✅ 5. 예측값 제출",
    "text": "✅ 5. 예측값 제출\n\n(주의) x_test 를 모델에 넣어 나온 예측값을 제출해야함\n\n#(실기시험 안내사항)\n# 아래 코드 예측변수와 수험번호를 개인별로 변경하여 활용\n# pd.DataFrame({'cust_id':cust_id, 'target':y_result}).to_csv('0030000.csv', index=False)\n\n# 모델을 사용하여 테스트 데이터 예측\ny_result = model.predict(x_test)\nresult = pd.DataFrame({'cust_id':cust_id, 'target':y_result})\nprint(result[:5])\n\n   cust_id  target\n0      280  186.51\n1      412  255.92\n2       68   77.97\n3      324  184.22\n4      101  111.14\n\n\n\n# ★tip : 데이터를 저장한 다음 불러와서 제대로 제출했는지 확인해보자\n# pd.DataFrame({'result':y_result}).to_csv('수험번호.csv', index=False)\n# df2 = pd.read_csv(\"수험번호.csv\")\n# print(df2.head())"
  },
  {
    "objectID": "BigData_Analysis/실기_1유형.html",
    "href": "BigData_Analysis/실기_1유형.html",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "",
    "text": "실기 1 유형"
  },
  {
    "objectID": "BigData_Analysis/실기_1유형.html#데이터-다루기-유형",
    "href": "BigData_Analysis/실기_1유형.html#데이터-다루기-유형",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "✅ 데이터 다루기 유형",
    "text": "✅ 데이터 다루기 유형\n\n\n데이터 타입(object, int, float, bool 등)\n기초통계량(평균, 중앙값, 사분위수, IQR, 표준편차 등)\n데이터 인덱싱, 필터링, 정렬, 변경 등\n중복값, 결측치, 이상치 처리 (제거 or 대체)\n데이터 Scaling (데이터 표준화(z), 데이터 정규화(min-max))\n데이터 합치기\n날짜/시간 데이터, index 다루기"
  },
  {
    "objectID": "BigData_Analysis/실기_1유형.html#핵심문제-27개-110",
    "href": "BigData_Analysis/실기_1유형.html#핵심문제-27개-110",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "✅ 핵심문제 27개 (1~10)",
    "text": "✅ 핵심문제 27개 (1~10)\n\n# 데이터 불러오기\nimport pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"mtcars.txt\")\ndf.head()\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\nMazda RX4\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\nHornet 4 Drive\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n4\nHornet Sportabout\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n\n\n\n\n# 문제 1\n# mpg 변수의 제 1사분위수를 구하고 정수값으로 출력하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\nQ1 = df['mpg'].quantile(.25)\nprint(round(Q1))\n\n15\n\n\n\n# 문제 2\n# mpg 값이 19이상 21이하인 데이터의 수를 구하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이 1)\ncond1 = (df[(df['mpg']&gt;=19) & (df['mpg']&lt;=21)])\nprint(len(cond1))\n\n# (풀이 2)\n# cond1 = (df['mpg']&gt;=19)\n# cond2 = (df['mpg']&lt;=21)\n# print(len(df[cond1&cond2]))\n\n5\n\n\n\n# 문제 3\n# hp 변수의 IQR 값을 구하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\nQ1 = df['hp'].quantile(.25)\nQ3 = df['hp'].quantile(.75)\nIQR = Q3 - Q1\nprint(IQR)\n\n83.5\n\n\n\n# 문제 4 \n# wt 변수의 상위 10개 값의 총합을 구하여 소수점을 버리고 정수로 출력하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\ntop10 = df.sort_values('wt', ascending=False)\nsum_top10 = sum(top10['wt'].head(10))\nprint(int(sum_top10))\n\n# top_10 = df.sort_values('wt',ascending=False).head(10)\n# print(int(sum(top_10['wt']))) #주의: 소수점 반올림이 아니라 버리는 문제\n\n42\n\n\n\n# 문제 5\n# 전체 자동차에서 cyl가 6인 비율이 얼마인지 소수점 첫째짜리까지 출력하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\ncond1 = len(df[df['cyl']==6])\ncond2 = len(df['cyl'])\nprint(round(cond1/cond2, 1))\n\n0.2\n\n\n\n# 문제 6\n# 첫번째 행부터 순서대로 10개 뽑은 후 mpg 열의 평균값을 반올림하여 정수로 출력하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\ncond1 = df.head(10) # df[:10], df.loc[0:9]\nmean_mpg = cond1['mpg'].mean()\nprint(round(mean_mpg))\n\n20\n\n\n\n# 문제 7\n# 첫번째 행부터 순서대로 50% 까지 데이터를 뽑아 wt 변수의 중앙값을 구하시오\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (풀이)\n# 50% 데이터에 해당하는 행의 수 (정수로 구하기)\np50 = int(len(df)*0.5)\ndf50 = df[:p50]\nprint(df50['wt'].median())\n\n# len(df)/2\n# cond1 = df[:17]\n# cond2 = cond1['wt'].median()\n# print(cond2)\n\n3.44\n\n\n\n# 문제 8\n# 결측값이 있는 데이터의 수를 출력하시오\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    '판매수' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n\n\n\n날짜\n제품\n판매수\n개당수익\n\n\n\n\n0\n20220103\nA\n3.0\n300\n\n\n1\n20220105\nB\nNaN\n400\n\n\n2\nNone\nNone\n5.0\n500\n\n\n3\n20230127\nB\n10.0\n600\n\n\n4\n20220203\nA\n10.0\n400\n\n\n\n\n\n\n\n\n# (풀이)\nm_value = df.isnull().sum()\nprint(m_value.sum())\n\n5\n\n\n\n# 문제 9 \n# '판매수' 컬럼의 결측값을 판매수의 중앙값으로 대체하고 판매수의 \n#  평균값을 정수로 출력하시오\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    '판매수' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\n\n\n# (풀이)\ndf[df['판매수'].isnull()]\nmedian = df['판매수'].median()\ndf['판매수'] = df['판매수'].fillna(median)\nmean = df['판매수'].mean()\nprint(int(mean))\n\n15\n\n\n\n# 문제 10\n# 판매수 컬럼에 결측치가 있는 행을 제거하고\n# 첫번째 행부터 순서대로 50%까지 데이터를 추출하여\n# 판매수 변수의 Q1(제1사분위수) 값을 정수로 출력하시오\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    '판매수' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\n\n\n# (풀이)\n# 결측치 삭제(행 기준)\ndf = df['판매수'].dropna()\n# 첫번째 행부터 순서대로 50%까지의 데이터 추출\nper50 = int(len(df)*0.5)\ndf = df[:per50]\n# 값구하기\nprint(round(df.quantile(.25)))\n\n# df[df['판매수'].isnull()]\n# df['판매수'] = df['판매수'].dropna()\n# int(len(df['판매수'])/2)\n# cond1 = df['판매수'].head(6)\n# Q1 = cond1.quantile(.25)\n# print(int(Q1))\n\n5"
  },
  {
    "objectID": "BigData_Analysis/실기_1유형.html#핵심문제-27개-1120",
    "href": "BigData_Analysis/실기_1유형.html#핵심문제-27개-1120",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "✅ 핵심문제 27개 (11~20)",
    "text": "✅ 핵심문제 27개 (11~20)\n\ndf = pd.read_csv('mtcars.txt')\ndf.head()\n\n\n\n\n\n\n\n\ncar\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\nMazda RX4\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\nHornet 4 Drive\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n4\nHornet Sportabout\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n\n\n\n\n# 문제 11\n# cyl가 4인 자동차와 6인 자동차 그룹의 mpg 평균값이 차이를 절대값으로 정수로 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\ncyl_4 = df[df['cyl']==4]\ncyl_4_mean = cyl_4['mpg'].mean()\n\ncyl_6 = df[df['cyl']==6]\ncyl_6_mean = cyl_6['mpg'].mean()\n\nprint(round(abs(cyl_4_mean - cyl_6_mean)))\n\n7\n\n\n\n# 문제 12\n# hp 변수에 대해 데이터표준화(Z-score)를 진행하고 이상치의 수를 구하시오\n# (단, 이상치는 Z값이 1.5를 초과하거나 -1.5미만인 값이다)\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\n# Z = (X-평균) / 표준편차\nstd = df['hp'].std()\nmean_hp = df['hp'].mean()\ndf['zscore'] = (df['hp']-mean_hp)/std\n\ncond1 = (df['zscore']&gt;1.5)\ncond2 = (df['zscore']&lt;-1.5)\nprint(len(df[cond1] + df[cond2]))\n\n2\n\n\n\n# 문제 13\n# mpg 컬럼을 최소최대 Scaling을 진행한 후 0.7보다 큰 값을 가지는 레코드 수를 구하라\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\nfrom sklearn.preprocessing import MinMaxScaler\nmscaler = MinMaxScaler()\ndf['mpg'] = mscaler.fit_transform(df[['mpg']])\nprint(len(df[df['mpg']&gt;0.7]))\n\n# 공식 : (x-min) / (max-min)\n# min_mpg = df['mpg'].min()\n# max_mpg = df['mpg'].max()\n# df['mpg'] = (df['mpg'] - min_mpg)/(max_mpg - min_mpg)\n# cond1 = (df['mpg']&gt;0.7)\n# print(len(df[cond1]))\n\n5\n\n\n\n# 문제 14\n# wt 컬럼에 대해 상자그림 기준으로 이상치의 개수를 구하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\n# 이상치 : Q1, Q3 로부터 1.5*IQR을 넘어가는 값\nQ1 = df['wt'].quantile(.25)\nQ3 = df['wt'].quantile(.75)\nIQR = Q3-Q1\n\nupper = Q3 + (1.5*IQR)\nlower = Q1 - (1.5*IQR)\n\ncond1 = (df['wt'] &gt; upper)\ncond2 = (df['wt'] &lt; lower)\n\nprint(len(df[cond1] + df[cond2]))\n\n3\n\n\n\n# 문제 15\n# 판매수 컬럼의 결측치를 최소값으로 대체하고\n# 결측치가 있을 때와 최소값으로 대체했을 때\n# 평균값의 차이를 절대값으로 정수로 출력하시오\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    '판매수' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf\n\n\n\n\n\n\n\n\n날짜\n제품\n판매수\n개당수익\n\n\n\n\n0\n20220103\nA\n3.0\n300\n\n\n1\n20220105\nB\nNaN\n400\n\n\n2\nNone\nNone\n5.0\n500\n\n\n3\n20230127\nB\n10.0\n600\n\n\n4\n20220203\nA\n10.0\n400\n\n\n5\n20220205\nNone\n10.0\n500\n\n\n6\n20230210\nA\n15.0\n500\n\n\n7\n20230223\nB\n15.0\n600\n\n\n8\n20230312\nA\n20.0\n600\n\n\n9\n20230422\nB\nNaN\n700\n\n\n10\n20220505\nA\n30.0\n600\n\n\n11\n20230511\nA\n40.0\n600\n\n\n\n\n\n\n\n\n# (풀이)\n# 데이터 복사\ndf2 = df.copy()\n# 최소값으로 결측치 대체\nmin = df['판매수'].min()\ndf2['판매수'] = df2['판매수'].fillna(min)\n\n# 결측치가 있을때, 대체했을때 평균\nm_yes = df['판매수'].mean()\nm_no = df2['판매수'].mean()\n\nprint(round(abs(m_yes - m_no)))\n\n2\n\n\n\n# 문제 16\n# vs변수가 0이 아닌 차량 중에 mpg 값이 가장 큰 차량의 hp 값을 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\ncond1 = df[df['vs']!=0]\n# mpg 값이 가장 큰 차량(내림차순)\ncond1 = cond1.sort_values('mpg',ascending=False)\n# cond1\nprint(cond1['hp'].iloc[0])\n\n65\n\n\n\n# 문제 17\n# gear 변수값이 3, 4인 두 그룹의 hp 표준편차값의 차이를 절대값으로 \n# 소수점 첫째자리까지 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\n# 두개 그룹으로 필터링\ncond1 = df[df['gear']==3]\ncond2 = df[df['gear']==4]\n# std 구하기\nstd3 = cond1['hp'].std()\nstd4 = cond2['hp'].std()\n\nprint(round(abs(std3 - std4),1))\n\n21.8\n\n\n\n# 문제 18\n# gear 변수의 갑별로 그룹화하여 mpg 평균값을 산출하고\n# 평균값이 높은 그룹의 mpg 제3사분위수 값을 구하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\ndf['gear'].unique()\ngear3 = df[df['gear']==3]\ngear4 = df[df['gear']==4]\ngear5 = df[df['gear']==5]\n\nm_3 = gear3['mpg'].mean()\nm_4 = gear4['mpg'].mean()\nm_5 = gear5['mpg'].mean()\n\n(m_3, m_4, m_5) # m_4의 평균값이 가장 큼\nQ3 = gear4['mpg'].quantile(.75)\nprint(Q3)\n\n28.075\n\n\n\n# (풀이_v2)\n# gear, mpg 변수만 필터링\ndf = df.loc[:, ['gear', 'mpg']]\n\n# gear 변수로 그룹화하여 mpg 평균값 보기\n# print(df.groupby('gear').mean()) # gear 4그룹이 제일 높음\n\n# gear=4인 그룹의 mpg Q3 구하기\ngear4 = df[df['gear']==4]\nprint(gear4['mpg'].quantile(.75))\n\n28.075\n\n\n\n# 문제 19\n# hp 항목의 상위 7번째 값으로 상위 7개 값을 변환한 후,\n# hp가 150 이상인 데이터를 추출하여 hp의 평균값을 반올림하여 정수로 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\n# hp 열 기준으로 내림차순 정렬\ndf = df.sort_values('hp', ascending=False)\n\n# 인덱스 초기화 - 내림차순으로 정렬시 최초의 인덱스로 있기에\ndf = df.reset_index(drop=True) # drop=True 는 기존 index 삭제\n# print(df)\n\n# hp 상위 7번째 값 확인\ntop7 = df['hp'].loc[6] # top7 = 205\n\n# np.where 활용\nimport numpy as np\ndf['hp'] = np.where(df['hp'] &gt;= 205, top7, df['hp'])\n# np.where(조건, 조건에 해당할 때 값, 그렇지 않을 때 값)\n\n# hp 150이상인 데이터\ncond1 = (df['hp']&gt;=150)\ndf = df[cond1]\nprint(round(df['hp'].mean()))\n\n187\n\n\n\n# 문제 20\n# car 변수에 Merc 무구가 포함된 자동차의 mpg 평균값을 정수로 출력하시오\ndf = pd.read_csv('mtcars.txt')\n\n\n# (풀이)\ndf2 = df[df['car'].str.contains('Merc')] # 문자열 추출 알아두기\nprint(round(df2['mpg'].mean()))\n\n# 시험환경에서 답구하는 방법(reset_index() 사용 후)\n# 시험에서는 car가 index로 되어 있음\n# df.reset_index(inplace=True)\n# df2 = df[df['index'].str.contains(\"Merc\")] \n# print(round(df2['mpg'].mean()))\n\n19"
  },
  {
    "objectID": "BigData_Analysis/실기_1유형.html#핵심문제-27개-2127",
    "href": "BigData_Analysis/실기_1유형.html#핵심문제-27개-2127",
    "title": "빅분기 실기 - 1유형 문제풀이",
    "section": "✅ 핵심문제 27개 (21~27)",
    "text": "✅ 핵심문제 27개 (21~27)\n\n# 문제 21\n# 22년 1분기 A제품의 매출액을 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    '판매수' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n\n\n\n날짜\n제품\n판매수\n개당수익\n\n\n\n\n0\n20220103\nA\n3\n300\n\n\n1\n20220105\nB\n5\n400\n\n\n2\n20230105\nA\n5\n500\n\n\n3\n20230127\nB\n10\n600\n\n\n4\n20220203\nA\n10\n400\n\n\n\n\n\n\n\n\n# # (풀이)\n# cond1 = df[df['날짜']&lt;='20220431']\n# cond2 = cond1[cond1['제품']=='A']\n# cond2['매출액'] = (cond2['판매수']*cond2['개당수익'])\n# cond2\n\n# print(cond2['매출액'].sum())\n\n\n# (풀이_v2)\n# 데이터 타입 datetime으로 변경\ndf['날짜'] = pd.to_datetime(df['날짜'])\n\n# 년, 월, 일 변수(열) 추가하기\ndf['year'] = df['날짜'].dt.year\ndf['month'] = df['날짜'].dt.month\ndf['day'] = df['날짜'].dt.day\n\n# 매출액 변수 추가하기\ndf['매출액'] = df['판매수'] * df['개당수익']\n\n# 22년으로 필터링\ndf = df[df['year']==2022]\ndf.head()\n\n# 1,2,3월 매출액 계산\nm1 = df[df['month']==1]['매출액'].sum()\nm2 = df[df['month']==2]['매출액'].sum()\nm3 = df[df['month']==3]['매출액'].sum()\nprint(m1+m2+m3)\n\n11900\n\n\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['날짜'] = pd.to_datetime(df['날짜'])\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['year'] = df['날짜'].dt.year\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['month'] = df['날짜'].dt.month\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['day'] = df['날짜'].dt.day\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['매출액'] = df['판매수'] * df['개당수익']\n\n\n\n# 문제 22\n# 22년과 23년의 총 매출액 차이를 절대값으로 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    '판매수' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n\n\n\n날짜\n제품\n판매수\n개당수익\n\n\n\n\n0\n20220103\nA\n3\n300\n\n\n1\n20220105\nB\n5\n400\n\n\n2\n20230105\nA\n5\n500\n\n\n3\n20230127\nB\n10\n600\n\n\n4\n20220203\nA\n10\n400\n\n\n\n\n\n\n\n\n# (풀이)\n# 데이터 타입 datetime으로 변경\ndf['날짜'] = pd.to_datetime(df['날짜'])\n\n# 년 변수(열) 추가하기\ndf['year'] = df['날짜'].dt.year\n\n# 매출액 변수 추가하기\ndf['매출액'] = df['판매수'] * df['개당수익']\n\ncond22 = df[df['year']==2022]\ncond23 = df[df['year']==2023]\n\nprint(abs((cond22['매출액'].sum()) - (cond23['매출액'].sum())))\n\n48600\n\n\n\n# 문제 23\n# 23년 총 매출액이 큰 제품의 23년 판매수를 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    '판매수' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n\n\n\n날짜\n제품\n판매수\n개당수익\n\n\n\n\n0\n20220103\nA\n3\n300\n\n\n1\n20220105\nB\n5\n400\n\n\n2\n20230105\nA\n5\n500\n\n\n3\n20230127\nB\n10\n600\n\n\n4\n20220203\nA\n10\n400\n\n\n\n\n\n\n\n\n# 데이터 타입 datetime으로 변경\ndf['날짜'] = pd.to_datetime(df['날짜'])\n\n# 년 변수(열) 추가하기\ndf['year'] = df['날짜'].dt.year\n\n# 매출액 변수 추가하기\ndf['매출액'] = df['판매수'] * df['개당수익']\n\ndf = df[df['year']==2023]\n# df.head()\n\n# 23년 A 매출액과 B 매출액 별도로 구하기\n# A 매출액\ndf_a = df[df['제품']=='A']\na_sales = df_a['매출액'].sum()\n# print(a_sales) # 46000\n\n# B 매출액\ndf_b = df[df['제품']=='B']\nb_sales = df_b['매출액'].sum()\n# print(b_sales) # 32500\n\n# A 제품의 23년 판매수\na_sum = df_a['판매수'].sum()\nprint(a_sum)\n\n80\n\n\n\n# 문제 24\n# 매출액이 4천원 초과, 1만원 미만인 데이터 수를 출력하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '날짜' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    '제품' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    '판매수' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    '개당수익' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n\n\n\n날짜\n제품\n판매수\n개당수익\n\n\n\n\n0\n20220103\nA\n3\n300\n\n\n1\n20220105\nB\n5\n400\n\n\n2\n20230105\nA\n5\n500\n\n\n3\n20230127\nB\n10\n600\n\n\n4\n20220203\nA\n10\n400\n\n\n\n\n\n\n\n\n# (풀이)\ndf['매출액'] = df['판매수'] * df['개당수익']\ncond1 = df['매출액']&gt;4000\ncond2 = df['매출액']&lt;10000\n\ndf = df[cond1&cond2]\nprint(len(df))\n\n4\n\n\n\n# 문제 25\n# 23년 9월 24일 16:00~22:00 사이에 전체 제품의 판매수를 구하시오\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600]})\ntime = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf['time'] = time\ndf = df[['time', '물품', '판매수', '개당수익']]\ndf\n\n\n\n\n\n\n\n\ntime\n물품\n판매수\n개당수익\n\n\n\n\n0\n2023-09-24 12:25:00\nA\n5\n500\n\n\n1\n2023-09-24 16:48:25\nB\n10\n600\n\n\n2\n2023-09-24 21:11:50\nA\n15\n500\n\n\n3\n2023-09-25 01:35:15\nB\n15\n600\n\n\n4\n2023-09-25 05:58:40\nA\n20\n600\n\n\n5\n2023-09-25 10:22:05\nB\n25\n700\n\n\n6\n2023-09-25 14:45:30\nA\n40\n600\n\n\n\n\n\n\n\n\n# (풀이)\n# 컬럼과 인덱스에 둘다 time 변수를 놓고 풀이\n# 데이터 타입 datetime으로 변경 (항상 df.info()로 데이터 타입 확인할 것)\ndf['time'] = pd.to_datetime(df['time'])\n\n# index 새로 지정\ndf = df.set_index('time', drop=False) # default는 True\n\n# 9월 24일 필터링 // df['변수'].between( , )\ndf_after = df[df['time'].between('2023-09-24', '2023-09-25')] # 25일은 미포함\n\n# 시간 필터링 16:00 ~ 22:00 (주의: 시간이 index에 위치해야 함)\ndf = df_after.between_time(start_time='16:00', end_time='22:00') # 포함 기준\n\n# 전체 판매수 구하기\nprint(df['판매수'].sum())\n\n25\n\n\n\n# (풀이2) // 위에 df 새로 불러와서 실행해보기\n# 데이터 타입 datetime으로 변경\ndf['time'] = pd.to_datetime(df['time'])\n\n# index 새로 지정\ndf = df.set_index('time')\n\n# loc 함수로 필터링\ndf = df.loc[(df.index &gt;= '2023-09-24 16:00:00') & (df.index &lt;= '2023-09-24 22:00:00')]\n\n# 전체 판매수 구하기\nprint(df['판매수'].sum())\n\n25\n\n\n\n# 문제 26\n# 9월 25일 00:10~12:00까지의 B물품의 매출액 총합을 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600]})\ndf['time'] = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf = df[['time', '물품', '판매수', '개당수익']]\ndf = df.set_index('time',drop=True)\ndf\n\n\n\n\n\n\n\n\n물품\n판매수\n개당수익\n\n\ntime\n\n\n\n\n\n\n\n2023-09-24 12:25:00\nA\n5\n500\n\n\n2023-09-24 16:48:25\nB\n10\n600\n\n\n2023-09-24 21:11:50\nA\n15\n500\n\n\n2023-09-25 01:35:15\nB\n15\n600\n\n\n2023-09-25 05:58:40\nA\n20\n600\n\n\n2023-09-25 10:22:05\nB\n25\n700\n\n\n2023-09-25 14:45:30\nA\n40\n600\n\n\n\n\n\n\n\n\n# (풀이)\n# 매출액 변수 추가\ndf['매출액'] = df['판매수'] * df['개당수익']\n\n# loc 함수로 필터링\ndf = df.loc[(df.index &gt;= '2023-09-25 00:10:00') & (df.index &lt;= '2023-09-25 12:00:00')]\n\n# B 물품의 매출액 총합\ncond1 = df[df['물품']=='B']\nprint(cond1['매출액'].sum())\n\n26500\n\n\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\1645811142.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['매출액'] = df['판매수'] * df['개당수익']\n\n\n\n# 문제 27\n# 9월 24일 12:00~24:00까지의 A물품의 매출액 총합을 구하시오\n# (매출액 = 판매수*개당수익)\n\n# 데이터 생성(수정금지)\ndf = pd.DataFrame({\n    '물품' : ['A','B','A','B','A','B','A'],\n    '판매수' : [5,10,15,15,20,25,40],\n    '개당수익' : [500,600,500,600,600,700,600]})\ndf['time'] = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf = df[['time', '물품', '판매수', '개당수익']]\ndf = df.set_index('time',drop=True)\ndf\n\n\n\n\n\n\n\n\n물품\n판매수\n개당수익\n\n\ntime\n\n\n\n\n\n\n\n2023-09-24 12:25:00\nA\n5\n500\n\n\n2023-09-24 16:48:25\nB\n10\n600\n\n\n2023-09-24 21:11:50\nA\n15\n500\n\n\n2023-09-25 01:35:15\nB\n15\n600\n\n\n2023-09-25 05:58:40\nA\n20\n600\n\n\n2023-09-25 10:22:05\nB\n25\n700\n\n\n2023-09-25 14:45:30\nA\n40\n600\n\n\n\n\n\n\n\n\n# (풀이)\n# 매출액 변수 추가\ndf['매출액'] = df['판매수'] * df['개당수익']\n\n# loc 함수로 필터링\ndf = df.loc[(df.index &gt;= '2023-09-24 12:00:00') & (df.index &lt; '2023-09-25 00:00:00')]\n\n# A 물품의 매출액 총합\ncond1 = df[df['물품']=='A']\nprint(cond1['매출액'].sum())\n\n10000"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_1.html",
    "href": "BigData_Analysis/실기_2유형_1.html",
    "title": "빅분기 실기 - 2유형 문제풀이(1)",
    "section": "",
    "text": "실기 2 유형(1)\n# !pip install IPython\nfrom IPython.display import Image\nImage('실기_2(1).png')\n지도학습만 현재까지 나옴\nImage('실기_2(2).png')"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_1.html#제2유형_연습하기_와인종류분류",
    "href": "BigData_Analysis/실기_2유형_1.html#제2유형_연습하기_와인종류분류",
    "title": "빅분기 실기 - 2유형 문제풀이(1)",
    "section": "제2유형_연습하기_와인종류분류",
    "text": "제2유형_연습하기_와인종류분류"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_1.html#데이터-분석-분서",
    "href": "BigData_Analysis/실기_2유형_1.html#데이터-분석-분서",
    "title": "빅분기 실기 - 2유형 문제풀이(1)",
    "section": "✅ 데이터 분석 분서",
    "text": "✅ 데이터 분석 분서\n\n1. 라이브러리 및 데이터 확인\n\n\n2. 데이터 탐색(EDA)\n\n\n3. 데이터 전처리 및 분리\n\n\n4. 모델링 및 성능평가\n\n\n5. 예측값 제출"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_1.html#라이브러리-및-데이터-확인-1",
    "href": "BigData_Analysis/실기_2유형_1.html#라이브러리-및-데이터-확인-1",
    "title": "빅분기 실기 - 2유형 문제풀이(1)",
    "section": "✅ 1. 라이브러리 및 데이터 확인",
    "text": "✅ 1. 라이브러리 및 데이터 확인\n\nimport pandas as pd\nimport numpy as np\n\n\n####### 실기환경 복사 영역 #######\nimport pandas as pd\nimport numpy as np\n# 실기 시험 데이터셋으로 세팅하기 (수정금지)\n\nfrom sklearn.datasets import load_wine\n# 와인 데이터셋을 로드합니다\nwine = load_wine()\nx = pd.DataFrame(wine.data, columns=wine.feature_names)\ny = pd.DataFrame(wine.target)\n\n# 실기 시험 데이터셋으로 세팅하기 (수정금지)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2,\n                                                   stratify=y,\n                                                   random_state=2023)\nx_test = pd.DataFrame(x_test)\nx_train = pd.DataFrame(x_train)\ny_train = pd.DataFrame(y_train)\n\nx_test.reset_index()\ny_train.columns = ['target']\n####### 실기환경 복사 영역 #######\n\n🍷 와인의 종류를 분류해보자\n\n데이터의 결측치, 이상치에 대해 처리하고\n분류모델을 사용하여 정확도, F1 score, AUC 값을 산출하시오\n제출은 result 변수에 담아 양식에 맞게 제출하시오\n\n\n# 데이터 설명\n# print(wine.DESCR)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_1.html#데이터-탐색eda-1",
    "href": "BigData_Analysis/실기_2유형_1.html#데이터-탐색eda-1",
    "title": "빅분기 실기 - 2유형 문제풀이(1)",
    "section": "✅ 2. 데이터 탐색(EDA)",
    "text": "✅ 2. 데이터 탐색(EDA)\n\n# 데이터 행/열 확인\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\n\n(142, 13)\n(36, 13)\n(142, 1)\n\n\n\n# 초기 데이터 확인\nprint(x_train.head(3))\nprint(x_test.head(3))\nprint(y_train.head(3))\n\n     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n52     13.82        1.75  2.42               14.0      111.0           3.88   \n146    13.88        5.04  2.23               20.0       80.0           0.98   \n44     13.05        1.77  2.10               17.0      107.0           3.00   \n\n     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n52         3.74                  0.32             1.87             7.05  1.01   \n146        0.34                  0.40             0.68             4.90  0.58   \n44         3.00                  0.28             2.03             5.04  0.88   \n\n     od280/od315_of_diluted_wines  proline  \n52                           3.26   1190.0  \n146                          1.33    415.0  \n44                           3.35    885.0  \n     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n168    13.58        2.58  2.69               24.5      105.0           1.55   \n144    12.25        3.88  2.20               18.5      112.0           1.38   \n151    12.79        2.67  2.48               22.0      112.0           1.48   \n\n     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n168        0.84                  0.39             1.54             8.66  0.74   \n144        0.78                  0.29             1.14             8.21  0.65   \n151        1.36                  0.24             1.26            10.80  0.48   \n\n     od280/od315_of_diluted_wines  proline  \n168                          1.80    750.0  \n144                          2.00    855.0  \n151                          1.47    480.0  \n     target\n52        0\n146       2\n44        0\n\n\n\n# 변수명과 데이터 타입이 매칭이 되는지, 결측치가 있는지 확인\nprint(x_train.info())\nprint(x_test.info())\nprint(y_train.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 142 entries, 52 to 115\nData columns (total 13 columns):\n #   Column                        Non-Null Count  Dtype  \n---  ------                        --------------  -----  \n 0   alcohol                       142 non-null    float64\n 1   malic_acid                    142 non-null    float64\n 2   ash                           142 non-null    float64\n 3   alcalinity_of_ash             142 non-null    float64\n 4   magnesium                     142 non-null    float64\n 5   total_phenols                 142 non-null    float64\n 6   flavanoids                    142 non-null    float64\n 7   nonflavanoid_phenols          142 non-null    float64\n 8   proanthocyanins               142 non-null    float64\n 9   color_intensity               142 non-null    float64\n 10  hue                           142 non-null    float64\n 11  od280/od315_of_diluted_wines  142 non-null    float64\n 12  proline                       142 non-null    float64\ndtypes: float64(13)\nmemory usage: 15.5 KB\nNone\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 36 entries, 168 to 55\nData columns (total 13 columns):\n #   Column                        Non-Null Count  Dtype  \n---  ------                        --------------  -----  \n 0   alcohol                       36 non-null     float64\n 1   malic_acid                    36 non-null     float64\n 2   ash                           36 non-null     float64\n 3   alcalinity_of_ash             36 non-null     float64\n 4   magnesium                     36 non-null     float64\n 5   total_phenols                 36 non-null     float64\n 6   flavanoids                    36 non-null     float64\n 7   nonflavanoid_phenols          36 non-null     float64\n 8   proanthocyanins               36 non-null     float64\n 9   color_intensity               36 non-null     float64\n 10  hue                           36 non-null     float64\n 11  od280/od315_of_diluted_wines  36 non-null     float64\n 12  proline                       36 non-null     float64\ndtypes: float64(13)\nmemory usage: 3.9 KB\nNone\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 142 entries, 52 to 115\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype\n---  ------  --------------  -----\n 0   target  142 non-null    int32\ndtypes: int32(1)\nmemory usage: 1.7 KB\nNone\n\n\n\n# x_train과 x_test 데이터의 기초통계량을 잘 비교해보세요\nprint(x_train.describe())\nprint(x_test.describe())\nprint(y_train.describe())\n\n          alcohol  malic_acid         ash  alcalinity_of_ash   magnesium  \\\ncount  142.000000  142.000000  142.000000         142.000000  142.000000   \nmean    13.025915    2.354296    2.340211          19.354225   98.732394   \nstd      0.812423    1.142722    0.279910           3.476825   13.581859   \nmin     11.030000    0.740000    1.360000          10.600000   70.000000   \n25%     12.370000    1.610000    2.190000          16.800000   88.000000   \n50%     13.050000    1.820000    2.320000          19.300000   97.000000   \n75%     13.685000    3.115000    2.510000          21.500000  106.750000   \nmax     14.830000    5.800000    3.230000          30.000000  151.000000   \n\n       total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\ncount     142.000000  142.000000            142.000000       142.000000   \nmean        2.303592    2.043592              0.361479         1.575070   \nstd         0.633955    1.033597              0.124627         0.576798   \nmin         0.980000    0.340000              0.140000         0.410000   \n25%         1.757500    1.227500              0.270000         1.242500   \n50%         2.335000    2.100000              0.325000         1.555000   \n75%         2.800000    2.917500              0.437500         1.950000   \nmax         3.880000    5.080000              0.630000         3.580000   \n\n       color_intensity         hue  od280/od315_of_diluted_wines      proline  \ncount       142.000000  142.000000                    142.000000   142.000000  \nmean          5.005070    0.950394                      2.603592   742.112676  \nstd           2.150186    0.220736                      0.709751   317.057395  \nmin           1.280000    0.540000                      1.270000   290.000000  \n25%           3.300000    0.782500                      1.922500   496.250000  \n50%           4.850000    0.960000                      2.780000   660.000000  \n75%           6.122500    1.097500                      3.170000   981.250000  \nmax          13.000000    1.710000                      3.920000  1680.000000  \n         alcohol  malic_acid        ash  alcalinity_of_ash   magnesium  \\\ncount  36.000000   36.000000  36.000000           36.00000   36.000000   \nmean   12.900833    2.265556   2.470278           20.05000  103.722222   \nstd     0.813112    1.021943   0.226066            2.70275   16.371772   \nmin    11.640000    0.890000   2.000000           14.60000   84.000000   \n25%    12.230000    1.592500   2.300000           18.00000   91.500000   \n50%    12.835000    1.885000   2.470000           19.50000  101.000000   \n75%    13.635000    2.792500   2.605000           21.70000  112.000000   \nmax    14.390000    4.950000   3.220000           26.50000  162.000000   \n\n       total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\ncount      36.000000   36.000000             36.000000        36.000000   \nmean        2.261667    1.972778              0.363333         1.653333   \nstd         0.600259    0.858882              0.125516         0.558012   \nmin         1.350000    0.660000              0.130000         0.840000   \n25%         1.715000    1.175000              0.267500         1.320000   \n50%         2.420000    2.175000              0.395000         1.550000   \n75%         2.602500    2.682500              0.435000         1.972500   \nmax         3.850000    3.490000              0.660000         3.280000   \n\n       color_intensity        hue  od280/od315_of_diluted_wines     proline  \ncount        36.000000  36.000000                     36.000000    36.00000  \nmean          5.267222   0.985278                      2.643611   765.75000  \nstd           2.915076   0.258694                      0.720100   309.94851  \nmin           2.080000   0.480000                      1.290000   278.00000  \n25%           2.875000   0.787500                      2.037500   542.50000  \n50%           4.325000   0.985000                      2.790000   682.50000  \n75%           6.900000   1.167500                      3.192500   996.25000  \nmax          11.750000   1.450000                      4.000000  1480.00000  \n           target\ncount  142.000000\nmean     0.936620\nstd      0.773816\nmin      0.000000\n25%      0.000000\n50%      1.000000\n75%      2.000000\nmax      2.000000\n\n\n\n# y 데이터도 구체적으로 살펴볼 필요있음\nprint(y_train.head())\n\nprint(y_train.value_counts())\n\n     target\n52        0\n146       2\n44        0\n67        1\n43        0\ntarget\n1         57\n0         47\n2         38\ndtype: int64"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_1.html#데이터-전처리-및-분리-1",
    "href": "BigData_Analysis/실기_2유형_1.html#데이터-전처리-및-분리-1",
    "title": "빅분기 실기 - 2유형 문제풀이(1)",
    "section": "✅ 3. 데이터 전처리 및 분리",
    "text": "✅ 3. 데이터 전처리 및 분리\n\n1) 결측치, 2) 이상치, 3) 변수 처리하기\n\n# 결측치 확인\nprint(x_train.isnull().sum())\nprint(x_test.isnull().sum())\nprint(y_train.isnull().sum())\n\nalcohol                         0\nmalic_acid                      0\nash                             0\nalcalinity_of_ash               0\nmagnesium                       0\ntotal_phenols                   0\nflavanoids                      0\nnonflavanoid_phenols            0\nproanthocyanins                 0\ncolor_intensity                 0\nhue                             0\nod280/od315_of_diluted_wines    0\nproline                         0\ndtype: int64\nalcohol                         0\nmalic_acid                      0\nash                             0\nalcalinity_of_ash               0\nmagnesium                       0\ntotal_phenols                   0\nflavanoids                      0\nnonflavanoid_phenols            0\nproanthocyanins                 0\ncolor_intensity                 0\nhue                             0\nod280/od315_of_diluted_wines    0\nproline                         0\ndtype: int64\ntarget    0\ndtype: int64\n\n\n\n# 결측치 제거\n# df = df.dropna()\n# print(df)\n\n# 참고사항\n# print(df.dropna().shape) # 행기준으로 삭제\n\n\n# 결측치 대체(평균값, 중앙값, 최빈값)\n# 연속형 변수 : 중앙값, 평균값\n#     - df['변수명'].median()\n#     - df['변수명'].mean()\n# 범주형 변수 : 최빈값\n\n# df['변수명'] = df['변수명'].fillna(대체할 값)\n\n\n# 이상치 대체(예시)\n# df['변수명'] = np.where(df['변수명']&gt;=5, 대체할 값, df['변수명'])\n\n\n# 변수처리\n\n# 불필요한 변수 제거\n# df = df.drop(columns = ['변수1', '변수2'])\n# df = df.drop(['변수1', '변수2'], axis=1)\n\n# 필요시 변수 추가(파생변수 생성)\n# df['파생변수명'] = df['A'] * df['B'] (파생변수 생성 수식)\n\n# 원핫인코딩(가변수 처리)\n# x_train = pd.get_dummies(x_train)\n# x_test = pd.get_dummies(x_test)\n# print(x_train.info())\n# print(x_test.info())\n\n\n\n데이터 분리\n\n# 데이터를 훈련 세트와 검증용 세트로 분할 (80% 훈련, 20% 검증용)\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train['target'],\n                                                   test_size = 0.2,\n                                                   stratify = y_train['target'],\n                                                   random_state = 2023)\nprint(x_train.shape)\nprint(x_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)\n\n(113, 13)\n(29, 13)\n(113,)\n(29,)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_1.html#모델링-및-성능평가-1",
    "href": "BigData_Analysis/실기_2유형_1.html#모델링-및-성능평가-1",
    "title": "빅분기 실기 - 2유형 문제풀이(1)",
    "section": "✅ 4. 모델링 및 성능평가",
    "text": "✅ 4. 모델링 및 성능평가\n\n# 랜덤포레스트 모델 사용 (참고: 회귀모델은 RandomForestRegressor)\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nmodel.fit(x_train, y_train)\n\nRandomForestClassifier()\n\n\n\n# 모델을 사용하여 테스트 데이터 예측\ny_pred = model.predict(x_val)\n\n\n# 모델 성늘 평가 (정확도, F1 score, 민감도, 특이도 등)\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score\nacc = accuracy_score(y_val, y_pred)              # (실제값, 예측값)\nf1 = f1_score(y_val, y_pred, average = 'macro')  # (실제값, 예측값)\n# auc = roc_auc_score(y_val, y_pred)             # (실제값, 예측값)  * AUC는 이진분류\n\n\n# 정확도(Accuracy)\nprint(acc)\n\n1.0\n\n\n\n# macro f1 score\nprint(f1)\n\n1.0"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_1.html#예측값-제출-1",
    "href": "BigData_Analysis/실기_2유형_1.html#예측값-제출-1",
    "title": "빅분기 실기 - 2유형 문제풀이(1)",
    "section": "✅ 5. 예측값 제출",
    "text": "✅ 5. 예측값 제출\n\n(주의) test 셋을 모델에 넣어 나온 예측값을 제출해야함\n\n# (실기시험 안내사항)\n# 아래 코드 예측변수와 수험번호를 개인별로 변경하여 활용\n# pd.DataFrame({ 'result':y_result }).to_csv('수험번호_csv', index=False)\n\n# 모델을 사용하여 테스트 데이터 예측\n\n# 1. 특정 클래스로 분류할 경우 (predict)\ny_result = model.predict(x_test)\nprint(y_result[:5])\n\n# 2. 특정 클래스로 분류될 확률을 구할 경우 (predict_proba)\ny_result_prob = model.predict_proba(x_test)\nprint(y_result_prob[:5])\n\n# 이해해보기\nresult_prob = pd.DataFrame({\n    'result' : y_result,\n    'prob_0' : y_result_prob[:, 0],\n    'prob_1' : y_result_prob[:, 1],\n    'prob_2' : y_result_prob[:, 2],\n})\n# Class 0일 확률 : y_result_prob[:, 0]\n# Class 1일 확률 : y_result_prob[:, 1]\n# Class 2일 확률 : y_result_prob[:, 2]\n\nprint(result_prob[:5])\n\n[2 2 2 0 1]\n[[0.01 0.05 0.94]\n [0.13 0.17 0.7 ]\n [0.   0.08 0.92]\n [0.99 0.01 0.  ]\n [0.08 0.81 0.11]]\n   result  prob_0  prob_1  prob_2\n0       2    0.01    0.05    0.94\n1       2    0.13    0.17    0.70\n2       2    0.00    0.08    0.92\n3       0    0.99    0.01    0.00\n4       1    0.08    0.81    0.11"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html",
    "href": "BigData_Analysis/실기_2유형_3.html",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "",
    "text": "실기 2 유형(3)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html#데이터-분석-순서",
    "href": "BigData_Analysis/실기_2유형_3.html#데이터-분석-순서",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "✅ 데이터 분석 순서",
    "text": "✅ 데이터 분석 순서\n\n1. 라이브러리 및 데이터 확인\n\n\n2. 데이터 탐색(EDA)\n\n\n3. 데이터 전처리 및 분리\n\n\n4. 모델링 및 성능평가\n\n\n5. 예측값 제출"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html#라이브러리-및-데이터-확인-1",
    "href": "BigData_Analysis/실기_2유형_3.html#라이브러리-및-데이터-확인-1",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "✅ 1. 라이브러리 및 데이터 확인",
    "text": "✅ 1. 라이브러리 및 데이터 확인\n\nimport pandas as pd\nimport numpy as np\n\n\n###### 실기환경 복사 영역 ######\n# 실기 시험 데이터셋으로 셋팅하기 (수정금지)\n\nimport seaborn as sns\n# tips 데이터셋 로드\ndf = sns.load_dataset('tips')\n\nx = df.drop(['tip'], axis=1)\ny = df['tip']\n\n\n# 실시 시험 데이터셋으로 셋팅하기 (수정금지)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2,\n                                                   random_state = 2023)\n\nx_test = pd.DataFrame(x_test.reset_index())\nx_train = pd.DataFrame(x_train.reset_index())\ny_train = pd.DataFrame(y_train.reset_index())\n\nx_test.rename(columns = {'index':'cust_id'}, inplace=True)\nx_train.rename(columns = {'index':'cust_id'}, inplace=True)\ny_train.columns = ['cust_id', 'target']\n###### 실기환경 복사 영역 ######"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html#레스토랑의-예측-문제",
    "href": "BigData_Analysis/실기_2유형_3.html#레스토랑의-예측-문제",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "레스토랑의 예측 문제",
    "text": "레스토랑의 예측 문제"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html#데이터의-결측치-이상치-변수에-대해-처리하고",
    "href": "BigData_Analysis/실기_2유형_3.html#데이터의-결측치-이상치-변수에-대해-처리하고",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "- 데이터의 결측치, 이상치, 변수에 대해 처리하고",
    "text": "- 데이터의 결측치, 이상치, 변수에 대해 처리하고"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html#회귀모델을-사용하여-rsq-mse-값을-산출하시오",
    "href": "BigData_Analysis/실기_2유형_3.html#회귀모델을-사용하여-rsq-mse-값을-산출하시오",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "- 회귀모델을 사용하여 Rsq, MSE 값을 산출하시오",
    "text": "- 회귀모델을 사용하여 Rsq, MSE 값을 산출하시오"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html#데이터셋-설명",
    "href": "BigData_Analysis/실기_2유형_3.html#데이터셋-설명",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "데이터셋 설명",
    "text": "데이터셋 설명\n\n\ntotal_bill(): 손님의 식사 총 청구액\ntip(팁): 팁의 양\nsex(성별): 손님의 성별\nsmoker(흡연자): 손님의 흡연 여부(“Yes” 또는 “No”)\nday(요일): 식사가 이루어진 요일\ntime(시간): 점심 또는 저녁 중 언제 식사가 이루어졌는지\nsize(인원 수): 식사에 참석한 인원 수"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html#데이터-탐색eda-1",
    "href": "BigData_Analysis/실기_2유형_3.html#데이터-탐색eda-1",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "✅ 2. 데이터 탐색(EDA)",
    "text": "✅ 2. 데이터 탐색(EDA)\n\n# 데이터의 행/열 확인\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\n\n(195, 7)\n(49, 7)\n(195, 2)\n\n\n\n# 초기 데이터 확인\n\nprint(x_train.head(3))\nprint(x_test.head(3))\nprint(y_train.head(3))\n\n   cust_id  total_bill     sex smoker  day    time  size\n0      158       13.39  Female     No  Sun  Dinner     2\n1      186       20.90  Female    Yes  Sun  Dinner     3\n2       21       20.29  Female     No  Sat  Dinner     2\n   cust_id  total_bill     sex smoker  day    time  size\n0      154       19.77    Male     No  Sun  Dinner     4\n1        4       24.59  Female     No  Sun  Dinner     4\n2       30        9.55    Male     No  Sat  Dinner     2\n   cust_id  target\n0      158    2.61\n1      186    3.50\n2       21    2.75\n\n\n\n# 변수명과 데이터 타입이 매칭이 되는지, 결측치가 있는지 확인해보세요\n\nprint(x_train.info())\nprint(x_test.info())\nprint(y_train.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 195 entries, 0 to 194\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   cust_id     195 non-null    int64   \n 1   total_bill  195 non-null    float64 \n 2   sex         195 non-null    category\n 3   smoker      195 non-null    category\n 4   day         195 non-null    category\n 5   time        195 non-null    category\n 6   size        195 non-null    int64   \ndtypes: category(4), float64(1), int64(2)\nmemory usage: 6.0 KB\nNone\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 49 entries, 0 to 48\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   cust_id     49 non-null     int64   \n 1   total_bill  49 non-null     float64 \n 2   sex         49 non-null     category\n 3   smoker      49 non-null     category\n 4   day         49 non-null     category\n 5   time        49 non-null     category\n 6   size        49 non-null     int64   \ndtypes: category(4), float64(1), int64(2)\nmemory usage: 2.0 KB\nNone\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 195 entries, 0 to 194\nData columns (total 2 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   cust_id  195 non-null    int64  \n 1   target   195 non-null    float64\ndtypes: float64(1), int64(1)\nmemory usage: 3.2 KB\nNone\n\n\n\n# x_train 과 x_test 데이터의 기초통계량을 잘 비교해보세요\n\nprint(x_train.describe())\nprint(x_test.describe())\nprint(y_train.describe())\n\n          cust_id  total_bill        size\ncount  195.000000  195.000000  195.000000\nmean   122.056410   20.054667    2.543590\nstd     70.668034    8.961645    0.942631\nmin      0.000000    3.070000    1.000000\n25%     59.500000   13.420000    2.000000\n50%    121.000000   17.920000    2.000000\n75%    182.500000   24.395000    3.000000\nmax    243.000000   50.810000    6.000000\n          cust_id  total_bill       size\ncount   49.000000   49.000000  49.000000\nmean   119.285714   18.716531   2.673469\nstd     70.918674    8.669864   0.987162\nmin      2.000000    5.750000   2.000000\n25%     62.000000   12.740000   2.000000\n50%    123.000000   16.660000   2.000000\n75%    180.000000   21.010000   3.000000\nmax    239.000000   44.300000   6.000000\n          cust_id      target\ncount  195.000000  195.000000\nmean   122.056410    3.021692\nstd     70.668034    1.402690\nmin      0.000000    1.000000\n25%     59.500000    2.000000\n50%    121.000000    2.920000\n75%    182.500000    3.530000\nmax    243.000000   10.000000\n\n\n\n# object, category 데이터도 추가 확인\n# print(x_trian.describe(include='object'))\n# print(x_test.describe(include='object'))\n\nprint(x_train.describe(include='category'))\nprint(x_test.describe(include='category'))\n\n         sex smoker  day    time\ncount    195    195  195     195\nunique     2      2    4       2\ntop     Male     No  Sat  Dinner\nfreq     125    120   71     142\n         sex smoker  day    time\ncount     49     49   49      49\nunique     2      2    4       2\ntop     Male     No  Sat  Dinner\nfreq      32     31   16      34\n\n\n\n# y데이터도 구체적으로 살펴보세요\nprint(y_train.head())\n\n   cust_id  target\n0      158    2.61\n1      186    3.50\n2       21    2.75\n3       74    2.20\n4       43    1.32\n\n\n\n# y데이터도 구체적으로 살펴보세요\nprint(y_train.describe().T)\n\n         count        mean        std  min   25%     50%     75%    max\ncust_id  195.0  122.056410  70.668034  0.0  59.5  121.00  182.50  243.0\ntarget   195.0    3.021692   1.402690  1.0   2.0    2.92    3.53   10.0"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html#데이터-전처리-및-분리-1",
    "href": "BigData_Analysis/실기_2유형_3.html#데이터-전처리-및-분리-1",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "✅ 3. 데이터 전처리 및 분리",
    "text": "✅ 3. 데이터 전처리 및 분리\n\n1) 결측치, 2) 이상치, 3) 변수 처리하기\n\n# 결측치 확인\n\nprint(x_train.isnull().sum())\nprint(x_test.isnull().sum())\nprint(y_train.isnull().sum())\n\ncust_id       0\ntotal_bill    0\nsex           0\nsmoker        0\nday           0\ntime          0\nsize          0\ndtype: int64\ncust_id       0\ntotal_bill    0\nsex           0\nsmoker        0\nday           0\ntime          0\nsize          0\ndtype: int64\ncust_id    0\ntarget     0\ndtype: int64\n\n\n\n# 결측치 제거\n# df = df.dropna()\n# print(df)\n\n# 참고사항\n# print(df.dropna().shape) # 행 기준으로 삭제\n\n\n# 결측치 대체 (평균값, 중앙값, 최빈값)\n# 연속형 변수 :  중앙값, 평균값\n# df['변수명'].median()\n# df['변수명'].mean()\n\n# 범주형 변수 :  최빈값\n# df['변수명'].mode()\n\n\n# df['변수명'] = df['변수명'].fillna(대체할값)\n\n\n# 이상치 대체\n# (참고) df['변수명'] = np.where(df['변수명'] &gt;= 5, 대체할 값, df['변수명'])\n\n\n# 변수처리\n\n# 불필요한 변수 제거\n# df = df.drop(columns = ['변수1', '변수2'])\n# df = df.drop(['변수1', '변수2'], axis=1)\n\n# 필요시 변수 추가(파생변수 생성)\n# df['파생변수명'] = df['A'] * df['B'] (파생변수 생성 수식)\n\n# 원핫인코딩(가변수 처리)\n# x_train = pd.get_dummies(x_train)\n# x_test = pd.get_dummies(x_test)\n\n# print(x_train.info())\n# print(x_test.info())\n\n\n# 변수처리\n\n# 불필요한 변수(columns) 제거\n# cust_id 는 불필요한 변수이므로 제거합니다.\n# 단, test셋의 cust_id가 나중에 제출이 필요하기 떄문에 별도로 저장\n\ncust_id = x_test['cust_id'].copy()\n\n# 각 데이터에서 cust_id 변수 제거\nx_train = x_train.drop(columns = ['cust_id']) # drop(columns = ['변수1', '변수2'])\nx_test = x_test.drop(columns = ['cust_id'])\n\n\n# 변수처리(원핫인코딩)\nprint(x_train.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 195 entries, 0 to 194\nData columns (total 6 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   total_bill  195 non-null    float64 \n 1   sex         195 non-null    category\n 2   smoker      195 non-null    category\n 3   day         195 non-null    category\n 4   time        195 non-null    category\n 5   size        195 non-null    int64   \ndtypes: category(4), float64(1), int64(1)\nmemory usage: 4.5 KB\nNone\n\n\n\n# 변수처리(원핫인코딩)\nx_train = pd.get_dummies(x_train)\nx_test = pd.get_dummies(x_test)\n\nprint(x_train.info())\nprint(x_test.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 195 entries, 0 to 194\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   total_bill   195 non-null    float64\n 1   size         195 non-null    int64  \n 2   sex_Male     195 non-null    uint8  \n 3   sex_Female   195 non-null    uint8  \n 4   smoker_Yes   195 non-null    uint8  \n 5   smoker_No    195 non-null    uint8  \n 6   day_Thur     195 non-null    uint8  \n 7   day_Fri      195 non-null    uint8  \n 8   day_Sat      195 non-null    uint8  \n 9   day_Sun      195 non-null    uint8  \n 10  time_Lunch   195 non-null    uint8  \n 11  time_Dinner  195 non-null    uint8  \ndtypes: float64(1), int64(1), uint8(10)\nmemory usage: 5.1 KB\nNone\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 49 entries, 0 to 48\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   total_bill   49 non-null     float64\n 1   size         49 non-null     int64  \n 2   sex_Male     49 non-null     uint8  \n 3   sex_Female   49 non-null     uint8  \n 4   smoker_Yes   49 non-null     uint8  \n 5   smoker_No    49 non-null     uint8  \n 6   day_Thur     49 non-null     uint8  \n 7   day_Fri      49 non-null     uint8  \n 8   day_Sat      49 non-null     uint8  \n 9   day_Sun      49 non-null     uint8  \n 10  time_Lunch   49 non-null     uint8  \n 11  time_Dinner  49 non-null     uint8  \ndtypes: float64(1), int64(1), uint8(10)\nmemory usage: 1.4 KB\nNone\n\n\n\n\n데이터 분리\n\n# 데이터를 훈련 세트와 검증용 세트로 분할 (80% 훈련, 20% 검증용)\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train['target'],\n                                                   test_size = 0.2,\n                                                   random_state = 23)\n# 분류 모델에서는 층화(starify)를 할 필요가 없다. 연속형인 경우에만 사용\n\nprint(x_train.shape)\nprint(x_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)\n\n(156, 12)\n(39, 12)\n(156,)\n(39,)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html#모델링-밑-성능평가",
    "href": "BigData_Analysis/실기_2유형_3.html#모델링-밑-성능평가",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "✅ 4. 모델링 밑 성능평가",
    "text": "✅ 4. 모델링 밑 성능평가\n\n# 랜덤포레스트 모델 사용 (참고: 분류모델은 RandomForestClassifier)\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(random_state=2023)\nmodel.fit(x_train, y_train)\n\nRandomForestRegressor(random_state=2023)\n\n\n\n# 모델을 사용하여 테스트 데이터 예측\ny_pred = model.predict(x_val)\n\n\n# 모델 성능 평가 (R-squared, MSE 등)\nfrom sklearn.metrics import r2_score, mean_squared_error\nr2 = r2_score(y_val, y_pred)            # (실제값, 예측값)\nmse = mean_squared_error(y_val, y_pred) # (실제값, 예측값)\n\n\n# MSE\nprint(mse)\n\n0.9812277338461534\n\n\n\n# RMSE\nrmse = mse**0.5\nprint(rmse)\n\n0.990569398803614\n\n\n\n# R2 score(R-squared)\nprint(r2)\n\n0.4286497615634072"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_3.html#예측값-제출-1",
    "href": "BigData_Analysis/실기_2유형_3.html#예측값-제출-1",
    "title": "빅분기 실기 - 2유형 문제풀이(3)",
    "section": "✅ 5. 예측값 제출",
    "text": "✅ 5. 예측값 제출\n\n(주의) x_test 를 모델에 넣어 나온 예측값을 제출해야함\n\n#(실기시험 안내사항)\n# 아래 코드 예측변수와 수험번호를 개인별로 변경하여 활용\n# pd.DataFrame({'cust_id':cust_id, 'target':y_result}).to_csv('0030000.csv', index=False)\n\n# 모델을 사용하여 테스트 데이터 예측\ny_result = model.predict(x_test)\nresult = pd.DataFrame({'cust_id':cust_id, 'target':y_result})\nprint(result[:5])\n\n   cust_id  target\n0      154  3.2266\n1        4  4.1160\n2       30  1.8966\n3       75  1.8735\n4       33  3.0267\n\n\n\n# ★tip : 데이터를 저장한 다음 불러와서 제대로 제출했는지 확인해보자\n# pd.DataFrame({'result':y_result}).to_csv('수험번호.csv', index=False)\n# df2 = pd.read_csv(\"수험번호.csv\")\n# print(df2.head())"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_5.html",
    "href": "BigData_Analysis/실기_2유형_5.html",
    "title": "빅분기 실기 - 2유형 문제풀이(5)",
    "section": "",
    "text": "실기 2 유형(5)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_5.html#데이터-분석-순서",
    "href": "BigData_Analysis/실기_2유형_5.html#데이터-분석-순서",
    "title": "빅분기 실기 - 2유형 문제풀이(5)",
    "section": "✅ 데이터 분석 순서",
    "text": "✅ 데이터 분석 순서\n\n1. 라이브러리 및 데이터 확인\n\n\n2. 데이터 탐색(EDA)\n\n\n3. 데이터 전처리 및 분리\n\n\n4. 모델링 및 성능평가\n\n\n5. 예측값 제출"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_5.html#라이브러리-및-데이터-확인-1",
    "href": "BigData_Analysis/실기_2유형_5.html#라이브러리-및-데이터-확인-1",
    "title": "빅분기 실기 - 2유형 문제풀이(5)",
    "section": "✅ 1. 라이브러리 및 데이터 확인",
    "text": "✅ 1. 라이브러리 및 데이터 확인\n\nimport pandas as pd\nimport numpy as np\n\n\n###############  복사 영역  ###############\n# 실기 시험 데이터셋으로 셋팅하기 (수정금지)\n\n# Seaborn의 내장 타이타닉 데이터셋을 불러옵니다.\nimport seaborn as sns\ndf = sns.load_dataset('titanic')\n\nx = df.drop('survived', axis=1)\ny = df['survived']\n\n# 실기 시험 데이터셋으로 셋팅하기 (수정금지)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, \n                                                    stratify=y,\n                                                    random_state=2023)\n\nx_test = pd.DataFrame(x_test)\nx_train = pd.DataFrame(x_train)\ny_train = pd.DataFrame(y_train)\ny_test = pd.DataFrame(y_test) # 평가용\n\nx_test.reset_index()\ny_train.columns = ['target'] \ny_test.columns = ['target'] \n###############  복사 영역  ###############\n\n\n타이타닉 생존자 예측 문제\n\n\n- 데이터의 결측치, 중복 변수값에 대해 처리하고\n\n\n- 분류모델을 사용하여 Accuracy, F1 score, AUC 값을 산출하시오.\n\n\n데이터 설명\n\n\nsurvival : 0 = No, 1 = Yes -pclass : 객실 등급(1,2,3) -sex : 성별 -age : 나이 -sibsp : 타이타닉호에 탑승한 형제/배우자의 수 -parch : 타이타닉호에 탑승한 부모/자녀의 수 -fare : 요금 -embarked : 탑승지 이름(C, Q, S) Cherbourg / Queenstown / Southampton -(중복)class : 객실 등급(First, Second, Third) -who : man, women, child -adult_male : 성인남자인지 여부(True=성인남자, False 그외) -deck : 선실번호 첫 알파벳(A,B,C,D,E,F,G) -(중복) embark_town : 탑승지 이름(Cherbourg, Queenstown, Southampton) -(중복) alive : 생존여부(no:사망, yes:생존) -alone : 혼자 탑승했는지 여부(True=혼자, False=가족과 함께)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_5.html#데이터-탐색eda-1",
    "href": "BigData_Analysis/실기_2유형_5.html#데이터-탐색eda-1",
    "title": "빅분기 실기 - 2유형 문제풀이(5)",
    "section": "✅ 2. 데이터 탐색(EDA)",
    "text": "✅ 2. 데이터 탐색(EDA)\n\n# 데이터의 행/열 확인\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\n\n(712, 14)\n(179, 14)\n(712, 1)\n\n\n\n# 초기 데이터 확인\n\nprint(x_train.head())\nprint(x_test.head())\nprint(y_train.head())\n\n     pclass     sex   age  sibsp  parch   fare embarked   class    who  \\\n3         1  female  35.0      1      0  53.10        S   First  woman   \n517       3    male   NaN      0      0  24.15        Q   Third    man   \n861       2    male  21.0      1      0  11.50        S  Second    man   \n487       1    male  58.0      0      0  29.70        C   First    man   \n58        2  female   5.0      1      2  27.75        S  Second  child   \n\n     adult_male deck  embark_town alive  alone  \n3         False    C  Southampton   yes  False  \n517        True  NaN   Queenstown    no   True  \n861        True  NaN  Southampton    no  False  \n487        True    B    Cherbourg    no   True  \n58        False  NaN  Southampton   yes  False  \n     pclass     sex   age  sibsp  parch      fare embarked   class    who  \\\n800       2    male  34.0      0      0   13.0000        S  Second    man   \n341       1  female  24.0      3      2  263.0000        S   First  woman   \n413       2    male   NaN      0      0    0.0000        S  Second    man   \n575       3    male  19.0      0      0   14.5000        S   Third    man   \n202       3    male  34.0      0      0    6.4958        S   Third    man   \n\n     adult_male deck  embark_town alive  alone  \n800        True  NaN  Southampton    no   True  \n341       False    C  Southampton   yes  False  \n413        True  NaN  Southampton    no   True  \n575        True  NaN  Southampton    no   True  \n202        True  NaN  Southampton    no   True  \n     target\n3         1\n517       0\n861       0\n487       0\n58        1\n\n\n\n# 변수명과 데이터 타입이 매칭이 되는지, 결측치가 있는지 확인해보세요\n\nprint(x_train.info())\nprint(x_test.info())\nprint(y_train.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 712 entries, 3 to 608\nData columns (total 14 columns):\n #   Column       Non-Null Count  Dtype   \n---  ------       --------------  -----   \n 0   pclass       712 non-null    int64   \n 1   sex          712 non-null    object  \n 2   age          579 non-null    float64 \n 3   sibsp        712 non-null    int64   \n 4   parch        712 non-null    int64   \n 5   fare         712 non-null    float64 \n 6   embarked     710 non-null    object  \n 7   class        712 non-null    category\n 8   who          712 non-null    object  \n 9   adult_male   712 non-null    bool    \n 10  deck         164 non-null    category\n 11  embark_town  710 non-null    object  \n 12  alive        712 non-null    object  \n 13  alone        712 non-null    bool    \ndtypes: bool(2), category(2), float64(2), int64(3), object(5)\nmemory usage: 64.4+ KB\nNone\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 179 entries, 800 to 410\nData columns (total 14 columns):\n #   Column       Non-Null Count  Dtype   \n---  ------       --------------  -----   \n 0   pclass       179 non-null    int64   \n 1   sex          179 non-null    object  \n 2   age          135 non-null    float64 \n 3   sibsp        179 non-null    int64   \n 4   parch        179 non-null    int64   \n 5   fare         179 non-null    float64 \n 6   embarked     179 non-null    object  \n 7   class        179 non-null    category\n 8   who          179 non-null    object  \n 9   adult_male   179 non-null    bool    \n 10  deck         39 non-null     category\n 11  embark_town  179 non-null    object  \n 12  alive        179 non-null    object  \n 13  alone        179 non-null    bool    \ndtypes: bool(2), category(2), float64(2), int64(3), object(5)\nmemory usage: 16.6+ KB\nNone\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 712 entries, 3 to 608\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype\n---  ------  --------------  -----\n 0   target  712 non-null    int64\ndtypes: int64(1)\nmemory usage: 11.1 KB\nNone\n\n\n\n# x_train 과 x_test 데이터의 기초통계량을 잘 비교해보세요\n\nprint(x_train.describe())\nprint(x_test.describe())\nprint(y_train.describe())\n\n           pclass         age       sibsp       parch        fare\ncount  712.000000  579.000000  712.000000  712.000000  712.000000\nmean     2.307584   29.479568    0.518258    0.372191   31.741836\nstd      0.834926   14.355304    1.094522    0.792341   45.403910\nmin      1.000000    0.420000    0.000000    0.000000    0.000000\n25%      2.000000   20.000000    0.000000    0.000000    7.895800\n50%      3.000000   28.000000    0.000000    0.000000   14.454200\n75%      3.000000   38.000000    1.000000    0.000000   31.275000\nmax      3.000000   74.000000    8.000000    6.000000  512.329200\n           pclass         age       sibsp       parch        fare\ncount  179.000000  135.000000  179.000000  179.000000  179.000000\nmean     2.312849   30.640741    0.541899    0.418994   34.043364\nstd      0.842950   15.258427    1.137797    0.859760   64.097184\nmin      1.000000    1.000000    0.000000    0.000000    0.000000\n25%      2.000000   22.000000    0.000000    0.000000    7.925000\n50%      3.000000   29.000000    0.000000    0.000000   14.500000\n75%      3.000000   39.000000    1.000000    0.000000   30.285400\nmax      3.000000   80.000000    8.000000    5.000000  512.329200\n           target\ncount  712.000000\nmean     0.383427\nstd      0.486563\nmin      0.000000\n25%      0.000000\n50%      0.000000\n75%      1.000000\nmax      1.000000\n\n\n\n# object, category 데이터도 추가확인\n\nprint(x_train.describe(include=\"object\"))\nprint(x_test.describe(include=\"object\"))\n\nprint(x_train.describe(include=\"category\"))\nprint(x_test.describe(include=\"category\"))\n\n         sex embarked  who  embark_town alive\ncount    712      710  712          710   712\nunique     2        3    3            3     2\ntop     male        S  man  Southampton    no\nfreq     469      518  432          518   439\n         sex embarked  who  embark_town alive\ncount    179      179  179          179   179\nunique     2        3    3            3     2\ntop     male        S  man  Southampton    no\nfreq     108      126  105          126   110\n        class deck\ncount     712  164\nunique      3    7\ntop     Third    C\nfreq      391   47\n        class deck\ncount     179   39\nunique      3    7\ntop     Third    C\nfreq      100   12\n\n\n\n# y 데이터도 구체적으로 살펴보세요\nprint(y_train.head())\n\n     target\n3         1\n517       0\n861       0\n487       0\n58        1\n\n\n\n# y 데이터도 구체적으로 살펴보세요\nprint(y_train.value_counts())\n\ntarget\n0         439\n1         273\ndtype: int64"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_5.html#데이터-전처리-및-분리-1",
    "href": "BigData_Analysis/실기_2유형_5.html#데이터-전처리-및-분리-1",
    "title": "빅분기 실기 - 2유형 문제풀이(5)",
    "section": "✅ 3. 데이터 전처리 및 분리",
    "text": "✅ 3. 데이터 전처리 및 분리\n\n1) 결측치, 2) 이상치, 3) 변수 처리하기\n\n# 결측치 확인\n\nprint(x_train.isnull().sum())\nprint(x_test.isnull().sum())\nprint(y_train.isnull().sum())\n\npclass           0\nsex              0\nage            133\nsibsp            0\nparch            0\nfare             0\nembarked         2\nclass            0\nwho              0\nadult_male       0\ndeck           548\nembark_town      2\nalive            0\nalone            0\ndtype: int64\npclass           0\nsex              0\nage             44\nsibsp            0\nparch            0\nfare             0\nembarked         0\nclass            0\nwho              0\nadult_male       0\ndeck           140\nembark_town      0\nalive            0\nalone            0\ndtype: int64\ntarget    0\ndtype: int64\n\n\n\n# 결측치 제거\n# df = df.dropna()\n# print(df)\n\n# 참고사항\n# print(df.dropna().shape) # 행 기준으로 삭제\n\n\n# 결측치 대체\n# x_train(712, 14) : age(133), embarked(2), deck(548), embark_town(2)\n# x_test(179, 14) : age(44), deck(140)\n\n# 변수 제거\n# (중복)class\n# (중복) embark_town\n# (중복) alive \n# (결측치 다수) deck\n\n\n# 중복변수 제거\nx_train = x_train.drop(['class', 'embark_town', 'alive', 'deck'], axis=1)\nx_test = x_test.drop(['class', 'embark_town', 'alive', 'deck'], axis=1)\n\n\n# 변수 제거 확인\nprint(x_train.info())\nprint(x_test.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 712 entries, 3 to 608\nData columns (total 10 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   pclass      712 non-null    int64  \n 1   sex         712 non-null    object \n 2   age         579 non-null    float64\n 3   sibsp       712 non-null    int64  \n 4   parch       712 non-null    int64  \n 5   fare        712 non-null    float64\n 6   embarked    710 non-null    object \n 7   who         712 non-null    object \n 8   adult_male  712 non-null    bool   \n 9   alone       712 non-null    bool   \ndtypes: bool(2), float64(2), int64(3), object(3)\nmemory usage: 51.5+ KB\nNone\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 179 entries, 800 to 410\nData columns (total 10 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   pclass      179 non-null    int64  \n 1   sex         179 non-null    object \n 2   age         135 non-null    float64\n 3   sibsp       179 non-null    int64  \n 4   parch       179 non-null    int64  \n 5   fare        179 non-null    float64\n 6   embarked    179 non-null    object \n 7   who         179 non-null    object \n 8   adult_male  179 non-null    bool   \n 9   alone       179 non-null    bool   \ndtypes: bool(2), float64(2), int64(3), object(3)\nmemory usage: 12.9+ KB\nNone\n\n\n\n# 결측치 대체\n# x_train(712, 14) : age(133), embarked(2)\n# x_test(179, 14) : age(44)\n\n# age 변수 \nmed_age = x_train['age'].median()\nx_train['age'] = x_train['age'].fillna(med_age)\nx_test['age'] = x_test['age'].fillna(med_age) # train data의 중앙값으로 \n\n# embarked(object 타입은 대체시 주로 최빈값으로)\nmode_et = x_train['embarked'].mode()\nx_train['embarked'] = x_train['embarked'].fillna(mode_et[0]) # 최빈값 [0] 주의\n\n\n# 결측치 대체 여부 확인\nprint(x_train.isnull().sum())\nprint(x_test.isnull().sum())\n\npclass        0\nsex           0\nage           0\nsibsp         0\nparch         0\nfare          0\nembarked      0\nwho           0\nadult_male    0\nalone         0\ndtype: int64\npclass        0\nsex           0\nage           0\nsibsp         0\nparch         0\nfare          0\nembarked      0\nwho           0\nadult_male    0\nalone         0\ndtype: int64\n\n\n\n# 변수처리(원핫인코딩) - object 변수만 적용됨\nx_train = pd.get_dummies(x_train)\nx_test = pd.get_dummies(x_test)\n\nprint(x_train.info())\nprint(x_test.info())\n\n# advanced 버전 사용\nx_train_ad = x_train.copy()\nx_test_ad = x_test.copy()\ny_train_ad = y_train.copy()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 712 entries, 3 to 608\nData columns (total 15 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   pclass      712 non-null    int64  \n 1   age         712 non-null    float64\n 2   sibsp       712 non-null    int64  \n 3   parch       712 non-null    int64  \n 4   fare        712 non-null    float64\n 5   adult_male  712 non-null    bool   \n 6   alone       712 non-null    bool   \n 7   sex_female  712 non-null    uint8  \n 8   sex_male    712 non-null    uint8  \n 9   embarked_C  712 non-null    uint8  \n 10  embarked_Q  712 non-null    uint8  \n 11  embarked_S  712 non-null    uint8  \n 12  who_child   712 non-null    uint8  \n 13  who_man     712 non-null    uint8  \n 14  who_woman   712 non-null    uint8  \ndtypes: bool(2), float64(2), int64(3), uint8(8)\nmemory usage: 40.3 KB\nNone\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 179 entries, 800 to 410\nData columns (total 15 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   pclass      179 non-null    int64  \n 1   age         179 non-null    float64\n 2   sibsp       179 non-null    int64  \n 3   parch       179 non-null    int64  \n 4   fare        179 non-null    float64\n 5   adult_male  179 non-null    bool   \n 6   alone       179 non-null    bool   \n 7   sex_female  179 non-null    uint8  \n 8   sex_male    179 non-null    uint8  \n 9   embarked_C  179 non-null    uint8  \n 10  embarked_Q  179 non-null    uint8  \n 11  embarked_S  179 non-null    uint8  \n 12  who_child   179 non-null    uint8  \n 13  who_man     179 non-null    uint8  \n 14  who_woman   179 non-null    uint8  \ndtypes: bool(2), float64(2), int64(3), uint8(8)\nmemory usage: 10.1 KB\nNone\n\n\n\n# (참고사항)원핫인코딩 후 변수의 수가 다른 경우\n# =&gt; x_test의 변수의 수가 x_train 보다 많은 경우 (혹은 그 반대인 경우)\n# 원핫인코딩 후 Feature 수가 다를 경우\n# x_train = pd.get_dummies(x_train)\n# x_test = pd.get_dummies(x_test)\n# x_train.info()\n# x_test.info()\n# 해결방법(x_test의 변수가 수가 더 많은 경우의 코드)\n# x_train = x_train.reindex(columns = x_test.columns, fill_value=0)\n# x_train.info()\n\n\n\n데이터 분리\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_train,\n                                                 y_train['target'],\n                                                 test_size = 0.2,\n                                                 stratify = y_train['target'],\n                                                 random_state = 2023)\nprint(x_train.shape)\nprint(x_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)\n\n(569, 15)\n(143, 15)\n(569,)\n(143,)"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_5.html#모델링-및-성능평가-1",
    "href": "BigData_Analysis/실기_2유형_5.html#모델링-및-성능평가-1",
    "title": "빅분기 실기 - 2유형 문제풀이(5)",
    "section": "✅ 4. 모델링 및 성능평가",
    "text": "✅ 4. 모델링 및 성능평가\n\n# 랜덤포레스트 모델 사용 (참고: 회귀모델은 RandomForestRegressor)\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(random_state = 2023)\nmodel.fit(x_train, y_train)\n\nRandomForestClassifier(random_state=2023)\n\n\n\n# 모델을 사용하여 테스트 데이터 예측\ny_pred = model.predict(x_val)\n\n\n# 모델 성능 평가 (accuracy, f1 score, AUC 등)\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score\nacc = accuracy_score(y_val, y_pred)  # (실제값, 예측값)\nf1 = f1_score(y_val, y_pred)         # (실제값, 예측값)\nauc = roc_auc_score(y_val, y_pred)   # (실제값, 예측값)\n\n\nprint(acc) # 정확도(Accuracy)\nprint(f1) # f1 score\nprint(auc) # AUC\n\n0.8531468531468531\n0.8108108108108109\n0.8465909090909092\n\n\n\n# 참고사항\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_val, y_pred)\nprint(cm)\n\n# ####    예측\n# ####   0  1\n# 실제 0 TN FP\n# 실제 1 FN TP\n\n[[77 11]\n [10 45]]\n\n\n\n실제 test 셋으로 성능평가를 한다면?\n\n# 모델을 사용하여 테스트 데이터 예측\ny_pred_f = model.predict(x_test)\n# 모델 성능 평가 (정확도, F1 score, AUC)\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\nacc_f = accuracy_score(y_test, y_pred_f) # (실제값, 예측값)\nf1_f = f1_score(y_test, y_pred_f) # (실제값, 예측값)\nauc_f = roc_auc_score(y_test, y_pred_f) # (실제값,예측값)\n\n\nprint(acc_f) # 정확도(Accuracy)\nprint(f1_f) #f1 score\nprint(auc_f) #AUC\n\n0.7821229050279329\n0.7153284671532847\n0.7687088274044797"
  },
  {
    "objectID": "BigData_Analysis/실기_2유형_5.html#예측값-제출-1",
    "href": "BigData_Analysis/실기_2유형_5.html#예측값-제출-1",
    "title": "빅분기 실기 - 2유형 문제풀이(5)",
    "section": "✅ 5. 예측값 제출",
    "text": "✅ 5. 예측값 제출\n\n(주의) x_test 를 모델에 넣어 나온 예측값을 제출해야 함\n\n#(실기시험 안내사항)\n# 아래 코드 예측변수와 수험번호를 개인별로 변경하여 활용\n# pd.DataFrame({'cust_id':cust_id, 'target':y_result}).to_csv('0030000.csv', index=False)\n\n# 모델을 사용하여 테스트 데이터 예측\n\n# 1. 특정 클래스로 분류할 경우 (predict)\ny_result = model.predict(x_test)\nprint(y_result[:5])\n\n# 2. 특정 클래스로 분류될 확률을 구할 경우 (predict_proba)\ny_result_prob = model.predict_proba(x_test)\nprint(y_result_prob[:5])\n\n# 이해해보기\nresult_prob = pd.DataFrame({\n    'result' : y_result,\n    'prob_0' : y_result_prob[:,0]\n})\n# setosa 일 확률 : y_result_prob[:, 0]\n# 그 외 종일 확률 : y_result_prob[:, 1]\n\nprint(result_prob[:5])\n\n[1 1 0 0 0]\n[[0.32 0.68]\n [0.24 0.76]\n [1.   0.  ]\n [0.93 0.07]\n [0.93 0.07]]\n   result  prob_0\n0       1    0.32\n1       1    0.24\n2       0    1.00\n3       0    0.93\n4       0    0.93\n\n\n\n# ★tip : 데이터를 저장한 다음 불러와서 제대로 제출했는지 확인해보자\n# pd.DataFrame({'result':y_result}).to_csv('수험번호.csv', index=False)\n# df2 = pd.read_csv(\"수험번호.csv\")\n# print(df2.head())"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_2.html",
    "href": "BigData_Analysis/실기_3유형_2.html",
    "title": "빅분기 실기 - 3유형 문제풀이(2)",
    "section": "",
    "text": "실기 3 유형(2)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_2.html#검정방법",
    "href": "BigData_Analysis/실기_3유형_2.html#검정방법",
    "title": "빅분기 실기 - 3유형 문제풀이(2)",
    "section": "✅ 검정방법",
    "text": "✅ 검정방법"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_2.html#대응표본쌍체-동일한-객체의-전-vs-후-평균비교",
    "href": "BigData_Analysis/실기_3유형_2.html#대응표본쌍체-동일한-객체의-전-vs-후-평균비교",
    "title": "빅분기 실기 - 3유형 문제풀이(2)",
    "section": "1. 대응표본(쌍체) : 동일한 객체의 전 vs 후 평균비교",
    "text": "1. 대응표본(쌍체) : 동일한 객체의 전 vs 후 평균비교\n\n(정규성o) 대응표본(쌍체) t검정(paired t-test) : 동일한 객체의 전 vs 후 평균비교\n(정규성x) 윌콕슨 부호순위 검정(wilcoxon)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_2.html#독립표본-a-집단의-평균-vs-b-집단의-평균",
    "href": "BigData_Analysis/실기_3유형_2.html#독립표본-a-집단의-평균-vs-b-집단의-평균",
    "title": "빅분기 실기 - 3유형 문제풀이(2)",
    "section": "2. 독립표본 : A 집단의 평균 vs B 집단의 평균",
    "text": "2. 독립표본 : A 집단의 평균 vs B 집단의 평균\n\n(정규성o) 독립표본 t검정(2sample t-test)\n(정규성x) 윌콕슨 순위합 검정(ranksums)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_2.html#가설검정-순서중요",
    "href": "BigData_Analysis/실기_3유형_2.html#가설검정-순서중요",
    "title": "빅분기 실기 - 3유형 문제풀이(2)",
    "section": "✅ 가설검정 순서(중요!!)",
    "text": "✅ 가설검정 순서(중요!!)\n\n1. 대응표본(쌍체) t검정(paired t-test)\n\n\n가설검정\n유의수준 확인\n정규성 검정 (주의) 차이값에 대한 정규성\n검정실시(통계량, p-value 확인)\n귀무가설 기각여부 결정(채택/기각)\n\n\n\n\n2. 독립표본 t검정(2sample t-test)\n\n\n가설검정\n유의수준 확인\n정규성 검정 (주의) 두 집단 모두 정규성을 따를 경우\n등분산 검정\n검정실시(통계량, p-value 확인)\n귀무가설 기각여부 결정(채택/기각)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_2.html#예제문제",
    "href": "BigData_Analysis/실기_3유형_2.html#예제문제",
    "title": "빅분기 실기 - 3유형 문제풀이(2)",
    "section": "✅ 예제문제",
    "text": "✅ 예제문제\n\nCase 1) 대응표본(쌍체) t 검정(paired t-test)\n문제 1-1 다음은 혈압약을 먹기 전, 후의 혈압 데이터이다.\n혈압약을 먹기 전, 후의 차이가 있는지 쌍체 t검정을 실시하시오.\n(유의수준 5%)\n\nbefore : 혈압약을 먹기전 혈압, after : 혈압약을 먹은 후 혈압\nH_0(귀무가설) : after - before = 0\nH_1(대립가설) : after - before != 0\n\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom scipy.stats import shapiro\n\n\n# 데이터 만들기\ndf = pd.DataFrame( {\n    'before': [120, 135, 122, 124, 135, 122, 145, 160, 155, 142, 144, 135, 167],\n    'after' : [110, 132, 123, 119, 123, 115, 140, 162, 142, 138, 135, 142, 160] })\n\n\n# 1. 가설검정\n# H0 : 약을 먹기전과 먹은 후의 혈압 평균은 같다(효과가 없다)\n# H1 : 약을 먹기전과 먹은 후의 혈압 평균은 같지 않다(효과가 있다)\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3. 정규성 검정 (차이값에 대해 정규성 확인)\nstatistic, pvalue = stats.shapiro(df['after'] - df['before'])\nprint(round(statistic,4), round(pvalue,4))\n\n0.9589 0.7363\n\n\n\np-value 값(0.7363)이 유의수준 (0.05)보다 크다.\n귀무가설 (H0) 채택(정규성검정의 H0 : 정규분포를 따른다)\n\n\n# 4.1 (정규성o) 대응표본(쌍체) t검정(paired t-test)\nstatistic, pvalue = stats.ttest_rel(df['after'], df['before']\n                                    , alternative='two-sided')\nprint(round(statistic,4), round(pvalue,4))\n\n-3.1382 0.0086\n\n\n\n# 4.2 (정규성x) wilcoxon 부호순위 검정\nstatistic, pvalue = stats.wilcoxon(df['after']-df['before'],\n                                  alternative='two-sided')\nprint(round(statistic,4), round(pvalue,4))\n\n11.0 0.0134\n\n\n\n# 5. 귀무가설 기각 여부 결정(채택/기각)\n# p-value 값(0.0086)이 0.05보다 작기 떄문에 귀무가설을 기각한다\n# 즉, 약을 먹기 전과 먹은 후의 혈압은 같지 않다(효과가 있다)\n\n# 답 : 기각(H1)\n\n차이가 있는지 없는지 확인은 양측검정 // 증가, 감소를 확인하는 것은 단측검정\n문제 1-2\n다음은 혈압약을 먹기 전, 후의 혈압 데이터이다.\n혈압약을 먹은 후 혈압이 감소했는지 확인하기 위해 쌍체 t 검정을 실시하시오\n(유의수준 5%)\n\nbefore : 혈압약을 먹기전 혈압, after : 혈압약을 먹은 후의 혈압\nH0(귀무가설) : after - before &gt;=0\nH1(대립가설) : after - before &lt; 0\n\n\n# 데이터 만들기\ndf = pd.DataFrame( {\n    'before': [120, 135, 122, 124, 135, 122, 145, 160, 155, 142, 144, 135, 167],\n    'after' : [110, 132, 123, 119, 123, 115, 140, 162, 142, 138, 135, 142, 160] })\nprint(df.head(3))\n\n   before  after\n0     120    110\n1     135    132\n2     122    123\n\n\n\n# 1. 가설검정\n# H0 : 약을 먹은 후 혈압이 같거나 증가했다. (after - before &gt;= 0)\n# H1 : 약을 먹은 후 혈압이 감소했다.        (after - before &lt;  0) \n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\nstatistic, pvalue = stats.shapiro(df['after']-df['before'])\nprint(round(statistic, 4), round(pvalue ,4))\n\n0.9589 0.7363\n\n\np-value가 0.05보다 크다 귀무가설 채택\n\n# 4.1 (정규성o) 대응표본(쌍체) t검정(paired t-test)\nstatistic, pvalue = stats.ttest_rel(df['after'], df['before']\n                                    , alternative='less')\nprint(round(statistic,4), round(pvalue,4))\n\n-3.1382 0.0043\n\n\n\n# 4.2 (정규성x) wilcoxon 부호순위 검정\nstatistic, pvalue = stats.wilcoxon(df['after']-df['before'],\n                                  alternative='less')\nprint(round(statistic,4), round(pvalue,4))\n\n11.0 0.0067\n\n\n\n# 5. 귀무가설 기각 여부 결정(채택/기각)\n# p-value 값(0.0043)이 0.05보다 작기 떄문에 귀무가설을 기각한다\n# 즉, 약을 먹은 후 혈압이 감소했다고 할 수 있다.(효과가 있다)\n\n# 답 : 기각(H1)\n\n\n\nCase 2) 독립표본 t검정(2sample t-test)\n문제 2-1\n다음은 A그룹과 B그룹 인원의 혈압 데이터이다.\n두 그룹의 혈압평균이 다르다고 할 수 있는지 독립표본 t검정을 실시하시오.\n(유의수준 5%)\n\nA : A그룹 인원의 혈압, B : B그룹 인원의 혈압\nH0(귀무가설) : A = B\nH1(대립가설) : A != B\n\n\n# 데이터 만들기\ndf = pd.DataFrame( {\n    'A': [120, 135, 122, 124, 135, 122, 145, 160, 155, 142, 144, 135, 167],\n    'B' : [110, 132, 123, 119, 123, 115, 140, 162, 142, 138, 135, 142, 160] })\nprint(df.head(3))\n\n     A    B\n0  120  110\n1  135  132\n2  122  123\n\n\n\n# 1. 가설검정\n# H0(귀무가설) : A그룹과 B그룹의 혈압 평균은 같다.      (A = B)\n# H1(대립가설) : A그룹과 B그룹의 혈압 평균은 같지 않다. (A != B)\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3. 정규성 검정\n# H0(귀무가설) : 정규분포를 따른다.\n# H1(대립가설) : 정규분포를 따르지 않는다.\n\nstatisticA, pvalueA = stats.shapiro(df['A'])\nstatisticB, pvalueB = stats.shapiro(df['B'])\nprint(round(statisticA, 4), round(pvalueA, 4))\nprint(round(statisticB, 4), round(pvalueB, 4))\n\n0.9314 0.3559\n0.9498 0.5956\n\n\n\np-value 값(0.3559, 0.5956)이 유의수준(0.05)보다 크다.\n귀무가설(H0) 채택\n만약 하나라도 정규분포를 따르지 않는다면 비모수 검정방법을 써야 함\n(윌콕슨의 순위합 검정 ranksums)\n\n\n# 4. 등분산성 검정\n# H0(귀무가설) : 등분산 한다.\n# H1(대립가설) : 등분산 하지 않는다.\nstatistic, pvalue = stats.bartlett(df['A'], df['B'])\nprint(round(statistic, 4), round(pvalue, 4))\n\n0.0279 0.8673\n\n\n\np-value 값이 유의수준(0.05) 보다 크다.\n귀무가설(H0) 채택 =&gt; 등분산성을 따른다고 할 수 있다.\n\n\n# 5.1 (정규성o, 등분산성 o/x) t검정\nstatistic, pvalue = stats.ttest_ind(df['A'], df['B'],\n                                   equal_var = True,\n                                   alternative='two-sided')\n# 만약 등분산 하지 않으면 False로 설정\nprint(round(statistic, 4), round(pvalue, 4))\n\n0.8192 0.4207\n\n\n\n# 5.2 (정규성x)윌콕슨의 순위합 검정\nstatistic, pvalue = stats.ranksums(df['A'], df['B'],\n                                   alternative='two-sided')\nprint(round(statistic, 4), round(pvalue, 4))\n\n0.8462 0.3975\n\n\n\n# 6. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값이 0.05보다 크기 떄문에 귀무가설을 채택한다\n# 즉, A그룹과 B그룹의 혈압 평균은 같다고 할 수 있다.\n\n# 답 : 채택(H0)\n\n\n# (참고) 평균데이터 확인\nprint(round(df['A'].mean(), 4))\nprint(round(df['B'].mean(), 4))\n\n138.9231\n133.9231\n\n\n문제 2-2\n다음은 A그룹과 B그룹 인원의 혈압 데이터이다.\nA그룹의 혈압 평균이 B그룹보다 크다고 할 수 있는지 독립표본 t검정을 실시하시오.\n(유의수준 5%)\n\nA : A그룹 인원의 혈압, B : B그룹 인원의 혈압\nH0(귀무가설) : A - B &lt;= 0 (or A &lt;= B)\nH1(대립가설) : A - B &gt; 0 (or A &gt; B)\n\n\n# 데이터 만들기\ndf = pd.DataFrame( {\n    'A': [120, 135, 122, 124, 135, 122, 145, 160, 155, 142, 144, 135, 167],\n    'B' : [110, 132, 123, 119, 123, 115, 140, 162, 142, 138, 135, 142, 160] })\nprint(df.head(3))\n\n     A    B\n0  120  110\n1  135  132\n2  122  123\n\n\n\n# 1. 가설검정\n# H0(귀무가설) : A그룹의 혈압 평균이 B그룹보다 작거나 같다. (A &lt;= B)\n# H1(대립가설) : A그룹의 혈압 평균이 B그룹보다 크다.        (A &gt;  B)\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3. 정규성 검정(차이값에 대해 정규성 확인)\n# H0(귀무가설) : 정규분포를 따른다.\n# H1(대립가설) : 정규분포를 따르지 않는다.\n\nstatisticA, pvalueA = stats.shapiro(df['A'])\nstatisticB, pvalueB = stats.shapiro(df['B'])\nprint(round(statisticA, 4), round(pvalueA, 4))\nprint(round(statisticB, 4), round(pvalueB, 4))\n\n0.9314 0.3559\n0.9498 0.5956\n\n\n\np-value 값(0.3559, 0.5956)이 유의수준(0.05)보다 크다.\n귀무가설(H0) 채택\n만약 하나라도 정규분포를 따르지 않는다면 비모수 검정방법을 써야 함\n(윌콕슨의 순위합 검정 ranksums)\n\n\n# 4. 등분산성 검정\n# H0(귀무가설) : 등분산 한다.\n# H1(대립가설) : 등분산 하지 않는다.\nstatistic, pvalue = stats.bartlett(df['A'], df['B'])\nprint(round(statistic, 4), round(pvalue, 4))\n\n0.0279 0.8673\n\n\n\n# 5.1 (정규성o, 등분산성 o/x) t검정\nstatistic, pvalue = stats.ttest_ind(df['A'], df['B'],\n                                   equal_var = True,\n                                   alternative='greater')\n# 만약 등분산 하지 않으면 False로 설정\nprint(round(statistic, 4), round(pvalue, 4))\n\n0.8192 0.2104\n\n\n\n# 5.2 (정규성x)윌콕슨의 순위합 검정\nstatistic, pvalue = stats.ranksums(df['A'], df['B'],\n                                   alternative='greater')\n# A그룹의 평균값이 B그룹의 평균값보다 클 수 있는가를 대립가설(H1)으로 설정했기에 greater\nprint(round(statistic, 4), round(pvalue, 4))\n\n0.8462 0.1987\n\n\n\n# 6. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값이 0.05보다 크기 떄문에 귀무가설을 채택한다\n# 즉, A그룹의 혈압 평균이 B그룹보다 작거나 같다고 할 수 있다.\n# (A그룹의 혈압 평균이 B그룹보다 크다고 할 수 없다)\n\n# 답 : 채택(H0)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_4.html",
    "href": "BigData_Analysis/실기_3유형_4.html",
    "title": "빅분기 실기 - 3유형 문제풀이(4)",
    "section": "",
    "text": "실기 3 유형(4)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_4.html#분석-case",
    "href": "BigData_Analysis/실기_3유형_4.html#분석-case",
    "title": "빅분기 실기 - 3유형 문제풀이(4)",
    "section": "✅ 분석 Case",
    "text": "✅ 분석 Case\n\nCase 1. 적합도 검정 - 각 범주에 속할 확률이 같은지?\n\n\nCase 2. 독립성 검정 - 두개의 범주형 변수가 서로 독립인지?"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_4.html#가설검정-순서중요",
    "href": "BigData_Analysis/실기_3유형_4.html#가설검정-순서중요",
    "title": "빅분기 실기 - 3유형 문제풀이(4)",
    "section": "✅ 가설검정 순서(중요!!)",
    "text": "✅ 가설검정 순서(중요!!)\n\n\n가설검정\n유의수준 확인\n검정실시(통계량, p-value 확인)\n귀무가설 기각여부 결정(채택/기각)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_4.html#예제문제",
    "href": "BigData_Analysis/실기_3유형_4.html#예제문제",
    "title": "빅분기 실기 - 3유형 문제풀이(4)",
    "section": "✅ 예제문제",
    "text": "✅ 예제문제\n\nCase 1. 적합도 검정 - 각 범주에 속할 확률이 같은지?\n\n\n문제 1-1\n\n\n랜덤 박스에 상품 A, B, C, D가 들어있다.\n\n\n다음은 랜덤박스에서 100번 상품을 꺼냈을 떄의 상품 데이터라고 할 때\n\n\n상품이 동일한 비율로 들어있다고 할 수 있는지 검정해보시오.\n\nimport pandas as pd\nimport numpy as np\n\n\n# 데이터 생성\nrow1 = [30,20,15,35]\ndf = pd.DataFrame([row1], columns=['A','B','C','D'])\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n30\n20\n15\n35\n\n\n\n\n\n\n\n\n# 1. 가설검정\n# H0 : 랜덤박스에 상품 A,B,C,D가 동일한 비율로 들어있다.\n# H1 : 랜덤박스에 상품 A,B,C,D가 동일한 비율로 들어있지 않다.\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3. 검정실시(통계량, p-value)\nfrom scipy.stats import chisquare\n# chisquare(f_obs=f_obs, f_exp=f_exp)   # 관측값, 기대값\n\n# 관측값과 기대값 구하기\nf_obs = [30,20,15,35]\n# f_obs = df.iloc[0]\nf_exp = [25,25,25,25]\n\nstatistic, pvalue = chisquare(f_obs=f_obs, f_exp=f_exp)\nprint(statistic)\nprint(pvalue)\n\n10.0\n0.01856613546304325\n\n\n\n# 4. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값이 0.05보다 작기 떄문에 귀무가설을 기각한다.\n# 즉, 랜덤박스에 상품 A,B,C,D가 동일한 비율로 들어있지 않다고 할 수 있다.\n\n# 답 : 기각\n\n\n\n문제 1-1\n\n\n랜덤 박스에 상품 A, B, C가 들어있다.\n\n\n다음은 랜덤박스에서 150번 상품을 꺼냈을 떄의 상품 데이터라고 할 때\n\n\n상품별로 A 30%, B 15%, C 55% 비율로 들어있다고 할 수 있는지 검정해보시오.\n\nimport pandas as pd\nimport numpy as np\n\n\n# 데이터 생성\nrow1 = [50,25,75]\ndf = pd.DataFrame([row1], columns=['A','B','C'])\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n50\n25\n75\n\n\n\n\n\n\n\n\n# 1. 가설검정\n# H0 : 랜덤박스에 상품 A,B,C가 30%, 15%, 55%의 비율로 들어있다.\n# H1 : 랜덤박스에 상품 A,B,C가 30%, 15%, 55%의비율로 들어있지 않다.\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3. 검정실시(통계량, p-value)\nfrom scipy.stats import chisquare\n# chisquare(f_obs=f_obs, f_exp=f_exp)   # 관측값, 기대값\n\n# 관측값과 기대값 구하기\nf_obs = [50,25,75]\n# f_obs = df.iloc[0]\n\na = 150*0.3\nb = 150*0.15\nc = 150*0.55\nf_exp = [a,b,c]\n\nstatistic, pvalue = chisquare(f_obs=f_obs, f_exp=f_exp)\nprint(statistic)\nprint(pvalue)\n\n1.5151515151515151\n0.46880153914023537\n\n\n\n# 4. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값이 0.05보다 크기 떄문에 귀무가설을 채택한다.\n# 즉, 랜덤박스에 상품 A,B,C가 30%, 15%, 55%의 비율로 들어있다고 할 수 있다.\n\n# 답 : 채택\n\n\n\nCase 2. 독립성 검정 - 두개의 범주형 변수가 서로 독립인지?\n\n\n문제 2-1\n\n\n연령대에 따라 먹는 아이스크림의 차이가 있는지 독립성 검정을 실시하시오.\n\nimport pandas as pd\nimport numpy as np\n\n\n# 데이터 생성\nrow1, row2 = [200,190,250],[220,250,300]\ndf = pd.DataFrame([row1, row2], columns=['딸기','초코','바닐라']\n                  ,index=['10대', '20대'])\ndf\n\n\n\n\n\n\n\n\n딸기\n초코\n바닐라\n\n\n\n\n10대\n200\n190\n250\n\n\n20대\n220\n250\n300\n\n\n\n\n\n\n\n\n# 1. 가설설정\n# H0 : 연령대와 먹는 아이스크림의 종류는 서로 관련이 없다(두 변수는 서로 독립이다.)\n# H1 : 연령대와 먹는 아이스크림의 종류는 서로 관련이 있다(두 변수는 서로 독립이 아니다.)\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3.검정실시(통계량, p-value, 기대값 확인)\nfrom scipy.stats import chi2_contingency\n\nstatistic, pvalue, dof, expected = chi2_contingency(df)\n# 공식문서상에 : statistic(통계량), pvalue, dof(자유도), expected_freq(기대값)\n\n# 아래와 같이 입력해도 동일한 결과\n# statistic, pvalue, dof, expected = chi2_contingency([row1, row2])\n# statistic, pvalue, dof, expected = chi2_contingency([df.iloc[0], df.iloc[1]])\n\nprint(statistic)\nprint(pvalue)\nprint(dof) # 자유도 = (행-1) * (열*1)\nprint(np.round(expected, 2)) # 반올림하고 싶다면 np.round()\n\n1.708360126075226\n0.4256320394874311\n2\n[[190.64 199.72 249.65]\n [229.36 240.28 300.35]]\n\n\n\n# 4. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값이 0.05보다 크기 떄문에 귀무가설을 채택한다.\n# 즉, 연령대와 먹는 아이스크림의 종류는 서로 관련이 없다고 할 수 있다.\n\n# 답 : 채택\n\n\n\n(추가) 만약 데이터 형태가 다를경우?\n\n# (Case1) 만약 데이터가 아래와 같이 주어진다면?\ndf = pd.DataFrame({\n    '아이스크림' : ['딸기', '초코', '바닐라', '딸기', '초코', '바닐라'],\n    '연령' : ['10대','10대','10대','20대','20대','20대'],\n    '인원' : [200,190,250,220,250,300]\n})\ndf\n\n\n\n\n\n\n\n\n아이스크림\n연령\n인원\n\n\n\n\n0\n딸기\n10대\n200\n\n\n1\n초코\n10대\n190\n\n\n2\n바닐라\n10대\n250\n\n\n3\n딸기\n20대\n220\n\n\n4\n초코\n20대\n250\n\n\n5\n바닐라\n20대\n300\n\n\n\n\n\n\n\n\n# pd.crosstab(index= , columns= , values= , aggfunc=sum)\ntable = pd.crosstab(index=df['연령'] , columns=df['아이스크림'] ,\n                    values= df['인원'], aggfunc=sum)\ntable\n# 주의 : index, columns에 순서를 꼭 확인하기\n\n\n\n\n\n\n\n아이스크림\n딸기\n바닐라\n초코\n\n\n연령\n\n\n\n\n\n\n\n10대\n200\n250\n190\n\n\n20대\n220\n300\n250\n\n\n\n\n\n\n\n\n# 3.검정실시(통계량, p-value, 기대값 확인)\nfrom scipy.stats import chi2_contingency\n\nstatistic, pvalue, dof, expected = chi2_contingency(table)\n# 공식문서상에 : statistic(통계량), pvalue, dof(자유도), expected_freq(기대값)\n\nprint(statistic)\nprint(pvalue)\nprint(dof) # 자유도 = (행-1) * (열*1)\nprint(np.round(expected, 2)) # 반올림하고 싶다면 np.round()\n\n1.708360126075226\n0.4256320394874311\n2\n[[190.64 249.65 199.72]\n [229.36 300.35 240.28]]\n\n\n\n# (Case2) 만약 데이터가 아래와 같이 주어진다면?\n# (이해를 위한 참고용 입니다. 빈도수 카운팅)\ndf = pd.DataFrame({\n    '아이스크림' : ['딸기', '초코', '바닐라', '딸기', '초코', '바닐라'],\n    '연령' : ['10대','10대','10대','20대','20대','20대']\n})\ndf\n\n\n\n\n\n\n\n\n아이스크림\n연령\n\n\n\n\n0\n딸기\n10대\n\n\n1\n초코\n10대\n\n\n2\n바닐라\n10대\n\n\n3\n딸기\n20대\n\n\n4\n초코\n20대\n\n\n5\n바닐라\n20대\n\n\n\n\n\n\n\n\n# pd.crosstab(index, columns)\npd.crosstab(df['연령'],df['아이스크림'])\n\n\n\n\n\n\n\n아이스크림\n딸기\n바닐라\n초코\n\n\n연령\n\n\n\n\n\n\n\n10대\n1\n1\n1\n\n\n20대\n1\n1\n1\n\n\n\n\n\n\n\n\n\n문제2-2\n\n\n타이타닉에 데이터에서 성별(sex)과 생존여부(survied) 변수간\n\n\n독립성 검정을 실시하시오\n\nimport seaborn as sns\ndf = sns.load_dataset('titanic')\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 15 columns):\n #   Column       Non-Null Count  Dtype   \n---  ------       --------------  -----   \n 0   survived     891 non-null    int64   \n 1   pclass       891 non-null    int64   \n 2   sex          891 non-null    object  \n 3   age          714 non-null    float64 \n 4   sibsp        891 non-null    int64   \n 5   parch        891 non-null    int64   \n 6   fare         891 non-null    float64 \n 7   embarked     889 non-null    object  \n 8   class        891 non-null    category\n 9   who          891 non-null    object  \n 10  adult_male   891 non-null    bool    \n 11  deck         203 non-null    category\n 12  embark_town  889 non-null    object  \n 13  alive        891 non-null    object  \n 14  alone        891 non-null    bool    \ndtypes: bool(2), category(2), float64(2), int64(4), object(5)\nmemory usage: 80.7+ KB\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n\n\n\n\n\n\n# pd.crosstab(index, columns)\ntable = pd.crosstab(df['sex'], df['survived'])\nprint(table)\n\nsurvived    0    1\nsex               \nfemale     81  233\nmale      468  109\n\n\n\n# 1. 가설설정\n# H0 : 성별과 생존 여부는 서로 관련이 없다(두 변수는 서로 독립이다)\n# H1 : 성별과 생존 여부는 서로 관련이 있다(두 변수는 서로 독립이 아니다)\n\n\n# 2. 유의수준 확인 : 유의수준 5%로 확인\n\n\n# 3.검정실시(통계량, p-value, 기대값 확인)\nfrom scipy.stats import chi2_contingency\n\nstatistic, pvalue, dof, expected = chi2_contingency(table)\n# 공식문서상에 : statistic(통계량), pvalue, dof(자유도), expected_freq(기대값)\n\nprint(statistic)\nprint(pvalue)\nprint(dof) # 자유도 = (행-1) * (열*1)\nprint(np.round(expected, 2)) # 반올림하고 싶다면 np.round()\n\n260.71702016732104\n1.1973570627755645e-58\n1\n[[193.47 120.53]\n [355.53 221.47]]\n\n\n\n# 4. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값(1.1973570627755645e-58)이 0.05보다 작기 떄문에 귀무가설을 기각한다.\n# 즉, 성별과 생존 여부는 서로 관련이 있다고 할 수 있다.\n\n# 답 : 기각\n\n\n\n데이터를 변경해보면서 이해해봅시다.\n\n# 임의 데이터 생성\nsex, survived = [160,160], [250,220]\ntable = pd.DataFrame([sex, survived], columns=['0','1'], index=['female','male'])\nprint(table)\n\n          0    1\nfemale  160  160\nmale    250  220\n\n\n\n# 3.검정실시(통계량, p-value, 기대값 확인)\nfrom scipy.stats import chi2_contingency\n\nstatistic, pvalue, dof, expected = chi2_contingency(table)\n# 공식문서상에 : statistic(통계량), pvalue, dof(자유도), expected_freq(기대값)\n\nprint(statistic)\nprint(pvalue)\nprint(dof) # 자유도 = (행-1) * (열*1)\nprint(np.round(expected, 2)) # 반올림하고 싶다면 np.round()\n\n0.6541895872879862\n0.41861876333789727\n1\n[[166.08 153.92]\n [243.92 226.08]]\n\n\n\n# 4. 귀무가설 기각여부 결정(채택/기각)\n# p-value 값이 0.05보다 크기 떄문에 귀무가설을 채택한다.\n# 즉, 성별과 생존 여부는 서로 관련이 없다고 할 수 있다.\n\n# 답 : 채택"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_6.html",
    "href": "BigData_Analysis/실기_3유형_6.html",
    "title": "빅분기 실기 - 3유형 문제풀이(6)",
    "section": "",
    "text": "실기 3 유형(6)"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_6.html#타이타닉-데이터-불러오기생존자-예측-데이터",
    "href": "BigData_Analysis/실기_3유형_6.html#타이타닉-데이터-불러오기생존자-예측-데이터",
    "title": "빅분기 실기 - 3유형 문제풀이(6)",
    "section": "타이타닉 데이터 불러오기(생존자 예측 데이터)",
    "text": "타이타닉 데이터 불러오기(생존자 예측 데이터)\n\n# 데이터 불러오기\nimport pandas as pd\nimport numpy as np\n\n# Seaborn의 내장 타이타닉 데이터셋을 불러옵니다.\nimport seaborn as sns\ndf = sns.load_dataset('titanic')\n\n\nprint(df.head())\n\n   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n0         0       3    male  22.0      1      0   7.2500        S  Third   \n1         1       1  female  38.0      1      0  71.2833        C  First   \n2         1       3  female  26.0      0      0   7.9250        S  Third   \n3         1       1  female  35.0      1      0  53.1000        S  First   \n4         0       3    male  35.0      0      0   8.0500        S  Third   \n\n     who  adult_male deck  embark_town alive  alone  \n0    man        True  NaN  Southampton    no  False  \n1  woman       False    C    Cherbourg   yes  False  \n2  woman       False  NaN  Southampton   yes   True  \n3  woman       False    C  Southampton   yes  False  \n4    man        True  NaN  Southampton    no   True  \n\n\n\n# 분석 데이터 설정\ndf = df[['survived', 'sex', 'sibsp', 'fare']] \n# sex:성별, sibsp:탑승한 부모 및 자녀수 fare:요금\n\nprint(df.head())\n\n   survived     sex  sibsp     fare\n0         0    male      1   7.2500\n1         1  female      1  71.2833\n2         1  female      0   7.9250\n3         1  female      1  53.1000\n4         0    male      0   8.0500\n\n\n\n로지스틱 회귀식 : P(1일 확률) = 1 / (1+exp(-f(x))\n\nf(x) = b0 + b1x1 + b2x2 + b3x3\nln(P/1-P) = b0 + b1x1 + b2x2 + b3x3\n(P = 생존할 확률, x1=sex, x2=sibsp, x3=fare)\n\n\n# 데이터 전처리\n# 변수처리\n# 문자형 타입의 데이터의 경우 숫자로 변경해준다.\n# *** 실제 시험에서 지시사항을 따를 것 ***\n\n# 성별을 map 함수를 활용해서 각각 1과 0에 할당한다. (여성을 1, 남성을 0)\n# (실제 시험의 지시 조건에 따를 것)\ndf['sex'] = df['sex'].map({'female':1,\n                          'male':0})\nprint(df.head())\n\n   survived  sex  sibsp     fare\n0         0    0      1   7.2500\n1         1    1      1  71.2833\n2         1    1      0   7.9250\n3         1    1      1  53.1000\n4         0    0      0   8.0500\n\n\n\nprint(df.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 4 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   survived  891 non-null    int64  \n 1   sex       891 non-null    int64  \n 2   sibsp     891 non-null    int64  \n 3   fare      891 non-null    float64\ndtypes: float64(1), int64(3)\nmemory usage: 28.0 KB\nNone"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_6.html#sklearn-라이브러리-활용",
    "href": "BigData_Analysis/실기_3유형_6.html#sklearn-라이브러리-활용",
    "title": "빅분기 실기 - 3유형 문제풀이(6)",
    "section": "1. sklearn 라이브러리 활용",
    "text": "1. sklearn 라이브러리 활용\n\n# 독립변수와 종속변수 설정\nx = df.drop(['survived'], axis=1) # x = df[['sex', 'age', 'fare']]\ny = df['survived']\n\n(주의) LogisticRegrresion() 객체안에 반드시 penalty=None으로 입력해야 함\n\n# 모델링\nfrom sklearn.linear_model import LogisticRegression # 회귀는 LinearRegression\n\n# 반드시 penalty = None으로 입력할 것해야 함.  default='l2'\nmodel1 = LogisticRegression(penalty='none')\nmodel1.fit(x, y)\n\nLogisticRegression(penalty='none')\n\n\n\n# 로지스틱회귀분석 관련 지표 출력\n\n# 1.회귀계수 출력 : model.coef_\nprint(np.round(model1.coef_, 4))        # 전체 회귀계수\nprint(np.round(model1.coef_[0,0], 4))   # x1의 회귀계수\nprint(np.round(model1.coef_[0,1], 4))   # x2의 회귀계수\nprint(np.round(model1.coef_[0,2], 4))   # x3의 회귀계수\n\n# 2. 회귀계수(절편) : model.intercept_\nprint(np.round(model1.intercept_, 4))\n\n[[ 2.5668 -0.4017  0.0138]]\n2.5668\n-0.4017\n0.0138\n[-1.6964]\n\n\n\n로지스틱 회귀식 : P(1일 확률) = 1 / (1+exp(-f(x))\n\nf(x) = b0 + b1x1 + b2x2 + b3x3\nln(P/1-P) = b0 + b1x1 + b2x2 + b3x3\n(P = 생존할 확률, x1=sex, x2=sibsp, x3=fare)\n\n\n\n결과 : ln(P/1-P) = -1.6964 + 2.5668sex - 0.4017sibsp + 0.0138fare\n\n# 3-1. 로지스틱 회귀모형에서 sibsp 변수가 한단위 증가할 때 생존할 오즈가 몇 배 \n#      증가하는지 반올림하여 소수점 셋째 자리까지 구하시오.\n\n# exp(b2) 를 구하면 된다.\nresult = np.exp(model1.coef_[0,1]) #인덱싱 주의하세요.\nprint(round(result,3))\n\n# 해석 : sibsp 변수가 한 단위 증가할 때 생존할 오즈가 0.669배 증가한다.\n#        생존할 오즈가 33% 감소한다.(생존할 확률이 감소한다.)\n\n0.669\n\n\n\n# 3-2. 로지스틱 회귀모형에서 여성일 경우 남성에 비해 오즈가 몇 배 증가하는지\n#      반올림하여 소수점 셋째 자리까지 구하시오\n\n# exp(b2) 를 구하면 된다.\nresult2 = np.exp(model1.coef_[0,0]) #인덱싱 주의하세요.\nprint(round(result2,3))\n\n# 해석 : 여성일 경우 남성에 비해 생존할 오즈가 13.024배 증가한다.\n#        생존할 오즈가 13배 증가한다.(생존할 확률이 증가한다.)\n\n13.024"
  },
  {
    "objectID": "BigData_Analysis/실기_3유형_6.html#statsmodels-라이브러리-사용",
    "href": "BigData_Analysis/실기_3유형_6.html#statsmodels-라이브러리-사용",
    "title": "빅분기 실기 - 3유형 문제풀이(6)",
    "section": "2. statsmodels 라이브러리 사용",
    "text": "2. statsmodels 라이브러리 사용\n(주의) 실제 오즈가 몇 배 증가했는지 계산하는 문제가 나온다면\nsklearn 라이브러리를 사용하여 회귀계수를 직접구해서 계산할 것(소수점이 결과값에 영향을 줄 수 있음)\n\n# 모델링\nimport statsmodels.api as sm\n\nx = sm.add_constant(x)       # 주의 : 상수항 추가해줘야 함\nmodel2 = sm.Logit(y,x).fit() # 주의할 것 : y, x 순으로 입력해야 함\nsummary = model2.summary()\nprint(summary)\n\nOptimization terminated successfully.\n         Current function value: 0.483846\n         Iterations 6\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:               survived   No. Observations:                  891\nModel:                          Logit   Df Residuals:                      887\nMethod:                           MLE   Df Model:                            3\nDate:                Wed, 29 Nov 2023   Pseudo R-squ.:                  0.2734\nTime:                        19:06:26   Log-Likelihood:                -431.11\nconverged:                       True   LL-Null:                       -593.33\nCovariance Type:            nonrobust   LLR p-value:                 5.094e-70\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -1.6964      0.129    -13.134      0.000      -1.950      -1.443\nsex            2.5668      0.179     14.321      0.000       2.216       2.918\nsibsp         -0.4017      0.095     -4.222      0.000      -0.588      -0.215\nfare           0.0138      0.003      5.367      0.000       0.009       0.019\n==============================================================================\n\n\n\n(결과 비교해보기) 두 라이브러리 모두 같은 결과값을 출력\n\n회귀식 : P(1일 확률) = 1 / (1+exp(-f(x))\nf(x) = b0 + b1x1 + b2x2 + b3x3\nln(P/1-P) = b0 + b1x1 + b2x2 + b3x3\n(P=생존할 확률, x1=sex, x2=sibsp, x3=fare)\n\n1. sklearn : ln(P/1-P) = -1.6964 + 2.5668sex - 0.4017sibsp + 0.0138fare\n2. statsmodel : ln(P/1-P) = -1.6964 + 2.5668sex - 0.4017sibsp + 0.0138fare"
  },
  {
    "objectID": "BigData_Analysis.html",
    "href": "BigData_Analysis.html",
    "title": "Big Data Analysis Engineer",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n빅분기 실기 - 3유형 문제풀이(1)\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nsklearn\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n빅분기 실기 - 3유형 문제풀이(2)\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nsklearn\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n빅분기 실기 - 3유형 문제풀이(3)\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nscipy\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n빅분기 실기 - 3유형 문제풀이(4)\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nscipy\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n빅분기 실기 - 3유형 문제풀이(5)\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nscipy\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n빅분기 실기 - 3유형 문제풀이(6)\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nscipy\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n빅분기 실기 - 2유형 문제풀이(5)\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nsklearn\n\n\n\n\n\n\n\n\n\n\n\nOct 23, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n빅분기 실기 - 2유형 문제풀이(1)\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nsklearn\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n빅분기 실기 - 2유형 문제풀이(2)\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nsklearn\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n빅분기 실기 - 2유형 문제풀이(3)\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nsklearn\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n빅분기 실기 - 2유형 문제풀이(4)\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nsklearn\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n빅분기 실기 - 1유형 문제풀이\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n빅분기 실기 - 파이썬 기초\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Data_Mining/2022-03-18-Python Language Basics.html",
    "href": "Data_Mining/2022-03-18-Python Language Basics.html",
    "title": "Python 기초",
    "section": "",
    "text": "basic 기본 코드 실습\n\n\nPython language bascis\n\nlanguage semantics\n\nIdentation, not braces // 들여쓰기\nfor x in array: if x &lt; pivot : less.append(x) else: greater.append(x)\na=5; b=6; c=7\n\na=5; b=6; c=7\n\n\nc\n\n7\n\n\n\n\nEverything is an object // 오브젝트를 기본 할당하고 동작함\n\n\nComments\nresult =[] for line in file_handle: # keep the empty line for now # if len(line) == 0: # continue result.append(line.replace(‘foo’,‘bar’)) print(“Reach this line”) #simple statis report\n\n\nFunction and object method calls\nresult = f(x,y,z) g()\nobj.some_method(x,y,z) result = f(a,b,c,d=5,e=‘foo’)\n\n\nVariables and argument passing\n\na=[1,2,3]\n\n\nb=a\n\n\na.append(4) #뒤에 붙여줌 \nb\n\n[1, 2, 3, 4]\n\n\n\na=5\ntype(a) #int - integer - 수치형 \n\nint\n\n\n\na='foo'\ntype(a)\n\nstr\n\n\n\n'5'+5\n\nTypeError: can only concatenate str (not \"int\") to str\n\n\n\na=4.5\nb=2\n# String formatting, to be visted later\nprint('a is {0}, b is {1}'.format(type(a),type(b)))\na/b\n\na is &lt;class 'float'&gt;, b is &lt;class 'int'&gt;\n\n\n2.25\n\n\n\na=5\nisinstance(a,int)\n\nTrue\n\n\n\na=5; b=4.5\nisinstance(a,(int,float))\nisinstance(b,(int,float))\n\nTrue\n\n\n\na='foo' #Tab 키를 누르면 다양한 옵션이 나오면서 대문자화 카운트등 여러옵션 존재\n\n\na.&lt;Press Tab&gt;\n\nSyntaxError: invalid syntax (3084391244.py, line 1)\n\n\n\na.upper()\n\n'FOO'\n\n\n\ngetattr(a,'split') #object에 속한 속성을 가지고 온다 \n\n&lt;function str.split(sep=None, maxsplit=-1)&gt;\n\n\n\n\n\nDuck typing\n\ndef isiterable(obj):\n    try:\n        iter(obj)\n        return True\n    except TypeError: # not literable\n        return False\n\n\nisiterable('a string') \n\nTrue\n\n\n\nisiterable([1,2,3])\n\nTrue\n\n\n\nisiterable(5)\n\nFalse\n\n\nif not isinstance(x,list) and isiterable(x): x=list(x)\n\n\nImports\nIn Python a module is simply a file with the .py extension containing Python code. Suppose that we had the following module:\nIf you wanted to access the variables and functions defined in some_module.py, from another file in the same directory we could do:\n\nimport some_module #같은 디렉토리의 .py파일을 불러와서 사용가능 \n\nresult = some_module.f(5)\nresult\n\n7\n\n\n\npi = some_module.PI\npi\n\n3.14159\n\n\nOr equivalently:\n\nfrom some_module import f,g,PI\nresult = g(5,PI)\nresult\n\n8.14159\n\n\nBy using the as keyword you can give imports different variable names:\n\nimport some_module as sm\nfrom some_module import PI as pi, g as gf\n\nr1 = sm.f(pi)\nr2 = gf(6,pi)\n\n\nr1\n\n5.14159\n\n\n\nr2\n\n9.14159\n\n\n\n\nBinary operators and comparisons\nMost of the binary math operations and comparisons are as you might expect:\n\n5-7\n12+21.5\n5 &lt;= 2\n\nFalse\n\n\n\na = [1,2,3]\nb=a\nc=list()\na is b\na is not c \n\nTrue\n\n\n\na == c\n\nFalse\n\n\n\na = None\na is None\n\nTrue\n\n\n\nMutable and immutable objects\nMost object in Python such as list, dict, NumPy arrays, and most user-defined types(classes), are mutable.\nThis means that the object or values that they contain can be modified\n\na_list = ['foo',2,[4,5]]\na_list[2] = (3,4)\na_list\n\n['foo', 2, (3, 4)]\n\n\nOthers, like strings and tuples, are immutable:\n\na_tuple = (3,5,(4,5))\na_tuple[1] = 'four' #tuple은 변경이 불가능 하다 / list는 가능  \n\nTypeError: 'tuple' object does not support item assignment\n\n\n\n\n\nScalar Types\n\nNumeric types\nThe primary Python types for numbers are int and float. An int can store arbitrarily large numbers:\n\nival = 17239871\nival ** 6\n\n26254519291092456596965462913230729701102721\n\n\nFloating-point numbers are represented with the Python float type. Under the hood each one is a double-precision(64-bit) value. They can also be expressed with scientific notation:\n\nfval = 7.2343\nfval2 = 6.78e-5\n\n\n3/2\n\n1.5\n\n\n\ntype(3/2)\n\nfloat\n\n\n\n3//2 #몫\n\n1\n\n\n\ntype(3//2)\n\nint\n\n\n\n\n\nStrings\nMany people use Python for its powerful and flexible built-in string processing capabilities.\nYou can write string literals using either single quotes or double quotes:\n\na = 'one way of writing a string'\nb = 'another way'\n\nFor multiline strings with line breaks, you can use triple qutoes, either ’’’ or ““”\n\nc = \"\"\" \nThis is a longer string that\nspans multiple lines\n\"\"\"\n\n\nc\n\n' \\nThis is a longer string that\\nspans multiple lines\\n'\n\n\nIt may surprise you that this string c actually contains for lines of text;\nthe line breaks after ““” and after lines are include in the string.\nWe can count the new line characters with the count method on c:\n\nc.count('\\n')\n\n3\n\n\nPython strings are immutable; you cannot modify a string:\n\na = 'this is a string'\na[10] = 'f' #변형불가 \n\nTypeError: 'str' object does not support item assignment\n\n\n\nb = a.replace('string','longer string')\nb\n\n'this is a longer string'\n\n\n\na #변형은 안되나 대체는 가능\n\n'this is a string'\n\n\nMany Python object can be converted to a string using the str function\n\na = 5.6\ns = str(a)\nprint(s)\n\n5.6\n\n\nStrings are a sequnce of Unicode characters and therefore can be treated like other sequences, such as lists and tuples(which we will explore in more detail in the next chapter):\n\ns='python'\nlist(s)\n\n['p', 'y', 't', 'h', 'o', 'n']\n\n\n\ns[:3]\n\n'pyt'\n\n\nThe syntax s[:3] is called slicing and is implemented for many kinds of Python sequences. This will be explained in moire detail later on, as it it used extensively in the book.\nThe backslash character  is an escape character, meaning that it is used to specify special characters like newline or Unicode characters. To write a string literal with backslashes, you need to escape them:\n\nprint('12\\n34') #\\n is Enter \n\n12\n34\n\n\n\ns = '12\\\\34' #백슬래쉬를 문자열로 바꾼다 \nprint(s)\n\n12\\34\n\n\nAdding two strings together concatenates them and produces a new string:\n\na='this is the first half '\nb='and this is the second half'\na+b\n\n'this is the first half and this is the second half'\n\n\nString templating or formatting is another important topic.\nThe number of ways tod do so has expanded with the advent of Python 3,\nand here I will briefly describe the mechanics of one of the main interfaces.\nString objects have a format method that can be used to substitute formatted arguments into the string, producting a new string:\n\ntemplate = '{0:.2f} {1:s} are worth US${2:d}'\ntemplate\n\n'{0:.2f} {1:s} are worth US${2:d}'\n\n\n\n{0:.2f} means to format the first argument as a floating-point number with two decimal places.\n{1:s} means to format the second argument as a string.\n{2:d} means to format the third argument as an exact integer\n\n\ntemplate.format(4.560, 'Argentine Pesos',1)\n\n'4.56 Argentine Pesos are worth US$1'\n\n\n\ntemplate.format(1263.23,'won',1)\n\n'1263.23 won are worth US$1'\n\n\n\n\nBooleans\n\nThe two boolean value in python are written as True as False.\n\n\nComprasions and other conditional expressions evaluate to either True or False.\n\n\nBoolean values are combined with the and or keywords:\n\nTrue and True\n\nTrue\n\n\n\nFalse or True\n\nTrue\n\n\n\n\nType casting\nThe str, bool, int, and float types are also functions that can be used to cast values\n\ns='3.14159'\nfval=float(s)\ntype(fval)\n\nfloat\n\n\n\nint(fval)\n\n3\n\n\n\nbool(fval)\n\nTrue\n\n\n\nbool(0)\n\nFalse\n\n\n\n\n\nNone\nNone is the Python null value type. If a function does not explicitly return a value, it implicitly returns None:\n\na = None\na is None\n\nTrue\n\n\n\nb = 5\nb is not None\n\nTrue\n\n\nNone is also a common default vlaue for function arguments:\n\ndef add_and_maybe_multiply(a,b,c=None):\n    result = a+b\n\n    if c is not None:\n        result = result * c\n    \n    return result\n\n\nadd_and_maybe_multiply(5,3)\n\n8\n\n\n\nadd_and_maybe_multiply(5,3,10)\n\n80\n\n\nWhile a technical point, it’s worth bearing in mind that None is not only a reserved keyword but also a unique instance of NoneType:\n\ntype(None)\n\nNoneType\n\n\n\n\nDates and times\nThe built-in Python datetime module provides datetime, date, and time types.\nThe datetime type, as you may imagine, combines the information stored in date and time and is the most commonly used:\n\nfrom datetime import datetime, date, time\ndt = datetime(2011,10,29,20,30,21) #year,month,day,hour,minute,second\ndt\n\ndatetime.datetime(2011, 10, 29, 20, 30, 21)\n\n\n\ndt.day\n\n29\n\n\n\ndt.minute\n\n30\n\n\nGiven a datetime instance, you can extract the equivalent date and time objects by calling methods on the datetime of the same name:\n\ndt.date()\n\ndatetime.date(2011, 10, 29)\n\n\n\ndt.time()\n\ndatetime.time(20, 30, 21)\n\n\nThe strfime method formats a datetime as a string:\n\ndt.strftime('%m/%d/%Y %H:%M')\n\n'10/29/2011 20:30'\n\n\n\ndt.strftime('%Y/%m/%d %H:%M')\n\n'2011/10/29 20:30'\n\n\nString can be converted (parsed) into datetime objects with the strptime function:\n\ndatetime.strptime('20091031',\"%Y%m%d\")\n\ndatetime.datetime(2009, 10, 31, 0, 0)\n\n\nWhen you are aggregating or otherwise grouping time series data, it will occasionally be useful to replace time fields of a series of datetimes-for example,replacing the minute and second fields with zero:\n\ndt.replace(minute=0,second=0)\n\ndatetime.datetime(2011, 10, 29, 20, 0)\n\n\n\ndt2 = datetime(2011,11,15,22,30) \ndelta =dt2 - dt #dt = datetime(2011,10,29,20,30,21)\ndelta\n\ndatetime.timedelta(days=17, seconds=7179)\n\n\n\ntype(delta)\n\ndatetime.timedelta\n\n\n\ndt\ndt + delta\n\ndatetime.datetime(2011, 11, 15, 22, 30)\n\n\n\n\n\nControl Flow\nPython has several built-in keywords for conditonal logic, loops, and other standard control flow concepts found in other porgramming languages.\n\nif, elif, and else\nThe if statement is one of the most well-known control flow statement types.\nIt checks a conditon that, if True, evaluates the code in the block that follows:\n\nx = -5\nif x &lt; 0:\n    print('It is negative')\n\nIt is negative\n\n\nAn if statement can be optionally followed by one or mor elif blocks and a catch all else block if all of the conditions are False\n\nx = -5\n\nif x &lt; 0 :\n    print('It is negative')\nelif x == 0 :\n    print('Equal to zero')\nelif 0 &lt; x &lt; 5 :\n    print('Postive but smaller than 5')\nelse :\n    print('Postive and larger than or equal to 5')\n\nIt is negative\n\n\nIf any of the conditions is True, no futher elif or else blocks will be reached.\nWith a compound condition using and or or, conditions are evaluated left to right and will short-circuit:\n\na = 5; b = 7\nc = 8; d = 4\nif a &lt; b or c &gt; d :\n    print('Made it')\n\nMade it\n\n\nIn this examplme, the comparison c &gt; d never get evaluated because the first compar- ison was True. It is also possible to chain comparisons:\n\n4 &gt; 3 &gt; 2 &gt; 1\n\nTrue\n\n\n\n3 &gt; 5 or 2 &gt; 1\n\nTrue\n\n\n\n3&gt;5&gt;2&gt;1\n\nFalse\n\n\n\n\nfor loops\nfor loops are fir iterating over a collection (like a list or tuple) or an iterater. They standard syntax for a for loop is:\nfor value in collection: # do something with value\nYou can advance a for loop to the next iteration, skipping the remainder of the block, using the continue keyword.\nConsider this code, which sums up integers in a list and skips None values\n\nsequnce = [1,2,None,4,None,5]\ntotal = 0\n\nfor value in sequnce:\n    total += value\n\nTypeError: unsupported operand type(s) for +=: 'int' and 'NoneType'\n\n\n\nsequnce = [1,2,None,4,None,5]\ntotal = 0\n\nfor value in sequnce:\n    if value is None:\n        continue\n    total += value\n\n\ntotal\n\n12\n\n\nA for loop cna be exited altogether with the break keyword. This code sums ele- ments of the list until a 5 is reached:\n\nsequnce = [1,2,0,4,6,5,2,1]\ntotal_until_5 = 0\n\nfor value in sequnce:\n    if value == 5:\n        break\n    total_until_5 += value\n\n\ntotal_until_5 #값이 5가 되면 멈추는 조건으로 1+2+0+4+6까지 계산후 5가 오기에 for문이 종료된다 \n\n13\n\n\nThe break keyword only terminates the innermost for loop; any outer for loops will continue to run:\n\nfor i in range(4):\n    for j in range(4):\n        if j&gt;i:\n            break\n        print((i,j))\n\n(0, 0)\n(1, 0)\n(1, 1)\n(2, 0)\n(2, 1)\n(2, 2)\n(3, 0)\n(3, 1)\n(3, 2)\n(3, 3)\n\n\nAs we will see in more detail, if the elements in the collection or iterator are sequences (tuples or list, say), they can be conveniently unpacked into variables in the for loop statement:\nfor a,b,c in iterator: # do something\n\nfor a,b,c in [[1,2,3],[4,5,6],[7,8,9]]:\n    print(a,b,c)\n\n1 2 3\n4 5 6\n7 8 9\n\n\n\nWhile loops\nA while looops specifies a conditon and a block o f code that is to be excused until the condition evaluates to False or the loops is explicitly ended with break:\n\nx = 256\ntotal = 0\nwhile x &gt; 0:\n    if total &gt; 500 :\n        break\n    total += x\n    x = x //2\n\n\ntotal #504\n\n504\n\n\n\nx\n\n4\n\n\n\n256+128+64+32+16+8\n\n504\n\n\n\n\npass\npass is the “no-op”(No Operation) statement in Python. It can be used in block where no action is to be taken (or as a placeholder for code not yer implemented); it is only required becauese Python uese whitespace to delimit blocks:\n\nx = -1\n\nif x &lt; 0:\n    print('negative')\nelif x == 0:\n    # TODO: put something smart here\n    pass\nelse:\n    print('Postive!')\n\nnegative\n\n\n\n\nrange\nThe range function returns an iterator that yields a sequnce of evenly spaced intergers:\n\nrange(10)\n\nrange(0, 10)\n\n\n\nlist(range(10))\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\nBoth a start,end,and step(Which may be negative) can be given:\n\nlist(range(0,20,2)) #등차수열\n\n[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n\n\n\nlist(range(5,0,-1)) #리버스 인덱스\n\n[5, 4, 3, 2, 1]\n\n\nAs you can see, range prodices integers up to but not including the endpoint.\nA common use of range is for iterating through sequcnes by index:\n\nseq = [1,2,3,4]\nfor i in range(len(seq)):\n    val = seq[i]\n\n\nval\n\n4\n\n\nWhile you can use fuctions loke list to store all the integers generated by range in some other data structure, often the default iterator form will be what you want. This snippet sums all numbers form 0 to 99,999 that are multiples of 3 or 5:\n\nsum = 0\nfor i in range(10000) :\n    # % is the modulo operator\n    if i % 3 == 0 or i % 5 == 0:\n        sum += 1\n\n\n\n\nTernary expressions\nvalue = true - expr if conditon else false - expr\nHere, true-expr and false-expr cna be any Python expressions. It has the identical effect as the more verbose:\nif conditon: value = true-expr else: value = false-expr\n\nx = 5\n'Non-negative' if x &gt;= 0 else 'Negative'\n\n'Non-negative'\n\n\n\nx = 5\n\na = 100 if x&gt;= 0 else -100\na\n\n100"
  },
  {
    "objectID": "Data_Mining/2022-05-24-exercise-coordinate-reference-systems.html",
    "href": "Data_Mining/2022-05-24-exercise-coordinate-reference-systems.html",
    "title": "Coordinate Refrence Systems",
    "section": "",
    "text": "Coordinate Reference Systems\n\nThis notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link.\n\n\nimport pandas as pd\nimport geopandas as gpd\n\nfrom shapely.geometry import LineString\n\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.geospatial.ex2 import *\n\n\nExercises\n\n1) Load the data.\nRun the next code cell (without changes) to load the GPS data into a pandas DataFrame birds_df.\n다음 코드 셀(변경 없이)을 실행하여 GPS 데이터를 pandas DataFrame birds_df에 로드합니다.\n\n# Load the data and print the first 5 rows\nbirds_df = pd.read_csv(\"../input/geospatial-learn-course-data/purple_martin.csv\", parse_dates=['timestamp'])\nprint(\"There are {} different birds in the dataset.\".format(birds_df[\"tag-local-identifier\"].nunique()))\nbirds_df.head()\n\nThere are 11 birds in the dataset, where each bird is identified by a unique value in the “tag-local-identifier” column. Each bird has several measurements, collected at different times of the year.\nUse the next code cell to create a GeoDataFrame birds.\n- birds should have all of the columns from birds_df, along with a “geometry” column that contains Point objects with (longitude, latitude) locations.\n-birds에는 birds_df의 모든 열과 함께 (경도, 위도) 위치가 있는 Point 개체가 포함된 “geometry” 열이 있어야 합니다. - Set the CRS of birds to {'init': 'epsg:4326'}.\n\n# Your code here: Create the GeoDataFrame\nbirds = gpd.GeoDataFrame(birds_df, geometry=gpd.points_from_xy(birds_df[\"location-long\"], birds_df[\"location-lat\"]))\n\n# Your code here: Set the CRS to {'init': 'epsg:4326'}\nbirds.crs = {'init' :'epsg:4326'}\n\n# Check your answer\nq_1.check()\n\n\n# Lines below will give you a hint or solution code\n#q_1.hint()\n#q_1.solution()\n\n\n\n2) Plot the data.\nNext, we load in the 'naturalearth_lowres' dataset from GeoPandas, and set americas to a GeoDataFrame containing the boundaries of all countries in the Americas (both North and South America). Run the next code cell without changes.\n다음으로 GeoPandas에서 naturalearth_lowres 데이터 세트를 로드하고 americas를 미주(북미와 남미 모두)의 모든 국가 경계를 포함하는 GeoDataFrame으로 설정합니다. 변경 없이 다음 코드 셀을 실행합니다.\n\n# Load a GeoDataFrame with country boundaries in North/South America, print the first 5 rows\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\namericas = world.loc[world['continent'].isin(['North America', 'South America'])]\namericas.head()\n\nUse the next code cell to create a single plot that shows both: (1) the country boundaries in the americas GeoDataFrame, and (2) all of the points in the birds_gdf GeoDataFrame.\n다음 코드 셀을 사용하여 (1) americas GeoDataFrame의 국가 경계와 (2) birds_gdf GeoDataFrame의 모든 점을 모두 표시하는 단일 플롯을 만듭니다.\nDon’t worry about any special styling here; just create a preliminary plot, as a quick sanity check that all of the data was loaded properly. In particular, you don’t have to worry about color-coding the points to differentiate between birds, and you don’t have to differentiate starting points from ending points. We’ll do that in the next part of the exercise.\n\n# Your code here\nax = americas.plot(figsize=(8,8), color='red', linestyle=':', edgecolor='black')\namericas.plot(markersize=1, ax=ax)\n# Uncomment to see a hint\n#q_2.hint()\n\n\n# Get credit for your work after you have created a map\nq_2.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_2.solution()\n\n\n\n3) Where does each bird start and end its journey? (Part 1)\nNow, we’re ready to look more closely at each bird’s path. Run the next code cell to create two GeoDataFrames: - path_gdf contains LineString objects that show the path of each bird. It uses the LineString() method to create a LineString object from a list of Point objects. - path_gdf에는 각 새의 경로를 표시하는 LineString 개체가 포함되어 있습니다. LineString() 메서드를 사용하여 Point 개체 목록에서 LineString 개체를 만듭니다. - start_gdf contains the starting points for each bird. - start_gdf는 각 새의 시작 지점을 포함합니다.\n\n# GeoDataFrame showing path for each bird\npath_df = birds.groupby(\"tag-local-identifier\")['geometry'].apply(list).apply(lambda x: LineString(x)).reset_index()\npath_gdf = gpd.GeoDataFrame(path_df, geometry=path_df.geometry)\npath_gdf.crs = {'init' :'epsg:4326'}\n\n# GeoDataFrame showing starting point for each bird\nstart_df = birds.groupby(\"tag-local-identifier\")['geometry'].apply(list).apply(lambda x: x[0]).reset_index()\nstart_gdf = gpd.GeoDataFrame(start_df, geometry=start_df.geometry)\nstart_gdf.crs = {'init' :'epsg:4326'}\n\n# Show first five rows of GeoDataFrame\nstart_gdf.head()\n\nUse the next code cell to create a GeoDataFrame end_gdf containing the final location of each bird.\n- The format should be identical to that of start_gdf, with two columns (“tag-local-identifier” and “geometry”), where the “geometry” column contains Point objects. - 형식은 두 개의 열(“tag-local-identifier” 및 “geometry”)이 있는 start_gdf'의 형식과 동일해야 합니다. 여기서 \"geometry\" 열은 Point 개체를 포함합니다. - Set the CRS ofend_gdfto{‘init’: ‘epsg:4326’}. -end_gdf의 CRS를{‘init’: ‘epsg:4326’}`으로 설정합니다.\n\n# Your code here\nend_df = birds.groupby(\"tag-local-identifier\")['geometry'].apply(list).apply(lambda x: x[-1]).reset_index()\nend_gdf = gpd.GeoDataFrame(end_df, geometry=end_df.geometry)\nend_gdf.crs = {'init' :'epsg:4326'}\n\n# Check your answer\nq_3.check()\n\n\n# Lines below will give you a hint or solution code\n#q_3.hint()\n#q_3.solution()\n\n\n\n4) Where does each bird start and end its journey? (Part 2)\nUse the GeoDataFrames from the question above (path_gdf, start_gdf, and end_gdf) to visualize the paths of all birds on a single map. You may also want to use the americas GeoDataFrame.\n위 질문의 GeoDataFrames(path_gdf, start_gdf, end_gdf)를 사용하여 단일 지도에서 모든 새의 경로를 시각화하세요. americas GeoDataFrame을 사용할 수도 있습니다.\n\n# Your code here\nax = americas.plot(figsize=(10,10), color='none', edgecolor='gainsboro', zorder=3)\n\n# Add wild lands, campsites, and foot trails to the base map\nstart_gdf.plot(color='lightgreen', ax=ax)\npath_gdf.plot(color='maroon', markersize=2, ax=ax)\nend_gdf.plot(color='black', markersize=1, ax=ax) \n\n# Uncomment to see a hint\n#q_4.hint()\n\n\n# Get credit for your work after you have created a map\nq_4.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_4.solution()\n\n\n\n5) Where are the protected areas in South America? (Part 1)\nIt looks like all of the birds end up somewhere in South America. But are they going to protected areas?\nIn the next code cell, you’ll create a GeoDataFrame protected_areas containing the locations of all of the protected areas in South America. The corresponding shapefile is located at filepath protected_filepath.\n다음 코드 셀에서는 남미의 모든 보호 지역 위치를 포함하는 GeoDataFrame protected_areas를 생성합니다. 해당 shapefile은 파일 경로 protected_filepath에 있습니다.\n\n# Path of the shapefile to load\nprotected_filepath = \"../input/geospatial-learn-course-data/SAPA_Aug2019-shapefile/SAPA_Aug2019-shapefile/SAPA_Aug2019-shapefile-polygons.shp\"\n\n# Your code here\nprotected_areas = gpd.read_file(protected_filepath)\n\n# Check your answer\nq_5.check()\n\n\n# Lines below will give you a hint or solution code\n#q_5.hint()\n#q_5.solution()\n\n\n\n6) Where are the protected areas in South America? (Part 2)\nCreate a plot that uses the protected_areas GeoDataFrame to show the locations of the protected areas in South America. (You’ll notice that some protected areas are on land, while others are in marine waters.)\n‘protected_areas’ GeoDataFrame을 사용하여 남아메리카의 보호 지역 위치를 표시하는 플롯을 만듭니다. (일부 보호 구역은 육지에 있고 다른 보호 구역은 바다에 있음을 알 수 있습니다.)\n\n# Country boundaries in South America\nsouth_america = americas.loc[americas['continent']=='South America']\n\n# Your code here: plot protected areas in South America\nax = south_america.plot(figsize=(8,8), color='whitesmoke', edgecolor='black')\nprotected_areas.plot(markersize=1, ax=ax,alpha=0.4)\n\n# Uncomment to see a hint\n#q_6.hint()\n\n\n# Get credit for your work after you have created a map\nq_6.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_6.solution()\n\n\n\n7) What percentage of South America is protected?\nYou’re interested in determining what percentage of South America is protected, so that you know how much of South America is suitable for the birds.\nAs a first step, you calculate the total area of all protected lands in South America (not including marine area). To do this, you use the “REP_AREA” and “REP_M_AREA” columns, which contain the total area and total marine area, respectively, in square kilometers.\nRun the code cell below without changes.\n\nP_Area = sum(protected_areas['REP_AREA']-protected_areas['REP_M_AREA'])\nprint(\"South America has {} square kilometers of protected areas.\".format(P_Area))\n\nThen, to finish the calculation, you’ll use the south_america GeoDataFrame.\n\nsouth_america.head()\n\nCalculate the total area of South America by following these steps: - Calculate the area of each country using the area attribute of each polygon (with EPSG 3035 as the CRS), and add up the results. The calculated area will be in units of square meters. - 각 폴리곤(CRS로 EPSG 3035 사용)의 ‘area’ 속성을 사용하여 각 국가의 면적을 계산하고 결과를 합산합니다. 계산된 면적은 평방 미터 단위입니다. - Convert your answer to have units of square kilometeters. - 평방 킬로미터 단위가 되도록 답을 변환하십시오.\n\n# Your code here: Calculate the total area of South America (in square kilometers)\ntotalArea = sum(south_america.geometry.to_crs(epsg=3035).area)/10**6\ntotalArea \n# Check your answer\nq_7.check()\n\n\n# Lines below will give you a hint or solution code\n#q_7.hint()\n#q_7.solution()\n\nRun the code cell below to calculate the percentage of South America that is protected.\n\n# What percentage of South America is protected?\npercentage_protected = P_Area/totalArea\nprint('Approximately {}% of South America is protected.'.format(round(percentage_protected*100, 2)))\n\n\n\n8) Where are the birds in South America?\nSo, are the birds in protected areas?\n그렇다면 새들은 보호 구역에 있습니까?\nCreate a plot that shows for all birds, all of the locations where they were discovered in South America. Also plot the locations of all protected areas in South America.\n모든 새, 남미에서 발견된 모든 위치를 보여주는 플롯을 만듭니다. 또한 남아메리카의 모든 보호 지역의 위치를 ​​표시합니다.\nTo exclude protected areas that are purely marine areas (with no land component), you can use the “MARINE” column (and plot only the rows in protected_areas[protected_areas['MARINE']!='2'], instead of every row in the protected_areas GeoDataFrame).\n순수한 해양 지역(토지 구성요소 없음)인 보호 지역을 제외하려면 “MARINE” 열을 사용할 수 있습니다(그리고 protected_areas[protected_areas['MARINE']!='2'] protected_areas GeoDataFrame의 모든 행).\n\n# Your code here\nax = south_america.plot(figsize=(8,8), color='whitesmoke', edgecolor='black')\nprotected_areas[protected_areas['MARINE']!='2'].plot(ax=ax, alpha=0.4, zorder=1)\nbirds[birds.geometry.y &lt; 0].plot(ax=ax, color='red', alpha=0.6, markersize=10, zorder=2)\n\n# Uncomment to see a hint\n#q_8.hint()\n\n\n# Get credit for your work after you have created a map\nq_8.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_8.solution()\n\n\n\n\nKeep going\nCreate stunning interactive maps with your geospatial data.\n\nHave questions or comments? Visit the course discussion forum to chat with other learners."
  },
  {
    "objectID": "Data_Mining/2022-05-24-exercise-your-first-map.html",
    "href": "Data_Mining/2022-05-24-exercise-your-first-map.html",
    "title": "Your First Map",
    "section": "",
    "text": "Your First Map\n\nThis notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link.\n\n\nimport geopandas as gpd\n\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.geospatial.ex1 import *\n\n\n1) Get the data.\nUse the next cell to load the shapefile located at loans_filepath to create a GeoDataFrame world_loans.\n다음 셀을 사용하여 loans_filepath에 있는 shapefile을 로드하여 GeoDataFrame world_loans를 생성합니다.\n\nloans_filepath = \"../input/geospatial-learn-course-data/kiva_loans/kiva_loans/kiva_loans.shp\"\n\n# Your code here: Load the data\nworld_loans = gpd.read_file(loans_filepath)\n\n# Check your answer\nq_1.check()\n\n# Uncomment to view the first five rows of the data\n#world_loans.head()\n\n\n\n2) Plot the data.\nRun the next code cell without changes to load a GeoDataFrame world containing country boundaries.\n변경 없이 다음 코드 셀을 실행하여 국가 경계가 포함된 GeoDataFrame ’world’를 로드합니다.\n\n# Lines below will give you a hint or solution code\n#q_1.hint()\n#q_1.solution()\n\n\n# This dataset is provided in GeoPandas\nworld_filepath = gpd.datasets.get_path('naturalearth_lowres')\nworld = gpd.read_file(world_filepath)\nworld.head()\n\nUse the world and world_loans GeoDataFrames to visualize Kiva loan locations across the world.\nworld 및 world_loans GeoDataFrames를 사용하여 전 세계의 Kiva loan locations를 시각화합니다.\n\n# Your code here\nax = world.plot(figsize=(20,20), color='none', edgecolor='gainsboro',zorder=3)\nworld_loans.plot(color='skyblue', markersize=2,ax=ax)\n# Uncomment to see a hint\n#q_2.hint()\n\n\n# Get credit for your work after you have created a map\nq_2.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_2.solution()\n\n\n\n3) Select loans based in the Philippines.\nNext, you’ll focus on loans that are based in the Philippines. Use the next code cell to create a GeoDataFrame PHL_loans which contains all rows from world_loans with loans that are based in the Philippines.\n다음으로 필리핀에 기반을 둔 대출에 중점을 둘 것입니다. 다음 코드 셀을 사용하여 필리핀에 기반을 둔 대출이 있는 world_loans의 모든 행을 포함하는 GeoDataFrame PHL_loans를 만듭니다.\n\n# Your code here\nPHL_loans = world_loans.loc[world_loans.country==\"Philippines\"].copy()\n\n\n# Check your answer\nq_3.check()\n\n\n# Lines below will give you a hint or solution code\n#q_3.hint()\n#q_3.solution()\n\n\n\n4) Understand loans in the Philippines.\nRun the next code cell without changes to load a GeoDataFrame PHL containing boundaries for all islands in the Philippines.\n필리핀의 모든 섬에 대한 경계를 포함하는 GeoDataFrame PHL을 로드하려면 변경 없이 다음 코드 셀을 실행하십시오.\n\n# Load a KML file containing island boundaries\ngpd.io.file.fiona.drvsupport.supported_drivers['KML'] = 'rw'\nPHL = gpd.read_file(\"../input/geospatial-learn-course-data/Philippines_AL258.kml\", driver='KML')\nPHL.head()\n\nUse the PHL and PHL_loans GeoDataFrames to visualize loans in the Philippines.\n‘PHL’ 및 ‘PHL_loans’ GeoDataFrames를 사용하여 필리핀의 대출을 시각화합니다.\n\n# Your code here\nax = PHL.plot(figsize=(20,20), color='none', edgecolor='gainsboro', zorder=3)\nPHL_loans.plot(color='skyblue', markersize=2, ax=ax)\n\n# Uncomment to see a hint\n#q_4.a.hint()\n\n\n# Get credit for your work after you have created a map\nq_4.a.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_4.a.solution()\n\nCan you identify any islands where it might be useful to recruit new Field Partners? Do any islands currently look outside of Kiva’s reach?\nYou might find this map useful to answer the question.\n\n# View the solution (Run this code cell to receive credit!)\nq_4.b.solution()\n\n\n\nKeep going\nContinue to learn about coordinate reference systems.\n\nHave questions or comments? Visit the course discussion forum to chat with other learners."
  },
  {
    "objectID": "Data_Mining/2022-05-26-exercise-manipulating-geospatial-data.html",
    "href": "Data_Mining/2022-05-26-exercise-manipulating-geospatial-data.html",
    "title": "Manipulating Geospatial Data",
    "section": "",
    "text": "Manipulating Geospatial Data\n\nThis notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link.\n\n\nimport math\nimport pandas as pd\nimport geopandas as gpd\n#from geopy.geocoders import Nominatim            # What you'd normally run\nfrom learntools.geospatial.tools import Nominatim # Just for this exercise\n\nimport folium \nfrom folium import Marker\nfrom folium.plugins import MarkerCluster\n\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.geospatial.ex4 import *\n\nYou’ll use the embed_map() function from the previous exercise to visualize your maps.\n\ndef embed_map(m, file_name):\n    from IPython.display import IFrame\n    m.save(file_name)\n    return IFrame(file_name, width='100%', height='500px')\n\n\nExercises\n\n1) Geocode the missing locations.\nRun the next code cell to create a DataFrame starbucks containing Starbucks locations in the state of California.\n\n# Load and preview Starbucks locations in California\nstarbucks = pd.read_csv(\"../input/geospatial-learn-course-data/starbucks_locations.csv\")\nstarbucks.head()\n\nMost of the stores have known (latitude, longitude) locations. But, all of the locations in the city of Berkeley are missing.\n\n# How many rows in each column have missing values?\nprint(starbucks.isnull().sum())\n\n# View rows with missing locations\nrows_with_missing = starbucks[starbucks[\"City\"]==\"Berkeley\"]\nrows_with_missing\n\nUse the code cell below to fill in these values with the Nominatim geocoder.\nNote that in the tutorial, we used Nominatim() (from geopy.geocoders) to geocode values, and this is what you can use in your own projects outside of this course.\n튜토리얼에서 우리는 값을 지오코딩하기 위해 Nominatim()(geopy.geocoders에서)을 사용했으며 이것은 이 과정 이외의 자신의 프로젝트에서 사용할 수 있는 것입니다.\nIn this exercise, you will use a slightly different function Nominatim() (from learntools.geospatial.tools). This function was imported at the top of the notebook and works identically to the function from GeoPandas.\n이 연습에서는 약간 다른 함수 Nominatim()(learntools.geospatial.tools에서)을 사용합니다. 이 기능은 노트북 상단에서 가져온 것으로 GeoPandas의 기능과 동일하게 작동합니다.\nSo, in other words, as long as: - you don’t change the import statements at the top of the notebook, and - you call the geocoding function as geocode() in the code cell below,\nyour code will work as intended!\n\n# Create the geocoder\ngeolocator = Nominatim(user_agent=\"kaggle_learn\")\n\n# Your code here\n\ndef my_geocoder(row):\n    point = geolocator.geocode(row).point\n    return pd.Series({'Latitude': point.latitude, 'Longitude': point.longitude})\n\nberkeley_locations = rows_with_missing.apply(lambda x: my_geocoder(x['Address']), axis=1)\nstarbucks.update(berkeley_locations)\n\n# Check your answer\nq_1.check()\n\n\n# Line below will give you solution code\n#q_1.solution()\n\n\n\n2) View Berkeley locations.\nLet’s take a look at the locations you just found. Visualize the (latitude, longitude) locations in Berkeley in the OpenStreetMap style.\n방금 찾은 위치를 살펴보겠습니다. OpenStreetMap 스타일로 버클리의 (위도, 경도) 위치를 시각화합니다.\n\n# Create a base map\nm_2 = folium.Map(location=[37.88,-122.26], zoom_start=13)\n\n# Your code here: Add a marker for each Berkeley location\nfor idx, row in starbucks[starbucks[\"City\"]=='Berkeley'].iterrows():\n    Marker([row['Latitude'], row['Longitude']]).add_to(m_2)\n\n# Uncomment to see a hint\n#q_2.a.hint()\n\n# Show the map\nembed_map(m_2, 'q_2.html')\n\n\n# Get credit for your work after you have created a map\nq_2.a.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_2.a.solution()\n\nConsidering only the five locations in Berkeley, how many of the (latitude, longitude) locations seem potentially correct (are located in the correct city)?\n\n# View the solution (Run this code cell to receive credit!)\nq_2.b.solution()\n\n\n\n3) Consolidate your data.\nRun the code below to load a GeoDataFrame CA_counties containing the name, area (in square kilometers), and a unique id (in the “GEOID” column) for each county in the state of California. The “geometry” column contains a polygon with county boundaries.\n\nCA_counties = gpd.read_file(\"../input/geospatial-learn-course-data/CA_county_boundaries/CA_county_boundaries/CA_county_boundaries.shp\")\nCA_counties.head()\n\nNext, we create three DataFrames: - CA_pop contains an estimate of the population of each county. - CA_high_earners contains the number of households with an income of at least $150,000 per year. - CA_median_age contains the median age for each county.\n\nCA_pop = pd.read_csv(\"../input/geospatial-learn-course-data/CA_county_population.csv\", index_col=\"GEOID\")\nCA_high_earners = pd.read_csv(\"../input/geospatial-learn-course-data/CA_county_high_earners.csv\", index_col=\"GEOID\")\nCA_median_age = pd.read_csv(\"../input/geospatial-learn-course-data/CA_county_median_age.csv\", index_col=\"GEOID\")\n\nUse the next code cell to join the CA_counties GeoDataFrame with CA_pop, CA_high_earners, and CA_median_age.\nName the resultant GeoDataFrame CA_stats, and make sure it has 8 columns: “GEOID”, “name”, “area_sqkm”, “geometry”, “population”, “high_earners”, and “median_age”. Also, make sure the CRS is set to {'init': 'epsg:4326'}.\n결과 GeoDataFrame의 이름을 CA_stats로 지정하고 “GEOID”, “name”, “area_sqkm”, “geometry”, “population”, “high_earners” 및 “median_age”의 8개 열이 있는지 확인합니다. 또한 CRS가 {'init': 'epsg:4326'}으로 설정되어 있는지 확인하십시오.\n\n# Your code here\ncols_to_add = CA_pop.join([CA_high_earners, CA_median_age]).reset_index()\nCA_stats = CA_counties.merge(cols_to_add, on=\"GEOID\")\n\n# Check your answer\nq_3.check()\n\n\n# Lines below will give you a hint or solution code\n#q_3.hint()\n#q_3.solution()\n\nNow that we have all of the data in one place, it’s much easier to calculate statistics that use a combination of columns. Run the next code cell to create a “density” column with the population density.\n\nCA_stats[\"density\"] = CA_stats[\"population\"] / CA_stats[\"area_sqkm\"]\n\n\n\n4) Which counties look promising?\nCollapsing all of the information into a single GeoDataFrame also makes it much easier to select counties that meet specific criteria.\nUse the next code cell to create a GeoDataFrame sel_counties that contains a subset of the rows (and all of the columns) from the CA_stats GeoDataFrame. In particular, you should select counties where:\n다음 코드 셀을 사용하여 CA_stats GeoDataFrame에서 행(및 모든 열)의 하위 집합을 포함하는 GeoDataFrame sel_counties를 만듭니다. 특히 다음과 같은 카운티를 선택해야 합니다.\n\nthere are at least 100,000 households making $150,000 per year,\nthe median age is less than 38.5, and\nthe density of inhabitants is at least 285 (per square kilometer).\n\nAdditionally, selected counties should satisfy at least one of the following criteria: - there are at least 500,000 households making $150,000 per year, - the median age is less than 35.5, or - the density of inhabitants is at least 1400 (per square kilometer).\n\n# Your code here\nsel_counties = CA_stats[((CA_stats.high_earners &gt; 100000) &\n                         (CA_stats.median_age &lt; 38.5) &\n                         (CA_stats.density &gt; 285) &\n                         ((CA_stats.median_age &lt; 35.5) |\n                         (CA_stats.density &gt; 1400) |\n                         (CA_stats.high_earners &gt; 500000)))]\n\n# Check your answer\nq_4.check()\n\n\n# Lines below will give you a hint or solution code\n#q_4.hint()\n#q_4.solution()\n\n\n\n5) How many stores did you identify?\nWhen looking for the next Starbucks Reserve Roastery location, you’d like to consider all of the stores within the counties that you selected. So, how many stores are within the selected counties?\nTo prepare to answer this question, run the next code cell to create a GeoDataFrame starbucks_gdf with all of the starbucks locations.\n\nstarbucks_gdf = gpd.GeoDataFrame(starbucks, geometry=gpd.points_from_xy(starbucks.Longitude, starbucks.Latitude))\nstarbucks_gdf.crs = {'init': 'epsg:4326'}\n\nSo, how many stores are in the counties you selected?\n그렇다면 선택한 카운티에는 몇 개의 매장이 있습니까?\n\n# Fill in your answer\nlocations_of_interest = gpd.sjoin(starbucks_gdf, sel_counties)\nnum_stores = len(locations_of_interest)\n\n# Check your answer\nq_5.check()\n\n\n# Lines below will give you a hint or solution code\n#q_5.hint()\n#q_5.solution()\n\n\n\n6) Visualize the store locations.\nCreate a map that shows the locations of the stores that you identified in the previous question.\n이전 질문에서 식별한 상점의 위치를 ​​보여주는 지도를 만드십시오.\n\n# Create a base map\nm_6 = folium.Map(location=[37,-120], zoom_start=6)\n\n# Your code here: show selected store locations\nmc = MarkerCluster()\n\nlocations_of_interest = gpd.sjoin(starbucks_gdf, sel_counties)\nfor idx, row in locations_of_interest.iterrows():\n    if not math.isnan(row['Longitude']) and not math.isnan(row['Latitude']):\n        mc.add_child(folium.Marker([row['Latitude'], row['Longitude']]))\n\nm_6.add_child(mc)\n\n# Uncomment to see a hint\n#q_6.hint()\n\n# Show the map\nembed_map(m_6, 'q_6.html')\n\n\n# Get credit for your work after you have created a map\n#q_6.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_6.solution()\n\n\n\n\nKeep going\nLearn about how proximity analysis can help you to understand the relationships between points on a map.\n\nHave questions or comments? Visit the course discussion forum to chat with other learners."
  },
  {
    "objectID": "Data_Mining/2022-06-07-last.html",
    "href": "Data_Mining/2022-06-07-last.html",
    "title": "COVID-19 Analysis & Visualization",
    "section": "",
    "text": "COVID-19 Analysis & Visualization\n\n\n서론 \n\n분석 배경 및 목적 \n\n분석 배경 \n분석 배경 - 2022년 6월 현재 코로나의 상황은 매일 10000명의 확진자가 나오고 있는 상황이지만 코로나가 처음 발병하고 나서와는 조금은 다른 반응이다. 최근 정부에서는 집단 면역이 90% 이상 형성이 되어있으며 확진자의 추세 또한 감소세를 보이고 있는 상황에서 2020년 01월부터 2020년 06월까지 수집된 해당 데이터를 기반으로 과연 과거와 현재의 차이는 얼마나 있고 당시 정부와 뉴스에서 주장하던 코로나에 대한 정보는 과연 타당하였고 올바른 정보였는지 궁금하여 해당 주제를 선정하여 분석을 진행하게 되었습니다.\n\n\n분석 목적 \n분석 목적 - 자료를 제공한 데이콘에서는 해당 자료들은 이용해서 다음과 같은 인공지능 AI를 활용 코로나 확산 방지와 예방을 위한 인사이트 / 시각화 발굴. 이라는 목적을 가지고 진행을 하였습니다. 해서 저는 당시 기간동안 가장 많이 확진된 연령층과 주된 감염 원인과 그 이유에 대해서 알아보고, 어떤 연령층에게 가장 치명적인 질병이었느지와 당시 정부의 방역 대책은 타당하였는지 에 대해서 목적을 가지고 해당 분석을 진행하였습니다.\n\n\n\n데이터 소개 \n\n\n데이터 카테고리 \n\nCase Data\n\n\nCase: 한국의 COVID-19 감염 사례 데이터\n\n\nPatient Data\n\n\nPatientInfo: 한국의 코로나19 환자 역학 데이터\nPatientRoute: 국내 코로나19 환자 경로 데이터\n\n\nTime Series Data\n\n\nTime: 한국의 코로나19 상태의 시계열 데이터\nTimeAge: 한국의 연령별 코로나19 현황 시계열 데이터\nTimeGender: 한국의 성별에 따른 코로나19 현황의 시계열 데이터\nTimeProvince: 한국의 지역별 코로나19 현황 시계열 데이터\n\n\nAdditional Data\n\n\nRegion: 대한민국 내 지역의 위치 및 통계 자료\nWeather: 한국 지역의 날씨 데이터\nSearchTrend: 국내 최대 포털사이트 네이버에서 검색된 키워드의 트렌드 데이터\nSeoulFloating: 대한민국 서울 유동인구 데이터(SK텔레콤 빅데이터 허브에서)\nPolicy: 한국의 코로나19에 대한 정부 정책 데이터\n\n\n\n데이터 형태 \n\n색상이 의미하는 것은 비슷한 속성을 가지고 있다는 것입니다.\n열 사이에 선이 연결되어 있다는 것은 열의 값이 부분적으로 공유됨을 의미합니다.\n점선은 약한 관련성을 의미합니다.\n\nhttps://user-images.githubusercontent.com/50820635/86225695-8dca0580-bbc5-11ea-9e9b-b0ca33414d8a.PNG\n\n\n데이터 세부 설명 \n\nimport numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('../notebook/coronavirusdataset'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n../notebook/coronavirusdataset\\Case.csv\n../notebook/coronavirusdataset\\PatientInfo.csv\n../notebook/coronavirusdataset\\PatientRoute.csv\n../notebook/coronavirusdataset\\Policy.csv\n../notebook/coronavirusdataset\\Region.csv\n../notebook/coronavirusdataset\\SearchTrend.csv\n../notebook/coronavirusdataset\\SeoulFloating.csv\n../notebook/coronavirusdataset\\Time.csv\n../notebook/coronavirusdataset\\TimeAge.csv\n../notebook/coronavirusdataset\\TimeGender.csv\n../notebook/coronavirusdataset\\TimeProvince.csv\n../notebook/coronavirusdataset\\Weather.csv\n\n\n\npath = '../notebook/coronavirusdataset/'\n\ncase = p_info = pd.read_csv(path+'Case.csv')\npatientinfo = pd.read_csv(path+'PatientInfo.csv')\npatientroute = pd.read_csv(path+'PatientRoute.csv')\ntime = pd.read_csv(path+'Time.csv')\ntimeage = pd.read_csv(path+'TimeAge.csv')\ntimegender = pd.read_csv(path+'TimeGender.csv')\ntimeprovince = pd.read_csv(path+'TimeProvince.csv')\nregion = pd.read_csv(path+'Region.csv')\nweather = pd.read_csv(path+'Weather.csv')\nsearchtrend = pd.read_csv(path+'SearchTrend.csv')\nseoulfloating = pd.read_csv(path+'SeoulFloating.csv')\npolicy = pd.read_csv(path+'Policy.csv')\n\n\nCase\n\n한국의 COVID-19 감염 사례 데이터\n\ncase_id: the ID of the infection case\n\n\ncase_id(7) = region_code(5) + case_number(2)\nYou can check the region_code in ‘Region.csv’\nprovince: Special City / Metropolitan City / Province(-do)\ncity: City(-si) / Country (-gun) / District (-gu)\n\nThe value ‘from other city’ means that where the group infection started is other city.\n\ngroup: TRUE: group infection / FALSE: not group\n\nIf the value is ‘TRUE’ in this column, the value of ‘infection_cases’ means the name of group.\nThe values named ‘contact with patient’, ‘overseas inflow’ and ‘etc’ are not group infection.\n\ninfection_case: the infection case (the name of group or other cases)\n\nThe value ‘overseas inflow’ means that the infection is from other country.\nThe value ‘etc’ includes individual cases, cases where relevance classification is ongoing after investigation, and cases under investigation.\n\nconfirmed: the accumulated number of the confirmed\nlatitude: the latitude of the group (WGS84)\nlongitude: the longitude of the group (WGS84)\n\n\ncase.head()\n\n\n\n\n\n\n\n\ncase_id\nprovince\ncity\ngroup\ninfection_case\nconfirmed\nlatitude\nlongitude\n\n\n\n\n0\n1000001\nSeoul\nYongsan-gu\nTrue\nItaewon Clubs\n139\n37.538621\n126.992652\n\n\n1\n1000002\nSeoul\nGwanak-gu\nTrue\nRichway\n119\n37.48208\n126.901384\n\n\n2\n1000003\nSeoul\nGuro-gu\nTrue\nGuro-gu Call Center\n95\n37.508163\n126.884387\n\n\n3\n1000004\nSeoul\nYangcheon-gu\nTrue\nYangcheon Table Tennis Club\n43\n37.546061\n126.874209\n\n\n4\n1000005\nSeoul\nDobong-gu\nTrue\nDay Care Center\n43\n37.679422\n127.044374\n\n\n\n\n\n\n\n\nPatientInfo\n\n한국의 코로나19 환자 역학 데이터\n\npatient_id: the ID of the patient\n\n\npatient_id(10) = region_code(5) + patient_number(5)\nYou can check the region_code in ‘Region.csv’\nThere are two types of the patient_number\n\nlocal_num: The number given by the local government.\nglobal_num: The number given by the KCDC\n\nsex: the sex of the patient\nage: the age of the patient\n\n0s: 0 ~ 9\n10s: 10 ~ 19 …\n90s: 90 ~ 99\n100s: 100 ~ 109\n\ncountry: the country of the patient\nprovince: the province of the patient\ncity: the city of the patient\ninfection_case: the case of infection\ninfected_by: the ID of who infected the patient\n\nThis column refers to the ‘patient_id’ column.\n\ncontact_number: the number of contacts with people\nsymptom_onset_date: the date of symptom onset\nconfirmed_date: the date of being confirmed\nreleased_date: the date of being released\ndeceased_date: the date of being deceased\nstate: isolated / released / deceased\n\nisolated: being isolated in the hospital\nreleased: being released from the hospital\ndeceased: being deceased\n\n\n\npatientinfo.head()\n\n\n\n\n\n\n\n\npatient_id\nsex\nage\ncountry\nprovince\ncity\ninfection_case\ninfected_by\ncontact_number\nsymptom_onset_date\nconfirmed_date\nreleased_date\ndeceased_date\nstate\n\n\n\n\n0\n1000000001\nmale\n50s\nKorea\nSeoul\nGangseo-gu\noverseas inflow\nNaN\n75\n2020-01-22\n2020-01-23\n2020-02-05\nNaN\nreleased\n\n\n1\n1000000002\nmale\n30s\nKorea\nSeoul\nJungnang-gu\noverseas inflow\nNaN\n31\nNaN\n2020-01-30\n2020-03-02\nNaN\nreleased\n\n\n2\n1000000003\nmale\n50s\nKorea\nSeoul\nJongno-gu\ncontact with patient\n2002000001\n17\nNaN\n2020-01-30\n2020-02-19\nNaN\nreleased\n\n\n3\n1000000004\nmale\n20s\nKorea\nSeoul\nMapo-gu\noverseas inflow\nNaN\n9\n2020-01-26\n2020-01-30\n2020-02-15\nNaN\nreleased\n\n\n4\n1000000005\nfemale\n20s\nKorea\nSeoul\nSeongbuk-gu\ncontact with patient\n1000000002\n2\nNaN\n2020-01-31\n2020-02-24\nNaN\nreleased\n\n\n\n\n\n\n\n\nPatientRoute\n\n한국의 코로나19 환자 경로 데이터\n\npatient_id: the ID of the patient\ndate: YYYY-MM-DD\nprovince: Special City / Metropolitan City / Province(-do)\ncity: City(-si) / Country (-gun) / District (-gu)\nlatitude: the latitude of the visit (WGS84)\nlongitude: the longitude of the visit (WGS84)\n\n\npatientroute.head()\n\n\n\n\n\n\n\n\npatient_id\nglobal_num\ndate\nprovince\ncity\ntype\nlatitude\nlongitude\n\n\n\n\n0\n1000000001\n2.0\n2020-01-22\nGyeonggi-do\nGimpo-si\nairport\n37.615246\n126.715632\n\n\n1\n1000000001\n2.0\n2020-01-24\nSeoul\nJung-gu\nhospital\n37.567241\n127.005659\n\n\n2\n1000000002\n5.0\n2020-01-25\nSeoul\nSeongbuk-gu\netc\n37.592560\n127.017048\n\n\n3\n1000000002\n5.0\n2020-01-26\nSeoul\nSeongbuk-gu\nstore\n37.591810\n127.016822\n\n\n4\n1000000002\n5.0\n2020-01-26\nSeoul\nSeongdong-gu\npublic_transportation\n37.563992\n127.029534\n\n\n\n\n\n\n\n\nTime\n\n한국의 COVID-19 상태의 시계열 데이터\n\ndate: YYYY-MM-DD\ntime: Time (0 = AM 12:00 / 16 = PM 04:00)\n\nThe time for KCDC to open the information has been changed from PM 04:00 to AM 12:00 since March 2nd.\n\ntest: the accumulated number of tests\n\nA test is a diagnosis of an infection.\n\nnegative: the accumulated number of negative results\nconfirmed: the accumulated number of positive results\nreleased: the accumulated number of releases\ndeceased: the accumulated number of deceases\n\n\ntime.head()\n\n\n\n\n\n\n\n\ndate\ntime\ntest\nnegative\nconfirmed\nreleased\ndeceased\n\n\n\n\n0\n2020-01-20\n16\n1\n0\n1\n0\n0\n\n\n1\n2020-01-21\n16\n1\n0\n1\n0\n0\n\n\n2\n2020-01-22\n16\n4\n3\n1\n0\n0\n\n\n3\n2020-01-23\n16\n22\n21\n1\n0\n0\n\n\n4\n2020-01-24\n16\n27\n25\n2\n0\n0\n\n\n\n\n\n\n\n\nTimeAge\n\n한국의 연령별 코로나19 현황 시계열 데이터\n\ndate: YYYY-MM-DD\n\nThe status in terms of the age has been presented since March 2nd.\n\ntime: Time\nage: the age of patients\nconfirmed: the accumulated number of the confirmed\ndeceased: the accumulated number of the deceased\n\n\ntimeage.head()\n\n\n\n\n\n\n\n\ndate\ntime\nage\nconfirmed\ndeceased\n\n\n\n\n0\n2020-03-02\n0\n0s\n32\n0\n\n\n1\n2020-03-02\n0\n10s\n169\n0\n\n\n2\n2020-03-02\n0\n20s\n1235\n0\n\n\n3\n2020-03-02\n0\n30s\n506\n1\n\n\n4\n2020-03-02\n0\n40s\n633\n1\n\n\n\n\n\n\n\n\nTimeGender\n\n한국의 성별에 따른 COVID-19 현황의 시계열 데이터\n\ndate: YYYY-MM-DD\n\nThe status in terms of the gender has been presented since March 2nd.\n\ntime: Time\nsex: the gender of patients\nconfirmed: the accumulated number of the confirmed\ndeceased: the accumulated number of the deceased\n\n\ntimegender.head()\n\n\n\n\n\n\n\n\ndate\ntime\nsex\nconfirmed\ndeceased\n\n\n\n\n0\n2020-03-02\n0\nmale\n1591\n13\n\n\n1\n2020-03-02\n0\nfemale\n2621\n9\n\n\n2\n2020-03-03\n0\nmale\n1810\n16\n\n\n3\n2020-03-03\n0\nfemale\n3002\n12\n\n\n4\n2020-03-04\n0\nmale\n1996\n20\n\n\n\n\n\n\n\n\nTimeProvince\n\n한국의 지역별 코로나19 현황 시계열 데이터\n\ndate: YYYY-MM-DD\ntime: Time\nprovince: the province of South Korea\nconfirmed: the accumulated number of the confirmed in the province\n\nThe confirmed status in terms of the provinces has been presented since Feburary 21th.\nThe value before Feburary 21th can be different.\n\nreleased: the accumulated number of the released in the province\n\nThe confirmed status in terms of the provinces has been presented since March 5th. -The value before March 5th can be different.\n\ndeceased: the accumulated number of the deceased in the province\n\nThe confirmed status in terms of the provinces has been presented since March 5th.\nThe value before March 5th can be different.\n\n\n\ntimeprovince.head()\n\n\n\n\n\n\n\n\ndate\ntime\nprovince\nconfirmed\nreleased\ndeceased\n\n\n\n\n0\n2020-01-20\n16\nSeoul\n0\n0\n0\n\n\n1\n2020-01-20\n16\nBusan\n0\n0\n0\n\n\n2\n2020-01-20\n16\nDaegu\n0\n0\n0\n\n\n3\n2020-01-20\n16\nIncheon\n1\n0\n0\n\n\n4\n2020-01-20\n16\nGwangju\n0\n0\n0\n\n\n\n\n\n\n\n\nRegion\n\n대한민국 내 지역의 위치 및 통계 자료\n\ncode: the code of the region\nprovince: Special City / Metropolitan City / Province(-do)\ncity: City(-si) / Country (-gun) / District (-gu)\nlatitude: the latitude of the visit (WGS84)\nlongitude: the longitude of the visit (WGS84)\nelementary_school_count: the number of elementary schools\nkindergarten_count: the number of kindergartens\nuniversity_count: the number of universities\nacademy_ratio: the ratio of academies\nelderly_population_ratio: the ratio of the elderly population\nelderly_alone_ratio: the ratio of elderly households living alone\nnursing_home_count: the number of nursing homes\n\nSource of the statistic: KOSTAT (Statistics Korea)\n\nregion.head()\n\n\n\n\n\n\n\n\ncode\nprovince\ncity\nlatitude\nlongitude\nelementary_school_count\nkindergarten_count\nuniversity_count\nacademy_ratio\nelderly_population_ratio\nelderly_alone_ratio\nnursing_home_count\n\n\n\n\n0\n10000\nSeoul\nSeoul\n37.566953\n126.977977\n607\n830\n48\n1.44\n15.38\n5.8\n22739\n\n\n1\n10010\nSeoul\nGangnam-gu\n37.518421\n127.047222\n33\n38\n0\n4.18\n13.17\n4.3\n3088\n\n\n2\n10020\nSeoul\nGangdong-gu\n37.530492\n127.123837\n27\n32\n0\n1.54\n14.55\n5.4\n1023\n\n\n3\n10030\nSeoul\nGangbuk-gu\n37.639938\n127.025508\n14\n21\n0\n0.67\n19.49\n8.5\n628\n\n\n4\n10040\nSeoul\nGangseo-gu\n37.551166\n126.849506\n36\n56\n1\n1.17\n14.39\n5.7\n1080\n\n\n\n\n\n\n\n\nWeather\n\n한국 지역의 날씨 데이터\n\ncode: the code of the region\nprovince: Special City / Metropolitan City / Province(-do)\ndate: YYYY-MM-DD\navg_temp: the average temperature\nmin_temp: the lowest temperature\nmax_temp: the highest temperature\nprecipitation: the daily precipitation\nmax_wind_speed: the maximum wind speed\nmost_wind_direction: the most frequent wind direction\navg_relative_humidity: the average relative humidity\n\nSource of the weather data: KMA (Korea Meteorological Administration)\n\nweather.head()\n\n\n\n\n\n\n\n\ncode\nprovince\ndate\navg_temp\nmin_temp\nmax_temp\nprecipitation\nmax_wind_speed\nmost_wind_direction\navg_relative_humidity\n\n\n\n\n0\n10000\nSeoul\n2016-01-01\n1.2\n-3.3\n4.0\n0.0\n3.5\n90.0\n73.0\n\n\n1\n11000\nBusan\n2016-01-01\n5.3\n1.1\n10.9\n0.0\n7.4\n340.0\n52.1\n\n\n2\n12000\nDaegu\n2016-01-01\n1.7\n-4.0\n8.0\n0.0\n3.7\n270.0\n70.5\n\n\n3\n13000\nGwangju\n2016-01-01\n3.2\n-1.5\n8.1\n0.0\n2.7\n230.0\n73.1\n\n\n4\n14000\nIncheon\n2016-01-01\n3.1\n-0.4\n5.7\n0.0\n5.3\n180.0\n83.9\n\n\n\n\n\n\n\n\nSearchTrend\n\n국내 최대 포털인 네이버에서 검색된 키워드의 트렌드 데이터\n\ndate: YYYY-MM-DD\ncold: the search volume of ‘cold’ in Korean language\n\nThe unit means relative value by setting the highest search volume in the period to 100.\n\nflu: the search volume of ‘flu’ in Korean language\n\nSame as above.\n\npneumonia: the search volume of ‘pneumonia’ in Korean language -Same as above.\ncoronavirus: the search volume of ‘coronavirus’ in Korean language -Same as above.\n\nSource of the data: NAVER DataLab\n\nsearchtrend.head()\n\n\n\n\n\n\n\n\ndate\ncold\nflu\npneumonia\ncoronavirus\n\n\n\n\n0\n2016-01-01\n0.11663\n0.05590\n0.15726\n0.00736\n\n\n1\n2016-01-02\n0.13372\n0.17135\n0.20826\n0.00890\n\n\n2\n2016-01-03\n0.14917\n0.22317\n0.19326\n0.00845\n\n\n3\n2016-01-04\n0.17463\n0.18626\n0.29008\n0.01145\n\n\n4\n2016-01-05\n0.17226\n0.15072\n0.24562\n0.01381\n\n\n\n\n\n\n\n\nSeoulFloating\n\n대한민국 서울 유동인구 데이터(SK텔레콤 빅데이터 허브에서)\n\ndate: YYYY-MM-DD\nhour: Hour\nbirth_year: the birth year of the floating population\nsext: he sex of the floating population\nprovince: Special City / Metropolitan City / Province(-do)\ncity: City(-si) / Country (-gun) / District (-gu)\nfp_num: the number of floating population\n\nSource of the data: SKT Big Data Hub\n\nseoulfloating.head()\n\n\n\n\n\n\n\n\ndate\nhour\nbirth_year\nsex\nprovince\ncity\nfp_num\n\n\n\n\n0\n2020-01-01\n0\n20\nfemale\nSeoul\nDobong-gu\n19140\n\n\n1\n2020-01-01\n0\n20\nmale\nSeoul\nDobong-gu\n19950\n\n\n2\n2020-01-01\n0\n20\nfemale\nSeoul\nDongdaemun-gu\n25450\n\n\n3\n2020-01-01\n0\n20\nmale\nSeoul\nDongdaemun-gu\n27050\n\n\n4\n2020-01-01\n0\n20\nfemale\nSeoul\nDongjag-gu\n28880\n\n\n\n\n\n\n\n\nPolicy\n\n한국의 COVID-19에 대한 정부 정책 데이터\n\npolicy_id: the ID of the policy\ncountry: the country that implemented the policy\ntype: the type of the policy\ngov_policy: the policy of the government\ndetail: the detail of the policy\nstart_date: the start date of the policy\nend_date: the end date of the policy\n\n\npolicy.head()\n\n\n\n\n\n\n\n\npolicy_id\ncountry\ntype\ngov_policy\ndetail\nstart_date\nend_date\n\n\n\n\n0\n1\nKorea\nAlert\nInfectious Disease Alert Level\nLevel 1 (Blue)\n2020-01-03\n2020-01-19\n\n\n1\n2\nKorea\nAlert\nInfectious Disease Alert Level\nLevel 2 (Yellow)\n2020-01-20\n2020-01-27\n\n\n2\n3\nKorea\nAlert\nInfectious Disease Alert Level\nLevel 3 (Orange)\n2020-01-28\n2020-02-22\n\n\n3\n4\nKorea\nAlert\nInfectious Disease Alert Level\nLevel 4 (Red)\n2020-02-23\nNaN\n\n\n4\n5\nKorea\nImmigration\nSpecial Immigration Procedure\nfrom China\n2020-02-04\nNaN\n\n\n\n\n\n\n\n\n\n\n본론 \n\n주제1 - 어떤 연령층이 가장 많이 확진되었는가? \n\n주제1 - EDA \n\n# import packages - 사용할 패키지 불러오기\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom matplotlib import pyplot as plt\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\n\ntime.head() #time 데이터의 상위 5개를 확인\n\n\n\n\n\n\n\n\ndate\ntime\ntest\nnegative\nconfirmed\nreleased\ndeceased\n\n\n\n\n0\n2020-01-20\n16\n1\n0\n1\n0\n0\n\n\n1\n2020-01-21\n16\n1\n0\n1\n0\n0\n\n\n2\n2020-01-22\n16\n4\n3\n1\n0\n0\n\n\n3\n2020-01-23\n16\n22\n21\n1\n0\n0\n\n\n4\n2020-01-24\n16\n27\n25\n2\n0\n0\n\n\n\n\n\n\n\n\n#시간의 흐름에 따른 확진자 추이\nfig = go.Figure() #빈 도화지를 만든다는 개념\n\nfig.add_trace(go.Scatter(x=time['date'], y=time['confirmed'],\n                    mode='lines', \n                    name='확진(confirmed)')) \n                    #Scatter 형태의 플랏으로 x축은 time데이터의 date컬럼을 사용하고 y축은 time데이터의 confirmed 컬럼을 사용하고 표현 방법은 line이며 선의 이름은 확진으로 지정\n\nfig.update_layout(title='시간의 흐름에 따른 확진자 추이',\n                   xaxis_title='Date',\n                   yaxis_title='Number') #그래프의 제목과 x축 y축의 이름을 지정\n\nfig.show() #그래프를 출력해서 보이도록\n\n\n                                                \n\n\n해당 그래프를 보면 시간의 흐름에 따른 누적 확진자의 수를 나타낸것으로 20년 3월부터 가파른 경사를 보이면서 우상향해서 증가하는 추세를 보이고 있습니다\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=time['date'], y=time['confirmed'],\n                    mode='lines', \n                    name='확진(confirmed)'))\n\nfig.add_trace(go.Scatter(x=time['date'], y=time['released'],\n                    mode='lines', \n                    name='해제(released)'))\n                    \nfig.add_trace(go.Scatter(x=time['date'], y=time['deceased'],\n                    mode='lines', \n                    name='사망(deceased)'))\n\nfig.update_layout(title='시간의 흐름에 따른 코로나의 추이',\n                   xaxis_title='Date',\n                   yaxis_title='Number')\n\nfig.show()\n\n\n                                                \n\n\n다음 그래프는 시간의 흐름에 따른 코로나의 추이로 확진자와 격리해제자의 추이와 사망자의 추이에 대해서 보여주고 있으며 확진자와 격리해제자의 추이는 유사하게 우상향하는 모습을 보여주고 있으며 사망자는 확진자와 격리자의 수에 비해서는 적어서 눈에 보이는 변화는 없습니다.\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=time['date'], y=time['confirmed'],\n                    mode='lines', \n                    name='확진(confirmed)'))\n\nfig.add_trace(go.Scatter(x=time['date'],y=time['negative'],\n             mode='lines', name='음성(Negative)'))\n\nfig.add_trace(go.Scatter(x=time['date'],y=time['test'],\n             mode='markers', name='검사(Test)'))\n\nfig.update_layout(title='시간의 흐름에 따른 코로나의 검사 추이',\n                   xaxis_title='Date',\n                   yaxis_title='Number')\n\nfig.show()\n\n\n                                                \n\n\n해당 그래프는 시간의 흐름에 따른 코로나의 검사 추이로 검사수와 음성의 수가 거의 붙어서 우상향하는 모습이고 확진자는 이에 비해 변동이 없어 보이는 모습입니다. 이를 통해 검사를 많이 했지만 이에 비해서 확진이 된 정도는 상당히 적음을 알 수 있습니다.\n\nfig = go.Figure()\nfig.add_trace(go.Bar(x=time['date'],y=time['confirmed'].diff(), \n                     name='confirmed', marker_color='rgba(152, 0, 0, .8)'))\n\nfig.update_layout(title='일단위 확진자 수',\n                   xaxis_title='Date',\n                   yaxis_title='Number')\n\nfig.show()\n\n\n                                                \n\n\n다음은 일단위 확진자의 수를 보여주고 있습니다. 20년 3월 인근에서 800여명 까지 일일 확진되는 모습을 보이고 점차 감소하는 모습을 보이는 형태입니다\n\n\n주제1 - 연령대별 확진 비율 \n\ndisplay(timeage.head()) #timeage 데이터셋의 기본적인 형태를 파악하기 위해 상위 5개의 행만 추출\ndisplay(timeage.age.unique()) #timeage 데이터셋에서 age 컬럼에서 어떤 연령층이 있는지 unique 함수를 통해서 추출\n\n\n\n\n\n\n\n\ndate\ntime\nage\nconfirmed\ndeceased\n\n\n\n\n0\n2020-03-02\n0\n0s\n32\n0\n\n\n1\n2020-03-02\n0\n10s\n169\n0\n\n\n2\n2020-03-02\n0\n20s\n1235\n0\n\n\n3\n2020-03-02\n0\n30s\n506\n1\n\n\n4\n2020-03-02\n0\n40s\n633\n1\n\n\n\n\n\n\n\narray(['0s', '10s', '20s', '30s', '40s', '50s', '60s', '70s', '80s'],\n      dtype=object)\n\n\n\nage_list = timeage.age.unique()\nage_list #앞에서 설명한 연령대를 따로 추출하여 age_list라는 곳에 할당을 시킴 \n\narray(['0s', '10s', '20s', '30s', '40s', '50s', '60s', '70s', '80s'],\n      dtype=object)\n\n\n\nfig, ax = plt.subplots(figsize = (13,7)) #도화지(Figure : fig)를 깔고 그래프를 그릴 구역(Axes : ax)을 정의합니다. figsize를 통해서 도화지의 크기를 지정해준다\n#이는 objection oriented API으로 그래프의 각 부분을 객체로 지정하고 그리는 유형이다\nsns.barplot(age_list,timeage.confirmed[-9:])\nax.set_xlabel('age',size=13) #연령\nax.set_ylabel('number of case',size=13) #케이스의 횟수\nplt.title('Confirmed Cases by Age')\nplt.show()\n\nc:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning:\n\nPass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n\n\n\n\n\n\n연령대별로 분석을 해본결과 20대가 압도적으로 많은 수를 차지하고 있는 형태의 플랏을 볼 수가 있다\n2020년 연령대별 인구\n\nhttps://kosis.kr/visual/populationKorea/experienceYard/populationPyramid.do?mb=N&menuId=M_3_2\n\n해당 자료를 이용해서 인구수와 확진 비율을 확인하여 과연 20대가 인구수가 많아서 이렇게 많이 확진이 되었는가에 대해서 알아본다\n\nage_order = pd.DataFrame() #빈 데이터 프레임을 생성\nage_order['age']  = age_list #앞서 생성한 age_list를 새로 만드는 데이터 프레임에 age라는 이름의 컬럼으로 할당\nage_order['population'] = [4054901, 4769187, 7037893, 7174782, 8257903, 8575336, 6476602, 3598811, 1657942] #population이라는 컬럼에 통계청 홈페이지에서 확인한 값을 입력\nage_order['proportion'] = round(age_order['population']/sum(age_order['population'])*100,2) \n#인구 비율을 구하기 위해 모든 인구수를 더하고 각 연령별로 나누고 소수점으로 나오는것을 방지하기 위해 100을 곱하고 소수점 2번째 자리까지 표현이 되도록 설정\nage_order = age_order.sort_values('age') #age를 기준으로 재정렬\nage_order.set_index(np.arange(1,10),inplace=True) #인덱스의 설정을 1~10순으로 들어가도록 설정하고 본래 있던것은 대체해서 사용하도록\nage_order\n\n\n\n\n\n\n\n\nage\npopulation\nproportion\n\n\n\n\n1\n0s\n4054901\n7.86\n\n\n2\n10s\n4769187\n9.24\n\n\n3\n20s\n7037893\n13.64\n\n\n4\n30s\n7174782\n13.90\n\n\n5\n40s\n8257903\n16.00\n\n\n6\n50s\n8575336\n16.62\n\n\n7\n60s\n6476602\n12.55\n\n\n8\n70s\n3598811\n6.97\n\n\n9\n80s\n1657942\n3.21\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(13, 7)) \nplt.title('Korea Age Proportion', fontsize=17)\nsns.barplot(age_list, age_order.proportion[-9:])\nax.set_xlabel('Age', size=13)\nax.set_ylabel('Rate (%)', size=13)\nplt.show() \n#한국의 2020년 연령별 인구의 수를 나타낸 표이다 \n\nc:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning:\n\nPass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n\n\n\n\n\n\n예상과는 다르게 20대가 가장 많은 인구수를 가지고 있는 연령대가 아닌 40,50대가 가장 인구수가 많은 연령대임을 알 수 있다. 이로 20대 인구수가 다른 연령층에 비해 많기 때문에 확진이 많이 된것은 아니다.\n\nconfirmed_by_population = age_order.sort_values('age') #'age'라는 컬럼으로 정렬\nconfirmed_by_population['confirmed'] = list(timeage[-9:].confirmed) #confirmed라는 컬럼을 만들고 timeage의 해당 리스트를 할당 시킴\n\n\nconfirmed_by_population['confirmed_ratio'] = confirmed_by_population['confirmed']/confirmed_by_population['population'] *100 #인구비율에 따른 확진 비율 컬럼 추가\ndisplay(confirmed_by_population)\n\n\n\n\n\n\n\n\nage\npopulation\nproportion\nconfirmed\nconfirmed_ratio\n\n\n\n\n1\n0s\n4054901\n7.86\n193\n0.004760\n\n\n2\n10s\n4769187\n9.24\n708\n0.014845\n\n\n3\n20s\n7037893\n13.64\n3362\n0.047770\n\n\n4\n30s\n7174782\n13.90\n1496\n0.020851\n\n\n5\n40s\n8257903\n16.00\n1681\n0.020356\n\n\n6\n50s\n8575336\n16.62\n2286\n0.026658\n\n\n7\n60s\n6476602\n12.55\n1668\n0.025754\n\n\n8\n70s\n3598811\n6.97\n850\n0.023619\n\n\n9\n80s\n1657942\n3.21\n556\n0.033536\n\n\n\n\n\n\n\n\n## 3. Plot confirmed rate by age\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Population-adjusted Confirmed Rate by Age', fontsize=17)\nsns.barplot(age_list, confirmed_by_population.confirmed_ratio[-9:])\nax.set_xlabel('Age', size=13)\nax.set_ylabel('Confirmed rate (%)', size=13)\nplt.show() #인구 비율에 따른 확진 확률\n\nc:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning:\n\nPass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n\n\n\n\n\n\n인구 비율에 따른 확진자의 수를 보아도 20대가 인구수가 많은 연령대인 40,50대 보다도 확연하게 많은 것을 알 수 있으며 오히려 80대 이상의 연령대가 차지하는 비율이 증가하였다.\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(23, 7)) #1행 2열의 도화지를 생성\n\n## 1. Confirmed Cases by Age\nax[0].set_title('Confirmed Cases by Age', fontsize=15)\nax[0].bar(age_list, confirmed_by_population.confirmed)\n\n## 2. Population-adjusted Confirmed Rate\nax[1].set_title('Population-adjusted Confirmed Rate', fontsize=15)\nax[1].bar(age_list, confirmed_by_population.confirmed_ratio)\n\nplt.show() \n\n\n\n\n다음 두개의 플랏을 보면 앞선 그래프에서는 20대의 확진 확률이 다른 연령대에 비해서 앞도적으로 높았지만 인구의 비율에 따른 확진의 비율을 나타내는 두번째 플랏을 보면 아직도 20대가 다른 연령대에 비해서 높기는 하지만 첫번째 그래프에 비해서는 조금은 낮아진 모습과 60~80대까지 연령층의 비중이 조금은 증가 했다는 사실을 두개의 플랏을 비교하면서 알 수있습니다 따라서 저는 다른 연령에 비해 압도적으로 많은 확진 비율을 가지고 있는 20대에 대해서 집중적으로 분석을 해보도록 하겠습니다.\n\n\n주제1 - 연령대별 확진 경로 \n\npatientinfo.head()\n\n\n\n\n\n\n\n\npatient_id\nsex\nage\ncountry\nprovince\ncity\ninfection_case\ninfected_by\ncontact_number\nsymptom_onset_date\nconfirmed_date\nreleased_date\ndeceased_date\nstate\n\n\n\n\n0\n1000000001\nmale\n50s\nKorea\nSeoul\nGangseo-gu\noverseas inflow\nNaN\n75\n2020-01-22\n2020-01-23\n2020-02-05\nNaN\nreleased\n\n\n1\n1000000002\nmale\n30s\nKorea\nSeoul\nJungnang-gu\noverseas inflow\nNaN\n31\nNaN\n2020-01-30\n2020-03-02\nNaN\nreleased\n\n\n2\n1000000003\nmale\n50s\nKorea\nSeoul\nJongno-gu\ncontact with patient\n2002000001\n17\nNaN\n2020-01-30\n2020-02-19\nNaN\nreleased\n\n\n3\n1000000004\nmale\n20s\nKorea\nSeoul\nMapo-gu\noverseas inflow\nNaN\n9\n2020-01-26\n2020-01-30\n2020-02-15\nNaN\nreleased\n\n\n4\n1000000005\nfemale\n20s\nKorea\nSeoul\nSeongbuk-gu\ncontact with patient\n1000000002\n2\nNaN\n2020-01-31\n2020-02-24\nNaN\nreleased\n\n\n\n\n\n\n\n\npatientinfo['infection_case'] = patientinfo['infection_case'].astype(str).apply(lambda x:x.split()[0])\n#PatientInfo['infection_case']\ninfectionCase = patientinfo.pivot_table(index='infection_case',columns='age',\nvalues='patient_id',aggfunc=\"count\")\n#infectionCase\n#전체 감염 케이스\npatientTotal = infectionCase.fillna(0).sum(axis=1)\npatientTotal = patientTotal.sort_values(ascending = False)[:5]\n# 20대 감염 케이스\npatient20s = infectionCase['20s'].dropna()\npatient20sTop = patient20s.sort_values(ascending=False)[:5]\n\n\ndisplay(patientTotal)\ndisplay(patient20sTop)\n\ninfection_case\ncontact     1112.0\nnan          827.0\noverseas     653.0\netc          638.0\nGuro-gu      112.0\ndtype: float64\n\n\ninfection_case\noverseas       269.0\nnan            221.0\ncontact        172.0\netc            127.0\nShincheonji     41.0\nName: 20s, dtype: float64\n\n\n\nfig = go.Figure()\nfig.add_trace(go.Pie(labels=patientTotal.index, values=patientTotal.values,\n                     textinfo='label+percent'))\n\nfig.update_layout(title='Confirmed infection case Total AGe')\n\nfig.show()\n\n\n                                                \n\n\n전체 연령측의 감염원인에 대해서 본다면 접촉에 의한 확진이 33% 해외 입국이 19% 그외 nan과 etc가 각각 24,19%의 비율을 차지하고 있다\n\nfig = go.Figure()\nfig.add_trace(go.Pie(labels=patient20sTop.index, values=patient20sTop.values,\n                     textinfo='label+percent'))\n\nfig.update_layout(title='Confirmed infection case 20s AGe')\n\nfig.show()\n\n\n                                                \n\n\n20대 연령의 그룹은 전체연령에 비해 해외입국과 nan이 각각 32 26%를 차지 하고 있다. 그러나 해당 데이터에는 원인을 알수 없는 nan데이가 전체의 1/4가량을 차지 하기 때문에 정확한 분석을 하기 어렵다\n그렇다면 확진된 20대가 많이 돌아다닌 장소에 대해서 patientinfo 데이터를 이용해서 찾아보겠습니다\n\npatientroute = pd.read_csv(path+'PatientRoute.csv')\n\n\npatientroute.head()\n\n\n\n\n\n\n\n\npatient_id\nglobal_num\ndate\nprovince\ncity\ntype\nlatitude\nlongitude\n\n\n\n\n0\n1000000001\n2.0\n2020-01-22\nGyeonggi-do\nGimpo-si\nairport\n37.615246\n126.715632\n\n\n1\n1000000001\n2.0\n2020-01-24\nSeoul\nJung-gu\nhospital\n37.567241\n127.005659\n\n\n2\n1000000002\n5.0\n2020-01-25\nSeoul\nSeongbuk-gu\netc\n37.592560\n127.017048\n\n\n3\n1000000002\n5.0\n2020-01-26\nSeoul\nSeongbuk-gu\nstore\n37.591810\n127.016822\n\n\n4\n1000000002\n5.0\n2020-01-26\nSeoul\nSeongdong-gu\npublic_transportation\n37.563992\n127.029534\n\n\n\n\n\n\n\n\npatientroute[['patient_id','date','type']] #필요한 컬럼만 선택\n\n\n\n\n\n\n\n\npatient_id\ndate\ntype\n\n\n\n\n0\n1000000001\n2020-01-22\nairport\n\n\n1\n1000000001\n2020-01-24\nhospital\n\n\n2\n1000000002\n2020-01-25\netc\n\n\n3\n1000000002\n2020-01-26\nstore\n\n\n4\n1000000002\n2020-01-26\npublic_transportation\n\n\n...\n...\n...\n...\n\n\n6709\n6100000090\n2020-03-24\nairport\n\n\n6710\n6100000090\n2020-03-24\nairport\n\n\n6711\n6100000090\n2020-03-25\nstore\n\n\n6712\n6100000090\n2020-03-25\nhospital\n\n\n6713\n6100000090\n2020-03-25\nstore\n\n\n\n\n6714 rows × 3 columns\n\n\n\n\nplaces = patientroute.type.unique()\nsimproute = patientroute[['patient_id','date','type']]\nagedf = patientinfo[['patient_id','age']]\nsimproutewage = pd.merge(simproute,agedf,how='left')\nfiplot = simproutewage.set_index('type')\nfiplot_count = fiplot.groupby('type').count().patient_id.sort_values()\n\n\nfig = fiplot_count.iplot(asFigure = True, kind='bar')\nfig.show()\n\n\n                                                \n\n\n해당 그래프는 전체 연령의 확진 원인에 대한 것을 카운트 시킨 결과로 etc와 hospital이 가장 많은 것을 보이나 병원은 확진자가 이상증세를 느끼고 찾아가는 당연한 경로이므로 제외를 하고 etc 또한 어느 곳에 다녀왔는지 정확하게 알 수 없어서 제외를 하고 다시 진행을 해보겠습니다.\n\ntwtfi = fiplot[fiplot.age == '20s']\nuntwtfi = fiplot[fiplot.age != '20s']\ntwt = twtfi.groupby('type').count().patient_id\nuntwt = untwtfi.groupby('type').count().patient_id\ntwt = twt[~twt.index.isin( ['etc','hospital'])]\nuntwt = untwt[~untwt.index.isin( ['etc','hospital'])]\nfig = go.Figure()\nfig.add_trace(go.Bar(x = twt.index, \n                     y = twt,\n                     name = '20s',\n                     marker_color='indianred'))\nfig.add_trace(go.Bar(x = untwt.index, \n                     y = untwt,\n                     name = 'except 20s',\n                     marker_color='lightsalmon'))\nfig.update_layout(barmode='group', xaxis_tickangle=-45)\nfig.show()\n\n\n                                                \n\n\n다음 그래프는 20대와 그외의 연령층이 확진된 원인에 대한것으로 앞서 말한것 처럼 etc와 hospital은 가장 많은 비율을 차지 하지만 분석을 하는데 크게 도움이 되지 않는다고 판단을 하여 제외를 하고 진행을 한 결과 이다.\n공통적으로 많이 방문하는 store과 church는 비슷한 양상을 보여주고 있습니다. 하지만, 20대는 restaurant, pc방, cafe, bar 등에서 훨씬 많은 방문비율을 확인할 수 있었습니다.\n\n\n\n주제2 - 코로나는 누구에게 가장 치명적인가 \n\n주제2 - 연령별로 확진자의 치명률 \n\ntime.head()\n\n\n\n\n\n\n\n\ndate\ntime\ntest\nnegative\nconfirmed\nreleased\ndeceased\n\n\n\n\n0\n2020-01-20\n16\n1\n0\n1\n0\n0\n\n\n1\n2020-01-21\n16\n1\n0\n1\n0\n0\n\n\n2\n2020-01-22\n16\n4\n3\n1\n0\n0\n\n\n3\n2020-01-23\n16\n22\n21\n1\n0\n0\n\n\n4\n2020-01-24\n16\n27\n25\n2\n0\n0\n\n\n\n\n\n\n\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=time['date'], y=time['deceased'],\n                    mode='lines', # Line plot만 그리기\n                    name='사망(deceased)'))\n\nfig.update_layout(title='시간의 흐름에 따른 확진자 사망 추이',\n                   xaxis_title='Date',\n                   yaxis_title='Number')\n\nfig.show()\n\n\n                                                \n\n\n해당 그래프는 시간의 흐름에 따른 확진자의 사망 추이로서 2020년 3월 부터 계속해서 우상향하는 모습을 볼 수 있다\n\nfig, ax = plt.subplots(figsize = (13,7)) #도화지(Figure : fig)를 깔고 그래프를 그릴 구역(Axes : ax)을 정의합니다. figsize를 통해서 도화지의 크기를 지정해준다\n#이는 objection oriented API으로 그래프의 각 부분을 객체로 지정하고 그리는 유형이다\nsns.barplot(age_list,timeage.deceased[-9:])\nax.set_xlabel('age',size=13) #연령\nax.set_ylabel('number of case',size=13) #케이스의 횟수\nplt.title('Deceased Cases by Age')\nplt.show()\n\nc:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning:\n\nPass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n\n\n\n\n\n\n나이가 많아질수록 사망자의 비율이 높다 과연 나이가 많아질수록 인구수가 많아져서 이러한 현상이 나오는 지 인구 비율에 따른 사망자에 대해서 다시 한번 살펴보자\n\nconfirmed_by_population = age_order.sort_values('age')\nconfirmed_by_population['deceased'] = list(timeage[-9:].deceased)\n\n# 2. Get confirmed ratio regarding population\nconfirmed_by_population['deceased_ratio'] = confirmed_by_population['deceased']/confirmed_by_population['population'] *100\ndisplay(confirmed_by_population)\n\n\n\n\n\n\n\n\nage\npopulation\nproportion\ndeceased\ndeceased_ratio\n\n\n\n\n1\n0s\n4054901\n7.86\n0\n0.000000\n\n\n2\n10s\n4769187\n9.24\n0\n0.000000\n\n\n3\n20s\n7037893\n13.64\n0\n0.000000\n\n\n4\n30s\n7174782\n13.90\n2\n0.000028\n\n\n5\n40s\n8257903\n16.00\n3\n0.000036\n\n\n6\n50s\n8575336\n16.62\n15\n0.000175\n\n\n7\n60s\n6476602\n12.55\n41\n0.000633\n\n\n8\n70s\n3598811\n6.97\n82\n0.002279\n\n\n9\n80s\n1657942\n3.21\n139\n0.008384\n\n\n\n\n\n\n\n\n## 3. Plot confirmed rate by age\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Population-adjusted Deceased Rate by Age', fontsize=17)\nsns.barplot(age_list, confirmed_by_population.deceased_ratio[-9:])\nax.set_xlabel('Age', size=13)\nax.set_ylabel('Deceased rate (%)', size=13)\nplt.show() #인구 비율에 따른 확진 확률\n\nc:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning:\n\nPass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n\n\n\n\n\n\n인구 비율에 따른 사망자의 비율을 살펴본 결과이다 0~20대 까지는 사망자는 없으며 30대부터 확진으로 인한 사망자가 존재한다. 그러나 80대 이상의 연령층의 경우는 가장 많은 인구수를 가진 연령층도 아니지만 사망자의 비중이 가장 높은 것을 볼 수 있다. 이로 코로나 바이러스는 고연령층 일수록 가장 치명적인 질병임을 예측할 수 있다.\n그렇다면 고연령층의 확진 원인에 대해서 알아보자\n\n\n주제2 - 연령대의 확진 원인 \n\naged_pat = patientinfo[(patientinfo['age'] == '60s')|(patientinfo['age'] == '70s')|\n                (patientinfo['age'] == '80s')][['province','age','infection_case']]\n                \naged_inf = pd.DataFrame(aged_pat['infection_case'].value_counts())\n#고연령측의 확진 원인\n\n\npatientinfo['infection_case'] = patientinfo['infection_case'].astype(str).apply(lambda x:x.split()[0])\n#PatientInfo['infection_case']\ninfectionCase = patientinfo.pivot_table(index='infection_case',columns='age',\nvalues='patient_id',aggfunc=\"count\")\n#infectionCase\n#전체 감염 케이스\npatientTotal = infectionCase.fillna(0).sum(axis=1)\npatientTotal = patientTotal.sort_values(ascending = False)[:5]\n# 60대 감염 케이스\npatient60s = infectionCase['60s'].dropna()\npatient60sTop = patient60s.sort_values(ascending=False)[:5]\n# 70대 감염 케이스\npatient70s = infectionCase['70s'].dropna()\npatient70sTop = patient70s.sort_values(ascending=False)[:5]\n# 80대 감염 케이스\npatient80s = infectionCase['80s'].dropna()\npatient80sTop = patient80s.sort_values(ascending=False)[:5]\n\n\nfig = go.Figure()\nfig.add_trace(go.Pie(labels=patient60sTop.index, values=patient60sTop.values,\n                     textinfo='label+percent'))\n\nfig.update_layout(title='Confirmed infection case 60s AGe')\n\nfig.show()\n\n\n                                                \n\n\n\nfig = go.Figure()\nfig.add_trace(go.Pie(labels=patient70sTop.index, values=patient70sTop.values,\n                     textinfo='label+percent'))\n\nfig.update_layout(title='Confirmed infection case 70s AGe')\n\nfig.show()\n\n\n                                                \n\n\n\nfig = go.Figure()\nfig.add_trace(go.Pie(labels=patient80sTop.index, values=patient80sTop.values,\n                     textinfo='label+percent'))\n\nfig.update_layout(title='Confirmed infection case 80s AGe')\n\nfig.show()\n\n\n                                                \n\n\n60,70,80대의 확진 원인을 보니 nan과 etc가 많기는 하지만 다른 연령대에 비해서 contact가 많다는 사실을 알수 있다. 그래도 nan과 etc가 많아 확진의 주요 원인을 접촉에 의해서 라고 단정할 수는 없다\n그렇다면 나이가 많은 사람들은 완치 기간이 길어서 치명률이 높은 것이지 이에 대한 관계에 대해 알아보았습니다\n\n\n주제2 - 연령대별 회복기간 \n\npatientinfo\n\n\n\n\n\n\n\n\npatient_id\nsex\nage\ncountry\nprovince\ncity\ninfection_case\ninfected_by\ncontact_number\nsymptom_onset_date\nconfirmed_date\nreleased_date\ndeceased_date\nstate\n\n\n\n\n0\n1000000001\nmale\n50s\nKorea\nSeoul\nGangseo-gu\noverseas\nNaN\n75\n2020-01-22\n2020-01-23\n2020-02-05\nNaN\nreleased\n\n\n1\n1000000002\nmale\n30s\nKorea\nSeoul\nJungnang-gu\noverseas\nNaN\n31\nNaN\n2020-01-30\n2020-03-02\nNaN\nreleased\n\n\n2\n1000000003\nmale\n50s\nKorea\nSeoul\nJongno-gu\ncontact\n2002000001\n17\nNaN\n2020-01-30\n2020-02-19\nNaN\nreleased\n\n\n3\n1000000004\nmale\n20s\nKorea\nSeoul\nMapo-gu\noverseas\nNaN\n9\n2020-01-26\n2020-01-30\n2020-02-15\nNaN\nreleased\n\n\n4\n1000000005\nfemale\n20s\nKorea\nSeoul\nSeongbuk-gu\ncontact\n1000000002\n2\nNaN\n2020-01-31\n2020-02-24\nNaN\nreleased\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5160\n7000000015\nfemale\n30s\nKorea\nJeju-do\nJeju-do\noverseas\nNaN\n25\nNaN\n2020-05-30\n2020-06-13\nNaN\nreleased\n\n\n5161\n7000000016\nNaN\nNaN\nKorea\nJeju-do\nJeju-do\noverseas\nNaN\nNaN\nNaN\n2020-06-16\n2020-06-24\nNaN\nreleased\n\n\n5162\n7000000017\nNaN\nNaN\nBangladesh\nJeju-do\nJeju-do\noverseas\nNaN\n72\nNaN\n2020-06-18\nNaN\nNaN\nisolated\n\n\n5163\n7000000018\nNaN\nNaN\nBangladesh\nJeju-do\nJeju-do\noverseas\nNaN\nNaN\nNaN\n2020-06-18\nNaN\nNaN\nisolated\n\n\n5164\n7000000019\nNaN\nNaN\nBangladesh\nJeju-do\nJeju-do\noverseas\nNaN\nNaN\nNaN\n2020-06-18\nNaN\nNaN\nisolated\n\n\n\n\n5165 rows × 14 columns\n\n\n\n\nfrom datetime import datetime\n\n\npat_rel = patientinfo[['age','confirmed_date','released_date']]\n#pat_rel['diff'] = pat_rel.released_date - pat_rel.confirmed_date\nstr(pat_rel.confirmed_date)\npat_rel.released_date = pd.to_datetime(pat_rel['released_date'], format='%Y %m %d')\npat_rel.confirmed_date = pd.to_datetime(pat_rel['confirmed_date'], format='%Y %m %d')\npat_rel['diff'] = pat_rel.released_date - pat_rel.confirmed_date\npat_rel = pat_rel[:5161] #격리 날짜 없는 것 삭제\ndisplay(pat_rel)\n\n\n\n\n\n\n\n\nage\nconfirmed_date\nreleased_date\ndiff\n\n\n\n\n0\n50s\n2020-01-23\n2020-02-05\n13 days\n\n\n1\n30s\n2020-01-30\n2020-03-02\n32 days\n\n\n2\n50s\n2020-01-30\n2020-02-19\n20 days\n\n\n3\n20s\n2020-01-30\n2020-02-15\n16 days\n\n\n4\n20s\n2020-01-31\n2020-02-24\n24 days\n\n\n...\n...\n...\n...\n...\n\n\n5156\n30s\n2020-04-03\n2020-05-19\n46 days\n\n\n5157\n20s\n2020-04-03\n2020-05-05\n32 days\n\n\n5158\n10s\n2020-04-14\n2020-04-26\n12 days\n\n\n5159\n30s\n2020-05-09\n2020-06-12\n34 days\n\n\n5160\n30s\n2020-05-30\n2020-06-13\n14 days\n\n\n\n\n5161 rows × 4 columns\n\n\n\n\ndisplay(pat_rel['diff'].mean()) \ndisplay(pat_rel['diff'].min()) \ndisplay(pat_rel['diff'].max()) \n\nTimedelta('24 days 17:48:39.041614123')\n\n\nTimedelta('0 days 00:00:00')\n\n\nTimedelta('114 days 00:00:00')\n\n\n평균 완치일은 24일\n최대 완치일은 114일 입니다.\n\npat_rel['over_avg'] = np.where(pat_rel['diff']&gt;'24 days 17:48:39.041614123',1,0)\nover_av_released = pat_rel[pat_rel['over_avg']==1]\nunder_av_released = pat_rel[pat_rel['over_avg']==0]\n\nover_av=pd.DataFrame(over_av_released['age'].value_counts().sort_index()).reset_index()\nunder_av=pd.DataFrame(under_av_released['age'].value_counts().sort_index()).reset_index()\n\n#연령대층별로 감염자수가 확연히 다르기때문에 각 연령층별의 비율로 계산\nunder_av['per']=under_av['age']/(under_av['age']+over_av['age']) \nover_av['per']=over_av['age']/(under_av['age']+over_av['age'])\n\n#컬럼 재정리\nunder_av.columns=['age', 'count', 'under_per']\nover_av.columns=['age', 'count', 'over_per']\n\n\nover_av = pd.DataFrame({'age':['0s','10s','20s','30s','40s','50s','60s','70s','80s','90s'],\n                             'count':[7,20,156,90,86,119,90,46,39,8],\n                             'over_per':[0.106061,0.006289,0.026212,0.264856,0.172414,0.135647,0.232877,0.326087,0.2598887,0.487500]})\n\n\nfig = go.Figure()\n\nfig.add_trace(go.Bar(x=under_av.under_per, y=under_av.age, name='빠른 완치 기간',\n                     orientation='h'))\n\nfig.add_trace(go.Bar(x=over_av.over_per, y=over_av.age, name='오랜 완치 기간',\n                     text=over_av.over_per, texttemplate='%{x:.1%}', textposition='inside',\n                    textfont=dict(color='white'),\n                    orientation='h'))\nfig.update_layout(barmode='stack',\n                  paper_bgcolor='rgb(248, 248, 255)',\n                  plot_bgcolor='rgb(248, 248, 255)',\n                 )\nfig.update_layout(title='연령대에 따른 회복 기간')\n\nfig.show()\n\n\n                                                \n\n\n그래프를 본다면 0~20대의 연령츠은 10% 미만으로 평균보다 빠른 완치 기간을 가지고 있음을 알 수 있습니다. 고연령층인 70,80,90대의 경우 32,26,48%로 다른 연령층에 비해서 높기는 하지만 과반을 넘지 않기에 고연령층이라고 모두가 장기간의 회복 기간을 가진다고 판단하기 어렵습니다.\n\n\n\n주제3 - 정부의 정책은 타당했는가? \n\n주제3 - 감염병 경보 단계 공표 시점 \n\npolicy.head()\n\n\n\n\n\n\n\n\npolicy_id\ncountry\ntype\ngov_policy\ndetail\nstart_date\nend_date\n\n\n\n\n0\n1\nKorea\nAlert\nInfectious Disease Alert Level\nLevel 1 (Blue)\n2020-01-03\n2020-01-19\n\n\n1\n2\nKorea\nAlert\nInfectious Disease Alert Level\nLevel 2 (Yellow)\n2020-01-20\n2020-01-27\n\n\n2\n3\nKorea\nAlert\nInfectious Disease Alert Level\nLevel 3 (Orange)\n2020-01-28\n2020-02-22\n\n\n3\n4\nKorea\nAlert\nInfectious Disease Alert Level\nLevel 4 (Red)\n2020-02-23\nNaN\n\n\n4\n5\nKorea\nImmigration\nSpecial Immigration Procedure\nfrom China\n2020-02-04\nNaN\n\n\n\n\n\n\n\n\npolicy.isna().sum() #여기서 end_date의 NA값이 너무 많은 것을 알 수 있다. 따라서 기점을 start_date로 지정하고 사용을 하겠다.\n\npolicy_id      0\ncountry        0\ntype           0\ngov_policy     0\ndetail         2\nstart_date     0\nend_date      37\ndtype: int64\n\n\n\npolicy_alerts = policy[policy.type == 'Alert']\ndisplay(policy_alerts)\n\n\n\n\n\n\n\n\npolicy_id\ncountry\ntype\ngov_policy\ndetail\nstart_date\nend_date\n\n\n\n\n0\n1\nKorea\nAlert\nInfectious Disease Alert Level\nLevel 1 (Blue)\n2020-01-03\n2020-01-19\n\n\n1\n2\nKorea\nAlert\nInfectious Disease Alert Level\nLevel 2 (Yellow)\n2020-01-20\n2020-01-27\n\n\n2\n3\nKorea\nAlert\nInfectious Disease Alert Level\nLevel 3 (Orange)\n2020-01-28\n2020-02-22\n\n\n3\n4\nKorea\nAlert\nInfectious Disease Alert Level\nLevel 4 (Red)\n2020-02-23\nNaN\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(13,7))\nplt.title(\"Infection Disease Alert Level\",size=17)\nplt.plot(time.date.unique(), time.confirmed.diff(),\n         color = 'gray', lw = 3)\nax.set_xticks(ax.get_xticks()[::int(len(time.date.unique())/8)])\n\nfor day, color in zip(policy_alerts.start_date.values[1:], ['yellow','orange','red']):\n    ax.axvline(day, ls=':', color=color, lw=4)\nax.legend(['confirmed','alert level 2','alert level 3','alert level 4'])\nplt.xlabel('date')\nplt.ylabel('Number of Confirmed')\nplt.show()\n\n\n\n\n다음은 감염병의 경보 단계 별로 해당 시점과 확진자의 일일 추이를 나타낸 것으로 2,3단계는 발생을 하고 국내에 들어온 시점에 공표가 되었고 가장 강력한 단계인 4단계는 일일 확진자가 정점에 이르기 전에 공표가 된 사실을 알수 있습니다.\n그렇다면 정부의 거리두기는 어떠하였는지 알아보겠습니다.\n\n\n주제3 - 정부의 거리두기 공표 시점 \n\npolicy_social = policy[policy.type == 'Social'][:-1]\ndisplay(policy_social)\n\n\n\n\n\n\n\n\npolicy_id\ncountry\ntype\ngov_policy\ndetail\nstart_date\nend_date\n\n\n\n\n28\n29\nKorea\nSocial\nSocial Distancing Campaign\nStrong\n2020-02-29\n2020-03-21\n\n\n29\n30\nKorea\nSocial\nSocial Distancing Campaign\nStrong\n2020-03-22\n2020-04-19\n\n\n30\n31\nKorea\nSocial\nSocial Distancing Campaign\nWeak\n2020-04-20\n2020-05-05\n\n\n31\n32\nKorea\nSocial\nSocial Distancing Campaign\nWeak(1st)\n2020-05-06\nNaN\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(13,7))\nplt.title(\"Social Distancing Campaign\",size=17)\nplt.plot(time.date.unique(), time.confirmed.diff(),\n         color = 'gray', lw = 3)\nax.set_xticks(ax.get_xticks()[::int(len(time.date.unique())/8)])\n\nfor day, color in zip(policy_social.start_date.values[:], ['red','red','orange']):\n    ax.axvline(day, ls=':', color=color, lw=4)\nax.legend(['confirmed','strong','strong','weak'])\nplt.xlabel('date')\nplt.ylabel('Number of Confirmed')\nplt.show()\n\n\n\n\n3월 23일에 감염병 경고가 4단계 발표 되고 나서 3월 29일부터 사회적 거리두기를 진행 하였습니다. 두번의 강한 거리두기를 유지하고 4월 20일부터 거리두기의 단계를 낮췄습니다. 거리두기를 확진자의 정점에 다다르는 시점에서 발표하고 그 이후 감소하는 일일 확진자의 수를 확인할 수 있다 따라서 분석을 하고 있는 해당 기간 동안은 정부의 방역 정책은 성공을 했다고 할 수 있다\n\n\n\n\n결론 \n\n20대가 분석 기간동안 가장 많이 확진이 된 연령층이었으며, 20대의 인구수가 다른 연령층에 비해서 많아서가 아닌 다른 연령층에 비해서 활동 반경이 넓고 활발하며 불필요한 방문 지역과 장소에 자주 방문을 하였기 때문에 20대가 가장 많이 확진된 연령층이라는 결론을 내렸습니다.\n코로나 바이러스는 고연령층이 될수록 더욱 치명적이라는 사실을 얻었습니다. 치명률이 인구 대비 비율로 살펴보아도 80대 이상이 가장 압도적으로 많았고 60,70대 또한 적지 않은 치명률을 가지고 있음을 알게 되었으며, 완치까지 걸리는 기간은 고연령층일수록 저연령층에 비해서 평균 이상으로 오래 걸리기는 하였으나 과반이상이거나 다른 연령층에 비해서 조금은 많치만 크게 상관성이 있어보지는 않았습니다.\n정부의 거리두기는 당시 신천지로 인해서 일일 확진자가 800명 가량 나오던 시점에 공표되고 그 이후로 분석기간 동안에는 감소세를 보였습니다. 이로 정부의 거리두기는 적어도 당시에는 타당했다라는 결론을 내리게 되었습니다.\n\n코로나 종식 및 예방을 위해서는 해외 유입에 의한 확진자를 차단해야합니다. 현재 입국자에 대한 검사 및 2주 자가격리 등 많은 노력이 진행되고 있습니다. 하지만, 그럼에도 유의사항을 잘 따르지 않는 일부 인원에 의해서 신천지와 같은 큰 집단 감염이 발생될 수 있다는 사실을 잊지 말아야합니다. 따라서, 코로나에 대한 경각심과 인식을 잘 심어주어야하며, 특히나 가장 안일하게 생각하는 20대의 인식 변화를 이끌어야 할 것입니다. 또한, 20대의 행동 패턴 및 방문 경로를 바탕으로 감염 위험이 있는 업종은 특히나 더욱 신경써서 사회적 거리두기, 마스크 착용, 손세정제와 손씻기 등을 더욱 권장하도록 해야합니다.\n분석을 하면서 느낀 한계점은 자료에 etc나 NaN으로 표시된 자료의 형태가 많아서 정확한 원인들을 찾기에 어려움이 있었습니다.\n자료 출처 및 참고 출처 - https://dacon.io/competitions/official/235590/overview/description (데이콘 - 코로나 데이터 시각화 AI 경진대회) - https://www.kaggle.com/datasets/kimjihoo/coronavirusdataset (kaggle - Data Science for COVID-19 in South Korea) - https://chancoding.tistory.com/119 (plotly line plot) - https://plotly.com/python/pie-charts/(about pie plot)"
  },
  {
    "objectID": "Data_Visualization/chp5.html",
    "href": "Data_Visualization/chp5.html",
    "title": "ggplot2 Tutorial",
    "section": "",
    "text": "ggplot2 실습\n\n\n\n데이터 시각화란 데이터를 그래프 등의 시각적 요소로 요약하여 보여주는 것을 의미한다. R에서는 데이터 시각화를 R의 기본 기능에 포함된 graphics 패키지를 사용하여 시각화하는 방법과 ggplot2패키지를 이용하는 방법이 있다. 이 장에서는 ggplot2를 이용하여 데이터를 시각화하는 기본적인 방법을 배운다. 여기서는 통계분석에 필요한 기본적인 그래프를 그리기 위한 기본적인 문법을 소개하는 것이지 ggplot2에 대한 체계적인 설명을 하지 않을 것이다. ggplot2는 자유로운 형식으로 그래프를 그릴 수 있는 그래프 문법을 가지고 있기 때문에, ggplot2에 대한 더 체계적인 이해를 원하는 독자는 졸저 ’R 프로그래밍’의 ggplot2를 이용한 데이터 시각화를 참조하기 바란다.\n\n\nR은 패키지란 단위로 R에서 사용할 수 있는 기능을 제공한다. R을 설치하면 base, stat, dataset, graphics 등의 기본 패키지가 자동으로 설치되고, R을 시작할 때마다 이러한 기본 패키지가 자동으로 적대되어 사용될 수 있도록 준비된다. 만약 R에서 기본으로 제공하는 패키지 말고 다른 패키지를 사용하려면 그 패키지를 R에 설치해야 한다. ggplot2 패키지는 기본 기능에 포함되지 않으므로 먼저 설치를 해야 한다.\nggplot2 패키지를 설치하려면 다음 명령을 실행하면 된다. 패키지의 이름은 문자열이므로 따옴표 안에 기술해야 한다.\n\n# install.packages('ggplot2')\n\n또는 RStudio의 우측 하단의 Packages 탭에서 [Install]을 클릭한 후 ggplot2라고 입력을 하면 된다. 패키지 설치는 한 번만 수행하면 된다.\n패키지를 사용하려면 메모리에 적재를 하여야 한다. 패키지를 메모리에 적재하는 것은 library() 함수를 사용한다. 이 때 주의할 점은 이미 설치된 패키지를 지정할 때는 따옴표 없이 변수처럼 패키지를 기술해야 한다는 것이다.\n\nlibrary(ggplot2)\n\n패키지의 설치는 한 번만 수행하면 되지만, 패키지를 메모리에 적재하는 작업을 패키지를 사용할 때마다 수행하여야 한다. 한번 메모리에 적재된 패키지는 R 세션이 종료되기 전까지 유지된다. 그러므로 하나의 R 세션에서는 다시 library() 함수로 동일한 패키지를 적재하지 않아도 된다. 그러나 R 세션을 종료하고 다시 시작하였다면, 기본 패키지가 아니면 자동 적재되지 않으므로 사용하기 전에 패키지를 다시 적재하여야 한다.\n\n\n\n이 절에서는 ggplot2에서 제공하는 mpg 데이터를 이용하여 ‘배기량이 커지면 연비가 낮아지는가?’ 라는 물음을 그래프를 이용하여 탐색해 보자. mpg는 1999년과 2008년에 미국 EPA에서 조사하여 발표한 자동차 주요 모델별 연비 데이터이다.\n다음 명령을 이용하여 mpg 데이터를 출력해 보자. mpg 데이터는 tibble이라는 데이터 프레임의 일종으로, 사용자의 화면의 크기에 따라 출력 내용을 조정한다. 그러므로 화면의 크기에 따라 출력되는 내용이 책과는 조금 다를 수 있다.\nYou can also embed plots, for example:\n\nmpg\n#&gt; # A tibble: 234 × 11\n#&gt;    manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n#&gt;    &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n#&gt;  1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n#&gt;  2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n#&gt;  3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n#&gt;  4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n#&gt;  5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n#&gt;  6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n#&gt;  7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n#&gt;  8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n#&gt;  9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n#&gt; 10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n#&gt; # ℹ 224 more rows\n\nmpg는 1999년과 2008년에 미국 EPA에서 조사하여 발표한 자동차 주요 모델별 연비 데이터이다. 데이터는 234 개의 행이 있으며, 각 행은 다음과 같은 변수로 구성되어 있다.\n\nmanufacturer : 자동차 제조사\nmodel : 자동차 모델명\ndispl : 자동차 배기량\nyear : 제조년도\ncyl : 엔진 실린더 수\ntrans : 자동차 트랜스미션 종류\ndrv : 자동차 구동 방식. f = 전륜구동, r = 후륜구동, 4 = 사륜구동\ncty : 도심 연비 (마일/갤론)\nhwy : 고속도로 연비 (마일/갤론)\nfl : 연료 종류\nclass : 자동차 분류\nmpg 데이터에 대한 더 자세한 설명은 콘솔에 다음을 입력하여 R 도움말을 참조하기 바란다.\n\n\n?mpg\n\n\n\nmpg 데이터로부터 배기량과 고속도로 연비의 관계를 살펴보기 위해서 배기량(displ)을 x 축으로, 고속도로 연비(hwy)를 y 축으로 하는 산점도를 그려보자. 산점도에서 배기량이 커짐지면 연비가 줄어드는 경향을 관찰할 수 있다.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point()\n\n\n\n\n그러면 이 산점도 그린 ggplot2 명령어의 문법을 살펴보자.\n\nggplot2의 명령어는 항상 ggplot() 함수로 시작하고, + 연산자를 사용하여 그래프에 추가될 요소를 덧붙여 나간다. 이렇게 함수를 +로 연결하여 사용하는 방식은 ggplot2 패키지의 독특한 문법으로 대부분의 다른 R 명령어에서는 이러한 방식을 사용하지 않는다.\nggplot() 함수는 그래프의 좌표축과 좌표평면을 만드는 함수이다. 그러므로 다음처럼 ggplot() 함수만 사용하고 그래프에 추가할 요소를 지정하지 않으면 좌표축과 좌표평면만 그린다.\n\n\nggplot(mpg, aes(x = displ, y = hwy))\n\n\n\n\n\nggplot() 함수의 첫번째 인수는 그래프를 그릴 때 사용할 데이터를 지정하고, 두번째 인수는 그래프 속성과 데이터 열의 관계를 지정한다. 그래프 속성과 데이터 열의 관계는 항상 aes() 함수 내에 기술되고, 다음처럼 &lt;그래프 속성&gt;=&lt;데이터 열&gt;의 형식으로 기술된다.\n\n\nggplot(데이터, aes(속성1=열1, 속성2=열2, ...)) + geom함수()\n\n앞의 산점도에서는 x라는 그래프의 가로축 속성에 mpg 데이터의 배기량 열 displ이 매핑되었고, y라는 그래프의 세로축 속성에 고속도로 연비 열 hwy가 매핑되었다. 다음은 그래프의 가로축에 데이터의 도심 연비 열인 cty을 매핑하여 산점도를 그린 예이다. 도심 연비가 좋은 차가 고속도로 연비도 좋다는 것을 알 수 있다.\n\nggplot(mpg, aes(x = cty, y = hwy)) + geom_point()\n\n\n\n\n\nggplot() 함수에 +로 연결되는 geom 함수는 그래프에 그릴 도형을 지정한다. geom_point() 함수는 ggplot() 함수에 정의된 그래프 속성과 열의 관계를 이용하여 그래프에 점(points)이라는 도형을 그린다. ggplot2에는 점을 그리는 geom_point() 함수뿐 아니라 다양한 도형을 그리는 geom 함수들이 있다. 만약 다음처럼 geom_point() 함수가 아니라 geom_smooth() 함수를 연결하면 점이 아니라 데이터의 추세선을 ggplot() 함수에 정의된 그래프 속성과 열의 관계를 이용하여 그래프에 그린다.\n\n\nggplot(mpg, aes(x = displ, y = hwy)) + geom_smooth()\n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nggplot() 함수에 여러 개의 geom 함수를 연결하여 두 개 이상의 그래픽적 도형을 그래프에 그릴 수 있다. 이 경우 먼저 기술된 geom 함수의 도형이 아래 층에 그려지고 뒤에 기술된 geom 함수의 도형이 윗 층에 그려진다.\n\n\n\n\n\n앞의 산점도에서 배기량에 따라 연비가 줄어드는 관계를 조금 벗어나는 관측치들이 있다.\n\nggplot(mpg, aes(x = displ, y = hwy)) + geom_point()\n\n\n\n\n이 예외적인 관측치들이 자동차 종류의 차이 때문에 발생했다, 라고 가설을 세웠다 하자. 이 가설을 확인해 보려면 자동차 종류별로 관측치를 시각화할 필요가 있다. 앞서 본 geom_point() 함수는 ’점’이라는 도형을 좌표평면 상에서 그린다. 점이라는 도형은 x-축의 위치(x)와 y-축의 위치(y)뿐 아니라 색상(color), 모양(shape), 크기(size), 투명도(alpha) 등의 다른 시각적 속성을 가지고 있다. 우리는 이러한 속성 중 하나에 mpg 데이터의 class 열을 대응시켜 자동차 종류 별로 좌표평면에서 시각적으로 구분되는 점으로 표현할 수 있다.\n\n\n다음은 관측치의 종류(class)에 따라 점을 서로 다른 색상(color)으로 표현한 예이다. 자동차의 종류에 따라 점이 다른 색상으로 표현되고, 어떤 색상이 어떤 자동차 종류에 대응되었는지에 대한 범례가 자동 생성된다.\n\nggplot(mpg, aes(x = displ, y = hwy, color = class)) + geom_point()\n\n\n\n\n앞선 그래프에서 이상치로 표현되었던 점들 중 한 점만 제외하고 모두 2seater 자동차의 관측치였음을 알 수 있다. 이 종류의 차는 스포츠카로 배기량에 비해 가벼운 몸체를 가지고 있어 예외적인 연비가 관측된 것으로 보인다.\n다음으로 class 열을 shape, size, alpha 등의 속성에 대응시켜 어떤 결과가 나오는지 살펴보자.\n\n\n\nshape 속성은 점의 모양을 결정한다. 다음은 앞의 산점도를 구동 방식(drv)에 따라 점의 모양이 다르게 표시한 예이다.\n\nggplot(mpg, aes(x = displ, y = hwy, shape = drv)) + geom_point()\n\n\n\n\n점의 모양과 색상을 하나의 데이터 열에 매핑하여 좀 더 데이터가 뚜렷이 구분되게 그래프를 그리기도 한다.\n\nggplot(mpg, aes(x = displ, y = hwy, shape = drv, color = drv)) +\n  geom_point()\n\n\n\n\n물론 다음처럼 점의 색상과 모양을 각각 데이터의 다른 열에 매핑할 수도 있다. 다음은 점의 색은 자동차의 종류(class)에 모양은 자동차의 구동방식(drv)에 매핑한 결과이다.\n\nggplot(mpg, aes(x = displ, y = hwy, shape = drv, color = class)) +\n  geom_point()\n\n\n\n\nshape을 사용할 때 주의할 점은 shape은 최대 6개의 모양으로만 점을 구분하기 때문에 class 열처럼 6개보 많은 종류가 있는 열에 매핑되면 데이터가 제대로 표시가 되지 않는다. 다음 예처럼 shape 속성에 class 열을 매핑하니 경고가 나타나고 suv 데이터를 표시하지 못한 것을 확인할 수 있다.\n\nggplot(mpg, aes(x = displ, y = hwy, shape = class)) + geom_point()\n#&gt; Warning: The shape palette can deal with a maximum of 6 discrete values because\n#&gt; more than 6 becomes difficult to discriminate; you have 7. Consider\n#&gt; specifying shapes manually if you must have them.\n#&gt; Warning: Removed 62 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n모양(shape) 속성은 소수의 구분되는 값으로 표현되는 범주형 변수를 표현하기 좋다. 데이터의 열이 연속형 변수이면 연속적인 값을 표현하기 좋은 가로축(x), 세로축(y), 크기(size), 투명도(alpha) 등을 이용하는 것이 좋다. 색상(color)은 범주형 변수와 연속형 변수에 모두 매핑될 수 있다. 범주형 변수로 매핑되면 구분되는 색상으로, 연속형 변수로 매핑되면 색상의 그라데이션으로 값을 표시한다.\n다음은 도심 연비와 고속도로 연비를 가로축과 세로축으로 하는 그래프에서 점의 크기 속성을 배기량 열에 매핑한 결과이다. 도심 연비와 고속도로 연비가 좋은 차들은 배기량이 작은 차임을 알 수 있다.\n\nggplot(mpg, aes(x = cty, y = hwy, size = displ)) + geom_point()\n\n\n\n\n다음은 동일한 도심 연비와 고속도로 연비 산점도에서 그래프에서 점의 색상을 배기량 열에 매핑한 결과이다. 범주형 변수가 매핑될 때와는 달리 색상의 연속적인 변화인 그라데이션을 사용하여 배기량을 표현하고 있음을 볼 수 있다.\n\nggplot(mpg, aes(x = cty, y = hwy, color = displ)) + geom_point()\n\n\n\n\n다음은 동일한 도심 연비와 고속도로 연비 산점도에서 그래프에서 점의 투명도를 실린더 수 열에 매핑한 결과이다.\n\nggplot(mpg, aes(x = cty, y = hwy, alpha = cyl)) + geom_point()\n\n\n\n\n\n\n\n\nggplot은 매우 강력한 기능을 가지고 있지만 Excel 등의 GUI 프로그램에만 익숙한 사람은 문자 기반 명령어를 입력하는 것에 어려움을 느낄 수 있다. 컴퓨터는 사람만큼의 유연성을 발휘하지 못하므로 컴퓨터는 자신이 실행해야 할 명령문의 문법에 매우 까다롭게 반응한다. ggplot 명령어 입력시 흔히 발생하는 문제들은 다음과 같다.\n\nR 명령문은 대소문자를 구분한다. 따라서 Color와 color는 ggplot에서 서로 다른 인수로 인식되어 오류가 발생한다.\nggplot 명령문의 키워드의 철자가 틀리면 다른 키워드로 간주하기 때문에 오류가 발생할 수 있다. 이를 방지하려면 키워드의 일부만 입력한 후 Tab 키를 눌러 자동완성 기능을 사용하여 입력하는 것을 권장한다.\nggplot2의 명령문을 입력할 때 여러 함수를 합쳐서 실행하기 위하여 + 연산자를 이용한다. ggplot2의 명령문이 길어지면 명령문을 여러 줄로 쓰는 것이 필요한데, 보통 +로 연결되는 곳에서 줄바꿈하는 것이 읽기에 좋다. 이 때 주의할 점이, 줄바꿈을 + 앞이 아니라 뒤에서 해야 한다는 것이다. + 앞에서 하면 R은 명령문의 입력이 완성된 것으로 간주하기 때문이다.\n\n다음은 산점도와 추세선을 한 그래프에 그린 예이다.\n\nggplot(mpg, aes(x = displ, y = hwy, color = drv)) + geom_point() + geom_smooth()\n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n그런데 위의 명령어는 길기 때문에 스크립트 파일을 작성할 때 보기에 불편하다. 이러한 경우에 위의 명령은 다음처럼 세 줄로 나누어 기술될 수 있다. 세 함수를 연결하는 + 위치가 어디에 있는지 살펴보라. (다음 예에서 왼쪽의 &gt; 프롬프트 아래 있는 +는 R 콘솔에서 명령문이 계속되고 있음을 나타내는 표시이다. 이 표시와 사용자가 입력한 +를 혼동하면 안 된다.)\n\nggplot(mpg, aes(x = displ, y = hwy, color = drv)) +\n  geom_point() +\n  geom_smooth()\n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n만약 다음처럼 + 위치가 잘못되면 오류가 발생한다. 왜 이런 결과가 나왔고 오류 메시지의 의미는 무엇일까? R은 Enter로 명령문을 구분한다. 그러므로 첫번째 줄은 +가 없으므로 완벽한 명령문이기 입력된 것으로 간주하고 실행이되어 좌표평면만 그린 것이다. 그러고 나서 두번째 줄을 새로운 명령문으로 실행을 한다. 그런데 갑자기 명령문이 +로 시작하니 R은 명령문에 오류가 있다고 판단한다. 왜냐하면 + 연산은 왼편과 오른편에 더할 요소가 있어야 하는데, 왼편의 요소가 기술되지 않았기 때문이다.\n\nggplot(mpg, aes(x = displ, y = hwy, color = drv))\n+ geom_point()\n\n\nR 명령문이 조금 길어지면 가장 흔하게 발생하는 실수가 ( )와 \" \"을 짝을 맞추어 제대로 입력하지 못하는 것이다. ggplot2의 명령문도 많은 함수를 사용하다 보니 이를 주의하여야 한다. 이러한 실수를 하게 되면면 R 콘솔은 명령이 계속 입력 중이라고 생각하여 &gt;가 아니라 +를 콘솔의 프롬프트로 표시한다. 이 경우 가장 간단한 해결책은 Esc 키를 눌러 명령 입력에서 빠져나와 다시 명령문을 입력하는 것이다.\n\n\n\n\nggplot2의 장점은 필요에 따라 다양한 형식의 그래프를 쉽게 만들 수 있고, 만들 수 있는 형식도 무궁무진하다는데 있다. 그리고 ggplot2 그래프의 계층적 구조가 이러한 무궁무진한 그래프 형식을 만들어 내는 핵심 요소라 할 수 있다. ggplot2는 좌표평면 위에 여러 계층으로 그래프를 겹쳐 그려서 하나의 좌표평면에 나타냄으로써 복잡한 형식의 그래프를 만들어 낼 수 있다.\n다음 그래프는 배기량과 고속도로 연비의 산점도와 추세선을 한 그래프에 그렸다. ggplot() 함수에 지정한 데이터와 그래프 속성과 데이터 열 매핑이 산점도(geom_point())와 추세선(geom_smooth())에 모두 동일하게 정의되었음을 볼 수 있다.\n\nggplot(mpg, aes(x = displ, y = hwy, color = drv)) +\n  geom_point() +\n  geom_smooth()\n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\nggplot() 함수가 여러 개의 geom 함수와 연결되면, 하나의 좌표평면에 각각의 geom() 함수의 결과를 층층이 그린다. 이 때, 명령문에 나타나는 순서에 따라 첫번째 나온 geom 함수의 도형이 가장 아래 계층에, 다음에 나오는 geom 함수의 도형이 차례로 그 윗 계층에 그려진다.\n\n\n앞의 배기량과 고속도로 연비의 산점도와 추세선을 그린 그래프에서 추세선을 선 종류(linttype)가 구동 방식(drv)에 따라 다르게 표현하고 싶다. 그런데 산점도는 점이라는 도형으로 그래프를 그리므로 선 종류라는 속성을 가지고 있지 않다. 그리고 산점도도 점의 모양(shape)이 구동 방식에 따라 다르게 표현하고 싶다고 하자. 마찬가지로 추세선은 선이라는 도형으로 그래프를 그리므로 점의 모양이라는 속성을 가지고 있지 않다.\n이렇듯 여러 geom 함수를 연결하여 그래프를 그릴 때, 특정 geom 함수에만 해당하는 속성은 해당 geom 함수에서 속성과 데이터 열을 매핑하는 것이 좋다. geom 함수도 ggplot() 함수처럼 aes() 함수를 이용하여 그래프 속성과 데이터 열을 매핑하는데, 이 매핑이 geom 함수의 첫 번째 인수로 기술된다는 점만 다르다.\n\nggplot(mpg, aes(x = displ, y = hwy, color = drv)) +\n  geom_point(aes(shape = drv)) +\n  geom_smooth(aes(linetype = drv))\n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n따라서 지금까지 배운 내용으로 ggplot2 그래프를 그리는 문법을 확장하면 다음과 같다.\n\nggplot(데이터, aes(공통속성1=열1, 공통속성2=열2, ...)) + \n  geom함수1(aes(geom함수1의 속성1=열1, geom함수1의 속성2=열2, ...)) + \n  geom함수2(aes(geom함수2의 속성1=열1, geom함수2의 속성2=열2, ...)) +\n  ....\n\n확장된 문법으로 맨처음 그린 배기량과 고속도로 연비의 산점도와 추세선 그래프에서, 산점도의 점은 구동 방식에 따라 다른 색으로 표시하지만, 추세선은 모든 데이터에 대하여 하나만 그리려면 어떻게 해야 할까? 답은 다음처럼 색상 속성을 공통 속성으로 ggplot()에 매핑하지 않고 산점도만의 속성 매핑이 되도록 geom_point()에 기술하는 것이다.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = drv)) +\n  geom_smooth()\n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n마찬가지로 추세선을 구동 방식에 따라 다른 색상으로 표시하나 점은 모두 동일한 색으로 표시하고 싶으면 다음처럼 색상이 추세선만의 속성 매핑이 되도록 geom_smooth()에 기술하는 것이다.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  geom_smooth(aes(color = drv))\n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n또한 ggplot() 함수에 데이터와 도형 속성에 대한 매핑이 되어 있어도, geom 함수에서 데이터와 도형 속성의 매핑을 재지정할 수도 있다. 이 경우 각 geom 함수에서 사용하는 data와 mapping은 다음 규칙에 의해 결정된다.\n\ngeom 함수는 ggplot() 함수에 설정된 data와 mapping을 상속받아 그래프를 그린다.\n만약 geom 함수에 data 인수가 설정되면 ggplot() 함수에 설정된 data는 무시된다.\n만약 geom 함수에 mapping 인수가 설정되면 ggplot() 함수에 설정된 mapping에 geom 함수에 설정된 mapping이 추가된다. 만약 동일한 도형 속성에 대한 정의가 두 군데 나타나면 geom 함수의 설정이 사용된다. 자세한 내용은 R 프로그래밍의 그래프 계층(layers)과 도형(geoms) 절을 참조하기 바란다.\n\n\n\n\n\n다음 그래프는 배기량과 고속도로 연비의 관계를 살펴보기 위하여 이 두 변수의 관계를 산점도로 살펴보고 나서, 이 두 변수의 관계가 자동차 종류에 따라 어떻게 달라지는지를 살펴보기 위해 그래프의 색상 속성을 자동차 종류를 나타내는 열에 매핑하여 다르게 표시되도록 하였다.\n\nggplot(mpg, aes(x = displ, y = hwy)) + geom_point()\n\n\n\nggplot(mpg, aes(x = displ, y = hwy, color = class)) + geom_point() \n\n\n\n\n이렇듯 두 변수의 관계를 제삼의 변수 관점에서 세분화하여 살펴보는 방법으로 제삼의 변수를 그래프 속성에 매핑하는 방법 말고도 제삼의 변수의 변수값에 따라 데이터를 별도의 그래프로 나누어 그려보는 방법이 있다. ggplot2에서는 이러한 방식을 측면(facets)으로 나누어 그래프를 그린다고 한다.\n\n\n다음은 facet_wrap() 함수의 사용법을 보여준다. ~ 은 R에서 수식을 표현할 때 사용되는데, facet_wrap() 함수는 수식을 첫 번재 인수로 입력받는다. facet_wrap() 함수는 ~ 우변에 서술된 변수의 변수값 별로 데이터를 나누어 그래프를 각각 그린다. 이 때 측면(facets)을 지정하는데 사용되는 변수는 범주형 데이터이어야 한다. facet_wrap()은 측면 그래프가 많아지면 줄바꿈하여 그래프를 표시한다. nrow나 ncol을 설정하면 그래프의 행과 열의 수를 지정하여 줄바꿈 처리를 제어할 수 있다.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  facet_wrap( ~ class, nrow = 2)\n\n\n\n\n측면으로 나누어 그려진 그래프는 서로 비교가 용이하도록 동일한 좌표축으로 그려진다. 측면 그래프의 상단에는 어떤 측면의 데이터에 대한 그래프인지를 표시한다. 맨 처음 측면 그래프는 2seater 측면에서 배기량과 고속도로 연비의 산점도를 보여주고, 맨 마지막 측면 그래프는 SUV 측면에서 배기량과 고속도로 연비의 산점도를 보여준다.\n두 개 이상의 변수를 조합하여 측면 그래프을 만드려면 다음처럼 수식의 우변에 두 개의 변수를 +로 연결하여 기술하면 된다. 다음은 구동 방식(drv)와 조사 년도(year)의 값에 따라 그래프를 나누어 그린 예이다. 역시 모든 그래프의 좌표축은 동일하고 그래프 상단에 어떤 측면의 그래프인지를 표시하고 있는데 윗줄에 표시된 내용은 구동 방식의 값이고 아랫줄은 조사년도의 값이다. 따라서 첫 번째 측면 그래프는 4륜 구동이고 1999년도 조사한 데이터 측면에서 배기량과 고속도로 연비의 산점도를 보여준다.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  facet_wrap(~drv + year, nrow = 2)\n\n\n\n\n\n\n\n그래프를 두 변수의 측면에서 나누어 그릴 때는 face_wrap() 보다는 facet_grid()를 사용하는 것이 좋다. facet_grid()도 수식을 첫 번재 인수로 입력 받는데, 수식의 좌변과 우변에 측면으로 나누는데 사용할 변수를 지정할 수 있다. 수식의 좌변에 기술된 변수를 기준으로 측면 그래프를 행으로 배열하고, 우변에 기술된 변수를 기준으로 측면 그래프를 열로 배열한다. 다음 그래프는 행은 구동 방식으로, 열은 실린더 수를 기준으로 나누어 측면 그래프를 그린 예이다. 그러므로 두 번째 행-세 번째 열의 그래프는 전륜 구동(f)이고 실린더가 6자동차의 산점도를 나타낸다.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  facet_grid(drv ~ cyl)\n\n\n\n\nfacet_wrap() 함수와 마찬가지로 수식의 좌변과 우변에 +로 하나 이상의 변수를 지정할 수도 있다.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  facet_grid(drv + year ~ cyl)\n\n\n\n\n\n\n\n\nggplot2에는 지금까지 설명한 문법 요소 외에도 통계 변환(stat), 위치 조정(position), 스케일 변환(scale), 좌표축 변환(coord), 테마(theme) 등의 요소가 있다. ggplot2를 사용하여 복잡한 시각화를 수행하려면 이러한 문법 요소에 대한 체계적 이해와 습득이 필요하다. 그러나 이 책은 데이터 시각화 전반을 소개하는 것이 목적이 아니기 때문에, 통계데이터 분석을 위한 그래프를 그릴 때 이러한 문법 요소가 필요하면 그 요소를 단편적으로 설명할 예정이다. 그러므로 좀 더 ggplot2 그래프에 대한 체계적인 이해를 원하는 독자는 R 프로그래밍의 ggplot2를 이용한 데이터 시각화를 참조하기 바란다.\n이 절의 나머지 부분에서는 나머지 문법 요소 중 그래프의 외양을 변경하는 매우 간단한 한 가지 문법 요소만 살펴보도록 한다.\n\n\nggplot2 패키지의 labs() 함수는 그래프의 제목, 좌표축 이름, 범례의 이름을 쉽게 바꿀 수 있게 해준다. 다음은 mpg 데이터의 배기량과 고속도로 연비의 산점도를 자동차 종류 별로 다른 색상으로 그린 예이다. 그런데 ggplot2에서는 기본적으로 좌표축 레이블과 색상의 범례 레이블로, 좌표축과 색상에 매핑된 열의 이름을 사용한다. 그리고 그래프에 제목은 달지 않는다.\n\nggplot(mpg, aes(x = displ, y = hwy, color = class)) + geom_point()\n\n\n\n\n만약 자동으로 부여된 레이블이 마음에 들지 않으면 이를 labs() 함수로 변경할 수 있다. 위 그래프에서 다음처럼 범례 이름, 축의 이름 한글로 바꾸고, 그래프의 제목도 달아 보자.\n\nggplot(mpg, aes(x = displ, y = hwy, color = class)) +\n  geom_point() +\n  labs(title = '배기량과 고속도로 연비 산점도',\n       x = '배기량(리터)',\n       y = '고속도로 연비',\n       color = '자동차 종류')\n\n\n\n\nlabs() 함수는 ggplot2 그래프에 + 연산으로 결합하여 사용되면, 그래픽 속성 매핑에 사용된 x, y, color 인수에 사용할 이름을 지정하면 된다. 그래프의 제목을 지정하려면 title이라는 인수를 사용한다."
  },
  {
    "objectID": "Data_Visualization/chp5.html#chapter-5-r-데이터-시각화-기초",
    "href": "Data_Visualization/chp5.html#chapter-5-r-데이터-시각화-기초",
    "title": "ggplot2 Tutorial",
    "section": "",
    "text": "데이터 시각화란 데이터를 그래프 등의 시각적 요소로 요약하여 보여주는 것을 의미한다. R에서는 데이터 시각화를 R의 기본 기능에 포함된 graphics 패키지를 사용하여 시각화하는 방법과 ggplot2패키지를 이용하는 방법이 있다. 이 장에서는 ggplot2를 이용하여 데이터를 시각화하는 기본적인 방법을 배운다. 여기서는 통계분석에 필요한 기본적인 그래프를 그리기 위한 기본적인 문법을 소개하는 것이지 ggplot2에 대한 체계적인 설명을 하지 않을 것이다. ggplot2는 자유로운 형식으로 그래프를 그릴 수 있는 그래프 문법을 가지고 있기 때문에, ggplot2에 대한 더 체계적인 이해를 원하는 독자는 졸저 ’R 프로그래밍’의 ggplot2를 이용한 데이터 시각화를 참조하기 바란다.\n\n\nR은 패키지란 단위로 R에서 사용할 수 있는 기능을 제공한다. R을 설치하면 base, stat, dataset, graphics 등의 기본 패키지가 자동으로 설치되고, R을 시작할 때마다 이러한 기본 패키지가 자동으로 적대되어 사용될 수 있도록 준비된다. 만약 R에서 기본으로 제공하는 패키지 말고 다른 패키지를 사용하려면 그 패키지를 R에 설치해야 한다. ggplot2 패키지는 기본 기능에 포함되지 않으므로 먼저 설치를 해야 한다.\nggplot2 패키지를 설치하려면 다음 명령을 실행하면 된다. 패키지의 이름은 문자열이므로 따옴표 안에 기술해야 한다.\n\n# install.packages('ggplot2')\n\n또는 RStudio의 우측 하단의 Packages 탭에서 [Install]을 클릭한 후 ggplot2라고 입력을 하면 된다. 패키지 설치는 한 번만 수행하면 된다.\n패키지를 사용하려면 메모리에 적재를 하여야 한다. 패키지를 메모리에 적재하는 것은 library() 함수를 사용한다. 이 때 주의할 점은 이미 설치된 패키지를 지정할 때는 따옴표 없이 변수처럼 패키지를 기술해야 한다는 것이다.\n\nlibrary(ggplot2)\n\n패키지의 설치는 한 번만 수행하면 되지만, 패키지를 메모리에 적재하는 작업을 패키지를 사용할 때마다 수행하여야 한다. 한번 메모리에 적재된 패키지는 R 세션이 종료되기 전까지 유지된다. 그러므로 하나의 R 세션에서는 다시 library() 함수로 동일한 패키지를 적재하지 않아도 된다. 그러나 R 세션을 종료하고 다시 시작하였다면, 기본 패키지가 아니면 자동 적재되지 않으므로 사용하기 전에 패키지를 다시 적재하여야 한다.\n\n\n\n이 절에서는 ggplot2에서 제공하는 mpg 데이터를 이용하여 ‘배기량이 커지면 연비가 낮아지는가?’ 라는 물음을 그래프를 이용하여 탐색해 보자. mpg는 1999년과 2008년에 미국 EPA에서 조사하여 발표한 자동차 주요 모델별 연비 데이터이다.\n다음 명령을 이용하여 mpg 데이터를 출력해 보자. mpg 데이터는 tibble이라는 데이터 프레임의 일종으로, 사용자의 화면의 크기에 따라 출력 내용을 조정한다. 그러므로 화면의 크기에 따라 출력되는 내용이 책과는 조금 다를 수 있다.\nYou can also embed plots, for example:\n\nmpg\n#&gt; # A tibble: 234 × 11\n#&gt;    manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n#&gt;    &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n#&gt;  1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n#&gt;  2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n#&gt;  3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n#&gt;  4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n#&gt;  5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n#&gt;  6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n#&gt;  7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n#&gt;  8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n#&gt;  9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n#&gt; 10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n#&gt; # ℹ 224 more rows\n\nmpg는 1999년과 2008년에 미국 EPA에서 조사하여 발표한 자동차 주요 모델별 연비 데이터이다. 데이터는 234 개의 행이 있으며, 각 행은 다음과 같은 변수로 구성되어 있다.\n\nmanufacturer : 자동차 제조사\nmodel : 자동차 모델명\ndispl : 자동차 배기량\nyear : 제조년도\ncyl : 엔진 실린더 수\ntrans : 자동차 트랜스미션 종류\ndrv : 자동차 구동 방식. f = 전륜구동, r = 후륜구동, 4 = 사륜구동\ncty : 도심 연비 (마일/갤론)\nhwy : 고속도로 연비 (마일/갤론)\nfl : 연료 종류\nclass : 자동차 분류\nmpg 데이터에 대한 더 자세한 설명은 콘솔에 다음을 입력하여 R 도움말을 참조하기 바란다.\n\n\n?mpg\n\n\n\nmpg 데이터로부터 배기량과 고속도로 연비의 관계를 살펴보기 위해서 배기량(displ)을 x 축으로, 고속도로 연비(hwy)를 y 축으로 하는 산점도를 그려보자. 산점도에서 배기량이 커짐지면 연비가 줄어드는 경향을 관찰할 수 있다.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point()\n\n\n\n\n그러면 이 산점도 그린 ggplot2 명령어의 문법을 살펴보자.\n\nggplot2의 명령어는 항상 ggplot() 함수로 시작하고, + 연산자를 사용하여 그래프에 추가될 요소를 덧붙여 나간다. 이렇게 함수를 +로 연결하여 사용하는 방식은 ggplot2 패키지의 독특한 문법으로 대부분의 다른 R 명령어에서는 이러한 방식을 사용하지 않는다.\nggplot() 함수는 그래프의 좌표축과 좌표평면을 만드는 함수이다. 그러므로 다음처럼 ggplot() 함수만 사용하고 그래프에 추가할 요소를 지정하지 않으면 좌표축과 좌표평면만 그린다.\n\n\nggplot(mpg, aes(x = displ, y = hwy))\n\n\n\n\n\nggplot() 함수의 첫번째 인수는 그래프를 그릴 때 사용할 데이터를 지정하고, 두번째 인수는 그래프 속성과 데이터 열의 관계를 지정한다. 그래프 속성과 데이터 열의 관계는 항상 aes() 함수 내에 기술되고, 다음처럼 &lt;그래프 속성&gt;=&lt;데이터 열&gt;의 형식으로 기술된다.\n\n\nggplot(데이터, aes(속성1=열1, 속성2=열2, ...)) + geom함수()\n\n앞의 산점도에서는 x라는 그래프의 가로축 속성에 mpg 데이터의 배기량 열 displ이 매핑되었고, y라는 그래프의 세로축 속성에 고속도로 연비 열 hwy가 매핑되었다. 다음은 그래프의 가로축에 데이터의 도심 연비 열인 cty을 매핑하여 산점도를 그린 예이다. 도심 연비가 좋은 차가 고속도로 연비도 좋다는 것을 알 수 있다.\n\nggplot(mpg, aes(x = cty, y = hwy)) + geom_point()\n\n\n\n\n\nggplot() 함수에 +로 연결되는 geom 함수는 그래프에 그릴 도형을 지정한다. geom_point() 함수는 ggplot() 함수에 정의된 그래프 속성과 열의 관계를 이용하여 그래프에 점(points)이라는 도형을 그린다. ggplot2에는 점을 그리는 geom_point() 함수뿐 아니라 다양한 도형을 그리는 geom 함수들이 있다. 만약 다음처럼 geom_point() 함수가 아니라 geom_smooth() 함수를 연결하면 점이 아니라 데이터의 추세선을 ggplot() 함수에 정의된 그래프 속성과 열의 관계를 이용하여 그래프에 그린다.\n\n\nggplot(mpg, aes(x = displ, y = hwy)) + geom_smooth()\n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nggplot() 함수에 여러 개의 geom 함수를 연결하여 두 개 이상의 그래픽적 도형을 그래프에 그릴 수 있다. 이 경우 먼저 기술된 geom 함수의 도형이 아래 층에 그려지고 뒤에 기술된 geom 함수의 도형이 윗 층에 그려진다.\n\n\n\n\n\n앞의 산점도에서 배기량에 따라 연비가 줄어드는 관계를 조금 벗어나는 관측치들이 있다.\n\nggplot(mpg, aes(x = displ, y = hwy)) + geom_point()\n\n\n\n\n이 예외적인 관측치들이 자동차 종류의 차이 때문에 발생했다, 라고 가설을 세웠다 하자. 이 가설을 확인해 보려면 자동차 종류별로 관측치를 시각화할 필요가 있다. 앞서 본 geom_point() 함수는 ’점’이라는 도형을 좌표평면 상에서 그린다. 점이라는 도형은 x-축의 위치(x)와 y-축의 위치(y)뿐 아니라 색상(color), 모양(shape), 크기(size), 투명도(alpha) 등의 다른 시각적 속성을 가지고 있다. 우리는 이러한 속성 중 하나에 mpg 데이터의 class 열을 대응시켜 자동차 종류 별로 좌표평면에서 시각적으로 구분되는 점으로 표현할 수 있다.\n\n\n다음은 관측치의 종류(class)에 따라 점을 서로 다른 색상(color)으로 표현한 예이다. 자동차의 종류에 따라 점이 다른 색상으로 표현되고, 어떤 색상이 어떤 자동차 종류에 대응되었는지에 대한 범례가 자동 생성된다.\n\nggplot(mpg, aes(x = displ, y = hwy, color = class)) + geom_point()\n\n\n\n\n앞선 그래프에서 이상치로 표현되었던 점들 중 한 점만 제외하고 모두 2seater 자동차의 관측치였음을 알 수 있다. 이 종류의 차는 스포츠카로 배기량에 비해 가벼운 몸체를 가지고 있어 예외적인 연비가 관측된 것으로 보인다.\n다음으로 class 열을 shape, size, alpha 등의 속성에 대응시켜 어떤 결과가 나오는지 살펴보자.\n\n\n\nshape 속성은 점의 모양을 결정한다. 다음은 앞의 산점도를 구동 방식(drv)에 따라 점의 모양이 다르게 표시한 예이다.\n\nggplot(mpg, aes(x = displ, y = hwy, shape = drv)) + geom_point()\n\n\n\n\n점의 모양과 색상을 하나의 데이터 열에 매핑하여 좀 더 데이터가 뚜렷이 구분되게 그래프를 그리기도 한다.\n\nggplot(mpg, aes(x = displ, y = hwy, shape = drv, color = drv)) +\n  geom_point()\n\n\n\n\n물론 다음처럼 점의 색상과 모양을 각각 데이터의 다른 열에 매핑할 수도 있다. 다음은 점의 색은 자동차의 종류(class)에 모양은 자동차의 구동방식(drv)에 매핑한 결과이다.\n\nggplot(mpg, aes(x = displ, y = hwy, shape = drv, color = class)) +\n  geom_point()\n\n\n\n\nshape을 사용할 때 주의할 점은 shape은 최대 6개의 모양으로만 점을 구분하기 때문에 class 열처럼 6개보 많은 종류가 있는 열에 매핑되면 데이터가 제대로 표시가 되지 않는다. 다음 예처럼 shape 속성에 class 열을 매핑하니 경고가 나타나고 suv 데이터를 표시하지 못한 것을 확인할 수 있다.\n\nggplot(mpg, aes(x = displ, y = hwy, shape = class)) + geom_point()\n#&gt; Warning: The shape palette can deal with a maximum of 6 discrete values because\n#&gt; more than 6 becomes difficult to discriminate; you have 7. Consider\n#&gt; specifying shapes manually if you must have them.\n#&gt; Warning: Removed 62 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n모양(shape) 속성은 소수의 구분되는 값으로 표현되는 범주형 변수를 표현하기 좋다. 데이터의 열이 연속형 변수이면 연속적인 값을 표현하기 좋은 가로축(x), 세로축(y), 크기(size), 투명도(alpha) 등을 이용하는 것이 좋다. 색상(color)은 범주형 변수와 연속형 변수에 모두 매핑될 수 있다. 범주형 변수로 매핑되면 구분되는 색상으로, 연속형 변수로 매핑되면 색상의 그라데이션으로 값을 표시한다.\n다음은 도심 연비와 고속도로 연비를 가로축과 세로축으로 하는 그래프에서 점의 크기 속성을 배기량 열에 매핑한 결과이다. 도심 연비와 고속도로 연비가 좋은 차들은 배기량이 작은 차임을 알 수 있다.\n\nggplot(mpg, aes(x = cty, y = hwy, size = displ)) + geom_point()\n\n\n\n\n다음은 동일한 도심 연비와 고속도로 연비 산점도에서 그래프에서 점의 색상을 배기량 열에 매핑한 결과이다. 범주형 변수가 매핑될 때와는 달리 색상의 연속적인 변화인 그라데이션을 사용하여 배기량을 표현하고 있음을 볼 수 있다.\n\nggplot(mpg, aes(x = cty, y = hwy, color = displ)) + geom_point()\n\n\n\n\n다음은 동일한 도심 연비와 고속도로 연비 산점도에서 그래프에서 점의 투명도를 실린더 수 열에 매핑한 결과이다.\n\nggplot(mpg, aes(x = cty, y = hwy, alpha = cyl)) + geom_point()\n\n\n\n\n\n\n\n\nggplot은 매우 강력한 기능을 가지고 있지만 Excel 등의 GUI 프로그램에만 익숙한 사람은 문자 기반 명령어를 입력하는 것에 어려움을 느낄 수 있다. 컴퓨터는 사람만큼의 유연성을 발휘하지 못하므로 컴퓨터는 자신이 실행해야 할 명령문의 문법에 매우 까다롭게 반응한다. ggplot 명령어 입력시 흔히 발생하는 문제들은 다음과 같다.\n\nR 명령문은 대소문자를 구분한다. 따라서 Color와 color는 ggplot에서 서로 다른 인수로 인식되어 오류가 발생한다.\nggplot 명령문의 키워드의 철자가 틀리면 다른 키워드로 간주하기 때문에 오류가 발생할 수 있다. 이를 방지하려면 키워드의 일부만 입력한 후 Tab 키를 눌러 자동완성 기능을 사용하여 입력하는 것을 권장한다.\nggplot2의 명령문을 입력할 때 여러 함수를 합쳐서 실행하기 위하여 + 연산자를 이용한다. ggplot2의 명령문이 길어지면 명령문을 여러 줄로 쓰는 것이 필요한데, 보통 +로 연결되는 곳에서 줄바꿈하는 것이 읽기에 좋다. 이 때 주의할 점이, 줄바꿈을 + 앞이 아니라 뒤에서 해야 한다는 것이다. + 앞에서 하면 R은 명령문의 입력이 완성된 것으로 간주하기 때문이다.\n\n다음은 산점도와 추세선을 한 그래프에 그린 예이다.\n\nggplot(mpg, aes(x = displ, y = hwy, color = drv)) + geom_point() + geom_smooth()\n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n그런데 위의 명령어는 길기 때문에 스크립트 파일을 작성할 때 보기에 불편하다. 이러한 경우에 위의 명령은 다음처럼 세 줄로 나누어 기술될 수 있다. 세 함수를 연결하는 + 위치가 어디에 있는지 살펴보라. (다음 예에서 왼쪽의 &gt; 프롬프트 아래 있는 +는 R 콘솔에서 명령문이 계속되고 있음을 나타내는 표시이다. 이 표시와 사용자가 입력한 +를 혼동하면 안 된다.)\n\nggplot(mpg, aes(x = displ, y = hwy, color = drv)) +\n  geom_point() +\n  geom_smooth()\n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n만약 다음처럼 + 위치가 잘못되면 오류가 발생한다. 왜 이런 결과가 나왔고 오류 메시지의 의미는 무엇일까? R은 Enter로 명령문을 구분한다. 그러므로 첫번째 줄은 +가 없으므로 완벽한 명령문이기 입력된 것으로 간주하고 실행이되어 좌표평면만 그린 것이다. 그러고 나서 두번째 줄을 새로운 명령문으로 실행을 한다. 그런데 갑자기 명령문이 +로 시작하니 R은 명령문에 오류가 있다고 판단한다. 왜냐하면 + 연산은 왼편과 오른편에 더할 요소가 있어야 하는데, 왼편의 요소가 기술되지 않았기 때문이다.\n\nggplot(mpg, aes(x = displ, y = hwy, color = drv))\n+ geom_point()\n\n\nR 명령문이 조금 길어지면 가장 흔하게 발생하는 실수가 ( )와 \" \"을 짝을 맞추어 제대로 입력하지 못하는 것이다. ggplot2의 명령문도 많은 함수를 사용하다 보니 이를 주의하여야 한다. 이러한 실수를 하게 되면면 R 콘솔은 명령이 계속 입력 중이라고 생각하여 &gt;가 아니라 +를 콘솔의 프롬프트로 표시한다. 이 경우 가장 간단한 해결책은 Esc 키를 눌러 명령 입력에서 빠져나와 다시 명령문을 입력하는 것이다.\n\n\n\n\nggplot2의 장점은 필요에 따라 다양한 형식의 그래프를 쉽게 만들 수 있고, 만들 수 있는 형식도 무궁무진하다는데 있다. 그리고 ggplot2 그래프의 계층적 구조가 이러한 무궁무진한 그래프 형식을 만들어 내는 핵심 요소라 할 수 있다. ggplot2는 좌표평면 위에 여러 계층으로 그래프를 겹쳐 그려서 하나의 좌표평면에 나타냄으로써 복잡한 형식의 그래프를 만들어 낼 수 있다.\n다음 그래프는 배기량과 고속도로 연비의 산점도와 추세선을 한 그래프에 그렸다. ggplot() 함수에 지정한 데이터와 그래프 속성과 데이터 열 매핑이 산점도(geom_point())와 추세선(geom_smooth())에 모두 동일하게 정의되었음을 볼 수 있다.\n\nggplot(mpg, aes(x = displ, y = hwy, color = drv)) +\n  geom_point() +\n  geom_smooth()\n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\nggplot() 함수가 여러 개의 geom 함수와 연결되면, 하나의 좌표평면에 각각의 geom() 함수의 결과를 층층이 그린다. 이 때, 명령문에 나타나는 순서에 따라 첫번째 나온 geom 함수의 도형이 가장 아래 계층에, 다음에 나오는 geom 함수의 도형이 차례로 그 윗 계층에 그려진다.\n\n\n앞의 배기량과 고속도로 연비의 산점도와 추세선을 그린 그래프에서 추세선을 선 종류(linttype)가 구동 방식(drv)에 따라 다르게 표현하고 싶다. 그런데 산점도는 점이라는 도형으로 그래프를 그리므로 선 종류라는 속성을 가지고 있지 않다. 그리고 산점도도 점의 모양(shape)이 구동 방식에 따라 다르게 표현하고 싶다고 하자. 마찬가지로 추세선은 선이라는 도형으로 그래프를 그리므로 점의 모양이라는 속성을 가지고 있지 않다.\n이렇듯 여러 geom 함수를 연결하여 그래프를 그릴 때, 특정 geom 함수에만 해당하는 속성은 해당 geom 함수에서 속성과 데이터 열을 매핑하는 것이 좋다. geom 함수도 ggplot() 함수처럼 aes() 함수를 이용하여 그래프 속성과 데이터 열을 매핑하는데, 이 매핑이 geom 함수의 첫 번째 인수로 기술된다는 점만 다르다.\n\nggplot(mpg, aes(x = displ, y = hwy, color = drv)) +\n  geom_point(aes(shape = drv)) +\n  geom_smooth(aes(linetype = drv))\n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n따라서 지금까지 배운 내용으로 ggplot2 그래프를 그리는 문법을 확장하면 다음과 같다.\n\nggplot(데이터, aes(공통속성1=열1, 공통속성2=열2, ...)) + \n  geom함수1(aes(geom함수1의 속성1=열1, geom함수1의 속성2=열2, ...)) + \n  geom함수2(aes(geom함수2의 속성1=열1, geom함수2의 속성2=열2, ...)) +\n  ....\n\n확장된 문법으로 맨처음 그린 배기량과 고속도로 연비의 산점도와 추세선 그래프에서, 산점도의 점은 구동 방식에 따라 다른 색으로 표시하지만, 추세선은 모든 데이터에 대하여 하나만 그리려면 어떻게 해야 할까? 답은 다음처럼 색상 속성을 공통 속성으로 ggplot()에 매핑하지 않고 산점도만의 속성 매핑이 되도록 geom_point()에 기술하는 것이다.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = drv)) +\n  geom_smooth()\n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n마찬가지로 추세선을 구동 방식에 따라 다른 색상으로 표시하나 점은 모두 동일한 색으로 표시하고 싶으면 다음처럼 색상이 추세선만의 속성 매핑이 되도록 geom_smooth()에 기술하는 것이다.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  geom_smooth(aes(color = drv))\n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n또한 ggplot() 함수에 데이터와 도형 속성에 대한 매핑이 되어 있어도, geom 함수에서 데이터와 도형 속성의 매핑을 재지정할 수도 있다. 이 경우 각 geom 함수에서 사용하는 data와 mapping은 다음 규칙에 의해 결정된다.\n\ngeom 함수는 ggplot() 함수에 설정된 data와 mapping을 상속받아 그래프를 그린다.\n만약 geom 함수에 data 인수가 설정되면 ggplot() 함수에 설정된 data는 무시된다.\n만약 geom 함수에 mapping 인수가 설정되면 ggplot() 함수에 설정된 mapping에 geom 함수에 설정된 mapping이 추가된다. 만약 동일한 도형 속성에 대한 정의가 두 군데 나타나면 geom 함수의 설정이 사용된다. 자세한 내용은 R 프로그래밍의 그래프 계층(layers)과 도형(geoms) 절을 참조하기 바란다.\n\n\n\n\n\n다음 그래프는 배기량과 고속도로 연비의 관계를 살펴보기 위하여 이 두 변수의 관계를 산점도로 살펴보고 나서, 이 두 변수의 관계가 자동차 종류에 따라 어떻게 달라지는지를 살펴보기 위해 그래프의 색상 속성을 자동차 종류를 나타내는 열에 매핑하여 다르게 표시되도록 하였다.\n\nggplot(mpg, aes(x = displ, y = hwy)) + geom_point()\n\n\n\nggplot(mpg, aes(x = displ, y = hwy, color = class)) + geom_point() \n\n\n\n\n이렇듯 두 변수의 관계를 제삼의 변수 관점에서 세분화하여 살펴보는 방법으로 제삼의 변수를 그래프 속성에 매핑하는 방법 말고도 제삼의 변수의 변수값에 따라 데이터를 별도의 그래프로 나누어 그려보는 방법이 있다. ggplot2에서는 이러한 방식을 측면(facets)으로 나누어 그래프를 그린다고 한다.\n\n\n다음은 facet_wrap() 함수의 사용법을 보여준다. ~ 은 R에서 수식을 표현할 때 사용되는데, facet_wrap() 함수는 수식을 첫 번재 인수로 입력받는다. facet_wrap() 함수는 ~ 우변에 서술된 변수의 변수값 별로 데이터를 나누어 그래프를 각각 그린다. 이 때 측면(facets)을 지정하는데 사용되는 변수는 범주형 데이터이어야 한다. facet_wrap()은 측면 그래프가 많아지면 줄바꿈하여 그래프를 표시한다. nrow나 ncol을 설정하면 그래프의 행과 열의 수를 지정하여 줄바꿈 처리를 제어할 수 있다.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  facet_wrap( ~ class, nrow = 2)\n\n\n\n\n측면으로 나누어 그려진 그래프는 서로 비교가 용이하도록 동일한 좌표축으로 그려진다. 측면 그래프의 상단에는 어떤 측면의 데이터에 대한 그래프인지를 표시한다. 맨 처음 측면 그래프는 2seater 측면에서 배기량과 고속도로 연비의 산점도를 보여주고, 맨 마지막 측면 그래프는 SUV 측면에서 배기량과 고속도로 연비의 산점도를 보여준다.\n두 개 이상의 변수를 조합하여 측면 그래프을 만드려면 다음처럼 수식의 우변에 두 개의 변수를 +로 연결하여 기술하면 된다. 다음은 구동 방식(drv)와 조사 년도(year)의 값에 따라 그래프를 나누어 그린 예이다. 역시 모든 그래프의 좌표축은 동일하고 그래프 상단에 어떤 측면의 그래프인지를 표시하고 있는데 윗줄에 표시된 내용은 구동 방식의 값이고 아랫줄은 조사년도의 값이다. 따라서 첫 번째 측면 그래프는 4륜 구동이고 1999년도 조사한 데이터 측면에서 배기량과 고속도로 연비의 산점도를 보여준다.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  facet_wrap(~drv + year, nrow = 2)\n\n\n\n\n\n\n\n그래프를 두 변수의 측면에서 나누어 그릴 때는 face_wrap() 보다는 facet_grid()를 사용하는 것이 좋다. facet_grid()도 수식을 첫 번재 인수로 입력 받는데, 수식의 좌변과 우변에 측면으로 나누는데 사용할 변수를 지정할 수 있다. 수식의 좌변에 기술된 변수를 기준으로 측면 그래프를 행으로 배열하고, 우변에 기술된 변수를 기준으로 측면 그래프를 열로 배열한다. 다음 그래프는 행은 구동 방식으로, 열은 실린더 수를 기준으로 나누어 측면 그래프를 그린 예이다. 그러므로 두 번째 행-세 번째 열의 그래프는 전륜 구동(f)이고 실린더가 6자동차의 산점도를 나타낸다.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  facet_grid(drv ~ cyl)\n\n\n\n\nfacet_wrap() 함수와 마찬가지로 수식의 좌변과 우변에 +로 하나 이상의 변수를 지정할 수도 있다.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  facet_grid(drv + year ~ cyl)\n\n\n\n\n\n\n\n\nggplot2에는 지금까지 설명한 문법 요소 외에도 통계 변환(stat), 위치 조정(position), 스케일 변환(scale), 좌표축 변환(coord), 테마(theme) 등의 요소가 있다. ggplot2를 사용하여 복잡한 시각화를 수행하려면 이러한 문법 요소에 대한 체계적 이해와 습득이 필요하다. 그러나 이 책은 데이터 시각화 전반을 소개하는 것이 목적이 아니기 때문에, 통계데이터 분석을 위한 그래프를 그릴 때 이러한 문법 요소가 필요하면 그 요소를 단편적으로 설명할 예정이다. 그러므로 좀 더 ggplot2 그래프에 대한 체계적인 이해를 원하는 독자는 R 프로그래밍의 ggplot2를 이용한 데이터 시각화를 참조하기 바란다.\n이 절의 나머지 부분에서는 나머지 문법 요소 중 그래프의 외양을 변경하는 매우 간단한 한 가지 문법 요소만 살펴보도록 한다.\n\n\nggplot2 패키지의 labs() 함수는 그래프의 제목, 좌표축 이름, 범례의 이름을 쉽게 바꿀 수 있게 해준다. 다음은 mpg 데이터의 배기량과 고속도로 연비의 산점도를 자동차 종류 별로 다른 색상으로 그린 예이다. 그런데 ggplot2에서는 기본적으로 좌표축 레이블과 색상의 범례 레이블로, 좌표축과 색상에 매핑된 열의 이름을 사용한다. 그리고 그래프에 제목은 달지 않는다.\n\nggplot(mpg, aes(x = displ, y = hwy, color = class)) + geom_point()\n\n\n\n\n만약 자동으로 부여된 레이블이 마음에 들지 않으면 이를 labs() 함수로 변경할 수 있다. 위 그래프에서 다음처럼 범례 이름, 축의 이름 한글로 바꾸고, 그래프의 제목도 달아 보자.\n\nggplot(mpg, aes(x = displ, y = hwy, color = class)) +\n  geom_point() +\n  labs(title = '배기량과 고속도로 연비 산점도',\n       x = '배기량(리터)',\n       y = '고속도로 연비',\n       color = '자동차 종류')\n\n\n\n\nlabs() 함수는 ggplot2 그래프에 + 연산으로 결합하여 사용되면, 그래픽 속성 매핑에 사용된 x, y, color 인수에 사용할 이름을 지정하면 된다. 그래프의 제목을 지정하려면 title이라는 인수를 사용한다."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Profile",
    "section": "",
    "text": "한남대학교 비즈니스통계학과 | 2018.03 ~ 2021.02\n한남대학교 빅데이터응용학과 | 2021.03 ~ 2024.02 \n\n\n\n\n\n과학기술정보통신부 장관상 | 2022.09\n한국수자원공사 사장상 | 2022.10\n지역사회 문제해결형 빅데이터/AI활용 공모전 동상 | 2023.12 \n\n\n\n\n\n데이터 청년 캠퍼스 | 2022.08 ~ 2022.09\n빅리더 AI 산학협력 | 2022.08 ~ 2022.10\n멋쟁이사자처럼 11기 | 2023.03 ~ 2023.12\n태블로 신병훈련소 21기 | 2023.10 ~ 2023.11\n\n\n\n\n\n데이터 분석 준전문가 (ADsP) | 2023.06\nSQL 개발자 (SQLD) | 2023.10\n빅데이터분석기사_필기 (BAE) | 2023.10"
  },
  {
    "objectID": "index.html#hyunsoo-kim",
    "href": "index.html#hyunsoo-kim",
    "title": "Profile",
    "section": "",
    "text": "한남대학교 비즈니스통계학과 | 2018.03 ~ 2021.02\n한남대학교 빅데이터응용학과 | 2021.03 ~ 2024.02 \n\n\n\n\n\n과학기술정보통신부 장관상 | 2022.09\n한국수자원공사 사장상 | 2022.10\n지역사회 문제해결형 빅데이터/AI활용 공모전 동상 | 2023.12 \n\n\n\n\n\n데이터 청년 캠퍼스 | 2022.08 ~ 2022.09\n빅리더 AI 산학협력 | 2022.08 ~ 2022.10\n멋쟁이사자처럼 11기 | 2023.03 ~ 2023.12\n태블로 신병훈련소 21기 | 2023.10 ~ 2023.11\n\n\n\n\n\n데이터 분석 준전문가 (ADsP) | 2023.06\nSQL 개발자 (SQLD) | 2023.10\n빅데이터분석기사_필기 (BAE) | 2023.10"
  },
  {
    "objectID": "Machine_Learning/02_end_to_end_machine_learning_project.html",
    "href": "Machine_Learning/02_end_to_end_machine_learning_project.html",
    "title": "02_end_to_end_machine_learning_project",
    "section": "",
    "text": "2장 – 머신러닝 프로젝트 처음부터 끝까지\n머신러닝 부동산 회사에 오신 것을 환영합니다! 여러분이 할 작업은 캘리포니아 지역 주택의 여러 특성을 사용해 중간 가격을 예측하는 것입니다.\n이 노트북은 2장의 모든 샘플 코드와 연습 문제 정답을 담고 있습니다."
  },
  {
    "objectID": "Machine_Learning/02_end_to_end_machine_learning_project.html#데이터-다운로드하기",
    "href": "Machine_Learning/02_end_to_end_machine_learning_project.html#데이터-다운로드하기",
    "title": "02_end_to_end_machine_learning_project",
    "section": "데이터 다운로드하기",
    "text": "데이터 다운로드하기\n\nimport os\nimport tarfile\nimport urllib.request\n\nDOWNLOAD_ROOT = \"https://raw.githubusercontent.com/rickiepark/handson-ml2/master/\"\nHOUSING_PATH = os.path.join(\"datasets\", \"housing\")\nHOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n\ndef fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n    if not os.path.isdir(housing_path):\n        os.makedirs(housing_path)\n    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n    urllib.request.urlretrieve(housing_url, tgz_path)\n    housing_tgz = tarfile.open(tgz_path)\n    housing_tgz.extractall(path=housing_path)\n    housing_tgz.close()\n\n\nfetch_housing_data()\n\n\nimport pandas as pd\n\ndef load_housing_data(housing_path=HOUSING_PATH):\n    csv_path = os.path.join(housing_path, \"housing.csv\")\n    return pd.read_csv(csv_path)"
  },
  {
    "objectID": "Machine_Learning/02_end_to_end_machine_learning_project.html#데이터-구조-훑어-보기",
    "href": "Machine_Learning/02_end_to_end_machine_learning_project.html#데이터-구조-훑어-보기",
    "title": "02_end_to_end_machine_learning_project",
    "section": "데이터 구조 훑어 보기",
    "text": "데이터 구조 훑어 보기\n\nhousing = load_housing_data()\nhousing.head()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n0\n-122.23\n37.88\n41.0\n880.0\n129.0\n322.0\n126.0\n8.3252\n452600.0\nNEAR BAY\n\n\n1\n-122.22\n37.86\n21.0\n7099.0\n1106.0\n2401.0\n1138.0\n8.3014\n358500.0\nNEAR BAY\n\n\n2\n-122.24\n37.85\n52.0\n1467.0\n190.0\n496.0\n177.0\n7.2574\n352100.0\nNEAR BAY\n\n\n3\n-122.25\n37.85\n52.0\n1274.0\n235.0\n558.0\n219.0\n5.6431\n341300.0\nNEAR BAY\n\n\n4\n-122.25\n37.85\n52.0\n1627.0\n280.0\n565.0\n259.0\n3.8462\n342200.0\nNEAR BAY\n\n\n\n\n\n\n\n\nhousing.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   longitude           20640 non-null  float64\n 1   latitude            20640 non-null  float64\n 2   housing_median_age  20640 non-null  float64\n 3   total_rooms         20640 non-null  float64\n 4   total_bedrooms      20433 non-null  float64\n 5   population          20640 non-null  float64\n 6   households          20640 non-null  float64\n 7   median_income       20640 non-null  float64\n 8   median_house_value  20640 non-null  float64\n 9   ocean_proximity     20640 non-null  object \ndtypes: float64(9), object(1)\nmemory usage: 1.6+ MB\n\n\n\nhousing[\"ocean_proximity\"].value_counts()\n\n&lt;1H OCEAN     9136\nINLAND        6551\nNEAR OCEAN    2658\nNEAR BAY      2290\nISLAND           5\nName: ocean_proximity, dtype: int64\n\n\n\nhousing.describe()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\n\n\n\n\ncount\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20433.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n\n\nmean\n-119.569704\n35.631861\n28.639486\n2635.763081\n537.870553\n1425.476744\n499.539680\n3.870671\n206855.816909\n\n\nstd\n2.003532\n2.135952\n12.585558\n2181.615252\n421.385070\n1132.462122\n382.329753\n1.899822\n115395.615874\n\n\nmin\n-124.350000\n32.540000\n1.000000\n2.000000\n1.000000\n3.000000\n1.000000\n0.499900\n14999.000000\n\n\n25%\n-121.800000\n33.930000\n18.000000\n1447.750000\n296.000000\n787.000000\n280.000000\n2.563400\n119600.000000\n\n\n50%\n-118.490000\n34.260000\n29.000000\n2127.000000\n435.000000\n1166.000000\n409.000000\n3.534800\n179700.000000\n\n\n75%\n-118.010000\n37.710000\n37.000000\n3148.000000\n647.000000\n1725.000000\n605.000000\n4.743250\n264725.000000\n\n\nmax\n-114.310000\n41.950000\n52.000000\n39320.000000\n6445.000000\n35682.000000\n6082.000000\n15.000100\n500001.000000\n\n\n\n\n\n\n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nhousing.hist(bins=50, figsize=(20,15))\nsave_fig(\"attribute_histogram_plots\")\nplt.show()\n\n그림 저장: attribute_histogram_plots"
  },
  {
    "objectID": "Machine_Learning/02_end_to_end_machine_learning_project.html#테스트-세트-만들기",
    "href": "Machine_Learning/02_end_to_end_machine_learning_project.html#테스트-세트-만들기",
    "title": "02_end_to_end_machine_learning_project",
    "section": "테스트 세트 만들기",
    "text": "테스트 세트 만들기\n\n# 노트북의 실행 결과가 동일하도록\nnp.random.seed(42)\n\n\nimport numpy as np\n\n# 예시로 만든 것입니다. 실전에서는 사이킷런의 train_test_split()를 사용하세요.\ndef split_train_test(data, test_ratio):\n    shuffled_indices = np.random.permutation(len(data))\n    test_set_size = int(len(data) * test_ratio)\n    test_indices = shuffled_indices[:test_set_size]\n    train_indices = shuffled_indices[test_set_size:]\n    return data.iloc[train_indices], data.iloc[test_indices]\n\n\ntrain_set, test_set = split_train_test(housing, 0.2)\nlen(train_set)\n\n16512\n\n\n\nlen(test_set)\n\n4128\n\n\n\nfrom zlib import crc32\n\ndef test_set_check(identifier, test_ratio):\n    return crc32(np.int64(identifier)) & 0xffffffff &lt; test_ratio * 2**32\n\ndef split_train_test_by_id(data, test_ratio, id_column):\n    ids = data[id_column]\n    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n    return data.loc[~in_test_set], data.loc[in_test_set]\n\n위의 test_set_check() 함수가 파이썬 2와 파이썬 3에서 모두 잘 동작합니다. 초판에서는 모든 해시 함수를 지원하는 다음 방식을 제안했지만 느리고 파이썬 2를 지원하지 않습니다.\n\nimport hashlib\n\ndef test_set_check(identifier, test_ratio, hash=hashlib.md5):\n    return hash(np.int64(identifier)).digest()[-1] &lt; 256 * test_ratio\n\n모든 해시 함수를 지원하고 파이썬 2와 파이썬 3에서 사용할 수 있는 함수를 원한다면 다음을 사용하세요.\n\ndef test_set_check(identifier, test_ratio, hash=hashlib.md5):\n    return bytearray(hash(np.int64(identifier)).digest())[-1] &lt; 256 * test_ratio\n\n\nhousing_with_id = housing.reset_index()   # `index` 열이 추가된 데이터프레임을 반환합니다\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")\n\n\nhousing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\")\n\n\ntest_set.head()\n\n\n\n\n\n\n\n\nindex\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\nid\n\n\n\n\n8\n8\n-122.26\n37.84\n42.0\n2555.0\n665.0\n1206.0\n595.0\n2.0804\n226700.0\nNEAR BAY\n-122222.16\n\n\n10\n10\n-122.26\n37.85\n52.0\n2202.0\n434.0\n910.0\n402.0\n3.2031\n281500.0\nNEAR BAY\n-122222.15\n\n\n11\n11\n-122.26\n37.85\n52.0\n3503.0\n752.0\n1504.0\n734.0\n3.2705\n241800.0\nNEAR BAY\n-122222.15\n\n\n12\n12\n-122.26\n37.85\n52.0\n2491.0\n474.0\n1098.0\n468.0\n3.0750\n213500.0\nNEAR BAY\n-122222.15\n\n\n13\n13\n-122.26\n37.84\n52.0\n696.0\n191.0\n345.0\n174.0\n2.6736\n191300.0\nNEAR BAY\n-122222.16\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n\n\ntest_set.head()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n20046\n-119.01\n36.06\n25.0\n1505.0\nNaN\n1392.0\n359.0\n1.6812\n47700.0\nINLAND\n\n\n3024\n-119.46\n35.14\n30.0\n2943.0\nNaN\n1565.0\n584.0\n2.5313\n45800.0\nINLAND\n\n\n15663\n-122.44\n37.80\n52.0\n3830.0\nNaN\n1310.0\n963.0\n3.4801\n500001.0\nNEAR BAY\n\n\n20484\n-118.72\n34.28\n17.0\n3051.0\nNaN\n1705.0\n495.0\n5.7376\n218600.0\n&lt;1H OCEAN\n\n\n9814\n-121.93\n36.62\n34.0\n2351.0\nNaN\n1063.0\n428.0\n3.7250\n278000.0\nNEAR OCEAN\n\n\n\n\n\n\n\n\nhousing[\"median_income\"].hist()\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x1ac9356f850&gt;\n\n\n\n\n\n\nhousing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n                               labels=[1, 2, 3, 4, 5])\n\n\nhousing[\"income_cat\"].value_counts()\n\n3    7236\n2    6581\n4    3639\n5    2362\n1     822\nName: income_cat, dtype: int64\n\n\n\nhousing[\"income_cat\"].hist()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]\n\n\nstrat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)\n\n3    0.350533\n2    0.318798\n4    0.176357\n5    0.114583\n1    0.039729\nName: income_cat, dtype: float64\n\n\n\nhousing[\"income_cat\"].value_counts() / len(housing)\n\n3    0.350581\n2    0.318847\n4    0.176308\n5    0.114438\n1    0.039826\nName: income_cat, dtype: float64\n\n\n\ndef income_cat_proportions(data):\n    return data[\"income_cat\"].value_counts() / len(data)\n\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n\ncompare_props = pd.DataFrame({\n    \"Overall\": income_cat_proportions(housing),\n    \"Stratified\": income_cat_proportions(strat_test_set),\n    \"Random\": income_cat_proportions(test_set),\n}).sort_index()\ncompare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] / compare_props[\"Overall\"] - 100\ncompare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 100\n\n\ncompare_props\n\n\n\n\n\n\n\n\nOverall\nStratified\nRandom\nRand. %error\nStrat. %error\n\n\n\n\n1\n0.039826\n0.039729\n0.040213\n0.973236\n-0.243309\n\n\n2\n0.318847\n0.318798\n0.324370\n1.732260\n-0.015195\n\n\n3\n0.350581\n0.350533\n0.358527\n2.266446\n-0.013820\n\n\n4\n0.176308\n0.176357\n0.167393\n-5.056334\n0.027480\n\n\n5\n0.114438\n0.114583\n0.109496\n-4.318374\n0.127011\n\n\n\n\n\n\n\n\nfor set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"income_cat\", axis=1, inplace=True)"
  },
  {
    "objectID": "Machine_Learning/02_end_to_end_machine_learning_project.html#지리적-데이터-시각화",
    "href": "Machine_Learning/02_end_to_end_machine_learning_project.html#지리적-데이터-시각화",
    "title": "02_end_to_end_machine_learning_project",
    "section": "지리적 데이터 시각화",
    "text": "지리적 데이터 시각화\n\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\nsave_fig(\"bad_visualization_plot\")\n\n그림 저장: bad_visualization_plot\n\n\n\n\n\n\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)\nsave_fig(\"better_visualization_plot\")\n\n그림 저장: better_visualization_plot\n\n\n\n\n\nsharex=False 매개변수는 x-축의 값과 범례를 표시하지 못하는 버그를 수정합니다. 이는 임시 방편입니다(https://github.com/pandas-dev/pandas/issues/10611 참조). 수정 사항을 알려준 Wilmer Arellano에게 감사합니다.\n\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n             s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n             c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n             sharex=False)\nplt.legend()\nsave_fig(\"housing_prices_scatterplot\")\n\n그림 저장: housing_prices_scatterplot\n\n\n\n\n\n\n# Download the California image\nimages_path = os.path.join(PROJECT_ROOT_DIR, \"images\", \"end_to_end_project\")\nos.makedirs(images_path, exist_ok=True)\nDOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\nfilename = \"california.png\"\nprint(\"Downloading\", filename)\nurl = DOWNLOAD_ROOT + \"images/end_to_end_project/\" + filename\nurllib.request.urlretrieve(url, os.path.join(images_path, filename))\n\nDownloading california.png\n\n\n('./images/end_to_end_project/california.png',\n &lt;http.client.HTTPMessage at 0x7f707866c3c8&gt;)\n\n\n\nimport matplotlib.image as mpimg\ncalifornia_img=mpimg.imread(os.path.join(images_path, filename))\nax = housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", figsize=(10,7),\n                  s=housing['population']/100, label=\"Population\",\n                  c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"),\n                  colorbar=False, alpha=0.4)\nplt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5,\n           cmap=plt.get_cmap(\"jet\"))\nplt.ylabel(\"Latitude\", fontsize=14)\nplt.xlabel(\"Longitude\", fontsize=14)\n\nprices = housing[\"median_house_value\"]\ntick_values = np.linspace(prices.min(), prices.max(), 11)\ncbar = plt.colorbar(ticks=tick_values/prices.max())\ncbar.ax.set_yticklabels([\"$%dk\"%(round(v/1000)) for v in tick_values], fontsize=14)\ncbar.set_label('Median House Value', fontsize=16)\n\nplt.legend(fontsize=16)\nsave_fig(\"california_housing_prices_plot\")\nplt.show()\n\n그림 저장: california_housing_prices_plot"
  },
  {
    "objectID": "Machine_Learning/02_end_to_end_machine_learning_project.html#상관관계-조사",
    "href": "Machine_Learning/02_end_to_end_machine_learning_project.html#상관관계-조사",
    "title": "02_end_to_end_machine_learning_project",
    "section": "상관관계 조사",
    "text": "상관관계 조사\n\ncorr_matrix = housing.corr()\n\n\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)\n\nmedian_house_value    1.000000\nmedian_income         0.687151\ntotal_rooms           0.135140\nhousing_median_age    0.114146\nhouseholds            0.064590\ntotal_bedrooms        0.047781\npopulation           -0.026882\nlongitude            -0.047466\nlatitude             -0.142673\nName: median_house_value, dtype: float64\n\n\n피어슨의 상관 계수(위키백과): \n\n# from pandas.tools.plotting import scatter_matrix # 옛날 버전의 판다스에서는\nfrom pandas.plotting import scatter_matrix\n\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n              \"housing_median_age\"]\nscatter_matrix(housing[attributes], figsize=(12, 8))\nsave_fig(\"scatter_matrix_plot\")\n\n그림 저장: scatter_matrix_plot\n\n\n\n\n\n\nhousing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n             alpha=0.1)\nplt.axis([0, 16, 0, 550000])\nsave_fig(\"income_vs_house_value_scatterplot\")\n\n그림 저장: income_vs_house_value_scatterplot"
  },
  {
    "objectID": "Machine_Learning/02_end_to_end_machine_learning_project.html#특성-조합으로-실험",
    "href": "Machine_Learning/02_end_to_end_machine_learning_project.html#특성-조합으로-실험",
    "title": "02_end_to_end_machine_learning_project",
    "section": "특성 조합으로 실험",
    "text": "특성 조합으로 실험\n\nhousing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\nhousing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]\n\n\ncorr_matrix = housing.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)\n\nmedian_house_value          1.000000\nmedian_income               0.687151\nrooms_per_household         0.146255\ntotal_rooms                 0.135140\nhousing_median_age          0.114146\nhouseholds                  0.064590\ntotal_bedrooms              0.047781\npopulation_per_household   -0.021991\npopulation                 -0.026882\nlongitude                  -0.047466\nlatitude                   -0.142673\nbedrooms_per_room          -0.259952\nName: median_house_value, dtype: float64\n\n\n\nhousing.plot(kind=\"scatter\", x=\"rooms_per_household\", y=\"median_house_value\",\n             alpha=0.2)\nplt.show()\n\n\n\n\n\nhousing.describe()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nrooms_per_household\nbedrooms_per_room\npopulation_per_household\n\n\n\n\ncount\n16512.000000\n16512.000000\n16512.000000\n16512.000000\n16354.000000\n16512.000000\n16512.000000\n16512.000000\n16512.000000\n16512.000000\n16354.000000\n16512.000000\n\n\nmean\n-119.575635\n35.639314\n28.653404\n2622.539789\n534.914639\n1419.687379\n497.011810\n3.875884\n207005.322372\n5.440406\n0.212873\n3.096469\n\n\nstd\n2.001828\n2.137963\n12.574819\n2138.417080\n412.665649\n1115.663036\n375.696156\n1.904931\n115701.297250\n2.611696\n0.057378\n11.584825\n\n\nmin\n-124.350000\n32.540000\n1.000000\n6.000000\n2.000000\n3.000000\n2.000000\n0.499900\n14999.000000\n1.130435\n0.100000\n0.692308\n\n\n25%\n-121.800000\n33.940000\n18.000000\n1443.000000\n295.000000\n784.000000\n279.000000\n2.566950\n119800.000000\n4.442168\n0.175304\n2.431352\n\n\n50%\n-118.510000\n34.260000\n29.000000\n2119.000000\n433.000000\n1164.000000\n408.000000\n3.541550\n179500.000000\n5.232342\n0.203027\n2.817661\n\n\n75%\n-118.010000\n37.720000\n37.000000\n3141.000000\n644.000000\n1719.000000\n602.000000\n4.745325\n263900.000000\n6.056361\n0.239816\n3.281420\n\n\nmax\n-114.310000\n41.950000\n52.000000\n39320.000000\n6210.000000\n35682.000000\n5358.000000\n15.000100\n500001.000000\n141.909091\n1.000000\n1243.333333"
  },
  {
    "objectID": "Machine_Learning/02_end_to_end_machine_learning_project.html#데이터-정제",
    "href": "Machine_Learning/02_end_to_end_machine_learning_project.html#데이터-정제",
    "title": "02_end_to_end_machine_learning_project",
    "section": "데이터 정제",
    "text": "데이터 정제\n책에 소개된 세 개의 옵션은 다음과 같습니다:\nhousing.dropna(subset=[\"total_bedrooms\"])    # 옵션 1\nhousing.drop(\"total_bedrooms\", axis=1)       # 옵션 2\nmedian = housing[\"total_bedrooms\"].median()  # 옵션 3\nhousing[\"total_bedrooms\"].fillna(median, inplace=True)\n각 옵션을 설명하기 위해 주택 데이터셋의 복사본을 만듭니다. 이 때 적어도 하나의 열이 비어 있는 행만 고릅니다. 이렇게 하면 각 옵션의 정확한 동작을 눈으로 쉽게 확인할 수 있습니다.\n\nsample_incomplete_rows = housing[housing.isnull().any(axis=1)].head()\nsample_incomplete_rows\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nocean_proximity\n\n\n\n\n1606\n-122.08\n37.88\n26.0\n2947.0\nNaN\n825.0\n626.0\n2.9330\nNEAR BAY\n\n\n10915\n-117.87\n33.73\n45.0\n2264.0\nNaN\n1970.0\n499.0\n3.4193\n&lt;1H OCEAN\n\n\n19150\n-122.70\n38.35\n14.0\n2313.0\nNaN\n954.0\n397.0\n3.7813\n&lt;1H OCEAN\n\n\n4186\n-118.23\n34.13\n48.0\n1308.0\nNaN\n835.0\n294.0\n4.2891\n&lt;1H OCEAN\n\n\n16885\n-122.40\n37.58\n26.0\n3281.0\nNaN\n1145.0\n480.0\n6.3580\nNEAR OCEAN\n\n\n\n\n\n\n\n\nsample_incomplete_rows.dropna(subset=[\"total_bedrooms\"])    # 옵션 1\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nocean_proximity\n\n\n\n\n\n\n\n\n\n\nsample_incomplete_rows.drop(\"total_bedrooms\", axis=1)       # 옵션 2\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\npopulation\nhouseholds\nmedian_income\nocean_proximity\n\n\n\n\n1606\n-122.08\n37.88\n26.0\n2947.0\n825.0\n626.0\n2.9330\nNEAR BAY\n\n\n10915\n-117.87\n33.73\n45.0\n2264.0\n1970.0\n499.0\n3.4193\n&lt;1H OCEAN\n\n\n19150\n-122.70\n38.35\n14.0\n2313.0\n954.0\n397.0\n3.7813\n&lt;1H OCEAN\n\n\n4186\n-118.23\n34.13\n48.0\n1308.0\n835.0\n294.0\n4.2891\n&lt;1H OCEAN\n\n\n16885\n-122.40\n37.58\n26.0\n3281.0\n1145.0\n480.0\n6.3580\nNEAR OCEAN\n\n\n\n\n\n\n\n\nmedian = housing[\"total_bedrooms\"].median()\nsample_incomplete_rows[\"total_bedrooms\"].fillna(median, inplace=True) # 옵션 3\n\n\nsample_incomplete_rows\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nocean_proximity\n\n\n\n\n1606\n-122.08\n37.88\n26.0\n2947.0\n433.0\n825.0\n626.0\n2.9330\nNEAR BAY\n\n\n10915\n-117.87\n33.73\n45.0\n2264.0\n433.0\n1970.0\n499.0\n3.4193\n&lt;1H OCEAN\n\n\n19150\n-122.70\n38.35\n14.0\n2313.0\n433.0\n954.0\n397.0\n3.7813\n&lt;1H OCEAN\n\n\n4186\n-118.23\n34.13\n48.0\n1308.0\n433.0\n835.0\n294.0\n4.2891\n&lt;1H OCEAN\n\n\n16885\n-122.40\n37.58\n26.0\n3281.0\n433.0\n1145.0\n480.0\n6.3580\nNEAR OCEAN\n\n\n\n\n\n\n\n\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy=\"median\")\n\n중간값이 수치형 특성에서만 계산될 수 있기 때문에 텍스트 특성을 삭제합니다:\n\nhousing_num = housing.drop(\"ocean_proximity\", axis=1)\n# 다른 방법: housing_num = housing.select_dtypes(include=[np.number])\n\n\nimputer.fit(housing_num)\n\nSimpleImputer(strategy='median')\n\n\n\nimputer.statistics_\n\narray([-118.51   ,   34.26   ,   29.     , 2119.     ,  433.     ,\n       1164.     ,  408.     ,    3.54155])\n\n\n각 특성의 중간 값이 수동으로 계산한 것과 같은지 확인해 보세요:\n\nhousing_num.median().values\n\narray([-118.51   ,   34.26   ,   29.     , 2119.     ,  433.     ,\n       1164.     ,  408.     ,    3.54155])\n\n\n훈련 세트를 변환합니다:\n\nX = imputer.transform(housing_num)\n\n\nhousing_tr = pd.DataFrame(X, columns=housing_num.columns,\n                          index=housing_num.index)\n\n\nhousing_tr.loc[sample_incomplete_rows.index.values]\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\n\n\n\n\n1606\n-122.08\n37.88\n26.0\n2947.0\n433.0\n825.0\n626.0\n2.9330\n\n\n10915\n-117.87\n33.73\n45.0\n2264.0\n433.0\n1970.0\n499.0\n3.4193\n\n\n19150\n-122.70\n38.35\n14.0\n2313.0\n433.0\n954.0\n397.0\n3.7813\n\n\n4186\n-118.23\n34.13\n48.0\n1308.0\n433.0\n835.0\n294.0\n4.2891\n\n\n16885\n-122.40\n37.58\n26.0\n3281.0\n433.0\n1145.0\n480.0\n6.3580\n\n\n\n\n\n\n\n\nimputer.strategy\n\n'median'\n\n\n\nhousing_tr.head()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\n\n\n\n\n12655\n-121.46\n38.52\n29.0\n3873.0\n797.0\n2237.0\n706.0\n2.1736\n\n\n15502\n-117.23\n33.09\n7.0\n5320.0\n855.0\n2015.0\n768.0\n6.3373\n\n\n2908\n-119.04\n35.37\n44.0\n1618.0\n310.0\n667.0\n300.0\n2.8750\n\n\n14053\n-117.13\n32.75\n24.0\n1877.0\n519.0\n898.0\n483.0\n2.2264\n\n\n20496\n-118.70\n34.28\n27.0\n3536.0\n646.0\n1837.0\n580.0\n4.4964"
  },
  {
    "objectID": "Machine_Learning/02_end_to_end_machine_learning_project.html#텍스트와-범주형-특성-다루기",
    "href": "Machine_Learning/02_end_to_end_machine_learning_project.html#텍스트와-범주형-특성-다루기",
    "title": "02_end_to_end_machine_learning_project",
    "section": "텍스트와 범주형 특성 다루기",
    "text": "텍스트와 범주형 특성 다루기\n이제 범주형 입력 특성인 ocean_proximity을 전처리합니다:\n\nhousing_cat = housing[[\"ocean_proximity\"]]\nhousing_cat.head(10)\n\n\n\n\n\n\n\n\nocean_proximity\n\n\n\n\n12655\nINLAND\n\n\n15502\nNEAR OCEAN\n\n\n2908\nINLAND\n\n\n14053\nNEAR OCEAN\n\n\n20496\n&lt;1H OCEAN\n\n\n1481\nNEAR BAY\n\n\n18125\n&lt;1H OCEAN\n\n\n5830\n&lt;1H OCEAN\n\n\n17989\n&lt;1H OCEAN\n\n\n4861\n&lt;1H OCEAN\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import OrdinalEncoder\n\nordinal_encoder = OrdinalEncoder()\nhousing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\nhousing_cat_encoded[:10]\n\narray([[1.],\n       [4.],\n       [1.],\n       [4.],\n       [0.],\n       [3.],\n       [0.],\n       [0.],\n       [0.],\n       [0.]])\n\n\n\nordinal_encoder.categories_\n\n[array(['&lt;1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\n       dtype=object)]\n\n\n\nfrom sklearn.preprocessing import OneHotEncoder\n\ncat_encoder = OneHotEncoder()\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\nhousing_cat_1hot\n\n&lt;16512x5 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 16512 stored elements in Compressed Sparse Row format&gt;\n\n\nOneHotEncoder는 기본적으로 희소 행렬을 반환합니다. 필요하면 toarray() 메서드를 사용해 밀집 배열로 변환할 수 있습니다:\n\nhousing_cat_1hot.toarray()\n\narray([[0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 1.],\n       [0., 1., 0., 0., 0.],\n       ...,\n       [1., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0.]])\n\n\n또는 OneHotEncoder를 만들 때 sparse=False로 지정할 수 있습니다:\n\ncat_encoder = OneHotEncoder(sparse=False)\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\nhousing_cat_1hot\n\narray([[0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 1.],\n       [0., 1., 0., 0., 0.],\n       ...,\n       [1., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0.]])\n\n\n\ncat_encoder.categories_\n\n[array(['&lt;1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\n       dtype=object)]"
  },
  {
    "objectID": "Machine_Learning/02_end_to_end_machine_learning_project.html#나만의-변환기",
    "href": "Machine_Learning/02_end_to_end_machine_learning_project.html#나만의-변환기",
    "title": "02_end_to_end_machine_learning_project",
    "section": "나만의 변환기",
    "text": "나만의 변환기\n추가 특성을 위해 사용자 정의 변환기를 만들어보죠:\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# 열 인덱스\nrooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room=True): # *args 또는 **kargs 없음\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    def fit(self, X, y=None):\n        return self  # 아무것도 하지 않습니다\n    def transform(self, X):\n        rooms_per_household = X[:, rooms_ix]CombinedAttributesAdde / X[:, households_ix]\n        population_per_household = X[:, population_ix] / X[:, households_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household,\n                         bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\n\nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nhousing_extra_attribs = attr_adder.transform(housing.to_numpy())\n\n책에서는 간단하게 인덱스 (3, 4, 5, 6)을 하드코딩했지만 다음처럼 동적으로 처리하는 것이 더 좋습니다:\n\ncol_names = \"total_rooms\", \"total_bedrooms\", \"population\", \"households\"\nrooms_ix, bedrooms_ix, population_ix, households_ix = [\n    housing.columns.get_loc(c) for c in col_names] # 열 인덱스 구하기\n\n또한 housing_extra_attribs는 넘파이 배열이기 때문에 열 이름이 없습니다(안타깝지만 사이킷런을 사용할 때 생기는 문제입니다). DataFrame으로 복원하려면 다음과 같이 할 수 있습니다:\n\nhousing_extra_attribs = pd.DataFrame(\n    housing_extra_attribs,\n    columns=list(housing.columns)+[\"rooms_per_household\", \"population_per_household\"],\n    index=housing.index)\nhousing_extra_attribs.head()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nocean_proximity\nrooms_per_household\npopulation_per_household\n\n\n\n\n12655\n-121.46\n38.52\n29.0\n3873.0\n797.0\n2237.0\n706.0\n2.1736\nINLAND\n5.485836\n3.168555\n\n\n15502\n-117.23\n33.09\n7.0\n5320.0\n855.0\n2015.0\n768.0\n6.3373\nNEAR OCEAN\n6.927083\n2.623698\n\n\n2908\n-119.04\n35.37\n44.0\n1618.0\n310.0\n667.0\n300.0\n2.875\nINLAND\n5.393333\n2.223333\n\n\n14053\n-117.13\n32.75\n24.0\n1877.0\n519.0\n898.0\n483.0\n2.2264\nNEAR OCEAN\n3.886128\n1.859213\n\n\n20496\n-118.7\n34.28\n27.0\n3536.0\n646.0\n1837.0\n580.0\n4.4964\n&lt;1H OCEAN\n6.096552\n3.167241"
  },
  {
    "objectID": "Machine_Learning/02_end_to_end_machine_learning_project.html#변환-파이프라인",
    "href": "Machine_Learning/02_end_to_end_machine_learning_project.html#변환-파이프라인",
    "title": "02_end_to_end_machine_learning_project",
    "section": "변환 파이프라인",
    "text": "변환 파이프라인\n수치형 특성을 전처리하기 위해 파이프라인을 만듭니다:\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('attribs_adder', CombinedAttributesAdder()),\n        ('std_scaler', StandardScaler()),\n    ])\n\nhousing_num_tr = num_pipeline.fit_transform(housing_num)\n\n\nhousing_num_tr\n\narray([[-0.94135046,  1.34743822,  0.02756357, ...,  0.01739526,\n         0.00622264, -0.12112176],\n       [ 1.17178212, -1.19243966, -1.72201763, ...,  0.56925554,\n        -0.04081077, -0.81086696],\n       [ 0.26758118, -0.1259716 ,  1.22045984, ..., -0.01802432,\n        -0.07537122, -0.33827252],\n       ...,\n       [-1.5707942 ,  1.31001828,  1.53856552, ..., -0.5092404 ,\n        -0.03743619,  0.32286937],\n       [-1.56080303,  1.2492109 , -1.1653327 , ...,  0.32814891,\n        -0.05915604, -0.45702273],\n       [-1.28105026,  2.02567448, -0.13148926, ...,  0.01407228,\n         0.00657083, -0.12169672]])\n\n\n\nfrom sklearn.compose import ColumnTransformer\n\nnum_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\n\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, num_attribs),\n        (\"cat\", OneHotEncoder(), cat_attribs),\n    ])\n\nhousing_prepared = full_pipeline.fit_transform(housing)\n\n\nhousing_prepared\n\narray([[-0.94135046,  1.34743822,  0.02756357, ...,  0.        ,\n         0.        ,  0.        ],\n       [ 1.17178212, -1.19243966, -1.72201763, ...,  0.        ,\n         0.        ,  1.        ],\n       [ 0.26758118, -0.1259716 ,  1.22045984, ...,  0.        ,\n         0.        ,  0.        ],\n       ...,\n       [-1.5707942 ,  1.31001828,  1.53856552, ...,  0.        ,\n         0.        ,  0.        ],\n       [-1.56080303,  1.2492109 , -1.1653327 , ...,  0.        ,\n         0.        ,  0.        ],\n       [-1.28105026,  2.02567448, -0.13148926, ...,  0.        ,\n         0.        ,  0.        ]])\n\n\n\nhousing_prepared.shape\n\n(16512, 16)\n\n\n다음은 (판다스 DataFrame 열의 일부를 선택하기 위해) DataFrameSelector 변환기와 FeatureUnion를 사용한 예전 방식입니다:\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# 수치형 열과 범주형 열을 선택하기 위한 클래스\nclass OldDataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names].values\n\n하나의 큰 파이프라인에 이들을 모두 결합하여 수치형과 범주형 특성을 전처리합니다:\n\nnum_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\n\nold_num_pipeline = Pipeline([\n        ('selector', OldDataFrameSelector(num_attribs)),\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('attribs_adder', CombinedAttributesAdder()),\n        ('std_scaler', StandardScaler()),\n    ])\n\nold_cat_pipeline = Pipeline([\n        ('selector', OldDataFrameSelector(cat_attribs)),\n        ('cat_encoder', OneHotEncoder(sparse=False)),\n    ])\n\n\nfrom sklearn.pipeline import FeatureUnion\n\nold_full_pipeline = FeatureUnion(transformer_list=[\n        (\"num_pipeline\", old_num_pipeline),\n        (\"cat_pipeline\", old_cat_pipeline),\n    ])\n\n\nold_housing_prepared = old_full_pipeline.fit_transform(housing)\nold_housing_prepared\n\narray([[-0.94135046,  1.34743822,  0.02756357, ...,  0.        ,\n         0.        ,  0.        ],\n       [ 1.17178212, -1.19243966, -1.72201763, ...,  0.        ,\n         0.        ,  1.        ],\n       [ 0.26758118, -0.1259716 ,  1.22045984, ...,  0.        ,\n         0.        ,  0.        ],\n       ...,\n       [-1.5707942 ,  1.31001828,  1.53856552, ...,  0.        ,\n         0.        ,  0.        ],\n       [-1.56080303,  1.2492109 , -1.1653327 , ...,  0.        ,\n         0.        ,  0.        ],\n       [-1.28105026,  2.02567448, -0.13148926, ...,  0.        ,\n         0.        ,  0.        ]])\n\n\nColumnTransformer의 결과와 동일합니다:\n\nnp.allclose(housing_prepared, old_housing_prepared)\n\nTrue"
  },
  {
    "objectID": "Machine_Learning/02_end_to_end_machine_learning_project.html#훈련-세트에서-훈련하고-평가하기",
    "href": "Machine_Learning/02_end_to_end_machine_learning_project.html#훈련-세트에서-훈련하고-평가하기",
    "title": "02_end_to_end_machine_learning_project",
    "section": "훈련 세트에서 훈련하고 평가하기",
    "text": "훈련 세트에서 훈련하고 평가하기\n\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)\n\nLinearRegression()\n\n\n\n# 훈련 샘플 몇 개를 사용해 전체 파이프라인을 적용해 보겠습니다\nsome_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\n\nprint(\"예측:\", lin_reg.predict(some_data_prepared))\n\n예측: [ 85657.90192014 305492.60737488 152056.46122456 186095.70946094\n 244550.67966089]\n\n\n실제 값과 비교합니다:\n\nprint(\"레이블:\", list(some_labels))\n\n레이블: [72100.0, 279600.0, 82700.0, 112500.0, 238300.0]\n\n\n\nsome_data_prepared\n\narray([[-0.94135046,  1.34743822,  0.02756357,  0.58477745,  0.64037127,\n         0.73260236,  0.55628602, -0.8936472 ,  0.01739526,  0.00622264,\n        -0.12112176,  0.        ,  1.        ,  0.        ,  0.        ,\n         0.        ],\n       [ 1.17178212, -1.19243966, -1.72201763,  1.26146668,  0.78156132,\n         0.53361152,  0.72131799,  1.292168  ,  0.56925554, -0.04081077,\n        -0.81086696,  0.        ,  0.        ,  0.        ,  0.        ,\n         1.        ],\n       [ 0.26758118, -0.1259716 ,  1.22045984, -0.46977281, -0.54513828,\n        -0.67467519, -0.52440722, -0.52543365, -0.01802432, -0.07537122,\n        -0.33827252,  0.        ,  1.        ,  0.        ,  0.        ,\n         0.        ],\n       [ 1.22173797, -1.35147437, -0.37006852, -0.34865152, -0.03636724,\n        -0.46761716, -0.03729672, -0.86592882, -0.59513997, -0.10680295,\n         0.96120521,  0.        ,  0.        ,  0.        ,  0.        ,\n         1.        ],\n       [ 0.43743108, -0.63581817, -0.13148926,  0.42717947,  0.27279028,\n         0.37406031,  0.22089846,  0.32575178,  0.2512412 ,  0.00610923,\n        -0.47451338,  1.        ,  0.        ,  0.        ,  0.        ,\n         0.        ]])\n\n\n\nfrom sklearn.metrics import mean_squared_error\n\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse\n\n68627.87390018745\n\n\n노트: 사이킷런 0.22 버전부터는 squared=False 매개변수로 mean_squared_error() 함수를 호출하면 RMSE를 바로 얻을 수 있습니다.\n\nfrom sklearn.metrics import mean_absolute_error\n\nlin_mae = mean_absolute_error(housing_labels, housing_predictions)\nlin_mae\n\n49438.66860915802\n\n\n\nfrom sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor(random_state=42)\ntree_reg.fit(housing_prepared, housing_labels)\n\nDecisionTreeRegressor(random_state=42)\n\n\n\nhousing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse\n\n0.0"
  },
  {
    "objectID": "Machine_Learning/02_end_to_end_machine_learning_project.html#교차-검증을-사용한-평가",
    "href": "Machine_Learning/02_end_to_end_machine_learning_project.html#교차-검증을-사용한-평가",
    "title": "02_end_to_end_machine_learning_project",
    "section": "교차 검증을 사용한 평가",
    "text": "교차 검증을 사용한 평가\n\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n                         scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)\n\n\ndef display_scores(scores):\n    print(\"점수:\", scores)\n    print(\"평균:\", scores.mean())\n    print(\"표준 편차:\", scores.std())\n\ndisplay_scores(tree_rmse_scores)\n\n점수: [72831.45749112 69973.18438322 69528.56551415 72517.78229792\n 69145.50006909 79094.74123727 68960.045444   73344.50225684\n 69826.02473916 71077.09753998]\n평균: 71629.89009727491\n표준 편차: 2914.035468468928\n\n\n\nlin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n                             scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)\n\n점수: [71762.76364394 64114.99166359 67771.17124356 68635.19072082\n 66846.14089488 72528.03725385 73997.08050233 68802.33629334\n 66443.28836884 70139.79923956]\n평균: 69104.07998247063\n표준 편차: 2880.3282098180694\n\n\n노트: 사이킷런 0.22 버전에서 n_estimators의 기본값이 100으로 바뀌기 때문에 향후를 위해 n_estimators=100로 지정합니다(책에는 등장하지 않습니다).\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\nforest_reg.fit(housing_prepared, housing_labels)\n\nRandomForestRegressor(random_state=42)\n\n\n\nhousing_predictions = forest_reg.predict(housing_prepared)\nforest_mse = mean_squared_error(housing_labels, housing_predictions)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse\n\n18650.698705770003\n\n\n\nfrom sklearn.model_selection import cross_val_score\n\nforest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n                                scoring=\"neg_mean_squared_error\", cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\ndisplay_scores(forest_rmse_scores)\n\n점수: [51559.63379638 48737.57100062 47210.51269766 51875.21247297\n 47577.50470123 51863.27467888 52746.34645573 50065.1762751\n 48664.66818196 54055.90894609]\n평균: 50435.58092066179\n표준 편차: 2203.3381412764606\n\n\n\nscores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\npd.Series(np.sqrt(-scores)).describe()\n\ncount       10.000000\nmean     69104.079982\nstd       3036.132517\nmin      64114.991664\n25%      67077.398482\n50%      68718.763507\n75%      71357.022543\nmax      73997.080502\ndtype: float64\n\n\n\nfrom sklearn.svm import SVR\n\nsvm_reg = SVR(kernel=\"linear\")\nsvm_reg.fit(housing_prepared, housing_labels)\nhousing_predictions = svm_reg.predict(housing_prepared)\nsvm_mse = mean_squared_error(housing_labels, housing_predictions)\nsvm_rmse = np.sqrt(svm_mse)\nsvm_rmse\n\n111095.06635291968"
  },
  {
    "objectID": "Machine_Learning/02_end_to_end_machine_learning_project.html#그리드-탐색",
    "href": "Machine_Learning/02_end_to_end_machine_learning_project.html#그리드-탐색",
    "title": "02_end_to_end_machine_learning_project",
    "section": "그리드 탐색",
    "text": "그리드 탐색\n\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    # 12(=3×4)개의 하이퍼파라미터 조합을 시도합니다.\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n    # bootstrap은 False로 하고 6(=2×3)개의 조합을 시도합니다.\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n  ]\n\nforest_reg = RandomForestRegressor(random_state=42)\n# 다섯 개의 폴드로 훈련하면 총 (12+6)*5=90번의 훈련이 일어납니다.\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\ngrid_search.fit(housing_prepared, housing_labels)\n\nGridSearchCV(cv=5, estimator=RandomForestRegressor(random_state=42),\n             param_grid=[{'max_features': [2, 4, 6, 8],\n                          'n_estimators': [3, 10, 30]},\n                         {'bootstrap': [False], 'max_features': [2, 3, 4],\n                          'n_estimators': [3, 10]}],\n             return_train_score=True, scoring='neg_mean_squared_error')\n\n\n최상의 파라미터 조합은 다음과 같습니다:\n\ngrid_search.best_params_\n\n{'max_features': 8, 'n_estimators': 30}\n\n\n\ngrid_search.best_estimator_\n\nRandomForestRegressor(max_features=8, n_estimators=30, random_state=42)\n\n\n그리드서치에서 테스트한 하이퍼파라미터 조합의 점수를 확인합니다:\n\ncvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)\n\n63895.161577951665 {'max_features': 2, 'n_estimators': 3}\n54916.32386349543 {'max_features': 2, 'n_estimators': 10}\n52885.86715332332 {'max_features': 2, 'n_estimators': 30}\n60075.3680329983 {'max_features': 4, 'n_estimators': 3}\n52495.01284985185 {'max_features': 4, 'n_estimators': 10}\n50187.24324926565 {'max_features': 4, 'n_estimators': 30}\n58064.73529982314 {'max_features': 6, 'n_estimators': 3}\n51519.32062366315 {'max_features': 6, 'n_estimators': 10}\n49969.80441627874 {'max_features': 6, 'n_estimators': 30}\n58895.824998155826 {'max_features': 8, 'n_estimators': 3}\n52459.79624724529 {'max_features': 8, 'n_estimators': 10}\n49898.98913455217 {'max_features': 8, 'n_estimators': 30}\n62381.765106921855 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\n54476.57050944266 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\n59974.60028085155 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\n52754.5632813202 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\n57831.136061214274 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\n51278.37877140253 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\n\n\n\npd.DataFrame(grid_search.cv_results_)\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_max_features\nparam_n_estimators\nparam_bootstrap\nparams\nsplit0_test_score\nsplit1_test_score\n...\nmean_test_score\nstd_test_score\nrank_test_score\nsplit0_train_score\nsplit1_train_score\nsplit2_train_score\nsplit3_train_score\nsplit4_train_score\nmean_train_score\nstd_train_score\n\n\n\n\n0\n0.067164\n0.000877\n0.004181\n0.000047\n2\n3\nNaN\n{'max_features': 2, 'n_estimators': 3}\n-4.119912e+09\n-3.723465e+09\n...\n-4.082592e+09\n1.867375e+08\n18\n-1.155630e+09\n-1.089726e+09\n-1.153843e+09\n-1.118149e+09\n-1.093446e+09\n-1.122159e+09\n2.834288e+07\n\n\n1\n0.219164\n0.001392\n0.011773\n0.000110\n2\n10\nNaN\n{'max_features': 2, 'n_estimators': 10}\n-2.973521e+09\n-2.810319e+09\n...\n-3.015803e+09\n1.139808e+08\n11\n-5.982947e+08\n-5.904781e+08\n-6.123850e+08\n-5.727681e+08\n-5.905210e+08\n-5.928894e+08\n1.284978e+07\n\n\n2\n0.657425\n0.003093\n0.033022\n0.000531\n2\n30\nNaN\n{'max_features': 2, 'n_estimators': 30}\n-2.801229e+09\n-2.671474e+09\n...\n-2.796915e+09\n7.980892e+07\n9\n-4.412567e+08\n-4.326398e+08\n-4.553722e+08\n-4.320746e+08\n-4.311606e+08\n-4.385008e+08\n9.184397e+06\n\n\n3\n0.113365\n0.001084\n0.004352\n0.000104\n4\n3\nNaN\n{'max_features': 4, 'n_estimators': 3}\n-3.528743e+09\n-3.490303e+09\n...\n-3.609050e+09\n1.375683e+08\n16\n-9.782368e+08\n-9.806455e+08\n-1.003780e+09\n-1.016515e+09\n-1.011270e+09\n-9.980896e+08\n1.577372e+07\n\n\n4\n0.364473\n0.002683\n0.011766\n0.000099\n4\n10\nNaN\n{'max_features': 4, 'n_estimators': 10}\n-2.742620e+09\n-2.609311e+09\n...\n-2.755726e+09\n1.182604e+08\n7\n-5.063215e+08\n-5.257983e+08\n-5.081984e+08\n-5.174405e+08\n-5.282066e+08\n-5.171931e+08\n8.882622e+06\n\n\n5\n1.094907\n0.004319\n0.032924\n0.000188\n4\n30\nNaN\n{'max_features': 4, 'n_estimators': 30}\n-2.522176e+09\n-2.440241e+09\n...\n-2.518759e+09\n8.488084e+07\n3\n-3.776568e+08\n-3.902106e+08\n-3.885042e+08\n-3.830866e+08\n-3.894779e+08\n-3.857872e+08\n4.774229e+06\n\n\n6\n0.149577\n0.001430\n0.004289\n0.000041\n6\n3\nNaN\n{'max_features': 6, 'n_estimators': 3}\n-3.362127e+09\n-3.311863e+09\n...\n-3.371513e+09\n1.378086e+08\n13\n-8.909397e+08\n-9.583733e+08\n-9.000201e+08\n-8.964731e+08\n-9.151927e+08\n-9.121998e+08\n2.444837e+07\n\n\n7\n0.502295\n0.002231\n0.011696\n0.000079\n6\n10\nNaN\n{'max_features': 6, 'n_estimators': 10}\n-2.622099e+09\n-2.669655e+09\n...\n-2.654240e+09\n6.967978e+07\n5\n-4.939906e+08\n-5.145996e+08\n-5.023512e+08\n-4.959467e+08\n-5.147087e+08\n-5.043194e+08\n8.880106e+06\n\n\n8\n1.533953\n0.009125\n0.033168\n0.000290\n6\n30\nNaN\n{'max_features': 6, 'n_estimators': 30}\n-2.446142e+09\n-2.446594e+09\n...\n-2.496981e+09\n7.357046e+07\n2\n-3.760968e+08\n-3.876636e+08\n-3.875307e+08\n-3.760938e+08\n-3.861056e+08\n-3.826981e+08\n5.418747e+06\n\n\n9\n0.200251\n0.004116\n0.004347\n0.000070\n8\n3\nNaN\n{'max_features': 8, 'n_estimators': 3}\n-3.590333e+09\n-3.232664e+09\n...\n-3.468718e+09\n1.293758e+08\n14\n-9.505012e+08\n-9.166119e+08\n-9.033910e+08\n-9.070642e+08\n-9.459386e+08\n-9.247014e+08\n1.973471e+07\n\n\n10\n0.659325\n0.003863\n0.011855\n0.000197\n8\n10\nNaN\n{'max_features': 8, 'n_estimators': 10}\n-2.721311e+09\n-2.675886e+09\n...\n-2.752030e+09\n6.258030e+07\n6\n-4.998373e+08\n-4.997970e+08\n-5.099880e+08\n-5.047868e+08\n-5.348043e+08\n-5.098427e+08\n1.303601e+07\n\n\n11\n1.977234\n0.008652\n0.032824\n0.000151\n8\n30\nNaN\n{'max_features': 8, 'n_estimators': 30}\n-2.492636e+09\n-2.444818e+09\n...\n-2.489909e+09\n7.086483e+07\n1\n-3.801679e+08\n-3.832972e+08\n-3.823818e+08\n-3.778452e+08\n-3.817589e+08\n-3.810902e+08\n1.916605e+06\n\n\n12\n0.104956\n0.001028\n0.005049\n0.000138\n2\n3\nFalse\n{'bootstrap': False, 'max_features': 2, 'n_est...\n-4.020842e+09\n-3.951861e+09\n...\n-3.891485e+09\n8.648595e+07\n17\n-0.000000e+00\n-4.306828e+01\n-1.051392e+04\n-0.000000e+00\n-0.000000e+00\n-2.111398e+03\n4.201294e+03\n\n\n13\n0.348037\n0.004065\n0.014048\n0.000118\n2\n10\nFalse\n{'bootstrap': False, 'max_features': 2, 'n_est...\n-2.901352e+09\n-3.036875e+09\n...\n-2.967697e+09\n4.582448e+07\n10\n-0.000000e+00\n-3.876145e+00\n-9.462528e+02\n-0.000000e+00\n-0.000000e+00\n-1.900258e+02\n3.781165e+02\n\n\n14\n0.141391\n0.002086\n0.005016\n0.000045\n3\n3\nFalse\n{'bootstrap': False, 'max_features': 3, 'n_est...\n-3.687132e+09\n-3.446245e+09\n...\n-3.596953e+09\n8.011960e+07\n15\n-0.000000e+00\n-0.000000e+00\n-0.000000e+00\n-0.000000e+00\n-0.000000e+00\n0.000000e+00\n0.000000e+00\n\n\n15\n0.465390\n0.003114\n0.014140\n0.000077\n3\n10\nFalse\n{'bootstrap': False, 'max_features': 3, 'n_est...\n-2.837028e+09\n-2.619558e+09\n...\n-2.783044e+09\n8.862580e+07\n8\n-0.000000e+00\n-0.000000e+00\n-0.000000e+00\n-0.000000e+00\n-0.000000e+00\n0.000000e+00\n0.000000e+00\n\n\n16\n0.181031\n0.001810\n0.005012\n0.000027\n4\n3\nFalse\n{'bootstrap': False, 'max_features': 4, 'n_est...\n-3.549428e+09\n-3.318176e+09\n...\n-3.344440e+09\n1.099355e+08\n12\n-0.000000e+00\n-0.000000e+00\n-0.000000e+00\n-0.000000e+00\n-0.000000e+00\n0.000000e+00\n0.000000e+00\n\n\n17\n0.589645\n0.005947\n0.014093\n0.000140\n4\n10\nFalse\n{'bootstrap': False, 'max_features': 4, 'n_est...\n-2.692499e+09\n-2.542704e+09\n...\n-2.629472e+09\n8.510266e+07\n4\n-0.000000e+00\n-0.000000e+00\n-0.000000e+00\n-0.000000e+00\n-0.000000e+00\n0.000000e+00\n0.000000e+00\n\n\n\n\n18 rows × 23 columns"
  },
  {
    "objectID": "Machine_Learning/02_end_to_end_machine_learning_project.html#랜덤-탐색",
    "href": "Machine_Learning/02_end_to_end_machine_learning_project.html#랜덤-탐색",
    "title": "02_end_to_end_machine_learning_project",
    "section": "랜덤 탐색",
    "text": "랜덤 탐색\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_distribs = {\n        'n_estimators': randint(low=1, high=200),\n        'max_features': randint(low=1, high=8),\n    }\n\nforest_reg = RandomForestRegressor(random_state=42)\nrnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\nrnd_search.fit(housing_prepared, housing_labels)\n\nRandomizedSearchCV(cv=5, estimator=RandomForestRegressor(random_state=42),\n                   param_distributions={'max_features': &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7f7071466e48&gt;,\n                                        'n_estimators': &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7f7071e76780&gt;},\n                   random_state=42, scoring='neg_mean_squared_error')\n\n\n\ncvres = rnd_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)\n\n49117.55344336652 {'max_features': 7, 'n_estimators': 180}\n51450.63202856348 {'max_features': 5, 'n_estimators': 15}\n50692.53588182537 {'max_features': 3, 'n_estimators': 72}\n50783.614493515 {'max_features': 5, 'n_estimators': 21}\n49162.89877456354 {'max_features': 7, 'n_estimators': 122}\n50655.798471042704 {'max_features': 3, 'n_estimators': 75}\n50513.856319990606 {'max_features': 3, 'n_estimators': 88}\n49521.17201976928 {'max_features': 5, 'n_estimators': 100}\n50302.90440763418 {'max_features': 3, 'n_estimators': 150}\n65167.02018649492 {'max_features': 5, 'n_estimators': 2}"
  },
  {
    "objectID": "Machine_Learning/02_end_to_end_machine_learning_project.html#최상의-모델과-오차-분석",
    "href": "Machine_Learning/02_end_to_end_machine_learning_project.html#최상의-모델과-오차-분석",
    "title": "02_end_to_end_machine_learning_project",
    "section": "최상의 모델과 오차 분석",
    "text": "최상의 모델과 오차 분석\n\nfeature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances\n\narray([6.96542523e-02, 6.04213840e-02, 4.21882202e-02, 1.52450557e-02,\n       1.55545295e-02, 1.58491147e-02, 1.49346552e-02, 3.79009225e-01,\n       5.47789150e-02, 1.07031322e-01, 4.82031213e-02, 6.79266007e-03,\n       1.65706303e-01, 7.83480660e-05, 1.52473276e-03, 3.02816106e-03])\n\n\n\nextra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n#cat_encoder = cat_pipeline.named_steps[\"cat_encoder\"] # 예전 방식\ncat_encoder = full_pipeline.named_transformers_[\"cat\"]\ncat_one_hot_attribs = list(cat_encoder.categories_[0])\nattributes = num_attribs + extra_attribs + cat_one_hot_attribs\nsorted(zip(feature_importances, attributes), reverse=True)\n\n[(0.3790092248170967, 'median_income'),\n (0.16570630316895876, 'INLAND'),\n (0.10703132208204354, 'pop_per_hhold'),\n (0.06965425227942929, 'longitude'),\n (0.0604213840080722, 'latitude'),\n (0.054778915018283726, 'rooms_per_hhold'),\n (0.048203121338269206, 'bedrooms_per_room'),\n (0.04218822024391753, 'housing_median_age'),\n (0.015849114744428634, 'population'),\n (0.015554529490469328, 'total_bedrooms'),\n (0.01524505568840977, 'total_rooms'),\n (0.014934655161887776, 'households'),\n (0.006792660074259966, '&lt;1H OCEAN'),\n (0.0030281610628962747, 'NEAR OCEAN'),\n (0.0015247327555504937, 'NEAR BAY'),\n (7.834806602687504e-05, 'ISLAND')]"
  },
  {
    "objectID": "Machine_Learning/02_end_to_end_machine_learning_project.html#테스트-세트로-시스템-평가하기",
    "href": "Machine_Learning/02_end_to_end_machine_learning_project.html#테스트-세트로-시스템-평가하기",
    "title": "02_end_to_end_machine_learning_project",
    "section": "테스트 세트로 시스템 평가하기",
    "text": "테스트 세트로 시스템 평가하기\n\nfinal_model = grid_search.best_estimator_\n\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\n\n\nfinal_rmse\n\n47873.26095812988\n\n\n테스트 RMSE에 대한 95% 신뢰 구간을 계산할 수 있습니다:\n\nfrom scipy import stats\n\nconfidence = 0.95\nsquared_errors = (final_predictions - y_test) ** 2\nnp.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n                         loc=squared_errors.mean(),\n                         scale=stats.sem(squared_errors)))\n\narray([45893.36082829, 49774.46796717])\n\n\n다음과 같이 수동으로 계산할 수도 있습니다:\n\nm = len(squared_errors)\nmean = squared_errors.mean()\ntscore = stats.t.ppf((1 + confidence) / 2, df=m - 1)\ntmargin = tscore * squared_errors.std(ddof=1) / np.sqrt(m)\nnp.sqrt(mean - tmargin), np.sqrt(mean + tmargin)\n\n(45893.360828285535, 49774.46796717361)\n\n\n또는 t-점수 대신 z-점수를 사용할 수도 있습니다:\n\nzscore = stats.norm.ppf((1 + confidence) / 2)\nzmargin = zscore * squared_errors.std(ddof=1) / np.sqrt(m)\nnp.sqrt(mean - zmargin), np.sqrt(mean + zmargin)\n\n(45893.9540110131, 49773.921030650374)"
  },
  {
    "objectID": "Machine_Learning/02_end_to_end_machine_learning_project.html#전처리와-예측을-포함한-전체-파이프라인",
    "href": "Machine_Learning/02_end_to_end_machine_learning_project.html#전처리와-예측을-포함한-전체-파이프라인",
    "title": "02_end_to_end_machine_learning_project",
    "section": "전처리와 예측을 포함한 전체 파이프라인",
    "text": "전처리와 예측을 포함한 전체 파이프라인\n\nfull_pipeline_with_predictor = Pipeline([\n        (\"preparation\", full_pipeline),\n        (\"linear\", LinearRegression())\n    ])\n\nfull_pipeline_with_predictor.fit(housing, housing_labels)\nfull_pipeline_with_predictor.predict(some_data)\n\narray([ 85657.90192014, 305492.60737488, 152056.46122456, 186095.70946094,\n       244550.67966089])"
  },
  {
    "objectID": "Machine_Learning/02_end_to_end_machine_learning_project.html#joblib를-사용한-모델-저장",
    "href": "Machine_Learning/02_end_to_end_machine_learning_project.html#joblib를-사용한-모델-저장",
    "title": "02_end_to_end_machine_learning_project",
    "section": "joblib를 사용한 모델 저장",
    "text": "joblib를 사용한 모델 저장\n\nmy_model = full_pipeline_with_predictor\n\n\nimport joblib\njoblib.dump(my_model, \"my_model.pkl\") # DIFF\n#...\nmy_model_loaded = joblib.load(\"my_model.pkl\") # DIFF"
  },
  {
    "objectID": "Machine_Learning/02_end_to_end_machine_learning_project.html#randomizedsearchcv를-위한-scipy-분포-함수",
    "href": "Machine_Learning/02_end_to_end_machine_learning_project.html#randomizedsearchcv를-위한-scipy-분포-함수",
    "title": "02_end_to_end_machine_learning_project",
    "section": "RandomizedSearchCV를 위한 Scipy 분포 함수",
    "text": "RandomizedSearchCV를 위한 Scipy 분포 함수\n\nfrom scipy.stats import geom, expon\ngeom_distrib=geom(0.5).rvs(10000, random_state=42)\nexpon_distrib=expon(scale=1).rvs(10000, random_state=42)\nplt.hist(geom_distrib, bins=50)\nplt.show()\nplt.hist(expon_distrib, bins=50)\nplt.show()"
  },
  {
    "objectID": "Machine_Learning/02_end_to_end_machine_learning_project.html#section",
    "href": "Machine_Learning/02_end_to_end_machine_learning_project.html#section",
    "title": "02_end_to_end_machine_learning_project",
    "section": "1.",
    "text": "1.\n질문: 서포트 벡터 머신 회귀(sklearn.svm.SVR)를 kernel=“linear”(하이퍼파라미터 C를 바꿔가며)나 kernel=“rbf”(하이퍼파라미터 C와 gamma를 바꿔가며) 등의 다양한 하이퍼파라미터 설정으로 시도해보세요. 지금은 이 하이퍼파라미터가 무엇을 의미하는지 너무 신경 쓰지 마세요. 최상의 SVR 모델은 무엇인가요?\n경고: 사용하는 하드웨어에 따라 다음 셀을 실행하는데 30분 또는 그 이상 걸릴 수 있습니다.\n\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n        {'kernel': ['linear'], 'C': [10., 30., 100., 300., 1000., 3000., 10000., 30000.0]},\n        {'kernel': ['rbf'], 'C': [1.0, 3.0, 10., 30., 100., 300., 1000.0],\n         'gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]},\n    ]\n\nsvm_reg = SVR()\ngrid_search = GridSearchCV(svm_reg, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2)\ngrid_search.fit(housing_prepared, housing_labels)\n\n최상 모델의 (5-폴드 교차 검증으로 평가한) 점수는 다음과 같습니다:\n\nnegative_mse = grid_search.best_score_\nrmse = np.sqrt(-negative_mse)\nrmse\n\n70286.61835383571\n\n\nRandomForestRegressor보다 훨씬 좋지 않네요. 최상의 하이퍼파라미터를 확인해 보겠습니다:\n\ngrid_search.best_params_\n\n{'C': 30000.0, 'kernel': 'linear'}\n\n\n선형 커널이 RBF 커널보다 성능이 나은 것 같습니다. C는 테스트한 것 중에 최대값이 선택되었습니다. 따라서 (작은 값들은 지우고) 더 큰 값의 C로 그리드서치를 다시 실행해 보아야 합니다. 아마도 더 큰 값의 C에서 성능이 높아질 것입니다."
  },
  {
    "objectID": "Machine_Learning/02_end_to_end_machine_learning_project.html#section-1",
    "href": "Machine_Learning/02_end_to_end_machine_learning_project.html#section-1",
    "title": "02_end_to_end_machine_learning_project",
    "section": "2.",
    "text": "2.\n질문: GridSearchCV를 RandomizedSearchCV로 바꿔보세요.\n경고: 사용하는 하드웨어에 따라 다음 셀을 실행하는데 45분 또는 그 이상 걸릴 수 있습니다.\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import expon, reciprocal\n\n# expon(), reciprocal()와 그외 다른 확률 분포 함수에 대해서는\n# https://docs.scipy.org/doc/scipy/reference/stats.html를 참고하세요.\n\n# 노트: kernel 매개변수가 \"linear\"일 때는 gamma가 무시됩니다.\nparam_distribs = {\n        'kernel': ['linear', 'rbf'],\n        'C': reciprocal(20, 200000),\n        'gamma': expon(scale=1.0),\n    }\n\nsvm_reg = SVR()\nrnd_search = RandomizedSearchCV(svm_reg, param_distributions=param_distribs,\n                                n_iter=50, cv=5, scoring='neg_mean_squared_error',\n                                verbose=2, random_state=42)\nrnd_search.fit(housing_prepared, housing_labels)\n\n최상 모델의 (5-폴드 교차 검증으로 평가한) 점수는 다음과 같습니다:\n\nnegative_mse = rnd_search.best_score_\nrmse = np.sqrt(-negative_mse)\nrmse\n\n54751.69009488048\n\n\n이제 RandomForestRegressor의 성능에 훨씬 가까워졌습니다(하지만 아직 차이가 납니다). 최상의 하이퍼파라미터를 확인해 보겠습니다:\n\nrnd_search.best_params_\n\n{'C': 157055.10989448498, 'gamma': 0.26497040005002437, 'kernel': 'rbf'}\n\n\n이번에는 RBF 커널에 대해 최적의 하이퍼파라미터 조합을 찾았습니다. 보통 랜덤서치가 같은 시간안에 그리드서치보다 더 좋은 하이퍼파라미터를 찾습니다.\n여기서 사용된 scale=1.0인 지수 분포를 살펴보겠습니다. 일부 샘플은 1.0보다 아주 크거나 작습니다. 하지만 로그 분포를 보면 대부분의 값이 exp(-2)와 exp(+2), 즉 0.1과 7.4 사이에 집중되어 있음을 알 수 있습니다.\n\nexpon_distrib = expon(scale=1.)\nsamples = expon_distrib.rvs(10000, random_state=42)\nplt.figure(figsize=(10, 4))\nplt.subplot(121)\nplt.title(\"Exponential distribution (scale=1.0)\")\nplt.hist(samples, bins=50)\nplt.subplot(122)\nplt.title(\"Log of this distribution\")\nplt.hist(np.log(samples), bins=50)\nplt.show()\n\n\n\n\nC에 사용된 분포는 매우 다릅니다. 주어진 범위안에서 균등 분포로 샘플링됩니다. 그래서 오른쪽 로그 분포가 거의 일정하게 나타납니다. 이런 분포는 원하는 스케일이 정확이 무엇인지 모를 때 사용하면 좋습니다:\n\nreciprocal_distrib = reciprocal(20, 200000)\nsamples = reciprocal_distrib.rvs(10000, random_state=42)\nplt.figure(figsize=(10, 4))\nplt.subplot(121)\nplt.title(\"Reciprocal distribution (scale=1.0)\")\nplt.hist(samples, bins=50)\nplt.subplot(122)\nplt.title(\"Log of this distribution\")\nplt.hist(np.log(samples), bins=50)\nplt.show()\n\n\n\n\nreciprocal() 함수는 하이퍼파라미터의 스케일에 대해 전혀 감을 잡을 수 없을 때 사용합니다(오른쪽 그래프에서 볼 수 있듯이 주어진 범위안에서 모든 값이 균등합니다). 반면 지수 분포는 하이퍼파라미터의 스케일을 (어느정도) 알고 있을 때 사용하는 것이 좋습니다."
  },
  {
    "objectID": "Machine_Learning/02_end_to_end_machine_learning_project.html#section-2",
    "href": "Machine_Learning/02_end_to_end_machine_learning_project.html#section-2",
    "title": "02_end_to_end_machine_learning_project",
    "section": "3.",
    "text": "3.\n질문: 가장 중요한 특성을 선택하는 변환기를 준비 파이프라인에 추가해보세요.\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\ndef indices_of_top_k(arr, k):\n    return np.sort(np.argpartition(np.array(arr), -k)[-k:])\n\nclass TopFeatureSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, feature_importances, k):\n        self.feature_importances = feature_importances\n        self.k = k\n    def fit(self, X, y=None):\n        self.feature_indices_ = indices_of_top_k(self.feature_importances, self.k)\n        return self\n    def transform(self, X):\n        return X[:, self.feature_indices_]\n\n노트: 이 특성 선택 클래스는 이미 어떤 식으로든 특성 중요도를 계산했다고 가정합니다(가령 RandomForestRegressor을 사용하여). TopFeatureSelector의 fit() 메서드에서 직접 계산할 수도 있지만 (캐싱을 사용하지 않을 경우) 그리드서치나 랜덤서치의 모든 하이퍼파라미터 조합에 대해 계산이 일어나기 때문에 매우 느려집니다.\n선택할 특성의 개수를 지정합니다:\n\nk = 5\n\n최상의 k개 특성의 인덱스를 확인해 보겠습니다:\n\ntop_k_feature_indices = indices_of_top_k(feature_importances, k)\ntop_k_feature_indices\n\narray([ 0,  1,  7,  9, 12])\n\n\n\nnp.array(attributes)[top_k_feature_indices]\n\narray(['longitude', 'latitude', 'median_income', 'pop_per_hhold',\n       'INLAND'], dtype='&lt;U18')\n\n\n최상의 k개 특성이 맞는지 다시 확인합니다:\n\nsorted(zip(feature_importances, attributes), reverse=True)[:k]\n\n[(0.3790092248170967, 'median_income'),\n (0.16570630316895876, 'INLAND'),\n (0.10703132208204354, 'pop_per_hhold'),\n (0.06965425227942929, 'longitude'),\n (0.0604213840080722, 'latitude')]\n\n\n좋습니다. 이제 이전에 정의한 준비 파이프라인과 특성 선택기를 추가한 새로운 파이프라인을 만듭니다:\n\npreparation_and_feature_selection_pipeline = Pipeline([\n    ('preparation', full_pipeline),\n    ('feature_selection', TopFeatureSelector(feature_importances, k))\n])\n\n\nhousing_prepared_top_k_features = preparation_and_feature_selection_pipeline.fit_transform(housing)\n\n처음 3개 샘플의 특성을 확인해 보겠습니다:\n\nhousing_prepared_top_k_features[0:3]\n\narray([[-0.94135046,  1.34743822, -0.8936472 ,  0.00622264,  1.        ],\n       [ 1.17178212, -1.19243966,  1.292168  , -0.04081077,  0.        ],\n       [ 0.26758118, -0.1259716 , -0.52543365, -0.07537122,  1.        ]])\n\n\n최상의 k개 특성이 맞는지 다시 확인합니다:\n\nhousing_prepared[0:3, top_k_feature_indices]\n\narray([[-0.94135046,  1.34743822, -0.8936472 ,  0.00622264,  1.        ],\n       [ 1.17178212, -1.19243966,  1.292168  , -0.04081077,  0.        ],\n       [ 0.26758118, -0.1259716 , -0.52543365, -0.07537122,  1.        ]])\n\n\n성공입니다! :)"
  },
  {
    "objectID": "Machine_Learning/02_end_to_end_machine_learning_project.html#section-3",
    "href": "Machine_Learning/02_end_to_end_machine_learning_project.html#section-3",
    "title": "02_end_to_end_machine_learning_project",
    "section": "4.",
    "text": "4.\n질문: 전체 데이터 준비 과정과 최종 예측을 하나의 파이프라인으로 만들어보세요.\n\nprepare_select_and_predict_pipeline = Pipeline([\n    ('preparation', full_pipeline),\n    ('feature_selection', TopFeatureSelector(feature_importances, k)),\n    ('svm_reg', SVR(**rnd_search.best_params_))\n])\n\n\nprepare_select_and_predict_pipeline.fit(housing, housing_labels)\n\nPipeline(steps=[('preparation',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('attribs_adder',\n                                                                   CombinedAttributesAdder()),\n                                                                  ('std_scaler',\n                                                                   StandardScaler())]),\n                                                  ['longitude', 'latitude',\n                                                   'housing_median_age',\n                                                   'total_rooms',\n                                                   'total_bedrooms',\n                                                   'population', 'households',\n                                                   'median_income']),\n                                                 ('cat', OneHotEncoder(...\n                 TopFeatureSelector(feature_importances=array([6.96542523e-02, 6.04213840e-02, 4.21882202e-02, 1.52450557e-02,\n       1.55545295e-02, 1.58491147e-02, 1.49346552e-02, 3.79009225e-01,\n       5.47789150e-02, 1.07031322e-01, 4.82031213e-02, 6.79266007e-03,\n       1.65706303e-01, 7.83480660e-05, 1.52473276e-03, 3.02816106e-03]),\n                                    k=5)),\n                ('svm_reg',\n                 SVR(C=157055.10989448498, gamma=0.26497040005002437))])\n\n\n몇 개의 샘플에 전체 파이프라인을 적용해 보겠습니다:\n\nsome_data = housing.iloc[:4]\nsome_labels = housing_labels.iloc[:4]\n\nprint(\"Predictions:\\t\", prepare_select_and_predict_pipeline.predict(some_data))\nprint(\"Labels:\\t\\t\", list(some_labels))\n\nPredictions:     [ 83384.49158095 299407.90439234  92272.03345144 150173.16199041]\nLabels:      [72100.0, 279600.0, 82700.0, 112500.0]\n\n\n전체 파이프라인이 잘 작동하는 것 같습니다. 물론 예측 성능이 아주 좋지는 않습니다. SVR보다 RandomForestRegressor가 더 나은 것 같습니다."
  },
  {
    "objectID": "Machine_Learning/02_end_to_end_machine_learning_project.html#section-4",
    "href": "Machine_Learning/02_end_to_end_machine_learning_project.html#section-4",
    "title": "02_end_to_end_machine_learning_project",
    "section": "5.",
    "text": "5.\n질문: GridSearchCV를 사용해 준비 단계의 옵션을 자동으로 탐색해보세요.\n경고: 사용하는 하드웨어에 따라 다음 셀을 실행하는데 45분 또는 그 이상 걸릴 수 있습니다.\n노트: 아래 코드에서 훈련 도중 경고를 피하기 위해 OneHotEncoder의 handle_unknown 하이퍼파라미터를 'ignore'로 지정했습니다. 그렇지 않으면 OneHotEncoder는 기본적으로 handle_unkown='error'를 사용하기 때문에 데이터를 변활할 때 훈련할 때 없던 범주가 있으면 에러를 냅니다. 기본값을 사용하면 훈련 세트에 모든 카테고리가 들어 있지 않은 폴드를 평가할 때 GridSearchCV가 에러를 일으킵니다. 'ISLAND' 범주에는 샘플이 하나이기 때문에 일어날 가능성이 높습니다. 일부 폴드에서는 테스트 세트 안에 포함될 수 있습니다. 따라서 이런 폴드는 GridSearchCV에서 무시하여 피하는 것이 좋습니다.\n\nfull_pipeline.named_transformers_[\"cat\"].handle_unknown = 'ignore'\n\nparam_grid = [{\n    'preparation__num__imputer__strategy': ['mean', 'median', 'most_frequent'],\n    'feature_selection__k': list(range(1, len(feature_importances) + 1))\n}]\n\ngrid_search_prep = GridSearchCV(prepare_select_and_predict_pipeline, param_grid, cv=5,\n                                scoring='neg_mean_squared_error', verbose=2)\ngrid_search_prep.fit(housing, housing_labels)\n\n\ngrid_search_prep.best_params_\n\n{'feature_selection__k': 1, 'preparation__num__imputer__strategy': 'mean'}\n\n\n최상의 Imputer 정책은 most_frequent이고 거의 모든 특성이 유용합니다(16개 중 15개). 마지막 특성(ISLAND)은 잡음이 추가될 뿐입니다.\n축하합니다! 이제 머신러닝에 대해 꽤 많은 것을 알게 되었습니다. :)"
  },
  {
    "objectID": "Machine_Learning/04_training_linear_models.html",
    "href": "Machine_Learning/04_training_linear_models.html",
    "title": "04_training_linear_models",
    "section": "",
    "text": "4장 – 모델 훈련\n이 노트북은 4장에 있는 모든 샘플 코드와 연습문제 해답을 가지고 있습니다."
  },
  {
    "objectID": "Machine_Learning/04_training_linear_models.html#배치-경사-하강법",
    "href": "Machine_Learning/04_training_linear_models.html#배치-경사-하강법",
    "title": "04_training_linear_models",
    "section": "배치 경사 하강법",
    "text": "배치 경사 하강법\n식 4-6: 비용 함수의 그레이디언트 벡터\n$ () = ^T ( - ) $\n식 4-7: 경사 하강법의 스텝\n$ ^{()} = - () $\n\neta = 0.1  # 학습률\nn_iterations = 1000\nm = 100\n\ntheta = np.random.randn(2,1)  # 랜덤 초기화\n\nfor iteration in range(n_iterations):\n    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n    theta = theta - eta * gradients\n\n\ntheta\n\narray([[4.21509616],\n       [2.77011339]])\n\n\n\nX_new_b.dot(theta)\n\narray([[4.21509616],\n       [9.75532293]])\n\n\n\ntheta_path_bgd = []\n\ndef plot_gradient_descent(theta, eta, theta_path=None):\n    m = len(X_b)\n    plt.plot(X, y, \"b.\")\n    n_iterations = 1000\n    for iteration in range(n_iterations):\n        if iteration &lt; 10:\n            y_predict = X_new_b.dot(theta)\n            style = \"b-\" if iteration &gt; 0 else \"r--\"\n            plt.plot(X_new, y_predict, style)\n        gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n        theta = theta - eta * gradients\n        if theta_path is not None:\n            theta_path.append(theta)\n    plt.xlabel(\"$x_1$\", fontsize=18)\n    plt.axis([0, 2, 0, 15])\n    plt.title(r\"$\\eta = {}$\".format(eta), fontsize=16)\n\n\nnp.random.seed(42)\ntheta = np.random.randn(2,1)  # random initialization\n\nplt.figure(figsize=(10,4))\nplt.subplot(131); plot_gradient_descent(theta, eta=0.02)\nplt.ylabel(\"$y$\", rotation=0, fontsize=18)\nplt.subplot(132); plot_gradient_descent(theta, eta=0.1, theta_path=theta_path_bgd)\nplt.subplot(133); plot_gradient_descent(theta, eta=0.5)\n\nsave_fig(\"gradient_descent_plot\")\nplt.show()\n\n그림 저장: gradient_descent_plot"
  },
  {
    "objectID": "Machine_Learning/04_training_linear_models.html#확률적-경사-하강법",
    "href": "Machine_Learning/04_training_linear_models.html#확률적-경사-하강법",
    "title": "04_training_linear_models",
    "section": "확률적 경사 하강법",
    "text": "확률적 경사 하강법\n\ntheta_path_sgd = []\nm = len(X_b)\nnp.random.seed(42)\n\n\nn_epochs = 50\nt0, t1 = 5, 50  # 학습 스케줄 하이퍼파라미터\n\ndef learning_schedule(t):\n    return t0 / (t + t1)\n\ntheta = np.random.randn(2,1)  # 랜덤 초기화\n\nfor epoch in range(n_epochs):\n    for i in range(m):\n        if epoch == 0 and i &lt; 20:                    # 책에는 없음\n            y_predict = X_new_b.dot(theta)           # 책에는 없음\n            style = \"b-\" if i &gt; 0 else \"r--\"         # 책에는 없음\n            plt.plot(X_new, y_predict, style)        # 책에는 없음\n        random_index = np.random.randint(m)\n        xi = X_b[random_index:random_index+1]\n        yi = y[random_index:random_index+1]\n        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n        eta = learning_schedule(epoch * m + i)\n        theta = theta - eta * gradients\n        theta_path_sgd.append(theta)                 # 책에는 없음\n\nplt.plot(X, y, \"b.\")                                 # 책에는 없음\nplt.xlabel(\"$x_1$\", fontsize=18)                     # 책에는 없음\nplt.ylabel(\"$y$\", rotation=0, fontsize=18)           # 책에는 없음\nplt.axis([0, 2, 0, 15])                              # 책에는 없음\nsave_fig(\"sgd_plot\")                                 # 책에는 없음\nplt.show()                                           # 책에는 없음\n\n그림 저장: sgd_plot\n\n\n\n\n\n\ntheta\n\narray([[4.21076011],\n       [2.74856079]])\n\n\n\nfrom sklearn.linear_model import SGDRegressor\n\nsgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1, random_state=42)\nsgd_reg.fit(X, y.ravel())\n\nSGDRegressor(eta0=0.1, penalty=None, random_state=42)\n\n\n\nsgd_reg.intercept_, sgd_reg.coef_\n\n(array([4.24365286]), array([2.8250878]))"
  },
  {
    "objectID": "Machine_Learning/04_training_linear_models.html#미니배치-경사-하강법",
    "href": "Machine_Learning/04_training_linear_models.html#미니배치-경사-하강법",
    "title": "04_training_linear_models",
    "section": "미니배치 경사 하강법",
    "text": "미니배치 경사 하강법\n\ntheta_path_mgd = []\n\nn_iterations = 50\nminibatch_size = 20\n\nnp.random.seed(42)\ntheta = np.random.randn(2,1)  # 랜덤 초기화\n\nt0, t1 = 200, 1000\ndef learning_schedule(t):\n    return t0 / (t + t1)\n\nt = 0\nfor epoch in range(n_iterations):\n    shuffled_indices = np.random.permutation(m)\n    X_b_shuffled = X_b[shuffled_indices]\n    y_shuffled = y[shuffled_indices]\n    for i in range(0, m, minibatch_size):\n        t += 1\n        xi = X_b_shuffled[i:i+minibatch_size]\n        yi = y_shuffled[i:i+minibatch_size]\n        gradients = 2/minibatch_size * xi.T.dot(xi.dot(theta) - yi)\n        eta = learning_schedule(t)\n        theta = theta - eta * gradients\n        theta_path_mgd.append(theta)\n\n\ntheta\n\narray([[4.25214635],\n       [2.7896408 ]])\n\n\n\ntheta_path_bgd = np.array(theta_path_bgd)\ntheta_path_sgd = np.array(theta_path_sgd)\ntheta_path_mgd = np.array(theta_path_mgd)\n\n\nplt.figure(figsize=(7,4))\nplt.plot(theta_path_sgd[:, 0], theta_path_sgd[:, 1], \"r-s\", linewidth=1, label=\"Stochastic\")\nplt.plot(theta_path_mgd[:, 0], theta_path_mgd[:, 1], \"g-+\", linewidth=2, label=\"Mini-batch\")\nplt.plot(theta_path_bgd[:, 0], theta_path_bgd[:, 1], \"b-o\", linewidth=3, label=\"Batch\")\nplt.legend(loc=\"upper left\", fontsize=16)\nplt.xlabel(r\"$\\theta_0$\", fontsize=20)\nplt.ylabel(r\"$\\theta_1$   \", fontsize=20, rotation=0)\nplt.axis([2.5, 4.5, 2.3, 3.9])\nsave_fig(\"gradient_descent_paths_plot\")\nplt.show()\n\n그림 저장: gradient_descent_paths_plot"
  },
  {
    "objectID": "Machine_Learning/04_training_linear_models.html#릿지-회귀",
    "href": "Machine_Learning/04_training_linear_models.html#릿지-회귀",
    "title": "04_training_linear_models",
    "section": "릿지 회귀",
    "text": "릿지 회귀\n\nnp.random.seed(42)\nm = 20\nX = 3 * np.random.rand(m, 1)\ny = 1 + 0.5 * X + np.random.randn(m, 1) / 1.5\nX_new = np.linspace(0, 3, 100).reshape(100, 1)\n\n식 4-8: 릿지 회귀의 비용 함수\n$ J() = () + _{i=1}^{n}{_i}^2 $\n\nfrom sklearn.linear_model import Ridge\nridge_reg = Ridge(alpha=1, solver=\"cholesky\", random_state=42)\nridge_reg.fit(X, y)\nridge_reg.predict([[1.5]])\n\narray([[1.55071465]])\n\n\n\nridge_reg = Ridge(alpha=1, solver=\"sag\", random_state=42)\nridge_reg.fit(X, y)\nridge_reg.predict([[1.5]])\n\narray([[1.5507201]])\n\n\n\nfrom sklearn.linear_model import Ridge\n\ndef plot_model(model_class, polynomial, alphas, **model_kargs):\n    for alpha, style in zip(alphas, (\"b-\", \"g--\", \"r:\")):\n        model = model_class(alpha, **model_kargs) if alpha &gt; 0 else LinearRegression()\n        if polynomial:\n            model = Pipeline([\n                    (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n                    (\"std_scaler\", StandardScaler()),\n                    (\"regul_reg\", model),\n                ])\n        model.fit(X, y)\n        y_new_regul = model.predict(X_new)\n        lw = 2 if alpha &gt; 0 else 1\n        plt.plot(X_new, y_new_regul, style, linewidth=lw, label=r\"$\\alpha = {}$\".format(alpha))\n    plt.plot(X, y, \"b.\", linewidth=3)\n    plt.legend(loc=\"upper left\", fontsize=15)\n    plt.xlabel(\"$x_1$\", fontsize=18)\n    plt.axis([0, 3, 0, 4])\n\nplt.figure(figsize=(8,4))\nplt.subplot(121)\nplot_model(Ridge, polynomial=False, alphas=(0, 10, 100), random_state=42)\nplt.ylabel(\"$y$\", rotation=0, fontsize=18)\nplt.subplot(122)\nplot_model(Ridge, polynomial=True, alphas=(0, 10**-5, 1), random_state=42)\n\nsave_fig(\"ridge_regression_plot\")\nplt.show()\n\n그림 저장: ridge_regression_plot\n\n\n\n\n\n노트: 향후 버전이 바뀌더라도 동일한 결과를 만들기 위해 사이킷런 0.21 버전의 기본값인 max_iter=1000과 tol=1e-3으로 지정합니다.\n\nsgd_reg = SGDRegressor(penalty=\"l2\", max_iter=1000, tol=1e-3, random_state=42)\nsgd_reg.fit(X, y.ravel())\nsgd_reg.predict([[1.5]])\n\narray([1.47012588])\n\n\n식 4-10: 라쏘 회귀의 비용 함수\n$ J() = () + _{i=1}^{n}| _i | $"
  },
  {
    "objectID": "Machine_Learning/04_training_linear_models.html#라쏘-회귀",
    "href": "Machine_Learning/04_training_linear_models.html#라쏘-회귀",
    "title": "04_training_linear_models",
    "section": "라쏘 회귀",
    "text": "라쏘 회귀\n\nfrom sklearn.linear_model import Lasso\n\nplt.figure(figsize=(8,4))\nplt.subplot(121)\nplot_model(Lasso, polynomial=False, alphas=(0, 0.1, 1), random_state=42)\nplt.ylabel(\"$y$\", rotation=0, fontsize=18)\nplt.subplot(122)\nplot_model(Lasso, polynomial=True, alphas=(0, 10**-7, 1), random_state=42)\n\nsave_fig(\"lasso_regression_plot\")\nplt.show()\n\n/home/haesun/handson-ml2/.env/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:646: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.803e+00, tolerance: 9.295e-04\n  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n\n\n그림 저장: lasso_regression_plot\n\n\n\n\n\n\nfrom sklearn.linear_model import Lasso\nlasso_reg = Lasso(alpha=0.1)\nlasso_reg.fit(X, y)\nlasso_reg.predict([[1.5]])\n\narray([1.53788174])"
  },
  {
    "objectID": "Machine_Learning/04_training_linear_models.html#엘라스틱넷",
    "href": "Machine_Learning/04_training_linear_models.html#엘라스틱넷",
    "title": "04_training_linear_models",
    "section": "엘라스틱넷",
    "text": "엘라스틱넷\n식 4-12: 엘라스틱넷 비용 함수\n$ J() = () + r _{i=1}^{n}| i | + {i=1}^{n}{{_i}^2} $\n\nfrom sklearn.linear_model import ElasticNet\nelastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\nelastic_net.fit(X, y)\nelastic_net.predict([[1.5]])\n\narray([1.54333232])"
  },
  {
    "objectID": "Machine_Learning/04_training_linear_models.html#조기-종료",
    "href": "Machine_Learning/04_training_linear_models.html#조기-종료",
    "title": "04_training_linear_models",
    "section": "조기 종료",
    "text": "조기 종료\n\nnp.random.seed(42)\nm = 100\nX = 6 * np.random.rand(m, 1) - 3\ny = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)\n\nX_train, X_val, y_train, y_val = train_test_split(X[:50], y[:50].ravel(), test_size=0.5, random_state=10)\n\n\nfrom copy import deepcopy\n\npoly_scaler = Pipeline([\n        (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n        (\"std_scaler\", StandardScaler())\n    ])\n\nX_train_poly_scaled = poly_scaler.fit_transform(X_train)\nX_val_poly_scaled = poly_scaler.transform(X_val)\n\nsgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,\n                       penalty=None, learning_rate=\"constant\", eta0=0.0005, random_state=42)\n\nminimum_val_error = float(\"inf\")\nbest_epoch = None\nbest_model = None\nfor epoch in range(1000):\n    sgd_reg.fit(X_train_poly_scaled, y_train)  # 중지된 곳에서 다시 시작합니다\n    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n    val_error = mean_squared_error(y_val, y_val_predict)\n    if val_error &lt; minimum_val_error:\n        minimum_val_error = val_error\n        best_epoch = epoch\n        best_model = deepcopy(sgd_reg)\n\n그래프를 그립니다:\n\nsgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,\n                       penalty=None, learning_rate=\"constant\", eta0=0.0005, random_state=42)\n\nn_epochs = 500\ntrain_errors, val_errors = [], []\nfor epoch in range(n_epochs):\n    sgd_reg.fit(X_train_poly_scaled, y_train)\n    y_train_predict = sgd_reg.predict(X_train_poly_scaled)\n    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n    train_errors.append(mean_squared_error(y_train, y_train_predict))\n    val_errors.append(mean_squared_error(y_val, y_val_predict))\n\nbest_epoch = np.argmin(val_errors)\nbest_val_rmse = np.sqrt(val_errors[best_epoch])\n\nplt.annotate('Best model',\n             xy=(best_epoch, best_val_rmse),\n             xytext=(best_epoch, best_val_rmse + 1),\n             ha=\"center\",\n             arrowprops=dict(facecolor='black', shrink=0.05),\n             fontsize=16,\n            )\n\nbest_val_rmse -= 0.03  # just to make the graph look better\nplt.plot([0, n_epochs], [best_val_rmse, best_val_rmse], \"k:\", linewidth=2)\nplt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"Validation set\")\nplt.plot(np.sqrt(train_errors), \"r--\", linewidth=2, label=\"Training set\")\nplt.legend(loc=\"upper right\", fontsize=14)\nplt.xlabel(\"Epoch\", fontsize=14)\nplt.ylabel(\"RMSE\", fontsize=14)\nsave_fig(\"early_stopping_plot\")\nplt.show()\n\n그림 저장: early_stopping_plot\n\n\n\n\n\n\nbest_epoch, best_model\n\n(239,\n SGDRegressor(eta0=0.0005, learning_rate='constant', max_iter=1, penalty=None,\n              random_state=42, tol=-inf, warm_start=True))\n\n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nt1a, t1b, t2a, t2b = -1, 3, -1.5, 1.5\n\nt1s = np.linspace(t1a, t1b, 500)\nt2s = np.linspace(t2a, t2b, 500)\nt1, t2 = np.meshgrid(t1s, t2s)\nT = np.c_[t1.ravel(), t2.ravel()]\nXr = np.array([[1, 1], [1, -1], [1, 0.5]])\nyr = 2 * Xr[:, :1] + 0.5 * Xr[:, 1:]\n\nJ = (1/len(Xr) * np.sum((T.dot(Xr.T) - yr.T)**2, axis=1)).reshape(t1.shape)\n\nN1 = np.linalg.norm(T, ord=1, axis=1).reshape(t1.shape)\nN2 = np.linalg.norm(T, ord=2, axis=1).reshape(t1.shape)\n\nt_min_idx = np.unravel_index(np.argmin(J), J.shape)\nt1_min, t2_min = t1[t_min_idx], t2[t_min_idx]\n\nt_init = np.array([[0.25], [-1]])\n\n\ndef bgd_path(theta, X, y, l1, l2, core = 1, eta = 0.05, n_iterations = 200):\n    path = [theta]\n    for iteration in range(n_iterations):\n        gradients = core * 2/len(X) * X.T.dot(X.dot(theta) - y) + l1 * np.sign(theta) + l2 * theta\n        theta = theta - eta * gradients\n        path.append(theta)\n    return np.array(path)\n\nfig, axes = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(10.1, 8))\nfor i, N, l1, l2, title in ((0, N1, 2., 0, \"Lasso\"), (1, N2, 0,  2., \"Ridge\")):\n    JR = J + l1 * N1 + l2 * 0.5 * N2**2\n    \n    tr_min_idx = np.unravel_index(np.argmin(JR), JR.shape)\n    t1r_min, t2r_min = t1[tr_min_idx], t2[tr_min_idx]\n\n    levelsJ=(np.exp(np.linspace(0, 1, 20)) - 1) * (np.max(J) - np.min(J)) + np.min(J)\n    levelsJR=(np.exp(np.linspace(0, 1, 20)) - 1) * (np.max(JR) - np.min(JR)) + np.min(JR)\n    levelsN=np.linspace(0, np.max(N), 10)\n    \n    path_J = bgd_path(t_init, Xr, yr, l1=0, l2=0)\n    path_JR = bgd_path(t_init, Xr, yr, l1, l2)\n    path_N = bgd_path(np.array([[2.0], [0.5]]), Xr, yr, np.sign(l1)/3, np.sign(l2), core=0)\n\n    ax = axes[i, 0]\n    ax.grid(True)\n    ax.axhline(y=0, color='k')\n    ax.axvline(x=0, color='k')\n    ax.contourf(t1, t2, N / 2., levels=levelsN)\n    ax.plot(path_N[:, 0], path_N[:, 1], \"y--\")\n    ax.plot(0, 0, \"ys\")\n    ax.plot(t1_min, t2_min, \"ys\")\n    ax.set_title(r\"$\\ell_{}$ penalty\".format(i + 1), fontsize=16)\n    ax.axis([t1a, t1b, t2a, t2b])\n    if i == 1:\n        ax.set_xlabel(r\"$\\theta_1$\", fontsize=16)\n    ax.set_ylabel(r\"$\\theta_2$\", fontsize=16, rotation=0)\n\n    ax = axes[i, 1]\n    ax.grid(True)\n    ax.axhline(y=0, color='k')\n    ax.axvline(x=0, color='k')\n    ax.contourf(t1, t2, JR, levels=levelsJR, alpha=0.9)\n    ax.plot(path_JR[:, 0], path_JR[:, 1], \"w-o\")\n    ax.plot(path_N[:, 0], path_N[:, 1], \"y--\")\n    ax.plot(0, 0, \"ys\")\n    ax.plot(t1_min, t2_min, \"ys\")\n    ax.plot(t1r_min, t2r_min, \"rs\")\n    ax.set_title(title, fontsize=16)\n    ax.axis([t1a, t1b, t2a, t2b])\n    if i == 1:\n        ax.set_xlabel(r\"$\\theta_1$\", fontsize=16)\n\nsave_fig(\"lasso_vs_ridge_plot\")\nplt.show()\n\n그림 저장: lasso_vs_ridge_plot"
  },
  {
    "objectID": "Machine_Learning/04_training_linear_models.html#결정-경계",
    "href": "Machine_Learning/04_training_linear_models.html#결정-경계",
    "title": "04_training_linear_models",
    "section": "결정 경계",
    "text": "결정 경계\n\nt = np.linspace(-10, 10, 100)\nsig = 1 / (1 + np.exp(-t))\nplt.figure(figsize=(9, 3))\nplt.plot([-10, 10], [0, 0], \"k-\")\nplt.plot([-10, 10], [0.5, 0.5], \"k:\")\nplt.plot([-10, 10], [1, 1], \"k:\")\nplt.plot([0, 0], [-1.1, 1.1], \"k-\")\nplt.plot(t, sig, \"b-\", linewidth=2, label=r\"$\\sigma(t) = \\frac{1}{1 + e^{-t}}$\")\nplt.xlabel(\"t\")\nplt.legend(loc=\"upper left\", fontsize=20)\nplt.axis([-10, 10, -0.1, 1.1])\nsave_fig(\"logistic_function_plot\")\nplt.show()\n\n그림 저장: logistic_function_plot\n\n\n\n\n\n식 4-16: 하나의 훈련 샘플에 대한 비용 함수\n$ c() =\n\\[\\begin{cases}\n  -\\log(\\hat{p}) & \\text{if } y = 1, \\\\\n  -\\log(1 - \\hat{p}) & \\text{if } y = 0.\n\\end{cases}\\]\n$\n식 4-17: 로지스틱 회귀 비용 함수(로그 손실)\n$ J() = - _{i=1}^{m}{} $\n식 4-18: 로지스틱 비용 함수의 편도 함수\n$ () = _{i=1}{m}(T ^{(i)}) - y^{(i)}), x_j^{(i)} $\n\nfrom sklearn import datasets\niris = datasets.load_iris()\nlist(iris.keys())\n\n['data',\n 'target',\n 'frame',\n 'target_names',\n 'DESCR',\n 'feature_names',\n 'filename',\n 'data_module']\n\n\n\nprint(iris.DESCR)\n\n.. _iris_dataset:\n\nIris plants dataset\n--------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 150 (50 in each of three classes)\n    :Number of Attributes: 4 numeric, predictive attributes and the class\n    :Attribute Information:\n        - sepal length in cm\n        - sepal width in cm\n        - petal length in cm\n        - petal width in cm\n        - class:\n                - Iris-Setosa\n                - Iris-Versicolour\n                - Iris-Virginica\n                \n    :Summary Statistics:\n\n    ============== ==== ==== ======= ===== ====================\n                    Min  Max   Mean    SD   Class Correlation\n    ============== ==== ==== ======= ===== ====================\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n    ============== ==== ==== ======= ===== ====================\n\n    :Missing Attribute Values: None\n    :Class Distribution: 33.3% for each of 3 classes.\n    :Creator: R.A. Fisher\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n    :Date: July, 1988\n\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\nfrom Fisher's paper. Note that it's the same as in R, but not as in the UCI\nMachine Learning Repository, which has two wrong data points.\n\nThis is perhaps the best known database to be found in the\npattern recognition literature.  Fisher's paper is a classic in the field and\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\ndata set contains 3 classes of 50 instances each, where each class refers to a\ntype of iris plant.  One class is linearly separable from the other 2; the\nlatter are NOT linearly separable from each other.\n\n.. topic:: References\n\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n     Mathematical Statistics\" (John Wiley, NY, 1950).\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n     Structure and Classification Rule for Recognition in Partially Exposed\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n     on Information Theory, May 1972, 431-433.\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n     conceptual clustering system finds 3 classes in the data.\n   - Many, many more ...\n\n\n\nX = iris[\"data\"][:, 3:]  # 꽃잎 너비\ny = (iris[\"target\"] == 2).astype(int)  # Iris virginica이면 1 아니면 0\n\n노트: 향후 버전이 바뀌더라도 동일한 결과를 만들기 위해 사이킷런 0.22 버전의 기본값인 solver=\"lbfgs\"로 지정합니다.\n\nfrom sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression(solver=\"lbfgs\", random_state=42)\nlog_reg.fit(X, y)\n\nLogisticRegression(random_state=42)\n\n\n\nX_new = np.linspace(0, 3, 1000).reshape(-1, 1)\ny_proba = log_reg.predict_proba(X_new)\n\nplt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris virginica\")\nplt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2, label=\"Not Iris virginica\")\n\n\n\n\n책에 실린 그림은 조금 더 예쁘게 꾸몄습니다:\n\nX_new = np.linspace(0, 3, 1000).reshape(-1, 1)\ny_proba = log_reg.predict_proba(X_new)\ndecision_boundary = X_new[y_proba[:, 1] &gt;= 0.5][0]\n\nplt.figure(figsize=(8, 3))\nplt.plot(X[y==0], y[y==0], \"bs\")\nplt.plot(X[y==1], y[y==1], \"g^\")\nplt.plot([decision_boundary, decision_boundary], [-1, 2], \"k:\", linewidth=2)\nplt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris virginica\")\nplt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2, label=\"Not Iris virginica\")\nplt.text(decision_boundary+0.02, 0.15, \"Decision  boundary\", fontsize=14, color=\"k\", ha=\"center\")\nplt.arrow(decision_boundary[0], 0.08, -0.3, 0, head_width=0.05, head_length=0.1, fc='b', ec='b')\nplt.arrow(decision_boundary[0], 0.92, 0.3, 0, head_width=0.05, head_length=0.1, fc='g', ec='g')\nplt.xlabel(\"Petal width (cm)\", fontsize=14)\nplt.ylabel(\"Probability\", fontsize=14)\nplt.legend(loc=\"center left\", fontsize=14)\nplt.axis([0, 3, -0.02, 1.02])\nsave_fig(\"logistic_regression_plot\")\nplt.show()\n\n그림 저장: logistic_regression_plot\n\n\n\n\n\n\ndecision_boundary\n\narray([1.66066066])\n\n\n\nlog_reg.predict([[1.7], [1.5]])\n\narray([1, 0])"
  },
  {
    "objectID": "Machine_Learning/04_training_linear_models.html#소프트맥스-회귀",
    "href": "Machine_Learning/04_training_linear_models.html#소프트맥스-회귀",
    "title": "04_training_linear_models",
    "section": "소프트맥스 회귀",
    "text": "소프트맥스 회귀\n\nfrom sklearn.linear_model import LogisticRegression\n\nX = iris[\"data\"][:, (2, 3)]  # petal length, petal width\ny = (iris[\"target\"] == 2).astype(int)\n\nlog_reg = LogisticRegression(solver=\"lbfgs\", C=10**10, random_state=42)\nlog_reg.fit(X, y)\n\nx0, x1 = np.meshgrid(\n        np.linspace(2.9, 7, 500).reshape(-1, 1),\n        np.linspace(0.8, 2.7, 200).reshape(-1, 1),\n    )\nX_new = np.c_[x0.ravel(), x1.ravel()]\n\ny_proba = log_reg.predict_proba(X_new)\n\nplt.figure(figsize=(10, 4))\nplt.plot(X[y==0, 0], X[y==0, 1], \"bs\")\nplt.plot(X[y==1, 0], X[y==1, 1], \"g^\")\n\nzz = y_proba[:, 1].reshape(x0.shape)\ncontour = plt.contour(x0, x1, zz, cmap=plt.cm.brg)\n\n\nleft_right = np.array([2.9, 7])\nboundary = -(log_reg.coef_[0][0] * left_right + log_reg.intercept_[0]) / log_reg.coef_[0][1]\n\nplt.clabel(contour, inline=1, fontsize=12)\nplt.plot(left_right, boundary, \"k--\", linewidth=3)\nplt.text(3.5, 1.5, \"Not Iris virginica\", fontsize=14, color=\"b\", ha=\"center\")\nplt.text(6.5, 2.3, \"Iris virginica\", fontsize=14, color=\"g\", ha=\"center\")\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.ylabel(\"Petal width\", fontsize=14)\nplt.axis([2.9, 7, 0.8, 2.7])\nsave_fig(\"logistic_regression_contour_plot\")\nplt.show()\n\n그림 저장: logistic_regression_contour_plot\n\n\n\n\n\n식 4-20: 소프트맥스 함수\n$ _k = (())_k = $\n식 4-22: 크로스 엔트로피 비용 함수\n$ J() = - {i=1}^{m}{k=1}{K}{y_k{(i)}(_k^{(i)})} $\n식 4-23: 클래스 k에 대한 크로스 엔트로피의 그레이디언트 벡터\n$ {^{(k)}} , J() = {i=1}^{m}{ ( ^{(i)}_k - y_k^{(i)} ) ^{(i)}} $\n\nX = iris[\"data\"][:, (2, 3)]  # 꽃잎 길이, 꽃잎 너비\ny = iris[\"target\"]\n\nsoftmax_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10, random_state=42)\nsoftmax_reg.fit(X, y)\n\nLogisticRegression(C=10, multi_class='multinomial', random_state=42)\n\n\n\nx0, x1 = np.meshgrid(\n        np.linspace(0, 8, 500).reshape(-1, 1),\n        np.linspace(0, 3.5, 200).reshape(-1, 1),\n    )\nX_new = np.c_[x0.ravel(), x1.ravel()]\n\n\ny_proba = softmax_reg.predict_proba(X_new)\ny_predict = softmax_reg.predict(X_new)\n\nzz1 = y_proba[:, 1].reshape(x0.shape)\nzz = y_predict.reshape(x0.shape)\n\nplt.figure(figsize=(10, 4))\nplt.plot(X[y==2, 0], X[y==2, 1], \"g^\", label=\"Iris virginica\")\nplt.plot(X[y==1, 0], X[y==1, 1], \"bs\", label=\"Iris versicolor\")\nplt.plot(X[y==0, 0], X[y==0, 1], \"yo\", label=\"Iris setosa\")\n\nfrom matplotlib.colors import ListedColormap\ncustom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n\nplt.contourf(x0, x1, zz, cmap=custom_cmap)\ncontour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)\nplt.clabel(contour, inline=1, fontsize=12)\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.ylabel(\"Petal width\", fontsize=14)\nplt.legend(loc=\"center left\", fontsize=14)\nplt.axis([0, 7, 0, 3.5])\nsave_fig(\"softmax_regression_contour_plot\")\nplt.show()\n\n그림 저장: softmax_regression_contour_plot\n\n\n\n\n\n\nsoftmax_reg.predict([[5, 2]])\n\narray([2])\n\n\n\nsoftmax_reg.predict_proba([[5, 2]])\n\narray([[6.38014896e-07, 5.74929995e-02, 9.42506362e-01]])"
  },
  {
    "objectID": "Machine_Learning/04_training_linear_models.html#to-11.",
    "href": "Machine_Learning/04_training_linear_models.html#to-11.",
    "title": "04_training_linear_models",
    "section": "1. to 11.",
    "text": "1. to 11.\n부록 A를 참고하세요."
  },
  {
    "objectID": "Machine_Learning/04_training_linear_models.html#조기-종료를-사용한-배치-경사-하강법으로-소프트맥스-회귀-구현하기",
    "href": "Machine_Learning/04_training_linear_models.html#조기-종료를-사용한-배치-경사-하강법으로-소프트맥스-회귀-구현하기",
    "title": "04_training_linear_models",
    "section": "12. 조기 종료를 사용한 배치 경사 하강법으로 소프트맥스 회귀 구현하기",
    "text": "12. 조기 종료를 사용한 배치 경사 하강법으로 소프트맥스 회귀 구현하기\n(사이킷런을 사용하지 않고)\n먼저 데이터를 로드합니다. 앞서 사용했던 Iris 데이터셋을 재사용하겠습니다.\n\nX = iris[\"data\"][:, (2, 3)]  # 꽃잎 길이, 꽃잎 넓이\ny = iris[\"target\"]\n\n모든 샘플에 편향을 추가합니다 (\\(x_0 = 1\\)):\n\nX_with_bias = np.c_[np.ones([len(X), 1]), X]\n\n결과를 일정하게 유지하기 위해 랜덤 시드를 지정합니다:\n\nnp.random.seed(2042)\n\n데이터셋을 훈련 세트, 검증 세트, 테스트 세트로 나누는 가장 쉬운 방법은 사이킷런의 train_test_split() 함수를 사용하는 것입니다. 하지만 이 연습문제의 목적은 직접 만들어 보면서 알고리즘을 이해하는 것이므로 다음과 같이 수동으로 나누어 보겠습니다:\n\ntest_ratio = 0.2\nvalidation_ratio = 0.2\ntotal_size = len(X_with_bias)\n\ntest_size = int(total_size * test_ratio)\nvalidation_size = int(total_size * validation_ratio)\ntrain_size = total_size - test_size - validation_size\n\nrnd_indices = np.random.permutation(total_size)\n\nX_train = X_with_bias[rnd_indices[:train_size]]\ny_train = y[rnd_indices[:train_size]]\nX_valid = X_with_bias[rnd_indices[train_size:-test_size]]\ny_valid = y[rnd_indices[train_size:-test_size]]\nX_test = X_with_bias[rnd_indices[-test_size:]]\ny_test = y[rnd_indices[-test_size:]]\n\n타깃은 클래스 인덱스(0, 1 그리고 2)이지만 소프트맥스 회귀 모델을 훈련시키기 위해 필요한 것은 타깃 클래스의 확률입니다. 각 샘플에서 확률이 1인 타깃 클래스를 제외한 다른 클래스의 확률은 0입니다(다른 말로하면 주어진 샘플에 대한 클래스 확률이 원-핫 벡터입니다). 클래스 인덱스를 원-핫 벡터로 바꾸는 간단한 함수를 작성하겠습니다:\n\ndef to_one_hot(y):\n    n_classes = y.max() + 1\n    m = len(y)\n    Y_one_hot = np.zeros((m, n_classes))\n    Y_one_hot[np.arange(m), y] = 1\n    return Y_one_hot\n\n10개 샘플만 넣어 이 함수를 테스트해 보죠:\n\ny_train[:10]\n\narray([0, 1, 2, 1, 1, 0, 1, 1, 1, 0])\n\n\n\nto_one_hot(y_train[:10])\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.]])\n\n\n잘 되네요, 이제 훈련 세트와 테스트 세트의 타깃 클래스 확률을 담은 행렬을 만들겠습니다:\n\nY_train_one_hot = to_one_hot(y_train)\nY_valid_one_hot = to_one_hot(y_valid)\nY_test_one_hot = to_one_hot(y_test)\n\n이제 소프트맥스 함수를 만듭니다. 다음 공식을 참고하세요:\n\\(\\sigma\\left(\\mathbf{s}(\\mathbf{x})\\right)_k = \\dfrac{\\exp\\left(s_k(\\mathbf{x})\\right)}{\\sum\\limits_{j=1}^{K}{\\exp\\left(s_j(\\mathbf{x})\\right)}}\\)\n\ndef softmax(logits):\n    exps = np.exp(logits)\n    exp_sums = np.sum(exps, axis=1, keepdims=True)\n    return exps / exp_sums\n\n훈련을 위한 준비를 거의 마쳤습니다. 입력과 출력의 개수를 정의합니다:\n\nn_inputs = X_train.shape[1] # == 3 (특성 2개와 편향)\nn_outputs = len(np.unique(y_train))   # == 3 (3개의 붓꽃 클래스)\n\n이제 좀 복잡한 훈련 파트입니다! 이론적으로는 간단합니다. 그냥 수학 공식을 파이썬 코드로 바꾸기만 하면 됩니다. 하지만 실제로는 꽤 까다로운 면이 있습니다. 특히, 항이나 인덱스의 순서가 뒤섞이기 쉽습니다. 제대로 작동할 것처럼 코드를 작성했더라도 실제 제대로 계산하지 못합니다. 확실하지 않을 때는 각 항의 크기를 기록하고 이에 상응하는 코드가 같은 크기를 만드는지 확인합니다. 각 항을 독립적으로 평가해서 출력해 보는 것도 좋습니다. 사실 사이킷런에 이미 잘 구현되어 있기 때문에 이렇게 할 필요는 없습니다. 하지만 직접 만들어 보면 어떻게 작동하는지 이해하는데 도움이 됩니다.\n구현할 공식은 비용함수입니다:\n\\(J(\\mathbf{\\Theta}) = - \\dfrac{1}{m}\\sum\\limits_{i=1}^{m}\\sum\\limits_{k=1}^{K}{y_k^{(i)}\\log\\left(\\hat{p}_k^{(i)}\\right)}\\)\n그리고 그레이디언트 공식입니다:\n\\(\\nabla_{\\mathbf{\\theta}^{(k)}} \\, J(\\mathbf{\\Theta}) = \\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{ \\left ( \\hat{p}^{(i)}_k - y_k^{(i)} \\right ) \\mathbf{x}^{(i)}}\\)\n\\(\\hat{p}_k^{(i)} = 0\\)이면 \\(\\log\\left(\\hat{p}_k^{(i)}\\right)\\)를 계산할 수 없습니다. nan 값을 피하기 위해 \\(\\log\\left(\\hat{p}_k^{(i)}\\right)\\)에 아주 작은 값 \\(\\epsilon\\)을 추가하겠습니다.\n\neta = 0.01\nn_iterations = 5001\nm = len(X_train)\nepsilon = 1e-7\n\nTheta = np.random.randn(n_inputs, n_outputs)\n\nfor iteration in range(n_iterations):\n    logits = X_train.dot(Theta)\n    Y_proba = softmax(logits)\n    if iteration % 500 == 0:\n        loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n        print(iteration, loss)\n    error = Y_proba - Y_train_one_hot\n    gradients = 1/m * X_train.T.dot(error)\n    Theta = Theta - eta * gradients\n\n0 5.446205811872683\n500 0.8350062641405651\n1000 0.6878801447192402\n1500 0.6012379137693314\n2000 0.5444496861981872\n2500 0.5038530181431525\n3000 0.47292289721922487\n3500 0.44824244188957774\n4000 0.4278651093928793\n4500 0.41060071429187134\n5000 0.3956780375390374\n\n\n바로 이겁니다! 소프트맥스 모델을 훈련시켰습니다. 모델 파라미터를 확인해 보겠습니다:\n\nTheta\n\narray([[ 3.32094157, -0.6501102 , -2.99979416],\n       [-1.1718465 ,  0.11706172,  0.10507543],\n       [-0.70224261, -0.09527802,  1.4786383 ]])\n\n\n검증 세트에 대한 예측과 정확도를 확인해 보겠습니다:\n\nlogits = X_valid.dot(Theta)\nY_proba = softmax(logits)\ny_predict = np.argmax(Y_proba, axis=1)\n\naccuracy_score = np.mean(y_predict == y_valid)\naccuracy_score\n\n0.9666666666666667\n\n\n와우, 이 모델이 매우 잘 작동하는 것 같습니다. 연습을 위해서 \\(\\ell_2\\) 규제를 조금 추가해 보겠습니다. 다음 코드는 위와 거의 동일하지만 손실에 \\(\\ell_2\\) 페널티가 추가되었고 그래디언트에도 항이 추가되었습니다(Theta의 첫 번째 원소는 편향이므로 규제하지 않습니다). 학습률 eta도 증가시켜 보겠습니다.\n\neta = 0.1\nn_iterations = 5001\nm = len(X_train)\nepsilon = 1e-7\nalpha = 0.1  # 규제 하이퍼파라미터\n\nTheta = np.random.randn(n_inputs, n_outputs)\n\nfor iteration in range(n_iterations):\n    logits = X_train.dot(Theta)\n    Y_proba = softmax(logits)\n    if iteration % 500 == 0:\n        xentropy_loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n        l2_loss = 1/2 * np.sum(np.square(Theta[1:]))\n        loss = xentropy_loss + alpha * l2_loss\n        print(iteration, loss)\n    error = Y_proba - Y_train_one_hot\n    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n    Theta = Theta - eta * gradients\n\n0 6.629842469083912\n500 0.5339667976629505\n1000 0.503640075014894\n1500 0.4946891059460321\n2000 0.4912968418075477\n2500 0.48989924700933296\n3000 0.4892990598451198\n3500 0.489035124439786\n4000 0.4889173621830818\n4500 0.4888643337449303\n5000 0.48884031207388184\n\n\n추가된 \\(\\ell_2\\) 페널티 때문에 이전보다 손실이 조금 커보이지만 더 잘 작동하는 모델이 되었을까요? 확인해 보죠:\n\nlogits = X_valid.dot(Theta)\nY_proba = softmax(logits)\ny_predict = np.argmax(Y_proba, axis=1)\n\naccuracy_score = np.mean(y_predict == y_valid)\naccuracy_score\n\n1.0\n\n\n와우, 완벽한 정확도네요! 운이 좋은 검증 세트일지 모르지만 잘 된 것은 맞습니다.\n이제 조기 종료를 추가해 보죠. 이렇게 하려면 매 반복에서 검증 세트에 대한 손실을 계산해서 오차가 증가하기 시작할 때 멈춰야 합니다.\n\neta = 0.1 \nn_iterations = 5001\nm = len(X_train)\nepsilon = 1e-7\nalpha = 0.1  # 규제 하이퍼파라미터\nbest_loss = np.infty\n\nTheta = np.random.randn(n_inputs, n_outputs)\n\nfor iteration in range(n_iterations):\n    logits = X_train.dot(Theta)\n    Y_proba = softmax(logits)\n    error = Y_proba - Y_train_one_hot\n    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n    Theta = Theta - eta * gradients\n\n    logits = X_valid.dot(Theta)\n    Y_proba = softmax(logits)\n    xentropy_loss = -np.mean(np.sum(Y_valid_one_hot * np.log(Y_proba + epsilon), axis=1))\n    l2_loss = 1/2 * np.sum(np.square(Theta[1:]))\n    loss = xentropy_loss + alpha * l2_loss\n    if iteration % 500 == 0:\n        print(iteration, loss)\n    if loss &lt; best_loss:\n        best_loss = loss\n    else:\n        print(iteration - 1, best_loss)\n        print(iteration, loss, \"조기 종료!\")\n        break\n\n0 4.7096017363419875\n500 0.5739711987633518\n1000 0.5435638529109127\n1500 0.5355752782580262\n2000 0.5331959249285544\n2500 0.5325946767399383\n2765 0.5325460966791898\n2766 0.5325460971327977 조기 종료!\n\n\n\nlogits = X_valid.dot(Theta)\nY_proba = softmax(logits)\ny_predict = np.argmax(Y_proba, axis=1)\n\naccuracy_score = np.mean(y_predict == y_valid)\naccuracy_score\n\n1.0\n\n\n여전히 완벽하지만 더 빠릅니다.\n이제 전체 데이터셋에 대한 모델의 예측을 그래프로 나타내 보겠습니다:\n\nx0, x1 = np.meshgrid(\n        np.linspace(0, 8, 500).reshape(-1, 1),\n        np.linspace(0, 3.5, 200).reshape(-1, 1),\n    )\nX_new = np.c_[x0.ravel(), x1.ravel()]\nX_new_with_bias = np.c_[np.ones([len(X_new), 1]), X_new]\n\nlogits = X_new_with_bias.dot(Theta)\nY_proba = softmax(logits)\ny_predict = np.argmax(Y_proba, axis=1)\n\nzz1 = Y_proba[:, 1].reshape(x0.shape)\nzz = y_predict.reshape(x0.shape)\n\nplt.figure(figsize=(10, 4))\nplt.plot(X[y==2, 0], X[y==2, 1], \"g^\", label=\"Iris virginica\")\nplt.plot(X[y==1, 0], X[y==1, 1], \"bs\", label=\"Iris versicolor\")\nplt.plot(X[y==0, 0], X[y==0, 1], \"yo\", label=\"Iris setosa\")\n\nfrom matplotlib.colors import ListedColormap\ncustom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n\nplt.contourf(x0, x1, zz, cmap=custom_cmap)\ncontour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)\nplt.clabel(contour, inline=1, fontsize=12)\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.ylabel(\"Petal width\", fontsize=14)\nplt.legend(loc=\"upper left\", fontsize=14)\nplt.axis([0, 7, 0, 3.5])\nplt.show()\n\n\n\n\n이제 테스트 세트에 대한 모델의 최종 정확도를 측정해 보겠습니다:\n\nlogits = X_test.dot(Theta)\nY_proba = softmax(logits)\ny_predict = np.argmax(Y_proba, axis=1)\n\naccuracy_score = np.mean(y_predict == y_test)\naccuracy_score\n\n0.9333333333333333\n\n\n완벽했던 최종 모델의 성능이 조금 떨어졌습니다. 이런 차이는 데이터셋이 작기 때문일 것입니다. 훈련 세트와 검증 세트, 테스트 세트를 어떻게 샘플링했는지에 따라 매우 다른 결과를 얻을 수 있습니다. 몇 번 랜덤 시드를 바꾸고 이 코드를 다시 실행해 보면 결과가 달라지는 것을 확인할 수 있습니다."
  },
  {
    "objectID": "Machine_Learning/06_decision_trees.html",
    "href": "Machine_Learning/06_decision_trees.html",
    "title": "06_decision_trees",
    "section": "",
    "text": "6장 – 결정 트리\n이 노트북은 6장에 있는 모든 샘플 코드와 연습문제 해답을 가지고 있습니다."
  },
  {
    "objectID": "Machine_Learning/06_decision_trees.html#to-6.",
    "href": "Machine_Learning/06_decision_trees.html#to-6.",
    "title": "06_decision_trees",
    "section": "1. to 6.",
    "text": "1. to 6.\n부록 A 참조."
  },
  {
    "objectID": "Machine_Learning/06_decision_trees.html#section",
    "href": "Machine_Learning/06_decision_trees.html#section",
    "title": "06_decision_trees",
    "section": "7.",
    "text": "7.\n문제: moons 데이터셋에 결정 트리를 훈련시키고 세밀하게 튜닝해보세요.\n\nmake_moons(n_samples=1000, noise=0.4)를 사용해 데이터셋을 생성합니다.\n\nrandom_state=42를 지정하여 결과를 일정하게 만듭니다:\n\nfrom sklearn.datasets import make_moons\n\nX, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\n\n\n이를 train_test_split()을 사용해 훈련 세트와 테스트 세트로 나눕니다\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nDecisionTreeClassifier의 최적의 매개변수를 찾기 위해 교차 검증과 함께 그리드 탐색을 수행합니다(GridSearchCV를 사용하면 됩니다). 힌트: 여러 가지 max_leaf_nodes 값을 시도해보세요.\n\n\nfrom sklearn.model_selection import GridSearchCV\n\nparams = {'max_leaf_nodes': list(range(2, 100)), 'min_samples_split': [2, 3, 4]}\ngrid_search_cv = GridSearchCV(DecisionTreeClassifier(random_state=42), params, verbose=1, cv=3)\n\ngrid_search_cv.fit(X_train, y_train)\n\nFitting 3 folds for each of 294 candidates, totalling 882 fits\n\n\nGridSearchCV(cv=3, estimator=DecisionTreeClassifier(random_state=42),\n             param_grid={'max_leaf_nodes': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n                                            13, 14, 15, 16, 17, 18, 19, 20, 21,\n                                            22, 23, 24, 25, 26, 27, 28, 29, 30,\n                                            31, ...],\n                         'min_samples_split': [2, 3, 4]},\n             verbose=1)\n\n\n\ngrid_search_cv.best_estimator_\n\nDecisionTreeClassifier(max_leaf_nodes=17, random_state=42)\n\n\n\n찾은 매개변수를 사용해 전체 훈련 세트에 대해 모델을 훈련시키고 테스트 세트에서 성능을 측정합니다. 대략 85~87%의 정확도가 나올 것입니다.\n\n기본적으로 GridSearchCV는 전체 훈련 세트로 찾은 최적의 모델을 다시 훈련시킵니다(refit=False로 지정해서 바꿀 수 있습니다). 그래서 별도로 작업할 필요가 없습니다. 모델의 정확도를 바로 평가할 수 있습니다:\n\nfrom sklearn.metrics import accuracy_score\n\ny_pred = grid_search_cv.predict(X_test)\naccuracy_score(y_test, y_pred)\n\n0.8695"
  },
  {
    "objectID": "Machine_Learning/06_decision_trees.html#section-1",
    "href": "Machine_Learning/06_decision_trees.html#section-1",
    "title": "06_decision_trees",
    "section": "8.",
    "text": "8.\n문제: 랜덤 포레스트를 만들어보세요.\n\n이전 연습문제에 이어서, 훈련 세트의 서브셋을 1,000개 생성합니다. 각각은 무작위로 선택된 100개의 샘플을 담고 있습니다. 힌트: 사이킷런의 ShuffleSplit을 사용할 수 있습니다.\n\n\nfrom sklearn.model_selection import ShuffleSplit\n\nn_trees = 1000\nn_instances = 100\n\nmini_sets = []\n\nrs = ShuffleSplit(n_splits=n_trees, test_size=len(X_train) - n_instances, random_state=42)\nfor mini_train_index, mini_test_index in rs.split(X_train):\n    X_mini_train = X_train[mini_train_index]\n    y_mini_train = y_train[mini_train_index]\n    mini_sets.append((X_mini_train, y_mini_train))\n\n\n앞에서 찾은 최적의 매개변수를 사용해 각 서브셋에 결정 트리를 훈련시킵니다. 테스트 세트로 이 1,000개의 결정 트리를 평가합니다. 더 작은 데이터셋에서 훈련되었기 때문에 이 결정 트리는 앞서 만든 결정 트리보다 성능이 떨어져 약 80%의 정확도를 냅니다.\n\n\nfrom sklearn.base import clone\n\nforest = [clone(grid_search_cv.best_estimator_) for _ in range(n_trees)]\n\naccuracy_scores = []\n\nfor tree, (X_mini_train, y_mini_train) in zip(forest, mini_sets):\n    tree.fit(X_mini_train, y_mini_train)\n    \n    y_pred = tree.predict(X_test)\n    accuracy_scores.append(accuracy_score(y_test, y_pred))\n\nnp.mean(accuracy_scores)\n\n0.8054499999999999\n\n\n\n이제 마술을 부릴 차례입니다. 각 테스트 세트 샘플에 대해 1,000개의 결정 트리 예측을 만들고 다수로 나온 예측만 취합니다(사이파이의 mode() 함수를 사용할 수 있습니다). 그러면 테스트 세트에 대한 _다수결 예측_이 만들어집니다.\n\n\nY_pred = np.empty([n_trees, len(X_test)], dtype=np.uint8)\n\nfor tree_index, tree in enumerate(forest):\n    Y_pred[tree_index] = tree.predict(X_test)\n\n\nfrom scipy.stats import mode\n\ny_pred_majority_votes, n_votes = mode(Y_pred, axis=0)\n\n\n테스트 세트에서 이 예측을 평가합니다. 앞서 만든 모델보다 조금 높은(약 0.5~1.5% 정도) 정확도를 얻게 될 것입니다. 축하합니다. 랜덤 포레스트 분류기를 훈련시켰습니다!\n\n\naccuracy_score(y_test, y_pred_majority_votes.reshape([-1]))\n\n0.872"
  },
  {
    "objectID": "Machine_Learning/08_dimensionality_reduction.html",
    "href": "Machine_Learning/08_dimensionality_reduction.html",
    "title": "08_dimensionality_reduction",
    "section": "",
    "text": "8장 – 차원 축소\n이 노트북은 8장에 있는 모든 샘플 코드와 연습문제 해답을 가지고 있습니다."
  },
  {
    "objectID": "Machine_Learning/08_dimensionality_reduction.html#주성분",
    "href": "Machine_Learning/08_dimensionality_reduction.html#주성분",
    "title": "08_dimensionality_reduction",
    "section": "주성분",
    "text": "주성분\n\\(X = U \\sum V^T\\) 에서 \\(V\\)가 주성분\n\n(m, m) (m, n) (n, n)\n\nm: 샘플 개수, n: 특성 개수\n\nX_centered = X - X.mean(axis=0)\nU, s, Vt = np.linalg.svd(X_centered)\nc1 = Vt.T[:, 0]\nc2 = Vt.T[:, 1]\n\n\nm, n = X.shape\n\nS = np.zeros(X_centered.shape)\nS[:n, :n] = np.diag(s)\n\n\nnp.allclose(X_centered, U.dot(S).dot(Vt))\n\nTrue"
  },
  {
    "objectID": "Machine_Learning/08_dimensionality_reduction.html#d-차원으로-투영하기",
    "href": "Machine_Learning/08_dimensionality_reduction.html#d-차원으로-투영하기",
    "title": "08_dimensionality_reduction",
    "section": "d 차원으로 투영하기",
    "text": "d 차원으로 투영하기\n\nW2 = Vt.T[:, :2]\nX2D = X_centered.dot(W2)\n\n\nX2D_using_svd = X2D"
  },
  {
    "objectID": "Machine_Learning/08_dimensionality_reduction.html#사이킷런-사용하기",
    "href": "Machine_Learning/08_dimensionality_reduction.html#사이킷런-사용하기",
    "title": "08_dimensionality_reduction",
    "section": "사이킷런 사용하기",
    "text": "사이킷런 사용하기\n사이킷런에서는 PCA가 아주 간단합니다. 데이터셋에서 평균을 빼는 작업도 대신 처리해 줍니다:\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX2D = pca.fit_transform(X)\n\n\nX2D[:5]\n\narray([[ 1.26203346,  0.42067648],\n       [-0.08001485, -0.35272239],\n       [ 1.17545763,  0.36085729],\n       [ 0.89305601, -0.30862856],\n       [ 0.73016287, -0.25404049]])\n\n\n\nX2D_using_svd[:5]\n\narray([[-1.26203346, -0.42067648],\n       [ 0.08001485,  0.35272239],\n       [-1.17545763, -0.36085729],\n       [-0.89305601,  0.30862856],\n       [-0.73016287,  0.25404049]])\n\n\n데이터셋을 조금 다르게해서 PCA를 실행하면 결과가 달라질 것입니다. 일반적으로 달라지는 것은 일부 축이 반대로 바뀌는 정도입니다. 이 예에서 사이킷런의 PCA는 두 축이 반대로 뒤집힌 것외에는 SVD 방식을 사용한 것과 통일한 투영 결과를 만듭니다:\n\nnp.allclose(X2D, -X2D_using_svd)\n\nTrue\n\n\n평면(PCA 2D 부분공간)에 투영된 3D 포인트를 복원합니다.\n\nX3D_inv = pca.inverse_transform(X2D)\n\n물론, 투영 단게에서 일부 정보를 잃어버리기 때문에 복원된 3D 포인트가 원본 3D 포인트와 완전히 똑같지는 않습니다:\n\nnp.allclose(X3D_inv, X)\n\nFalse\n\n\n재구성 오차를 계산합니다:\n\nnp.mean(np.sum(np.square(X3D_inv - X), axis=1))\n\n0.010170337792848549\n\n\nSVD 방식의 역변환은 다음과 같습니다:\n\nX3D_inv_using_svd = X2D_using_svd.dot(Vt[:2, :])\n\n사이킷런의 PCA 클래스는 자동으로 평균을 뺏던 것을 복원해주기 때문에 두 방식의 재구성 오차가 동일하지는 않습니다. 하지만 평균을 빼면 동일한 재구성을 얻을 수 있습니다:\n\nnp.allclose(X3D_inv_using_svd, X3D_inv - pca.mean_)\n\nTrue\n\n\nPCA 객체를 사용하여 계산된 주성분을 참조할 수 있습니다:\n\npca.components_\n\narray([[-0.93636116, -0.29854881, -0.18465208],\n       [ 0.34027485, -0.90119108, -0.2684542 ]])\n\n\nSVD 방법으로 계산된 처음 두 개의 주성분과 비교해 보겠습니다:\n\nVt[:2]\n\narray([[ 0.93636116,  0.29854881,  0.18465208],\n       [-0.34027485,  0.90119108,  0.2684542 ]])\n\n\n축이 뒤집힌 것을 알 수 있습니다."
  },
  {
    "objectID": "Machine_Learning/08_dimensionality_reduction.html#설명된-분산의-비율",
    "href": "Machine_Learning/08_dimensionality_reduction.html#설명된-분산의-비율",
    "title": "08_dimensionality_reduction",
    "section": "설명된 분산의 비율",
    "text": "설명된 분산의 비율\n이제 설명된 분산 비율을 확인해 보겠습니다:\n\npca.explained_variance_ratio_\n\narray([0.84248607, 0.14631839])\n\n\n첫 번째 차원이 84.2%의 분산을 포함하고 있고 두 번째는 14.6%의 분산을 설명합니다.\n2D로 투영했기 때문에 분산의 1.1%을 잃었습니다:\n\n1 - pca.explained_variance_ratio_.sum()\n\n0.011195535570688975\n\n\nSVD 방식을 사용했을 때 설명된 분산의 비율을 계산하는 방법은 다음과 같습니다(s는 행렬 S의 대각 성분입니다):\n\nnp.square(s) / np.square(s).sum()\n\narray([0.84248607, 0.14631839, 0.01119554])\n\n\n이를 그래프로 멋지게 그려보죠! :)\n&lt;그림 8-2. 2차원에 가깝게 배치된 3차원 데이터셋&gt; 생성 코드\n3D 화살표를 그래기 위한 유틸리티 클래스입니다(http://stackoverflow.com/questions/11140163 에서 복사했습니다)\n\nfrom matplotlib.patches import FancyArrowPatch\nfrom mpl_toolkits.mplot3d import proj3d\n\nclass Arrow3D(FancyArrowPatch):\n    def __init__(self, xs, ys, zs, *args, **kwargs):\n        FancyArrowPatch.__init__(self, (0,0), (0,0), *args, **kwargs)\n        self._verts3d = xs, ys, zs\n\n    def draw(self, renderer):\n        xs3d, ys3d, zs3d = self._verts3d\n        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, self.axes.M)\n        self.set_positions((xs[0],ys[0]),(xs[1],ys[1]))\n        FancyArrowPatch.draw(self, renderer)\n\nx와 y의 함수로 평면을 표현합니다.\n\naxes = [-1.8, 1.8, -1.3, 1.3, -1.0, 1.0]\n\nx1s = np.linspace(axes[0], axes[1], 10)\nx2s = np.linspace(axes[2], axes[3], 10)\nx1, x2 = np.meshgrid(x1s, x2s)\n\nC = pca.components_\nR = C.T.dot(C)\nz = (R[0, 2] * x1 + R[1, 2] * x2) / (1 - R[2, 2])\n\n3D 데이터셋, 평면 그리고 이 평면으로의 투영을 그립니다.\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(figsize=(6, 3.8))\nax = fig.add_subplot(111, projection='3d')\n\nX3D_above = X[X[:, 2] &gt; X3D_inv[:, 2]]\nX3D_below = X[X[:, 2] &lt;= X3D_inv[:, 2]]\n\nax.plot(X3D_below[:, 0], X3D_below[:, 1], X3D_below[:, 2], \"bo\", alpha=0.5)\n\nax.plot_surface(x1, x2, z, alpha=0.2, color=\"k\")\nnp.linalg.norm(C, axis=0)\nax.add_artist(Arrow3D([0, C[0, 0]],[0, C[0, 1]],[0, C[0, 2]], mutation_scale=15, lw=1, arrowstyle=\"-|&gt;\", color=\"k\"))\nax.add_artist(Arrow3D([0, C[1, 0]],[0, C[1, 1]],[0, C[1, 2]], mutation_scale=15, lw=1, arrowstyle=\"-|&gt;\", color=\"k\"))\nax.plot([0], [0], [0], \"k.\")\n\nfor i in range(m):\n    if X[i, 2] &gt; X3D_inv[i, 2]:\n        ax.plot([X[i][0], X3D_inv[i][0]], [X[i][1], X3D_inv[i][1]], [X[i][2], X3D_inv[i][2]], \"k-\")\n        pass\n    else:\n        ax.plot([X[i][0], X3D_inv[i][0]], [X[i][1], X3D_inv[i][1]], [X[i][2], X3D_inv[i][2]], \"-\", color=\"#505050\")\n    \nax.plot(X3D_inv[:, 0], X3D_inv[:, 1], X3D_inv[:, 2], \"k+\")\nax.plot(X3D_inv[:, 0], X3D_inv[:, 1], X3D_inv[:, 2], \"k.\")\nax.plot(X3D_above[:, 0], X3D_above[:, 1], X3D_above[:, 2], \"bo\")\nax.set_xlabel(\"$x_1$\", fontsize=18, labelpad=10)\nax.set_ylabel(\"$x_2$\", fontsize=18, labelpad=10)\nax.set_zlabel(\"$x_3$\", fontsize=18, labelpad=10)\nax.set_xlim(axes[0:2])\nax.set_ylim(axes[2:4])\nax.set_zlim(axes[4:6])\n\n# Note: 맷플롯립 3.0.0 버전은 버그가 있기 때문에\n# 3D 그래프를 잘 출력하지 못합니다.\n# https://github.com/matplotlib/matplotlib/issues/12239 를 참조하세요.\n# 따라서 최신 버전으로 업그레이드해야 합니다.\n# 만약 업그레이드할 수 없다면 3D 그래프를 그리기 전에 다음 코드를 실행하세요:\n# for spine in ax.spines.values():\n#     spine.set_visible(False)\n\nsave_fig(\"dataset_3d_plot\")\nplt.show()\n\n그림 저장 dataset_3d_plot\n\n\n\n\n\n&lt;그림 8-3. 투영하여 만들어진 새로운 2D 데이터셋&gt; 생성 코드\n\nfig = plt.figure()\nax = fig.add_subplot(111, aspect='equal')\n\nax.plot(X2D[:, 0], X2D[:, 1], \"k+\")\nax.plot(X2D[:, 0], X2D[:, 1], \"k.\")\nax.plot([0], [0], \"ko\")\nax.arrow(0, 0, 0, 1, head_width=0.05, length_includes_head=True, head_length=0.1, fc='k', ec='k')\nax.arrow(0, 0, 1, 0, head_width=0.05, length_includes_head=True, head_length=0.1, fc='k', ec='k')\nax.set_xlabel(\"$z_1$\", fontsize=18)\nax.set_ylabel(\"$z_2$\", fontsize=18, rotation=0)\nax.axis([-1.5, 1.3, -1.2, 1.2])\nax.grid(True)\nsave_fig(\"dataset_2d_plot\")\n\n그림 저장 dataset_2d_plot\n\n\n\n\n\n&lt;그림 8-4. 스위스롤 데이터셋&gt; 생성 코드\n\nfrom sklearn.datasets import make_swiss_roll\nX, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n\n\naxes = [-11.5, 14, -2, 23, -12, 15]\n\nfig = plt.figure(figsize=(6, 5))\nax = fig.add_subplot(111, projection='3d')\n\nax.scatter(X[:, 0], X[:, 1], X[:, 2], c=t, cmap=plt.cm.hot)\nax.view_init(10, -70)\nax.set_xlabel(\"$x_1$\", fontsize=18)\nax.set_ylabel(\"$x_2$\", fontsize=18)\nax.set_zlabel(\"$x_3$\", fontsize=18)\nax.set_xlim(axes[0:2])\nax.set_ylim(axes[2:4])\nax.set_zlim(axes[4:6])\n\nsave_fig(\"swiss_roll_plot\")\nplt.show()\n\n그림 저장 swiss_roll_plot\n\n\n\n\n\n&lt;그림 8-5. 평면에 그냥 투영시켜서 뭉개진 것(왼쪽)과 스위스 롤을 펼쳐 놓은 것(오른쪽)&gt; 생성 코드\n\nplt.figure(figsize=(11, 4))\n\nplt.subplot(121)\nplt.scatter(X[:, 0], X[:, 1], c=t, cmap=plt.cm.hot)\nplt.axis(axes[:4])\nplt.xlabel(\"$x_1$\", fontsize=18)\nplt.ylabel(\"$x_2$\", fontsize=18, rotation=0)\nplt.grid(True)\n\nplt.subplot(122)\nplt.scatter(t, X[:, 1], c=t, cmap=plt.cm.hot)\nplt.axis([4, 15, axes[2], axes[3]])\nplt.xlabel(\"$z_1$\", fontsize=18)\nplt.grid(True)\n\nsave_fig(\"squished_swiss_roll_plot\")\nplt.show()\n\n그림 저장 squished_swiss_roll_plot\n\n\n\n\n\n&lt;그림 8-6. 저차원에서 항상 간단하지 않은 결정 경계&gt; 생성 코드\n\nfrom matplotlib import gridspec\n\naxes = [-11.5, 14, -2, 23, -12, 15]\n\nx2s = np.linspace(axes[2], axes[3], 10)\nx3s = np.linspace(axes[4], axes[5], 10)\nx2, x3 = np.meshgrid(x2s, x3s)\n\nfig = plt.figure(figsize=(6, 5))\nax = plt.subplot(111, projection='3d')\n\npositive_class = X[:, 0] &gt; 5\nX_pos = X[positive_class]\nX_neg = X[~positive_class]\nax.view_init(10, -70)\nax.plot(X_neg[:, 0], X_neg[:, 1], X_neg[:, 2], \"y^\")\nax.plot_wireframe(5, x2, x3, alpha=0.5)\nax.plot(X_pos[:, 0], X_pos[:, 1], X_pos[:, 2], \"gs\")\nax.set_xlabel(\"$x_1$\", fontsize=18)\nax.set_ylabel(\"$x_2$\", fontsize=18)\nax.set_zlabel(\"$x_3$\", fontsize=18)\nax.set_xlim(axes[0:2])\nax.set_ylim(axes[2:4])\nax.set_zlim(axes[4:6])\n\nsave_fig(\"manifold_decision_boundary_plot1\")\nplt.show()\n\nfig = plt.figure(figsize=(5, 4))\nax = plt.subplot(111)\n\nplt.plot(t[positive_class], X[positive_class, 1], \"gs\")\nplt.plot(t[~positive_class], X[~positive_class, 1], \"y^\")\nplt.axis([4, 15, axes[2], axes[3]])\nplt.xlabel(\"$z_1$\", fontsize=18)\nplt.ylabel(\"$x_2$\", fontsize=18, rotation=0)\nplt.grid(True)\n\nsave_fig(\"manifold_decision_boundary_plot2\")\nplt.show()\n\nfig = plt.figure(figsize=(6, 5))\nax = plt.subplot(111, projection='3d')\n\npositive_class = 2 * (t[:] - 4) &gt; X[:, 1]\nX_pos = X[positive_class]\nX_neg = X[~positive_class]\nax.view_init(10, -70)\nax.plot(X_neg[:, 0], X_neg[:, 1], X_neg[:, 2], \"y^\")\nax.plot(X_pos[:, 0], X_pos[:, 1], X_pos[:, 2], \"gs\")\nax.set_xlabel(\"$x_1$\", fontsize=18)\nax.set_ylabel(\"$x_2$\", fontsize=18)\nax.set_zlabel(\"$x_3$\", fontsize=18)\nax.set_xlim(axes[0:2])\nax.set_ylim(axes[2:4])\nax.set_zlim(axes[4:6])\n\nsave_fig(\"manifold_decision_boundary_plot3\")\nplt.show()\n\nfig = plt.figure(figsize=(5, 4))\nax = plt.subplot(111)\n\nplt.plot(t[positive_class], X[positive_class, 1], \"gs\")\nplt.plot(t[~positive_class], X[~positive_class, 1], \"y^\")\nplt.plot([4, 15], [0, 22], \"b-\", linewidth=2)\nplt.axis([4, 15, axes[2], axes[3]])\nplt.xlabel(\"$z_1$\", fontsize=18)\nplt.ylabel(\"$x_2$\", fontsize=18, rotation=0)\nplt.grid(True)\n\nsave_fig(\"manifold_decision_boundary_plot4\")\nplt.show()\n\n그림 저장 manifold_decision_boundary_plot1\n그림 저장 manifold_decision_boundary_plot2\n그림 저장 manifold_decision_boundary_plot3\n그림 저장 manifold_decision_boundary_plot4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;그림 8-7. 투영할 부분 공간 선택하기&gt; 생성 코드\n\nangle = np.pi / 5\nstretch = 5\nm = 200\n\nnp.random.seed(3)\nX = np.random.randn(m, 2) / 10\nX = X.dot(np.array([[stretch, 0],[0, 1]])) # stretch\nX = X.dot([[np.cos(angle), np.sin(angle)], [-np.sin(angle), np.cos(angle)]]) # rotate\n\nu1 = np.array([np.cos(angle), np.sin(angle)])\nu2 = np.array([np.cos(angle - 2 * np.pi/6), np.sin(angle - 2 * np.pi/6)])\nu3 = np.array([np.cos(angle - np.pi/2), np.sin(angle - np.pi/2)])\n\nX_proj1 = X.dot(u1.reshape(-1, 1))\nX_proj2 = X.dot(u2.reshape(-1, 1))\nX_proj3 = X.dot(u3.reshape(-1, 1))\n\nplt.figure(figsize=(8,4))\nplt.subplot2grid((3,2), (0, 0), rowspan=3)\nplt.plot([-1.4, 1.4], [-1.4*u1[1]/u1[0], 1.4*u1[1]/u1[0]], \"k-\", linewidth=1)\nplt.plot([-1.4, 1.4], [-1.4*u2[1]/u2[0], 1.4*u2[1]/u2[0]], \"k--\", linewidth=1)\nplt.plot([-1.4, 1.4], [-1.4*u3[1]/u3[0], 1.4*u3[1]/u3[0]], \"k:\", linewidth=2)\nplt.plot(X[:, 0], X[:, 1], \"bo\", alpha=0.5)\nplt.axis([-1.4, 1.4, -1.4, 1.4])\nplt.arrow(0, 0, u1[0], u1[1], head_width=0.1, linewidth=5, length_includes_head=True, head_length=0.1, fc='k', ec='k')\nplt.arrow(0, 0, u3[0], u3[1], head_width=0.1, linewidth=5, length_includes_head=True, head_length=0.1, fc='k', ec='k')\nplt.text(u1[0] + 0.1, u1[1] - 0.05, r\"$\\mathbf{c_1}$\", fontsize=22)\nplt.text(u3[0] + 0.1, u3[1], r\"$\\mathbf{c_2}$\", fontsize=22)\nplt.xlabel(\"$x_1$\", fontsize=18)\nplt.ylabel(\"$x_2$\", fontsize=18, rotation=0)\nplt.grid(True)\n\nplt.subplot2grid((3,2), (0, 1))\nplt.plot([-2, 2], [0, 0], \"k-\", linewidth=1)\nplt.plot(X_proj1[:, 0], np.zeros(m), \"bo\", alpha=0.3)\nplt.gca().get_yaxis().set_ticks([])\nplt.gca().get_xaxis().set_ticklabels([])\nplt.axis([-2, 2, -1, 1])\nplt.grid(True)\n\nplt.subplot2grid((3,2), (1, 1))\nplt.plot([-2, 2], [0, 0], \"k--\", linewidth=1)\nplt.plot(X_proj2[:, 0], np.zeros(m), \"bo\", alpha=0.3)\nplt.gca().get_yaxis().set_ticks([])\nplt.gca().get_xaxis().set_ticklabels([])\nplt.axis([-2, 2, -1, 1])\nplt.grid(True)\n\nplt.subplot2grid((3,2), (2, 1))\nplt.plot([-2, 2], [0, 0], \"k:\", linewidth=2)\nplt.plot(X_proj3[:, 0], np.zeros(m), \"bo\", alpha=0.3)\nplt.gca().get_yaxis().set_ticks([])\nplt.axis([-2, 2, -1, 1])\nplt.xlabel(\"$z_1$\", fontsize=18)\nplt.grid(True)\n\nsave_fig(\"pca_best_projection_plot\")\nplt.show()\n\n그림 저장 pca_best_projection_plot"
  },
  {
    "objectID": "Machine_Learning/08_dimensionality_reduction.html#적절한-차원수-선택하기",
    "href": "Machine_Learning/08_dimensionality_reduction.html#적절한-차원수-선택하기",
    "title": "08_dimensionality_reduction",
    "section": "적절한 차원수 선택하기",
    "text": "적절한 차원수 선택하기\n\nfrom sklearn.datasets import fetch_openml\n\nmnist = fetch_openml('mnist_784', version=1, as_frame=False)\nmnist.target = mnist.target.astype(np.uint8)\n\n\nfrom sklearn.model_selection import train_test_split\n\nX = mnist[\"data\"]\ny = mnist[\"target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\n\npca = PCA()\npca.fit(X_train)\ncumsum = np.cumsum(pca.explained_variance_ratio_)\nd = np.argmax(cumsum &gt;= 0.95) + 1\n\n\nd\n\n154\n\n\n&lt;그림 8-8. 차원 수에 대한 함수로 나타낸 설명된 분산&gt; 생성 코드\n\nplt.figure(figsize=(6,4))\nplt.plot(cumsum, linewidth=3)\nplt.axis([0, 400, 0, 1])\nplt.xlabel(\"Dimensions\")\nplt.ylabel(\"Explained Variance\")\nplt.plot([d, d], [0, 0.95], \"k:\")\nplt.plot([0, d], [0.95, 0.95], \"k:\")\nplt.plot(d, 0.95, \"ko\")\nplt.annotate(\"Elbow\", xy=(65, 0.85), xytext=(70, 0.7),\n             arrowprops=dict(arrowstyle=\"-&gt;\"), fontsize=16)\nplt.grid(True)\nsave_fig(\"explained_variance_plot\")\nplt.show()\n\n그림 저장 explained_variance_plot\n\n\n\n\n\n\npca = PCA(n_components=0.95)\nX_reduced = pca.fit_transform(X_train)\n\n\npca.n_components_\n\n154\n\n\n\nnp.sum(pca.explained_variance_ratio_)\n\n0.9504334914295707"
  },
  {
    "objectID": "Machine_Learning/08_dimensionality_reduction.html#압축을-위한-pca",
    "href": "Machine_Learning/08_dimensionality_reduction.html#압축을-위한-pca",
    "title": "08_dimensionality_reduction",
    "section": "압축을 위한 PCA",
    "text": "압축을 위한 PCA\n\npca = PCA(n_components=154)\nX_reduced = pca.fit_transform(X_train)\nX_recovered = pca.inverse_transform(X_reduced)\n\n&lt;그림 8-9. 분산의 95%가 유지된 MNIST 압축&gt; 생성 코드\n\ndef plot_digits(instances, images_per_row=5, **options):\n    size = 28\n    images_per_row = min(len(instances), images_per_row)\n    # n_rows = ceil(len(instances) / images_per_row) 와 동일합니다:\n    n_rows = (len(instances) - 1) // images_per_row + 1\n\n    # 필요하면 그리드 끝을 채우기 위해 빈 이미지를 추가합니다:\n    n_empty = n_rows * images_per_row - len(instances)\n    padded_instances = np.concatenate([instances, np.zeros((n_empty, size * size))], axis=0)\n\n    # 배열의 크기를 바꾸어 28×28 이미지를 담은 그리드로 구성합니다:\n    image_grid = padded_instances.reshape((n_rows, images_per_row, size, size))\n\n    # 축 0(이미지 그리드의 수직축)과 2(이미지의 수직축)를 합치고 축 1과 3(두 수평축)을 합칩니다. \n    # 먼저 transpose()를 사용해 결합하려는 축을 옆으로 이동한 다음 합칩니다:\n    big_image = image_grid.transpose(0, 2, 1, 3).reshape(n_rows * size,\n                                                         images_per_row * size)\n    # 하나의 큰 이미지를 얻었으므로 출력하면 됩니다:\n    plt.imshow(big_image, cmap = mpl.cm.binary, **options)\n    plt.axis(\"off\")\n\n\nplt.figure(figsize=(7, 4))\nplt.subplot(121)\nplot_digits(X_train[::2100])\nplt.title(\"Original\", fontsize=16)\nplt.subplot(122)\nplot_digits(X_recovered[::2100])\nplt.title(\"Compressed\", fontsize=16)\n\nsave_fig(\"mnist_compression_plot\")\n\n그림 저장 mnist_compression_plot\n\n\n\n\n\n\nX_reduced_pca = X_reduced"
  },
  {
    "objectID": "Machine_Learning/08_dimensionality_reduction.html#랜덤-pca",
    "href": "Machine_Learning/08_dimensionality_reduction.html#랜덤-pca",
    "title": "08_dimensionality_reduction",
    "section": "랜덤 PCA",
    "text": "랜덤 PCA\n\nrnd_pca = PCA(n_components=154, svd_solver=\"randomized\", random_state=42)\nX_reduced = rnd_pca.fit_transform(X_train)"
  },
  {
    "objectID": "Machine_Learning/08_dimensionality_reduction.html#점진적-pca",
    "href": "Machine_Learning/08_dimensionality_reduction.html#점진적-pca",
    "title": "08_dimensionality_reduction",
    "section": "점진적 PCA",
    "text": "점진적 PCA\n\nfrom sklearn.decomposition import IncrementalPCA\n\nn_batches = 100\ninc_pca = IncrementalPCA(n_components=154)\nfor X_batch in np.array_split(X_train, n_batches):\n    print(\".\", end=\"\") # 책에는 없음\n    inc_pca.partial_fit(X_batch)\n\nX_reduced = inc_pca.transform(X_train)\n\n....................................................................................................\n\n\n\nX_recovered_inc_pca = inc_pca.inverse_transform(X_reduced)\n\n압축이 잘 되었는지 확인해 보죠:\n\nplt.figure(figsize=(7, 4))\nplt.subplot(121)\nplot_digits(X_train[::2100])\nplt.subplot(122)\nplot_digits(X_recovered_inc_pca[::2100])\nplt.tight_layout()\n\n\n\n\n\nX_reduced_inc_pca = X_reduced\n\n일반 PCA와 점진적 PCA로 MNIST 데이터를 변환한 결과를 비교해 보겠습니다. 먼저 평균이 같은지 확인합니다:\n\nnp.allclose(pca.mean_, inc_pca.mean_)\n\nTrue\n\n\n하지만 결과는 완전히 동일하지 않습니다. 점진적 PCA는 아주 훌륭한 근사치를 제공하지만 완벽하지는 않습니다:\n\nnp.allclose(X_reduced_pca, X_reduced_inc_pca)\n\nFalse\n\n\nmemmap() 사용하기\nmemmap() 구조를 만들고 MNIST 데이터를 복사합니다. 이는 일반적으로 별도의 프로그램에서 먼저 수행됩니다:\n\nfilename = \"my_mnist.data\"\nm, n = X_train.shape\n\nX_mm = np.memmap(filename, dtype='float32', mode='write', shape=(m, n))\nX_mm[:] = X_train\n\n이제 데이터가 디스크에 저장되었는지 확인하기 위해 memmap() 객체를 삭제합니다.\n\ndel X_mm\n\n다음에 다른 프로그램에서 데이터를 로드하여 훈련에 사용합니다:\n\nX_mm = np.memmap(filename, dtype=\"float32\", mode=\"readonly\", shape=(m, n))\n\nbatch_size = m // n_batches\ninc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\ninc_pca.fit(X_mm)\n\nIncrementalPCA(batch_size=525, n_components=154)\n\n\n시간 복잡도\n주성분 개수를 바꾸어가며 점진적 PCA와 랜덤 PCA에 비해 일반 PCA 시간을 재어보겠습니다:\n\nrnd_pca = PCA(n_components=154, svd_solver=\"randomized\", random_state=42)\nX_reduced = rnd_pca.fit_transform(X_train)\n\n\nimport time\n\nfor n_components in (2, 10, 154):\n    print(\"n_components =\", n_components)\n    regular_pca = PCA(n_components=n_components, svd_solver=\"full\")\n    inc_pca = IncrementalPCA(n_components=n_components, batch_size=500)\n    rnd_pca = PCA(n_components=n_components, random_state=42, svd_solver=\"randomized\")\n\n    for pca in (regular_pca, inc_pca, rnd_pca):\n        t1 = time.time()\n        pca.fit(X_train)\n        t2 = time.time()\n        print(\"    {}: {:.1f} 초\".format(pca.__class__.__name__, t2 - t1))\n\nn_components = 2\n    PCA: 18.2 초\n    IncrementalPCA: 107.4 초\n    PCA: 5.4 초\nn_components = 10\n    PCA: 26.8 초\n    IncrementalPCA: 112.8 초\n    PCA: 7.5 초\nn_components = 154\n    PCA: 26.6 초\n    IncrementalPCA: 163.0 초\n    PCA: 13.5 초\n\n\n이번에는 데이터셋의 크기(샘플의 수)를 바꾸어가며 일반 PCA와 랜덤 PCA를 비교해 보겠습니다:\n\ntimes_rpca = []\ntimes_pca = []\nsizes = [1000, 10000, 20000, 30000, 40000, 50000, 70000, 100000, 200000, 500000]\nfor n_samples in sizes:\n    X = np.random.randn(n_samples, 5)\n    pca = PCA(n_components=2, svd_solver=\"randomized\", random_state=42)\n    t1 = time.time()\n    pca.fit(X)\n    t2 = time.time()\n    times_rpca.append(t2 - t1)\n    pca = PCA(n_components=2, svd_solver=\"full\")\n    t1 = time.time()\n    pca.fit(X)\n    t2 = time.time()\n    times_pca.append(t2 - t1)\n\nplt.plot(sizes, times_rpca, \"b-o\", label=\"RPCA\")\nplt.plot(sizes, times_pca, \"r-s\", label=\"PCA\")\nplt.xlabel(\"n_samples\")\nplt.ylabel(\"Training time\")\nplt.legend(loc=\"upper left\")\nplt.title(\"PCA and Randomized PCA time complexity \")\n\nText(0.5, 1.0, 'PCA and Randomized PCA time complexity ')\n\n\n\n\n\n이번에는 특성의 개수를 달리하면서 2,000 샘플이 있는 데이터셋에서 성능을 비교해 보겠습니다:\n\ntimes_rpca = []\ntimes_pca = []\nsizes = [1000, 2000, 3000, 4000, 5000, 6000]\nfor n_features in sizes:\n    X = np.random.randn(2000, n_features)\n    pca = PCA(n_components=2, random_state=42, svd_solver=\"randomized\")\n    t1 = time.time()\n    pca.fit(X)\n    t2 = time.time()\n    times_rpca.append(t2 - t1)\n    pca = PCA(n_components=2, svd_solver=\"full\")\n    t1 = time.time()\n    pca.fit(X)\n    t2 = time.time()\n    times_pca.append(t2 - t1)\n\nplt.plot(sizes, times_rpca, \"b-o\", label=\"RPCA\")\nplt.plot(sizes, times_pca, \"r-s\", label=\"PCA\")\nplt.xlabel(\"n_features\")\nplt.ylabel(\"Training time\")\nplt.legend(loc=\"upper left\")\nplt.title(\"PCA and Randomized PCA time complexity \")\nplt.show()"
  },
  {
    "objectID": "Machine_Learning/08_dimensionality_reduction.html#커널-선택과-하이퍼파라미터-튜닝",
    "href": "Machine_Learning/08_dimensionality_reduction.html#커널-선택과-하이퍼파라미터-튜닝",
    "title": "08_dimensionality_reduction",
    "section": "커널 선택과 하이퍼파라미터 튜닝",
    "text": "커널 선택과 하이퍼파라미터 튜닝\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\n\nclf = Pipeline([\n        (\"kpca\", KernelPCA(n_components=2)),\n        (\"log_reg\", LogisticRegression(solver=\"lbfgs\"))\n    ])\n\nparam_grid = [{\n        \"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n        \"kpca__kernel\": [\"rbf\", \"sigmoid\"]\n    }]\n\ngrid_search = GridSearchCV(clf, param_grid, cv=3)\ngrid_search.fit(X, y)\n\nGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('kpca', KernelPCA(n_components=2)),\n                                       ('log_reg', LogisticRegression())]),\n             param_grid=[{'kpca__gamma': array([0.03      , 0.03222222, 0.03444444, 0.03666667, 0.03888889,\n       0.04111111, 0.04333333, 0.04555556, 0.04777778, 0.05      ]),\n                          'kpca__kernel': ['rbf', 'sigmoid']}])\n\n\n\nprint(grid_search.best_params_)\n\n{'kpca__gamma': 0.043333333333333335, 'kpca__kernel': 'rbf'}\n\n\n\nrbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.0433,\n                    fit_inverse_transform=True)\nX_reduced = rbf_pca.fit_transform(X)\nX_preimage = rbf_pca.inverse_transform(X_reduced)\n\n\nfrom sklearn.metrics import mean_squared_error\n\nmean_squared_error(X, X_preimage)\n\n32.786308795766104"
  },
  {
    "objectID": "Machine_Learning/08_dimensionality_reduction.html#to-8.",
    "href": "Machine_Learning/08_dimensionality_reduction.html#to-8.",
    "title": "08_dimensionality_reduction",
    "section": "1. to 8.",
    "text": "1. to 8.\n부록 A 참조."
  },
  {
    "objectID": "Machine_Learning/08_dimensionality_reduction.html#section",
    "href": "Machine_Learning/08_dimensionality_reduction.html#section",
    "title": "08_dimensionality_reduction",
    "section": "9.",
    "text": "9.\n문제: (3장에서 소개한) MNIST 데이터셋을 로드하고 훈련 세트와 테스트 세트로 분할합니다(처음 60,000개는 훈련을 위한 샘플이고 나머지 10,000개는 테스트용입니다).\n앞서 로드한 MNIST 데이터셋을 사용합니다.\n\nX_train = mnist['data'][:60000]\ny_train = mnist['target'][:60000]\n\nX_test = mnist['data'][60000:]\ny_test = mnist['target'][60000:]\n\n문제: 이 데이터셋에 랜덤 포레스트 분류기를 훈련시키고 얼마나 오래 걸리는지 시간을 잰 다음, 테스트 세트로 만들어진 모델을 평가합니다.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n\n\nimport time\n\nt0 = time.time()\nrnd_clf.fit(X_train, y_train)\nt1 = time.time()\n\n\nprint(\"훈련 시간: {:.2f}s\".format(t1 - t0))\n\n훈련 시간: 54.82s\n\n\n\nfrom sklearn.metrics import accuracy_score\n\ny_pred = rnd_clf.predict(X_test)\naccuracy_score(y_test, y_pred)\n\n0.9705\n\n\n문제: 그런 다음 PCA를 사용해 설명된 분산이 95%가 되도록 차원을 축소합니다.\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=0.95)\nX_train_reduced = pca.fit_transform(X_train)\n\n문제: 이 축소된 데이터셋에 새로운 랜덤 포레스트 분류기를 훈련시키고 얼마나 오래 걸리는지 확인합니다. 훈련 속도가 더 빨라졌나요?\n\nrnd_clf2 = RandomForestClassifier(n_estimators=100, random_state=42)\nt0 = time.time()\nrnd_clf2.fit(X_train_reduced, y_train)\nt1 = time.time()\n\n\nprint(\"훈련 시간: {:.2f}s\".format(t1 - t0))\n\n훈련 시간: 133.43s\n\n\n이런! 훈련이 두 배 이상 느려졌습니다! 어떻게 이럴 수 있죠? 이 장에서 보았듯이 차원 축소는 언제나 훈련 시간을 줄여주지 못합니다. 데이터셋, 모델, 훈련 알고리즘에 따라 달라집니다. 그림 8-6(위에 있는 manifold_decision_boundary_plot* 그래프)을 참고하세요. 랜덤 포레스트 분류기 대신 소프트맥스 분류기를 적용하면 PCA를 사용해서 훈련 시간을 3배나 줄일 수 있습니다. 잠시 후에 실제로 한번 해보겠습니다. 하지만 먼저 새로운 랜덤 포레스트 분류기의 정밀도를 확인해 보죠.\n문제: 이제 테스트 세트에서 이 분류기를 평가해보세요. 이전 분류기와 비교해서 어떤가요?\n\nX_test_reduced = pca.transform(X_test)\n\ny_pred = rnd_clf2.predict(X_test_reduced)\naccuracy_score(y_test, y_pred)\n\n0.9481\n\n\n차원 축소를 했을 때 유용한 정보를 일부 잃었기 때문에 성능이 조금 감소되는 것이 일반적입니다. 그렇지만 이 경우에는 성능 감소가 좀 심각한 것 같습니다. PCA가 별로 도움이 되지 않네요. 훈련 시간도 느려지고 성능도 감소했습니다. :(\n소프트맥스 회귀를 사용하면 도움이 되는지 확인해 보겠습니다:\n\nfrom sklearn.linear_model import LogisticRegression\n\nlog_clf = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", random_state=42)\nt0 = time.time()\nlog_clf.fit(X_train, y_train)\nt1 = time.time()\n\n/home/haesun/handson-ml2/.env/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\n\n\nprint(\"훈련 시간: {:.2f}s\".format(t1 - t0))\n\n훈련 시간: 71.91s\n\n\n\ny_pred = log_clf.predict(X_test)\naccuracy_score(y_test, y_pred)\n\n0.9255\n\n\n좋네요. 소프트맥스 회귀는 랜덤 포레스트 분류기보다 이 데이터셋에서 훈련하는데 더 많은 시간이 걸리고 테스트 세트에서의 성능도 더 나쁩니다. 하지만 지금 관심 사항은 아닙니다. PCA가 소프트맥스 회귀에 얼마나 도움이 되는지가 궁금합니다. 축소된 데이터셋에 소프트맥스 회귀 모델을 훈련시켜 보겠습니다:\n\nlog_clf2 = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", random_state=42)\nt0 = time.time()\nlog_clf2.fit(X_train_reduced, y_train)\nt1 = time.time()\n\n/home/haesun/handson-ml2/.env/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\n\n\nprint(\"훈련 시간: {:.2f}s\".format(t1 - t0))\n\n훈련 시간: 26.99s\n\n\n와우! 차원 축소가 속도를 2배 이상 빠르게 만들었습니다. :) 모델의 정확도를 확인해 보겠습니다:\n\ny_pred = log_clf2.predict(X_test_reduced)\naccuracy_score(y_test, y_pred)\n\n0.9201\n\n\n성능이 조금 감소되었지만 애플리케이션에 따라서 2배 이상의 속도 향상에 대한 댓가로 적절한 것 같습니다.\n여기서 알 수 있는 것: PCA는 속도를 아주 빠르게 만들어 주지만 항상 그런 것은 아니다!"
  },
  {
    "objectID": "Machine_Learning/08_dimensionality_reduction.html#section-1",
    "href": "Machine_Learning/08_dimensionality_reduction.html#section-1",
    "title": "08_dimensionality_reduction",
    "section": "10.",
    "text": "10.\n문제: t-SNE 알고리즘을 사용해 MNIST 데이터셋을 2차원으로 축소시키고 맷플롯립으로 그래프를 그려보세요. 이미지의 타깃 클래스마다 10가지 색깔로 나타낸 산점도를 그릴 수 있습니다.\n앞서 로드한 MNIST 데이터셋을 사용합니다.\n전체 60,000개의 이미지에 차원 축소를 하면 매우 오랜 시간이 걸리므로 10,000개의 이미지만 무작위로 선택하여 사용하겠습니다:\n\nnp.random.seed(42)\n\nm = 10000\nidx = np.random.permutation(60000)[:m]\n\nX = mnist['data'][idx]\ny = mnist['target'][idx]\n\n이제 t-SNE를 사용해 2D로 차원을 축소해 그래프로 나타냅니다:\n\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, random_state=42)\nX_reduced = tsne.fit_transform(X)\n\n/home/haesun/handson-ml2/.env/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n  FutureWarning,\n/home/haesun/handson-ml2/.env/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n  FutureWarning,\n\n\n산점도를 그리기 위해 맷플롯립의 scatter() 함수를 사용합니다. 각 숫자마다 다른 색깔을 사용합니다:\n\nplt.figure(figsize=(13,10))\nplt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, cmap=\"jet\")\nplt.axis('off')\nplt.colorbar()\nplt.show()\n\n\n\n\n아름답지 않나요? :) 이 그래프는 어떤 숫자가 다른 것과 구분이 쉬운지 알려 줍니다(가령, 0, 6, 8이 잘 구분되어 있습니다). 그리고 어떤 숫자가 구분이 어려운지 알려 줍니다(가령, 4, 9, 5, 3 등입니다).\n많이 겹쳐진 것 같은 숫자 2, 3, 5에 집중해 보겠습니다.\n\nplt.figure(figsize=(9,9))\ncmap = mpl.cm.get_cmap(\"jet\")\nfor digit in (2, 3, 5):\n    plt.scatter(X_reduced[y == digit, 0], X_reduced[y == digit, 1], c=[cmap(digit / 9)])\nplt.axis('off')\nplt.show()\n\n\n\n\n이 3개의 숫자에 t-SNE를 실행시켜 더 나은 이미지를 만들 수 있는지 보겠습니다:\n\nidx = (y == 2) | (y == 3) | (y == 5) \nX_subset = X[idx]\ny_subset = y[idx]\n\ntsne_subset = TSNE(n_components=2, random_state=42)\nX_subset_reduced = tsne_subset.fit_transform(X_subset)\n\n/home/haesun/handson-ml2/.env/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n  FutureWarning,\n/home/haesun/handson-ml2/.env/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n  FutureWarning,\n\n\n\nplt.figure(figsize=(9,9))\nfor digit in (2, 3, 5):\n    plt.scatter(X_subset_reduced[y_subset == digit, 0], X_subset_reduced[y_subset == digit, 1], c=[cmap(digit / 9)])\nplt.axis('off')\nplt.show()\n\n\n\n\n훨씬 좋네요. 이제 군집이 덜 겹쳐졌습니다. 하지만 숫자 3이 여러 군데 흩어져 있습니다. 또한 2와 5의 군집은 두개로 보입니다. 각 군집에 숫자를 몇 개씩 나타내면 이런 이유를 훨씬 이해하는기 좋습니다. 그렇게 한번 해보죠.\n문제: 또는 샘플의 위치에 각기 다른 색깔의 숫자를 나타낼 수도 있고, 숫자 이미지 자체의 크기를 줄여서 그릴 수도 있습니다(모든 숫자를 다 그리면 그래프가 너무 복잡해지므로 무작위로 선택한 샘플만 그리거나, 인접한 곳에 다른 샘플이 그려져 있지 않은 경우에만 그립니다). 잘 분리된 숫자의 군집을 시각화할 수 있을 것입니다.\n(위에 있는 산점도와 비슷하게) 산점도와 색깔있는 숫자를 쓰기위해 plot_digits() 함수를 만듭니다. 이 숫자 사이의 거리가 최소가 되도록 합니다. 숫자 이미지가 있다면 대신 이를 사용합니다. 이 코드는 사이킷런의 훌륭한 데모(plot_lle_digits, 데이터셋은 다릅니다)를 참고했습니다.\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom matplotlib.offsetbox import AnnotationBbox, OffsetImage\n\ndef plot_digits(X, y, min_distance=0.05, images=None, figsize=(13, 10)):\n    # 입력 특성의 스케일을 0에서 1 사이로 만듭니다.\n    X_normalized = MinMaxScaler().fit_transform(X)\n    # 그릴 숫자의 좌표 목록을 만듭니다.\n    # 반복문 아래에서 `if` 문장을 쓰지 않기 위해 시작할 때 이미 그래프가 그려져 있다고 가정합니다.\n    neighbors = np.array([[10., 10.]])\n    # 나머지는 이해하기 쉽습니다.\n    plt.figure(figsize=figsize)\n    cmap = mpl.cm.get_cmap(\"jet\")\n    digits = np.unique(y)\n    for digit in digits:\n        plt.scatter(X_normalized[y == digit, 0], X_normalized[y == digit, 1], c=[cmap(digit / 9)])\n    plt.axis(\"off\")\n    ax = plt.gcf().gca()  # 현재 그래프의 축을 가져옵니다.\n    for index, image_coord in enumerate(X_normalized):\n        closest_distance = np.linalg.norm(neighbors - image_coord, axis=1).min()\n        if closest_distance &gt; min_distance:\n            neighbors = np.r_[neighbors, [image_coord]]\n            if images is None:\n                plt.text(image_coord[0], image_coord[1], str(int(y[index])),\n                         color=cmap(y[index] / 9), fontdict={\"weight\": \"bold\", \"size\": 16})\n            else:\n                image = images[index].reshape(28, 28)\n                imagebox = AnnotationBbox(OffsetImage(image, cmap=\"binary\"), image_coord)\n                ax.add_artist(imagebox)\n\n시작해 보죠! 먼저 색깔이 입혀진 숫자를 써 보겠습니다:\n\nplot_digits(X_reduced, y)\n\n\n\n\n꽤 좋습니다. 하지만 아름답지는 않네요. 숫자 이미지를 사용해 보겠습니다:\n\nplot_digits(X_reduced, y, images=X, figsize=(35, 25))\n\n\n\n\n\nplot_digits(X_subset_reduced, y_subset, images=X_subset, figsize=(22, 22))\n\n\n\n\n문제: PCA, LLE, MDS 같은 차원 축소 알고리즘을 적용해보고 시각화 결과를 비교해보세요.\nPCA부터 시작해 보죠. 얼마나 오래 걸리는지도 재어 보겠습니다:\n\nfrom sklearn.decomposition import PCA\nimport time\n\nt0 = time.time()\nX_pca_reduced = PCA(n_components=2, random_state=42).fit_transform(X)\nt1 = time.time()\nprint(\"PCA 시간: {:.1f}s.\".format(t1 - t0))\nplot_digits(X_pca_reduced, y)\nplt.show()\n\nPCA 시간: 1.7s.\n\n\n\n\n\n와우, PCA가 아주 빠르네요! 몇 개의 군집이 보이지만 너무 겹쳐져 있습니다. LLE를 사용해 보죠:\n\nfrom sklearn.manifold import LocallyLinearEmbedding\n\nt0 = time.time()\nX_lle_reduced = LocallyLinearEmbedding(n_components=2, random_state=42).fit_transform(X)\nt1 = time.time()\nprint(\"LLE 시간: {:.1f}s.\".format(t1 - t0))\nplot_digits(X_lle_reduced, y)\nplt.show()\n\nLLE 시간: 151.6s.\n\n\n\n\n\n시간이 좀 걸리고 결과도 아주 좋지는 않습니다. 분산의 95%를 보존하도록 먼저 PCA를 적용하면 어떻게 되는지 보겠습니다:\n\nfrom sklearn.pipeline import Pipeline\n\npca_lle = Pipeline([\n    (\"pca\", PCA(n_components=0.95, random_state=42)),\n    (\"lle\", LocallyLinearEmbedding(n_components=2, random_state=42)),\n])\nt0 = time.time()\nX_pca_lle_reduced = pca_lle.fit_transform(X)\nt1 = time.time()\nprint(\"PCA+LLE 시간: {:.1f}s.\".format(t1 - t0))\nplot_digits(X_pca_lle_reduced, y)\nplt.show()\n\nPCA+LLE 시간: 162.3s.\n\n\n\n\n\n결과는 비슷하지만 걸린 시간은 4배나 줄었습니다.\nMDS를 시도해 보죠. 10,000개 샘플을 적용하면 이 알고리즘은 너무 오래걸리므로 2,000개만 시도해 보겠습니다:\n\nfrom sklearn.manifold import MDS\n\nm = 2000\nt0 = time.time()\nX_mds_reduced = MDS(n_components=2, random_state=42).fit_transform(X[:m])\nt1 = time.time()\nprint(\"MDS 시간 {:.1f}s (on just 2,000 MNIST images instead of 10,000).\".format(t1 - t0))\nplot_digits(X_mds_reduced, y[:m])\nplt.show()\n\nMDS 시간 160.1s (on just 2,000 MNIST images instead of 10,000).\n\n\n\n\n\n아 이건 좋지 않아 보이네요. 모든 군집이 너무 중복되어 있습니다. 먼저 PCA를 적용하면 빨라질까요?\n\nfrom sklearn.pipeline import Pipeline\n\npca_mds = Pipeline([\n    (\"pca\", PCA(n_components=0.95, random_state=42)),\n    (\"mds\", MDS(n_components=2, random_state=42)),\n])\nt0 = time.time()\nX_pca_mds_reduced = pca_mds.fit_transform(X[:2000])\nt1 = time.time()\nprint(\"PCA+MDS 시간 {:.1f}s (on 2,000 MNIST images).\".format(t1 - t0))\nplot_digits(X_pca_mds_reduced, y[:2000])\nplt.show()\n\nPCA+MDS 시간 164.5s (on 2,000 MNIST images).\n\n\n\n\n\n같은 결과에 속도도 동일합니다. PCA가 도움이 되지 않네요.\nLDA를 시도해 보죠:\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nt0 = time.time()\nX_lda_reduced = LinearDiscriminantAnalysis(n_components=2).fit_transform(X, y)\nt1 = time.time()\nprint(\"LDA 시간 {:.1f}s.\".format(t1 - t0))\nplot_digits(X_lda_reduced, y, figsize=(12,12))\nplt.show()\n\nLDA 시간 9.3s.\n\n\n\n\n\n매우 빨라 처음엔 괜찮아 보이지만 자세히 보면 몇 개의 군집이 심각하게 중복되어 있습니다.\n아마 이 비교에서 t-SNE가 승자같네요. 시간을 재어 보진 않았으니 한번 해보죠:\n\nfrom sklearn.manifold import TSNE\n\nt0 = time.time()\nX_tsne_reduced = TSNE(n_components=2, random_state=42).fit_transform(X)\nt1 = time.time()\nprint(\"t-SNE 시간 {:.1f}s.\".format(t1 - t0))\nplot_digits(X_tsne_reduced, y)\nplt.show()\n\n/home/haesun/handson-ml2/.env/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n  FutureWarning,\n/home/haesun/handson-ml2/.env/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n  FutureWarning,\n\n\nt-SNE 시간 142.1s.\n\n\n\n\n\nLLE 보다 두 배나 느립니다. 하지만 MDS 보단 훨씬 빠르고 결과물도 아주 좋습니다. PCA가 속도를 높여줄 수 있는지 확인해 보겠습니다:\n\npca_tsne = Pipeline([\n    (\"pca\", PCA(n_components=0.95, random_state=42)),\n    (\"tsne\", TSNE(n_components=2, random_state=42)),\n])\nt0 = time.time()\nX_pca_tsne_reduced = pca_tsne.fit_transform(X)\nt1 = time.time()\nprint(\"PCA+t-SNE 시간 {:.1f}s.\".format(t1 - t0))\nplot_digits(X_pca_tsne_reduced, y)\nplt.show()\n\n/home/haesun/handson-ml2/.env/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n  FutureWarning,\n/home/haesun/handson-ml2/.env/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n  FutureWarning,\n\n\nPCA+t-SNE 시간 147.9s.\n\n\n\n\n\n네, 결과물에 영향을 미치지 않으면서 PCA 속도가 2배 이상 정도 향상되었습니다. 이것이 제일 좋네요!"
  },
  {
    "objectID": "Machine_Learning/10_neural_nets_with_keras.html",
    "href": "Machine_Learning/10_neural_nets_with_keras.html",
    "title": "10_neural_nets_with_keras",
    "section": "",
    "text": "10장 – 케라스를 사용한 인공 신경망 소개\n이 노트북은 10장에 있는 모든 샘플 코드와 연습문제 해답을 가지고 있습니다."
  },
  {
    "objectID": "Machine_Learning/10_neural_nets_with_keras.html#to-9.",
    "href": "Machine_Learning/10_neural_nets_with_keras.html#to-9.",
    "title": "10_neural_nets_with_keras",
    "section": "1. to 9.",
    "text": "1. to 9.\n부록 A 참조."
  },
  {
    "objectID": "Machine_Learning/10_neural_nets_with_keras.html#section",
    "href": "Machine_Learning/10_neural_nets_with_keras.html#section",
    "title": "10_neural_nets_with_keras",
    "section": "10.",
    "text": "10.\n문제: 심층 MLP를 MNIST 데이터셋에 훈련해보세요(keras.datasets.mnist.load_data() 함수를 사용해 데이터를 적재할 수 있습니다). 98% 이상의 정확도를 얻을 수 있는지 확인해보세요. 이 장에서 소개한 방법을 사용해 최적의 학습률을 찾아보세요(즉 학습률을 지수적으로 증가시키면서 손실을 그래프로 그립니다. 그다음 손실이 다시 증가하는 지점을 찾습니다). 모든 부가 기능을 추가해보세요. 즉, 체크포인트를 저장하고, 조기 종료를 사용하고, 텐서보드를 사용해 학습 곡선을 그려보세요.\n데이터셋을 적재해보죠:\n\n(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n11493376/11490434 [==============================] - 0s 0us/step\n11501568/11490434 [==============================] - 0s 0us/step\n\n\n패션 MNIST 데이터셋처럼 MNIST 훈련 세트는 28x28 픽셀의 흑백 이미지 60,000개로 이루어져 있습니다:\n\nX_train_full.shape\n\n(60000, 28, 28)\n\n\n각 픽셀 강도는 바이트(0~255)로 표현됩니다:\n\nX_train_full.dtype\n\ndtype('uint8')\n\n\n전체 훈련 세트를 검증 세트와 (더 작은) 훈련 세트로 나누어 보겠습니다. 패션 MNIST처럼 픽셀 강도를 255로 나누어 0-1 범위의 실수로 변환합니다:\n\nX_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\ny_valid, y_train = y_train_full[:5000], y_train_full[5000:]\nX_test = X_test / 255.\n\n맷플롯립의 imshow() 함수와 'binary' 컬러 맵으로 이미지를 출력해 보죠:\n\nplt.imshow(X_train[0], cmap=\"binary\")\nplt.axis('off')\nplt.show()\n\n\n\n\n레이블은 (uint8로 표현된) 0에서 9까지 클래스 아이디입니다. 편리하게도 클래스 아이디는 이미지가 나타내는 숫자와 같습니다. 따라서 class_names 배열을 만들 필요가 없습니다:\n\ny_train\n\narray([7, 3, 4, ..., 5, 6, 8], dtype=uint8)\n\n\n검증 세트는 5,000개의 이미지를 담고 있고 테스트 세트는 10,000개의 이미지를 담고 있습니다:\n\nX_valid.shape\n\n(5000, 28, 28)\n\n\n\nX_test.shape\n\n(10000, 28, 28)\n\n\n이 데이터셋에 있는 이미지 샘플 몇 개를 출력해 보죠:\n\nn_rows = 4\nn_cols = 10\nplt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\nfor row in range(n_rows):\n    for col in range(n_cols):\n        index = n_cols * row + col\n        plt.subplot(n_rows, n_cols, index + 1)\n        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n        plt.axis('off')\n        plt.title(y_train[index], fontsize=12)\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\nplt.show()\n\n\n\n\n간단한 밀집 신경망을 만들고 최적의 학습률을 찾아 보겠습니다. 반복마다 학습률을 증가시키기 위해 콜백을 사용합니다. 이 콜백은 반복마다 학습률과 손실을 기록합니다:\n\nK = keras.backend\n\nclass ExponentialLearningRate(keras.callbacks.Callback):\n    def __init__(self, factor):\n        self.factor = factor\n        self.rates = []\n        self.losses = []\n    def on_batch_end(self, batch, logs):\n        self.rates.append(K.get_value(self.model.optimizer.lr))\n        self.losses.append(logs[\"loss\"])\n        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)\n\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(300, activation=\"relu\"),\n    keras.layers.Dense(100, activation=\"relu\"),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\n\n작은 학습률 1e-3에서 시작하여 반복마다 0.5%씩 증가합니다:\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n              metrics=[\"accuracy\"])\nexpon_lr = ExponentialLearningRate(factor=1.005)\n\n/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n\n\n모델을 1 에포크만 훈련해 보죠:\n\nhistory = model.fit(X_train, y_train, epochs=1,\n                    validation_data=(X_valid, y_valid),\n                    callbacks=[expon_lr])\n\n   1/1719 [..............................] - ETA: 6:39 - loss: 2.5649 - accuracy: 0.0625WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0019s vs `on_train_batch_end` time: 0.0023s). Check your callbacks.\n1719/1719 [==============================] - 6s 3ms/step - loss: nan - accuracy: 0.5935 - val_loss: nan - val_accuracy: 0.0958\n\n\n학습률에 대한 함수로 손실을 그릴 수 있습니다:\n\nplt.plot(expon_lr.rates, expon_lr.losses)\nplt.gca().set_xscale('log')\nplt.hlines(min(expon_lr.losses), min(expon_lr.rates), max(expon_lr.rates))\nplt.axis([min(expon_lr.rates), max(expon_lr.rates), 0, expon_lr.losses[0]])\nplt.grid()\nplt.xlabel(\"Learning rate\")\nplt.ylabel(\"Loss\")\n\nText(0, 0.5, 'Loss')\n\n\n\n\n\n학습률이 6e-1을 지날 떄 손실이 갑자기 솟구치기 때문에 3e-1을 학습률로 사용하겠습니다:\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(300, activation=\"relu\"),\n    keras.layers.Dense(100, activation=\"relu\"),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\n\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=keras.optimizers.SGD(learning_rate=3e-1),\n              metrics=[\"accuracy\"])\n\n/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n\n\n\nrun_index = 1 # 실행할 때마다 이 값을 늘립니다\nrun_logdir = os.path.join(os.curdir, \"my_mnist_logs\", \"run_{:03d}\".format(run_index))\nrun_logdir\n\n'./my_mnist_logs/run_001'\n\n\n\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\ncheckpoint_cb = keras.callbacks.ModelCheckpoint(\"my_mnist_model.h5\", save_best_only=True)\ntensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n\nhistory = model.fit(X_train, y_train, epochs=100,\n                    validation_data=(X_valid, y_valid),\n                    callbacks=[checkpoint_cb, early_stopping_cb, tensorboard_cb])\n\nEpoch 1/100\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2363 - accuracy: 0.9264 - val_loss: 0.1022 - val_accuracy: 0.9696\nEpoch 2/100\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.0944 - accuracy: 0.9703 - val_loss: 0.0907 - val_accuracy: 0.9730\nEpoch 3/100\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.0676 - accuracy: 0.9785 - val_loss: 0.0840 - val_accuracy: 0.9754\nEpoch 4/100\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.0482 - accuracy: 0.9844 - val_loss: 0.0695 - val_accuracy: 0.9812\nEpoch 5/100\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.0342 - accuracy: 0.9890 - val_loss: 0.0813 - val_accuracy: 0.9808\nEpoch 6/100\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.0303 - accuracy: 0.9898 - val_loss: 0.0692 - val_accuracy: 0.9828\nEpoch 7/100\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.0237 - accuracy: 0.9920 - val_loss: 0.0810 - val_accuracy: 0.9812\nEpoch 8/100\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.0193 - accuracy: 0.9935 - val_loss: 0.0863 - val_accuracy: 0.9764\nEpoch 9/100\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.0185 - accuracy: 0.9937 - val_loss: 0.0949 - val_accuracy: 0.9798\nEpoch 10/100\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.0141 - accuracy: 0.9951 - val_loss: 0.0842 - val_accuracy: 0.9828\nEpoch 11/100\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.0144 - accuracy: 0.9955 - val_loss: 0.1024 - val_accuracy: 0.9798\nEpoch 12/100\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.0081 - accuracy: 0.9973 - val_loss: 0.1284 - val_accuracy: 0.9728\nEpoch 13/100\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.0078 - accuracy: 0.9976 - val_loss: 0.1002 - val_accuracy: 0.9820\nEpoch 14/100\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.0129 - accuracy: 0.9959 - val_loss: 0.0897 - val_accuracy: 0.9836\nEpoch 15/100\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.0083 - accuracy: 0.9974 - val_loss: 0.0812 - val_accuracy: 0.9858\nEpoch 16/100\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.0095 - accuracy: 0.9967 - val_loss: 0.1058 - val_accuracy: 0.9814\nEpoch 17/100\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.0080 - accuracy: 0.9974 - val_loss: 0.0867 - val_accuracy: 0.9862\nEpoch 18/100\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.0997 - val_accuracy: 0.9850\nEpoch 19/100\n1719/1719 [==============================] - 3s 2ms/step - loss: 0.0043 - accuracy: 0.9987 - val_loss: 0.0981 - val_accuracy: 0.9842\nEpoch 20/100\n1719/1719 [==============================] - 4s 2ms/step - loss: 0.0016 - accuracy: 0.9995 - val_loss: 0.0913 - val_accuracy: 0.9856\nEpoch 21/100\n1719/1719 [==============================] - 3s 2ms/step - loss: 3.5843e-04 - accuracy: 0.9999 - val_loss: 0.0860 - val_accuracy: 0.9862\nEpoch 22/100\n1719/1719 [==============================] - 4s 2ms/step - loss: 1.0448e-04 - accuracy: 1.0000 - val_loss: 0.0844 - val_accuracy: 0.9878\nEpoch 23/100\n1719/1719 [==============================] - 3s 2ms/step - loss: 5.7757e-05 - accuracy: 1.0000 - val_loss: 0.0855 - val_accuracy: 0.9876\nEpoch 24/100\n1719/1719 [==============================] - 3s 2ms/step - loss: 4.7349e-05 - accuracy: 1.0000 - val_loss: 0.0859 - val_accuracy: 0.9876\nEpoch 25/100\n1719/1719 [==============================] - 4s 2ms/step - loss: 4.1029e-05 - accuracy: 1.0000 - val_loss: 0.0863 - val_accuracy: 0.9876\nEpoch 26/100\n1719/1719 [==============================] - 4s 2ms/step - loss: 3.6531e-05 - accuracy: 1.0000 - val_loss: 0.0869 - val_accuracy: 0.9874\n\n\n\nmodel = keras.models.load_model(\"my_mnist_model.h5\") # rollback to best model\nmodel.evaluate(X_test, y_test)\n\n313/313 [==============================] - 1s 2ms/step - loss: 0.0716 - accuracy: 0.9792\n\n\n[0.07158412784337997, 0.979200005531311]\n\n\n98% 정확도를 얻었습니다. 마지막으로 텐서보드를 사용해 학습 곡선을 살펴보겠습니다:\n\n%tensorboard --logdir=./my_mnist_logs --port=6007"
  },
  {
    "objectID": "Machine_Learning/12_custom_models_and_training_with_tensorflow.html",
    "href": "Machine_Learning/12_custom_models_and_training_with_tensorflow.html",
    "title": "12_custom_models_and_training_with_tensorflow",
    "section": "",
    "text": "12장 – 텐서플로를 사용한 사용자 정의 모델과 훈련\n이 노트북은 12장에 있는 모든 샘플 코드와 연습문제 해답을 가지고 있습니다."
  },
  {
    "objectID": "Machine_Learning/12_custom_models_and_training_with_tensorflow.html#텐서와-연산",
    "href": "Machine_Learning/12_custom_models_and_training_with_tensorflow.html#텐서와-연산",
    "title": "12_custom_models_and_training_with_tensorflow",
    "section": "텐서와 연산",
    "text": "텐서와 연산\n\n텐서\n\ntf.constant([[1., 2., 3.], [4., 5., 6.]]) # 행렬\n\n&lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[1., 2., 3.],\n       [4., 5., 6.]], dtype=float32)&gt;\n\n\n\ntf.constant(42) # 스칼라\n\n&lt;tf.Tensor: shape=(), dtype=int32, numpy=42&gt;\n\n\n\nt = tf.constant([[1., 2., 3.], [4., 5., 6.]])\nt\n\n&lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[1., 2., 3.],\n       [4., 5., 6.]], dtype=float32)&gt;\n\n\n\nt.shape\n\nTensorShape([2, 3])\n\n\n\nt.dtype\n\ntf.float32\n\n\n\n\n인덱싱\n\nt[:, 1:]\n\n&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[2., 3.],\n       [5., 6.]], dtype=float32)&gt;\n\n\n\nt[..., 1, tf.newaxis]\n\n&lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy=\narray([[2.],\n       [5.]], dtype=float32)&gt;\n\n\n\n\n연산\n\nt + 10\n\n&lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[11., 12., 13.],\n       [14., 15., 16.]], dtype=float32)&gt;\n\n\n\ntf.square(t)\n\n&lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[ 1.,  4.,  9.],\n       [16., 25., 36.]], dtype=float32)&gt;\n\n\n\nt @ tf.transpose(t)\n\n&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[14., 32.],\n       [32., 77.]], dtype=float32)&gt;\n\n\n\n\nkeras.backend 사용하기\n\nfrom tensorflow import keras\nK = keras.backend\nK.square(K.transpose(t)) + 10\n\n&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=\narray([[11., 26.],\n       [14., 35.],\n       [19., 46.]], dtype=float32)&gt;\n\n\n\n\n넘파이 변환\n\na = np.array([2., 4., 5.])\ntf.constant(a)\n\n&lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 5.])&gt;\n\n\n\nt.numpy()\n\narray([[1., 2., 3.],\n       [4., 5., 6.]], dtype=float32)\n\n\n\nnp.array(t)\n\narray([[1., 2., 3.],\n       [4., 5., 6.]], dtype=float32)\n\n\n\ntf.square(a)\n\n&lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([ 4., 16., 25.])&gt;\n\n\n\nnp.square(t)\n\narray([[ 1.,  4.,  9.],\n       [16., 25., 36.]], dtype=float32)\n\n\n\n\n타입 변환\n\ntry:\n    tf.constant(2.0) + tf.constant(40)\nexcept tf.errors.InvalidArgumentError as ex:\n    print(ex)\n\ncannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2]\n\n\n\ntry:\n    tf.constant(2.0) + tf.constant(40., dtype=tf.float64)\nexcept tf.errors.InvalidArgumentError as ex:\n    print(ex)\n\ncannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a double tensor [Op:AddV2]\n\n\n\nt2 = tf.constant(40., dtype=tf.float64)\ntf.constant(2.0) + tf.cast(t2, tf.float32)\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=42.0&gt;\n\n\n\n\n문자열\n\ntf.constant(b\"hello world\")\n\n&lt;tf.Tensor: shape=(), dtype=string, numpy=b'hello world'&gt;\n\n\n\ntf.constant(\"café\")\n\n&lt;tf.Tensor: shape=(), dtype=string, numpy=b'caf\\xc3\\xa9'&gt;\n\n\n\nu = tf.constant([ord(c) for c in \"café\"])\nu\n\n&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([ 99,  97, 102, 233], dtype=int32)&gt;\n\n\n\nb = tf.strings.unicode_encode(u, \"UTF-8\")\ntf.strings.length(b, unit=\"UTF8_CHAR\")\n\n&lt;tf.Tensor: shape=(), dtype=int32, numpy=4&gt;\n\n\n\ntf.strings.unicode_decode(b, \"UTF-8\")\n\n&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([ 99,  97, 102, 233], dtype=int32)&gt;\n\n\n\n\n문자열 배열\n\np = tf.constant([\"Café\", \"Coffee\", \"caffè\", \"咖啡\"])\n\n\ntf.strings.length(p, unit=\"UTF8_CHAR\")\n\n&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([4, 6, 5, 2], dtype=int32)&gt;\n\n\n\nr = tf.strings.unicode_decode(p, \"UTF8\")\nr\n\n&lt;tf.RaggedTensor [[67, 97, 102, 233], [67, 111, 102, 102, 101, 101], [99, 97, 102, 102, 232], [21654, 21857]]&gt;\n\n\n\nprint(r)\n\n&lt;tf.RaggedTensor [[67, 97, 102, 233], [67, 111, 102, 102, 101, 101], [99, 97, 102, 102, 232], [21654, 21857]]&gt;\n\n\n\n\n래그드 텐서\n\nprint(r[1])\n\ntf.Tensor([ 67 111 102 102 101 101], shape=(6,), dtype=int32)\n\n\n\nprint(r[1:3])\n\n&lt;tf.RaggedTensor [[67, 111, 102, 102, 101, 101], [99, 97, 102, 102, 232]]&gt;\n\n\n\nr2 = tf.ragged.constant([[65, 66], [], [67]])\nprint(tf.concat([r, r2], axis=0))\n\n&lt;tf.RaggedTensor [[67, 97, 102, 233], [67, 111, 102, 102, 101, 101], [99, 97, 102, 102, 232], [21654, 21857], [65, 66], [], [67]]&gt;\n\n\n\nr3 = tf.ragged.constant([[68, 69, 70], [71], [], [72, 73]])\nprint(tf.concat([r, r3], axis=1))\n\n&lt;tf.RaggedTensor [[67, 97, 102, 233, 68, 69, 70], [67, 111, 102, 102, 101, 101, 71], [99, 97, 102, 102, 232], [21654, 21857, 72, 73]]&gt;\n\n\n\ntf.strings.unicode_encode(r3, \"UTF-8\")\n\n&lt;tf.Tensor: shape=(4,), dtype=string, numpy=array([b'DEF', b'G', b'', b'HI'], dtype=object)&gt;\n\n\n\nr.to_tensor()\n\n&lt;tf.Tensor: shape=(4, 6), dtype=int32, numpy=\narray([[   67,    97,   102,   233,     0,     0],\n       [   67,   111,   102,   102,   101,   101],\n       [   99,    97,   102,   102,   232,     0],\n       [21654, 21857,     0,     0,     0,     0]], dtype=int32)&gt;\n\n\n\n\n희소 텐서\n\ns = tf.SparseTensor(indices=[[0, 1], [1, 0], [2, 3]],\n                    values=[1., 2., 3.],\n                    dense_shape=[3, 4])\n\n\nprint(s)\n\nSparseTensor(indices=tf.Tensor(\n[[0 1]\n [1 0]\n [2 3]], shape=(3, 2), dtype=int64), values=tf.Tensor([1. 2. 3.], shape=(3,), dtype=float32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64))\n\n\n\ntf.sparse.to_dense(s)\n\n&lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy=\narray([[0., 1., 0., 0.],\n       [2., 0., 0., 0.],\n       [0., 0., 0., 3.]], dtype=float32)&gt;\n\n\n\ns2 = s * 2.0\n\n\ntry:\n    s3 = s + 1.\nexcept TypeError as ex:\n    print(ex)\n\nunsupported operand type(s) for +: 'SparseTensor' and 'float'\n\n\n\ns4 = tf.constant([[10., 20.], [30., 40.], [50., 60.], [70., 80.]])\ntf.sparse.sparse_dense_matmul(s, s4)\n\n&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=\narray([[ 30.,  40.],\n       [ 20.,  40.],\n       [210., 240.]], dtype=float32)&gt;\n\n\n\ns5 = tf.SparseTensor(indices=[[0, 2], [0, 1]],\n                     values=[1., 2.],\n                     dense_shape=[3, 4])\nprint(s5)\n\nSparseTensor(indices=tf.Tensor(\n[[0 2]\n [0 1]], shape=(2, 2), dtype=int64), values=tf.Tensor([1. 2.], shape=(2,), dtype=float32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64))\n\n\n\ntry:\n    tf.sparse.to_dense(s5)\nexcept tf.errors.InvalidArgumentError as ex:\n    print(ex)\n\nindices[1] is out of order. Many sparse ops require sorted indices.\n  Use `tf.sparse.reorder` to create a correctly ordered copy.\n\n [Op:SparseToDense]\n\n\n\ns6 = tf.sparse.reorder(s5)\ntf.sparse.to_dense(s6)\n\n&lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy=\narray([[0., 2., 1., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]], dtype=float32)&gt;\n\n\n\n\n집합\n\nset1 = tf.constant([[2, 3, 5, 7], [7, 9, 0, 0]])\nset2 = tf.constant([[4, 5, 6], [9, 10, 0]])\ntf.sparse.to_dense(tf.sets.union(set1, set2))\n\n&lt;tf.Tensor: shape=(2, 6), dtype=int32, numpy=\narray([[ 2,  3,  4,  5,  6,  7],\n       [ 0,  7,  9, 10,  0,  0]], dtype=int32)&gt;\n\n\n\ntf.sparse.to_dense(tf.sets.difference(set1, set2))\n\n&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=\narray([[2, 3, 7],\n       [7, 0, 0]], dtype=int32)&gt;\n\n\n\ntf.sparse.to_dense(tf.sets.intersection(set1, set2))\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[5, 0],\n       [0, 9]], dtype=int32)&gt;\n\n\n\n\n변수\n\nv = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n\n\nv.assign(2 * v)\n\n&lt;tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\narray([[ 2.,  4.,  6.],\n       [ 8., 10., 12.]], dtype=float32)&gt;\n\n\n\nv[0, 1].assign(42)\n\n&lt;tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\narray([[ 2., 42.,  6.],\n       [ 8., 10., 12.]], dtype=float32)&gt;\n\n\n\nv[:, 2].assign([0., 1.])\n\n&lt;tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\narray([[ 2., 42.,  0.],\n       [ 8., 10.,  1.]], dtype=float32)&gt;\n\n\n\ntry:\n    v[1] = [7., 8., 9.]\nexcept TypeError as ex:\n    print(ex)\n\n'ResourceVariable' object does not support item assignment\n\n\n\nv.scatter_nd_update(indices=[[0, 0], [1, 2]],\n                    updates=[100., 200.])\n\n&lt;tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\narray([[100.,  42.,   0.],\n       [  8.,  10., 200.]], dtype=float32)&gt;\n\n\n\nsparse_delta = tf.IndexedSlices(values=[[1., 2., 3.], [4., 5., 6.]],\n                                indices=[1, 0])\nv.scatter_update(sparse_delta)\n\n&lt;tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\narray([[4., 5., 6.],\n       [1., 2., 3.]], dtype=float32)&gt;\n\n\n\n\n텐서 배열\n\narray = tf.TensorArray(dtype=tf.float32, size=3)\narray = array.write(0, tf.constant([1., 2.]))\narray = array.write(1, tf.constant([3., 10.]))\narray = array.write(2, tf.constant([5., 7.]))\n\n\narray.read(1)\n\n&lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([ 3., 10.], dtype=float32)&gt;\n\n\n\narray.stack()\n\n&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=\narray([[1., 2.],\n       [0., 0.],\n       [5., 7.]], dtype=float32)&gt;\n\n\n\nmean, variance = tf.nn.moments(array.stack(), axes=0)\nmean\n\n&lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 3.], dtype=float32)&gt;\n\n\n\nvariance\n\n&lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([4.6666665, 8.666667 ], dtype=float32)&gt;"
  },
  {
    "objectID": "Machine_Learning/12_custom_models_and_training_with_tensorflow.html#사용자-정의-손실-함수",
    "href": "Machine_Learning/12_custom_models_and_training_with_tensorflow.html#사용자-정의-손실-함수",
    "title": "12_custom_models_and_training_with_tensorflow",
    "section": "사용자 정의 손실 함수",
    "text": "사용자 정의 손실 함수\n캘리포니아 주택 데이터셋을 로드하여 준비해 보겠습니다. 먼저 이 데이터셋을 로드한 다음 훈련 세트, 검증 세트, 테스트 세트로 나눕니다. 마지막으로 스케일을 변경합니다:\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nhousing = fetch_california_housing()\nX_train_full, X_test, y_train_full, y_test = train_test_split(\n    housing.data, housing.target.reshape(-1, 1), random_state=42)\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X_train_full, y_train_full, random_state=42)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_valid_scaled = scaler.transform(X_valid)\nX_test_scaled = scaler.transform(X_test)\n\nDownloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n\n\n\ndef huber_fn(y_true, y_pred):\n    error = y_true - y_pred\n    is_small_error = tf.abs(error) &lt; 1\n    squared_loss = tf.square(error) / 2\n    linear_loss  = tf.abs(error) - 0.5\n    return tf.where(is_small_error, squared_loss, linear_loss)\n\n\nplt.figure(figsize=(8, 3.5))\nz = np.linspace(-4, 4, 200)\nplt.plot(z, huber_fn(0, z), \"b-\", linewidth=2, label=\"huber($z$)\")\nplt.plot(z, z**2 / 2, \"b:\", linewidth=1, label=r\"$\\frac{1}{2}z^2$\")\nplt.plot([-1, -1], [0, huber_fn(0., -1.)], \"r--\")\nplt.plot([1, 1], [0, huber_fn(0., 1.)], \"r--\")\nplt.gca().axhline(y=0, color='k')\nplt.gca().axvline(x=0, color='k')\nplt.axis([-4, 4, 0, 4])\nplt.grid(True)\nplt.xlabel(\"$z$\")\nplt.legend(fontsize=14)\nplt.title(\"Huber loss\", fontsize=14)\nplt.show()\n\n\n\n\n\ninput_shape = X_train.shape[1:]\n\nmodel = keras.models.Sequential([\n    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                       input_shape=input_shape),\n    keras.layers.Dense(1),\n])\n\n\nmodel.compile(loss=huber_fn, optimizer=\"nadam\", metrics=[\"mae\"])\n\n\nmodel.fit(X_train_scaled, y_train, epochs=2,\n          validation_data=(X_valid_scaled, y_valid))\n\nEpoch 1/2\n363/363 [==============================] - 3s 4ms/step - loss: 0.6235 - mae: 0.9953 - val_loss: 0.2862 - val_mae: 0.5866\nEpoch 2/2\n363/363 [==============================] - 1s 4ms/step - loss: 0.2197 - mae: 0.5177 - val_loss: 0.2382 - val_mae: 0.5281\n\n\n&lt;keras.callbacks.History at 0x7fd47b0dab50&gt;"
  },
  {
    "objectID": "Machine_Learning/12_custom_models_and_training_with_tensorflow.html#사용자-정의-요소를-가진-모델을-저장하고-로드하기",
    "href": "Machine_Learning/12_custom_models_and_training_with_tensorflow.html#사용자-정의-요소를-가진-모델을-저장하고-로드하기",
    "title": "12_custom_models_and_training_with_tensorflow",
    "section": "사용자 정의 요소를 가진 모델을 저장하고 로드하기",
    "text": "사용자 정의 요소를 가진 모델을 저장하고 로드하기\n\nmodel.save(\"my_model_with_a_custom_loss.h5\")\n\n\nmodel = keras.models.load_model(\"my_model_with_a_custom_loss.h5\",\n                                custom_objects={\"huber_fn\": huber_fn})\n\n\nmodel.fit(X_train_scaled, y_train, epochs=2,\n          validation_data=(X_valid_scaled, y_valid))\n\nEpoch 1/2\n363/363 [==============================] - 2s 5ms/step - loss: 0.2054 - mae: 0.4982 - val_loss: 0.2209 - val_mae: 0.5050\nEpoch 2/2\n363/363 [==============================] - 1s 4ms/step - loss: 0.1999 - mae: 0.4900 - val_loss: 0.2127 - val_mae: 0.4986\n\n\n&lt;keras.callbacks.History at 0x7fd476f36ed0&gt;\n\n\n\ndef create_huber(threshold=1.0):\n    def huber_fn(y_true, y_pred):\n        error = y_true - y_pred\n        is_small_error = tf.abs(error) &lt; threshold\n        squared_loss = tf.square(error) / 2\n        linear_loss  = threshold * tf.abs(error) - threshold**2 / 2\n        return tf.where(is_small_error, squared_loss, linear_loss)\n    return huber_fn\n\n\nmodel.compile(loss=create_huber(2.0), optimizer=\"nadam\", metrics=[\"mae\"])\n\n\nmodel.fit(X_train_scaled, y_train, epochs=2,\n          validation_data=(X_valid_scaled, y_valid))\n\nEpoch 1/2\n363/363 [==============================] - 2s 5ms/step - loss: 0.2226 - mae: 0.4892 - val_loss: 0.2540 - val_mae: 0.4907\nEpoch 2/2\n363/363 [==============================] - 1s 4ms/step - loss: 0.2184 - mae: 0.4844 - val_loss: 0.2372 - val_mae: 0.4879\n\n\n&lt;keras.callbacks.History at 0x7fd47afc66d0&gt;\n\n\n\nmodel.save(\"my_model_with_a_custom_loss_threshold_2.h5\")\n\n\nmodel = keras.models.load_model(\"my_model_with_a_custom_loss_threshold_2.h5\",\n                                custom_objects={\"huber_fn\": create_huber(2.0)})\n\n\nmodel.fit(X_train_scaled, y_train, epochs=2,\n          validation_data=(X_valid_scaled, y_valid))\n\nEpoch 1/2\n363/363 [==============================] - 2s 5ms/step - loss: 0.2147 - mae: 0.4800 - val_loss: 0.2133 - val_mae: 0.4654\nEpoch 2/2\n363/363 [==============================] - 2s 4ms/step - loss: 0.2119 - mae: 0.4762 - val_loss: 0.1992 - val_mae: 0.4643\n\n\n&lt;keras.callbacks.History at 0x7fd47adb2b10&gt;\n\n\n\nclass HuberLoss(keras.losses.Loss):\n    def __init__(self, threshold=1.0, **kwargs):\n        self.threshold = threshold\n        super().__init__(**kwargs)\n    def call(self, y_true, y_pred):\n        error = y_true - y_pred\n        is_small_error = tf.abs(error) &lt; self.threshold\n        squared_loss = tf.square(error) / 2\n        linear_loss  = self.threshold * tf.abs(error) - self.threshold**2 / 2\n        return tf.where(is_small_error, squared_loss, linear_loss)\n    def get_config(self):\n        base_config = super().get_config()\n        return {**base_config, \"threshold\": self.threshold}\n\n\nmodel = keras.models.Sequential([\n    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                       input_shape=input_shape),\n    keras.layers.Dense(1),\n])\n\n\nmodel.compile(loss=HuberLoss(2.), optimizer=\"nadam\", metrics=[\"mae\"])\n\n\nmodel.fit(X_train_scaled, y_train, epochs=2,\n          validation_data=(X_valid_scaled, y_valid))\n\nEpoch 1/2\n363/363 [==============================] - 2s 5ms/step - loss: 0.7095 - mae: 0.8863 - val_loss: 0.3378 - val_mae: 0.5485\nEpoch 2/2\n363/363 [==============================] - 2s 4ms/step - loss: 0.2416 - mae: 0.5083 - val_loss: 0.2660 - val_mae: 0.5089\n\n\n&lt;keras.callbacks.History at 0x7fd47ac65a10&gt;\n\n\n\nmodel.save(\"my_model_with_a_custom_loss_class.h5\")\n\n\nmodel = keras.models.load_model(\"my_model_with_a_custom_loss_class.h5\",\n                                custom_objects={\"HuberLoss\": HuberLoss})\n\n\nmodel.fit(X_train_scaled, y_train, epochs=2,\n          validation_data=(X_valid_scaled, y_valid))\n\nEpoch 1/2\n363/363 [==============================] - 2s 4ms/step - loss: 0.2286 - mae: 0.4970 - val_loss: 0.2120 - val_mae: 0.4723\nEpoch 2/2\n363/363 [==============================] - 1s 4ms/step - loss: 0.2216 - mae: 0.4904 - val_loss: 0.2045 - val_mae: 0.4725\n\n\n&lt;keras.callbacks.History at 0x7fd47aa55150&gt;\n\n\n\nmodel.loss.threshold\n\n2.0"
  },
  {
    "objectID": "Machine_Learning/12_custom_models_and_training_with_tensorflow.html#그외-사용자-정의-함수",
    "href": "Machine_Learning/12_custom_models_and_training_with_tensorflow.html#그외-사용자-정의-함수",
    "title": "12_custom_models_and_training_with_tensorflow",
    "section": "그외 사용자 정의 함수",
    "text": "그외 사용자 정의 함수\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n\ndef my_softplus(z): # tf.nn.softplus(z) 값을 반환합니다\n    return tf.math.log(tf.exp(z) + 1.0)\n\ndef my_glorot_initializer(shape, dtype=tf.float32):\n    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n\ndef my_l1_regularizer(weights):\n    return tf.reduce_sum(tf.abs(0.01 * weights))\n\ndef my_positive_weights(weights): # tf.nn.relu(weights) 값을 반환합니다\n    return tf.where(weights &lt; 0., tf.zeros_like(weights), weights)\n\n\nlayer = keras.layers.Dense(1, activation=my_softplus,\n                           kernel_initializer=my_glorot_initializer,\n                           kernel_regularizer=my_l1_regularizer,\n                           kernel_constraint=my_positive_weights)\n\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n\nmodel = keras.models.Sequential([\n    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                       input_shape=input_shape),\n    keras.layers.Dense(1, activation=my_softplus,\n                       kernel_regularizer=my_l1_regularizer,\n                       kernel_constraint=my_positive_weights,\n                       kernel_initializer=my_glorot_initializer),\n])\n\n\nmodel.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[\"mae\"])\n\n\nmodel.fit(X_train_scaled, y_train, epochs=2,\n          validation_data=(X_valid_scaled, y_valid))\n\nEpoch 1/2\n363/363 [==============================] - 2s 5ms/step - loss: 1.5542 - mae: 0.8962 - val_loss: 1.4154 - val_mae: 0.5607\nEpoch 2/2\n363/363 [==============================] - 2s 4ms/step - loss: 0.5943 - mae: 0.5256 - val_loss: 1.4399 - val_mae: 0.5137\n\n\n&lt;keras.callbacks.History at 0x7fd47a9865d0&gt;\n\n\n\nmodel.save(\"my_model_with_many_custom_parts.h5\")\n\n\nmodel = keras.models.load_model(\n    \"my_model_with_many_custom_parts.h5\",\n    custom_objects={\n       \"my_l1_regularizer\": my_l1_regularizer,\n       \"my_positive_weights\": my_positive_weights,\n       \"my_glorot_initializer\": my_glorot_initializer,\n       \"my_softplus\": my_softplus,\n    })\n\n\nclass MyL1Regularizer(keras.regularizers.Regularizer):\n    def __init__(self, factor):\n        self.factor = factor\n    def __call__(self, weights):\n        return tf.reduce_sum(tf.abs(self.factor * weights))\n    def get_config(self):\n        return {\"factor\": self.factor}\n\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n\nmodel = keras.models.Sequential([\n    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                       input_shape=input_shape),\n    keras.layers.Dense(1, activation=my_softplus,\n                       kernel_regularizer=MyL1Regularizer(0.01),\n                       kernel_constraint=my_positive_weights,\n                       kernel_initializer=my_glorot_initializer),\n])\n\n\nmodel.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[\"mae\"])\n\n\nmodel.fit(X_train_scaled, y_train, epochs=2,\n          validation_data=(X_valid_scaled, y_valid))\n\nEpoch 1/2\n363/363 [==============================] - 3s 5ms/step - loss: 1.5542 - mae: 0.8962 - val_loss: 1.4154 - val_mae: 0.5607\nEpoch 2/2\n363/363 [==============================] - 2s 5ms/step - loss: 0.5943 - mae: 0.5256 - val_loss: 1.4399 - val_mae: 0.5137\n\n\n&lt;keras.callbacks.History at 0x7fd479f76250&gt;\n\n\n\nmodel.save(\"my_model_with_many_custom_parts.h5\")\n\n\nmodel = keras.models.load_model(\n    \"my_model_with_many_custom_parts.h5\",\n    custom_objects={\n       \"MyL1Regularizer\": MyL1Regularizer,\n       \"my_positive_weights\": my_positive_weights,\n       \"my_glorot_initializer\": my_glorot_initializer,\n       \"my_softplus\": my_softplus,\n    })"
  },
  {
    "objectID": "Machine_Learning/12_custom_models_and_training_with_tensorflow.html#사용자-정의-지표",
    "href": "Machine_Learning/12_custom_models_and_training_with_tensorflow.html#사용자-정의-지표",
    "title": "12_custom_models_and_training_with_tensorflow",
    "section": "사용자 정의 지표",
    "text": "사용자 정의 지표\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n\nmodel = keras.models.Sequential([\n    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                       input_shape=input_shape),\n    keras.layers.Dense(1),\n])\n\n\nmodel.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[create_huber(2.0)])\n\n\nmodel.fit(X_train_scaled, y_train, epochs=2)\n\nEpoch 1/2\n363/363 [==============================] - 2s 4ms/step - loss: 2.0982 - huber_fn: 0.9192\nEpoch 2/2\n363/363 [==============================] - 1s 4ms/step - loss: 0.6052 - huber_fn: 0.2733\n\n\n&lt;keras.callbacks.History at 0x7fd476f3bbd0&gt;\n\n\n노트: 손실과 지표에 같은 함수를 사용하면 다른 결과가 나올 수 있습니다. 이는 일반적으로 부동 소수점 정밀도 오차 때문입니다. 수학 식이 동일하더라도 연산은 동일한 순서대로 실행되지 않습니다. 이로 인해 작은 차이가 발생합니다. 또한 샘플 가중치를 사용하면 정밀도보다 더 큰 오차가 생깁니다:\n\n에포크에서 손실은 지금까지 본 모든 배치 손실의 평균입니다. 각 배치 손실은 가중치가 적용된 샘플 손실의 합을 배치 크기 로 나눈 것입니다(샘플 가중치의 합으로 나눈 것이 아닙니다. 따라서 배치 손실은 손실의 가중 평균이 아닙니다).\n에포크에서 지표는 가중치가 적용된 샘플 손실의 합을 지금까지 본 모든 샘플 가중치의 합으로 나눈 것입니다. 다른 말로하면 모든 샘플 손실의 가중 평균입니다. 따라서 위와 같지 않습니다.\n\n수학적으로 말하면 손실 = 지표 * 샘플 가중치의 평균(더하기 약간의 부동 소수점 정밀도 오차)입니다.\n\nmodel.compile(loss=create_huber(2.0), optimizer=\"nadam\", metrics=[create_huber(2.0)])\n\n\nsample_weight = np.random.rand(len(y_train))\nhistory = model.fit(X_train_scaled, y_train, epochs=2, sample_weight=sample_weight)\n\nEpoch 1/2\n363/363 [==============================] - 2s 3ms/step - loss: 0.1175 - huber_fn: 0.2399\nEpoch 2/2\n363/363 [==============================] - 1s 3ms/step - loss: 0.1131 - huber_fn: 0.2297\n\n\n\nhistory.history[\"loss\"][0], history.history[\"huber_fn\"][0] * sample_weight.mean()\n\n(0.11749907582998276, 0.11906625573138947)\n\n\n\n스트리밍 지표\n\nprecision = keras.metrics.Precision()\nprecision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=0.8&gt;\n\n\n\nprecision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=0.5&gt;\n\n\n\nprecision.result()\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=0.5&gt;\n\n\n\nprecision.variables\n\n[&lt;tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)&gt;,\n &lt;tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)&gt;]\n\n\n\nprecision.reset_states()\n\n스트리밍 지표 만들기:\n\nclass HuberMetric(keras.metrics.Metric):\n    def __init__(self, threshold=1.0, **kwargs):\n        super().__init__(**kwargs) # 기본 매개변수 처리 (예를 들면, dtype)\n        self.threshold = threshold\n        self.huber_fn = create_huber(threshold)\n        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        metric = self.huber_fn(y_true, y_pred)\n        self.total.assign_add(tf.reduce_sum(metric))\n        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n    def result(self):\n        return self.total / self.count\n    def get_config(self):\n        base_config = super().get_config()\n        return {**base_config, \"threshold\": self.threshold}\n\n\nm = HuberMetric(2.)\n\n# total = 2 * |10 - 2| - 2²/2 = 14\n# count = 1\n# result = 14 / 1 = 14\nm(tf.constant([[2.]]), tf.constant([[10.]])) \n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=14.0&gt;\n\n\n\n# total = total + (|1 - 0|² / 2) + (2 * |9.25 - 5| - 2² / 2) = 14 + 7 = 21\n# count = count + 2 = 3\n# result = total / count = 21 / 3 = 7\nm(tf.constant([[0.], [5.]]), tf.constant([[1.], [9.25]]))\n\nm.result()\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=7.0&gt;\n\n\n\nm.variables\n\n[&lt;tf.Variable 'total:0' shape=() dtype=float32, numpy=21.0&gt;,\n &lt;tf.Variable 'count:0' shape=() dtype=float32, numpy=3.0&gt;]\n\n\n\nm.reset_states()\nm.variables\n\n[&lt;tf.Variable 'total:0' shape=() dtype=float32, numpy=0.0&gt;,\n &lt;tf.Variable 'count:0' shape=() dtype=float32, numpy=0.0&gt;]\n\n\nHuberMetric 클래스가 잘 동작하는지 확인해 보죠:\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n\nmodel = keras.models.Sequential([\n    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                       input_shape=input_shape),\n    keras.layers.Dense(1),\n])\n\n\nmodel.compile(loss=create_huber(2.0), optimizer=\"nadam\", metrics=[HuberMetric(2.0)])\n\n\nmodel.fit(X_train_scaled.astype(np.float32), y_train.astype(np.float32), epochs=2)\n\nEpoch 1/2\n363/363 [==============================] - 2s 3ms/step - loss: 0.8707 - huber_metric: 0.8707\nEpoch 2/2\n363/363 [==============================] - 1s 3ms/step - loss: 0.2595 - huber_metric: 0.2595\n\n\n&lt;keras.callbacks.History at 0x7fd47a4e9fd0&gt;\n\n\n\nmodel.save(\"my_model_with_a_custom_metric.h5\")\n\n\nmodel = keras.models.load_model(\"my_model_with_a_custom_metric.h5\",\n                                custom_objects={\"huber_fn\": create_huber(2.0),\n                                                \"HuberMetric\": HuberMetric})\n\n\nmodel.fit(X_train_scaled.astype(np.float32), y_train.astype(np.float32), epochs=2)\n\nEpoch 1/2\n363/363 [==============================] - 2s 3ms/step - loss: 0.2350 - huber_metric: 0.2350\nEpoch 2/2\n363/363 [==============================] - 1s 3ms/step - loss: 0.2278 - huber_metric: 0.2278\n\n\n&lt;keras.callbacks.History at 0x7fd47a326690&gt;\n\n\n경고: 텐서플로 2.2에서 tf.keras가 model.metrics의 0번째 위치에 지표를 추가합니다(텐서플로 이슈 #38150 참조). 따라서 HuberMetric에 접근하려면 model.metrics[0] 대신 model.metrics[-1]를 사용해야 합니다.\n\nmodel.metrics[-1].threshold\n\n2.0\n\n\n잘 동작하는군요! 다음처럼 더 간단하게 클래스를 만들 수 있습니다:\n\nclass HuberMetric(keras.metrics.Mean):\n    def __init__(self, threshold=1.0, name='HuberMetric', dtype=None):\n        self.threshold = threshold\n        self.huber_fn = create_huber(threshold)\n        super().__init__(name=name, dtype=dtype)\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        metric = self.huber_fn(y_true, y_pred)\n        super(HuberMetric, self).update_state(metric, sample_weight)\n    def get_config(self):\n        base_config = super().get_config()\n        return {**base_config, \"threshold\": self.threshold}        \n\n이 클래스는 크기를 잘 처리하고 샘플 가중치도 지원합니다.\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n\nmodel = keras.models.Sequential([\n    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                       input_shape=input_shape),\n    keras.layers.Dense(1),\n])\n\n\nmodel.compile(loss=keras.losses.Huber(2.0), optimizer=\"nadam\", weighted_metrics=[HuberMetric(2.0)])\n\n\nsample_weight = np.random.rand(len(y_train))\nhistory = model.fit(X_train_scaled.astype(np.float32), y_train.astype(np.float32),\n                    epochs=2, sample_weight=sample_weight)\n\nEpoch 1/2\n363/363 [==============================] - 2s 4ms/step - loss: 0.4455 - HuberMetric: 0.8978\nEpoch 2/2\n363/363 [==============================] - 1s 4ms/step - loss: 0.1305 - HuberMetric: 0.2631\n\n\n\nhistory.history[\"loss\"][0], history.history[\"HuberMetric\"][0] * sample_weight.mean()\n\n(0.44554364681243896, 0.44554368685750223)\n\n\n\nmodel.save(\"my_model_with_a_custom_metric_v2.h5\")\n\n\nmodel = keras.models.load_model(\"my_model_with_a_custom_metric_v2.h5\",\n                                custom_objects={\"HuberMetric\": HuberMetric})\n\n\nmodel.fit(X_train_scaled.astype(np.float32), y_train.astype(np.float32), epochs=2)\n\nEpoch 1/2\n363/363 [==============================] - 2s 4ms/step - loss: 0.2377 - HuberMetric: 0.2377\nEpoch 2/2\n363/363 [==============================] - 1s 4ms/step - loss: 0.2279 - HuberMetric: 0.2279\n\n\n&lt;keras.callbacks.History at 0x7fd479ff9c90&gt;\n\n\n\nmodel.metrics[-1].threshold\n\n2.0"
  },
  {
    "objectID": "Machine_Learning/12_custom_models_and_training_with_tensorflow.html#사용자-정의-층",
    "href": "Machine_Learning/12_custom_models_and_training_with_tensorflow.html#사용자-정의-층",
    "title": "12_custom_models_and_training_with_tensorflow",
    "section": "사용자 정의 층",
    "text": "사용자 정의 층\n\nexponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))\n\n\nexponential_layer([-1., 0., 1.])\n\n&lt;tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.36787948, 1.        , 2.7182817 ], dtype=float32)&gt;\n\n\n회귀 모델이 예측할 값이 양수이고 스케일이 매우 다른 경우 (예를 들어, 0.001, 10., 10000) 출력층에 지수 함수를 추가하면 유용할 수 있습니다:\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n\nmodel = keras.models.Sequential([\n    keras.layers.Dense(30, activation=\"relu\", input_shape=input_shape),\n    keras.layers.Dense(1),\n    exponential_layer\n])\nmodel.compile(loss=\"mse\", optimizer=\"sgd\")\nmodel.fit(X_train_scaled, y_train, epochs=5,\n          validation_data=(X_valid_scaled, y_valid))\nmodel.evaluate(X_test_scaled, y_test)\n\nEpoch 1/5\n363/363 [==============================] - 2s 3ms/step - loss: 1.0631 - val_loss: 0.4457\nEpoch 2/5\n363/363 [==============================] - 1s 3ms/step - loss: 0.4562 - val_loss: 0.3798\nEpoch 3/5\n363/363 [==============================] - 1s 3ms/step - loss: 0.4029 - val_loss: 0.3548\nEpoch 4/5\n363/363 [==============================] - 1s 3ms/step - loss: 0.3851 - val_loss: 0.3464\nEpoch 5/5\n363/363 [==============================] - 1s 3ms/step - loss: 0.3708 - val_loss: 0.3449\n162/162 [==============================] - 0s 2ms/step - loss: 0.3586\n\n\n0.3586340546607971\n\n\n\nclass MyDense(keras.layers.Layer):\n    def __init__(self, units, activation=None, **kwargs):\n        super().__init__(**kwargs)\n        self.units = units\n        self.activation = keras.activations.get(activation)\n\n    def build(self, batch_input_shape):\n        self.kernel = self.add_weight(\n            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n            initializer=\"glorot_normal\")\n        self.bias = self.add_weight(\n            name=\"bias\", shape=[self.units], initializer=\"zeros\")\n        super().build(batch_input_shape) # must be at the end\n\n    def call(self, X):\n        return self.activation(X @ self.kernel + self.bias)\n\n    def compute_output_shape(self, batch_input_shape):\n        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n\n    def get_config(self):\n        base_config = super().get_config()\n        return {**base_config, \"units\": self.units,\n                \"activation\": keras.activations.serialize(self.activation)}\n\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n\nmodel = keras.models.Sequential([\n    MyDense(30, activation=\"relu\", input_shape=input_shape),\n    MyDense(1)\n])\n\n\nmodel.compile(loss=\"mse\", optimizer=\"nadam\")\nmodel.fit(X_train_scaled, y_train, epochs=2,\n          validation_data=(X_valid_scaled, y_valid))\nmodel.evaluate(X_test_scaled, y_test)\n\nEpoch 1/2\n363/363 [==============================] - 2s 4ms/step - loss: 2.2563 - val_loss: 0.9472\nEpoch 2/2\n363/363 [==============================] - 1s 4ms/step - loss: 0.6485 - val_loss: 0.6219\n162/162 [==============================] - 0s 2ms/step - loss: 0.5474\n\n\n0.5473726987838745\n\n\n\nmodel.save(\"my_model_with_a_custom_layer.h5\")\n\n\nmodel = keras.models.load_model(\"my_model_with_a_custom_layer.h5\",\n                                custom_objects={\"MyDense\": MyDense})\n\n\nclass MyMultiLayer(keras.layers.Layer):\n    def call(self, X):\n        X1, X2 = X\n        print(\"X1.shape: \", X1.shape ,\" X2.shape: \", X2.shape) # 사용자 정의 층 디버깅\n        return X1 + X2, X1 * X2\n\n    def compute_output_shape(self, batch_input_shape):\n        batch_input_shape1, batch_input_shape2 = batch_input_shape\n        return [batch_input_shape1, batch_input_shape2]\n\n사용자 정의 층은 다음처럼 함수형 API를 사용해 호출할 수 있습니다:\n\ninputs1 = keras.layers.Input(shape=[2])\ninputs2 = keras.layers.Input(shape=[2])\noutputs1, outputs2 = MyMultiLayer()((inputs1, inputs2))\n\nX1.shape:  (None, 2)  X2.shape:  (None, 2)\n\n\ncall() 메서드는 심볼릭 입력을 받습니다. 이 입력의 크기는 부분적으로만 지정되어 있습니다(이 시점에서는 배치 크기를 모릅니다. 그래서 첫 번째 차원이 None입니다):\n사용자 층에 실제 데이터를 전달할 수도 있습니다. 이를 테스트하기 위해 각 데이터셋의 입력을 각각 네 개의 특성을 가진 두 부분으로 나누겠습니다:\n\ndef split_data(data):\n    columns_count = data.shape[-1]\n    half = columns_count // 2\n    return data[:, :half], data[:, half:]\n\nX_train_scaled_A, X_train_scaled_B = split_data(X_train_scaled)\nX_valid_scaled_A, X_valid_scaled_B = split_data(X_valid_scaled)\nX_test_scaled_A, X_test_scaled_B = split_data(X_test_scaled)\n\n# 분할된 데이터 크기 출력\nX_train_scaled_A.shape, X_train_scaled_B.shape\n\n((11610, 4), (11610, 4))\n\n\n크기가 완전하게 지정된 것을 볼 수 있습니다:\n\noutputs1, outputs2 = MyMultiLayer()((X_train_scaled_A, X_train_scaled_B))\n\nX1.shape:  (11610, 4)  X2.shape:  (11610, 4)\n\n\n함수형 API를 사용해 완전한 모델을 만들어 보겠습니다(이 모델은 간단한 예제이므로 놀라운 성능을 기대하지 마세요):\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\ninput_A = keras.layers.Input(shape=X_train_scaled_A.shape[-1])\ninput_B = keras.layers.Input(shape=X_train_scaled_B.shape[-1])\nhidden_A, hidden_B = MyMultiLayer()((input_A, input_B))\nhidden_A = keras.layers.Dense(30, activation='selu')(hidden_A)\nhidden_B = keras.layers.Dense(30, activation='selu')(hidden_B)\nconcat = keras.layers.Concatenate()((hidden_A, hidden_B))\noutput = keras.layers.Dense(1)(concat)\nmodel = keras.models.Model(inputs=[input_A, input_B], outputs=[output])\n\nX1.shape:  (None, 4)  X2.shape:  (None, 4)\n\n\n\nmodel.compile(loss='mse', optimizer='nadam')\n\n\nmodel.fit((X_train_scaled_A, X_train_scaled_B), y_train, epochs=2,\n          validation_data=((X_valid_scaled_A, X_valid_scaled_B), y_valid))\n\nEpoch 1/2\nX1.shape:  (None, 4)  X2.shape:  (None, 4)\nX1.shape:  (None, 4)  X2.shape:  (None, 4)\n353/363 [============================&gt;.] - ETA: 0s - loss: 2.1461X1.shape:  (None, 4)  X2.shape:  (None, 4)\n363/363 [==============================] - 2s 4ms/step - loss: 2.1142 - val_loss: 1.3630\nEpoch 2/2\n363/363 [==============================] - 2s 4ms/step - loss: 0.9684 - val_loss: 0.9773\n\n\n&lt;keras.callbacks.History at 0x7fd479af8410&gt;\n\n\n훈련과 테스트에서 다르게 동작하는 층을 만들어 보죠:\n\nclass AddGaussianNoise(keras.layers.Layer):\n    def __init__(self, stddev, **kwargs):\n        super().__init__(**kwargs)\n        self.stddev = stddev\n\n    def call(self, X, training=None):\n        if training:\n            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n            return X + noise\n        else:\n            return X\n\n    def compute_output_shape(self, batch_input_shape):\n        return batch_input_shape\n\n다음은 사용자 정의 층을 사용하는 간단한 모델입니다:\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nmodel = keras.models.Sequential([\n    AddGaussianNoise(stddev=1.0),\n    keras.layers.Dense(30, activation=\"selu\"),\n    keras.layers.Dense(1)\n])\n\n\nmodel.compile(loss=\"mse\", optimizer=\"nadam\")\nmodel.fit(X_train_scaled, y_train, epochs=2,\n          validation_data=(X_valid_scaled, y_valid))\nmodel.evaluate(X_test_scaled, y_test)\n\nEpoch 1/2\n363/363 [==============================] - 3s 4ms/step - loss: 2.3857 - val_loss: 7.6082\nEpoch 2/2\n363/363 [==============================] - 1s 4ms/step - loss: 1.0571 - val_loss: 4.4597\n162/162 [==============================] - 0s 2ms/step - loss: 0.7560\n\n\n0.7559615969657898"
  },
  {
    "objectID": "Machine_Learning/12_custom_models_and_training_with_tensorflow.html#사용자-정의-모델",
    "href": "Machine_Learning/12_custom_models_and_training_with_tensorflow.html#사용자-정의-모델",
    "title": "12_custom_models_and_training_with_tensorflow",
    "section": "사용자 정의 모델",
    "text": "사용자 정의 모델\n\nX_new_scaled = X_test_scaled\n\n\nclass ResidualBlock(keras.layers.Layer):\n    def __init__(self, n_layers, n_neurons, **kwargs):\n        super().__init__(**kwargs)\n        self.hidden = [keras.layers.Dense(n_neurons, activation=\"elu\",\n                                          kernel_initializer=\"he_normal\")\n                       for _ in range(n_layers)]\n\n    def call(self, inputs):\n        Z = inputs\n        for layer in self.hidden:\n            Z = layer(Z)\n        return inputs + Z\n\n\nclass ResidualRegressor(keras.models.Model):\n    def __init__(self, output_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.hidden1 = keras.layers.Dense(30, activation=\"elu\",\n                                          kernel_initializer=\"he_normal\")\n        self.block1 = ResidualBlock(2, 30)\n        self.block2 = ResidualBlock(2, 30)\n        self.out = keras.layers.Dense(output_dim)\n\n    def call(self, inputs):\n        Z = self.hidden1(inputs)\n        for _ in range(1 + 3):\n            Z = self.block1(Z)\n        Z = self.block2(Z)\n        return self.out(Z)\n\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n\nmodel = ResidualRegressor(1)\nmodel.compile(loss=\"mse\", optimizer=\"nadam\")\nhistory = model.fit(X_train_scaled, y_train, epochs=5)\nscore = model.evaluate(X_test_scaled, y_test)\ny_pred = model.predict(X_new_scaled)\n\nEpoch 1/5\n363/363 [==============================] - 3s 5ms/step - loss: 9.1324\nEpoch 2/5\n363/363 [==============================] - 2s 5ms/step - loss: 1.0580\nEpoch 3/5\n363/363 [==============================] - 2s 5ms/step - loss: 0.8870\nEpoch 4/5\n363/363 [==============================] - 2s 5ms/step - loss: 0.5834\nEpoch 5/5\n363/363 [==============================] - 2s 5ms/step - loss: 0.6460\n162/162 [==============================] - 0s 2ms/step - loss: 0.6507\n\n\n\nmodel.save(\"my_custom_model.ckpt\")\n\nWARNING:absl:Found untraced functions such as dense_1_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_2_layer_call_and_return_conditional_losses, dense_2_layer_call_fn, dense_3_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: my_custom_model.ckpt/assets\n\n\nINFO:tensorflow:Assets written to: my_custom_model.ckpt/assets\n\n\n\nmodel = keras.models.load_model(\"my_custom_model.ckpt\")\n\n\nhistory = model.fit(X_train_scaled, y_train, epochs=5)\n\nEpoch 1/5\n363/363 [==============================] - 3s 5ms/step - loss: 0.8036\nEpoch 2/5\n363/363 [==============================] - 2s 5ms/step - loss: 0.5590\nEpoch 3/5\n363/363 [==============================] - 2s 5ms/step - loss: 0.4959\nEpoch 4/5\n363/363 [==============================] - 2s 5ms/step - loss: 0.4256\nEpoch 5/5\n363/363 [==============================] - 2s 5ms/step - loss: 0.4237\n\n\n대신 시퀀셜 API를 사용하는 모델을 정의할 수 있습니다:\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n\nblock1 = ResidualBlock(2, 30)\nmodel = keras.models.Sequential([\n    keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\"),\n    block1, block1, block1, block1,\n    ResidualBlock(2, 30),\n    keras.layers.Dense(1)\n])\n\n\nmodel.compile(loss=\"mse\", optimizer=\"nadam\")\nhistory = model.fit(X_train_scaled, y_train, epochs=5)\nscore = model.evaluate(X_test_scaled, y_test)\ny_pred = model.predict(X_new_scaled)\n\nEpoch 1/5\n363/363 [==============================] - 3s 5ms/step - loss: 0.8695\nEpoch 2/5\n363/363 [==============================] - 2s 5ms/step - loss: 0.4720\nEpoch 3/5\n363/363 [==============================] - 2s 5ms/step - loss: 0.5537\nEpoch 4/5\n363/363 [==============================] - 2s 5ms/step - loss: 0.3809\nEpoch 5/5\n363/363 [==============================] - 2s 5ms/step - loss: 0.4012\n162/162 [==============================] - 0s 2ms/step - loss: 0.4852"
  },
  {
    "objectID": "Machine_Learning/12_custom_models_and_training_with_tensorflow.html#모델-구성-요소에-기반한-손실과-지표",
    "href": "Machine_Learning/12_custom_models_and_training_with_tensorflow.html#모델-구성-요소에-기반한-손실과-지표",
    "title": "12_custom_models_and_training_with_tensorflow",
    "section": "모델 구성 요소에 기반한 손실과 지표",
    "text": "모델 구성 요소에 기반한 손실과 지표\n노트: 다음 코드는 책의 코드와 두 가지 다른 점이 있습니다: 1. 생성자에서 keras.metrics.Mean() 측정 지표를 만들고 call() 메서드에서 사용하여 평균 재구성 손실을 추적합니다. 훈련에서만 사용해야 하기 때문에 call() 메서드에 training 매개변수를 추가합니다. training이 True이면 reconstruction_mean를 업데이트하고 self.add_metric()를 호출합니다. 2. TF 2.2에 있는 이슈(#46858) 때문에 build() 메서드 안에서 super().build()를 호출하면 안됩니다.\n\nclass ReconstructingRegressor(keras.Model):\n    def __init__(self, output_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.hidden = [keras.layers.Dense(30, activation=\"selu\",\n                                          kernel_initializer=\"lecun_normal\")\n                       for _ in range(5)]\n        self.out = keras.layers.Dense(output_dim)\n        self.reconstruction_mean = keras.metrics.Mean(name=\"reconstruction_error\")\n\n    def build(self, batch_input_shape):\n        n_inputs = batch_input_shape[-1]\n        self.reconstruct = keras.layers.Dense(n_inputs)\n        #super().build(batch_input_shape)\n\n    def call(self, inputs, training=None):\n        Z = inputs\n        for layer in self.hidden:\n            Z = layer(Z)\n        reconstruction = self.reconstruct(Z)\n        self.recon_loss = 0.05 * tf.reduce_mean(tf.square(reconstruction - inputs))\n        \n        if training:\n           result = self.reconstruction_mean(recon_loss)\n           self.add_metric(result)\n        return self.out(Z)\n    \n    def train_step(self, data):\n        x, y = data\n\n        with tf.GradientTape() as tape:\n            y_pred = self(x)\n            loss = self.compiled_loss(y, y_pred, regularization_losses=[self.recon_loss])\n\n        gradients = tape.gradient(loss, self.trainable_variables)\n        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n\n        return {m.name: m.result() for m in self.metrics}\n\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n\nmodel = ReconstructingRegressor(1)\nmodel.compile(loss=\"mse\", optimizer=\"nadam\")\nhistory = model.fit(X_train_scaled, y_train, epochs=2)\ny_pred = model.predict(X_test_scaled)\n\nEpoch 1/2\n363/363 [==============================] - 4s 6ms/step - loss: 0.7886 - reconstruction_error: 0.0000e+00\nEpoch 2/2\n363/363 [==============================] - 2s 6ms/step - loss: 0.4134 - reconstruction_error: 0.0000e+00"
  },
  {
    "objectID": "Machine_Learning/12_custom_models_and_training_with_tensorflow.html#자동-미분을-사용하여-그레이디언트-계산하기",
    "href": "Machine_Learning/12_custom_models_and_training_with_tensorflow.html#자동-미분을-사용하여-그레이디언트-계산하기",
    "title": "12_custom_models_and_training_with_tensorflow",
    "section": "자동 미분을 사용하여 그레이디언트 계산하기",
    "text": "자동 미분을 사용하여 그레이디언트 계산하기\n\ndef f(w1, w2):\n    return 3 * w1 ** 2 + 2 * w1 * w2\n\n\nw1, w2 = 5, 3\neps = 1e-6\n(f(w1 + eps, w2) - f(w1, w2)) / eps\n\n36.000003007075065\n\n\n\n(f(w1, w2 + eps) - f(w1, w2)) / eps\n\n10.000000003174137\n\n\n\nw1, w2 = tf.Variable(5.), tf.Variable(3.)\nwith tf.GradientTape() as tape:\n    z = f(w1, w2)\n\ngradients = tape.gradient(z, [w1, w2])\n\n\ngradients\n\n[&lt;tf.Tensor: shape=(), dtype=float32, numpy=36.0&gt;,\n &lt;tf.Tensor: shape=(), dtype=float32, numpy=10.0&gt;]\n\n\n\nwith tf.GradientTape() as tape:\n    z = f(w1, w2)\n\ndz_dw1 = tape.gradient(z, w1)\ntry:\n    dz_dw2 = tape.gradient(z, w2)\nexcept RuntimeError as ex:\n    print(ex)\n\nA non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)\n\n\n\nwith tf.GradientTape(persistent=True) as tape:\n    z = f(w1, w2)\n\ndz_dw1 = tape.gradient(z, w1)\ndz_dw2 = tape.gradient(z, w2) # works now!\ndel tape\n\n\ndz_dw1, dz_dw2\n\n(&lt;tf.Tensor: shape=(), dtype=float32, numpy=36.0&gt;,\n &lt;tf.Tensor: shape=(), dtype=float32, numpy=10.0&gt;)\n\n\n\nc1, c2 = tf.constant(5.), tf.constant(3.)\nwith tf.GradientTape() as tape:\n    z = f(c1, c2)\n\ngradients = tape.gradient(z, [c1, c2])\n\n\ngradients\n\n[None, None]\n\n\n\nwith tf.GradientTape() as tape:\n    tape.watch(c1)\n    tape.watch(c2)\n    z = f(c1, c2)\n\ngradients = tape.gradient(z, [c1, c2])\n\n\ngradients\n\n[&lt;tf.Tensor: shape=(), dtype=float32, numpy=36.0&gt;,\n &lt;tf.Tensor: shape=(), dtype=float32, numpy=10.0&gt;]\n\n\n\nwith tf.GradientTape() as tape:\n    z1 = f(w1, w2 + 2.)\n    z2 = f(w1, w2 + 5.)\n    z3 = f(w1, w2 + 7.)\n\ntape.gradient([z1, z2, z3], [w1, w2])\n\n[&lt;tf.Tensor: shape=(), dtype=float32, numpy=136.0&gt;,\n &lt;tf.Tensor: shape=(), dtype=float32, numpy=30.0&gt;]\n\n\n\nwith tf.GradientTape(persistent=True) as tape:\n    z1 = f(w1, w2 + 2.)\n    z2 = f(w1, w2 + 5.)\n    z3 = f(w1, w2 + 7.)\n\ntf.reduce_sum(tf.stack([tape.gradient(z, [w1, w2]) for z in (z1, z2, z3)]), axis=0)\ndel tape\n\n\nwith tf.GradientTape(persistent=True) as hessian_tape:\n    with tf.GradientTape() as jacobian_tape:\n        z = f(w1, w2)\n    jacobians = jacobian_tape.gradient(z, [w1, w2])\nhessians = [hessian_tape.gradient(jacobian, [w1, w2])\n            for jacobian in jacobians]\ndel hessian_tape\n\n\njacobians\n\n[&lt;tf.Tensor: shape=(), dtype=float32, numpy=36.0&gt;,\n &lt;tf.Tensor: shape=(), dtype=float32, numpy=10.0&gt;]\n\n\n\nhessians\n\n[[&lt;tf.Tensor: shape=(), dtype=float32, numpy=6.0&gt;,\n  &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.0&gt;],\n [&lt;tf.Tensor: shape=(), dtype=float32, numpy=2.0&gt;, None]]\n\n\n\ndef f(w1, w2):\n    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)\n\nwith tf.GradientTape() as tape:\n    z = f(w1, w2)\n\ntape.gradient(z, [w1, w2])\n\n[&lt;tf.Tensor: shape=(), dtype=float32, numpy=30.0&gt;, None]\n\n\n\nx = tf.Variable(100.)\nwith tf.GradientTape() as tape:\n    z = my_softplus(x)\n\ntape.gradient(z, [x])\n\n[&lt;tf.Tensor: shape=(), dtype=float32, numpy=nan&gt;]\n\n\n\ntf.math.log(tf.exp(tf.constant(30., dtype=tf.float32)) + 1.)\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=30.0&gt;\n\n\n\nx = tf.Variable([100.])\nwith tf.GradientTape() as tape:\n    z = my_softplus(x)\n\ntape.gradient(z, [x])\n\n[&lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([nan], dtype=float32)&gt;]\n\n\n\n@tf.custom_gradient\ndef my_better_softplus(z):\n    exp = tf.exp(z)\n    def my_softplus_gradients(grad):\n        return grad / (1 + 1 / exp)\n    return tf.math.log(exp + 1), my_softplus_gradients\n\n\ndef my_better_softplus(z):\n    return tf.where(z &gt; 30., z, tf.math.log(tf.exp(z) + 1.))\n\n\nx = tf.Variable([1000.])\nwith tf.GradientTape() as tape:\n    z = my_better_softplus(x)\n\nz, tape.gradient(z, [x])\n\n(&lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([1000.], dtype=float32)&gt;,\n [&lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([nan], dtype=float32)&gt;])"
  },
  {
    "objectID": "Machine_Learning/12_custom_models_and_training_with_tensorflow.html#텐서플로-함수",
    "href": "Machine_Learning/12_custom_models_and_training_with_tensorflow.html#텐서플로-함수",
    "title": "12_custom_models_and_training_with_tensorflow",
    "section": "텐서플로 함수",
    "text": "텐서플로 함수\n\ndef cube(x):\n    return x ** 3\n\n\ncube(2)\n\n8\n\n\n\ncube(tf.constant(2.0))\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=8.0&gt;\n\n\n\ntf_cube = tf.function(cube)\ntf_cube\n\n&lt;tensorflow.python.eager.def_function.Function at 0x7fd479c64b10&gt;\n\n\n\ntf_cube(2)\n\n&lt;tf.Tensor: shape=(), dtype=int32, numpy=8&gt;\n\n\n\ntf_cube(tf.constant(2.0))\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=8.0&gt;\n\n\n\nTF 함수와 콘크리트 함수\n\nconcrete_function = tf_cube.get_concrete_function(tf.constant(2.0))\nconcrete_function.graph\n\n&lt;tensorflow.python.framework.func_graph.FuncGraph at 0x7fd479280850&gt;\n\n\n\nconcrete_function(tf.constant(2.0))\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=8.0&gt;\n\n\n\nconcrete_function is tf_cube.get_concrete_function(tf.constant(2.0))\n\nTrue\n\n\n\n\n함수 정의와 그래프\n\nconcrete_function.graph\n\n&lt;tensorflow.python.framework.func_graph.FuncGraph at 0x7fd479280850&gt;\n\n\n\nops = concrete_function.graph.get_operations()\nops\n\n[&lt;tf.Operation 'x' type=Placeholder&gt;,\n &lt;tf.Operation 'pow/y' type=Const&gt;,\n &lt;tf.Operation 'pow' type=Pow&gt;,\n &lt;tf.Operation 'Identity' type=Identity&gt;]\n\n\n\npow_op = ops[2]\nlist(pow_op.inputs)\n\n[&lt;tf.Tensor 'x:0' shape=() dtype=float32&gt;,\n &lt;tf.Tensor 'pow/y:0' shape=() dtype=float32&gt;]\n\n\n\npow_op.outputs\n\n[&lt;tf.Tensor 'pow:0' shape=() dtype=float32&gt;]\n\n\n\nconcrete_function.graph.get_operation_by_name('x')\n\n&lt;tf.Operation 'x' type=Placeholder&gt;\n\n\n\nconcrete_function.graph.get_tensor_by_name('Identity:0')\n\n&lt;tf.Tensor 'Identity:0' shape=() dtype=float32&gt;\n\n\n\nconcrete_function.function_def.signature\n\nname: \"__inference_cube_1065956\"\ninput_arg {\n  name: \"x\"\n  type: DT_FLOAT\n}\noutput_arg {\n  name: \"identity\"\n  type: DT_FLOAT\n}\n\n\n\n\nTF 함수가 계산 그래프를 추출하기 위해 파이썬 함수를 트레이싱하는 방법\n\n@tf.function\ndef tf_cube(x):\n    print(\"print:\", x)\n    return x ** 3\n\n\nresult = tf_cube(tf.constant(2.0))\n\nprint: Tensor(\"x:0\", shape=(), dtype=float32)\n\n\n\nresult\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=8.0&gt;\n\n\n\nresult = tf_cube(2)\nresult = tf_cube(3)\nresult = tf_cube(tf.constant([[1., 2.]])) # New shape: trace!\nresult = tf_cube(tf.constant([[3., 4.], [5., 6.]])) # New shape: trace!\nresult = tf_cube(tf.constant([[7., 8.], [9., 10.], [11., 12.]])) # New shape: trace!\n\nprint: 2\nprint: 3\nprint: Tensor(\"x:0\", shape=(1, 2), dtype=float32)\nprint: Tensor(\"x:0\", shape=(2, 2), dtype=float32)\nWARNING:tensorflow:5 out of the last 5 calls to &lt;function tf_cube at 0x7fd47a20fd40&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\nprint: Tensor(\"x:0\", shape=(3, 2), dtype=float32)\nWARNING:tensorflow:6 out of the last 6 calls to &lt;function tf_cube at 0x7fd47a20fd40&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n\n\nWARNING:tensorflow:5 out of the last 5 calls to &lt;function tf_cube at 0x7fd47a20fd40&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\nWARNING:tensorflow:6 out of the last 6 calls to &lt;function tf_cube at 0x7fd47a20fd40&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n\n\n특정 입력 시그니처를 지정하는 것도 가능합니다:\n\n@tf.function(input_signature=[tf.TensorSpec([None, 28, 28], tf.float32)])\ndef shrink(images):\n    print(\"트레이싱\", images)\n    return images[:, ::2, ::2] # 행과 열의 절반을 버립니다\n\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n\nimg_batch_1 = tf.random.uniform(shape=[100, 28, 28])\nimg_batch_2 = tf.random.uniform(shape=[50, 28, 28])\npreprocessed_images = shrink(img_batch_1) # 함수 트레이싱\npreprocessed_images = shrink(img_batch_2) # 동일한 콘크리트 함수 재사용\n\n트레이싱 Tensor(\"images:0\", shape=(None, 28, 28), dtype=float32)\n\n\n\nimg_batch_3 = tf.random.uniform(shape=[2, 2, 2])\ntry:\n    preprocessed_images = shrink(img_batch_3)  # 다른 타입이나 크기 거부\nexcept ValueError as ex:\n    print(ex)\n\nPython inputs incompatible with input_signature:\n  inputs: (\n    tf.Tensor(\n[[[0.7413678  0.62854624]\n  [0.01738465 0.3431449 ]]\n\n [[0.51063764 0.3777541 ]\n  [0.07321596 0.02137029]]], shape=(2, 2, 2), dtype=float32))\n  input_signature: (\n    TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name=None))\n\n\n\n\n오토그래프를 사용해 제어 흐름 나타내기\nrange()를 사용한 정적인 for 반복:\n\n@tf.function\ndef add_10(x):\n    for i in range(10):\n        x += 1\n    return x\n\n\nadd_10(tf.constant(5))\n\n&lt;tf.Tensor: shape=(), dtype=int32, numpy=15&gt;\n\n\n\nadd_10.get_concrete_function(tf.constant(5)).graph.get_operations()\n\n[&lt;tf.Operation 'x' type=Placeholder&gt;,\n &lt;tf.Operation 'add/y' type=Const&gt;,\n &lt;tf.Operation 'add' type=AddV2&gt;,\n &lt;tf.Operation 'add_1/y' type=Const&gt;,\n &lt;tf.Operation 'add_1' type=AddV2&gt;,\n &lt;tf.Operation 'add_2/y' type=Const&gt;,\n &lt;tf.Operation 'add_2' type=AddV2&gt;,\n &lt;tf.Operation 'add_3/y' type=Const&gt;,\n &lt;tf.Operation 'add_3' type=AddV2&gt;,\n &lt;tf.Operation 'add_4/y' type=Const&gt;,\n &lt;tf.Operation 'add_4' type=AddV2&gt;,\n &lt;tf.Operation 'add_5/y' type=Const&gt;,\n &lt;tf.Operation 'add_5' type=AddV2&gt;,\n &lt;tf.Operation 'add_6/y' type=Const&gt;,\n &lt;tf.Operation 'add_6' type=AddV2&gt;,\n &lt;tf.Operation 'add_7/y' type=Const&gt;,\n &lt;tf.Operation 'add_7' type=AddV2&gt;,\n &lt;tf.Operation 'add_8/y' type=Const&gt;,\n &lt;tf.Operation 'add_8' type=AddV2&gt;,\n &lt;tf.Operation 'add_9/y' type=Const&gt;,\n &lt;tf.Operation 'add_9' type=AddV2&gt;,\n &lt;tf.Operation 'Identity' type=Identity&gt;]\n\n\ntf.while_loop()를 사용한 동적인 반복:\n\n@tf.function\ndef add_10(x):\n    condition = lambda i, x: tf.less(i, 10)\n    body = lambda i, x: (tf.add(i, 1), tf.add(x, 1))\n    final_i, final_x = tf.while_loop(condition, body, [tf.constant(0), x])\n    return final_x\n\n\nadd_10(tf.constant(5))\n\n&lt;tf.Tensor: shape=(), dtype=int32, numpy=15&gt;\n\n\n\nadd_10.get_concrete_function(tf.constant(5)).graph.get_operations()\n\n[&lt;tf.Operation 'x' type=Placeholder&gt;,\n &lt;tf.Operation 'Const' type=Const&gt;,\n &lt;tf.Operation 'while/maximum_iterations' type=Const&gt;,\n &lt;tf.Operation 'while/loop_counter' type=Const&gt;,\n &lt;tf.Operation 'while' type=StatelessWhile&gt;,\n &lt;tf.Operation 'Identity' type=Identity&gt;]\n\n\n(오토그래프에 의한) tf.range()를 사용한 동적인 for 반복:\n\n@tf.function\ndef add_10(x):\n    for i in tf.range(10):\n        x = x + 1\n    return x\n\n\nadd_10.get_concrete_function(tf.constant(0)).graph.get_operations()\n\n[&lt;tf.Operation 'x' type=Placeholder&gt;,\n &lt;tf.Operation 'range/start' type=Const&gt;,\n &lt;tf.Operation 'range/limit' type=Const&gt;,\n &lt;tf.Operation 'range/delta' type=Const&gt;,\n &lt;tf.Operation 'range' type=Range&gt;,\n &lt;tf.Operation 'sub' type=Sub&gt;,\n &lt;tf.Operation 'floordiv' type=FloorDiv&gt;,\n &lt;tf.Operation 'mod' type=FloorMod&gt;,\n &lt;tf.Operation 'zeros_like' type=Const&gt;,\n &lt;tf.Operation 'NotEqual' type=NotEqual&gt;,\n &lt;tf.Operation 'Cast' type=Cast&gt;,\n &lt;tf.Operation 'add' type=AddV2&gt;,\n &lt;tf.Operation 'zeros_like_1' type=Const&gt;,\n &lt;tf.Operation 'Maximum' type=Maximum&gt;,\n &lt;tf.Operation 'while/maximum_iterations' type=Const&gt;,\n &lt;tf.Operation 'while/loop_counter' type=Const&gt;,\n &lt;tf.Operation 'while' type=StatelessWhile&gt;,\n &lt;tf.Operation 'Identity' type=Identity&gt;]\n\n\n\n\nTF 함수에서 변수와 다른 자원 다루기\n\ncounter = tf.Variable(0)\n\n@tf.function\ndef increment(counter, c=1):\n    return counter.assign_add(c)\n\n\nincrement(counter)\nincrement(counter)\n\n&lt;tf.Tensor: shape=(), dtype=int32, numpy=2&gt;\n\n\n\nfunction_def = increment.get_concrete_function(counter).function_def\nfunction_def.signature.input_arg[0]\n\nname: \"counter\"\ntype: DT_RESOURCE\n\n\n\ncounter = tf.Variable(0)\n\n@tf.function\ndef increment(c=1):\n    return counter.assign_add(c)\n\n\nincrement()\nincrement()\n\n&lt;tf.Tensor: shape=(), dtype=int32, numpy=2&gt;\n\n\n\nfunction_def = increment.get_concrete_function().function_def\nfunction_def.signature.input_arg[0]\n\nname: \"assignaddvariableop_resource\"\ntype: DT_RESOURCE\n\n\n\nclass Counter:\n    def __init__(self):\n        self.counter = tf.Variable(0)\n\n    @tf.function\n    def increment(self, c=1):\n        return self.counter.assign_add(c)\n\n\nc = Counter()\nc.increment()\nc.increment()\n\n&lt;tf.Tensor: shape=(), dtype=int32, numpy=2&gt;\n\n\n\n@tf.function\ndef add_10(x):\n    for i in tf.range(10):\n        x += 1\n    return x\n\nprint(tf.autograph.to_code(add_10.python_function))\n\ndef tf__add(x):\n    with ag__.FunctionScope('add_10', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n        do_return = False\n        retval_ = ag__.UndefinedReturnValue()\n\n        def get_state():\n            return (x,)\n\n        def set_state(vars_):\n            nonlocal x\n            (x,) = vars_\n\n        def loop_body(itr):\n            nonlocal x\n            i = itr\n            x = ag__.ld(x)\n            x += 1\n        i = ag__.Undefined('i')\n        ag__.for_stmt(ag__.converted_call(ag__.ld(tf).range, (10,), None, fscope), None, loop_body, get_state, set_state, ('x',), {'iterate_names': 'i'})\n        try:\n            do_return = True\n            retval_ = ag__.ld(x)\n        except:\n            do_return = False\n            raise\n        return fscope.ret(retval_, do_return)\n\n\n\n\ndef display_tf_code(func):\n    from IPython.display import display, Markdown\n    if hasattr(func, \"python_function\"):\n        func = func.python_function\n    code = tf.autograph.to_code(func)\n    display(Markdown('```python\\n{}\\n```'.format(code)))\n\n\ndisplay_tf_code(add_10)\n\ndef tf__add(x):\n    with ag__.FunctionScope('add_10', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n        do_return = False\n        retval_ = ag__.UndefinedReturnValue()\n\n        def get_state():\n            return (x,)\n\n        def set_state(vars_):\n            nonlocal x\n            (x,) = vars_\n\n        def loop_body(itr):\n            nonlocal x\n            i = itr\n            x = ag__.ld(x)\n            x += 1\n        i = ag__.Undefined('i')\n        ag__.for_stmt(ag__.converted_call(ag__.ld(tf).range, (10,), None, fscope), None, loop_body, get_state, set_state, ('x',), {'iterate_names': 'i'})\n        try:\n            do_return = True\n            retval_ = ag__.ld(x)\n        except:\n            do_return = False\n            raise\n        return fscope.ret(retval_, do_return)"
  },
  {
    "objectID": "Machine_Learning/12_custom_models_and_training_with_tensorflow.html#tf.keras와-tf-함수를-함께-사용하거나-사용하지-않기",
    "href": "Machine_Learning/12_custom_models_and_training_with_tensorflow.html#tf.keras와-tf-함수를-함께-사용하거나-사용하지-않기",
    "title": "12_custom_models_and_training_with_tensorflow",
    "section": "tf.keras와 TF 함수를 함께 사용하거나 사용하지 않기",
    "text": "tf.keras와 TF 함수를 함께 사용하거나 사용하지 않기\n기본적으로 tf.keras는 자동으로 사용자 정의 코드를 TF 함수로 변환하기 때문에 tf.function()을 사용할 필요가 없습니다:\n\n# 사용자 손실 함수\ndef my_mse(y_true, y_pred):\n    print(\"my_mse() 손실 트레이싱\")\n    return tf.reduce_mean(tf.square(y_pred - y_true))\n\n\n# 사용자 지표 함수\ndef my_mae(y_true, y_pred):\n    print(\"my_mae() 지표 트레이싱\")\n    return tf.reduce_mean(tf.abs(y_pred - y_true))\n\n\n# 사용자 정의 층\nclass MyDense(keras.layers.Layer):\n    def __init__(self, units, activation=None, **kwargs):\n        super().__init__(**kwargs)\n        self.units = units\n        self.activation = keras.activations.get(activation)\n\n    def build(self, input_shape):\n        self.kernel = self.add_weight(name='kernel', \n                                      shape=(input_shape[1], self.units),\n                                      initializer='uniform',\n                                      trainable=True)\n        self.biases = self.add_weight(name='bias', \n                                      shape=(self.units,),\n                                      initializer='zeros',\n                                      trainable=True)\n        super().build(input_shape)\n\n    def call(self, X):\n        print(\"MyDense.call() 트레이싱\")\n        return self.activation(X @ self.kernel + self.biases)\n\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n\n# 사용자 정의 모델\nclass MyModel(keras.models.Model):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.hidden1 = MyDense(30, activation=\"relu\")\n        self.hidden2 = MyDense(30, activation=\"relu\")\n        self.output_ = MyDense(1)\n\n    def call(self, input):\n        print(\"MyModel.call() 트레이싱\")\n        hidden1 = self.hidden1(input)\n        hidden2 = self.hidden2(hidden1)\n        concat = keras.layers.concatenate([input, hidden2])\n        output = self.output_(concat)\n        return output\n\nmodel = MyModel()\n\n\nmodel.compile(loss=my_mse, optimizer=\"nadam\", metrics=[my_mae])\n\n\nmodel.fit(X_train_scaled, y_train, epochs=2,\n          validation_data=(X_valid_scaled, y_valid))\nmodel.evaluate(X_test_scaled, y_test)\n\nEpoch 1/2\nMyModel.call() 트레이싱\nMyDense.call() 트레이싱\nMyDense.call() 트레이싱\nMyDense.call() 트레이싱\nmy_mse() 손실 트레이싱\nmy_mae() 지표 트레이싱\nMyModel.call() 트레이싱\nMyDense.call() 트레이싱\nMyDense.call() 트레이싱\nMyDense.call() 트레이싱\nmy_mse() 손실 트레이싱\nmy_mae() 지표 트레이싱\n353/363 [============================&gt;.] - ETA: 0s - loss: 1.3459 - my_mae: 0.7978MyModel.call() 트레이싱\nMyDense.call() 트레이싱\nMyDense.call() 트레이싱\nMyDense.call() 트레이싱\nmy_mse() 손실 트레이싱\nmy_mae() 지표 트레이싱\n363/363 [==============================] - 3s 5ms/step - loss: 1.3255 - my_mae: 0.7900 - val_loss: 0.5569 - val_my_mae: 0.4819\nEpoch 2/2\n363/363 [==============================] - 1s 4ms/step - loss: 0.4419 - my_mae: 0.4767 - val_loss: 0.4664 - val_my_mae: 0.4576\n162/162 [==============================] - 0s 2ms/step - loss: 0.4164 - my_mae: 0.4639\n\n\n[0.4163525104522705, 0.4639028012752533]\n\n\ndynamic=True로 모델을 만들어 이 기능을 끌 수 있습니다(또는 모델의 생성자에서 super().__init__(dynamic=True, **kwargs)를 호출합니다):\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n\nmodel = MyModel(dynamic=True)\n\n\nmodel.compile(loss=my_mse, optimizer=\"nadam\", metrics=[my_mae])\n\n사용자 정의 코드는 반복마다 호출됩니다. 너무 많이 출력되는 것을 피하기 위해 작은 데이터셋으로 훈련, 검증, 평가해 보겠습니다:\n\nmodel.fit(X_train_scaled[:64], y_train[:64], epochs=1,\n          validation_data=(X_valid_scaled[:64], y_valid[:64]), verbose=0)\nmodel.evaluate(X_test_scaled[:64], y_test[:64], verbose=0)\n\nMyModel.call() 트레이싱\nMyDense.call() 트레이싱\nMyDense.call() 트레이싱\nMyDense.call() 트레이싱\nmy_mse() 손실 트레이싱\nmy_mae() 지표 트레이싱\nMyModel.call() 트레이싱\nMyDense.call() 트레이싱\nMyDense.call() 트레이싱\nMyDense.call() 트레이싱\nmy_mse() 손실 트레이싱\nmy_mae() 지표 트레이싱\nMyModel.call() 트레이싱\nMyDense.call() 트레이싱\nMyDense.call() 트레이싱\nMyDense.call() 트레이싱\nmy_mse() 손실 트레이싱\nmy_mae() 지표 트레이싱\nMyModel.call() 트레이싱\nMyDense.call() 트레이싱\nMyDense.call() 트레이싱\nMyDense.call() 트레이싱\nmy_mse() 손실 트레이싱\nmy_mae() 지표 트레이싱\nMyModel.call() 트레이싱\nMyDense.call() 트레이싱\nMyDense.call() 트레이싱\nMyDense.call() 트레이싱\nmy_mse() 손실 트레이싱\nmy_mae() 지표 트레이싱\nMyModel.call() 트레이싱\nMyDense.call() 트레이싱\nMyDense.call() 트레이싱\nMyDense.call() 트레이싱\nmy_mse() 손실 트레이싱\nmy_mae() 지표 트레이싱\n\n\n[5.507259368896484, 2.0566811561584473]\n\n\n또는 모델을 컴파일할 때 run_eagerly=True를 지정합니다:\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n\nmodel = MyModel()\n\n\nmodel.compile(loss=my_mse, optimizer=\"nadam\", metrics=[my_mae], run_eagerly=True)\n\n\nmodel.fit(X_train_scaled[:64], y_train[:64], epochs=1,\n          validation_data=(X_valid_scaled[:64], y_valid[:64]), verbose=0)\nmodel.evaluate(X_test_scaled[:64], y_test[:64], verbose=0)\n\nMyModel.call() 트레이싱\nMyDense.call() 트레이싱\nMyDense.call() 트레이싱\nMyDense.call() 트레이싱\nmy_mse() 손실 트레이싱\nmy_mae() 지표 트레이싱\nMyModel.call() 트레이싱\nMyDense.call() 트레이싱\nMyDense.call() 트레이싱\nMyDense.call() 트레이싱\nmy_mse() 손실 트레이싱\nmy_mae() 지표 트레이싱\nMyModel.call() 트레이싱\nMyDense.call() 트레이싱\nMyDense.call() 트레이싱\nMyDense.call() 트레이싱\nmy_mse() 손실 트레이싱\nmy_mae() 지표 트레이싱\nMyModel.call() 트레이싱\nMyDense.call() 트레이싱\nMyDense.call() 트레이싱\nMyDense.call() 트레이싱\nmy_mse() 손실 트레이싱\nmy_mae() 지표 트레이싱\nMyModel.call() 트레이싱\nMyDense.call() 트레이싱\nMyDense.call() 트레이싱\nMyDense.call() 트레이싱\nmy_mse() 손실 트레이싱\nmy_mae() 지표 트레이싱\nMyModel.call() 트레이싱\nMyDense.call() 트레이싱\nMyDense.call() 트레이싱\nMyDense.call() 트레이싱\nmy_mse() 손실 트레이싱\nmy_mae() 지표 트레이싱\n\n\n[5.507259368896484, 2.0566811561584473]"
  },
  {
    "objectID": "Machine_Learning/12_custom_models_and_training_with_tensorflow.html#사용자-정의-옵티마이저",
    "href": "Machine_Learning/12_custom_models_and_training_with_tensorflow.html#사용자-정의-옵티마이저",
    "title": "12_custom_models_and_training_with_tensorflow",
    "section": "사용자 정의 옵티마이저",
    "text": "사용자 정의 옵티마이저\n사용자 정의 옵티마이저를 정의하는 것은 일반적이지 않습니다. 하지만 어쩔 수 없이 만들어야 하는 상황이라면 다음 예를 참고하세요:\n\nclass MyMomentumOptimizer(keras.optimizers.Optimizer):\n    def __init__(self, learning_rate=0.001, momentum=0.9, name=\"MyMomentumOptimizer\", **kwargs):\n        \"\"\"super().__init__()를 호출하고 _set_hyper()를 사용해 하이퍼파라미터를 저장합니다\"\"\"\n        super().__init__(name, **kwargs)\n        self._set_hyper(\"learning_rate\", kwargs.get(\"lr\", learning_rate)) # lr=learning_rate을 처리\n        self._set_hyper(\"decay\", self._initial_decay) # \n        self._set_hyper(\"momentum\", momentum)\n    \n    def _create_slots(self, var_list):\n        \"\"\"모델 파라미터마다 연관된 옵티마이저 변수를 만듭니다.\n        텐서플로는 이런 옵티마이저 변수를 '슬롯'이라고 부릅니다.\n        모멘텀 옵티마이저에서는 모델 파라미터마다 하나의 모멘텀 슬롯이 필요합니다.\n        \"\"\"\n        for var in var_list:\n            self.add_slot(var, \"momentum\")\n\n    @tf.function\n    def _resource_apply_dense(self, grad, var):\n        \"\"\"슬롯을 업데이트하고 모델 파라미터에 대한 옵티마이저 스텝을 수행합니다.\n        \"\"\"\n        var_dtype = var.dtype.base_dtype\n        lr_t = self._decayed_lr(var_dtype) # 학습률 감쇠 처리\n        momentum_var = self.get_slot(var, \"momentum\")\n        momentum_hyper = self._get_hyper(\"momentum\", var_dtype)\n        momentum_var.assign(momentum_var * momentum_hyper - (1. - momentum_hyper)* grad)\n        var.assign_add(momentum_var * lr_t)\n\n    def _resource_apply_sparse(self, grad, var):\n        raise NotImplementedError\n\n    def get_config(self):\n        base_config = super().get_config()\n        return {\n            **base_config,\n            \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n            \"decay\": self._serialize_hyperparameter(\"decay\"),\n            \"momentum\": self._serialize_hyperparameter(\"momentum\"),\n        }\n\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n\nmodel = keras.models.Sequential([keras.layers.Dense(1, input_shape=[8])])\nmodel.compile(loss=\"mse\", optimizer=MyMomentumOptimizer())\nmodel.fit(X_train_scaled, y_train, epochs=5)\n\nEpoch 1/5\n363/363 [==============================] - 1s 2ms/step - loss: 3.8128\nEpoch 2/5\n363/363 [==============================] - 1s 2ms/step - loss: 1.4877\nEpoch 3/5\n363/363 [==============================] - 1s 2ms/step - loss: 0.9162\nEpoch 4/5\n363/363 [==============================] - 1s 2ms/step - loss: 0.7587\nEpoch 5/5\n363/363 [==============================] - 1s 2ms/step - loss: 0.7050\n\n\n&lt;keras.callbacks.History at 0x7fd478dd75d0&gt;"
  },
  {
    "objectID": "Machine_Learning/12_custom_models_and_training_with_tensorflow.html#to-11.",
    "href": "Machine_Learning/12_custom_models_and_training_with_tensorflow.html#to-11.",
    "title": "12_custom_models_and_training_with_tensorflow",
    "section": "1. to 11.",
    "text": "1. to 11.\n부록 A 참조."
  },
  {
    "objectID": "Machine_Learning/12_custom_models_and_training_with_tensorflow.html#층-정규화-를-수행하는-사용자-정의-층을-구현하세요.",
    "href": "Machine_Learning/12_custom_models_and_training_with_tensorflow.html#층-정규화-를-수행하는-사용자-정의-층을-구현하세요.",
    "title": "12_custom_models_and_training_with_tensorflow",
    "section": "12. 층 정규화 를 수행하는 사용자 정의 층을 구현하세요.",
    "text": "12. 층 정규화 를 수행하는 사용자 정의 층을 구현하세요.\n15장에서 순환 신경망을 사용할 때 이런 종류의 층을 사용합니다.\n\na.\n문제: build() 메서드에서 두 개의 훈련 가능한 가중치 α와 β를 정의합니다. 두 가중치 모두 크기가 input_shape[-1:]이고 데이터 타입은 tf.float32입니다. α는 1로 초기화되고 β는 0으로 초기화되어야 합니다.\n솔루션: 아래 참조.\n\n\nb.\n문제: call() 메서드는 샘플의 특성마다 평균 μ와 표준편차 σ를 계산해야 합니다. 이를 위해 전체 샘플의 평균 μ와 분산 σ2을 반환하는 tf.nn.moments(inputs, axes=-1, keepdims=True)을 사용할 수 있습니다(분산의 제곱근으로 표준편차를 계산합니다). 그다음 α⊗(X - μ)/(σ + ε) + β를 계산하여 반환합니다. 여기에서 ⊗는 원소별 곱셈(*)을 나타냅니다. ε은 안전을 위한 항입니다(0으로 나누어지는 것을 막기 위한 작은 상수. 예를 들면 0.001).\n\nclass LayerNormalization(keras.layers.Layer):\n    def __init__(self, eps=0.001, **kwargs):\n        super().__init__(**kwargs)\n        self.eps = eps\n\n    def build(self, batch_input_shape):\n        self.alpha = self.add_weight(\n            name=\"alpha\", shape=batch_input_shape[-1:],\n            initializer=\"ones\")\n        self.beta = self.add_weight(\n            name=\"beta\", shape=batch_input_shape[-1:],\n            initializer=\"zeros\")\n        super().build(batch_input_shape) # 반드시 끝에 와야 합니다\n\n    def call(self, X):\n        mean, variance = tf.nn.moments(X, axes=-1, keepdims=True)\n        return self.alpha * (X - mean) / (tf.sqrt(variance + self.eps)) + self.beta\n\n    def compute_output_shape(self, batch_input_shape):\n        return batch_input_shape\n\n    def get_config(self):\n        base_config = super().get_config()\n        return {**base_config, \"eps\": self.eps}\n\nε 하이퍼파라미터(eps)는 필수가 아닙니다. 또한 tf.sqrt(variance) + self.eps 보다 tf.sqrt(variance + self.eps)를 계산하는 것이 좋습니다. sqrt(z)의 도함수는 z=0에서 정의되지 않기 때문에 분산 벡터의 한 원소가 0에 가까우면 훈련이 이리저리 널뜁니다. 제곱근 안에 _ε_를 넣으면 이런 현상을 방지할 수 있습니다.\n\n\nc.\n문제: 사용자 정의 층이 keras.layers.LayerNormalization 층과 동일한(또는 거의 동일한) 출력을 만드는지 확인하세요.\n각 클래스의 객체를 만들고 데이터(예를 들면, 훈련 세트)를 적용해 보죠. 차이는 무시할 수 있는 수준입니다.\n\nX = X_train.astype(np.float32)\n\ncustom_layer_norm = LayerNormalization()\nkeras_layer_norm = keras.layers.LayerNormalization()\n\ntf.reduce_mean(keras.losses.mean_absolute_error(\n    keras_layer_norm(X), custom_layer_norm(X)))\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=3.80914e-08&gt;\n\n\n네 충분히 가깝네요. 조금 더 확실하게 알파와 베타를 완전히 랜덤하게 지정하고 다시 비교해 보죠:\n\nrandom_alpha = np.random.rand(X.shape[-1])\nrandom_beta = np.random.rand(X.shape[-1])\n\ncustom_layer_norm.set_weights([random_alpha, random_beta])\nkeras_layer_norm.set_weights([random_alpha, random_beta])\n\ntf.reduce_mean(keras.losses.mean_absolute_error(\n    keras_layer_norm(X), custom_layer_norm(X)))\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=1.695759e-08&gt;\n\n\n여전히 무시할 수 있는 수준입니다! 사용자 정의 층이 잘 동작합니다."
  },
  {
    "objectID": "Machine_Learning/12_custom_models_and_training_with_tensorflow.html#사용자-정의-훈련-반복을-사용해-패션-mnist-데이터셋으로-모델을-훈련해보세요.",
    "href": "Machine_Learning/12_custom_models_and_training_with_tensorflow.html#사용자-정의-훈련-반복을-사용해-패션-mnist-데이터셋으로-모델을-훈련해보세요.",
    "title": "12_custom_models_and_training_with_tensorflow",
    "section": "13. 사용자 정의 훈련 반복을 사용해 패션 MNIST 데이터셋으로 모델을 훈련해보세요.",
    "text": "13. 사용자 정의 훈련 반복을 사용해 패션 MNIST 데이터셋으로 모델을 훈련해보세요.\n패션 MNIST 데이터셋은 10장에서 소개했습니다.\n\na.\n문제: 에포크, 반복, 평균 훈련 손실, (반복마다 업데이트되는) 에포크의 평균 정확도는 물론 에포크 끝에서 검증 손실과 정확도를 출력하세요.\n\n(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\nX_train_full = X_train_full.astype(np.float32) / 255.\nX_valid, X_train = X_train_full[:5000], X_train_full[5000:]\ny_valid, y_train = y_train_full[:5000], y_train_full[5000:]\nX_test = X_test.astype(np.float32) / 255.\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n32768/29515 [=================================] - 0s 0us/step\n40960/29515 [=========================================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n26427392/26421880 [==============================] - 0s 0us/step\n26435584/26421880 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n16384/5148 [===============================================================================================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n4423680/4422102 [==============================] - 0s 0us/step\n4431872/4422102 [==============================] - 0s 0us/step\n\n\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(100, activation=\"relu\"),\n    keras.layers.Dense(10, activation=\"softmax\"),\n])\n\n\nn_epochs = 5\nbatch_size = 32\nn_steps = len(X_train) // batch_size\noptimizer = keras.optimizers.Nadam(learning_rate=0.01)\nloss_fn = keras.losses.sparse_categorical_crossentropy\nmean_loss = keras.metrics.Mean()\nmetrics = [keras.metrics.SparseCategoricalAccuracy()]\n\n/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n\n\n\nwith trange(1, n_epochs + 1, desc=\"All epochs\") as epochs:\n    for epoch in epochs:\n        with trange(1, n_steps + 1, desc=\"Epoch {}/{}\".format(epoch, n_epochs)) as steps:\n            for step in steps:\n                X_batch, y_batch = random_batch(X_train, y_train)\n                with tf.GradientTape() as tape:\n                    y_pred = model(X_batch)\n                    main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n                    loss = tf.add_n([main_loss] + model.losses)\n                gradients = tape.gradient(loss, model.trainable_variables)\n                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n                for variable in model.variables:\n                    if variable.constraint is not None:\n                        variable.assign(variable.constraint(variable))                    \n                status = OrderedDict()\n                mean_loss(loss)\n                status[\"loss\"] = mean_loss.result().numpy()\n                for metric in metrics:\n                    metric(y_batch, y_pred)\n                    status[metric.name] = metric.result().numpy()\n                steps.set_postfix(status)\n            y_pred = model(X_valid)\n            status[\"val_loss\"] = np.mean(loss_fn(y_valid, y_pred))\n            status[\"val_accuracy\"] = np.mean(keras.metrics.sparse_categorical_accuracy(\n                tf.constant(y_valid, dtype=np.float32), y_pred))\n            steps.set_postfix(status)\n        for metric in [mean_loss] + metrics:\n            metric.reset_states()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nb.\n문제: 상위 층과 하위 층에 학습률이 다른 옵티마이저를 따로 사용해보세요.\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n\nlower_layers = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(100, activation=\"relu\"),\n])\nupper_layers = keras.models.Sequential([\n    keras.layers.Dense(10, activation=\"softmax\"),\n])\nmodel = keras.models.Sequential([\n    lower_layers, upper_layers\n])\n\n\nlower_optimizer = keras.optimizers.SGD(learning_rate=1e-4)\nupper_optimizer = keras.optimizers.Nadam(learning_rate=1e-3)\n\n/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n\n\n\nn_epochs = 5\nbatch_size = 32\nn_steps = len(X_train) // batch_size\nloss_fn = keras.losses.sparse_categorical_crossentropy\nmean_loss = keras.metrics.Mean()\nmetrics = [keras.metrics.SparseCategoricalAccuracy()]\n\n\nwith trange(1, n_epochs + 1, desc=\"All epochs\") as epochs:\n    for epoch in epochs:\n        with trange(1, n_steps + 1, desc=\"Epoch {}/{}\".format(epoch, n_epochs)) as steps:\n            for step in steps:\n                X_batch, y_batch = random_batch(X_train, y_train)\n                with tf.GradientTape(persistent=True) as tape:\n                    y_pred = model(X_batch)\n                    main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n                    loss = tf.add_n([main_loss] + model.losses)\n                for layers, optimizer in ((lower_layers, lower_optimizer),\n                                          (upper_layers, upper_optimizer)):\n                    gradients = tape.gradient(loss, layers.trainable_variables)\n                    optimizer.apply_gradients(zip(gradients, layers.trainable_variables))\n                del tape\n                for variable in model.variables:\n                    if variable.constraint is not None:\n                        variable.assign(variable.constraint(variable))                    \n                status = OrderedDict()\n                mean_loss(loss)\n                status[\"loss\"] = mean_loss.result().numpy()\n                for metric in metrics:\n                    metric(y_batch, y_pred)\n                    status[metric.name] = metric.result().numpy()\n                steps.set_postfix(status)\n            y_pred = model(X_valid)\n            status[\"val_loss\"] = np.mean(loss_fn(y_valid, y_pred))\n            status[\"val_accuracy\"] = np.mean(keras.metrics.sparse_categorical_accuracy(\n                tf.constant(y_valid, dtype=np.float32), y_pred))\n            steps.set_postfix(status)\n        for metric in [mean_loss] + metrics:\n            metric.reset_states()"
  },
  {
    "objectID": "Machine_Learning/14_deep_computer_vision_with_cnns.html",
    "href": "Machine_Learning/14_deep_computer_vision_with_cnns.html",
    "title": "14_deep_computer_vision_with_cnns",
    "section": "",
    "text": "14장 – 합성곱 신경망을 사용한 컴퓨터 비전\n이 노트북은 14장에 있는 모든 샘플 코드와 연습문제 해답을 가지고 있습니다."
  },
  {
    "objectID": "Machine_Learning/14_deep_computer_vision_with_cnns.html#합성곱-층",
    "href": "Machine_Learning/14_deep_computer_vision_with_cnns.html#합성곱-층",
    "title": "14_deep_computer_vision_with_cnns",
    "section": "합성곱 층",
    "text": "합성곱 층\nkeras.layers.Conv2D()를 사용해 2D 합성곱 층을 만들어 보죠:\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nconv = keras.layers.Conv2D(filters=2, kernel_size=7, strides=1,\n                           padding=\"SAME\", activation=\"relu\", input_shape=outputs.shape)\n\n두 개의 테스트 이미지로 이 층을 호출합니다:\n\nconv_outputs = conv(images)\nconv_outputs.shape \n\nTensorShape([2, 427, 640, 2])\n\n\n출력은 4D 텐서입니다. 차원은 배치 크기, 높이, 너비, 채널입니다. 2개의 이미지를 입력으로 사용했기 때문에 첫 번째 차원(배치 크기)는 2입니다. 다음 두 차원은 출력 특성맵의 높이와 너비입니다. padding=\"SAME\"와 strides=1로 설정했기 때문에 출력 특성맵의 높이와 너비는 입력 이미지와 같습니다(이 경우 427×640). 마지막으로 이 합성곱 층은 2개의 필터를 사용합니다. 따라서 마지막 차원의 크기는 2입니다. 즉 입력 이미지마다 2개의 특성맵이 출력됩니다.\n필터는 초기에 랜덤하게 초기화되기 때문에 처음에는 랜덤한 패턴을 감지합니다. 이미지마다 출력된 2개의 특성맵을 확이해 보죠:\n\nplt.figure(figsize=(10,6))\nfor image_index in (0, 1):\n    for feature_map_index in (0, 1):\n        plt.subplot(2, 2, image_index * 2 + feature_map_index + 1)\n        plot_image(crop(conv_outputs[image_index, :, :, feature_map_index]))\nplt.show()\n\n\n\n\n필터가 초기에 랜덤하게 초기화되엇지만 두 번째 필터는 에지를 감지한 것처럼 보입니다. 랜덤하게 초기화된 필터는 종종 이런 식으로 동작합니다. 에지 감지는 이미지 처리에 매우 유용하기 때문에 운이 좋습니다.\n원한다면 필터를 앞에서 수동으로 정의한 필터를 사용하고 편향을 0으로 지정할 수 있습니다(합성곱 층이 훈련하는 동안 적절한 필터와 편향을 학습하기 때문에 실제로는 수동으로 필터와 편향을 지정할 필요가 거의 없습니다):\n\nconv.set_weights([filters, np.zeros(2)])\n\n이제 이 층을 동일한 두 이미지에서 다시 호출해 보죠. 그다음 출력 특성맵이 (앞에서처럼) 수직선과 수평선을 부각하는지 확인해 보겠습니다:\n\nconv_outputs = conv(images)\nconv_outputs.shape \n\nTensorShape([2, 427, 640, 2])\n\n\n\nplt.figure(figsize=(10,6))\nfor image_index in (0, 1):\n    for feature_map_index in (0, 1):\n        plt.subplot(2, 2, image_index * 2 + feature_map_index + 1)\n        plot_image(crop(conv_outputs[image_index, :, :, feature_map_index]))\nplt.show()"
  },
  {
    "objectID": "Machine_Learning/14_deep_computer_vision_with_cnns.html#valid-vs-same-패딩",
    "href": "Machine_Learning/14_deep_computer_vision_with_cnns.html#valid-vs-same-패딩",
    "title": "14_deep_computer_vision_with_cnns",
    "section": "VALID vs SAME 패딩",
    "text": "VALID vs SAME 패딩\n\ndef feature_map_size(input_size, kernel_size, strides=1, padding=\"SAME\"):\n    if padding == \"SAME\":\n        return (input_size - 1) // strides + 1\n    else:\n        return (input_size - kernel_size) // strides + 1\n\n\ndef pad_before_and_padded_size(input_size, kernel_size, strides=1):\n    fmap_size = feature_map_size(input_size, kernel_size, strides)\n    padded_size = max((fmap_size - 1) * strides + kernel_size, input_size)\n    pad_before = (padded_size - input_size) // 2\n    return pad_before, padded_size\n\n\ndef manual_same_padding(images, kernel_size, strides=1):\n    if kernel_size == 1:\n        return images.astype(np.float32)\n    batch_size, height, width, channels = images.shape\n    top_pad, padded_height = pad_before_and_padded_size(height, kernel_size, strides)\n    left_pad, padded_width  = pad_before_and_padded_size(width, kernel_size, strides)\n    padded_shape = [batch_size, padded_height, padded_width, channels]\n    padded_images = np.zeros(padded_shape, dtype=np.float32)\n    padded_images[:, top_pad:height+top_pad, left_pad:width+left_pad, :] = images\n    return padded_images\n\n\"SAME\" 패딩을 사용하는 것은 manual_same_padding()을 사용해 수동으로 패딩하고 \"VALID\" 패딩을 사용하는 것과 동일합니다(혼동될 수 있지만 \"VALID\" 패딩은 전혀 패딩을 하지 않는다는 뜻입니다):\n\nkernel_size = 7\nstrides = 2\n\nconv_valid = keras.layers.Conv2D(filters=1, kernel_size=kernel_size, strides=strides, padding=\"VALID\")\nconv_same = keras.layers.Conv2D(filters=1, kernel_size=kernel_size, strides=strides, padding=\"SAME\")\n\nvalid_output = conv_valid(manual_same_padding(images, kernel_size, strides))\n\n# conv_same의 가중치를 생성하기 위해 build() 메서드를 호출해야 합니다.\nconv_same.build(tf.TensorShape(images.shape))\n\n# conv_valid의 가중치를 conv_same으로 복사합니다.\nconv_same.set_weights(conv_valid.get_weights())\n\nsame_output = conv_same(images.astype(np.float32))\n\nassert np.allclose(valid_output.numpy(), same_output.numpy())"
  },
  {
    "objectID": "Machine_Learning/14_deep_computer_vision_with_cnns.html#최대-풀링",
    "href": "Machine_Learning/14_deep_computer_vision_with_cnns.html#최대-풀링",
    "title": "14_deep_computer_vision_with_cnns",
    "section": "최대 풀링",
    "text": "최대 풀링\n\nmax_pool = keras.layers.MaxPool2D(pool_size=2)\n\n\ncropped_images = np.array([crop(image) for image in images], dtype=np.float32)\noutput = max_pool(cropped_images)\n\n\nfig = plt.figure(figsize=(12, 8))\ngs = mpl.gridspec.GridSpec(nrows=1, ncols=2, width_ratios=[2, 1])\n\nax1 = fig.add_subplot(gs[0, 0])\nax1.set_title(\"Input\", fontsize=14)\nax1.imshow(cropped_images[0])  # 첫 번째 이미지 그리기\nax1.axis(\"off\")\nax2 = fig.add_subplot(gs[0, 1])\nax2.set_title(\"Output\", fontsize=14)\nax2.imshow(output[0])  # 첫 번째 이미지 출력 그리기\nax2.axis(\"off\")\nsave_fig(\"china_max_pooling\")\nplt.show()\n\n그림 저장 china_max_pooling"
  },
  {
    "objectID": "Machine_Learning/14_deep_computer_vision_with_cnns.html#깊이-방향depth-wise-풀링",
    "href": "Machine_Learning/14_deep_computer_vision_with_cnns.html#깊이-방향depth-wise-풀링",
    "title": "14_deep_computer_vision_with_cnns",
    "section": "깊이 방향(depth-wise) 풀링",
    "text": "깊이 방향(depth-wise) 풀링\n\nclass DepthMaxPool(keras.layers.Layer):\n    def __init__(self, pool_size, strides=None, padding=\"VALID\", **kwargs):\n        super().__init__(**kwargs)\n        if strides is None:\n            strides = pool_size\n        self.pool_size = pool_size\n        self.strides = strides\n        self.padding = padding\n    def call(self, inputs):\n        return tf.nn.max_pool(inputs,\n                              ksize=(1, 1, 1, self.pool_size),\n                              strides=(1, 1, 1, self.pool_size),\n                              padding=self.padding)\n\n\ndepth_pool = DepthMaxPool(3)\nwith tf.device(\"/cpu:0\"): # 아직 GPU 커널이 없습니다.\n    depth_output = depth_pool(cropped_images)\ndepth_output.shape\n\nTensorShape([2, 70, 120, 1])\n\n\n또는 Lambda 층을 사용합니다:\n\ndepth_pool = keras.layers.Lambda(lambda X: tf.nn.max_pool(\n    X, ksize=(1, 1, 1, 3), strides=(1, 1, 1, 3), padding=\"VALID\"))\nwith tf.device(\"/cpu:0\"): # 아직 GPU 커널이 없습니다.\n    depth_output = depth_pool(cropped_images)\ndepth_output.shape\n\nTensorShape([2, 70, 120, 1])\n\n\n\nplt.figure(figsize=(12, 8))\nplt.subplot(1, 2, 1)\nplt.title(\"Input\", fontsize=14)\nplot_color_image(cropped_images[0])  # 첫 번째 이미지 그리기\nplt.subplot(1, 2, 2)\nplt.title(\"Output\", fontsize=14)\nplot_image(depth_output[0, ..., 0])  # 첫 번째 이미지 출력 그리기\nplt.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "Machine_Learning/14_deep_computer_vision_with_cnns.html#평균-풀링",
    "href": "Machine_Learning/14_deep_computer_vision_with_cnns.html#평균-풀링",
    "title": "14_deep_computer_vision_with_cnns",
    "section": "평균 풀링",
    "text": "평균 풀링\n\navg_pool = keras.layers.AvgPool2D(pool_size=2)\n\n\noutput_avg = avg_pool(cropped_images)\n\n\nfig = plt.figure(figsize=(12, 8))\ngs = mpl.gridspec.GridSpec(nrows=1, ncols=2, width_ratios=[2, 1])\n\nax1 = fig.add_subplot(gs[0, 0])\nax1.set_title(\"Input\", fontsize=14)\nax1.imshow(cropped_images[0])  # 첫 번째 이미지 그리기\nax1.axis(\"off\")\nax2 = fig.add_subplot(gs[0, 1])\nax2.set_title(\"Output\", fontsize=14)\nax2.imshow(output_avg[0])  # 첫 번째 이미지 출력 그리기\nax2.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "Machine_Learning/14_deep_computer_vision_with_cnns.html#전역-평균-풀링",
    "href": "Machine_Learning/14_deep_computer_vision_with_cnns.html#전역-평균-풀링",
    "title": "14_deep_computer_vision_with_cnns",
    "section": "전역 평균 풀링",
    "text": "전역 평균 풀링\n\nglobal_avg_pool = keras.layers.GlobalAvgPool2D()\nglobal_avg_pool(cropped_images)\n\n&lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[0.2788777 , 0.22507192, 0.20967275],\n       [0.51288515, 0.45951638, 0.33423486]], dtype=float32)&gt;\n\n\n\noutput_global_avg2 = keras.layers.Lambda(lambda X: tf.reduce_mean(X, axis=[1, 2]))\noutput_global_avg2(cropped_images)\n\n&lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[0.2788777 , 0.22507192, 0.20967275],\n       [0.51288515, 0.45951638, 0.33423486]], dtype=float32)&gt;"
  },
  {
    "objectID": "Machine_Learning/14_deep_computer_vision_with_cnns.html#resnet-34",
    "href": "Machine_Learning/14_deep_computer_vision_with_cnns.html#resnet-34",
    "title": "14_deep_computer_vision_with_cnns",
    "section": "ResNet-34",
    "text": "ResNet-34\n\nDefaultConv2D = partial(keras.layers.Conv2D, kernel_size=3, strides=1,\n                        padding=\"SAME\", use_bias=False)\n\nclass ResidualUnit(keras.layers.Layer):\n    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n        super().__init__(**kwargs)\n        self.activation = keras.activations.get(activation)\n        self.main_layers = [\n            DefaultConv2D(filters, strides=strides),\n            keras.layers.BatchNormalization(),\n            self.activation,\n            DefaultConv2D(filters),\n            keras.layers.BatchNormalization()]\n        self.skip_layers = []\n        if strides &gt; 1:\n            self.skip_layers = [\n                DefaultConv2D(filters, kernel_size=1, strides=strides),\n                keras.layers.BatchNormalization()]\n\n    def call(self, inputs):\n        Z = inputs\n        for layer in self.main_layers:\n            Z = layer(Z)\n        skip_Z = inputs\n        for layer in self.skip_layers:\n            skip_Z = layer(skip_Z)\n        return self.activation(Z + skip_Z)\n\n\nmodel = keras.models.Sequential()\nmodel.add(DefaultConv2D(64, kernel_size=7, strides=2,\n                        input_shape=[224, 224, 3]))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Activation(\"relu\"))\nmodel.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"SAME\"))\nprev_filters = 64\nfor filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n    strides = 1 if filters == prev_filters else 2\n    model.add(ResidualUnit(filters, strides=strides))\n    prev_filters = filters\nmodel.add(keras.layers.GlobalAvgPool2D())\nmodel.add(keras.layers.Flatten())\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))\n\n\nmodel.summary()\n\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_8 (Conv2D)            (None, 112, 112, 64)      9408      \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 112, 112, 64)      256       \n_________________________________________________________________\nactivation (Activation)      (None, 112, 112, 64)      0         \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 56, 56, 64)        0         \n_________________________________________________________________\nresidual_unit (ResidualUnit) (None, 56, 56, 64)        74240     \n_________________________________________________________________\nresidual_unit_1 (ResidualUni (None, 56, 56, 64)        74240     \n_________________________________________________________________\nresidual_unit_2 (ResidualUni (None, 56, 56, 64)        74240     \n_________________________________________________________________\nresidual_unit_3 (ResidualUni (None, 28, 28, 128)       230912    \n_________________________________________________________________\nresidual_unit_4 (ResidualUni (None, 28, 28, 128)       295936    \n_________________________________________________________________\nresidual_unit_5 (ResidualUni (None, 28, 28, 128)       295936    \n_________________________________________________________________\nresidual_unit_6 (ResidualUni (None, 28, 28, 128)       295936    \n_________________________________________________________________\nresidual_unit_7 (ResidualUni (None, 14, 14, 256)       920576    \n_________________________________________________________________\nresidual_unit_8 (ResidualUni (None, 14, 14, 256)       1181696   \n_________________________________________________________________\nresidual_unit_9 (ResidualUni (None, 14, 14, 256)       1181696   \n_________________________________________________________________\nresidual_unit_10 (ResidualUn (None, 14, 14, 256)       1181696   \n_________________________________________________________________\nresidual_unit_11 (ResidualUn (None, 14, 14, 256)       1181696   \n_________________________________________________________________\nresidual_unit_12 (ResidualUn (None, 14, 14, 256)       1181696   \n_________________________________________________________________\nresidual_unit_13 (ResidualUn (None, 7, 7, 512)         3676160   \n_________________________________________________________________\nresidual_unit_14 (ResidualUn (None, 7, 7, 512)         4722688   \n_________________________________________________________________\nresidual_unit_15 (ResidualUn (None, 7, 7, 512)         4722688   \n_________________________________________________________________\nglobal_average_pooling2d_1 ( (None, 512)               0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 512)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 10)                5130      \n=================================================================\nTotal params: 21,306,826\nTrainable params: 21,289,802\nNon-trainable params: 17,024\n_________________________________________________________________"
  },
  {
    "objectID": "Machine_Learning/14_deep_computer_vision_with_cnns.html#사전-훈련된-모델-사용하기",
    "href": "Machine_Learning/14_deep_computer_vision_with_cnns.html#사전-훈련된-모델-사용하기",
    "title": "14_deep_computer_vision_with_cnns",
    "section": "사전 훈련된 모델 사용하기",
    "text": "사전 훈련된 모델 사용하기\n\nmodel = keras.applications.resnet50.ResNet50(weights=\"imagenet\")\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n102973440/102967424 [==============================] - 1s 0us/step\n102981632/102967424 [==============================] - 1s 0us/step\n\n\n\nimages_resized = tf.image.resize(images, [224, 224])\nplot_color_image(images_resized[0])\nplt.show()\n\n\n\n\n\nimages_resized = tf.image.resize_with_pad(images, 224, 224, antialias=True)\nplot_color_image(images_resized[0])\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\nimages_resized = tf.image.resize_with_crop_or_pad(images, 224, 224)\nplot_color_image(images_resized[0])\nplt.show()\n\n\n\n\n\nchina_box = [0, 0.03, 1, 0.68]\nflower_box = [0.19, 0.26, 0.86, 0.7]\nimages_resized = tf.image.crop_and_resize(images, [china_box, flower_box], [0, 1], [224, 224])\nplot_color_image(images_resized[0])\nplt.show()\nplot_color_image(images_resized[1])\nplt.show()\n\n\n\n\n\n\n\n\ninputs = keras.applications.resnet50.preprocess_input(images_resized * 255)\nY_proba = model.predict(inputs)\n\n\nY_proba.shape\n\n(2, 1000)\n\n\n\ntop_K = keras.applications.resnet50.decode_predictions(Y_proba, top=3)\nfor image_index in range(len(images)):\n    print(\"Image #{}\".format(image_index))\n    for class_id, name, y_proba in top_K[image_index]:\n        print(\"  {} - {:12s} {:.2f}%\".format(class_id, name, y_proba * 100))\n    print()\n\nDownloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n40960/35363 [==================================] - 0s 0us/step\n49152/35363 [=========================================] - 0s 0us/step\nImage #0\n  n03877845 - palace       43.39%\n  n02825657 - bell_cote    43.07%\n  n03781244 - monastery    11.70%\n\nImage #1\n  n04522168 - vase         53.96%\n  n07930864 - cup          9.52%\n  n11939491 - daisy        4.97%"
  },
  {
    "objectID": "Machine_Learning/14_deep_computer_vision_with_cnns.html#전이-학습을-위한-사전-훈련된-모델",
    "href": "Machine_Learning/14_deep_computer_vision_with_cnns.html#전이-학습을-위한-사전-훈련된-모델",
    "title": "14_deep_computer_vision_with_cnns",
    "section": "전이 학습을 위한 사전 훈련된 모델",
    "text": "전이 학습을 위한 사전 훈련된 모델\n\nimport tensorflow_datasets as tfds\n\ndataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)\n\nDownloading and preparing dataset tf_flowers/3.0.1 (download: 218.21 MiB, generated: 221.83 MiB, total: 440.05 MiB) to /root/tensorflow_datasets/tf_flowers/3.0.1...\n\nDataset tf_flowers downloaded and prepared to /root/tensorflow_datasets/tf_flowers/3.0.1. Subsequent calls will reuse this data.\n\n\nWARNING:absl:Dataset tf_flowers is hosted on GCS. It will automatically be downloaded to your\nlocal data directory. If you'd instead prefer to read directly from our public\nGCS bucket (recommended if you're running on GCP), you can instead pass\n`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\n\n\n\n\n\n\n\ninfo.splits\n\n{'train': &lt;tfds.core.SplitInfo num_examples=3670&gt;}\n\n\n\ninfo.splits[\"train\"]\n\n&lt;tfds.core.SplitInfo num_examples=3670&gt;\n\n\n\nclass_names = info.features[\"label\"].names\nclass_names\n\n['dandelion', 'daisy', 'tulips', 'sunflowers', 'roses']\n\n\n\nn_classes = info.features[\"label\"].num_classes\n\n\ndataset_size = info.splits[\"train\"].num_examples\ndataset_size\n\n3670\n\n\n경고: TFDS의 split API는 책이 출간된 후에 바뀌었습니다. 새로운 split API(S3 슬라이싱 API)는 사용하기 훨씬 간단합니다:\n\ntest_set_raw, valid_set_raw, train_set_raw = tfds.load(\n    \"tf_flowers\",\n    split=[\"train[:10%]\", \"train[10%:25%]\", \"train[25%:]\"],\n    as_supervised=True)\n\n\nplt.figure(figsize=(12, 10))\nindex = 0\nfor image, label in train_set_raw.take(9):\n    index += 1\n    plt.subplot(3, 3, index)\n    plt.imshow(image)\n    plt.title(\"Class: {}\".format(class_names[label]))\n    plt.axis(\"off\")\n\nplt.show()\n\n\n\n\n기본 전처리:\n\ndef preprocess(image, label):\n    resized_image = tf.image.resize(image, [224, 224])\n    final_image = keras.applications.xception.preprocess_input(resized_image)\n    return final_image, label\n\n조금 더 정교한 전처리 (하지만 훨씬 많은 데이터 증식을 할 수 있습니다):\n\ndef central_crop(image):\n    shape = tf.shape(image)\n    min_dim = tf.reduce_min([shape[0], shape[1]])\n    top_crop = (shape[0] - min_dim) // 4\n    bottom_crop = shape[0] - top_crop\n    left_crop = (shape[1] - min_dim) // 4\n    right_crop = shape[1] - left_crop\n    return image[top_crop:bottom_crop, left_crop:right_crop]\n\ndef random_crop(image):\n    shape = tf.shape(image)\n    min_dim = tf.reduce_min([shape[0], shape[1]]) * 90 // 100\n    return tf.image.random_crop(image, [min_dim, min_dim, 3])\n\ndef preprocess(image, label, randomize=False):\n    if randomize:\n        cropped_image = random_crop(image)\n        cropped_image = tf.image.random_flip_left_right(cropped_image)\n    else:\n        cropped_image = central_crop(image)\n    resized_image = tf.image.resize(cropped_image, [224, 224])\n    final_image = keras.applications.xception.preprocess_input(resized_image)\n    return final_image, label\n\nbatch_size = 32\ntrain_set = train_set_raw.shuffle(1000).repeat()\ntrain_set = train_set.map(partial(preprocess, randomize=True)).batch(batch_size).prefetch(1)\nvalid_set = valid_set_raw.map(preprocess).batch(batch_size).prefetch(1)\ntest_set = test_set_raw.map(preprocess).batch(batch_size).prefetch(1)\n\n\nplt.figure(figsize=(12, 12))\nfor X_batch, y_batch in train_set.take(1):\n    for index in range(9):\n        plt.subplot(3, 3, index + 1)\n        plt.imshow(X_batch[index] / 2 + 0.5)\n        plt.title(\"Class: {}\".format(class_names[y_batch[index]]))\n        plt.axis(\"off\")\n\nplt.show()\n\n\n\n\n\nplt.figure(figsize=(12, 12))\nfor X_batch, y_batch in test_set.take(1):\n    for index in range(9):\n        plt.subplot(3, 3, index + 1)\n        plt.imshow(X_batch[index] / 2 + 0.5)\n        plt.title(\"Class: {}\".format(class_names[y_batch[index]]))\n        plt.axis(\"off\")\n\nplt.show()\n\n\n\n\n\nbase_model = keras.applications.xception.Xception(weights=\"imagenet\",\n                                                  include_top=False)\navg = keras.layers.GlobalAveragePooling2D()(base_model.output)\noutput = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\nmodel = keras.models.Model(inputs=base_model.input, outputs=output)\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n83689472/83683744 [==============================] - 1s 0us/step\n83697664/83683744 [==============================] - 1s 0us/step\n\n\n\nfor index, layer in enumerate(base_model.layers):\n    print(index, layer.name)\n\n0 input_2\n1 block1_conv1\n2 block1_conv1_bn\n3 block1_conv1_act\n4 block1_conv2\n5 block1_conv2_bn\n6 block1_conv2_act\n7 block2_sepconv1\n8 block2_sepconv1_bn\n9 block2_sepconv2_act\n10 block2_sepconv2\n11 block2_sepconv2_bn\n12 conv2d_44\n13 block2_pool\n14 batch_normalization_36\n15 add\n16 block3_sepconv1_act\n17 block3_sepconv1\n18 block3_sepconv1_bn\n19 block3_sepconv2_act\n20 block3_sepconv2\n21 block3_sepconv2_bn\n22 conv2d_45\n23 block3_pool\n24 batch_normalization_37\n25 add_1\n26 block4_sepconv1_act\n27 block4_sepconv1\n28 block4_sepconv1_bn\n29 block4_sepconv2_act\n30 block4_sepconv2\n31 block4_sepconv2_bn\n32 conv2d_46\n33 block4_pool\n34 batch_normalization_38\n35 add_2\n36 block5_sepconv1_act\n37 block5_sepconv1\n38 block5_sepconv1_bn\n39 block5_sepconv2_act\n40 block5_sepconv2\n41 block5_sepconv2_bn\n42 block5_sepconv3_act\n43 block5_sepconv3\n44 block5_sepconv3_bn\n45 add_3\n46 block6_sepconv1_act\n47 block6_sepconv1\n48 block6_sepconv1_bn\n49 block6_sepconv2_act\n50 block6_sepconv2\n51 block6_sepconv2_bn\n52 block6_sepconv3_act\n53 block6_sepconv3\n54 block6_sepconv3_bn\n55 add_4\n56 block7_sepconv1_act\n57 block7_sepconv1\n58 block7_sepconv1_bn\n59 block7_sepconv2_act\n60 block7_sepconv2\n61 block7_sepconv2_bn\n62 block7_sepconv3_act\n63 block7_sepconv3\n64 block7_sepconv3_bn\n65 add_5\n66 block8_sepconv1_act\n67 block8_sepconv1\n68 block8_sepconv1_bn\n69 block8_sepconv2_act\n70 block8_sepconv2\n71 block8_sepconv2_bn\n72 block8_sepconv3_act\n73 block8_sepconv3\n74 block8_sepconv3_bn\n75 add_6\n76 block9_sepconv1_act\n77 block9_sepconv1\n78 block9_sepconv1_bn\n79 block9_sepconv2_act\n80 block9_sepconv2\n81 block9_sepconv2_bn\n82 block9_sepconv3_act\n83 block9_sepconv3\n84 block9_sepconv3_bn\n85 add_7\n86 block10_sepconv1_act\n87 block10_sepconv1\n88 block10_sepconv1_bn\n89 block10_sepconv2_act\n90 block10_sepconv2\n91 block10_sepconv2_bn\n92 block10_sepconv3_act\n93 block10_sepconv3\n94 block10_sepconv3_bn\n95 add_8\n96 block11_sepconv1_act\n97 block11_sepconv1\n98 block11_sepconv1_bn\n99 block11_sepconv2_act\n100 block11_sepconv2\n101 block11_sepconv2_bn\n102 block11_sepconv3_act\n103 block11_sepconv3\n104 block11_sepconv3_bn\n105 add_9\n106 block12_sepconv1_act\n107 block12_sepconv1\n108 block12_sepconv1_bn\n109 block12_sepconv2_act\n110 block12_sepconv2\n111 block12_sepconv2_bn\n112 block12_sepconv3_act\n113 block12_sepconv3\n114 block12_sepconv3_bn\n115 add_10\n116 block13_sepconv1_act\n117 block13_sepconv1\n118 block13_sepconv1_bn\n119 block13_sepconv2_act\n120 block13_sepconv2\n121 block13_sepconv2_bn\n122 conv2d_47\n123 block13_pool\n124 batch_normalization_39\n125 add_11\n126 block14_sepconv1\n127 block14_sepconv1_bn\n128 block14_sepconv1_act\n129 block14_sepconv2\n130 block14_sepconv2_bn\n131 block14_sepconv2_act\n\n\n\nfor layer in base_model.layers:\n    layer.trainable = False\n\noptimizer = keras.optimizers.SGD(learning_rate=0.2, momentum=0.9, decay=0.01)\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n              metrics=[\"accuracy\"])\nhistory = model.fit(train_set,\n                    steps_per_epoch=int(0.75 * dataset_size / batch_size),\n                    validation_data=valid_set,\n                    validation_steps=int(0.15 * dataset_size / batch_size),\n                    epochs=5)\n\nEpoch 1/5\n86/86 [==============================] - 19s 166ms/step - loss: 1.4720 - accuracy: 0.7925 - val_loss: 1.4752 - val_accuracy: 0.8070\nEpoch 2/5\n86/86 [==============================] - 14s 161ms/step - loss: 0.6576 - accuracy: 0.8914 - val_loss: 0.9815 - val_accuracy: 0.8732\nEpoch 3/5\n86/86 [==============================] - 14s 163ms/step - loss: 0.3623 - accuracy: 0.9248 - val_loss: 0.8554 - val_accuracy: 0.8732\nEpoch 4/5\n86/86 [==============================] - 14s 165ms/step - loss: 0.2582 - accuracy: 0.9335 - val_loss: 0.7072 - val_accuracy: 0.8842\nEpoch 5/5\n86/86 [==============================] - 14s 167ms/step - loss: 0.2029 - accuracy: 0.9440 - val_loss: 0.7266 - val_accuracy: 0.8732\n\n\n/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n\n\n\nfor layer in base_model.layers:\n    layer.trainable = True\n\noptimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9,\n                                 nesterov=True, decay=0.001)\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n              metrics=[\"accuracy\"])\nhistory = model.fit(train_set,\n                    steps_per_epoch=int(0.75 * dataset_size / batch_size),\n                    validation_data=valid_set,\n                    validation_steps=int(0.15 * dataset_size / batch_size),\n                    epochs=40)\n\nEpoch 1/40\n86/86 [==============================] - 61s 654ms/step - loss: 0.3505 - accuracy: 0.8881 - val_loss: 0.5942 - val_accuracy: 0.8346\nEpoch 2/40\n86/86 [==============================] - 56s 653ms/step - loss: 0.1274 - accuracy: 0.9622 - val_loss: 0.3924 - val_accuracy: 0.8713\nEpoch 3/40\n86/86 [==============================] - 57s 663ms/step - loss: 0.0621 - accuracy: 0.9800 - val_loss: 0.3522 - val_accuracy: 0.8952\nEpoch 4/40\n86/86 [==============================] - 57s 669ms/step - loss: 0.0572 - accuracy: 0.9855 - val_loss: 0.2722 - val_accuracy: 0.9228\nEpoch 5/40\n86/86 [==============================] - 57s 667ms/step - loss: 0.0347 - accuracy: 0.9866 - val_loss: 0.2827 - val_accuracy: 0.9283\nEpoch 6/40\n86/86 [==============================] - 57s 667ms/step - loss: 0.0209 - accuracy: 0.9935 - val_loss: 0.3221 - val_accuracy: 0.9191\nEpoch 7/40\n86/86 [==============================] - 58s 672ms/step - loss: 0.0152 - accuracy: 0.9942 - val_loss: 0.2947 - val_accuracy: 0.9265\nEpoch 8/40\n86/86 [==============================] - 57s 668ms/step - loss: 0.0323 - accuracy: 0.9913 - val_loss: 0.2998 - val_accuracy: 0.9191\nEpoch 9/40\n86/86 [==============================] - 57s 668ms/step - loss: 0.0259 - accuracy: 0.9920 - val_loss: 0.2959 - val_accuracy: 0.9210\nEpoch 10/40\n86/86 [==============================] - 57s 669ms/step - loss: 0.0154 - accuracy: 0.9956 - val_loss: 0.2782 - val_accuracy: 0.9283\nEpoch 11/40\n86/86 [==============================] - 58s 669ms/step - loss: 0.0065 - accuracy: 0.9982 - val_loss: 0.2862 - val_accuracy: 0.9265\nEpoch 12/40\n86/86 [==============================] - 57s 668ms/step - loss: 0.0147 - accuracy: 0.9956 - val_loss: 0.3143 - val_accuracy: 0.9283\nEpoch 13/40\n86/86 [==============================] - 58s 672ms/step - loss: 0.0114 - accuracy: 0.9953 - val_loss: 0.2800 - val_accuracy: 0.9301\nEpoch 14/40\n86/86 [==============================] - 58s 669ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.3000 - val_accuracy: 0.9283\nEpoch 15/40\n86/86 [==============================] - 57s 668ms/step - loss: 0.0054 - accuracy: 0.9989 - val_loss: 0.2987 - val_accuracy: 0.9320\nEpoch 16/40\n86/86 [==============================] - 58s 670ms/step - loss: 0.0052 - accuracy: 0.9982 - val_loss: 0.3017 - val_accuracy: 0.9301\nEpoch 17/40\n86/86 [==============================] - 57s 667ms/step - loss: 0.0080 - accuracy: 0.9967 - val_loss: 0.3127 - val_accuracy: 0.9301\nEpoch 18/40\n86/86 [==============================] - 58s 670ms/step - loss: 0.0103 - accuracy: 0.9964 - val_loss: 0.3017 - val_accuracy: 0.9283\nEpoch 19/40\n86/86 [==============================] - 57s 669ms/step - loss: 0.0059 - accuracy: 0.9982 - val_loss: 0.2901 - val_accuracy: 0.9320\nEpoch 20/40\n86/86 [==============================] - 57s 669ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.2862 - val_accuracy: 0.9338\nEpoch 21/40\n86/86 [==============================] - 58s 669ms/step - loss: 0.0034 - accuracy: 0.9985 - val_loss: 0.2909 - val_accuracy: 0.9301\nEpoch 22/40\n86/86 [==============================] - 57s 667ms/step - loss: 0.0064 - accuracy: 0.9975 - val_loss: 0.3024 - val_accuracy: 0.9320\nEpoch 23/40\n86/86 [==============================] - 58s 670ms/step - loss: 0.0044 - accuracy: 0.9985 - val_loss: 0.3059 - val_accuracy: 0.9301\nEpoch 24/40\n86/86 [==============================] - 58s 669ms/step - loss: 0.0055 - accuracy: 0.9978 - val_loss: 0.3201 - val_accuracy: 0.9265\nEpoch 25/40\n86/86 [==============================] - 58s 671ms/step - loss: 0.0059 - accuracy: 0.9971 - val_loss: 0.3059 - val_accuracy: 0.9320\nEpoch 26/40\n86/86 [==============================] - 57s 668ms/step - loss: 0.0047 - accuracy: 0.9985 - val_loss: 0.3042 - val_accuracy: 0.9283\nEpoch 27/40\n86/86 [==============================] - 57s 669ms/step - loss: 0.0028 - accuracy: 0.9989 - val_loss: 0.3020 - val_accuracy: 0.9283\nEpoch 28/40\n86/86 [==============================] - 57s 666ms/step - loss: 0.0036 - accuracy: 0.9985 - val_loss: 0.3165 - val_accuracy: 0.9283\nEpoch 29/40\n86/86 [==============================] - 57s 668ms/step - loss: 0.0042 - accuracy: 0.9982 - val_loss: 0.3185 - val_accuracy: 0.9265\nEpoch 30/40\n86/86 [==============================] - 57s 667ms/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.3248 - val_accuracy: 0.9301\nEpoch 31/40\n86/86 [==============================] - 57s 668ms/step - loss: 0.0053 - accuracy: 0.9982 - val_loss: 0.3121 - val_accuracy: 0.9320\nEpoch 32/40\n86/86 [==============================] - 57s 665ms/step - loss: 0.0062 - accuracy: 0.9989 - val_loss: 0.3128 - val_accuracy: 0.9301\nEpoch 33/40\n86/86 [==============================] - 57s 666ms/step - loss: 0.0031 - accuracy: 0.9985 - val_loss: 0.3130 - val_accuracy: 0.9265\nEpoch 34/40\n86/86 [==============================] - 57s 669ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.3077 - val_accuracy: 0.9283\nEpoch 35/40\n86/86 [==============================] - 57s 666ms/step - loss: 0.0015 - accuracy: 0.9993 - val_loss: 0.3113 - val_accuracy: 0.9301\nEpoch 36/40\n86/86 [==============================] - 57s 669ms/step - loss: 0.0035 - accuracy: 0.9982 - val_loss: 0.3278 - val_accuracy: 0.9320\nEpoch 37/40\n86/86 [==============================] - 57s 664ms/step - loss: 0.0032 - accuracy: 0.9982 - val_loss: 0.3225 - val_accuracy: 0.9283\nEpoch 38/40\n86/86 [==============================] - 57s 665ms/step - loss: 0.0037 - accuracy: 0.9982 - val_loss: 0.3203 - val_accuracy: 0.9265\nEpoch 39/40\n86/86 [==============================] - 57s 667ms/step - loss: 0.0022 - accuracy: 0.9989 - val_loss: 0.3246 - val_accuracy: 0.9246\nEpoch 40/40\n86/86 [==============================] - 57s 669ms/step - loss: 0.0022 - accuracy: 0.9989 - val_loss: 0.3255 - val_accuracy: 0.9246"
  },
  {
    "objectID": "Machine_Learning/14_deep_computer_vision_with_cnns.html#mapmean-average-precision",
    "href": "Machine_Learning/14_deep_computer_vision_with_cnns.html#mapmean-average-precision",
    "title": "14_deep_computer_vision_with_cnns",
    "section": "mAP(Mean Average Precision)",
    "text": "mAP(Mean Average Precision)\n\ndef maximum_precisions(precisions):\n    return np.flip(np.maximum.accumulate(np.flip(precisions)))\n\n\nrecalls = np.linspace(0, 1, 11)\n\nprecisions = [0.91, 0.94, 0.96, 0.94, 0.95, 0.92, 0.80, 0.60, 0.45, 0.20, 0.10]\nmax_precisions = maximum_precisions(precisions)\nmAP = max_precisions.mean()\nplt.plot(recalls, precisions, \"ro--\", label=\"Precision\")\nplt.plot(recalls, max_precisions, \"bo-\", label=\"Max Precision\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.plot([0, 1], [mAP, mAP], \"g:\", linewidth=3, label=\"mAP\")\nplt.grid(True)\nplt.axis([0, 1, 0, 1])\nplt.legend(loc=\"lower center\", fontsize=14)\nplt.show()\n\n\n\n\n전치 합성곱:\n\ntf.random.set_seed(42)\nX = images_resized.numpy()\n\nconv_transpose = keras.layers.Conv2DTranspose(filters=5, kernel_size=3, strides=2, padding=\"VALID\")\noutput = conv_transpose(X)\noutput.shape\n\nTensorShape([2, 449, 449, 5])\n\n\n\ndef normalize(X):\n    return (X - tf.reduce_min(X)) / (tf.reduce_max(X) - tf.reduce_min(X))\n\nfig = plt.figure(figsize=(12, 8))\ngs = mpl.gridspec.GridSpec(nrows=1, ncols=2, width_ratios=[1, 2])\n\nax1 = fig.add_subplot(gs[0, 0])\nax1.set_title(\"Input\", fontsize=14)\nax1.imshow(X[0])  # 첫 번째 이미지 그리기\nax1.axis(\"off\")\nax2 = fig.add_subplot(gs[0, 1])\nax2.set_title(\"Output\", fontsize=14)\nax2.imshow(normalize(output[0, ..., :3]), interpolation=\"bicubic\")  # 첫 번째 이미지 출력 그리기\nax2.axis(\"off\")\nplt.show()\n\n\n\n\n\ndef upscale_images(images, stride, kernel_size):\n    batch_size, height, width, channels = images.shape\n    upscaled = np.zeros((batch_size,\n                         (height - 1) * stride + 2 * kernel_size - 1,\n                         (width - 1) * stride + 2 * kernel_size - 1,\n                         channels))\n    upscaled[:,\n             kernel_size - 1:(height - 1) * stride + kernel_size:stride,\n             kernel_size - 1:(width - 1) * stride + kernel_size:stride,\n             :] = images\n    return upscaled\n\n\nupscaled = upscale_images(X, stride=2, kernel_size=3)\nweights, biases = conv_transpose.weights\nreversed_filters = np.flip(weights.numpy(), axis=[0, 1])\nreversed_filters = np.transpose(reversed_filters, [0, 1, 3, 2])\nmanual_output = tf.nn.conv2d(upscaled, reversed_filters, strides=1, padding=\"VALID\")\n\n\ndef normalize(X):\n    return (X - tf.reduce_min(X)) / (tf.reduce_max(X) - tf.reduce_min(X))\n\nfig = plt.figure(figsize=(12, 8))\ngs = mpl.gridspec.GridSpec(nrows=1, ncols=3, width_ratios=[1, 2, 2])\n\nax1 = fig.add_subplot(gs[0, 0])\nax1.set_title(\"Input\", fontsize=14)\nax1.imshow(X[0])  # 첫 번째 이미지 그리기\nax1.axis(\"off\")\nax2 = fig.add_subplot(gs[0, 1])\nax2.set_title(\"Upscaled\", fontsize=14)\nax2.imshow(upscaled[0], interpolation=\"bicubic\")\nax2.axis(\"off\")\nax3 = fig.add_subplot(gs[0, 2])\nax3.set_title(\"Output\", fontsize=14)\nax3.imshow(normalize(manual_output[0, ..., :3]), interpolation=\"bicubic\")  # 첫 번째 이미지 출력 그리기\nax3.axis(\"off\")\nplt.show()\n\n\n\n\n\nnp.allclose(output, manual_output.numpy(), atol=1e-7)\n\nTrue"
  },
  {
    "objectID": "Machine_Learning/14_deep_computer_vision_with_cnns.html#to-8.",
    "href": "Machine_Learning/14_deep_computer_vision_with_cnns.html#to-8.",
    "title": "14_deep_computer_vision_with_cnns",
    "section": "1. to 8.",
    "text": "1. to 8.\n부록 A 참조."
  },
  {
    "objectID": "Machine_Learning/14_deep_computer_vision_with_cnns.html#mnist에서-높은-정확도를-내는-cnn-만들기",
    "href": "Machine_Learning/14_deep_computer_vision_with_cnns.html#mnist에서-높은-정확도를-내는-cnn-만들기",
    "title": "14_deep_computer_vision_with_cnns",
    "section": "9. MNIST에서 높은 정확도를 내는 CNN 만들기",
    "text": "9. MNIST에서 높은 정확도를 내는 CNN 만들기\n연습문제: 자신만의 CNN을 만들고 MNIST 데이터셋에서 가능한 최대 정확도를 달성해보세요.\n다음 모델은 2개의 합성곱 층과 1개의 풀링 층을 사용합니다. 그다음 25% 드롭아웃하고 이어서 밀집 층을 놓고 50% 드롭아웃을 다시 적용합니다. 마지막에 출력층을 놓습니다. 이 모델은 테스트 세트에서 약 99.2% 정확도를 냅니다. 이 모델은 MNIST 캐글 경연대회에서 상위 20위 안에 포함되는 수준입니다(Chris Deotte가 이 포스트에서 설명했듯이 테스트 세트에서 훈련된 것 같은 99.79%보다 높은 정확도를 가진 모델은 무시합니다). 더 높일 수 있을까요? 테스트 세트에서 99.5~99.7% 사이의 정확도를 달성하려면 이미지 증식, 배치 정규화를 추가하고 1-cycle 같은 학습률 스케줄을 사용하고 앙상블 모델을 만들어야 합니다.\n\n(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\nX_train_full = X_train_full / 255.\nX_test = X_test / 255.\nX_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\ny_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n\nX_train = X_train[..., np.newaxis]\nX_valid = X_valid[..., np.newaxis]\nX_test = X_test[..., np.newaxis]\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n11493376/11490434 [==============================] - 0s 0us/step\n11501568/11490434 [==============================] - 0s 0us/step\n\n\n\nkeras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nmodel = keras.models.Sequential([\n    keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n    keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n    keras.layers.MaxPool2D(),\n    keras.layers.Flatten(),\n    keras.layers.Dropout(0.25),\n    keras.layers.Dense(128, activation=\"relu\"),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n              metrics=[\"accuracy\"])\n\nmodel.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\nmodel.evaluate(X_test, y_test)\n\nEpoch 1/10\n1719/1719 [==============================] - 10s 5ms/step - loss: 0.1973 - accuracy: 0.9403 - val_loss: 0.0445 - val_accuracy: 0.9882\nEpoch 2/10\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.0800 - accuracy: 0.9765 - val_loss: 0.0409 - val_accuracy: 0.9888\nEpoch 3/10\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.0609 - accuracy: 0.9814 - val_loss: 0.0330 - val_accuracy: 0.9912\nEpoch 4/10\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.0498 - accuracy: 0.9847 - val_loss: 0.0348 - val_accuracy: 0.9910\nEpoch 5/10\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.0408 - accuracy: 0.9868 - val_loss: 0.0361 - val_accuracy: 0.9902\nEpoch 6/10\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.0367 - accuracy: 0.9881 - val_loss: 0.0345 - val_accuracy: 0.9920\nEpoch 7/10\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.0328 - accuracy: 0.9893 - val_loss: 0.0372 - val_accuracy: 0.9906\nEpoch 8/10\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.0284 - accuracy: 0.9909 - val_loss: 0.0344 - val_accuracy: 0.9914\nEpoch 9/10\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.0276 - accuracy: 0.9915 - val_loss: 0.0368 - val_accuracy: 0.9918\nEpoch 10/10\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.0234 - accuracy: 0.9922 - val_loss: 0.0393 - val_accuracy: 0.9922\n313/313 [==============================] - 1s 2ms/step - loss: 0.0293 - accuracy: 0.9925\n\n\n[0.029317684471607208, 0.9925000071525574]"
  },
  {
    "objectID": "Machine_Learning/14_deep_computer_vision_with_cnns.html#전이-학습을-사용한-대규모-이미지-분류",
    "href": "Machine_Learning/14_deep_computer_vision_with_cnns.html#전이-학습을-사용한-대규모-이미지-분류",
    "title": "14_deep_computer_vision_with_cnns",
    "section": "10. 전이 학습을 사용한 대규모 이미지 분류",
    "text": "10. 전이 학습을 사용한 대규모 이미지 분류\n연습문제: 다음 단계를 따라 전이 학습을 사용해 대규모 이미지 분류를 수행해보세요:\n\n클래스마다 최소한 100개의 이미지가 들어 있는 훈련 세트를 만드세요. 예를 들어 위 치에 따라(해변, 산, 도심 등) 자신의 사진을 분류하거나, 기존의 데이터셋(예를 들 면, 텐서플로 데이터셋)을 사용할 수도 있습니다.\n이를 훈련 세트와 검증 세트, 테스트 세트로 나눕니다.\n적절한 전처리 연산과 선택적으로 데이터 증식을 수행하는 입력 파이프라인을 만듭 니다.\n이 데이터셋에서 사전훈련된 모델을 세부 튜닝합니다.\n\nFlowers 예제를 참고하세요."
  },
  {
    "objectID": "Machine_Learning/14_deep_computer_vision_with_cnns.html#section",
    "href": "Machine_Learning/14_deep_computer_vision_with_cnns.html#section",
    "title": "14_deep_computer_vision_with_cnns",
    "section": "11.",
    "text": "11.\n연습문제: 텐서플로의 스타일 전이 튜토리얼을 살펴보세요. 딥러 닝을 사용해 재미있는 그림을 생성할 수 있습니다.\n코랩을 사용해 튜토리얼을 따라해 보세요."
  },
  {
    "objectID": "Machine_Learning/16_nlp_with_rnns_and_attention.html",
    "href": "Machine_Learning/16_nlp_with_rnns_and_attention.html",
    "title": "16_nlp_with_rnns_and_attention",
    "section": "",
    "text": "16장 – RNN과 어텐션을 사용한 자연어 처리\n이 노트북은 16장에 있는 모든 샘플 코드를 담고 있습니다."
  },
  {
    "objectID": "Machine_Learning/16_nlp_with_rnns_and_attention.html#시퀀스를-셔플-윈도우-배치로-나누기",
    "href": "Machine_Learning/16_nlp_with_rnns_and_attention.html#시퀀스를-셔플-윈도우-배치로-나누기",
    "title": "16_nlp_with_rnns_and_attention",
    "section": "시퀀스를 셔플 윈도우 배치로 나누기",
    "text": "시퀀스를 셔플 윈도우 배치로 나누기\n예를 들어, 0~14까지 시퀀스를 2개씩 이동하면서 길이가 5인 윈도우로 나누어 보죠(가령,[0, 1, 2, 3, 4], [2, 3, 4, 5, 6], 등). 그다음 이를 섞고 입력(처음 네 개의 스텝)과 타깃(마지막 네 개의 스텝)으로 나눕니다(즉, [2, 3, 4, 5, 6]를 [[2, 3, 4, 5], [3, 4, 5, 6]]로 나눕니다). 그다음 입력/타깃 쌍 세 개로 구성된 배치를 만듭니다:\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nn_steps = 5\ndataset = tf.data.Dataset.from_tensor_slices(tf.range(15))\ndataset = dataset.window(n_steps, shift=2, drop_remainder=True)\ndataset = dataset.flat_map(lambda window: window.batch(n_steps))\ndataset = dataset.shuffle(10).map(lambda window: (window[:-1], window[1:]))\ndataset = dataset.batch(3).prefetch(1)\nfor index, (X_batch, Y_batch) in enumerate(dataset):\n    print(\"_\" * 20, \"Batch\", index, \"\\nX_batch\")\n    print(X_batch.numpy())\n    print(\"=\" * 5, \"\\nY_batch\")\n    print(Y_batch.numpy())\n\n____________________ Batch 0 \nX_batch\n[[6 7 8 9]\n [2 3 4 5]\n [4 5 6 7]]\n===== \nY_batch\n[[ 7  8  9 10]\n [ 3  4  5  6]\n [ 5  6  7  8]]\n____________________ Batch 1 \nX_batch\n[[ 0  1  2  3]\n [ 8  9 10 11]\n [10 11 12 13]]\n===== \nY_batch\n[[ 1  2  3  4]\n [ 9 10 11 12]\n [11 12 13 14]]"
  },
  {
    "objectID": "Machine_Learning/16_nlp_with_rnns_and_attention.html#데이터-로드하고-데이터셋-준비하기",
    "href": "Machine_Learning/16_nlp_with_rnns_and_attention.html#데이터-로드하고-데이터셋-준비하기",
    "title": "16_nlp_with_rnns_and_attention",
    "section": "데이터 로드하고 데이터셋 준비하기",
    "text": "데이터 로드하고 데이터셋 준비하기\n\nshakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\nfilepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\nwith open(filepath) as f:\n    shakespeare_text = f.read()\n\nDownloading data from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n1122304/1115394 [==============================] - 0s 0us/step\n1130496/1115394 [==============================] - 0s 0us/step\n\n\n\nprint(shakespeare_text[:148])\n\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\n\n\n\n\"\".join(sorted(set(shakespeare_text.lower())))\n\n\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\"\n\n\n\ntokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\ntokenizer.fit_on_texts(shakespeare_text)\n\n\ntokenizer.texts_to_sequences([\"First\"])\n\n[[20, 6, 9, 8, 3]]\n\n\n\ntokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])\n\n['f i r s t']\n\n\n\nmax_id = len(tokenizer.word_index) # 고유한 문자 개수\ndataset_size = tokenizer.document_count # 전체 문자 개수\n\n\n[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\ntrain_size = dataset_size * 90 // 100\ndataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n\n노트: 예전 코드에서는 dataset.repeat()를 사용해 데이터셋을 무한하게 반복할 수 있게 만들고 나중에 model.fit() 메서드를 호출할 때 steps_per_epoch 매개변수를 지정했습니다. 텐서플로 버그 때문에 이렇게 해야 했지만 이제는 수정되었기 때문에 코드를 간단하게 만들 수 있습니다. dataset.repeat()와 steps_per_epoch가 더 이상 필요하지 않습니다.\n\nn_steps = 100\nwindow_length = n_steps + 1 # 타깃 = 한 글자 앞선 입력\ndataset = dataset.window(window_length, shift=1, drop_remainder=True)\n\n\ndataset = dataset.flat_map(lambda window: window.batch(window_length))\n\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n\nbatch_size = 32\ndataset = dataset.shuffle(10000).batch(batch_size)\ndataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n\n\ndataset = dataset.map(\n    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n\n\ndataset = dataset.prefetch(1)\n\n\nfor X_batch, Y_batch in dataset.take(1):\n    print(X_batch.shape, Y_batch.shape)\n\n(32, 100, 39) (32, 100)"
  },
  {
    "objectID": "Machine_Learning/16_nlp_with_rnns_and_attention.html#모델-만들고-훈련하기",
    "href": "Machine_Learning/16_nlp_with_rnns_and_attention.html#모델-만들고-훈련하기",
    "title": "16_nlp_with_rnns_and_attention",
    "section": "모델 만들고 훈련하기",
    "text": "모델 만들고 훈련하기\n경고: 다음 코드는 하드웨어에 따라 실행하는데 24시간이 걸릴 수 있습니다. GPU를 사용하면 1~2시간 정도 걸릴 수 있습니다.\n노트: GRU 클래스는 다음 매개변수에서 기본값을 사용할 때에만 GPU를 사용합니다: activation, recurrent_activation, recurrent_dropout, unroll, use_bias reset_after. 이 때문에 (책과는 달리) recurrent_dropout=0.2를 주석 처리했습니다.\n\nmodel = keras.models.Sequential([\n    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n                     #dropout=0.2, recurrent_dropout=0.2),\n                     dropout=0.2),\n    keras.layers.GRU(128, return_sequences=True,\n                     #dropout=0.2, recurrent_dropout=0.2),\n                     dropout=0.2),\n    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n                                                    activation=\"softmax\"))\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\nhistory = model.fit(dataset, epochs=10)\n\nEpoch 1/10\n31368/31368 [==============================] - 376s 12ms/step - loss: 1.6206\nEpoch 2/10\n31368/31368 [==============================] - 351s 11ms/step - loss: 1.5369\nEpoch 3/10\n31368/31368 [==============================] - 347s 11ms/step - loss: 1.5171\nEpoch 4/10\n31368/31368 [==============================] - 344s 11ms/step - loss: 1.5053\nEpoch 5/10\n31368/31368 [==============================] - 346s 11ms/step - loss: 1.4980\nEpoch 6/10\n31368/31368 [==============================] - 344s 11ms/step - loss: 1.4927\nEpoch 7/10\n31368/31368 [==============================] - 344s 11ms/step - loss: 1.4891\nEpoch 8/10\n31368/31368 [==============================] - 347s 11ms/step - loss: 1.4864\nEpoch 9/10\n31368/31368 [==============================] - 345s 11ms/step - loss: 1.4842\nEpoch 10/10\n31368/31368 [==============================] - 346s 11ms/step - loss: 1.4821"
  },
  {
    "objectID": "Machine_Learning/16_nlp_with_rnns_and_attention.html#모델로-텍스트-생성하기",
    "href": "Machine_Learning/16_nlp_with_rnns_and_attention.html#모델로-텍스트-생성하기",
    "title": "16_nlp_with_rnns_and_attention",
    "section": "모델로 텍스트 생성하기",
    "text": "모델로 텍스트 생성하기\n\ndef preprocess(texts):\n    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n    return tf.one_hot(X, max_id)\n\n경고: predict_classes() 메서드는 deprecated 되었습니다. 대신 np.argmax(model(X_new), axis=-1)를 사용합니다.\n\nX_new = preprocess([\"How are yo\"])\n#Y_pred = model.predict_classes(X_new)\nY_pred = np.argmax(model(X_new), axis=-1)\ntokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence, last char\n\n'u'\n\n\n\ntf.random.set_seed(42)\n\ntf.random.categorical([[np.log(0.5), np.log(0.4), np.log(0.1)]], num_samples=40).numpy()\n\narray([[0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n        2, 0, 0, 1, 1, 1, 0, 0, 1, 2, 0, 0, 1, 1, 0, 0, 0, 0]])\n\n\n\ndef next_char(text, temperature=1):\n    X_new = preprocess([text])\n    y_proba = model(X_new)[0, -1:, :]\n    rescaled_logits = tf.math.log(y_proba) / temperature\n    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n\n\ntf.random.set_seed(42)\n\nnext_char(\"How are yo\", temperature=1)\n\n'u'\n\n\n\ndef complete_text(text, n_chars=50, temperature=1):\n    for _ in range(n_chars):\n        text += next_char(text, temperature)\n    return text\n\n\ntf.random.set_seed(42)\n\nprint(complete_text(\"t\", temperature=0.2))\n\nthe maid in padua for my father is a stood\nand so m\n\n\n\nprint(complete_text(\"t\", temperature=1))\n\ntoke on advised in sobel countryman,\nand signior gr\n\n\n\nprint(complete_text(\"t\", temperature=2))\n\ntpeniomently!\nwell maze: yet 'pale deficuruli-faeem"
  },
  {
    "objectID": "Machine_Learning/16_nlp_with_rnns_and_attention.html#상태가-있는-rnn",
    "href": "Machine_Learning/16_nlp_with_rnns_and_attention.html#상태가-있는-rnn",
    "title": "16_nlp_with_rnns_and_attention",
    "section": "상태가 있는 RNN",
    "text": "상태가 있는 RNN\n\ntf.random.set_seed(42)\n\n\ndataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\ndataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\ndataset = dataset.flat_map(lambda window: window.batch(window_length))\ndataset = dataset.batch(1)\ndataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\ndataset = dataset.map(\n    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\ndataset = dataset.prefetch(1)\n\n\nbatch_size = 32\nencoded_parts = np.array_split(encoded[:train_size], batch_size)\ndatasets = []\nfor encoded_part in encoded_parts:\n    dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n    dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n    datasets.append(dataset)\ndataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\ndataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\ndataset = dataset.map(\n    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\ndataset = dataset.prefetch(1)\n\n노트: 여기에서도 GPU 가속을 위해 (책과 달리) recurrent_dropout=0.2을 주석 처리합니다.\n\nmodel = keras.models.Sequential([\n    keras.layers.GRU(128, return_sequences=True, stateful=True,\n                     #dropout=0.2, recurrent_dropout=0.2,\n                     dropout=0.2,\n                     batch_input_shape=[batch_size, None, max_id]),\n    keras.layers.GRU(128, return_sequences=True, stateful=True,\n                     #dropout=0.2, recurrent_dropout=0.2),\n                     dropout=0.2),\n    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n                                                    activation=\"softmax\"))\n])\n\n\nclass ResetStatesCallback(keras.callbacks.Callback):\n    def on_epoch_begin(self, epoch, logs):\n        self.model.reset_states()\n\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\nhistory = model.fit(dataset, epochs=50,\n                    callbacks=[ResetStatesCallback()])\n\nEpoch 1/50\n313/313 [==============================] - 6s 12ms/step - loss: 2.6200\nEpoch 2/50\n313/313 [==============================] - 4s 12ms/step - loss: 2.2410\nEpoch 3/50\n313/313 [==============================] - 4s 12ms/step - loss: 2.1105\nEpoch 4/50\n313/313 [==============================] - 4s 12ms/step - loss: 2.0368\nEpoch 5/50\n313/313 [==============================] - 4s 12ms/step - loss: 1.9860\nEpoch 6/50\n313/313 [==============================] - 4s 12ms/step - loss: 1.9488\nEpoch 7/50\n313/313 [==============================] - 4s 12ms/step - loss: 1.9205\nEpoch 8/50\n313/313 [==============================] - 4s 12ms/step - loss: 1.8985\nEpoch 9/50\n313/313 [==============================] - 4s 12ms/step - loss: 1.8797\nEpoch 10/50\n313/313 [==============================] - 4s 12ms/step - loss: 1.8655\nEpoch 11/50\n313/313 [==============================] - 4s 12ms/step - loss: 1.8533\nEpoch 12/50\n313/313 [==============================] - 4s 12ms/step - loss: 1.8412\nEpoch 13/50\n313/313 [==============================] - 4s 11ms/step - loss: 1.8328\nEpoch 14/50\n313/313 [==============================] - 4s 12ms/step - loss: 1.8233\nEpoch 15/50\n313/313 [==============================] - 4s 11ms/step - loss: 1.8160\nEpoch 16/50\n313/313 [==============================] - 4s 11ms/step - loss: 1.8072\nEpoch 17/50\n313/313 [==============================] - 4s 11ms/step - loss: 1.8008\nEpoch 18/50\n313/313 [==============================] - 4s 11ms/step - loss: 1.7936\nEpoch 19/50\n313/313 [==============================] - 4s 12ms/step - loss: 1.7885\nEpoch 20/50\n313/313 [==============================] - 4s 11ms/step - loss: 1.7851\nEpoch 21/50\n313/313 [==============================] - 4s 12ms/step - loss: 1.7814\nEpoch 22/50\n313/313 [==============================] - 4s 11ms/step - loss: 1.7760\nEpoch 23/50\n313/313 [==============================] - 4s 12ms/step - loss: 1.7729\nEpoch 24/50\n313/313 [==============================] - 4s 11ms/step - loss: 1.7697\nEpoch 25/50\n313/313 [==============================] - 4s 11ms/step - loss: 1.7645\nEpoch 26/50\n313/313 [==============================] - 4s 11ms/step - loss: 1.7606\nEpoch 27/50\n313/313 [==============================] - 4s 12ms/step - loss: 1.7584\nEpoch 28/50\n313/313 [==============================] - 4s 11ms/step - loss: 1.7564\nEpoch 29/50\n313/313 [==============================] - 4s 11ms/step - loss: 1.7538\nEpoch 30/50\n313/313 [==============================] - 4s 12ms/step - loss: 1.7496\nEpoch 31/50\n313/313 [==============================] - 4s 12ms/step - loss: 1.7470\nEpoch 32/50\n313/313 [==============================] - 4s 11ms/step - loss: 1.7455\nEpoch 33/50\n313/313 [==============================] - 4s 12ms/step - loss: 1.7432\nEpoch 34/50\n313/313 [==============================] - 4s 11ms/step - loss: 1.7408\nEpoch 35/50\n313/313 [==============================] - 4s 12ms/step - loss: 1.7376\nEpoch 36/50\n313/313 [==============================] - 4s 11ms/step - loss: 1.7363\nEpoch 37/50\n313/313 [==============================] - 4s 11ms/step - loss: 1.7343\nEpoch 38/50\n313/313 [==============================] - 4s 12ms/step - loss: 1.7308\nEpoch 39/50\n313/313 [==============================] - 4s 12ms/step - loss: 1.7286\nEpoch 40/50\n313/313 [==============================] - 4s 12ms/step - loss: 1.7284\nEpoch 41/50\n313/313 [==============================] - 4s 12ms/step - loss: 1.7269\nEpoch 42/50\n313/313 [==============================] - 4s 12ms/step - loss: 1.7252\nEpoch 43/50\n313/313 [==============================] - 4s 12ms/step - loss: 1.7233\nEpoch 44/50\n313/313 [==============================] - 4s 12ms/step - loss: 1.7233\nEpoch 45/50\n313/313 [==============================] - 4s 12ms/step - loss: 1.7222\nEpoch 46/50\n313/313 [==============================] - 4s 11ms/step - loss: 1.7193\nEpoch 47/50\n313/313 [==============================] - 4s 12ms/step - loss: 1.7181\nEpoch 48/50\n313/313 [==============================] - 4s 12ms/step - loss: 1.7175\nEpoch 49/50\n313/313 [==============================] - 4s 12ms/step - loss: 1.7146\nEpoch 50/50\n313/313 [==============================] - 4s 12ms/step - loss: 1.7138\n\n\n모델에 다른 크기의 배치를 사용하려면 상태가 없는 복사본을 만들어야 합니다. 드롭아웃은 훈련에만 사용되기 때문에 삭제합니다:\n\nstateless_model = keras.models.Sequential([\n    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]),\n    keras.layers.GRU(128, return_sequences=True),\n    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n                                                    activation=\"softmax\"))\n])\n\n가중치를 복사하려면 먼저 (가중치를 만들기 위해) 모델을 빌드합니다:\n\nstateless_model.build(tf.TensorShape([None, None, max_id]))\n\n\nstateless_model.set_weights(model.get_weights())\nmodel = stateless_model\n\n\ntf.random.set_seed(42)\n\nprint(complete_text(\"t\"))\n\nthing idsumper your shint.\nwhy, he has go too stone"
  },
  {
    "objectID": "Machine_Learning/16_nlp_with_rnns_and_attention.html#사전-훈련된-임베딩-재사용하기",
    "href": "Machine_Learning/16_nlp_with_rnns_and_attention.html#사전-훈련된-임베딩-재사용하기",
    "title": "16_nlp_with_rnns_and_attention",
    "section": "사전 훈련된 임베딩 재사용하기",
    "text": "사전 훈련된 임베딩 재사용하기\n\ntf.random.set_seed(42)\n\n\nTFHUB_CACHE_DIR = os.path.join(os.curdir, \"my_tfhub_cache\")\nos.environ[\"TFHUB_CACHE_DIR\"] = TFHUB_CACHE_DIR\n\n\nimport tensorflow_hub as hub\n\nmodel = keras.Sequential([\n    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\",\n                   dtype=tf.string, input_shape=[], output_shape=[50]),\n    keras.layers.Dense(128, activation=\"relu\"),\n    keras.layers.Dense(1, activation=\"sigmoid\")\n])\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n              metrics=[\"accuracy\"])\n\n\nfor dirpath, dirnames, filenames in os.walk(TFHUB_CACHE_DIR):\n    for filename in filenames:\n        print(os.path.join(dirpath, filename))\n\n./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe.descriptor.txt\n./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/saved_model.pb\n./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/assets/tokens.txt\n./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/variables/variables.index\n./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/variables/variables.data-00000-of-00001\n\n\n\nimport tensorflow_datasets as tfds\n\ndatasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\ntrain_size = info.splits[\"train\"].num_examples\nbatch_size = 32\ntrain_set = datasets[\"train\"].batch(batch_size).prefetch(1)\nhistory = model.fit(train_set, epochs=5)\n\nEpoch 1/5\n782/782 [==============================] - 5s 5ms/step - loss: 0.5461 - accuracy: 0.7267\nEpoch 2/5\n782/782 [==============================] - 4s 5ms/step - loss: 0.5130 - accuracy: 0.7495\nEpoch 3/5\n782/782 [==============================] - 4s 5ms/step - loss: 0.5081 - accuracy: 0.7532\nEpoch 4/5\n782/782 [==============================] - 4s 5ms/step - loss: 0.5047 - accuracy: 0.7540\nEpoch 5/5\n782/782 [==============================] - 4s 5ms/step - loss: 0.5018 - accuracy: 0.7566"
  },
  {
    "objectID": "Machine_Learning/16_nlp_with_rnns_and_attention.html#자동-번역",
    "href": "Machine_Learning/16_nlp_with_rnns_and_attention.html#자동-번역",
    "title": "16_nlp_with_rnns_and_attention",
    "section": "자동 번역",
    "text": "자동 번역\n\ntf.random.set_seed(42)\n\n\nvocab_size = 100\nembed_size = 10\n\n\nimport tensorflow_addons as tfa\n\nencoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\ndecoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\nsequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n\nembeddings = keras.layers.Embedding(vocab_size, embed_size)\nencoder_embeddings = embeddings(encoder_inputs)\ndecoder_embeddings = embeddings(decoder_inputs)\n\nencoder = keras.layers.LSTM(512, return_state=True)\nencoder_outputs, state_h, state_c = encoder(encoder_embeddings)\nencoder_state = [state_h, state_c]\n\nsampler = tfa.seq2seq.sampler.TrainingSampler()\n\ndecoder_cell = keras.layers.LSTMCell(512)\noutput_layer = keras.layers.Dense(vocab_size)\ndecoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler,\n                                                 output_layer=output_layer)\nfinal_outputs, final_state, final_sequence_lengths = decoder(\n    decoder_embeddings, initial_state=encoder_state,\n    sequence_length=sequence_lengths)\nY_proba = tf.nn.softmax(final_outputs.rnn_output)\n\nmodel = keras.models.Model(\n    inputs=[encoder_inputs, decoder_inputs, sequence_lengths],\n    outputs=[Y_proba])\n\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n\n\nX = np.random.randint(100, size=10*1000).reshape(1000, 10)\nY = np.random.randint(100, size=15*1000).reshape(1000, 15)\nX_decoder = np.c_[np.zeros((1000, 1)), Y[:, :-1]]\nseq_lengths = np.full([1000], 15)\n\nhistory = model.fit([X, X_decoder, seq_lengths], Y, epochs=2)\n\nEpoch 1/2\n32/32 [==============================] - 4s 36ms/step - loss: 4.6054\nEpoch 2/2\n32/32 [==============================] - 1s 35ms/step - loss: 4.6031\n\n\n\n양방향 순환층\n\nmodel = keras.models.Sequential([\n    keras.layers.GRU(10, return_sequences=True, input_shape=[None, 10]),\n    keras.layers.Bidirectional(keras.layers.GRU(10, return_sequences=True))\n])\n\nmodel.summary()\n\nModel: \"sequential_5\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ngru_10 (GRU)                 (None, None, 10)          660       \n_________________________________________________________________\nbidirectional (Bidirectional (None, None, 20)          1320      \n=================================================================\nTotal params: 1,980\nTrainable params: 1,980\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\n위치 인코딩\n\nclass PositionalEncoding(keras.layers.Layer):\n    def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n        super().__init__(dtype=dtype, **kwargs)\n        if max_dims % 2 == 1: max_dims += 1 # max_dims must be even\n        p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2))\n        pos_emb = np.empty((1, max_steps, max_dims))\n        pos_emb[0, :, ::2] = np.sin(p / 10000**(2 * i / max_dims)).T\n        pos_emb[0, :, 1::2] = np.cos(p / 10000**(2 * i / max_dims)).T\n        self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n    def call(self, inputs):\n        shape = tf.shape(inputs)\n        return inputs + self.positional_embedding[:, :shape[-2], :shape[-1]]\n\n\nmax_steps = 201\nmax_dims = 512\npos_emb = PositionalEncoding(max_steps, max_dims)\nPE = pos_emb(np.zeros((1, max_steps, max_dims), np.float32))[0].numpy()\n\n\ni1, i2, crop_i = 100, 101, 150\np1, p2, p3 = 22, 60, 35\nfig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(9, 5))\nax1.plot([p1, p1], [-1, 1], \"k--\", label=\"$p = {}$\".format(p1))\nax1.plot([p2, p2], [-1, 1], \"k--\", label=\"$p = {}$\".format(p2), alpha=0.5)\nax1.plot(p3, PE[p3, i1], \"bx\", label=\"$p = {}$\".format(p3))\nax1.plot(PE[:,i1], \"b-\", label=\"$i = {}$\".format(i1))\nax1.plot(PE[:,i2], \"r-\", label=\"$i = {}$\".format(i2))\nax1.plot([p1, p2], [PE[p1, i1], PE[p2, i1]], \"bo\")\nax1.plot([p1, p2], [PE[p1, i2], PE[p2, i2]], \"ro\")\nax1.legend(loc=\"center right\", fontsize=14, framealpha=0.95)\nax1.set_ylabel(\"$P_{(p,i)}$\", rotation=0, fontsize=16)\nax1.grid(True, alpha=0.3)\nax1.hlines(0, 0, max_steps - 1, color=\"k\", linewidth=1, alpha=0.3)\nax1.axis([0, max_steps - 1, -1, 1])\nax2.imshow(PE.T[:crop_i], cmap=\"gray\", interpolation=\"bilinear\", aspect=\"auto\")\nax2.hlines(i1, 0, max_steps - 1, color=\"b\")\ncheat = 2 # need to raise the red line a bit, or else it hides the blue one\nax2.hlines(i2+cheat, 0, max_steps - 1, color=\"r\")\nax2.plot([p1, p1], [0, crop_i], \"k--\")\nax2.plot([p2, p2], [0, crop_i], \"k--\", alpha=0.5)\nax2.plot([p1, p2], [i2+cheat, i2+cheat], \"ro\")\nax2.plot([p1, p2], [i1, i1], \"bo\")\nax2.axis([0, max_steps - 1, 0, crop_i])\nax2.set_xlabel(\"$p$\", fontsize=16)\nax2.set_ylabel(\"$i$\", rotation=0, fontsize=16)\nsave_fig(\"positional_embedding_plot\")\nplt.show()\n\nSaving figure positional_embedding_plot\n\n\n\n\n\n\nembed_size = 512; max_steps = 500; vocab_size = 10000\nencoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\ndecoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\nembeddings = keras.layers.Embedding(vocab_size, embed_size)\nencoder_embeddings = embeddings(encoder_inputs)\ndecoder_embeddings = embeddings(decoder_inputs)\npositional_encoding = PositionalEncoding(max_steps, max_dims=embed_size)\nencoder_in = positional_encoding(encoder_embeddings)\ndecoder_in = positional_encoding(decoder_embeddings)\n\n다음은 (매우) 간소화한 Transformer입니다(실제 구조는 스킵 연결, 층 정규화, 밀집 층 그리고 가장 중요하게 일반적인 어텐션이 아니라 멀티-헤드 어텐션을 가집니다):\n\nZ = encoder_in\nfor N in range(6):\n    Z = keras.layers.Attention(use_scale=True)([Z, Z])\n\nencoder_outputs = Z\nZ = decoder_in\nfor N in range(6):\n    Z = keras.layers.Attention(use_scale=True, causal=True)([Z, Z])\n    Z = keras.layers.Attention(use_scale=True)([Z, encoder_outputs])\n\noutputs = keras.layers.TimeDistributed(\n    keras.layers.Dense(vocab_size, activation=\"softmax\"))(Z)\n\n다음은 기본적인 MultiHeadAttention 층의 구현입니다. 가까운 시일 내에 keras.layers에 추가될 것 같습니다. kernel_size=1인 (그리고 기본값 padding=\"valid\", strides=1을 사용하는) Conv1D 층은 TimeDistributed(Dense(...))과 같습니다.\n\nK = keras.backend\n\nclass MultiHeadAttention(keras.layers.Layer):\n    def __init__(self, n_heads, causal=False, use_scale=False, **kwargs):\n        self.n_heads = n_heads\n        self.causal = causal\n        self.use_scale = use_scale\n        super().__init__(**kwargs)\n    def build(self, batch_input_shape):\n        self.dims = batch_input_shape[0][-1]\n        self.q_dims, self.v_dims, self.k_dims = [self.dims // self.n_heads] * 3 # could be hyperparameters instead\n        self.q_linear = keras.layers.Conv1D(self.n_heads * self.q_dims, kernel_size=1, use_bias=False)\n        self.v_linear = keras.layers.Conv1D(self.n_heads * self.v_dims, kernel_size=1, use_bias=False)\n        self.k_linear = keras.layers.Conv1D(self.n_heads * self.k_dims, kernel_size=1, use_bias=False)\n        self.attention = keras.layers.Attention(causal=self.causal, use_scale=self.use_scale)\n        self.out_linear = keras.layers.Conv1D(self.dims, kernel_size=1, use_bias=False)\n        super().build(batch_input_shape)\n    def _multi_head_linear(self, inputs, linear):\n        shape = K.concatenate([K.shape(inputs)[:-1], [self.n_heads, -1]])\n        projected = K.reshape(linear(inputs), shape)\n        perm = K.permute_dimensions(projected, [0, 2, 1, 3])\n        return K.reshape(perm, [shape[0] * self.n_heads, shape[1], -1])\n    def call(self, inputs):\n        q = inputs[0]\n        v = inputs[1]\n        k = inputs[2] if len(inputs) &gt; 2 else v\n        shape = K.shape(q)\n        q_proj = self._multi_head_linear(q, self.q_linear)\n        v_proj = self._multi_head_linear(v, self.v_linear)\n        k_proj = self._multi_head_linear(k, self.k_linear)\n        multi_attended = self.attention([q_proj, v_proj, k_proj])\n        shape_attended = K.shape(multi_attended)\n        reshaped_attended = K.reshape(multi_attended, [shape[0], self.n_heads, shape_attended[1], shape_attended[2]])\n        perm = K.permute_dimensions(reshaped_attended, [0, 2, 1, 3])\n        concat = K.reshape(perm, [shape[0], shape_attended[1], -1])\n        return self.out_linear(concat)\n\n\nQ = np.random.rand(2, 50, 512)\nV = np.random.rand(2, 80, 512)\nmulti_attn = MultiHeadAttention(8)\nmulti_attn([Q, V]).shape\n\nTensorShape([2, 50, 512])"
  },
  {
    "objectID": "Machine_Learning/16_nlp_with_rnns_and_attention.html#to-7.",
    "href": "Machine_Learning/16_nlp_with_rnns_and_attention.html#to-7.",
    "title": "16_nlp_with_rnns_and_attention",
    "section": "1. to 7.",
    "text": "1. to 7.\n부록 A 참조"
  },
  {
    "objectID": "Machine_Learning/16_nlp_with_rnns_and_attention.html#section",
    "href": "Machine_Learning/16_nlp_with_rnns_and_attention.html#section",
    "title": "16_nlp_with_rnns_and_attention",
    "section": "8.",
    "text": "8.\n연습문제: 호크라이터와 슈미트후버는 LSTM에 관한 논문에서 임베딩된 레버 문법을 사용했습니다. 이는 ’BPBTSXXVPSEPE’와 같은 문자열을 만드는 인공 문법입니다. 이 주제에 대한 제니 오어의 훌륭한 소개(https://homl.info/108)를 확인해보세요. 특정 임베딩된 레버 문법 하나를 선택하고(제니 오어의 페이지에 있는 것과 같은), 그다음에 문자열이 이 문법을 따르는지 아닌지 구별하는 RNN을 훈련해보세요. 먼저 문법에 맞는 문자열 50%와 그렇지 않은 문자열 50%를 담은 훈련 배치를 생성하는 함수를 만들어야 합니다.\n먼저 문법에 맞는 문자열을 생성하는 함수가 필요합니다. 이 문법은 각 상태에서 가능한 전이 상태의 리스트입니다. 하나의 전이는 출력할 문자열(또는 생성할 문법)과 다음 상태를 지정합니다.\n\ndefault_reber_grammar = [\n    [(\"B\", 1)],           # (state 0) =B=&gt;(state 1)\n    [(\"T\", 2), (\"P\", 3)], # (state 1) =T=&gt;(state 2) or =P=&gt;(state 3)\n    [(\"S\", 2), (\"X\", 4)], # (state 2) =S=&gt;(state 2) or =X=&gt;(state 4)\n    [(\"T\", 3), (\"V\", 5)], # and so on...\n    [(\"X\", 3), (\"S\", 6)],\n    [(\"P\", 4), (\"V\", 6)],\n    [(\"E\", None)]]        # (state 6) =E=&gt;(terminal state)\n\nembedded_reber_grammar = [\n    [(\"B\", 1)],\n    [(\"T\", 2), (\"P\", 3)],\n    [(default_reber_grammar, 4)],\n    [(default_reber_grammar, 5)],\n    [(\"T\", 6)],\n    [(\"P\", 6)],\n    [(\"E\", None)]]\n\ndef generate_string(grammar):\n    state = 0\n    output = []\n    while state is not None:\n        index = np.random.randint(len(grammar[state]))\n        production, state = grammar[state][index]\n        if isinstance(production, list):\n            production = generate_string(grammar=production)\n        output.append(production)\n    return \"\".join(output)\n\n기본 레버 문법에 맞는 문자열을 몇 개 만들어 보겠습니다:\n\nnp.random.seed(42)\n\nfor _ in range(25):\n    print(generate_string(default_reber_grammar), end=\" \")\n\nBTXXTTVPXTVPXTTVPSE BPVPSE BTXSE BPVVE BPVVE BTSXSE BPTVPXTTTVVE BPVVE BTXSE BTXXVPSE BPTTTTTTTTVVE BTXSE BPVPSE BTXSE BPTVPSE BTXXTVPSE BPVVE BPVVE BPVVE BPTTVVE BPVVE BPVVE BTXXVVE BTXXVVE BTXXVPXVVE \n\n\n좋습니다. 이제 임베딩된 레버 문법에 맞는 문자열을 몇 개 만들어 보겠습니다:\n\nnp.random.seed(42)\n\nfor _ in range(25):\n    print(generate_string(embedded_reber_grammar), end=\" \")\n\nBTBPTTTVPXTVPXTTVPSETE BPBPTVPSEPE BPBPVVEPE BPBPVPXVVEPE BPBTXXTTTTVVEPE BPBPVPSEPE BPBTXXVPSEPE BPBTSSSSSSSXSEPE BTBPVVETE BPBTXXVVEPE BPBTXXVPSEPE BTBTXXVVETE BPBPVVEPE BPBPVVEPE BPBTSXSEPE BPBPVVEPE BPBPTVPSEPE BPBTXXVVEPE BTBPTVPXVVETE BTBPVVETE BTBTSSSSSSSXXVVETE BPBTSSSXXTTTTVPSEPE BTBPTTVVETE BPBTXXTVVEPE BTBTXSETE \n\n\n좋네요, 이제 이 문법을 따르지 않는 문자열을 생성할 함수를 만듭니다. 무작위하게 문자열을 만들 수 있지만 그렇게 하면 너무 문제가 쉬워지므로 대신 문법을 따르는 문자열을 만든 후 하나의 문자만 바꾸어 놓도록 하겠습니다:\n\nPOSSIBLE_CHARS = \"BEPSTVX\"\n\ndef generate_corrupted_string(grammar, chars=POSSIBLE_CHARS):\n    good_string = generate_string(grammar)\n    index = np.random.randint(len(good_string))\n    good_char = good_string[index]\n    bad_char = np.random.choice(sorted(set(chars) - set(good_char)))\n    return good_string[:index] + bad_char + good_string[index + 1:]\n\n잘못된 문자열 몇 개를 만들어 보죠:\n\nnp.random.seed(42)\n\nfor _ in range(25):\n    print(generate_corrupted_string(embedded_reber_grammar), end=\" \")\n\nBTBPTTTPPXTVPXTTVPSETE BPBTXEEPE BPBPTVVVEPE BPBTSSSSXSETE BPTTXSEPE BTBPVPXTTTTTTEVETE BPBTXXSVEPE BSBPTTVPSETE BPBXVVEPE BEBTXSETE BPBPVPSXPE BTBPVVVETE BPBTSXSETE BPBPTTTPTTTTTVPSEPE BTBTXXTTSTVPSETE BBBTXSETE BPBTPXSEPE BPBPVPXTTTTVPXTVPXVPXTTTVVEVE BTBXXXTVPSETE BEBTSSSSSXXVPXTVVETE BTBXTTVVETE BPBTXSTPE BTBTXXTTTVPSBTE BTBTXSETX BTBTSXSSTE \n\n\n문자열을 바로 RNN에 주입할 수는 없기 때문에 어떤 식으로든 인코딩해야 합니다. 한 가지 방법은 각 문자를 원-핫 인코딩하는 것입니다. 또 다른 방식은 임베딩을 사용하는 것입니다. 두 번째 방법을 사용해 보겠습니다(문자 개수가 작다면 원-핫 인코딩도 좋은 선택일 것입니다). 임베딩을 위해 각 문자열을 문자 ID의 시퀀스로 바꾸어야 합니다. POSSIBLE_CHARS의 문자열 인덱스를 사용해 이런 작업을 수행하는 함수를 만들어 보겠습니다:\n\ndef string_to_ids(s, chars=POSSIBLE_CHARS):\n    return [chars.index(c) for c in s]\n\n\nstring_to_ids(\"BTTTXXVVETE\")\n\n[0, 4, 4, 4, 6, 6, 5, 5, 1, 4, 1]\n\n\n이제 50%는 올바른 문자열 50%는 잘못된 문자열로 이루어진 데이터셋을 만듭니다:\n\ndef generate_dataset(size):\n    good_strings = [string_to_ids(generate_string(embedded_reber_grammar))\n                    for _ in range(size // 2)]\n    bad_strings = [string_to_ids(generate_corrupted_string(embedded_reber_grammar))\n                   for _ in range(size - size // 2)]\n    all_strings = good_strings + bad_strings\n    X = tf.ragged.constant(all_strings, ragged_rank=1)\n    y = np.array([[1.] for _ in range(len(good_strings))] +\n                 [[0.] for _ in range(len(bad_strings))])\n    return X, y\n\n\nnp.random.seed(42)\n\nX_train, y_train = generate_dataset(10000)\nX_valid, y_valid = generate_dataset(2000)\n\n첫 번째 훈련 샘플을 확인해 보겠습니다:\n\nX_train[0]\n\n&lt;tf.Tensor: shape=(22,), dtype=int32, numpy=\narray([0, 4, 0, 2, 4, 4, 4, 5, 2, 6, 4, 5, 2, 6, 4, 4, 5, 2, 3, 1, 4, 1],\n      dtype=int32)&gt;\n\n\n어떤 클래스에 속할까요?\n\ny_train[0]\n\narray([1.])\n\n\n완벽합니다! 이제 올바른 문자열을 구분할 RNN을 만들 준비가 되었습니다. 간단한 시퀀스 이진 분류기를 만듭니다:\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nembedding_size = 5\n\nmodel = keras.models.Sequential([\n    keras.layers.InputLayer(input_shape=[None], dtype=tf.int32, ragged=True),\n    keras.layers.Embedding(input_dim=len(POSSIBLE_CHARS), output_dim=embedding_size),\n    keras.layers.GRU(30),\n    keras.layers.Dense(1, activation=\"sigmoid\")\n])\noptimizer = keras.optimizers.SGD(learning_rate=0.02, momentum = 0.95, nesterov=True)\nmodel.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\nhistory = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n\n/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential_6/gru_12/RaggedToTensor/boolean_mask_1/GatherV2:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential_6/gru_12/RaggedToTensor/boolean_mask/GatherV2:0\", shape=(None, 5), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential_6/gru_12/RaggedToTensor/Shape:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n  \"shape. This may consume a large amount of memory.\" % value)\n\n\nEpoch 1/20\n313/313 [==============================] - 18s 53ms/step - loss: 0.6910 - accuracy: 0.5095 - val_loss: 0.6825 - val_accuracy: 0.5645\nEpoch 2/20\n313/313 [==============================] - 16s 52ms/step - loss: 0.6678 - accuracy: 0.5659 - val_loss: 0.6635 - val_accuracy: 0.6105\nEpoch 3/20\n313/313 [==============================] - 17s 53ms/step - loss: 0.6504 - accuracy: 0.5766 - val_loss: 0.6521 - val_accuracy: 0.6110\nEpoch 4/20\n313/313 [==============================] - 16s 52ms/step - loss: 0.6347 - accuracy: 0.5980 - val_loss: 0.6224 - val_accuracy: 0.6445\nEpoch 5/20\n313/313 [==============================] - 16s 52ms/step - loss: 0.6054 - accuracy: 0.6361 - val_loss: 0.5779 - val_accuracy: 0.6980\nEpoch 6/20\n313/313 [==============================] - 16s 52ms/step - loss: 0.5414 - accuracy: 0.7093 - val_loss: 0.4695 - val_accuracy: 0.7795\nEpoch 7/20\n313/313 [==============================] - 16s 52ms/step - loss: 0.3913 - accuracy: 0.8320 - val_loss: 0.2796 - val_accuracy: 0.8955\nEpoch 8/20\n313/313 [==============================] - 16s 53ms/step - loss: 0.4481 - accuracy: 0.7648 - val_loss: 0.5198 - val_accuracy: 0.6870\nEpoch 9/20\n313/313 [==============================] - 17s 53ms/step - loss: 0.4590 - accuracy: 0.7721 - val_loss: 0.3302 - val_accuracy: 0.8660\nEpoch 10/20\n313/313 [==============================] - 16s 53ms/step - loss: 0.2588 - accuracy: 0.9078 - val_loss: 0.1560 - val_accuracy: 0.9715\nEpoch 11/20\n313/313 [==============================] - 16s 52ms/step - loss: 0.1452 - accuracy: 0.9580 - val_loss: 0.1371 - val_accuracy: 0.9605\nEpoch 12/20\n313/313 [==============================] - 16s 52ms/step - loss: 0.0698 - accuracy: 0.9834 - val_loss: 0.0417 - val_accuracy: 0.9885\nEpoch 13/20\n313/313 [==============================] - 16s 52ms/step - loss: 0.0835 - accuracy: 0.9776 - val_loss: 0.0347 - val_accuracy: 0.9895\nEpoch 14/20\n313/313 [==============================] - 16s 53ms/step - loss: 0.0402 - accuracy: 0.9913 - val_loss: 0.0168 - val_accuracy: 0.9980\nEpoch 15/20\n313/313 [==============================] - 16s 52ms/step - loss: 0.0275 - accuracy: 0.9953 - val_loss: 0.0082 - val_accuracy: 0.9990\nEpoch 16/20\n313/313 [==============================] - 16s 52ms/step - loss: 0.0108 - accuracy: 0.9979 - val_loss: 0.0102 - val_accuracy: 0.9960\nEpoch 17/20\n313/313 [==============================] - 17s 53ms/step - loss: 0.0136 - accuracy: 0.9972 - val_loss: 0.0084 - val_accuracy: 0.9990\nEpoch 18/20\n313/313 [==============================] - 17s 53ms/step - loss: 0.0070 - accuracy: 0.9988 - val_loss: 0.0080 - val_accuracy: 0.9990\nEpoch 19/20\n313/313 [==============================] - 17s 53ms/step - loss: 0.0057 - accuracy: 0.9986 - val_loss: 0.0029 - val_accuracy: 0.9995\nEpoch 20/20\n313/313 [==============================] - 17s 53ms/step - loss: 0.0338 - accuracy: 0.9924 - val_loss: 0.0059 - val_accuracy: 0.9975\n\n\n이제 두 개의 까다로운 문자열로 이 RNN을 테스트해 보죠: 첫 번째는 잘못된 것이고 두 번째는 올바른 것입니다. 이 문자열은 마지막에서 두 번째 글자만 다릅니다. RNN이 이를 맞춘다면 두 번째 문자가 항상 끝에서 두 번째 문자와 같아야 한다는 패턴을 알게 됐다는 것을 의미합니다. 이렇게 하려면 꽤 긴 단기 기억(long short-term memory)이 필요합니다(그래서 GRU 셀을 사용했습니다).\n\ntest_strings = [\"BPBTSSSSSSSXXTTVPXVPXTTTTTVVETE\",\n                \"BPBTSSSSSSSXXTTVPXVPXTTTTTVVEPE\"]\nX_test = tf.ragged.constant([string_to_ids(s) for s in test_strings], ragged_rank=1)\n\ny_proba = model.predict(X_test)\nprint()\nprint(\"레버 문자열일 추정 확률:\")\nfor index, string in enumerate(test_strings):\n    print(\"{}: {:.2f}%\".format(string, 100 * y_proba[index][0]))\n\n\nEstimated probability that these are Reber strings:\nBPBTSSSSSSSXXTTVPXVPXTTTTTVVETE: 0.06%\nBPBTSSSSSSSXXTTVPXVPXTTTTTVVEPE: 91.51%\n\n\n쨘! 잘 작동하네요. 이 RNN이 매우 높은 신뢰도로 정확한 답을 냈습니다. :)"
  },
  {
    "objectID": "Machine_Learning/16_nlp_with_rnns_and_attention.html#section-1",
    "href": "Machine_Learning/16_nlp_with_rnns_and_attention.html#section-1",
    "title": "16_nlp_with_rnns_and_attention",
    "section": "9.",
    "text": "9.\n연습문제: 날짜 문자열 포맷을 변환하는 인코더-디코더 모델을 훈련하세요(예를 들어, ’April 22, 2019’에서 ’2019-04-22’로 바꿉니다).\n먼저 데이터셋을 만들어 보죠. 1000-01-01 ~ 9999-12-31 사이의 랜덤한 날짜를 사용하겠습니다:\n\nfrom datetime import date\n\n# strftime()의 %B 포맷은 로케일에 의존하기 때문에 사용할 수 있습니다.\nMONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n\ndef random_dates(n_dates):\n    min_date = date(1000, 1, 1).toordinal()\n    max_date = date(9999, 12, 31).toordinal()\n\n    ordinals = np.random.randint(max_date - min_date, size=n_dates) + min_date\n    dates = [date.fromordinal(ordinal) for ordinal in ordinals]\n\n    x = [MONTHS[dt.month - 1] + \" \" + dt.strftime(\"%d, %Y\") for dt in dates]\n    y = [dt.isoformat() for dt in dates]\n    return x, y\n\n다음은 입력과 출력 형식에 맞춘 랜덤한 몇 개의 날짜입니다:\n\nnp.random.seed(42)\n\nn_dates = 3\nx_example, y_example = random_dates(n_dates)\nprint(\"{:25s}{:25s}\".format(\"Input\", \"Target\"))\nprint(\"-\" * 50)\nfor idx in range(n_dates):\n    print(\"{:25s}{:25s}\".format(x_example[idx], y_example[idx]))\n\nInput                    Target                   \n--------------------------------------------------\nSeptember 20, 7075       7075-09-20               \nMay 15, 8579             8579-05-15               \nJanuary 11, 7103         7103-01-11               \n\n\n입력에 가능한 전체 문자를 나열해 보죠:\n\nINPUT_CHARS = \"\".join(sorted(set(\"\".join(MONTHS) + \"0123456789, \")))\nINPUT_CHARS\n\n' ,0123456789ADFJMNOSabceghilmnoprstuvy'\n\n\n그리고 다음은 출력에 가능한 전체 문자입니다:\n\nOUTPUT_CHARS = \"0123456789-\"\n\n이전 연습문제에서처럼 문자열을 문자 ID 리스트로 바꾸는 함수를 작성해 보겠습니다:\n\ndef date_str_to_ids(date_str, chars=INPUT_CHARS):\n    return [chars.index(c) for c in date_str]\n\n\ndate_str_to_ids(x_example[0], INPUT_CHARS)\n\n[19, 23, 31, 34, 23, 28, 21, 23, 32, 0, 4, 2, 1, 0, 9, 2, 9, 7]\n\n\n\ndate_str_to_ids(y_example[0], OUTPUT_CHARS)\n\n[7, 0, 7, 5, 10, 0, 9, 10, 2, 0]\n\n\n\ndef prepare_date_strs(date_strs, chars=INPUT_CHARS):\n    X_ids = [date_str_to_ids(dt, chars) for dt in date_strs]\n    X = tf.ragged.constant(X_ids, ragged_rank=1)\n    return (X + 1).to_tensor() # using 0 as the padding token ID\n\ndef create_dataset(n_dates):\n    x, y = random_dates(n_dates)\n    return prepare_date_strs(x, INPUT_CHARS), prepare_date_strs(y, OUTPUT_CHARS)\n\n\nnp.random.seed(42)\n\nX_train, Y_train = create_dataset(10000)\nX_valid, Y_valid = create_dataset(2000)\nX_test, Y_test = create_dataset(2000)\n\n\nY_train[0]\n\n&lt;tf.Tensor: shape=(10,), dtype=int32, numpy=array([ 8,  1,  8,  6, 11,  1, 10, 11,  3,  1], dtype=int32)&gt;\n\n\n\n첫 번째 버전: 기본적인 seq2seq 모델\n먼저 가장 간단한 모델을 시도해 보겠습니다: 입력 시퀀스가 먼저 (임베딩 층 뒤에 하나의 LSTM 층으로 구성된) 인코더를 통과하여 벡터로 출력됩니다. 그 다음 이 벡터가 (하나의 LSTM 층 뒤에 밀집 층으로 구성된) 디코더로 들어가 벡터의 시퀀스를 출력합니다. 각 벡터는 가능한 모든 출력 문자에 대한 추정 확률입니다.\n디코더는 시퀀스를 입력으로 기대하기 때문에 가능한 가장 긴 출력 시퀀스만큼 (인코더의 출력) 벡터를 반복합니다.\n\nembedding_size = 32\nmax_output_length = Y_train.shape[1]\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nencoder = keras.models.Sequential([\n    keras.layers.Embedding(input_dim=len(INPUT_CHARS) + 1,\n                           output_dim=embedding_size,\n                           input_shape=[None]),\n    keras.layers.LSTM(128)\n])\n\ndecoder = keras.models.Sequential([\n    keras.layers.LSTM(128, return_sequences=True),\n    keras.layers.Dense(len(OUTPUT_CHARS) + 1, activation=\"softmax\")\n])\n\nmodel = keras.models.Sequential([\n    encoder,\n    keras.layers.RepeatVector(max_output_length),\n    decoder\n])\n\noptimizer = keras.optimizers.Nadam()\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n              metrics=[\"accuracy\"])\nhistory = model.fit(X_train, Y_train, epochs=20,\n                    validation_data=(X_valid, Y_valid))\n\nEpoch 1/20\n313/313 [==============================] - 6s 9ms/step - loss: 1.8255 - accuracy: 0.3456 - val_loss: 1.3841 - val_accuracy: 0.4841\nEpoch 2/20\n313/313 [==============================] - 2s 7ms/step - loss: 1.2676 - accuracy: 0.5435 - val_loss: 1.1041 - val_accuracy: 0.6076\nEpoch 3/20\n313/313 [==============================] - 2s 7ms/step - loss: 1.0743 - accuracy: 0.6210 - val_loss: 1.1233 - val_accuracy: 0.5800\nEpoch 4/20\n313/313 [==============================] - 2s 7ms/step - loss: 1.1518 - accuracy: 0.5975 - val_loss: 0.9246 - val_accuracy: 0.6608\nEpoch 5/20\n313/313 [==============================] - 2s 8ms/step - loss: 0.7419 - accuracy: 0.7272 - val_loss: 0.6349 - val_accuracy: 0.7602\nEpoch 6/20\n313/313 [==============================] - 2s 7ms/step - loss: 0.6495 - accuracy: 0.7567 - val_loss: 0.5411 - val_accuracy: 0.7875\nEpoch 7/20\n313/313 [==============================] - 2s 7ms/step - loss: 0.4445 - accuracy: 0.8246 - val_loss: 0.3653 - val_accuracy: 0.8564\nEpoch 8/20\n313/313 [==============================] - 2s 7ms/step - loss: 0.4815 - accuracy: 0.8322 - val_loss: 0.3781 - val_accuracy: 0.8661\nEpoch 9/20\n313/313 [==============================] - 2s 7ms/step - loss: 0.2758 - accuracy: 0.9068 - val_loss: 0.2180 - val_accuracy: 0.9336\nEpoch 10/20\n313/313 [==============================] - 2s 7ms/step - loss: 0.1578 - accuracy: 0.9588 - val_loss: 0.1138 - val_accuracy: 0.9747\nEpoch 11/20\n313/313 [==============================] - 2s 7ms/step - loss: 0.0805 - accuracy: 0.9851 - val_loss: 0.0638 - val_accuracy: 0.9887\nEpoch 12/20\n313/313 [==============================] - 2s 7ms/step - loss: 0.0441 - accuracy: 0.9948 - val_loss: 0.0357 - val_accuracy: 0.9965\nEpoch 13/20\n313/313 [==============================] - 2s 7ms/step - loss: 0.1565 - accuracy: 0.9641 - val_loss: 0.0730 - val_accuracy: 0.9875\nEpoch 14/20\n313/313 [==============================] - 2s 7ms/step - loss: 0.0364 - accuracy: 0.9965 - val_loss: 0.0254 - val_accuracy: 0.9981\nEpoch 15/20\n313/313 [==============================] - 2s 7ms/step - loss: 0.0165 - accuracy: 0.9994 - val_loss: 0.0150 - val_accuracy: 0.9994\nEpoch 16/20\n313/313 [==============================] - 2s 7ms/step - loss: 0.0104 - accuracy: 0.9998 - val_loss: 0.0101 - val_accuracy: 0.9997\nEpoch 17/20\n313/313 [==============================] - 2s 7ms/step - loss: 0.0071 - accuracy: 0.9999 - val_loss: 0.0072 - val_accuracy: 0.9998\nEpoch 18/20\n313/313 [==============================] - 2s 7ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0054 - val_accuracy: 0.9999\nEpoch 19/20\n313/313 [==============================] - 2s 8ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 0.9999\nEpoch 20/20\n313/313 [==============================] - 2s 8ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 0.9999\n\n\n좋아 보이네요, 100% 검증 정확도를 달성했습니다! 이 모델을 사용해 예측을 만들어 보죠. 문자 ID 시퀀스를 문자열로 바꾸는 함수를 작성하겠습니다:\n\ndef ids_to_date_strs(ids, chars=OUTPUT_CHARS):\n    return [\"\".join([(\"?\" + chars)[index] for index in sequence])\n            for sequence in ids]\n\n이제 모델을 사용해 샘플 날짜를 변환합니다.\n\nX_new = prepare_date_strs([\"September 17, 2009\", \"July 14, 1789\"])\n\n\n#ids = model.predict_classes(X_new)\nids = np.argmax(model.predict(X_new), axis=-1)\nfor date_str in ids_to_date_strs(ids):\n    print(date_str)\n\n2009-09-17\n1789-07-14\n\n\n완벽합니다! :)\n하지만 (가장 긴 날짜에 해당하는) 길이가 18인 입력 문자열에서만 모델이 훈련되었기 때문에 짧은 시퀀스에서는 잘 동작하지 않습니다:\n\nX_new = prepare_date_strs([\"May 02, 2020\", \"July 14, 1789\"])\n\n\n#ids = model.predict_classes(X_new)\nids = np.argmax(model.predict(X_new), axis=-1)\nfor date_str in ids_to_date_strs(ids):\n    print(date_str)\n\n2020-08-02\n1789-02-14\n\n\n이런! 패딩을 사용해 훈련할 때와 동일한 길이의 시퀀스를 전달해야 할 것 같습니다. 이를 위해 헬퍼 함수를 작성해 보죠:\n\nmax_input_length = X_train.shape[1]\n\ndef prepare_date_strs_padded(date_strs):\n    X = prepare_date_strs(date_strs)\n    if X.shape[1] &lt; max_input_length:\n        X = tf.pad(X, [[0, 0], [0, max_input_length - X.shape[1]]])\n    return X\n\ndef convert_date_strs(date_strs):\n    X = prepare_date_strs_padded(date_strs)\n    #ids = model.predict_classes(X)\n    ids = np.argmax(model.predict(X), axis=-1)\n    return ids_to_date_strs(ids)\n\n\nconvert_date_strs([\"May 02, 2020\", \"July 14, 1789\"])\n\n['2020-05-02', '1789-07-14']\n\n\n좋네요! 물론 더 쉽게 날짜 변환 도구를 만들 수 있습니다(예를 들면, 정규식이나 더 단순한 문자열 조작). 하지만 신경망을 사용하는 것이 더 멋져 보이네요. ;-)\n하지만 실제 시퀀스-투-시퀀스 문제는 더 어렵습니다. 완벽함을 추구하기 위해 더 강력한 모델을 만들어 보겠습니다.\n\n\n두 번째 버전: 디코더에서 쉬프트된 타깃 주입하기(티처 포싱(teacher forcing))\n디코더에세 인코더 출력 벡터를 단순히 반복한 것을 주입하는 대신 한 타임 스텝 오른쪽으로 이동된 타깃 시퀀스를 주입할 수 있습니다. 이렇게 하면 각 타임 스텝에서 디코더는 이전 타깃 문자가 무엇인지 알게 됩니다. 이는 더 복잡한 시퀀스-투-시퀀스 문제를 다루는데 도움이 됩니다.\n각 타깃 시퀀스의 첫 번째 출력 문자는 이전 문자가 없기 때문에 시퀀스 시작(start-of-sequence, sos)을 나타내는 새로운 토큰이 필요합니다.\n추론에서는 타깃을 알지 못하므로 디코더에게 무엇을 주입해야 할까요? sos 토큰을 시작해서 한 번에 하나의 문자를 예측하고 디코더에게 지금까지 예측한 모든 문자를 주입할 수 있습니다(나중에 이 노트북에서 더 자세히 알아 보겠습니다).\n하지만 디코더의 LSTM이 스텝마다 이전 타깃을 입력으로 기대한다면 인코더의 벡터 출력을 어떻게 전달할까요? 한가지 방법은 출력 벡터를 무시하는 것입니다. 그리고 대신 인코더의 LSTM 상태를 디코더의 LSTM의 초기 상태로 사용합니다(이렇게 하려면 인코더의 LSTM과 디코더의 LSTM 유닛 개수가 같아야 합니다).\n그럼 (훈련, 검증, 테스트를 위한) 디코더의 입력을 만들어 보죠. sos 토큰은 가능한 출력 문자의 마지막 ID + 1으로 나타냅니다.\n\nsos_id = len(OUTPUT_CHARS) + 1\n\ndef shifted_output_sequences(Y):\n    sos_tokens = tf.fill(dims=(len(Y), 1), value=sos_id)\n    return tf.concat([sos_tokens, Y[:, :-1]], axis=1)\n\nX_train_decoder = shifted_output_sequences(Y_train)\nX_valid_decoder = shifted_output_sequences(Y_valid)\nX_test_decoder = shifted_output_sequences(Y_test)\n\n디코더의 훈련 입력을 확인해 보죠:\n\nX_train_decoder\n\n&lt;tf.Tensor: shape=(10000, 10), dtype=int32, numpy=\narray([[12,  8,  1, ..., 10, 11,  3],\n       [12,  9,  6, ...,  6, 11,  2],\n       [12,  8,  2, ...,  2, 11,  2],\n       ...,\n       [12, 10,  8, ...,  2, 11,  4],\n       [12,  2,  2, ...,  3, 11,  3],\n       [12,  8,  9, ...,  8, 11,  3]], dtype=int32)&gt;\n\n\n이제 모델을 만듭니다. 이제 더 이상 간단한 시퀀셜 모델이 아니므로 함수형 API를 사용하겠습니다:\n\nencoder_embedding_size = 32\ndecoder_embedding_size = 32\nlstm_units = 128\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nencoder_input = keras.layers.Input(shape=[None], dtype=tf.int32)\nencoder_embedding = keras.layers.Embedding(\n    input_dim=len(INPUT_CHARS) + 1,\n    output_dim=encoder_embedding_size)(encoder_input)\n_, encoder_state_h, encoder_state_c = keras.layers.LSTM(\n    lstm_units, return_state=True)(encoder_embedding)\nencoder_state = [encoder_state_h, encoder_state_c]\n\ndecoder_input = keras.layers.Input(shape=[None], dtype=tf.int32)\ndecoder_embedding = keras.layers.Embedding(\n    input_dim=len(OUTPUT_CHARS) + 2,\n    output_dim=decoder_embedding_size)(decoder_input)\ndecoder_lstm_output = keras.layers.LSTM(lstm_units, return_sequences=True)(\n    decoder_embedding, initial_state=encoder_state)\ndecoder_output = keras.layers.Dense(len(OUTPUT_CHARS) + 1,\n                                    activation=\"softmax\")(decoder_lstm_output)\n\nmodel = keras.models.Model(inputs=[encoder_input, decoder_input],\n                           outputs=[decoder_output])\n\noptimizer = keras.optimizers.Nadam()\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n              metrics=[\"accuracy\"])\nhistory = model.fit([X_train, X_train_decoder], Y_train, epochs=10,\n                    validation_data=([X_valid, X_valid_decoder], Y_valid))\n\nEpoch 1/10\n313/313 [==============================] - 6s 9ms/step - loss: 1.6803 - accuracy: 0.3743 - val_loss: 1.4168 - val_accuracy: 0.4505\nEpoch 2/10\n313/313 [==============================] - 2s 7ms/step - loss: 1.1884 - accuracy: 0.5587 - val_loss: 0.8931 - val_accuracy: 0.6714\nEpoch 3/10\n313/313 [==============================] - 2s 7ms/step - loss: 0.6520 - accuracy: 0.7671 - val_loss: 0.3952 - val_accuracy: 0.8698\nEpoch 4/10\n313/313 [==============================] - 2s 7ms/step - loss: 0.2255 - accuracy: 0.9431 - val_loss: 0.1285 - val_accuracy: 0.9754\nEpoch 5/10\n313/313 [==============================] - 2s 7ms/step - loss: 0.0803 - accuracy: 0.9895 - val_loss: 0.0490 - val_accuracy: 0.9964\nEpoch 6/10\n313/313 [==============================] - 2s 7ms/step - loss: 0.0714 - accuracy: 0.9882 - val_loss: 0.0286 - val_accuracy: 0.9991\nEpoch 7/10\n313/313 [==============================] - 2s 7ms/step - loss: 0.0188 - accuracy: 0.9998 - val_loss: 0.0150 - val_accuracy: 0.9998\nEpoch 8/10\n313/313 [==============================] - 2s 8ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 0.9999\nEpoch 9/10\n313/313 [==============================] - 2s 7ms/step - loss: 0.0417 - accuracy: 0.9935 - val_loss: 0.0104 - val_accuracy: 0.9998\nEpoch 10/10\n313/313 [==============================] - 2s 7ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0058 - val_accuracy: 0.9999\n\n\n이 모델도 100% 검증 정확도를 달성했지만 더 빠릅니다.\n이 모델을 사용해 몇 가지 예측을 수행해 보죠. 이번에는 한 문자씩 예측해야 합니다.\n\nsos_id = len(OUTPUT_CHARS) + 1\n\ndef predict_date_strs(date_strs):\n    X = prepare_date_strs_padded(date_strs)\n    Y_pred = tf.fill(dims=(len(X), 1), value=sos_id)\n    for index in range(max_output_length):\n        pad_size = max_output_length - Y_pred.shape[1]\n        X_decoder = tf.pad(Y_pred, [[0, 0], [0, pad_size]])\n        Y_probas_next = model.predict([X, X_decoder])[:, index:index+1]\n        Y_pred_next = tf.argmax(Y_probas_next, axis=-1, output_type=tf.int32)\n        Y_pred = tf.concat([Y_pred, Y_pred_next], axis=1)\n    return ids_to_date_strs(Y_pred[:, 1:])\n\n\npredict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])\n\n['1789-07-14', '2020-05-01']\n\n\n잘 동작하네요! :)\n\n\n세 번째 버전: TF-Addons의 seq2seq 구현 사용하기\n정확히 동일한 모델을 만들어 보죠. 하지만 TF-Addon의 seq2seq API를 사용하겠습니다. 아래 구현은 이 노트북의 위에 있는 TFA 예제와 거의 비슷합니다. 다만 모델 입력에 출력 시퀀스 길이를 지정하지 않습니다(하지만 출력 시퀀스의 길이가 매우 다른 프로젝트에서 필요하다면 쉽게 이를 추가할 수 있습니다).\n\nimport tensorflow_addons as tfa\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nencoder_embedding_size = 32\ndecoder_embedding_size = 32\nunits = 128\n\nencoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\ndecoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\nsequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n\nencoder_embeddings = keras.layers.Embedding(\n    len(INPUT_CHARS) + 1, encoder_embedding_size)(encoder_inputs)\n\ndecoder_embedding_layer = keras.layers.Embedding(\n    len(OUTPUT_CHARS) + 2, decoder_embedding_size)\ndecoder_embeddings = decoder_embedding_layer(decoder_inputs)\n\nencoder = keras.layers.LSTM(units, return_state=True)\nencoder_outputs, state_h, state_c = encoder(encoder_embeddings)\nencoder_state = [state_h, state_c]\n\nsampler = tfa.seq2seq.sampler.TrainingSampler()\n\ndecoder_cell = keras.layers.LSTMCell(units)\noutput_layer = keras.layers.Dense(len(OUTPUT_CHARS) + 1)\n\ndecoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell,\n                                                 sampler,\n                                                 output_layer=output_layer)\nfinal_outputs, final_state, final_sequence_lengths = decoder(\n    decoder_embeddings,\n    initial_state=encoder_state)\nY_proba = keras.layers.Activation(\"softmax\")(final_outputs.rnn_output)\n\nmodel = keras.models.Model(inputs=[encoder_inputs, decoder_inputs],\n                           outputs=[Y_proba])\noptimizer = keras.optimizers.Nadam()\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n              metrics=[\"accuracy\"])\nhistory = model.fit([X_train, X_train_decoder], Y_train, epochs=15,\n                    validation_data=([X_valid, X_valid_decoder], Y_valid))\n\nEpoch 1/15\n313/313 [==============================] - 13s 30ms/step - loss: 1.6778 - accuracy: 0.3657 - val_loss: 1.4651 - val_accuracy: 0.4271\nEpoch 2/15\n313/313 [==============================] - 9s 28ms/step - loss: 1.3794 - accuracy: 0.4618 - val_loss: 1.1807 - val_accuracy: 0.5543\nEpoch 3/15\n313/313 [==============================] - 9s 28ms/step - loss: 0.9574 - accuracy: 0.6449 - val_loss: 0.6447 - val_accuracy: 0.7751\nEpoch 4/15\n313/313 [==============================] - 9s 28ms/step - loss: 0.3974 - accuracy: 0.8755 - val_loss: 0.6728 - val_accuracy: 0.7954\nEpoch 5/15\n313/313 [==============================] - 9s 28ms/step - loss: 0.1182 - accuracy: 0.9829 - val_loss: 0.0621 - val_accuracy: 0.9962\nEpoch 6/15\n313/313 [==============================] - 9s 28ms/step - loss: 0.0608 - accuracy: 0.9930 - val_loss: 0.0320 - val_accuracy: 0.9988\nEpoch 7/15\n313/313 [==============================] - 9s 28ms/step - loss: 0.0219 - accuracy: 0.9997 - val_loss: 0.0177 - val_accuracy: 0.9997\nEpoch 8/15\n313/313 [==============================] - 9s 28ms/step - loss: 0.0129 - accuracy: 0.9999 - val_loss: 0.0116 - val_accuracy: 0.9999\nEpoch 9/15\n313/313 [==============================] - 9s 28ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.0077 - val_accuracy: 0.9999\nEpoch 10/15\n313/313 [==============================] - 9s 28ms/step - loss: 0.0544 - accuracy: 0.9894 - val_loss: 0.0106 - val_accuracy: 0.9999\nEpoch 11/15\n313/313 [==============================] - 9s 28ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 1.0000\nEpoch 12/15\n313/313 [==============================] - 9s 28ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.0037 - val_accuracy: 1.0000\nEpoch 13/15\n313/313 [==============================] - 9s 28ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\nEpoch 14/15\n313/313 [==============================] - 9s 28ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\nEpoch 15/15\n313/313 [==============================] - 9s 28ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n\n\n여기에서도 100% 검증 정확도를 달성했습니다! 이 모델을 사용하기 위해 predict_date_strs() 함수를 다시 사용하겠습니다:\n\npredict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])\n\n['1789-07-14', '2020-05-01']\n\n\n하지만 더 효율적으로 추론을 수행하는 방법이 있습니다. 지금까지 추론에서 새로운 문자마다 모델을 실행했습니다. 하지만TrainingSampler 대신에 GreedyEmbeddingSampler를 사용하는 새로운 디코더를 만들 수 있습니다.\n타임 스텝마다 GreedyEmbeddingSampler가 디코더의 출력에 argmax를 계산하고, 디코더 임베딩 층을 통해 토큰 ID를 얻을 수 있습니다. 그다음 다음 타임 스텝에 만들어진 임베딩을 디코더의 LSTM 셀에 주입합니다. 이런 방법을 통해 디코더를 한 번만 실행하여 전체 예측을 얻을 수 있습니다.\n\ninference_sampler = tfa.seq2seq.sampler.GreedyEmbeddingSampler(\n    embedding_fn=decoder_embedding_layer)\ninference_decoder = tfa.seq2seq.basic_decoder.BasicDecoder(\n    decoder_cell, inference_sampler, output_layer=output_layer,\n    maximum_iterations=max_output_length)\nbatch_size = tf.shape(encoder_inputs)[:1]\nstart_tokens = tf.fill(dims=batch_size, value=sos_id)\nfinal_outputs, final_state, final_sequence_lengths = inference_decoder(\n    start_tokens,\n    initial_state=encoder_state,\n    start_tokens=start_tokens,\n    end_token=0)\n\ninference_model = keras.models.Model(inputs=[encoder_inputs],\n                                     outputs=[final_outputs.sample_id])\n\n몇 개의 노트: * GreedyEmbeddingSampler는 start_tokens(디코더 시퀀스마다 sos ID를 담은 벡터)와 end_token(모델이 이 토큰을 출력할 때 디코더가 시퀀스 디코딩을 멈춥니다)이 필요합니다. * BasicDecoder를 만들 때 maximum_iterations를 설정해야 합니다. 그렇지 않으면 무한하게 반복할 수 있습니다(적어도 하나의 시퀀스에서 모델이 end_token을 출력하지 않는다면). 이렇게 되면 주피터 커널을 재시작해야 합니다. * 모든 디코더 입력이 이전 타임 스텝의 출력을 기반으로 동적으로 생성되기 때문에 디코더 입력은 더 이상 필요하지 않습니다. * 모델의 출력은 final_outputs.rnn_outputs의 소프트맥스가 아니라 final_outputs.sample_id입니다. 로짓 값을 얻고 싶다면 final_outputs.sample_id을 final_outputs.rnn_outputs으로 바꾸세요.\n이제 이 모델을 사용하는 간단한 함수를 작성하여 날짜 포맷 변환을 수행할 수 있습니다:\n\ndef fast_predict_date_strs(date_strs):\n    X = prepare_date_strs_padded(date_strs)\n    Y_pred = inference_model.predict(X)\n    return ids_to_date_strs(Y_pred)\n\n\nfast_predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])\n\n['1789-07-14', '2020-05-01']\n\n\n속도를 확인해 보죠:\n\n%timeit predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])\n\n1 loop, best of 5: 383 ms per loop\n\n\n\n%timeit fast_predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])\n\n10 loops, best of 5: 38.4 ms per loop\n\n\n10배 이상 빠릅니다! 긴 시퀀스를 다룰 때 속도는 더 차이가 날 것입니다.\n\n\n네 번째 버전: 스케줄 샘플러를 사용하는 TF-Addons의 seq2seq 구현\n경고: TF 버그 때문에 이 버전은 텐서플로 2.2 이상에서만 동작합니다.\n이전 모델을 훈련할 때 매 타임 스텝 _t_에서 타임 스텝 t-1의 타깃 토큰을 모델에게 전달합니다. 하지만 추론에서는 모델이 타임 스텝마다 이전 타깃을 얻을 수 없습니다. 대신에 이전 예측을 사용합니다. 따라서 이런 훈련과 추론 사이에 차이가 실망스러운 성능으로 이어질 수 있습니다. 이를 완화하기 위해 훈련하는 동안 타깃을 예측으로 점진적으로 바꿀 수 있습니다. 이렇게 하려면 TrainingSampler를 ScheduledEmbeddingTrainingSampler를 바꾸기만 하면 됩니다. 그리고 sampling_probability(디코더가 이전 타임 스텝의 타깃 대신에 이전 타임 스텝의 예측을 사용할 확률)를 점진적으로 증가시키기 위해 케라스 콜백을 사용합니다.\n\nimport tensorflow_addons as tfa\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nn_epochs = 20\nencoder_embedding_size = 32\ndecoder_embedding_size = 32\nunits = 128\n\nencoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\ndecoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\nsequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n\nencoder_embeddings = keras.layers.Embedding(\n    len(INPUT_CHARS) + 1, encoder_embedding_size)(encoder_inputs)\n\ndecoder_embedding_layer = keras.layers.Embedding(\n    len(OUTPUT_CHARS) + 2, decoder_embedding_size)\ndecoder_embeddings = decoder_embedding_layer(decoder_inputs)\n\nencoder = keras.layers.LSTM(units, return_state=True)\nencoder_outputs, state_h, state_c = encoder(encoder_embeddings)\nencoder_state = [state_h, state_c]\n\nsampler = tfa.seq2seq.sampler.ScheduledEmbeddingTrainingSampler(\n    sampling_probability=0.,\n    embedding_fn=decoder_embedding_layer)\n# sampler를 만들 다음 sampling_probability를 지정해야 합니다.\n# (see https://github.com/tensorflow/addons/pull/1714)\nsampler.sampling_probability = tf.Variable(0.)\n\ndecoder_cell = keras.layers.LSTMCell(units)\noutput_layer = keras.layers.Dense(len(OUTPUT_CHARS) + 1)\n\ndecoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell,\n                                                 sampler,\n                                                 output_layer=output_layer)\nfinal_outputs, final_state, final_sequence_lengths = decoder(\n    decoder_embeddings,\n    initial_state=encoder_state)\nY_proba = keras.layers.Activation(\"softmax\")(final_outputs.rnn_output)\n\nmodel = keras.models.Model(inputs=[encoder_inputs, decoder_inputs],\n                           outputs=[Y_proba])\noptimizer = keras.optimizers.Nadam()\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n              metrics=[\"accuracy\"])\n\ndef update_sampling_probability(epoch, logs):\n    proba = min(1.0, epoch / (n_epochs - 10))\n    sampler.sampling_probability.assign(proba)\n\nsampling_probability_cb = keras.callbacks.LambdaCallback(\n    on_epoch_begin=update_sampling_probability)\nhistory = model.fit([X_train, X_train_decoder], Y_train, epochs=n_epochs,\n                    validation_data=([X_valid, X_valid_decoder], Y_valid),\n                    callbacks=[sampling_probability_cb])\n\nEpoch 1/20\n313/313 [==============================] - 17s 42ms/step - loss: 1.6779 - accuracy: 0.3658 - val_loss: 1.4628 - val_accuracy: 0.4332\nEpoch 2/20\n313/313 [==============================] - 13s 41ms/step - loss: 1.4142 - accuracy: 0.4476 - val_loss: 1.3384 - val_accuracy: 0.4717\nEpoch 3/20\n313/313 [==============================] - 13s 41ms/step - loss: 1.1022 - accuracy: 0.5820 - val_loss: 0.8807 - val_accuracy: 0.6834\nEpoch 4/20\n313/313 [==============================] - 13s 42ms/step - loss: 0.6877 - accuracy: 0.7457 - val_loss: 0.4740 - val_accuracy: 0.8270\nEpoch 5/20\n313/313 [==============================] - 13s 41ms/step - loss: 0.3614 - accuracy: 0.8761 - val_loss: 0.2645 - val_accuracy: 0.9165\nEpoch 6/20\n313/313 [==============================] - 13s 41ms/step - loss: 0.2485 - accuracy: 0.9265 - val_loss: 0.1711 - val_accuracy: 0.9531\nEpoch 7/20\n313/313 [==============================] - 13s 41ms/step - loss: 0.1698 - accuracy: 0.9549 - val_loss: 0.1240 - val_accuracy: 0.9680\nEpoch 8/20\n313/313 [==============================] - 13s 41ms/step - loss: 0.0876 - accuracy: 0.9795 - val_loss: 0.0648 - val_accuracy: 0.9867\nEpoch 9/20\n313/313 [==============================] - 13s 41ms/step - loss: 0.0582 - accuracy: 0.9883 - val_loss: 0.0427 - val_accuracy: 0.9918\nEpoch 10/20\n313/313 [==============================] - 13s 42ms/step - loss: 0.0326 - accuracy: 0.9941 - val_loss: 0.0275 - val_accuracy: 0.9958\nEpoch 11/20\n313/313 [==============================] - 13s 41ms/step - loss: 0.0207 - accuracy: 0.9970 - val_loss: 0.0204 - val_accuracy: 0.9967\nEpoch 12/20\n313/313 [==============================] - 13s 40ms/step - loss: 0.0140 - accuracy: 0.9982 - val_loss: 0.0138 - val_accuracy: 0.9981\nEpoch 13/20\n313/313 [==============================] - 13s 41ms/step - loss: 0.1035 - accuracy: 0.9782 - val_loss: 0.0370 - val_accuracy: 0.9951\nEpoch 14/20\n313/313 [==============================] - 12s 40ms/step - loss: 0.0167 - accuracy: 0.9981 - val_loss: 0.0115 - val_accuracy: 0.9987\nEpoch 15/20\n313/313 [==============================] - 12s 40ms/step - loss: 0.0083 - accuracy: 0.9992 - val_loss: 0.0076 - val_accuracy: 0.9992\nEpoch 16/20\n313/313 [==============================] - 13s 41ms/step - loss: 0.0056 - accuracy: 0.9995 - val_loss: 0.0054 - val_accuracy: 0.9996\nEpoch 17/20\n313/313 [==============================] - 13s 41ms/step - loss: 0.0043 - accuracy: 0.9996 - val_loss: 0.0041 - val_accuracy: 0.9997\nEpoch 18/20\n313/313 [==============================] - 13s 41ms/step - loss: 0.0029 - accuracy: 0.9998 - val_loss: 0.0034 - val_accuracy: 0.9995\nEpoch 19/20\n313/313 [==============================] - 13s 41ms/step - loss: 0.0024 - accuracy: 0.9998 - val_loss: 0.0023 - val_accuracy: 0.9999\nEpoch 20/20\n313/313 [==============================] - 13s 41ms/step - loss: 0.0033 - accuracy: 0.9995 - val_loss: 0.0026 - val_accuracy: 0.9998\n\n\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_5/basic_decoder_3/decoder/while/gradients/model_5/basic_decoder_3/decoder/while/cond_1_grad/Identity_4:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_5/basic_decoder_3/decoder/while/gradients/model_5/basic_decoder_3/decoder/while/cond_1_grad/Identity_3:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_5/basic_decoder_3/decoder/while/gradients/model_5/basic_decoder_3/decoder/while/cond_1_grad/Identity_5:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n  \"shape. This may consume a large amount of memory.\" % value)\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_5/basic_decoder_3/decoder/while/gradients/model_5/basic_decoder_3/decoder/while/cond_grad/gradients/grad_ys_0_indices:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_5/basic_decoder_3/decoder/while/gradients/model_5/basic_decoder_3/decoder/while/cond_grad/gradients/grad_ys_0_values:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_5/basic_decoder_3/decoder/while/gradients/model_5/basic_decoder_3/decoder/while/cond_grad/gradients/grad_ys_0_shape:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n  \"shape. This may consume a large amount of memory.\" % value)\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_5/basic_decoder_3/decoder/while/gradients/model_5/basic_decoder_3/decoder/while/cond_grad/Identity_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_5/basic_decoder_3/decoder/while/gradients/model_5/basic_decoder_3/decoder/while/cond_grad/Identity:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_5/basic_decoder_3/decoder/while/gradients/model_5/basic_decoder_3/decoder/while/cond_grad/Identity_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n  \"shape. This may consume a large amount of memory.\" % value)\n\n\n검증 정확도가 100%는 아니지만 충분히 가깝습니다!\n추론에서도 GreedyEmbeddingSampler를 사용해 앞에서와 동일한 작업을 수행할 수 있습니다. 하지만 완성도를 높이기 위해 SampleEmbeddingSampler를 사용하겠습니다. 토큰 ID를 찾기 위해 모델 출력에 argmax를 적용하는 대신 로짓 출력에서 랜덤하게 토큰 ID를 샘플링하는 것만 다르고 거의 동일합니다. 텍스트를 생성하는 작업에 유용합니다. softmax_temperature 매개변수는 세익스피어와 같은 텍스트를 생성했을 때와 같은 목적을 가집니다(이 매개변수 값이 높을수록 더 랜덤한 텍스트가 생성됩니다).\n\nsoftmax_temperature = tf.Variable(1.)\n\ninference_sampler = tfa.seq2seq.sampler.SampleEmbeddingSampler(\n    embedding_fn=decoder_embedding_layer,\n    softmax_temperature=softmax_temperature)\ninference_decoder = tfa.seq2seq.basic_decoder.BasicDecoder(\n    decoder_cell, inference_sampler, output_layer=output_layer,\n    maximum_iterations=max_output_length)\nbatch_size = tf.shape(encoder_inputs)[:1]\nstart_tokens = tf.fill(dims=batch_size, value=sos_id)\nfinal_outputs, final_state, final_sequence_lengths = inference_decoder(\n    start_tokens,\n    initial_state=encoder_state,\n    start_tokens=start_tokens,\n    end_token=0)\n\ninference_model = keras.models.Model(inputs=[encoder_inputs],\n                                     outputs=[final_outputs.sample_id])\n\n\ndef creative_predict_date_strs(date_strs, temperature=1.0):\n    softmax_temperature.assign(temperature)\n    X = prepare_date_strs_padded(date_strs)\n    Y_pred = inference_model.predict(X)\n    return ids_to_date_strs(Y_pred)\n\n\ntf.random.set_seed(42)\n\ncreative_predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])\n\n['1789-07-14', '2020-05-00']\n\n\n기본 온도에서 날짜가 괜찮은 것 같군요. 온도를 조금 더 올려 보죠:\n\ntf.random.set_seed(42)\n\ncreative_predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"],\n                           temperature=5.)\n\n['7479307-19', '200040?400']\n\n\n이런 날짜가 너무 랜덤하네요. “창의적인” 날짜라고 부르죠.\n\n\n다섯 번째 버전: TFA seq2seq, 케라스 서브클래싱 API, 어텐션 메커니즘 사용하기\n이 문제의 시퀀스는 꽤 짧지만 긴 시퀀스를 처리하려면 어텐션 메커니즘을 사용해야 할 것입니다. 직접 어텐션 메커니즘을 구현할 수 있지만 TF-Addons에 있는 구현을 사용하는 것이 더 간단하고 효율적입니다. 케라스 서브클래싱 API를 사용해서 만들어 보죠.\n경고: 텐서플로 버그(이슈 참조) 때문에 즉시 실행 모드(eager mode)에서 get_initial_state() 메서드가 실패합니다. 따라서 지금은 call() 메서드에서 tf.function()을 자동으로 호출하는 (따라서 그래프 모드로 실행하는) 케라스 서브클래싱 API를 사용해야 합니다.\n이 구현에서는 간단하게 만들기 위해 다시 TrainingSampler를 사용합니다(하지만 ScheduledEmbeddingTrainingSampler를 사용해 쉽게 바꿀 수 있습니다). 추론에는 GreedyEmbeddingSampler를 사용합니다:\n\nclass DateTranslation(keras.models.Model):\n    def __init__(self, units=128, encoder_embedding_size=32,\n                 decoder_embedding_size=32, **kwargs):\n        super().__init__(**kwargs)\n        self.encoder_embedding = keras.layers.Embedding(\n            input_dim=len(INPUT_CHARS) + 1,\n            output_dim=encoder_embedding_size)\n        self.encoder = keras.layers.LSTM(units,\n                                         return_sequences=True,\n                                         return_state=True)\n        self.decoder_embedding = keras.layers.Embedding(\n            input_dim=len(OUTPUT_CHARS) + 2,\n            output_dim=decoder_embedding_size)\n        self.attention = tfa.seq2seq.LuongAttention(units)\n        decoder_inner_cell = keras.layers.LSTMCell(units)\n        self.decoder_cell = tfa.seq2seq.AttentionWrapper(\n            cell=decoder_inner_cell,\n            attention_mechanism=self.attention)\n        output_layer = keras.layers.Dense(len(OUTPUT_CHARS) + 1)\n        self.decoder = tfa.seq2seq.BasicDecoder(\n            cell=self.decoder_cell,\n            sampler=tfa.seq2seq.sampler.TrainingSampler(),\n            output_layer=output_layer)\n        self.inference_decoder = tfa.seq2seq.BasicDecoder(\n            cell=self.decoder_cell,\n            sampler=tfa.seq2seq.sampler.GreedyEmbeddingSampler(\n                embedding_fn=self.decoder_embedding),\n            output_layer=output_layer,\n            maximum_iterations=max_output_length)\n\n    def call(self, inputs, training=None):\n        encoder_input, decoder_input = inputs\n        encoder_embeddings = self.encoder_embedding(encoder_input)\n        encoder_outputs, encoder_state_h, encoder_state_c = self.encoder(\n            encoder_embeddings,\n            training=training)\n        encoder_state = [encoder_state_h, encoder_state_c]\n\n        self.attention(encoder_outputs,\n                       setup_memory=True)\n        \n        decoder_embeddings = self.decoder_embedding(decoder_input)\n\n        decoder_initial_state = self.decoder_cell.get_initial_state(\n            decoder_embeddings)\n        decoder_initial_state = decoder_initial_state.clone(\n            cell_state=encoder_state)\n        \n        if training:\n            decoder_outputs, _, _ = self.decoder(\n                decoder_embeddings,\n                initial_state=decoder_initial_state,\n                training=training)\n        else:\n            start_tokens = tf.zeros_like(encoder_input[:, 0]) + sos_id\n            decoder_outputs, _, _ = self.inference_decoder(\n                decoder_embeddings,\n                initial_state=decoder_initial_state,\n                start_tokens=start_tokens,\n                end_token=0)\n\n        return tf.nn.softmax(decoder_outputs.rnn_output)\n\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nmodel = DateTranslation()\noptimizer = keras.optimizers.Nadam()\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n              metrics=[\"accuracy\"])\nhistory = model.fit([X_train, X_train_decoder], Y_train, epochs=25,\n                    validation_data=([X_valid, X_valid_decoder], Y_valid))\n\nEpoch 1/25\n313/313 [==============================] - 19s 42ms/step - loss: 2.1368 - accuracy: 0.2335 - val_loss: 2.0080 - val_accuracy: 0.2648\nEpoch 2/25\n313/313 [==============================] - 13s 41ms/step - loss: 1.8487 - accuracy: 0.3307 - val_loss: 1.5100 - val_accuracy: 0.4396\nEpoch 3/25\n313/313 [==============================] - 13s 40ms/step - loss: 2.1037 - accuracy: 0.2437 - val_loss: 1.6046 - val_accuracy: 0.3954\nEpoch 4/25\n313/313 [==============================] - 13s 40ms/step - loss: 1.7171 - accuracy: 0.3651 - val_loss: 2.5416 - val_accuracy: 0.2658\nEpoch 5/25\n313/313 [==============================] - 13s 40ms/step - loss: 1.4480 - accuracy: 0.4810 - val_loss: 1.3507 - val_accuracy: 0.5063\nEpoch 6/25\n313/313 [==============================] - 13s 40ms/step - loss: 1.3200 - accuracy: 0.5156 - val_loss: 1.2034 - val_accuracy: 0.5402\nEpoch 7/25\n313/313 [==============================] - 13s 40ms/step - loss: 1.1148 - accuracy: 0.5612 - val_loss: 1.1936 - val_accuracy: 0.5586\nEpoch 8/25\n313/313 [==============================] - 13s 40ms/step - loss: 0.9277 - accuracy: 0.5986 - val_loss: 0.9126 - val_accuracy: 0.6054\nEpoch 9/25\n313/313 [==============================] - 13s 40ms/step - loss: 0.8577 - accuracy: 0.6169 - val_loss: 0.8899 - val_accuracy: 0.6176\nEpoch 10/25\n313/313 [==============================] - 13s 40ms/step - loss: 0.7644 - accuracy: 0.6612 - val_loss: 0.7695 - val_accuracy: 0.6752\nEpoch 11/25\n313/313 [==============================] - 12s 40ms/step - loss: 0.7101 - accuracy: 0.6922 - val_loss: 0.7124 - val_accuracy: 0.6978\nEpoch 12/25\n313/313 [==============================] - 13s 40ms/step - loss: 0.6503 - accuracy: 0.7088 - val_loss: 0.6945 - val_accuracy: 0.7086\nEpoch 13/25\n313/313 [==============================] - 13s 40ms/step - loss: 0.6199 - accuracy: 0.7190 - val_loss: 0.6227 - val_accuracy: 0.7230\nEpoch 14/25\n313/313 [==============================] - 13s 40ms/step - loss: 0.6372 - accuracy: 0.7171 - val_loss: 0.6330 - val_accuracy: 0.7210\nEpoch 15/25\n313/313 [==============================] - 13s 40ms/step - loss: 0.5939 - accuracy: 0.7314 - val_loss: 0.6056 - val_accuracy: 0.7382\nEpoch 16/25\n313/313 [==============================] - 13s 40ms/step - loss: 0.6529 - accuracy: 0.7228 - val_loss: 0.5973 - val_accuracy: 0.7352\nEpoch 17/25\n313/313 [==============================] - 13s 40ms/step - loss: 0.5760 - accuracy: 0.7416 - val_loss: 0.5807 - val_accuracy: 0.7454\nEpoch 18/25\n313/313 [==============================] - 12s 40ms/step - loss: 0.5532 - accuracy: 0.7517 - val_loss: 0.5717 - val_accuracy: 0.7523\nEpoch 19/25\n313/313 [==============================] - 13s 40ms/step - loss: 0.5168 - accuracy: 0.7727 - val_loss: 0.8258 - val_accuracy: 0.7143\nEpoch 20/25\n313/313 [==============================] - 13s 40ms/step - loss: 0.3717 - accuracy: 0.8395 - val_loss: 0.7097 - val_accuracy: 0.7858\nEpoch 21/25\n313/313 [==============================] - 13s 40ms/step - loss: 0.2551 - accuracy: 0.9105 - val_loss: 0.3303 - val_accuracy: 0.9349\nEpoch 22/25\n313/313 [==============================] - 13s 41ms/step - loss: 0.1716 - accuracy: 0.9569 - val_loss: 0.2559 - val_accuracy: 0.9586\nEpoch 23/25\n313/313 [==============================] - 13s 40ms/step - loss: 0.0999 - accuracy: 0.9795 - val_loss: 0.1478 - val_accuracy: 0.9835\nEpoch 24/25\n313/313 [==============================] - 13s 40ms/step - loss: 0.0724 - accuracy: 0.9858 - val_loss: 0.1377 - val_accuracy: 0.9775\nEpoch 25/25\n313/313 [==============================] - 13s 40ms/step - loss: 0.0576 - accuracy: 0.9853 - val_loss: 0.0527 - val_accuracy: 0.9877\n\n\n100% 검증 정확도는 아니지만 매우 가깝습니다. 수렴하는데 조금 오래 걸렸지만 반복마다 파라미터와 계산량이 많습니다. 그리고 스케줄 샘플러를 사용하지 않았습니다\n이 모델을 사용하기 위해 또 다른 작은 함수를 만듭니다:\n\ndef fast_predict_date_strs_v2(date_strs):\n    X = prepare_date_strs_padded(date_strs)\n    X_decoder = tf.zeros(shape=(len(X), max_output_length), dtype=tf.int32)\n    Y_probas = model.predict([X, X_decoder])\n    Y_pred = tf.argmax(Y_probas, axis=-1)\n    return ids_to_date_strs(Y_pred)\n\n\nfast_predict_date_strs_v2([\"July 14, 1789\", \"May 01, 2020\"])\n\n['1789-06-14', '181805-015']\n\n\nTF-Addons에는 몇 가지 흥미로운 기능이 있습니다: * 추론에 BasicDecoder 대신 BeamSearchDecoder를 사용하면 가장 높은 확률의 문자를 출력하는 대신 디코더가 몇 개의 후보 중에서 가장 가능성 있는 시퀀스만 유지합니다(자세한 내용은 책 16장을 참고하세요). * 입력이나 타깃 시퀀스의 길이가 매우 다르면 마스크를 설정하거나 sequence_length를 지정합니다. * ScheduledEmbeddingTrainingSampler 보다 더 유연한 ScheduledOutputTrainingSampler을 사용하여 타임 스텝 _t_의 출력을 타임 스텝 t+1에 주입하는 방법을 결정합니다. 기본적으로 argmax로 ID를 찾지 않고 임베딩 층에 통과시켜 출력을 셀에 바로 주입합니다. 또는 next_inputs_fn 함수를 지정하여 셀 출력을 다음 스텝의 입력으로 변환할 수 있습니다."
  },
  {
    "objectID": "Machine_Learning/16_nlp_with_rnns_and_attention.html#section-2",
    "href": "Machine_Learning/16_nlp_with_rnns_and_attention.html#section-2",
    "title": "16_nlp_with_rnns_and_attention",
    "section": "10.",
    "text": "10.\n연습문제: 텐서플로의 Neural Machine Translation with Attention(어텐션을 사용한 신경망 기계 번역) 튜토리얼을 살펴 보세요.\n코랩에서 페이지를 열고 설명을 따라 하세요. 또는 TF-Addons의 seq2seq 구현을 사용한 간단한 신경망 기계 번역 예제를 원하면 이전 문제의 솔루션을 확인하세요. 마지막 모델이 TF-Addons을 사용해 어텐션 메커니즘으로 NMT 모델을 만드는 간단한 예를 볼 수 있습니다."
  },
  {
    "objectID": "Machine_Learning/16_nlp_with_rnns_and_attention.html#section-3",
    "href": "Machine_Learning/16_nlp_with_rnns_and_attention.html#section-3",
    "title": "16_nlp_with_rnns_and_attention",
    "section": "11.",
    "text": "11.\n연습문제: 최신 언어 모델 중 하나(예를 들어 BERT)로 세익스피어가 쓴 것 같은 텍스트를 생성해보세요.\n최신 언어 모델을 사용하는 가장 간단한 방법은 허깅 페이스의 오픈 소스 라이브러리인 트랜스포머스를 사용하는 것입니다. 이 라이브러리는 자연어 처리(NLP)를 위한 최신 신경망 구조(BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet 등)와 사전훈련된 모델을 많이 제공합니다. 텐서플로와 파이토치를 지원합니다. 무엇보다도 사용하기 매우 쉽습니다.\n먼저 사전훈련된 모델을 로드해 보죠. 이 예제에서 추가적인 언어 모델(입력 임베딩에 연결된 가중치를 가진 선형층)을 위에 얹은 OpenAI의 GPT 모델을 사용하겠습니다. 모델을 임포트하고 사전훈련된 가중치를 로드합니다(약 445MB의 데이터가 ~/.cache/torch/transformers에 다운로드됩니다):\n\nfrom transformers import TFOpenAIGPTLMHeadModel\n\nmodel = TFOpenAIGPTLMHeadModel.from_pretrained(\"openai-gpt\")\n\n\n\n\n\n\n\nAll model checkpoint layers were used when initializing TFOpenAIGPTLMHeadModel.\n\nAll the layers of TFOpenAIGPTLMHeadModel were initialized from the model checkpoint at openai-gpt.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFOpenAIGPTLMHeadModel for predictions without further training.\n\n\n그다음 이 모델을 위한 특별한 토크나이저(tokenizer)가 필요합니다. spaCy와 ftfy가 설치되어 있으면 이를 사용하고 아니면 버트(Bert)의 BasicTokenizer 다음에 바이트-페어 인코딩(Byte-Pair Encoding)을 사용합니다(대부분의 경우에 잘 맞습니다).\n\nfrom transformers import OpenAIGPTTokenizer\n\ntokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\n\n\n\n\n\n\n\n\n\n\nftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n\n\n이 토크나이저를 사용해 시작 텍스트를 토큰화하고 인코딩해 보죠:\n\nprompt_text = \"This royal throne of kings, this sceptred isle\"\nencoded_prompt = tokenizer.encode(prompt_text,\n                                  add_special_tokens=False,\n                                  return_tensors=\"tf\")\nencoded_prompt\n\n&lt;tf.Tensor: shape=(1, 10), dtype=int32, numpy=\narray([[  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n        16187]], dtype=int32)&gt;\n\n\n쉽군요! 그다음 이 모델을 사용해 시작 텍스트 다음에 이어지는 텍스트를 생성해 보겠습니다. 시작 텍스트 다음에 40개의 토큰을 이어서 다섯 개의 다른 문장을 생성합니다. 하이퍼파라미터에 대한 자세한 내용은 (허깅 페이스) Patrick von Platen의 블로그를 참고하세요. 더 나은 결과를 얻기 위해 하이퍼파라미터를 조정해 볼 수 있습니다.\n\nnum_sequences = 5\nlength = 40\n\ngenerated_sequences = model.generate(\n    input_ids=encoded_prompt,\n    do_sample=True,\n    max_length=length + len(encoded_prompt[0]),\n    temperature=1.0,\n    top_k=0,\n    top_p=0.9,\n    repetition_penalty=1.0,\n    num_return_sequences=num_sequences,\n)\n\ngenerated_sequences\n\n&lt;tf.Tensor: shape=(5, 50), dtype=int32, numpy=\narray([[  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n        16187,   240,   509,   481,  9313,  6640,   498,  1389, 11031,\n          239,   481,  2204,   544,   525,   481,  4906,  3659,   498,\n          481,   653,   911,   498,     8, 38648,   641,  1236,   481,\n         5018,   498,   481, 21368,   488,   481,  6404,   948, 35174,\n          715,  1076,   763,   641,  2520],\n       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n        16187,   980,  1981,   557,   481,  2827,   498,   481,  1028,\n          498,  9606,   239,   244, 40477,   244,   862,  1256,   240,\n          547,  2185,   239,   244,  7395, 21800,   513,  2185,   239,\n          244,   599,   636,   512,   649,   485,   788,   257,   244,\n        40477,   481,  2228,   535,   741],\n       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n        16187,   239,   481,  3549,   498,   589,   547, 16375,   240,\n         4258,  1076,   498,   547,  5080,   260,  2228,   240,  1485,\n         6945,  2034,   547, 13509,   239,   256, 40477,   256,  1504,\n          240,   256,   603,   481,   618,   240,   256,   249,  1259,\n        20024,   547, 11244,   240,   488],\n       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n        16187,   487,   509,  3105,   500,   240,   616,   908,   487,\n          558,  2160,   781,   575,   240,   507,   544,  6322,   500,\n          481,  1279,   498,  2857, 24711,   504,   481,  7361,  2075,\n          498,   481, 16187,   240,   524,  1584,   759,   580,  1132,\n          822,   481,  4644,   498,  2857],\n       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n        16187,   240, 40477,   488,   481,  1922,  4198,   535,  8908,\n          240,   834,   240,   481,  2185,   498,  1092,   239, 40477,\n          244,   500,   481,  1385,   498,   481,  2878, 13658,   240,\n         2122,  2821,  1085,   589,  8162,   240,   244, 40477,   655,\n          544,   597,  1203,  3126,   500]], dtype=int32)&gt;\n\n\n생성한 시퀀스를 디코딩하여 출력해 보죠:\n\nfor sequence in generated_sequences:\n    text = tokenizer.decode(sequence, clean_up_tokenization_spaces=True)\n    print(text)\n    print(\"-\" * 80)\n\nthis royal throne of kings, this sceptred isle, was the largest collection of such affairs. the problem is that the descendents of the earls of astoria were under the rule of the sages and the throne took precedence over those who were forced\n--------------------------------------------------------------------------------\nthis royal throne of kings, this sceptred isle has passed as the beginning of the age of kings. \" \n \" well done, my lord. \" velvet complimented her lord. \" what would you like to see? \" \n the lady's eyes\n--------------------------------------------------------------------------------\nthis royal throne of kings, this sceptred isle. the bones of all my comrades, including those of my ex - lady, lie crushed upon my battlefield.'\n'ah,'said the king,'i must consult my contacts, and\n--------------------------------------------------------------------------------\nthis royal throne of kings, this sceptred isle he was born in, this door he had placed before him, it is located in the heart of galdir on the outer edge of the isle, his line can be found through the houses of gal\n--------------------------------------------------------------------------------\nthis royal throne of kings, this sceptred isle, \n and the pendragon's portal, too, the lord of light. \n \" in the course of the seven pillars, ye shall find all treasure, \" \n there is now three bodies in\n--------------------------------------------------------------------------------\n\n\nGPT-2, CTRL, Transformer-XL, XLNet와 같이 더 최신의 (그리고 더 큰) 모델을 시도해 볼 수 있습니다. 다양한 언어 모델과 함께 트랜스포머스 라이브러리에 모두 사전훈련된 모델로 준비되어 있습니다. 모델마다 전처리 단계는 조금씩 다르므로 트랜스포머스 문서에 있는 생성 예제를 참고하세요(이 예제는 파이토치를 사용하지만 모델 클래스 이름의 시작 부분을 TF로 바꾸고 .to() 메서드 호출을 삭제하고 \"pt\" 대신에 return_tensors=\"tf\"를 사용하면 텐서플로를 사용할 수 있습니다).\n이 장이 재미있었기를 바랍니다! :)"
  },
  {
    "objectID": "Machine_Learning/18_reinforcement_learning.html",
    "href": "Machine_Learning/18_reinforcement_learning.html",
    "title": "18_reinforcement_learning",
    "section": "",
    "text": "18장 – 강화학습\n이 노트북은 18장에 있는 모든 샘플 코드를 담고 있습니다."
  },
  {
    "objectID": "Machine_Learning/18_reinforcement_learning.html#더블-dqn",
    "href": "Machine_Learning/18_reinforcement_learning.html#더블-dqn",
    "title": "18_reinforcement_learning",
    "section": "더블 DQN",
    "text": "더블 DQN\n\nkeras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nmodel = keras.models.Sequential([\n    keras.layers.Dense(32, activation=\"elu\", input_shape=[4]),\n    keras.layers.Dense(32, activation=\"elu\"),\n    keras.layers.Dense(n_outputs)\n])\n\ntarget = keras.models.clone_model(model)\ntarget.set_weights(model.get_weights())\n\n\nbatch_size = 32\ndiscount_rate = 0.95\noptimizer = keras.optimizers.Adam(learning_rate=6e-3)\nloss_fn = keras.losses.Huber()\n\ndef training_step(batch_size):\n    experiences = sample_experiences(batch_size)\n    states, actions, rewards, next_states, dones = experiences\n    next_Q_values = model.predict(next_states)\n    best_next_actions = np.argmax(next_Q_values, axis=1)\n    next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()\n    next_best_Q_values = (target.predict(next_states) * next_mask).sum(axis=1)\n    target_Q_values = (rewards + \n                       (1 - dones) * discount_rate * next_best_Q_values)\n    target_Q_values = target_Q_values.reshape(-1, 1)\n    mask = tf.one_hot(actions, n_outputs)\n    with tf.GradientTape() as tape:\n        all_Q_values = model(states)\n        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n    grads = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n\nreplay_memory = deque(maxlen=2000)\n\n\nenv.seed(42)\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nrewards = []\nbest_score = 0\n\nfor episode in range(600):\n    obs = env.reset()    \n    for step in range(200):\n        epsilon = max(1 - episode / 500, 0.01)\n        obs, reward, done, info = play_one_step(env, obs, epsilon)\n        if done:\n            break\n    rewards.append(step)\n    if step &gt;= best_score:\n        best_weights = model.get_weights()\n        best_score = step\n    print(\"\\rEpisode: {}, Steps: {}, eps: {:.3f}\".format(episode, step + 1, epsilon), end=\"\")\n    if episode &gt;= 50:\n        training_step(batch_size)\n        if episode % 50 == 0:\n            target.set_weights(model.get_weights())\n    # Alternatively, you can do soft updates at each step:\n    #if episode &gt;= 50:\n        #target_weights = target.get_weights()\n        #online_weights = model.get_weights()\n        #for index in range(len(target_weights)):\n        #    target_weights[index] = 0.99 * target_weights[index] + 0.01 * online_weights[index]\n        #target.set_weights(target_weights)\n\nmodel.set_weights(best_weights)\n\nEpisode: 599, Steps: 55, eps: 0.0100\n\n\n\nplt.figure(figsize=(8, 4))\nplt.plot(rewards)\nplt.xlabel(\"Episode\", fontsize=14)\nplt.ylabel(\"Sum of rewards\", fontsize=14)\nsave_fig(\"double_dqn_rewards_plot\")\nplt.show()\n\nSaving figure double_dqn_rewards_plot\n\n\n\n\n\n\nenv.seed(43)\nstate = env.reset()\n\nframes = []\n\nfor step in range(200):\n    action = epsilon_greedy_policy(state)\n    state, reward, done, info = env.step(action)\n    if done:\n        break\n    img = env.render(mode=\"rgb_array\")\n    frames.append(img)\n   \nplot_animation(frames)"
  },
  {
    "objectID": "Machine_Learning/18_reinforcement_learning.html#tf-agents-환경",
    "href": "Machine_Learning/18_reinforcement_learning.html#tf-agents-환경",
    "title": "18_reinforcement_learning",
    "section": "TF-Agents 환경",
    "text": "TF-Agents 환경\n\ntf.random.set_seed(42)\nnp.random.seed(42)\n\n\nfrom tf_agents.environments import suite_gym\n\nenv = suite_gym.load(\"Breakout-v4\")\nenv\n\n&lt;tf_agents.environments.wrappers.TimeLimit at 0x7fe46bf261d0&gt;\n\n\n\nenv.gym\n\n&lt;gym.envs.atari.atari_env.AtariEnv at 0x7fe46bdbba50&gt;\n\n\n\nenv.seed(42)\nenv.reset()\n\nTimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([[[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        ...,\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]],\n\n       [[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        ...,\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]],\n\n       [[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        ...,\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]],\n\n       ...,\n\n       [[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        ...,\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]],\n\n       [[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        ...,\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]],\n\n       [[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        ...,\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]]], dtype=uint8))\n\n\n\nenv.step(1) # Fire\n\nTimeStep(step_type=array(1, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([[[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        ...,\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]],\n\n       [[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        ...,\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]],\n\n       [[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        ...,\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]],\n\n       ...,\n\n       [[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        ...,\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]],\n\n       [[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        ...,\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]],\n\n       [[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        ...,\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]]], dtype=uint8))\n\n\n\nimg = env.render(mode=\"rgb_array\")\n\nplt.figure(figsize=(6, 8))\nplt.imshow(img)\nplt.axis(\"off\")\nsave_fig(\"breakout_plot\")\nplt.show()\n\nSaving figure breakout_plot\n\n\n\n\n\n\nenv.current_time_step()\n\nTimeStep(step_type=array(1, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([[[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        ...,\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]],\n\n       [[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        ...,\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]],\n\n       [[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        ...,\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]],\n\n       ...,\n\n       [[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        ...,\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]],\n\n       [[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        ...,\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]],\n\n       [[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        ...,\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]]], dtype=uint8))"
  },
  {
    "objectID": "Machine_Learning/18_reinforcement_learning.html#환경-스펙",
    "href": "Machine_Learning/18_reinforcement_learning.html#환경-스펙",
    "title": "18_reinforcement_learning",
    "section": "환경 스펙",
    "text": "환경 스펙\n\nenv.observation_spec()\n\nBoundedArraySpec(shape=(210, 160, 3), dtype=dtype('uint8'), name='observation', minimum=0, maximum=255)\n\n\n\nenv.action_spec()\n\nBoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=3)\n\n\n\nenv.time_step_spec()\n\nTimeStep(step_type=ArraySpec(shape=(), dtype=dtype('int32'), name='step_type'), reward=ArraySpec(shape=(), dtype=dtype('float32'), name='reward'), discount=BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0), observation=BoundedArraySpec(shape=(210, 160, 3), dtype=dtype('uint8'), name='observation', minimum=0, maximum=255))"
  },
  {
    "objectID": "Machine_Learning/18_reinforcement_learning.html#환경-래퍼",
    "href": "Machine_Learning/18_reinforcement_learning.html#환경-래퍼",
    "title": "18_reinforcement_learning",
    "section": "환경 래퍼",
    "text": "환경 래퍼\nTF-Agents 래퍼로 TF-Agents 환경을 감쌀 수 있습니다:\n\nfrom tf_agents.environments.wrappers import ActionRepeat\n\nrepeating_env = ActionRepeat(env, times=4)\nrepeating_env\n\n&lt;tf_agents.environments.wrappers.ActionRepeat at 0x7fe46872cad0&gt;\n\n\n\nrepeating_env.unwrapped\n\n&lt;gym.envs.atari.atari_env.AtariEnv at 0x7fe46bdbba50&gt;\n\n\n가능한 래퍼 목록은 다음과 같습니다:\n\nimport tf_agents.environments.wrappers\n\nfor name in dir(tf_agents.environments.wrappers):\n    obj = getattr(tf_agents.environments.wrappers, name)\n    if hasattr(obj, \"__base__\") and issubclass(obj, tf_agents.environments.wrappers.PyEnvironmentBaseWrapper):\n        print(\"{:27s} {}\".format(name, obj.__doc__.split(\"\\n\")[0]))\n\nActionClipWrapper           Wraps an environment and clips actions to spec before applying.\nActionDiscretizeWrapper     Wraps an environment with continuous actions and discretizes them.\nActionOffsetWrapper         Offsets actions to be zero-based.\nActionRepeat                Repeates actions over n-steps while acummulating the received reward.\nFlattenObservationsWrapper  Wraps an environment and flattens nested multi-dimensional observations.\nGoalReplayEnvWrapper        Adds a goal to the observation, used for HER (Hindsight Experience Replay).\nHistoryWrapper              Adds observation and action history to the environment's observations.\nObservationFilterWrapper    Filters observations based on an array of indexes.\nOneHotActionWrapper         Converts discrete action to one_hot format.\nPerformanceProfiler         End episodes after specified number of steps.\nPyEnvironmentBaseWrapper    PyEnvironment wrapper forwards calls to the given environment.\nRunStats                    Wrapper that accumulates run statistics as the environment iterates.\nTimeLimit                   End episodes after specified number of steps.\n\n\nsuite_gym.load()는 TF-Agents 환경 래퍼와 짐 환경 래퍼로 환경을 만들고 래핑합니다(후자가 먼저 적용됩니다).\n\nfrom functools import partial\nfrom gym.wrappers import TimeLimit\n\nlimited_repeating_env = suite_gym.load(\n    \"Breakout-v4\",\n    gym_env_wrappers=[partial(TimeLimit, max_episode_steps=10000)],\n    env_wrappers=[partial(ActionRepeat, times=4)],\n)\n\n\nlimited_repeating_env\n\n&lt;tf_agents.environments.wrappers.ActionRepeat at 0x7fe4686ff550&gt;\n\n\n\nlimited_repeating_env.unwrapped\n\n&lt;gym.envs.atari.atari_env.AtariEnv at 0x7fe3de8b6c90&gt;\n\n\n아타리 브레이크아웃 환경을 만들고 기본 아타리 전처리 단계를 적용합니다:\n경고: 브레이크아웃은 게임 시작과 죽을 때마다 FIRE 버튼을 눌러야 합니다. 처음에는 FIRE 버튼을 누르는 것이 빨리 지는 것처럼 보이기 때문에 에이전트가 이를 배우는데 매우 오랜 시간이 걸릴 수 있습니다. 훈련 속도를 높이려면 AtariPreprocessing 래퍼 클래스를 상속하여 AtariPreprocessingWithAutoFire를 만들고 사용합니다. 이 클래스는 게임 시작과 말이 죽을 때마다 자동으로 FIRE(즉 플레이 행동 1)를 누릅니다. 일반적인 AtariPreprocessing 래퍼를 사용한 책의 코드와 다른 점입니다.\n\nfrom tf_agents.environments import suite_atari\nfrom tf_agents.environments.atari_preprocessing import AtariPreprocessing\nfrom tf_agents.environments.atari_wrappers import FrameStack4\n\nmax_episode_steps = 27000 # &lt;=&gt; 108k ALE frames since 1 step = 4 frames\nenvironment_name = \"BreakoutNoFrameskip-v4\"\n\nclass AtariPreprocessingWithAutoFire(AtariPreprocessing):\n    def reset(self, **kwargs):\n        obs = super().reset(**kwargs)\n        super().step(1) # FIRE to start\n        return obs\n    def step(self, action):\n        lives_before_action = self.ale.lives()\n        obs, rewards, done, info = super().step(action)\n        if self.ale.lives() &lt; lives_before_action and not done:\n            super().step(1) # FIRE to start after life lost\n        return obs, rewards, done, info\n\nenv = suite_atari.load(\n    environment_name,\n    max_episode_steps=max_episode_steps,\n    gym_env_wrappers=[AtariPreprocessingWithAutoFire, FrameStack4])\n\n\nenv\n\n&lt;tf_agents.environments.atari_wrappers.AtariTimeLimit at 0x7fe46bf46510&gt;\n\n\n몇 개의 스텝을 플레이하고 어떻게 동작하는지 확인합니다:\n\nenv.seed(42)\nenv.reset()\nfor _ in range(4):\n    time_step = env.step(3) # 왼쪽\n\n\ndef plot_observation(obs):\n    # 컬러 채널이 3개이기 때문에 4 프레임을 출력할 수 없습니다.\n    # 따라서 현재 프레임과 다른 프레임의 평균 값을 뺀 차이를 계산합니다.\n    # 그다음 이 차이를 현재 프레임의 빨강과 파랑 채널에 더해서 보라 색을 구합니다.\n    obs = obs.astype(np.float32)\n    img = obs[..., :3]\n    current_frame_delta = np.maximum(obs[..., 3] - obs[..., :3].mean(axis=-1), 0.)\n    img[..., 0] += current_frame_delta\n    img[..., 2] += current_frame_delta\n    img = np.clip(img / 150, 0, 1)\n    plt.imshow(img)\n    plt.axis(\"off\")\n\n\nplt.figure(figsize=(6, 6))\nplot_observation(time_step.observation)\nsave_fig(\"preprocessed_breakout_plot\")\nplt.show()\n\nSaving figure preprocessed_breakout_plot\n\n\n\n\n\n파이썬 환경을 TF 환경으로 변환합니다:\n\nfrom tf_agents.environments.tf_py_environment import TFPyEnvironment\n\ntf_env = TFPyEnvironment(env)"
  },
  {
    "objectID": "Machine_Learning/18_reinforcement_learning.html#dqn-만들기",
    "href": "Machine_Learning/18_reinforcement_learning.html#dqn-만들기",
    "title": "18_reinforcement_learning",
    "section": "DQN 만들기",
    "text": "DQN 만들기\n관측을 정규화하는 작은 클래스를 만듭니다. 이미지를 0~255 사이의 바이트로 저장하는 것이 램을 적게 사용하지만 신경망에는 0.0~1.0 사이의 실수를 전달해야 합니다:\nQ-네트워크를 만듭니다:\n\nfrom tf_agents.networks.q_network import QNetwork\n\npreprocessing_layer = keras.layers.Lambda(\n                          lambda obs: tf.cast(obs, np.float32) / 255.)\nconv_layer_params=[(32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 1)]\nfc_layer_params=[512]\n\nq_net = QNetwork(\n    tf_env.observation_spec(),\n    tf_env.action_spec(),\n    preprocessing_layers=preprocessing_layer,\n    conv_layer_params=conv_layer_params,\n    fc_layer_params=fc_layer_params)\n\nDQN 에이전트를 만듭니다:\n\nfrom tf_agents.agents.dqn.dqn_agent import DqnAgent\n\ntrain_step = tf.Variable(0)\nupdate_period = 4 # run a training step every 4 collect steps\noptimizer = keras.optimizers.RMSprop(learning_rate=2.5e-4, rho=0.95, momentum=0.0,\n                                     epsilon=0.00001, centered=True)\nepsilon_fn = keras.optimizers.schedules.PolynomialDecay(\n    initial_learning_rate=1.0, # initial ε\n    decay_steps=250000 // update_period, # &lt;=&gt; 1,000,000 ALE frames\n    end_learning_rate=0.01) # final ε\nagent = DqnAgent(tf_env.time_step_spec(),\n                 tf_env.action_spec(),\n                 q_network=q_net,\n                 optimizer=optimizer,\n                 target_update_period=2000, # &lt;=&gt; 32,000 ALE frames\n                 td_errors_loss_fn=keras.losses.Huber(reduction=\"none\"),\n                 gamma=0.99, # discount factor\n                 train_step_counter=train_step,\n                 epsilon_greedy=lambda: epsilon_fn(train_step))\nagent.initialize()\n\n재생 버퍼를 만듭니다(램을 많이 사용하기 때문에 메모리 부족 에러가 나오면 버퍼 크기를 줄이세요):\n경고: (책과 달리) 1,000,000이 아니고 100,000 크기의 재생 버퍼를 사용합니다. 대부분의 경우 메모리 부족 에러가 나기 때문입니다.\n\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\n\nreplay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n    data_spec=agent.collect_data_spec,\n    batch_size=tf_env.batch_size,\n    max_length=100000) # OOM 에러가 나면 줄이세요\n\nreplay_buffer_observer = replay_buffer.add_batch\n\n호출 횟수를 카운트하고 출력하는 간단한 사용자 정의 옵저버를 만듭니다(하나의 스텝으로 카운트하지 않는 두 에피소드 사이의 경계는 제외합니다):\n\nclass ShowProgress:\n    def __init__(self, total):\n        self.counter = 0\n        self.total = total\n    def __call__(self, trajectory):\n        if not trajectory.is_boundary():\n            self.counter += 1\n        if self.counter % 100 == 0:\n            print(\"\\r{}/{}\".format(self.counter, self.total), end=\"\")\n\n훈련 측정 지표를 추가해 보죠:\n\nfrom tf_agents.metrics import tf_metrics\n\ntrain_metrics = [\n    tf_metrics.NumberOfEpisodes(),\n    tf_metrics.EnvironmentSteps(),\n    tf_metrics.AverageReturnMetric(),\n    tf_metrics.AverageEpisodeLengthMetric(),\n]\n\n\ntrain_metrics[0].result()\n\n&lt;tf.Tensor: shape=(), dtype=int64, numpy=0&gt;\n\n\n\nfrom tf_agents.eval.metric_utils import log_metrics\nimport logging\nlogging.getLogger().setLevel(logging.INFO)\nlog_metrics(train_metrics)\n\nINFO:absl: \n         NumberOfEpisodes = 0\n         EnvironmentSteps = 0\n         AverageReturn = 0.0\n         AverageEpisodeLength = 0.0\n\n\n수집 드라이버를 만듭니다:\n\nfrom tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n\ncollect_driver = DynamicStepDriver(\n    tf_env,\n    agent.collect_policy,\n    observers=[replay_buffer_observer] + train_metrics,\n    num_steps=update_period) # collect 4 steps for each training iteration\n\n훈련 전에 초기 경험을 수집합니다:\n\nfrom tf_agents.policies.random_tf_policy import RandomTFPolicy\n\ninitial_collect_policy = RandomTFPolicy(tf_env.time_step_spec(),\n                                        tf_env.action_spec())\ninit_driver = DynamicStepDriver(\n    tf_env,\n    initial_collect_policy,\n    observers=[replay_buffer.add_batch, ShowProgress(20000)],\n    num_steps=20000) # &lt;=&gt; 80,000 ALE frames\nfinal_time_step, final_policy_state = init_driver.run()\n\n20000/20000\n\n\n3개의 스텝을 가진 2개의 서브 에피소드를 샘플링해서 출력해 보죠:\n노트: replay_buffer.get_next()는 deprecated 되었습니다. 대신 replay_buffer.as_dataset(..., single_deterministic_pass=False)를 사용해야 합니다.\n\ntf.random.set_seed(9) # 에피소드 끝에서 경로 샘플을 보여주기 위해\n\n#trajectories, buffer_info = replay_buffer.get_next( # get_next() is deprecated\n#    sample_batch_size=2, num_steps=3)\n\ntrajectories, buffer_info = next(iter(replay_buffer.as_dataset(\n    sample_batch_size=2,\n    num_steps=3,\n    single_deterministic_pass=False)))\n\n\ntrajectories._fields\n\n('step_type',\n 'observation',\n 'action',\n 'policy_info',\n 'next_step_type',\n 'reward',\n 'discount')\n\n\n\ntrajectories.observation.shape\n\nTensorShape([2, 3, 84, 84, 4])\n\n\n\nfrom tf_agents.trajectories.trajectory import to_transition\n\ntime_steps, action_steps, next_time_steps = to_transition(trajectories)\ntime_steps.observation.shape\n\nTensorShape([2, 2, 84, 84, 4])\n\n\n\ntrajectories.step_type.numpy()\n\narray([[1, 1, 1],\n       [1, 1, 1]], dtype=int32)\n\n\n\nplt.figure(figsize=(10, 6.8))\nfor row in range(2):\n    for col in range(3):\n        plt.subplot(2, 3, row * 3 + col + 1)\n        plot_observation(trajectories.observation[row, col].numpy())\nplt.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0, wspace=0.02)\nsave_fig(\"sub_episodes_plot\")\nplt.show()\n\nSaving figure sub_episodes_plot\n\n\n\n\n\n이제 데이터셋을 만들어 보죠:\n\ndataset = replay_buffer.as_dataset(\n    sample_batch_size=64,\n    num_steps=2,\n    num_parallel_calls=3).prefetch(3)\n\n성능을 높이기 위해 메인 함수를 TF 함수로 변환합니다:\n\nfrom tf_agents.utils.common import function\n\ncollect_driver.run = function(collect_driver.run)\nagent.train = function(agent.train)\n\n이제 메인 루프를 실행할 준비가 되었습니다!\n\ndef train_agent(n_iterations):\n    time_step = None\n    policy_state = agent.collect_policy.get_initial_state(tf_env.batch_size)\n    iterator = iter(dataset)\n    for iteration in range(n_iterations):\n        time_step, policy_state = collect_driver.run(time_step, policy_state)\n        trajectories, buffer_info = next(iterator)\n        train_loss = agent.train(trajectories)\n        print(\"\\r{} loss:{:.5f}\".format(\n            iteration, train_loss.loss.numpy()), end=\"\")\n        if iteration % 1000 == 0:\n            log_metrics(train_metrics)\n\n다음 셀에서 에이전트를 50,000 스텝 동안 훈련합니다. 그다음 다음 셀을 실행하여 에이전트의 동작을 살펴 보겠습니다. 이 두 셀을 원하는만큼 많이 실행할 수 있습니다. 에이전트는 점점 향상될 것입니다! 에이전트가 어느정도 좋은 동작을 수행하려면 200,000 반복 정도 걸릴 것입니다.\n\ntrain_agent(n_iterations=50000)\n\nWARNING:tensorflow:From /opt/conda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:201: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\nInstructions for updating:\nback_prop=False is deprecated. Consider using tf.stop_gradient instead.\nInstead of:\nresults = tf.foldr(fn, elems, back_prop=False)\nUse:\nresults = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n998 loss:0.000081998 loss:0.001812998 loss:0.00005&lt;&lt;244 more lines&gt;&gt;\n44998 loss:0.0016545998 loss:0.0013646998 loss:0.0010047998 loss:0.0011648998 loss:0.0004949999 loss:0.00073\n\n\nWARNING:tensorflow:From /opt/conda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:201: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\nInstructions for updating:\nback_prop=False is deprecated. Consider using tf.stop_gradient instead.\nInstead of:\nresults = tf.foldr(fn, elems, back_prop=False)\nUse:\nresults = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\nINFO:absl: \n         NumberOfEpisodes = 0\n         EnvironmentSteps = 4\n         AverageReturn = 0.0\n         AverageEpisodeLength = 0.0\nINFO:absl: \n         NumberOfEpisodes = 24\n         EnvironmentSteps = 4004\n         AverageReturn = 1.7000000476837158\n         AverageEpisodeLength = 184.1999969482422\nINFO:absl: \n         NumberOfEpisodes = 48\n         EnvironmentSteps = 8004\n         AverageReturn = 1.7000000476837158\n         AverageEpisodeLength = 182.39999389648438\nINFO:absl: \n         NumberOfEpisodes = 73\n         EnvironmentSteps = 12004\n         NumberOfEpisodes = 1003\n         EnvironmentSteps = 176004\n         AverageReturn = 5.099999904632568\n         AverageEpisodeLength = 246.5\nINFO:absl: \n         NumberOfEpisodes = 1019\n         EnvironmentSteps = 180004\n         AverageReturn = 5.199999809265137\n         AverageEpisodeLength = 256.6000061035156\nINFO:absl: \n         NumberOfEpisodes = 1035\n         EnvironmentSteps = 184004\n         AverageReturn = 4.599999904632568\n         AverageEpisodeLength = 252.1999969482422\nINFO:absl: \n         NumberOfEpisodes = 1050\n         EnvironmentSteps = 188004\n         AverageReturn = 5.699999809265137\n         AverageEpisodeLength = 276.5\nINFO:absl: \n         NumberOfEpisodes = 1063\n         EnvironmentSteps = 192004\n         AverageReturn = 5.900000095367432\n         AverageEpisodeLength = 296.3999938964844\nINFO:absl: \n         NumberOfEpisodes = 1077\n         EnvironmentSteps = 196004\n         AverageReturn = 7.800000190734863\n         AverageEpisodeLength = 308.29998779296875\n\n\n\nframes = []\ndef save_frames(trajectory):\n    global frames\n    frames.append(tf_env.pyenv.envs[0].render(mode=\"rgb_array\"))\n\nwatch_driver = DynamicStepDriver(\n    tf_env,\n    agent.policy,\n    observers=[save_frames, ShowProgress(1000)],\n    num_steps=1000)\nfinal_time_step, final_policy_state = watch_driver.run()\n\nplot_animation(frames)\n\n에이전트를 친구에게 보여주고 싶어서 애니메이션 GIF로 저장하고 싶다면 다음 방법을 사용하세요:\n\nimport PIL\n\nimage_path = os.path.join(\"images\", \"rl\", \"breakout.gif\")\nframe_images = [PIL.Image.fromarray(frame) for frame in frames[:150]]\nframe_images[0].save(image_path, format='GIF',\n                     append_images=frame_images[1:],\n                     save_all=True,\n                     duration=30,\n                     loop=0)\n\n\n%%html\n&lt;img src=\"images/rl/breakout.gif\" /&gt;"
  },
  {
    "objectID": "Machine_Learning/18_reinforcement_learning.html#deque-vs-로테이팅-리스트",
    "href": "Machine_Learning/18_reinforcement_learning.html#deque-vs-로테이팅-리스트",
    "title": "18_reinforcement_learning",
    "section": "Deque vs 로테이팅 리스트",
    "text": "Deque vs 로테이팅 리스트\ndeque 클래스는 추가(append)가 빠르지만 랜덤 접근은 느립니다(재생 메모리가 클 경우):\n\nfrom collections import deque\nnp.random.seed(42)\n\nmem = deque(maxlen=1000000)\nfor i in range(1000000):\n    mem.append(i)\n[mem[i] for i in np.random.randint(1000000, size=5)]\n\n[121958, 671155, 131932, 365838, 259178]\n\n\n\n%timeit mem.append(1)\n\n47.4 ns ± 3.02 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n\n\n\n%timeit [mem[i] for i in np.random.randint(1000000, size=5)]\n\n182 µs ± 6.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n\n\n또는 다음의 ReplayMemory 클래스 같은 로테이팅 리스트를 사용할 수 있습니다. 재생 메모리가 클 경우 랜덤 접근이 더 빠릅니다:\n\nclass ReplayMemory:\n    def __init__(self, max_size):\n        self.buffer = np.empty(max_size, dtype=np.object)\n        self.max_size = max_size\n        self.index = 0\n        self.size = 0\n\n    def append(self, obj):\n        self.buffer[self.index] = obj\n        self.size = min(self.size + 1, self.max_size)\n        self.index = (self.index + 1) % self.max_size\n\n    def sample(self, batch_size):\n        indices = np.random.randint(self.size, size=batch_size)\n        return self.buffer[indices]\n\n\nmem = ReplayMemory(max_size=1000000)\nfor i in range(1000000):\n    mem.append(i)\nmem.sample(5)\n\narray([757386, 904203, 190588, 595754, 865356], dtype=object)\n\n\n\n%timeit mem.append(1)\n\n519 ns ± 17.8 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n\n\n\n%timeit mem.sample(5)\n\n9.24 µs ± 227 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)"
  },
  {
    "objectID": "Machine_Learning/18_reinforcement_learning.html#사용자-정의-tf-agents-환경-만들기",
    "href": "Machine_Learning/18_reinforcement_learning.html#사용자-정의-tf-agents-환경-만들기",
    "title": "18_reinforcement_learning",
    "section": "사용자 정의 TF-Agents 환경 만들기",
    "text": "사용자 정의 TF-Agents 환경 만들기\n사용자 정의 TF-Agents 환경을 만들려면 PyEnvironment 클래스를 상속하는 클래스를 만들고 몇 개의 메서드를 구현해야 합니다. 예를 들어 다음과 같은 환경은 간단한 4x4 그리드를 표현합니다. 에이전트가 한쪽 코너 (0,0)에서 시작하여 반대쪽 코너 (3,3)으로 이동해야 합니다. 에이전트가 목적지에 도착하면 에피소드가 끝납니다(+10 보상을 받습니다). 또는 에이전트가 경계를 벗어나면 끝납니다(-1 보상). 행동은 위(0), 아래(1), 왼쪽(2), 오른쪽(3)이 가능합니다.\n\nclass MyEnvironment(tf_agents.environments.py_environment.PyEnvironment):\n    def __init__(self, discount=1.0):\n        super().__init__()\n        self._action_spec = tf_agents.specs.BoundedArraySpec(\n            shape=(), dtype=np.int32, name=\"action\", minimum=0, maximum=3)\n        self._observation_spec = tf_agents.specs.BoundedArraySpec(\n            shape=(4, 4), dtype=np.int32, name=\"observation\", minimum=0, maximum=1)\n        self.discount = discount\n\n    def action_spec(self):\n        return self._action_spec\n\n    def observation_spec(self):\n        return self._observation_spec\n\n    def _reset(self):\n        self._state = np.zeros(2, dtype=np.int32)\n        obs = np.zeros((4, 4), dtype=np.int32)\n        obs[self._state[0], self._state[1]] = 1\n        return tf_agents.trajectories.time_step.restart(obs)\n\n    def _step(self, action):\n        self._state += [(-1, 0), (+1, 0), (0, -1), (0, +1)][action]\n        reward = 0\n        obs = np.zeros((4, 4), dtype=np.int32)\n        done = (self._state.min() &lt; 0 or self._state.max() &gt; 3)\n        if not done:\n            obs[self._state[0], self._state[1]] = 1\n        if done or np.all(self._state == np.array([3, 3])):\n            reward = -1 if done else +10\n            return tf_agents.trajectories.time_step.termination(obs, reward)\n        else:\n            return tf_agents.trajectories.time_step.transition(obs, reward,\n                                                               self.discount)\n\n행동과 관측 스펙은 일반적으로 tf_agents.spec 패키지에 있는 ArraySpec이나 BoundedArraySpec의 인스턴스입니다(이 패키지에 있는 다른 스펙도 살펴 보세요). 선택적으로 render() 메서드, 자원을 해제하기 위한 close() 메서드를 정의할 수도 있습니다. 또한 reward와 discount를 32비트 실수 스칼라로 사용하고 싶지 않다면 time_step_spec() 메서드를 정의할 수 있습니다. 베이스 클래스는 현재 타임 스텝을 추적하므로 reset(), step() 대신에 _reset(), _step()을 구현해야 합니다.\n\nmy_env = MyEnvironment()\ntime_step = my_env.reset()\ntime_step\n\nTimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([[1, 0, 0, 0],\n       [0, 0, 0, 0],\n       [0, 0, 0, 0],\n       [0, 0, 0, 0]], dtype=int32))\n\n\n\ntime_step = my_env.step(1)\ntime_step\n\nTimeStep(step_type=array(1, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([[0, 0, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 0, 0],\n       [0, 0, 0, 0]], dtype=int32))"
  },
  {
    "objectID": "Machine_Learning/18_reinforcement_learning.html#to-7.",
    "href": "Machine_Learning/18_reinforcement_learning.html#to-7.",
    "title": "18_reinforcement_learning",
    "section": "1. to 7.",
    "text": "1. to 7.\n부록 A 참조"
  },
  {
    "objectID": "Machine_Learning/18_reinforcement_learning.html#section",
    "href": "Machine_Learning/18_reinforcement_learning.html#section",
    "title": "18_reinforcement_learning",
    "section": "8.",
    "text": "8.\n연습문제: 정책 그레이디언트를 사용해 OpenAI 짐의 LunarLander-v2 환경을 해결해보세요. 이를 위해 Box2D 패키지를 설치해야 합니다(%pip install -U gym[box2d]).\n먼저 LunarLander-v2 환경을 만들어 보죠:\n\nenv = gym.make(\"LunarLander-v2\")\n\n입력은 8차원입니다:\n\nenv.observation_space\n\nBox(-inf, inf, (8,), float32)\n\n\n\nenv.seed(42)\nobs = env.reset()\nobs\n\narray([-0.00499964,  1.4194578 , -0.506422  ,  0.37943238,  0.00580009,\n        0.11471219,  0.        ,  0.        ], dtype=float32)\n\n\n소스 코드를 보면 8D 관측(x, y, h, v, a, w, l, r)이 각각 다음에 해당합니다: * x,y: 우주선의 좌표. (0, 1.4) 근처의 랜덤한 위치에서 시작하고 (0, 0)에 있는 목적지 근처에 내려야 합니다. * h,v: 우주선의 수평, 수직 속도. 랜덤한 적은 속도로 시작합니다. * a,w: 우주선의 각도와 각속도. * l,r: 왼쪽이나 오른쪽 다리가 땅에 닿았는지(1.0) 아닌지(0.0) 여부.\n행동 공간은 이산적이며 4개의 가능한 행동이 있습니다:\n\nenv.action_space\n\nDiscrete(4)\n\n\nLunarLander-v2 설명을 보면 이 행동은 다음과 같습니다: * 아무것도 하지 않음 * 왼쪽 방향 엔진을 켬 * 주 엔진을 켬 * 오른쪽 방향 엔진을 켬\n(행동마다 하나씩) 4개의 출력 뉴런을 가진 간단한 정책 네트워크를 만들어 보죠:\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nn_inputs = env.observation_space.shape[0]\nn_outputs = env.action_space.n\n\nmodel = keras.models.Sequential([\n    keras.layers.Dense(32, activation=\"relu\", input_shape=[n_inputs]),\n    keras.layers.Dense(32, activation=\"relu\"),\n    keras.layers.Dense(n_outputs, activation=\"softmax\"),\n])\n\n출력 층에 CartPole-v1 환경처럼 시그모이드 활성화 함수를 사용하지 않고 대신에 소프트맥스 활성화 함수를 사용합니다. CartPole-v1 환경은 두 개의 행동만 있어서 이진 분류 모델이 맞기 때문입니다. 하지만 두 개 이상의 행동이 있으므로 다중 분류 모델이 됩니다.\n그다음 CartPole-v1 정책 그레이디언트 코드에서 정의한 play_one_step()와 play_multiple_episodes() 함수를 재사용합니다. 하지만 다중 분류 모델에 맞게 play_one_step()를 조금 수정하겠습니다. 그다음 수정된 play_one_step() 를 호출하고, 우주선이 최대 스텝 횟수 전에 랜딩하지 못하면 (또는 부서지면) 큰 페널티를 부여하도록 play_multiple_episodes() 함수를 수정합니다.\n\ndef lander_play_one_step(env, obs, model, loss_fn):\n    with tf.GradientTape() as tape:\n        probas = model(obs[np.newaxis])\n        logits = tf.math.log(probas + keras.backend.epsilon())\n        action = tf.random.categorical(logits, num_samples=1)\n        loss = tf.reduce_mean(loss_fn(action, probas))\n    grads = tape.gradient(loss, model.trainable_variables)\n    obs, reward, done, info = env.step(action[0, 0].numpy())\n    return obs, reward, done, grads\n\ndef lander_play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n    all_rewards = []\n    all_grads = []\n    for episode in range(n_episodes):\n        current_rewards = []\n        current_grads = []\n        obs = env.reset()\n        for step in range(n_max_steps):\n            obs, reward, done, grads = lander_play_one_step(env, obs, model, loss_fn)\n            current_rewards.append(reward)\n            current_grads.append(grads)\n            if done:\n                break\n        all_rewards.append(current_rewards)\n        all_grads.append(current_grads)\n    return all_rewards, all_grads\n\n앞에서와 동일한 discount_rewards()와 discount_and_normalize_rewards() 함수를 사용합니다:\n\ndef discount_rewards(rewards, discount_rate):\n    discounted = np.array(rewards)\n    for step in range(len(rewards) - 2, -1, -1):\n        discounted[step] += discounted[step + 1] * discount_rate\n    return discounted\n\ndef discount_and_normalize_rewards(all_rewards, discount_rate):\n    all_discounted_rewards = [discount_rewards(rewards, discount_rate)\n                              for rewards in all_rewards]\n    flat_rewards = np.concatenate(all_discounted_rewards)\n    reward_mean = flat_rewards.mean()\n    reward_std = flat_rewards.std()\n    return [(discounted_rewards - reward_mean) / reward_std\n            for discounted_rewards in all_discounted_rewards]\n\n이제 몇 개의 하이퍼파라미터를 정의합니다:\n\nn_iterations = 200\nn_episodes_per_update = 16\nn_max_steps = 1000\ndiscount_rate = 0.99\n\n여기서도 다중 분류 모델이기 때문에 이진 크로스 엔트로피가 아니라 범주형 크로스 엔트로피를 사용해야 합니다. 또한 lander_play_one_step() 함수가 클래스 확률이 아니라 클래스 레이블로 타깃을 설정하기 때문에 sparse_categorical_crossentropy() 손실 함수를 사용해야 합니다:\n\noptimizer = keras.optimizers.Nadam(learning_rate=0.005)\nloss_fn = keras.losses.sparse_categorical_crossentropy\n\n모델을 훈련할 준비가 되었네요. 시작해 보죠!\n\nenv.seed(42)\n\nmean_rewards = []\n\nfor iteration in range(n_iterations):\n    all_rewards, all_grads = lander_play_multiple_episodes(\n        env, n_episodes_per_update, n_max_steps, model, loss_fn)\n    mean_reward = sum(map(sum, all_rewards)) / n_episodes_per_update\n    print(\"\\rIteration: {}/{}, mean reward: {:.1f}  \".format(\n        iteration + 1, n_iterations, mean_reward), end=\"\")\n    mean_rewards.append(mean_reward)\n    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n                                                       discount_rate)\n    all_mean_grads = []\n    for var_index in range(len(model.trainable_variables)):\n        mean_grads = tf.reduce_mean(\n            [final_reward * all_grads[episode_index][step][var_index]\n             for episode_index, final_rewards in enumerate(all_final_rewards)\n                 for step, final_reward in enumerate(final_rewards)], axis=0)\n        all_mean_grads.append(mean_grads)\n    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n\nIteration: 200/200, mean reward: 134.2  \n\n\n학습 곡선을 그려 보겠습니다:\n\nimport matplotlib.pyplot as plt\n\nplt.plot(mean_rewards)\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Mean reward\")\nplt.grid()\nplt.show()\n\n\n\n\n결과를 확인해 보죠!\n\ndef lander_render_policy_net(model, n_max_steps=500, seed=42):\n    frames = []\n    env = gym.make(\"LunarLander-v2\")\n    env.seed(seed)\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    obs = env.reset()\n    for step in range(n_max_steps):\n        frames.append(env.render(mode=\"rgb_array\"))\n        probas = model(obs[np.newaxis])\n        logits = tf.math.log(probas + keras.backend.epsilon())\n        action = tf.random.categorical(logits, num_samples=1)\n        obs, reward, done, info = env.step(action[0, 0].numpy())\n        if done:\n            break\n    env.close()\n    return frames\n\n\nframes = lander_render_policy_net(model, seed=42)\nplot_animation(frames)\n\n꽤 괜찮군요. 더 오래 훈련하거나 하이퍼파라미터를 튜닝하여 200을 넘을 수 있는지 확인해 보세요."
  },
  {
    "objectID": "Machine_Learning/18_reinforcement_learning.html#section-1",
    "href": "Machine_Learning/18_reinforcement_learning.html#section-1",
    "title": "18_reinforcement_learning",
    "section": "9.",
    "text": "9.\n연습문제: 알고리즘에 상관없이 TF-Agents를 사용해 SpaceInvaders-v4 환경에서 사람을 능가하는 에이전트를 훈련해보세요.\n\"Breakout-v4\"를 \"SpaceInvaders-v4\"로 바꾸고 TF Agents를 사용해 브레이크아웃 게임하기 절에 있는 단계를 따라해 보세요. 하지만 몇 가지를 바꾸어야 합니다. 예를 들어 스페이스 인베이더 게임은 게임을 시작할 때 FIRE 버튼을 누를 필요가 없습니다. 대신 플레이어의 레이저 캐논이 몇 초간 깜빡거린 다음 자동으로 게임이 시작됩니다. 성능을 높이려면 에피소드를 시작할 때와 죽을 때마다 깜빡임 단계(약 40 스텝 동안 지속됩니다)를 건너 뛸 수 있습니다. 사실 이 단계에서는 아무것도 할 수 없고 아무것도 움직이지 않습니다. 건너 뛰는 방법은 AtariPreprocessingWithAutoFire 래퍼 대신에 다음과 같은 사용자 정의 환경 래퍼를 사용하는 것입니다:\n\nclass AtariPreprocessingWithSkipStart(AtariPreprocessing):\n    def skip_frames(self, num_skip):\n        for _ in range(num_skip):\n          super().step(0) # NOOP for num_skip steps\n    def reset(self, **kwargs):\n        obs = super().reset(**kwargs)\n        self.skip_frames(40)\n        return obs\n    def step(self, action):\n        lives_before_action = self.ale.lives()\n        obs, rewards, done, info = super().step(action)\n        if self.ale.lives() &lt; lives_before_action and not done:\n            self.skip_frames(40)\n        return obs, rewards, done, info\n\n또한 전처리된 이미지가 게임 플레이에 관한 충분한 정보를 담고 있는지 항상 확인해야 합니다. 예를 들어, 낮은 해상도에도 불구하고 레이저 캐논과 에일리언에서 발사된 총알은 항상 보여야 합니다. 이 경우에 브레이크아웃에서 수행했던 전처리가 스페이스 인베이더에도 잘 맞습니다. 하지만 다른 게임에서는 항상 확인해봐야 합니다. 이를 위해 에이전트가 랜덤하게 플레이하게 잠시 놔두고 전처리된 프레임을 기롭한 다음 애니메이션을 플레이하여 게임 플레이가 잘 보이는지 확인하세요.\n좋은 성능을 얻으려면 에이전트를 꽤 오랜 시간 동안 훈련해야 합니다. 안타깝게도 DQN 알고리즘은 스페이스 인베이더에서 사람을 뛰어 넘는 수준을 달성할 수 없습니다. 사람은 이 게임에서 효율적인 장기 전략을 학습할 수 있지만 DQN은 매우 짧은 전략만 학습할 수 있습니다. 하지만 지난 몇 년간 많은 발전이 있었습니다. 이제는 많은 RL 알고리즘이 이 게임에서 전문가의 수준을 뛰어 넘을 수 있습니다. State-of-the-Art for Space Invaders on paperswithcode.com를 참고하세요."
  },
  {
    "objectID": "Machine_Learning/18_reinforcement_learning.html#section-2",
    "href": "Machine_Learning/18_reinforcement_learning.html#section-2",
    "title": "18_reinforcement_learning",
    "section": "10.",
    "text": "10.\n연습문제: 10만 원 정도 여유가 있다면 라즈베리 파이 3와 저렴한 로보틱스 구성품을 구입해 텐서플로를 설치하고 실행할 수 있습니다! 예를 들어 루카스 비월드의 재미있는 포스트를 참고하거나, GoPiGo42나 BrickPi43를 둘러보세요. 간단한 작업부터 시작해보세요. 예를 들어 (조도 센서가 있다면) 로봇이 밝은 쪽으로 회전하거나 (초음파 센서가 있다면) 가까운 물체가 있는 쪽으로 움직이도록 해보세요. 그다음 딥러닝을 사용해보세요. 예를 들어 로봇에 카메라가 있다면 객체 탐지 알고리즘을 구현해 사람을 감지하고 가까이 다가가게 만들 수 있습니다. 강화 학습을 사용해 목표를 달성하기 위해 모터 사용법을 스스로 학습할 수도 있습니다.\n이제 여러분 차례입니다. 도전적이고 창의적으로, 무엇보다도 인내심을 가지고 한 발씩 나아가세요. 여러분은 할 수 있습니다!"
  },
  {
    "objectID": "Machine_Learning.html",
    "href": "Machine_Learning.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n19_training_and_deploying_at_scale\n\n\n\n\n\n\n\nSklearn\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nMay 1, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n18_reinforcement_learning\n\n\n\n\n\n\n\nSklearn\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n17_autoencoders_and_gans\n\n\n\n\n\n\n\nSklearn\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nApr 12, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n16_nlp_with_rnns_and_attention\n\n\n\n\n\n\n\nSklearn\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nApr 11, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n14_deep_computer_vision_with_cnns\n\n\n\n\n\n\n\nSklearn\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n13_loading_and_preprocessing_data\n\n\n\n\n\n\n\nSklearn\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n12_custom_models_and_training_with_tensorflow\n\n\n\n\n\n\n\nSklearn\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n11_training_deep_neural_networks\n\n\n\n\n\n\n\nSklearn\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nApr 2, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n10_neural_nets_with_keras\n\n\n\n\n\n\n\nSklearn\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nApr 1, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n15_processing_sequences_using_rnns_and_cnns\n\n\n\n\n\n\n\nSklearn\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nApr 1, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n09_unsupervised_learning\n\n\n\n\n\n\n\nSklearn\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n08_dimensionality_reduction\n\n\n\n\n\n\n\nSklearn\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n07_ensemble_learning_and_random_forests\n\n\n\n\n\n\n\nSklearn\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n06_decision_trees\n\n\n\n\n\n\n\nSklearn\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nMar 26, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n05_support_vector_machines\n\n\n\n\n\n\n\nSklearn\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n03_classification\n\n\n\n\n\n\n\nSklearn\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nMar 24, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n04_training_linear_models\n\n\n\n\n\n\n\nSklearn\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nMar 24, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n02_end_to_end_machine_learning_project\n\n\n\n\n\n\n\nSklearn\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nMar 23, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n01_the_machine_learning_landscape\n\n\n\n\n\n\n\nSklearn\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nMar 22, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "R_Basic.html",
    "href": "R_Basic.html",
    "title": "R Basic",
    "section": "",
    "text": "모두를 위한 R 데이터 분석 입문\n\n\n\n\n2 + 3  # 2 더하기 3\n## [1] 5\n(3 + 6) * 8\n## [1] 72\n2 ^ 3  # 2의 세제곱\n## [1] 8\n8 %% 3\n## [1] 2\n\n\n7 + 4\n## [1] 11\n\n\nlog(10) + 5 # 로그함수\n## [1] 7.302585\nsqrt(25) # 제곱근\n## [1] 5\nmax(5, 3, 2) # 가장 큰 값\n## [1] 5\n\n\na &lt;- 10\nb &lt;- 20\nc &lt;- a+b\nprint(c)\n## [1] 30\n\n\na &lt;- 125\na\n## [1] 125\nprint(a)\n## [1] 125\n\n\na &lt;- 10 # a에 숫자 저장\nb &lt;- 20\na + b # a+b의 결과 출력\n## [1] 30\na &lt;- \"A\" # a에 문자 저장\na + b # a+b의 결과 출력. 에러 발생\n## Error in a + b: non-numeric argument to binary operator\n\n\nx &lt;- c(1, 2, 3) # 숫자형 벡터\ny &lt;- c(\"a\", \"b\", \"c\") # 문자형 벡터\nz &lt;- c(TRUE, TRUE, FALSE, TRUE) # 논리형 벡터\nx ; y ;z\n## [1] 1 2 3\n## [1] \"a\" \"b\" \"c\"\n## [1]  TRUE  TRUE FALSE  TRUE\n\n\nw &lt;- c(1, 2, 3, \"a\", \"b\", \"c\")\nw\n## [1] \"1\" \"2\" \"3\" \"a\" \"b\" \"c\"\n\n\nv1 &lt;- 50:90\nv1\n##  [1] 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74\n## [26] 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90\nv2 &lt;- c(1, 2, 5, 50:90)\nv2\n##  [1]  1  2  5 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n## [26] 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90\n\n\nv3 &lt;- seq(1, 101, 3)\nv3\n##  [1]   1   4   7  10  13  16  19  22  25  28  31  34  37  40  43  46  49  52  55\n## [20]  58  61  64  67  70  73  76  79  82  85  88  91  94  97 100\nv4 &lt;- seq(0.1, 1.0, 0.1)\nv4\n##  [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n\n\nv5 &lt;- rep(1, times = 5) # 1을 5번 반복\nv5\n## [1] 1 1 1 1 1\nv6 &lt;- rep(1:5, times = 3) # 1에서 5까지 3번 반복\nv6\n##  [1] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5\nv7 &lt;- rep(c(1, 5, 9), times = 3) # 1, 5, 9를 3번 반복\nv7\n## [1] 1 5 9 1 5 9 1 5 9\nv8 &lt;- rep(1:5, each = 3) # 1에서 5를 각각 3번 반복\nv8\n##  [1] 1 1 1 2 2 2 3 3 3 4 4 4 5 5 5\n\nrep(1:3, each = 3, times = 3)\n##  [1] 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3\nrep(1:3, times = 3, each = 3)\n##  [1] 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3\n\n\nscore &lt;- c(90, 85, 70) # 성적\nscore\n## [1] 90 85 70\nnames(score) # score에 저장된 값들의 이름을 보이시오\n## NULL\nnames(score) &lt;- c(\"John\", \"Tom\", \"Jane\") # 값들에 이름을 부여\nnames(score) # score에 저장된 값들의 이름을 보이시오\n## [1] \"John\" \"Tom\"  \"Jane\"\nscore # 이름과 함께 값이 출력\n## John  Tom Jane \n##   90   85   70\n\n\nd &lt;- c(1, 4, 3, 7, 8)\nd[1]\n## [1] 1\nd[2]\n## [1] 4\nd[3]\n## [1] 3\nd[4]\n## [1] 7\nd[5]\n## [1] 8\nd[6]\n## [1] NA\nd[c(2, 4)]\n## [1] 4 7\n\n\nd &lt;- c(1, 4, 3, 7, 8)\nd[c(1, 3, 5)] # 1, 3, 5번째 값 출력\n## [1] 1 3 8\nd[1:3] # 처음 세 개의 값 출력\n## [1] 1 4 3\nd[seq(1, 5, 2)] # 홀수 번째 값 출력\n## [1] 1 3 8\nd[-2] # 2번째 값 제외하고 출력\n## [1] 1 3 7 8\nd[-c(3:5)] # 3~5번째 값은 제외하고 출력\n## [1] 1 4\n\n\nGNP &lt;- c(2000, 2450, 960)\nGNP\n## [1] 2000 2450  960\nnames(GNP) &lt;- c(\"Korea\", \"Japan\", \"Nepal\")\nGNP\n## Korea Japan Nepal \n##  2000  2450   960\nGNP[1]\n## Korea \n##  2000\nGNP[\"Korea\"]\n## Korea \n##  2000\nGNP_NEW &lt;- GNP[c(\"Korea\", \"Nepal\")]\nGNP_NEW\n## Korea Nepal \n##  2000   960\n\n\nv1 &lt;- c(1, 5, 7, 8, 9)\nv1\n## [1] 1 5 7 8 9\nv1[2] &lt;- 3 # v1의 2번째 값을 3으로 변경\nv1\n## [1] 1 3 7 8 9\nv1[c(1, 5)] &lt;- c(10, 20) # v1의 1, 5번째 값을 각각 10, 20으로 변경\nv1\n## [1] 10  3  7  8 20\n\n\nd &lt;- c(1, 4, 3, 7, 8)\n2 * d\n## [1]  2  8  6 14 16\nd - 5\n## [1] -4 -1 -2  2  3\n3 * d + 4\n## [1]  7 16 13 25 28\n\n\nx &lt;- c(1, 2, 3)\ny &lt;- c(4, 5, 6)\nx + y # 대응하는 원소끼리 더하여 출력\n## [1] 5 7 9\nx * y # 대응하는 원소끼리 곱하여 출력\n## [1]  4 10 18\nz &lt;- x + y # x, y를 더하여 z에 저장\nz\n## [1] 5 7 9\n\n\nd &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\nsum(d) # d에 포함된 값들의 합\n## [1] 55\nsum(2 * d) # d에 포함된 값들에 2를 곱한 후 합한 값\n## [1] 110\nlength(d) # d에 포함된 값들의 개수\n## [1] 10\nmean(d[1:5]) # 1~5번째 값들의 평균\n## [1] 3\nmax(d) # d에 포함된 값들의 최댓값\n## [1] 10\nmin(d) # d에 포함된 값들의 최솟값\n## [1] 1\nsort(d) # 오름차순 정렬\n##  [1]  1  2  3  4  5  6  7  8  9 10\nsort(d, decreasing = FALSE) # 오름차순 정렬\n##  [1]  1  2  3  4  5  6  7  8  9 10\nsort(d, decreasing = TRUE) # 내림차순 정렬\n##  [1] 10  9  8  7  6  5  4  3  2  1\nsort(d, TRUE) # 내림차순 정렬\n##  [1] 10  9  8  7  6  5  4  3  2  1\n\nv1 &lt;- median(d)\nv1\n## [1] 5.5\nv2 &lt;- sum(d) / length(d)\nv2\n## [1] 5.5\nmean(d)\n## [1] 5.5\n\n\nd &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9)\nd &gt;= 5\n## [1] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\nd[d &gt; 5] # 5보다 큰 값\n## [1] 6 7 8 9\nsum(d &gt; 5) # 5보다 큰 값의 개수를 출력\n## [1] 4\nsum(d[d &gt; 5]) # 5보다 큰 값의 합계를 출력\n## [1] 30\nd == 5\n## [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n\ncondi &lt;- d &gt; 5 & d &lt; 8 # 조건을 변수에 저장\nd[condi] # 조건에 맞는 값들을 선택\n## [1] 6 7\nd[d &gt; 5 & d &lt; 8]\n## [1] 6 7\n\n\nds &lt;- c(90, 85, 70, 84)\nmy.info &lt;- list(name = 'Tom', age = 60, status = TRUE, score = ds)\nmy.info # 리스트에 저장된 내용을 모두 출력\n## $name\n## [1] \"Tom\"\n## \n## $age\n## [1] 60\n## \n## $status\n## [1] TRUE\n## \n## $score\n## [1] 90 85 70 84\nmy.info[1] # 이름이랑 내용 다 출력\n## $name\n## [1] \"Tom\"\nmy.info[[1]] # 리스트의 첫 번째 값을 출력\n## [1] \"Tom\"\nmy.info$name # 리스트에서 값의 이름이 name인 값을 출력\n## [1] \"Tom\"\nmy.info[[4]] # 리스트의 네 번째 값을 출력\n## [1] 90 85 70 84\n\n\nbt &lt;- c('A', 'B', 'B', 'O', 'AB', 'A') # 문자형 벡터 bt 정의\nbt.new &lt;- factor(bt) # 팩터 bt.new 정의\nbt # 벡터 bt의 내용 출력\n## [1] \"A\"  \"B\"  \"B\"  \"O\"  \"AB\" \"A\"\nbt.new # 팩터 bt.new의 내용 출력\n## [1] A  B  B  O  AB A \n## Levels: A AB B O\nbt[5] # 벡터 bt의 5번째 값 출력\n## [1] \"AB\"\nbt.new[5] # 팩터 bt.new의 5번째 값 출력\n## [1] AB\n## Levels: A AB B O\nlevels(bt.new) # 팩터에 저장된 값의 종류를 출력\n## [1] \"A\"  \"AB\" \"B\"  \"O\"\nas.integer(bt.new) # 팩터의 문자값을 숫자로 바꾸어 출력\n## [1] 1 3 3 4 2 1\nbt.new[7] &lt;- 'B' # 팩터 bt.new의 7번째에 'B' 저장\nbt.new[8] &lt;- 'C' # 팩터 bt.new의 8번째에 'C' 저장\n## Warning in `[&lt;-.factor`(`*tmp*`, 8, value = \"C\"): invalid factor level, NA\n## generated\nbt.new # 팩터 bt.new의 내용 출력\n## [1] A    B    B    O    AB   A    B    &lt;NA&gt;\n## Levels: A AB B O\n\n\n\n\n\nz &lt;- matrix(1:20, nrow = 4, ncol = 5)\nz # 매트릭스 z의 내용을 출력\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\n\n\nz2 &lt;- matrix(1:20, nrow = 4, ncol = 5, byrow = T)\nz2 # 매트릭스 z2의 내용을 출력\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    2    3    4    5\n## [2,]    6    7    8    9   10\n## [3,]   11   12   13   14   15\n## [4,]   16   17   18   19   20\n\nz &lt;- matrix(1:16, nrow = 4, ncol = 5)\n## Warning in matrix(1:16, nrow = 4, ncol = 5): data length [16] is not a\n## sub-multiple or multiple of the number of columns [5]\nz\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13    1\n## [2,]    2    6   10   14    2\n## [3,]    3    7   11   15    3\n## [4,]    4    8   12   16    4\n\n\nx &lt;- 1:4 # 벡터 x 생성\ny &lt;- 5:8 # 벡터 y 생성\nz &lt;- matrix(1:20, nrow = 4, ncol = 5) # 매트릭스 z 생성\n\nm1 &lt;- cbind(x, y) # x와 y를 열 방향으로 결합하여 매트릭스 생성\nm1 # 매트릭스 m1의 내용을 출력\n##      x y\n## [1,] 1 5\n## [2,] 2 6\n## [3,] 3 7\n## [4,] 4 8\nm2 &lt;- rbind(x, y) # x와 y를 행 방향으로 결합하여 매트릭스 생성\nm2 # 매트릭스 m2의 내용을 출력\n##   [,1] [,2] [,3] [,4]\n## x    1    2    3    4\n## y    5    6    7    8\nm3 &lt;- rbind(m2, x) # m2와 벡터 x를 행 방향으로 결합\nm3 # 매트릭스 m3의 내용을 출력\n##   [,1] [,2] [,3] [,4]\n## x    1    2    3    4\n## y    5    6    7    8\n## x    1    2    3    4\nm4 &lt;- cbind(z, x) # 매트릭스 z와 벡터 x를 열 방향으로 결합\nm4 # 매트릭스 m4의 내용을 출력\n##                   x\n## [1,] 1 5  9 13 17 1\n## [2,] 2 6 10 14 18 2\n## [3,] 3 7 11 15 19 3\n## [4,] 4 8 12 16 20 4\n\nx &lt;- 1:5\nm5 &lt;- cbind(z, x)\n## Warning in cbind(z, x): number of rows of result is not a multiple of vector\n## length (arg 2)\nm5\n##                   x\n## [1,] 1 5  9 13 17 1\n## [2,] 2 6 10 14 18 2\n## [3,] 3 7 11 15 19 3\n## [4,] 4 8 12 16 20 4\n\n\nz &lt;- matrix(1:20, nrow = 4, ncol = 5) # 매트릭스 z 생성\nz # 매트릭스 z의 내용 출력\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\n\nz[2, 3] # 2행 3열에 있는 값\n## [1] 10\nz[1, 4] # 1행 4열에 있는 값\n## [1] 13\nz[2, ] # 2행에 있는 모든 값\n## [1]  2  6 10 14 18\nz[, 4] # 4열에 있는 모든 값\n## [1] 13 14 15 16\nz[, ]\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\nz\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\n\n\nz &lt;- matrix(1:20, nrow = 4, ncol = 5) # 매트릭스 z 생성\nz # 매트릭스 z의 내용 출력\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\n\nz[2, 1:3] # 2행의 값 중 1~3열에 있는 값\n## [1]  2  6 10\nz[1, c(1, 2, 4)] # 1행의 값 중 1, 2, 4열에 있는 값\n## [1]  1  5 13\nz[1:2, ] # 1, 2행에 있는 모든 값\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\nz[, c(1, 4)] # 1, 4열에 있는 모든 값\n##      [,1] [,2]\n## [1,]    1   13\n## [2,]    2   14\n## [3,]    3   15\n## [4,]    4   16\n\n\nscore &lt;- matrix(c(90, 85, 69, 78,\n                  85, 96, 49, 95,\n                  90, 80, 70, 60),\n                nrow = 4,\n                ncol = 3)\nscore\n##      [,1] [,2] [,3]\n## [1,]   90   85   90\n## [2,]   85   96   80\n## [3,]   69   49   70\n## [4,]   78   95   60\nrownames(score) &lt;- c('John', 'Tom', 'Mark', 'Jane')\ncolnames(score) &lt;- c('English', 'Math', 'Science')\nscore\n##      English Math Science\n## John      90   85      90\n## Tom       85   96      80\n## Mark      69   49      70\n## Jane      78   95      60\n\n\nscore['John', 'Math'] # John의 수학 성적\n## [1] 85\nscore['Tom', c('Math', 'Science')] # Tom의 수학, 과학 성적\n##    Math Science \n##      96      80\nscore['Mark', ] # Mark의 모든 과목 성적\n## English    Math Science \n##      69      49      70\nscore[, 'English'] # 모든 학생의 영어 성적\n## John  Tom Mark Jane \n##   90   85   69   78\nrownames(score) # score의 행의 이름\n## [1] \"John\" \"Tom\"  \"Mark\" \"Jane\"\ncolnames(score) # score의 열의 이름\n## [1] \"English\" \"Math\"    \"Science\"\ncolnames(score)[2] # score의 열의 이름 중 두 번째 값\n## [1] \"Math\"\n\n\ncity &lt;- c(\"Seoul\", \"Tokyo\", \"Washington\") # 문자로 이루어진 벡터\nrank &lt;- c(1, 3, 2) # 숫자로 이루어진 벡터\ncity.info &lt;- data.frame(city, rank) # 데이터프레임 생성\ncity.info # city.info의 내용 출력\n##         city rank\n## 1      Seoul    1\n## 2      Tokyo    3\n## 3 Washington    2\n\n\n# iris\niris[, c(1:2)] # 1, 2열의 모든 데이터\n##     Sepal.Length Sepal.Width\n## 1            5.1         3.5\n## 2            4.9         3.0\n## 3            4.7         3.2\n## 4            4.6         3.1\n## 5            5.0         3.6\n## 6            5.4         3.9\n## 7            4.6         3.4\n## 8            5.0         3.4\n## 9            4.4         2.9\n## 10           4.9         3.1\n## 11           5.4         3.7\n## 12           4.8         3.4\n## 13           4.8         3.0\n## 14           4.3         3.0\n## 15           5.8         4.0\n## 16           5.7         4.4\n## 17           5.4         3.9\n## 18           5.1         3.5\n## 19           5.7         3.8\n## 20           5.1         3.8\n## 21           5.4         3.4\n## 22           5.1         3.7\n## 23           4.6         3.6\n## 24           5.1         3.3\n## 25           4.8         3.4\n## 26           5.0         3.0\n## 27           5.0         3.4\n## 28           5.2         3.5\n## 29           5.2         3.4\n## 30           4.7         3.2\n## 31           4.8         3.1\n## 32           5.4         3.4\n## 33           5.2         4.1\n## 34           5.5         4.2\n## 35           4.9         3.1\n## 36           5.0         3.2\n## 37           5.5         3.5\n## 38           4.9         3.6\n## 39           4.4         3.0\n## 40           5.1         3.4\n## 41           5.0         3.5\n## 42           4.5         2.3\n## 43           4.4         3.2\n## 44           5.0         3.5\n## 45           5.1         3.8\n## 46           4.8         3.0\n## 47           5.1         3.8\n## 48           4.6         3.2\n## 49           5.3         3.7\n## 50           5.0         3.3\n## 51           7.0         3.2\n## 52           6.4         3.2\n## 53           6.9         3.1\n## 54           5.5         2.3\n## 55           6.5         2.8\n## 56           5.7         2.8\n## 57           6.3         3.3\n## 58           4.9         2.4\n## 59           6.6         2.9\n## 60           5.2         2.7\n## 61           5.0         2.0\n## 62           5.9         3.0\n## 63           6.0         2.2\n## 64           6.1         2.9\n## 65           5.6         2.9\n## 66           6.7         3.1\n## 67           5.6         3.0\n## 68           5.8         2.7\n## 69           6.2         2.2\n## 70           5.6         2.5\n## 71           5.9         3.2\n## 72           6.1         2.8\n## 73           6.3         2.5\n## 74           6.1         2.8\n## 75           6.4         2.9\n## 76           6.6         3.0\n## 77           6.8         2.8\n## 78           6.7         3.0\n## 79           6.0         2.9\n## 80           5.7         2.6\n## 81           5.5         2.4\n## 82           5.5         2.4\n## 83           5.8         2.7\n## 84           6.0         2.7\n## 85           5.4         3.0\n## 86           6.0         3.4\n## 87           6.7         3.1\n## 88           6.3         2.3\n## 89           5.6         3.0\n## 90           5.5         2.5\n## 91           5.5         2.6\n## 92           6.1         3.0\n## 93           5.8         2.6\n## 94           5.0         2.3\n## 95           5.6         2.7\n## 96           5.7         3.0\n## 97           5.7         2.9\n## 98           6.2         2.9\n## 99           5.1         2.5\n## 100          5.7         2.8\n## 101          6.3         3.3\n## 102          5.8         2.7\n## 103          7.1         3.0\n## 104          6.3         2.9\n## 105          6.5         3.0\n## 106          7.6         3.0\n## 107          4.9         2.5\n## 108          7.3         2.9\n## 109          6.7         2.5\n## 110          7.2         3.6\n## 111          6.5         3.2\n## 112          6.4         2.7\n## 113          6.8         3.0\n## 114          5.7         2.5\n## 115          5.8         2.8\n## 116          6.4         3.2\n## 117          6.5         3.0\n## 118          7.7         3.8\n## 119          7.7         2.6\n## 120          6.0         2.2\n## 121          6.9         3.2\n## 122          5.6         2.8\n## 123          7.7         2.8\n## 124          6.3         2.7\n## 125          6.7         3.3\n## 126          7.2         3.2\n## 127          6.2         2.8\n## 128          6.1         3.0\n## 129          6.4         2.8\n## 130          7.2         3.0\n## 131          7.4         2.8\n## 132          7.9         3.8\n## 133          6.4         2.8\n## 134          6.3         2.8\n## 135          6.1         2.6\n## 136          7.7         3.0\n## 137          6.3         3.4\n## 138          6.4         3.1\n## 139          6.0         3.0\n## 140          6.9         3.1\n## 141          6.7         3.1\n## 142          6.9         3.1\n## 143          5.8         2.7\n## 144          6.8         3.2\n## 145          6.7         3.3\n## 146          6.7         3.0\n## 147          6.3         2.5\n## 148          6.5         3.0\n## 149          6.2         3.4\n## 150          5.9         3.0\niris[, c(1, 3, 5)] # 1, 3, 5열의 모든 데이터\n##     Sepal.Length Petal.Length    Species\n## 1            5.1          1.4     setosa\n## 2            4.9          1.4     setosa\n## 3            4.7          1.3     setosa\n## 4            4.6          1.5     setosa\n## 5            5.0          1.4     setosa\n## 6            5.4          1.7     setosa\n## 7            4.6          1.4     setosa\n## 8            5.0          1.5     setosa\n## 9            4.4          1.4     setosa\n## 10           4.9          1.5     setosa\n## 11           5.4          1.5     setosa\n## 12           4.8          1.6     setosa\n## 13           4.8          1.4     setosa\n## 14           4.3          1.1     setosa\n## 15           5.8          1.2     setosa\n## 16           5.7          1.5     setosa\n## 17           5.4          1.3     setosa\n## 18           5.1          1.4     setosa\n## 19           5.7          1.7     setosa\n## 20           5.1          1.5     setosa\n## 21           5.4          1.7     setosa\n## 22           5.1          1.5     setosa\n## 23           4.6          1.0     setosa\n## 24           5.1          1.7     setosa\n## 25           4.8          1.9     setosa\n## 26           5.0          1.6     setosa\n## 27           5.0          1.6     setosa\n## 28           5.2          1.5     setosa\n## 29           5.2          1.4     setosa\n## 30           4.7          1.6     setosa\n## 31           4.8          1.6     setosa\n## 32           5.4          1.5     setosa\n## 33           5.2          1.5     setosa\n## 34           5.5          1.4     setosa\n## 35           4.9          1.5     setosa\n## 36           5.0          1.2     setosa\n## 37           5.5          1.3     setosa\n## 38           4.9          1.4     setosa\n## 39           4.4          1.3     setosa\n## 40           5.1          1.5     setosa\n## 41           5.0          1.3     setosa\n## 42           4.5          1.3     setosa\n## 43           4.4          1.3     setosa\n## 44           5.0          1.6     setosa\n## 45           5.1          1.9     setosa\n## 46           4.8          1.4     setosa\n## 47           5.1          1.6     setosa\n## 48           4.6          1.4     setosa\n## 49           5.3          1.5     setosa\n## 50           5.0          1.4     setosa\n## 51           7.0          4.7 versicolor\n## 52           6.4          4.5 versicolor\n## 53           6.9          4.9 versicolor\n## 54           5.5          4.0 versicolor\n## 55           6.5          4.6 versicolor\n## 56           5.7          4.5 versicolor\n## 57           6.3          4.7 versicolor\n## 58           4.9          3.3 versicolor\n## 59           6.6          4.6 versicolor\n## 60           5.2          3.9 versicolor\n## 61           5.0          3.5 versicolor\n## 62           5.9          4.2 versicolor\n## 63           6.0          4.0 versicolor\n## 64           6.1          4.7 versicolor\n## 65           5.6          3.6 versicolor\n## 66           6.7          4.4 versicolor\n## 67           5.6          4.5 versicolor\n## 68           5.8          4.1 versicolor\n## 69           6.2          4.5 versicolor\n## 70           5.6          3.9 versicolor\n## 71           5.9          4.8 versicolor\n## 72           6.1          4.0 versicolor\n## 73           6.3          4.9 versicolor\n## 74           6.1          4.7 versicolor\n## 75           6.4          4.3 versicolor\n## 76           6.6          4.4 versicolor\n## 77           6.8          4.8 versicolor\n## 78           6.7          5.0 versicolor\n## 79           6.0          4.5 versicolor\n## 80           5.7          3.5 versicolor\n## 81           5.5          3.8 versicolor\n## 82           5.5          3.7 versicolor\n## 83           5.8          3.9 versicolor\n## 84           6.0          5.1 versicolor\n## 85           5.4          4.5 versicolor\n## 86           6.0          4.5 versicolor\n## 87           6.7          4.7 versicolor\n## 88           6.3          4.4 versicolor\n## 89           5.6          4.1 versicolor\n## 90           5.5          4.0 versicolor\n## 91           5.5          4.4 versicolor\n## 92           6.1          4.6 versicolor\n## 93           5.8          4.0 versicolor\n## 94           5.0          3.3 versicolor\n## 95           5.6          4.2 versicolor\n## 96           5.7          4.2 versicolor\n## 97           5.7          4.2 versicolor\n## 98           6.2          4.3 versicolor\n## 99           5.1          3.0 versicolor\n## 100          5.7          4.1 versicolor\n## 101          6.3          6.0  virginica\n## 102          5.8          5.1  virginica\n## 103          7.1          5.9  virginica\n## 104          6.3          5.6  virginica\n## 105          6.5          5.8  virginica\n## 106          7.6          6.6  virginica\n## 107          4.9          4.5  virginica\n## 108          7.3          6.3  virginica\n## 109          6.7          5.8  virginica\n## 110          7.2          6.1  virginica\n## 111          6.5          5.1  virginica\n## 112          6.4          5.3  virginica\n## 113          6.8          5.5  virginica\n## 114          5.7          5.0  virginica\n## 115          5.8          5.1  virginica\n## 116          6.4          5.3  virginica\n## 117          6.5          5.5  virginica\n## 118          7.7          6.7  virginica\n## 119          7.7          6.9  virginica\n## 120          6.0          5.0  virginica\n## 121          6.9          5.7  virginica\n## 122          5.6          4.9  virginica\n## 123          7.7          6.7  virginica\n## 124          6.3          4.9  virginica\n## 125          6.7          5.7  virginica\n## 126          7.2          6.0  virginica\n## 127          6.2          4.8  virginica\n## 128          6.1          4.9  virginica\n## 129          6.4          5.6  virginica\n## 130          7.2          5.8  virginica\n## 131          7.4          6.1  virginica\n## 132          7.9          6.4  virginica\n## 133          6.4          5.6  virginica\n## 134          6.3          5.1  virginica\n## 135          6.1          5.6  virginica\n## 136          7.7          6.1  virginica\n## 137          6.3          5.6  virginica\n## 138          6.4          5.5  virginica\n## 139          6.0          4.8  virginica\n## 140          6.9          5.4  virginica\n## 141          6.7          5.6  virginica\n## 142          6.9          5.1  virginica\n## 143          5.8          5.1  virginica\n## 144          6.8          5.9  virginica\n## 145          6.7          5.7  virginica\n## 146          6.7          5.2  virginica\n## 147          6.3          5.0  virginica\n## 148          6.5          5.2  virginica\n## 149          6.2          5.4  virginica\n## 150          5.9          5.1  virginica\niris[, c(\"Sepal.Length\", \"Species\")] # 1, 5열의 모든 데이터\n##     Sepal.Length    Species\n## 1            5.1     setosa\n## 2            4.9     setosa\n## 3            4.7     setosa\n## 4            4.6     setosa\n## 5            5.0     setosa\n## 6            5.4     setosa\n## 7            4.6     setosa\n## 8            5.0     setosa\n## 9            4.4     setosa\n## 10           4.9     setosa\n## 11           5.4     setosa\n## 12           4.8     setosa\n## 13           4.8     setosa\n## 14           4.3     setosa\n## 15           5.8     setosa\n## 16           5.7     setosa\n## 17           5.4     setosa\n## 18           5.1     setosa\n## 19           5.7     setosa\n## 20           5.1     setosa\n## 21           5.4     setosa\n## 22           5.1     setosa\n## 23           4.6     setosa\n## 24           5.1     setosa\n## 25           4.8     setosa\n## 26           5.0     setosa\n## 27           5.0     setosa\n## 28           5.2     setosa\n## 29           5.2     setosa\n## 30           4.7     setosa\n## 31           4.8     setosa\n## 32           5.4     setosa\n## 33           5.2     setosa\n## 34           5.5     setosa\n## 35           4.9     setosa\n## 36           5.0     setosa\n## 37           5.5     setosa\n## 38           4.9     setosa\n## 39           4.4     setosa\n## 40           5.1     setosa\n## 41           5.0     setosa\n## 42           4.5     setosa\n## 43           4.4     setosa\n## 44           5.0     setosa\n## 45           5.1     setosa\n## 46           4.8     setosa\n## 47           5.1     setosa\n## 48           4.6     setosa\n## 49           5.3     setosa\n## 50           5.0     setosa\n## 51           7.0 versicolor\n## 52           6.4 versicolor\n## 53           6.9 versicolor\n## 54           5.5 versicolor\n## 55           6.5 versicolor\n## 56           5.7 versicolor\n## 57           6.3 versicolor\n## 58           4.9 versicolor\n## 59           6.6 versicolor\n## 60           5.2 versicolor\n## 61           5.0 versicolor\n## 62           5.9 versicolor\n## 63           6.0 versicolor\n## 64           6.1 versicolor\n## 65           5.6 versicolor\n## 66           6.7 versicolor\n## 67           5.6 versicolor\n## 68           5.8 versicolor\n## 69           6.2 versicolor\n## 70           5.6 versicolor\n## 71           5.9 versicolor\n## 72           6.1 versicolor\n## 73           6.3 versicolor\n## 74           6.1 versicolor\n## 75           6.4 versicolor\n## 76           6.6 versicolor\n## 77           6.8 versicolor\n## 78           6.7 versicolor\n## 79           6.0 versicolor\n## 80           5.7 versicolor\n## 81           5.5 versicolor\n## 82           5.5 versicolor\n## 83           5.8 versicolor\n## 84           6.0 versicolor\n## 85           5.4 versicolor\n## 86           6.0 versicolor\n## 87           6.7 versicolor\n## 88           6.3 versicolor\n## 89           5.6 versicolor\n## 90           5.5 versicolor\n## 91           5.5 versicolor\n## 92           6.1 versicolor\n## 93           5.8 versicolor\n## 94           5.0 versicolor\n## 95           5.6 versicolor\n## 96           5.7 versicolor\n## 97           5.7 versicolor\n## 98           6.2 versicolor\n## 99           5.1 versicolor\n## 100          5.7 versicolor\n## 101          6.3  virginica\n## 102          5.8  virginica\n## 103          7.1  virginica\n## 104          6.3  virginica\n## 105          6.5  virginica\n## 106          7.6  virginica\n## 107          4.9  virginica\n## 108          7.3  virginica\n## 109          6.7  virginica\n## 110          7.2  virginica\n## 111          6.5  virginica\n## 112          6.4  virginica\n## 113          6.8  virginica\n## 114          5.7  virginica\n## 115          5.8  virginica\n## 116          6.4  virginica\n## 117          6.5  virginica\n## 118          7.7  virginica\n## 119          7.7  virginica\n## 120          6.0  virginica\n## 121          6.9  virginica\n## 122          5.6  virginica\n## 123          7.7  virginica\n## 124          6.3  virginica\n## 125          6.7  virginica\n## 126          7.2  virginica\n## 127          6.2  virginica\n## 128          6.1  virginica\n## 129          6.4  virginica\n## 130          7.2  virginica\n## 131          7.4  virginica\n## 132          7.9  virginica\n## 133          6.4  virginica\n## 134          6.3  virginica\n## 135          6.1  virginica\n## 136          7.7  virginica\n## 137          6.3  virginica\n## 138          6.4  virginica\n## 139          6.0  virginica\n## 140          6.9  virginica\n## 141          6.7  virginica\n## 142          6.9  virginica\n## 143          5.8  virginica\n## 144          6.8  virginica\n## 145          6.7  virginica\n## 146          6.7  virginica\n## 147          6.3  virginica\n## 148          6.5  virginica\n## 149          6.2  virginica\n## 150          5.9  virginica\niris[1:5, ] # 1~5행의 모든 데이터\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\niris[1:5, c(1, 3)] # 1~5행의 데이터 중 1, 3열의 데이터\n##   Sepal.Length Petal.Length\n## 1          5.1          1.4\n## 2          4.9          1.4\n## 3          4.7          1.3\n## 4          4.6          1.5\n## 5          5.0          1.4\n\n\ndim(iris) # 행과 열의 개수 출력\n## [1] 150   5\nnrow(iris) # 행의 개수 출력\n## [1] 150\nncol(iris) # 열의 개수 출력\n## [1] 5\ncolnames(iris) # 열 이름 출력, names()와 결과 동일\n## [1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"\nhead(iris) # 데이터셋의 앞부분 일부 출력\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\ntail(iris) # 데이터셋의 뒷부분 일부 출력\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n## 145          6.7         3.3          5.7         2.5 virginica\n## 146          6.7         3.0          5.2         2.3 virginica\n## 147          6.3         2.5          5.0         1.9 virginica\n## 148          6.5         3.0          5.2         2.0 virginica\n## 149          6.2         3.4          5.4         2.3 virginica\n## 150          5.9         3.0          5.1         1.8 virginica\n\nhead(iris, 10)\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1           5.1         3.5          1.4         0.2  setosa\n## 2           4.9         3.0          1.4         0.2  setosa\n## 3           4.7         3.2          1.3         0.2  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\n## 5           5.0         3.6          1.4         0.2  setosa\n## 6           5.4         3.9          1.7         0.4  setosa\n## 7           4.6         3.4          1.4         0.3  setosa\n## 8           5.0         3.4          1.5         0.2  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\ntail(iris, 20)\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n## 131          7.4         2.8          6.1         1.9 virginica\n## 132          7.9         3.8          6.4         2.0 virginica\n## 133          6.4         2.8          5.6         2.2 virginica\n## 134          6.3         2.8          5.1         1.5 virginica\n## 135          6.1         2.6          5.6         1.4 virginica\n## 136          7.7         3.0          6.1         2.3 virginica\n## 137          6.3         3.4          5.6         2.4 virginica\n## 138          6.4         3.1          5.5         1.8 virginica\n## 139          6.0         3.0          4.8         1.8 virginica\n## 140          6.9         3.1          5.4         2.1 virginica\n## 141          6.7         3.1          5.6         2.4 virginica\n## 142          6.9         3.1          5.1         2.3 virginica\n## 143          5.8         2.7          5.1         1.9 virginica\n## 144          6.8         3.2          5.9         2.3 virginica\n## 145          6.7         3.3          5.7         2.5 virginica\n## 146          6.7         3.0          5.2         2.3 virginica\n## 147          6.3         2.5          5.0         1.9 virginica\n## 148          6.5         3.0          5.2         2.0 virginica\n## 149          6.2         3.4          5.4         2.3 virginica\n## 150          5.9         3.0          5.1         1.8 virginica\n\n\nstr(iris) # 데이터셋 요약 정보 보기\n## 'data.frame':    150 obs. of  5 variables:\n##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n##  $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\niris[, 5] # 품종 데이터 보기\n##   [1] setosa     setosa     setosa     setosa     setosa     setosa    \n##   [7] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [13] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [19] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [25] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [31] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [37] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [43] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [49] setosa     setosa     versicolor versicolor versicolor versicolor\n##  [55] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [61] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [67] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [73] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [79] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [85] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [91] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [97] versicolor versicolor versicolor versicolor virginica  virginica \n## [103] virginica  virginica  virginica  virginica  virginica  virginica \n## [109] virginica  virginica  virginica  virginica  virginica  virginica \n## [115] virginica  virginica  virginica  virginica  virginica  virginica \n## [121] virginica  virginica  virginica  virginica  virginica  virginica \n## [127] virginica  virginica  virginica  virginica  virginica  virginica \n## [133] virginica  virginica  virginica  virginica  virginica  virginica \n## [139] virginica  virginica  virginica  virginica  virginica  virginica \n## [145] virginica  virginica  virginica  virginica  virginica  virginica \n## Levels: setosa versicolor virginica\nunique(iris[, 5]) # 품종의 종류 보기(중복 제거)\n## [1] setosa     versicolor virginica \n## Levels: setosa versicolor virginica\ntable(iris[, \"Species\"]) # 품종의 종류별 행의 개수 세기\n## \n##     setosa versicolor  virginica \n##         50         50         50\n\n\niris[, -5]\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width\n## 1            5.1         3.5          1.4         0.2\n## 2            4.9         3.0          1.4         0.2\n## 3            4.7         3.2          1.3         0.2\n## 4            4.6         3.1          1.5         0.2\n## 5            5.0         3.6          1.4         0.2\n## 6            5.4         3.9          1.7         0.4\n## 7            4.6         3.4          1.4         0.3\n## 8            5.0         3.4          1.5         0.2\n## 9            4.4         2.9          1.4         0.2\n## 10           4.9         3.1          1.5         0.1\n## 11           5.4         3.7          1.5         0.2\n## 12           4.8         3.4          1.6         0.2\n## 13           4.8         3.0          1.4         0.1\n## 14           4.3         3.0          1.1         0.1\n## 15           5.8         4.0          1.2         0.2\n## 16           5.7         4.4          1.5         0.4\n## 17           5.4         3.9          1.3         0.4\n## 18           5.1         3.5          1.4         0.3\n## 19           5.7         3.8          1.7         0.3\n## 20           5.1         3.8          1.5         0.3\n## 21           5.4         3.4          1.7         0.2\n## 22           5.1         3.7          1.5         0.4\n## 23           4.6         3.6          1.0         0.2\n## 24           5.1         3.3          1.7         0.5\n## 25           4.8         3.4          1.9         0.2\n## 26           5.0         3.0          1.6         0.2\n## 27           5.0         3.4          1.6         0.4\n## 28           5.2         3.5          1.5         0.2\n## 29           5.2         3.4          1.4         0.2\n## 30           4.7         3.2          1.6         0.2\n## 31           4.8         3.1          1.6         0.2\n## 32           5.4         3.4          1.5         0.4\n## 33           5.2         4.1          1.5         0.1\n## 34           5.5         4.2          1.4         0.2\n## 35           4.9         3.1          1.5         0.2\n## 36           5.0         3.2          1.2         0.2\n## 37           5.5         3.5          1.3         0.2\n## 38           4.9         3.6          1.4         0.1\n## 39           4.4         3.0          1.3         0.2\n## 40           5.1         3.4          1.5         0.2\n## 41           5.0         3.5          1.3         0.3\n## 42           4.5         2.3          1.3         0.3\n## 43           4.4         3.2          1.3         0.2\n## 44           5.0         3.5          1.6         0.6\n## 45           5.1         3.8          1.9         0.4\n## 46           4.8         3.0          1.4         0.3\n## 47           5.1         3.8          1.6         0.2\n## 48           4.6         3.2          1.4         0.2\n## 49           5.3         3.7          1.5         0.2\n## 50           5.0         3.3          1.4         0.2\n## 51           7.0         3.2          4.7         1.4\n## 52           6.4         3.2          4.5         1.5\n## 53           6.9         3.1          4.9         1.5\n## 54           5.5         2.3          4.0         1.3\n## 55           6.5         2.8          4.6         1.5\n## 56           5.7         2.8          4.5         1.3\n## 57           6.3         3.3          4.7         1.6\n## 58           4.9         2.4          3.3         1.0\n## 59           6.6         2.9          4.6         1.3\n## 60           5.2         2.7          3.9         1.4\n## 61           5.0         2.0          3.5         1.0\n## 62           5.9         3.0          4.2         1.5\n## 63           6.0         2.2          4.0         1.0\n## 64           6.1         2.9          4.7         1.4\n## 65           5.6         2.9          3.6         1.3\n## 66           6.7         3.1          4.4         1.4\n## 67           5.6         3.0          4.5         1.5\n## 68           5.8         2.7          4.1         1.0\n## 69           6.2         2.2          4.5         1.5\n## 70           5.6         2.5          3.9         1.1\n## 71           5.9         3.2          4.8         1.8\n## 72           6.1         2.8          4.0         1.3\n## 73           6.3         2.5          4.9         1.5\n## 74           6.1         2.8          4.7         1.2\n## 75           6.4         2.9          4.3         1.3\n## 76           6.6         3.0          4.4         1.4\n## 77           6.8         2.8          4.8         1.4\n## 78           6.7         3.0          5.0         1.7\n## 79           6.0         2.9          4.5         1.5\n## 80           5.7         2.6          3.5         1.0\n## 81           5.5         2.4          3.8         1.1\n## 82           5.5         2.4          3.7         1.0\n## 83           5.8         2.7          3.9         1.2\n## 84           6.0         2.7          5.1         1.6\n## 85           5.4         3.0          4.5         1.5\n## 86           6.0         3.4          4.5         1.6\n## 87           6.7         3.1          4.7         1.5\n## 88           6.3         2.3          4.4         1.3\n## 89           5.6         3.0          4.1         1.3\n## 90           5.5         2.5          4.0         1.3\n## 91           5.5         2.6          4.4         1.2\n## 92           6.1         3.0          4.6         1.4\n## 93           5.8         2.6          4.0         1.2\n## 94           5.0         2.3          3.3         1.0\n## 95           5.6         2.7          4.2         1.3\n## 96           5.7         3.0          4.2         1.2\n## 97           5.7         2.9          4.2         1.3\n## 98           6.2         2.9          4.3         1.3\n## 99           5.1         2.5          3.0         1.1\n## 100          5.7         2.8          4.1         1.3\n## 101          6.3         3.3          6.0         2.5\n## 102          5.8         2.7          5.1         1.9\n## 103          7.1         3.0          5.9         2.1\n## 104          6.3         2.9          5.6         1.8\n## 105          6.5         3.0          5.8         2.2\n## 106          7.6         3.0          6.6         2.1\n## 107          4.9         2.5          4.5         1.7\n## 108          7.3         2.9          6.3         1.8\n## 109          6.7         2.5          5.8         1.8\n## 110          7.2         3.6          6.1         2.5\n## 111          6.5         3.2          5.1         2.0\n## 112          6.4         2.7          5.3         1.9\n## 113          6.8         3.0          5.5         2.1\n## 114          5.7         2.5          5.0         2.0\n## 115          5.8         2.8          5.1         2.4\n## 116          6.4         3.2          5.3         2.3\n## 117          6.5         3.0          5.5         1.8\n## 118          7.7         3.8          6.7         2.2\n## 119          7.7         2.6          6.9         2.3\n## 120          6.0         2.2          5.0         1.5\n## 121          6.9         3.2          5.7         2.3\n## 122          5.6         2.8          4.9         2.0\n## 123          7.7         2.8          6.7         2.0\n## 124          6.3         2.7          4.9         1.8\n## 125          6.7         3.3          5.7         2.1\n## 126          7.2         3.2          6.0         1.8\n## 127          6.2         2.8          4.8         1.8\n## 128          6.1         3.0          4.9         1.8\n## 129          6.4         2.8          5.6         2.1\n## 130          7.2         3.0          5.8         1.6\n## 131          7.4         2.8          6.1         1.9\n## 132          7.9         3.8          6.4         2.0\n## 133          6.4         2.8          5.6         2.2\n## 134          6.3         2.8          5.1         1.5\n## 135          6.1         2.6          5.6         1.4\n## 136          7.7         3.0          6.1         2.3\n## 137          6.3         3.4          5.6         2.4\n## 138          6.4         3.1          5.5         1.8\n## 139          6.0         3.0          4.8         1.8\n## 140          6.9         3.1          5.4         2.1\n## 141          6.7         3.1          5.6         2.4\n## 142          6.9         3.1          5.1         2.3\n## 143          5.8         2.7          5.1         1.9\n## 144          6.8         3.2          5.9         2.3\n## 145          6.7         3.3          5.7         2.5\n## 146          6.7         3.0          5.2         2.3\n## 147          6.3         2.5          5.0         1.9\n## 148          6.5         3.0          5.2         2.0\n## 149          6.2         3.4          5.4         2.3\n## 150          5.9         3.0          5.1         1.8\ncolSums(iris[, -5]) # 열별 합계\n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n##        876.5        458.6        563.7        179.9\ncolMeans(iris[, -5]) # 열별 평균\n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n##     5.843333     3.057333     3.758000     1.199333\nrowSums(iris[, -5]) # 행별 합계\n##   [1] 10.2  9.5  9.4  9.4 10.2 11.4  9.7 10.1  8.9  9.6 10.8 10.0  9.3  8.5 11.2\n##  [16] 12.0 11.0 10.3 11.5 10.7 10.7 10.7  9.4 10.6 10.3  9.8 10.4 10.4 10.2  9.7\n##  [31]  9.7 10.7 10.9 11.3  9.7  9.6 10.5 10.0  8.9 10.2 10.1  8.4  9.1 10.7 11.2\n##  [46]  9.5 10.7  9.4 10.7  9.9 16.3 15.6 16.4 13.1 15.4 14.3 15.9 11.6 15.4 13.2\n##  [61] 11.5 14.6 13.2 15.1 13.4 15.6 14.6 13.6 14.4 13.1 15.7 14.2 15.2 14.8 14.9\n##  [76] 15.4 15.8 16.4 14.9 12.8 12.8 12.6 13.6 15.4 14.4 15.5 16.0 14.3 14.0 13.3\n##  [91] 13.7 15.1 13.6 11.6 13.8 14.1 14.1 14.7 11.7 13.9 18.1 15.5 18.1 16.6 17.5\n## [106] 19.3 13.6 18.3 16.8 19.4 16.8 16.3 17.4 15.2 16.1 17.2 16.8 20.4 19.5 14.7\n## [121] 18.1 15.3 19.2 15.7 17.8 18.2 15.6 15.8 16.9 17.6 18.2 20.1 17.0 15.7 15.7\n## [136] 19.1 17.7 16.8 15.6 17.5 17.8 17.4 15.5 18.2 18.2 17.2 15.7 16.7 17.3 15.8\nrowMeans(iris[, -5]) # 행별 평균\n##   [1] 2.550 2.375 2.350 2.350 2.550 2.850 2.425 2.525 2.225 2.400 2.700 2.500\n##  [13] 2.325 2.125 2.800 3.000 2.750 2.575 2.875 2.675 2.675 2.675 2.350 2.650\n##  [25] 2.575 2.450 2.600 2.600 2.550 2.425 2.425 2.675 2.725 2.825 2.425 2.400\n##  [37] 2.625 2.500 2.225 2.550 2.525 2.100 2.275 2.675 2.800 2.375 2.675 2.350\n##  [49] 2.675 2.475 4.075 3.900 4.100 3.275 3.850 3.575 3.975 2.900 3.850 3.300\n##  [61] 2.875 3.650 3.300 3.775 3.350 3.900 3.650 3.400 3.600 3.275 3.925 3.550\n##  [73] 3.800 3.700 3.725 3.850 3.950 4.100 3.725 3.200 3.200 3.150 3.400 3.850\n##  [85] 3.600 3.875 4.000 3.575 3.500 3.325 3.425 3.775 3.400 2.900 3.450 3.525\n##  [97] 3.525 3.675 2.925 3.475 4.525 3.875 4.525 4.150 4.375 4.825 3.400 4.575\n## [109] 4.200 4.850 4.200 4.075 4.350 3.800 4.025 4.300 4.200 5.100 4.875 3.675\n## [121] 4.525 3.825 4.800 3.925 4.450 4.550 3.900 3.950 4.225 4.400 4.550 5.025\n## [133] 4.250 3.925 3.925 4.775 4.425 4.200 3.900 4.375 4.450 4.350 3.875 4.550\n## [145] 4.550 4.300 3.925 4.175 4.325 3.950\n\n\nz &lt;- matrix(1:20, nrow = 4, ncol = 5)\nz\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\nt(z) # 행과열 방향 전환\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    2    3    4\n## [2,]    5    6    7    8\n## [3,]    9   10   11   12\n## [4,]   13   14   15   16\n## [5,]   17   18   19   20\n\n\nIR.1 &lt;- subset(iris, Species == \"setosa\")\nIR.1\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1           5.1         3.5          1.4         0.2  setosa\n## 2           4.9         3.0          1.4         0.2  setosa\n## 3           4.7         3.2          1.3         0.2  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\n## 5           5.0         3.6          1.4         0.2  setosa\n## 6           5.4         3.9          1.7         0.4  setosa\n## 7           4.6         3.4          1.4         0.3  setosa\n## 8           5.0         3.4          1.5         0.2  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\n## 11          5.4         3.7          1.5         0.2  setosa\n## 12          4.8         3.4          1.6         0.2  setosa\n## 13          4.8         3.0          1.4         0.1  setosa\n## 14          4.3         3.0          1.1         0.1  setosa\n## 15          5.8         4.0          1.2         0.2  setosa\n## 16          5.7         4.4          1.5         0.4  setosa\n## 17          5.4         3.9          1.3         0.4  setosa\n## 18          5.1         3.5          1.4         0.3  setosa\n## 19          5.7         3.8          1.7         0.3  setosa\n## 20          5.1         3.8          1.5         0.3  setosa\n## 21          5.4         3.4          1.7         0.2  setosa\n## 22          5.1         3.7          1.5         0.4  setosa\n## 23          4.6         3.6          1.0         0.2  setosa\n## 24          5.1         3.3          1.7         0.5  setosa\n## 25          4.8         3.4          1.9         0.2  setosa\n## 26          5.0         3.0          1.6         0.2  setosa\n## 27          5.0         3.4          1.6         0.4  setosa\n## 28          5.2         3.5          1.5         0.2  setosa\n## 29          5.2         3.4          1.4         0.2  setosa\n## 30          4.7         3.2          1.6         0.2  setosa\n## 31          4.8         3.1          1.6         0.2  setosa\n## 32          5.4         3.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\n## 35          4.9         3.1          1.5         0.2  setosa\n## 36          5.0         3.2          1.2         0.2  setosa\n## 37          5.5         3.5          1.3         0.2  setosa\n## 38          4.9         3.6          1.4         0.1  setosa\n## 39          4.4         3.0          1.3         0.2  setosa\n## 40          5.1         3.4          1.5         0.2  setosa\n## 41          5.0         3.5          1.3         0.3  setosa\n## 42          4.5         2.3          1.3         0.3  setosa\n## 43          4.4         3.2          1.3         0.2  setosa\n## 44          5.0         3.5          1.6         0.6  setosa\n## 45          5.1         3.8          1.9         0.4  setosa\n## 46          4.8         3.0          1.4         0.3  setosa\n## 47          5.1         3.8          1.6         0.2  setosa\n## 48          4.6         3.2          1.4         0.2  setosa\n## 49          5.3         3.7          1.5         0.2  setosa\n## 50          5.0         3.3          1.4         0.2  setosa\n\nIR.2 &lt;- subset(iris, Sepal.Length &gt; 5.0 & Sepal.Width &gt; 4.0)\nIR.2\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 16          5.7         4.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\nIR.2[, c(2, 4)] # 2, 4열의 값만 추출\n##    Sepal.Width Petal.Width\n## 16         4.4         0.4\n## 33         4.1         0.1\n## 34         4.2         0.2\n\nIR.3 &lt;- subset(iris, Sepal.Length &gt; 5.0 | Sepal.Width &gt; 4.0)\nIR.3\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 1            5.1         3.5          1.4         0.2     setosa\n## 6            5.4         3.9          1.7         0.4     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 15           5.8         4.0          1.2         0.2     setosa\n## 16           5.7         4.4          1.5         0.4     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 34           5.5         4.2          1.4         0.2     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 49           5.3         3.7          1.5         0.2     setosa\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 114          5.7         2.5          5.0         2.0  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 120          6.0         2.2          5.0         1.5  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 122          5.6         2.8          4.9         2.0  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 127          6.2         2.8          4.8         1.8  virginica\n## 128          6.1         3.0          4.9         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\n\n\na &lt;- matrix(1:20, 4, 5)\nb &lt;- matrix(21:40, 4, 5)\na ; b\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   21   25   29   33   37\n## [2,]   22   26   30   34   38\n## [3,]   23   27   31   35   39\n## [4,]   24   28   32   36   40\n\n2 * a # 매트릭스 a에 저장된 값들에 2를 곱하기\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    2   10   18   26   34\n## [2,]    4   12   20   28   36\n## [3,]    6   14   22   30   38\n## [4,]    8   16   24   32   40\nb - 5\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   16   20   24   28   32\n## [2,]   17   21   25   29   33\n## [3,]   18   22   26   30   34\n## [4,]   19   23   27   31   35\n2 * a + 3 * b\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   65   85  105  125  145\n## [2,]   70   90  110  130  150\n## [3,]   75   95  115  135  155\n## [4,]   80  100  120  140  160\n\na + b\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   22   30   38   46   54\n## [2,]   24   32   40   48   56\n## [3,]   26   34   42   50   58\n## [4,]   28   36   44   52   60\nb - a\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   20   20   20   20   20\n## [2,]   20   20   20   20   20\n## [3,]   20   20   20   20   20\n## [4,]   20   20   20   20   20\nb / a\n##           [,1]     [,2]     [,3]     [,4]     [,5]\n## [1,] 21.000000 5.000000 3.222222 2.538462 2.176471\n## [2,] 11.000000 4.333333 3.000000 2.428571 2.111111\n## [3,]  7.666667 3.857143 2.818182 2.333333 2.052632\n## [4,]  6.000000 3.500000 2.666667 2.250000 2.000000\na * b\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   21  125  261  429  629\n## [2,]   44  156  300  476  684\n## [3,]   69  189  341  525  741\n## [4,]   96  224  384  576  800\n\na &lt;- a * 3\na\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    3   15   27   39   51\n## [2,]    6   18   30   42   54\n## [3,]    9   21   33   45   57\n## [4,]   12   24   36   48   60\nb &lt;- b - 5\nb\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   16   20   24   28   32\n## [2,]   17   21   25   29   33\n## [3,]   18   22   26   30   34\n## [4,]   19   23   27   31   35\n\n\nclass(iris) # iris 데이터셋의 자료구조 확인\n## [1] \"data.frame\"\nclass(state.x77) # state.x77 데이터셋의 자료구조 확인\n## [1] \"matrix\" \"array\"\nis.matrix(iris) # 데이터셋이 매트릭스인지를 확인하는 함수\n## [1] FALSE\nis.data.frame(iris) # 데이터셋이 데이터프레임인지를 확인하는 함수\n## [1] TRUE\nis.matrix(state.x77)\n## [1] TRUE\nis.data.frame(state.x77)\n## [1] FALSE\n\n\n# 매트릭스를 데이터프레임으로 변환\nst &lt;- data.frame(state.x77)\nhead(st)\n##            Population Income Illiteracy Life.Exp Murder HS.Grad Frost   Area\n## Alabama          3615   3624        2.1    69.05   15.1    41.3    20  50708\n## Alaska            365   6315        1.5    69.31   11.3    66.7   152 566432\n## Arizona          2212   4530        1.8    70.55    7.8    58.1    15 113417\n## Arkansas         2110   3378        1.9    70.66   10.1    39.9    65  51945\n## California      21198   5114        1.1    71.71   10.3    62.6    20 156361\n## Colorado         2541   4884        0.7    72.06    6.8    63.9   166 103766\nclass(st)\n## [1] \"data.frame\"\n\n\niris[, \"Species\"] # 결과=벡터. 매트릭스와 데이터프레임 모두 가능\n##   [1] setosa     setosa     setosa     setosa     setosa     setosa    \n##   [7] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [13] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [19] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [25] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [31] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [37] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [43] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [49] setosa     setosa     versicolor versicolor versicolor versicolor\n##  [55] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [61] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [67] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [73] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [79] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [85] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [91] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [97] versicolor versicolor versicolor versicolor virginica  virginica \n## [103] virginica  virginica  virginica  virginica  virginica  virginica \n## [109] virginica  virginica  virginica  virginica  virginica  virginica \n## [115] virginica  virginica  virginica  virginica  virginica  virginica \n## [121] virginica  virginica  virginica  virginica  virginica  virginica \n## [127] virginica  virginica  virginica  virginica  virginica  virginica \n## [133] virginica  virginica  virginica  virginica  virginica  virginica \n## [139] virginica  virginica  virginica  virginica  virginica  virginica \n## [145] virginica  virginica  virginica  virginica  virginica  virginica \n## Levels: setosa versicolor virginica\niris[, 5] # 결과=벡터. 매트릭스와 데이터프레임 모두 가능\n##   [1] setosa     setosa     setosa     setosa     setosa     setosa    \n##   [7] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [13] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [19] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [25] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [31] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [37] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [43] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [49] setosa     setosa     versicolor versicolor versicolor versicolor\n##  [55] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [61] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [67] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [73] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [79] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [85] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [91] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [97] versicolor versicolor versicolor versicolor virginica  virginica \n## [103] virginica  virginica  virginica  virginica  virginica  virginica \n## [109] virginica  virginica  virginica  virginica  virginica  virginica \n## [115] virginica  virginica  virginica  virginica  virginica  virginica \n## [121] virginica  virginica  virginica  virginica  virginica  virginica \n## [127] virginica  virginica  virginica  virginica  virginica  virginica \n## [133] virginica  virginica  virginica  virginica  virginica  virginica \n## [139] virginica  virginica  virginica  virginica  virginica  virginica \n## [145] virginica  virginica  virginica  virginica  virginica  virginica \n## Levels: setosa versicolor virginica\niris[\"Species\"] # 결과=데이터프레임. 데이터프레임만 가능\n##        Species\n## 1       setosa\n## 2       setosa\n## 3       setosa\n## 4       setosa\n## 5       setosa\n## 6       setosa\n## 7       setosa\n## 8       setosa\n## 9       setosa\n## 10      setosa\n## 11      setosa\n## 12      setosa\n## 13      setosa\n## 14      setosa\n## 15      setosa\n## 16      setosa\n## 17      setosa\n## 18      setosa\n## 19      setosa\n## 20      setosa\n## 21      setosa\n## 22      setosa\n## 23      setosa\n## 24      setosa\n## 25      setosa\n## 26      setosa\n## 27      setosa\n## 28      setosa\n## 29      setosa\n## 30      setosa\n## 31      setosa\n## 32      setosa\n## 33      setosa\n## 34      setosa\n## 35      setosa\n## 36      setosa\n## 37      setosa\n## 38      setosa\n## 39      setosa\n## 40      setosa\n## 41      setosa\n## 42      setosa\n## 43      setosa\n## 44      setosa\n## 45      setosa\n## 46      setosa\n## 47      setosa\n## 48      setosa\n## 49      setosa\n## 50      setosa\n## 51  versicolor\n## 52  versicolor\n## 53  versicolor\n## 54  versicolor\n## 55  versicolor\n## 56  versicolor\n## 57  versicolor\n## 58  versicolor\n## 59  versicolor\n## 60  versicolor\n## 61  versicolor\n## 62  versicolor\n## 63  versicolor\n## 64  versicolor\n## 65  versicolor\n## 66  versicolor\n## 67  versicolor\n## 68  versicolor\n## 69  versicolor\n## 70  versicolor\n## 71  versicolor\n## 72  versicolor\n## 73  versicolor\n## 74  versicolor\n## 75  versicolor\n## 76  versicolor\n## 77  versicolor\n## 78  versicolor\n## 79  versicolor\n## 80  versicolor\n## 81  versicolor\n## 82  versicolor\n## 83  versicolor\n## 84  versicolor\n## 85  versicolor\n## 86  versicolor\n## 87  versicolor\n## 88  versicolor\n## 89  versicolor\n## 90  versicolor\n## 91  versicolor\n## 92  versicolor\n## 93  versicolor\n## 94  versicolor\n## 95  versicolor\n## 96  versicolor\n## 97  versicolor\n## 98  versicolor\n## 99  versicolor\n## 100 versicolor\n## 101  virginica\n## 102  virginica\n## 103  virginica\n## 104  virginica\n## 105  virginica\n## 106  virginica\n## 107  virginica\n## 108  virginica\n## 109  virginica\n## 110  virginica\n## 111  virginica\n## 112  virginica\n## 113  virginica\n## 114  virginica\n## 115  virginica\n## 116  virginica\n## 117  virginica\n## 118  virginica\n## 119  virginica\n## 120  virginica\n## 121  virginica\n## 122  virginica\n## 123  virginica\n## 124  virginica\n## 125  virginica\n## 126  virginica\n## 127  virginica\n## 128  virginica\n## 129  virginica\n## 130  virginica\n## 131  virginica\n## 132  virginica\n## 133  virginica\n## 134  virginica\n## 135  virginica\n## 136  virginica\n## 137  virginica\n## 138  virginica\n## 139  virginica\n## 140  virginica\n## 141  virginica\n## 142  virginica\n## 143  virginica\n## 144  virginica\n## 145  virginica\n## 146  virginica\n## 147  virginica\n## 148  virginica\n## 149  virginica\n## 150  virginica\niris[5] # 결과=데이터프레임. 데이터프레임만 가능\n##        Species\n## 1       setosa\n## 2       setosa\n## 3       setosa\n## 4       setosa\n## 5       setosa\n## 6       setosa\n## 7       setosa\n## 8       setosa\n## 9       setosa\n## 10      setosa\n## 11      setosa\n## 12      setosa\n## 13      setosa\n## 14      setosa\n## 15      setosa\n## 16      setosa\n## 17      setosa\n## 18      setosa\n## 19      setosa\n## 20      setosa\n## 21      setosa\n## 22      setosa\n## 23      setosa\n## 24      setosa\n## 25      setosa\n## 26      setosa\n## 27      setosa\n## 28      setosa\n## 29      setosa\n## 30      setosa\n## 31      setosa\n## 32      setosa\n## 33      setosa\n## 34      setosa\n## 35      setosa\n## 36      setosa\n## 37      setosa\n## 38      setosa\n## 39      setosa\n## 40      setosa\n## 41      setosa\n## 42      setosa\n## 43      setosa\n## 44      setosa\n## 45      setosa\n## 46      setosa\n## 47      setosa\n## 48      setosa\n## 49      setosa\n## 50      setosa\n## 51  versicolor\n## 52  versicolor\n## 53  versicolor\n## 54  versicolor\n## 55  versicolor\n## 56  versicolor\n## 57  versicolor\n## 58  versicolor\n## 59  versicolor\n## 60  versicolor\n## 61  versicolor\n## 62  versicolor\n## 63  versicolor\n## 64  versicolor\n## 65  versicolor\n## 66  versicolor\n## 67  versicolor\n## 68  versicolor\n## 69  versicolor\n## 70  versicolor\n## 71  versicolor\n## 72  versicolor\n## 73  versicolor\n## 74  versicolor\n## 75  versicolor\n## 76  versicolor\n## 77  versicolor\n## 78  versicolor\n## 79  versicolor\n## 80  versicolor\n## 81  versicolor\n## 82  versicolor\n## 83  versicolor\n## 84  versicolor\n## 85  versicolor\n## 86  versicolor\n## 87  versicolor\n## 88  versicolor\n## 89  versicolor\n## 90  versicolor\n## 91  versicolor\n## 92  versicolor\n## 93  versicolor\n## 94  versicolor\n## 95  versicolor\n## 96  versicolor\n## 97  versicolor\n## 98  versicolor\n## 99  versicolor\n## 100 versicolor\n## 101  virginica\n## 102  virginica\n## 103  virginica\n## 104  virginica\n## 105  virginica\n## 106  virginica\n## 107  virginica\n## 108  virginica\n## 109  virginica\n## 110  virginica\n## 111  virginica\n## 112  virginica\n## 113  virginica\n## 114  virginica\n## 115  virginica\n## 116  virginica\n## 117  virginica\n## 118  virginica\n## 119  virginica\n## 120  virginica\n## 121  virginica\n## 122  virginica\n## 123  virginica\n## 124  virginica\n## 125  virginica\n## 126  virginica\n## 127  virginica\n## 128  virginica\n## 129  virginica\n## 130  virginica\n## 131  virginica\n## 132  virginica\n## 133  virginica\n## 134  virginica\n## 135  virginica\n## 136  virginica\n## 137  virginica\n## 138  virginica\n## 139  virginica\n## 140  virginica\n## 141  virginica\n## 142  virginica\n## 143  virginica\n## 144  virginica\n## 145  virginica\n## 146  virginica\n## 147  virginica\n## 148  virginica\n## 149  virginica\n## 150  virginica\niris$Species # 결과=벡터. 데이터프레임만 가능\n##   [1] setosa     setosa     setosa     setosa     setosa     setosa    \n##   [7] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [13] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [19] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [25] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [31] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [37] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [43] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [49] setosa     setosa     versicolor versicolor versicolor versicolor\n##  [55] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [61] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [67] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [73] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [79] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [85] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [91] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [97] versicolor versicolor versicolor versicolor virginica  virginica \n## [103] virginica  virginica  virginica  virginica  virginica  virginica \n## [109] virginica  virginica  virginica  virginica  virginica  virginica \n## [115] virginica  virginica  virginica  virginica  virginica  virginica \n## [121] virginica  virginica  virginica  virginica  virginica  virginica \n## [127] virginica  virginica  virginica  virginica  virginica  virginica \n## [133] virginica  virginica  virginica  virginica  virginica  virginica \n## [139] virginica  virginica  virginica  virginica  virginica  virginica \n## [145] virginica  virginica  virginica  virginica  virginica  virginica \n## Levels: setosa versicolor virginica\n\n\ngetwd()\n## [1] \"C:/Users/Hyunsoo Kim/Desktop/senior_grade/blog/my-quarto-website\"\n# setwd(\"G:/내 드라이브/202202/R_Basic/data\") # 작업 폴더 지정\nair &lt;- read.csv(\"./R_Basic/data/airquality.csv\", header = T) # .csv 파일 읽기\nhead(air)\n##                                    version.https...git.lfs.github.com.spec.v1\n## 1 oid sha256:6fdc84af524856a54abe063336bfea6511e9fb5dfcd2ec6e1dfa9e1e4d8c7357\n## 2                                                                   size 3044\n\n\nmy.iris &lt;- subset(iris, Species = 'Setosa') # Setosa 품종 데이터만 추출\n## Warning: In subset.data.frame(iris, Species = \"Setosa\") :\n##  extra argument 'Species' will be disregarded\nwrite.csv(my.iris, \"./R_Basic/data/my_iris_1.csv\") # .csv 파일에 저장하기\n\n\n\n\n\njob.type &lt;- 'A'\nif (job.type == 'B') {\n    bonus &lt;- 200 # 직무 유형이 B일 때 실행\n} else {\n    bonus &lt;- 100 # 직무 유형이 B가 아닌 나머지 경우 실행\n}\nprint(bonus)\n## [1] 100\n\n\njob.type &lt;- 'B'\nbonus &lt;- 100\nif (job.type == 'A') {\n    bonus &lt;- 200 # 직무 유형이 A일 때 실행\n}\nprint(bonus)\n## [1] 100\n\n\nscore &lt;- 85\n\nif (score &gt; 90) {\n    grade &lt;- 'A'\n} else if (score &gt; 80) {\n    grade &lt;- 'B'\n} else if (score &gt; 70) {\n    grade &lt;- 'C'\n} else if (score &gt; 60) {\n    grade &lt;- 'D'\n} else {\n    grade &lt;- 'F'\n}\n\nprint(grade)\n## [1] \"B\"\n\n\na &lt;- 10\nb &lt;- 20\nif (a &gt; 5 & b &gt; 5) {    # and 사용\n    print(a + b)\n}\n## [1] 30\n\nif (a &gt; 5 | b &gt; 30) {   # or 사용\n    print(a * b)\n}\n## [1] 200\n\nif (a &gt; 5 & b &gt; 30) {\n    print(a * b)\n}\n\nif (a &gt; 20 | b &gt; 30) {\n    print(a * b)\n}\n\nif (a &gt; 20 & b &gt; 15) {\n    print(a * b)\n}\n\nr_basic &lt;- 70\npython_basic &lt;- 82\n\nif (r_basic &gt; 80 & python_basic &gt; 80) {\n    grade &lt;- \"Excellent\"\n} else {\n    grade &lt;- \"Good\"\n}\ngrade\n## [1] \"Good\"\n\n\na &lt;- 10\nb &lt;- 20\n\nif (a &gt; b) {\n    c &lt;- a\n} else {\n    c &lt;- b\n}\nprint(c)\n## [1] 20\n\na &lt;- 10\nb &lt;- 20\n\nc &lt;- ifelse(a &gt; b, a, b)\nprint(c)\n## [1] 20\n\n\nfor(i in 1:5) {\n    print('*')\n}\n## [1] \"*\"\n## [1] \"*\"\n## [1] \"*\"\n## [1] \"*\"\n## [1] \"*\"\n\nfor (i in 1:5) {\n    print(i)\n}\n## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\n\nfor (i in 1:5) {\n    a &lt;- i * 2\n    print(a)\n}\n## [1] 2\n## [1] 4\n## [1] 6\n## [1] 8\n## [1] 10\n\n# for (i in 1:10000) {\n#     a &lt;- i * 2\n#     print(a)\n# }\n\n# for (i in 1:10000) {\n#     a &lt;- i * 2 / 1521 + 10000\n#     print(a)\n# }\n\n\nfor (i in 6:10) {\n    print(i)\n}\n## [1] 6\n## [1] 7\n## [1] 8\n## [1] 9\n## [1] 10\n\n\nfor(i in 1:9) {\n    cat('2 *', i, '=', 2 * i, '\\n')\n}\n## 2 * 1 = 2 \n## 2 * 2 = 4 \n## 2 * 3 = 6 \n## 2 * 4 = 8 \n## 2 * 5 = 10 \n## 2 * 6 = 12 \n## 2 * 7 = 14 \n## 2 * 8 = 16 \n## 2 * 9 = 18\n\nfor (i in 1:9) {\n    cat('2 *', i, '=', 2 * i)\n}\n## 2 * 1 = 22 * 2 = 42 * 3 = 62 * 4 = 82 * 5 = 102 * 6 = 122 * 7 = 142 * 8 = 162 * 9 = 18\n\nfor (i in 1:9) {\n    j &lt;- i:10\n    print(j)\n}\n##  [1]  1  2  3  4  5  6  7  8  9 10\n## [1]  2  3  4  5  6  7  8  9 10\n## [1]  3  4  5  6  7  8  9 10\n## [1]  4  5  6  7  8  9 10\n## [1]  5  6  7  8  9 10\n## [1]  6  7  8  9 10\n## [1]  7  8  9 10\n## [1]  8  9 10\n## [1]  9 10\n\n\nfor(i in 1:20) {\n    if (i %% 2 == 0) {  # 짝수인지 확인\n        print(i)\n    }\n}\n## [1] 2\n## [1] 4\n## [1] 6\n## [1] 8\n## [1] 10\n## [1] 12\n## [1] 14\n## [1] 16\n## [1] 18\n## [1] 20\n\n\nsum &lt;- 0\nfor (i in 1:100) {\n    sum &lt;- sum + i  # sum에 i 값을 누적\n}\nprint(sum)\n## [1] 5050\n\nsum &lt;- 0\nfor (i in 1:100) {\n    sum &lt;- sum + i\n    print(c(sum, i))\n}\n## [1] 1 1\n## [1] 3 2\n## [1] 6 3\n## [1] 10  4\n## [1] 15  5\n## [1] 21  6\n## [1] 28  7\n## [1] 36  8\n## [1] 45  9\n## [1] 55 10\n## [1] 66 11\n## [1] 78 12\n## [1] 91 13\n## [1] 105  14\n## [1] 120  15\n## [1] 136  16\n## [1] 153  17\n## [1] 171  18\n## [1] 190  19\n## [1] 210  20\n## [1] 231  21\n## [1] 253  22\n## [1] 276  23\n## [1] 300  24\n## [1] 325  25\n## [1] 351  26\n## [1] 378  27\n## [1] 406  28\n## [1] 435  29\n## [1] 465  30\n## [1] 496  31\n## [1] 528  32\n## [1] 561  33\n## [1] 595  34\n## [1] 630  35\n## [1] 666  36\n## [1] 703  37\n## [1] 741  38\n## [1] 780  39\n## [1] 820  40\n## [1] 861  41\n## [1] 903  42\n## [1] 946  43\n## [1] 990  44\n## [1] 1035   45\n## [1] 1081   46\n## [1] 1128   47\n## [1] 1176   48\n## [1] 1225   49\n## [1] 1275   50\n## [1] 1326   51\n## [1] 1378   52\n## [1] 1431   53\n## [1] 1485   54\n## [1] 1540   55\n## [1] 1596   56\n## [1] 1653   57\n## [1] 1711   58\n## [1] 1770   59\n## [1] 1830   60\n## [1] 1891   61\n## [1] 1953   62\n## [1] 2016   63\n## [1] 2080   64\n## [1] 2145   65\n## [1] 2211   66\n## [1] 2278   67\n## [1] 2346   68\n## [1] 2415   69\n## [1] 2485   70\n## [1] 2556   71\n## [1] 2628   72\n## [1] 2701   73\n## [1] 2775   74\n## [1] 2850   75\n## [1] 2926   76\n## [1] 3003   77\n## [1] 3081   78\n## [1] 3160   79\n## [1] 3240   80\n## [1] 3321   81\n## [1] 3403   82\n## [1] 3486   83\n## [1] 3570   84\n## [1] 3655   85\n## [1] 3741   86\n## [1] 3828   87\n## [1] 3916   88\n## [1] 4005   89\n## [1] 4095   90\n## [1] 4186   91\n## [1] 4278   92\n## [1] 4371   93\n## [1] 4465   94\n## [1] 4560   95\n## [1] 4656   96\n## [1] 4753   97\n## [1] 4851   98\n## [1] 4950   99\n## [1] 5050  100\nprint(sum)\n## [1] 5050\n\nsum &lt;- 0\nfor (i in 1:100) {\n    print(c(sum, i))\n    sum &lt;- sum + i\n}\n## [1] 0 1\n## [1] 1 2\n## [1] 3 3\n## [1] 6 4\n## [1] 10  5\n## [1] 15  6\n## [1] 21  7\n## [1] 28  8\n## [1] 36  9\n## [1] 45 10\n## [1] 55 11\n## [1] 66 12\n## [1] 78 13\n## [1] 91 14\n## [1] 105  15\n## [1] 120  16\n## [1] 136  17\n## [1] 153  18\n## [1] 171  19\n## [1] 190  20\n## [1] 210  21\n## [1] 231  22\n## [1] 253  23\n## [1] 276  24\n## [1] 300  25\n## [1] 325  26\n## [1] 351  27\n## [1] 378  28\n## [1] 406  29\n## [1] 435  30\n## [1] 465  31\n## [1] 496  32\n## [1] 528  33\n## [1] 561  34\n## [1] 595  35\n## [1] 630  36\n## [1] 666  37\n## [1] 703  38\n## [1] 741  39\n## [1] 780  40\n## [1] 820  41\n## [1] 861  42\n## [1] 903  43\n## [1] 946  44\n## [1] 990  45\n## [1] 1035   46\n## [1] 1081   47\n## [1] 1128   48\n## [1] 1176   49\n## [1] 1225   50\n## [1] 1275   51\n## [1] 1326   52\n## [1] 1378   53\n## [1] 1431   54\n## [1] 1485   55\n## [1] 1540   56\n## [1] 1596   57\n## [1] 1653   58\n## [1] 1711   59\n## [1] 1770   60\n## [1] 1830   61\n## [1] 1891   62\n## [1] 1953   63\n## [1] 2016   64\n## [1] 2080   65\n## [1] 2145   66\n## [1] 2211   67\n## [1] 2278   68\n## [1] 2346   69\n## [1] 2415   70\n## [1] 2485   71\n## [1] 2556   72\n## [1] 2628   73\n## [1] 2701   74\n## [1] 2775   75\n## [1] 2850   76\n## [1] 2926   77\n## [1] 3003   78\n## [1] 3081   79\n## [1] 3160   80\n## [1] 3240   81\n## [1] 3321   82\n## [1] 3403   83\n## [1] 3486   84\n## [1] 3570   85\n## [1] 3655   86\n## [1] 3741   87\n## [1] 3828   88\n## [1] 3916   89\n## [1] 4005   90\n## [1] 4095   91\n## [1] 4186   92\n## [1] 4278   93\n## [1] 4371   94\n## [1] 4465   95\n## [1] 4560   96\n## [1] 4656   97\n## [1] 4753   98\n## [1] 4851   99\n## [1] 4950  100\nprint(sum)\n## [1] 5050\n\n\nnorow &lt;- nrow(iris)                             # iris의 행의 수\nmylabel &lt;- c()                                  # 비어 있는 벡터 선언\nfor (i in 1:norow) {\n    if (iris$Petal.Length[i] &lt;= 1.6) {          # 꽃잎의 길이에 따라 레이블 결정\n        mylabel[i] &lt;- 'L'\n    } else if (iris$Petal.Length[i] &gt;= 5.1) {\n        mylabel[i] &lt;- 'H'\n    } else {\n        mylabel[i] &lt;- 'M'\n    }\n    print(c(iris$Petal.Length[i], mylabel))\n}\n## [1] \"1.4\" \"L\"  \n## [1] \"1.4\" \"L\"   \"L\"  \n## [1] \"1.3\" \"L\"   \"L\"   \"L\"  \n## [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"  \n## [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [1] \"1.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"  \n## [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"  \n## [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"  \n##  [1] \"1.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"  \n##  [1] \"1.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"  \n##  [1] \"1.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"  \n##  [1] \"1\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\"\n##  [1] \"1.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"  \n##  [1] \"1.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"  \n##  [1] \"4.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"  \n##  [1] \"4.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\"\n##  [1] \"4.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"  \n##  [1] \"3.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"  \n##  [1] \"4.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"  \n##  [1] \"4\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [58] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [1] \"4.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [58] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [1] \"4.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"  \n##  [1] \"4.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"  \n##  [1] \"4.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"5\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [58] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [77] \"M\" \"M\" \"M\"\n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"  \n##  [1] \"4.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [58] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [77] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [1] \"4.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [58] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [77] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [1] \"3.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [97] \"M\"  \n##  [1] \"4.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [97] \"M\"   \"M\"  \n##  [1] \"4.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [97] \"M\"   \"M\"   \"M\"  \n##   [1] \"3\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##   [1] \"4.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##   [1] \"6\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\"\n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"  \n##   [1] \"5.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"6.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n##   [1] \"6.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"  \n##   [1] \"5.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"  \n##   [1] \"6.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"  \n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n## [109] \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"  \n##   [1] \"5.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"  \n##   [1] \"5.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"6.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"6.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n## [109] \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n##   [1] \"5.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"  \n##   [1] \"4.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"  \n##   [1] \"6.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"  \n##   [1] \"4.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"  \n##   [1] \"5.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"  \n##   [1] \"6\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n## [109] \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"M\" \"H\" \"M\" \"H\"\n## [127] \"H\"\n##   [1] \"4.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"  \n##   [1] \"4.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"  \n##   [1] \"5.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"  \n##   [1] \"6.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"6.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"  \n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"6.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"4.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n##   [1] \"5.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"  \n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"  \n##   [1] \"5.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"   \"H\"  \n##   [1] \"5.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"   \"H\"   \"H\"  \n##   [1] \"5\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n## [109] \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"M\" \"H\" \"M\" \"H\"\n## [127] \"H\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\"\n## [145] \"H\" \"H\" \"H\" \"M\"\n##   [1] \"5.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"   \"H\"   \"H\"   \"M\"   \"H\"  \n##   [1] \"5.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"  \n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"\nprint(mylabel)                                  # 레이블 출력\n##   [1] \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\"\n## [109] \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"M\" \"H\" \"M\" \"H\" \"H\"\n## [127] \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\"\n## [145] \"H\" \"H\" \"M\" \"H\" \"H\" \"H\"\nnewds &lt;- data.frame(iris$Petal.Length, mylabel) # 꽃잎의 길이와 레이블 결합\nhead(newds)                                     # 새로운 데이터셋 내용 출력\n##   iris.Petal.Length mylabel\n## 1               1.4       L\n## 2               1.4       L\n## 3               1.3       L\n## 4               1.5       L\n## 5               1.4       L\n## 6               1.7       M\n\n\nsum &lt;- 0\ni &lt;- 1\nwhile (i &lt;= 100) {\n    sum &lt;- sum + i      # sum에 i 값을 누적\n    i &lt;- i + 1          # i 값을 1 증가시킴\n    print(c(sum, i))\n}\n## [1] 1 2\n## [1] 3 3\n## [1] 6 4\n## [1] 10  5\n## [1] 15  6\n## [1] 21  7\n## [1] 28  8\n## [1] 36  9\n## [1] 45 10\n## [1] 55 11\n## [1] 66 12\n## [1] 78 13\n## [1] 91 14\n## [1] 105  15\n## [1] 120  16\n## [1] 136  17\n## [1] 153  18\n## [1] 171  19\n## [1] 190  20\n## [1] 210  21\n## [1] 231  22\n## [1] 253  23\n## [1] 276  24\n## [1] 300  25\n## [1] 325  26\n## [1] 351  27\n## [1] 378  28\n## [1] 406  29\n## [1] 435  30\n## [1] 465  31\n## [1] 496  32\n## [1] 528  33\n## [1] 561  34\n## [1] 595  35\n## [1] 630  36\n## [1] 666  37\n## [1] 703  38\n## [1] 741  39\n## [1] 780  40\n## [1] 820  41\n## [1] 861  42\n## [1] 903  43\n## [1] 946  44\n## [1] 990  45\n## [1] 1035   46\n## [1] 1081   47\n## [1] 1128   48\n## [1] 1176   49\n## [1] 1225   50\n## [1] 1275   51\n## [1] 1326   52\n## [1] 1378   53\n## [1] 1431   54\n## [1] 1485   55\n## [1] 1540   56\n## [1] 1596   57\n## [1] 1653   58\n## [1] 1711   59\n## [1] 1770   60\n## [1] 1830   61\n## [1] 1891   62\n## [1] 1953   63\n## [1] 2016   64\n## [1] 2080   65\n## [1] 2145   66\n## [1] 2211   67\n## [1] 2278   68\n## [1] 2346   69\n## [1] 2415   70\n## [1] 2485   71\n## [1] 2556   72\n## [1] 2628   73\n## [1] 2701   74\n## [1] 2775   75\n## [1] 2850   76\n## [1] 2926   77\n## [1] 3003   78\n## [1] 3081   79\n## [1] 3160   80\n## [1] 3240   81\n## [1] 3321   82\n## [1] 3403   83\n## [1] 3486   84\n## [1] 3570   85\n## [1] 3655   86\n## [1] 3741   87\n## [1] 3828   88\n## [1] 3916   89\n## [1] 4005   90\n## [1] 4095   91\n## [1] 4186   92\n## [1] 4278   93\n## [1] 4371   94\n## [1] 4465   95\n## [1] 4560   96\n## [1] 4656   97\n## [1] 4753   98\n## [1] 4851   99\n## [1] 4950  100\n## [1] 5050  101\nprint(sum)\n## [1] 5050\n\n#---------------------------------------#\n# 오류 없이 계속 실행됨\n# sum &lt;- 0\n# i &lt;- 1\n# while(i &gt;= 1) {\n#   sum &lt;- sum + i # sum에 i 값을 누적\n#   i &lt;- i + 1 # i 값을 1 증가시킴\n#   print(c(sum,i))\n# }\n# print(sum)\n#---------------------------------------#\n\n\nsum &lt;- 0\nfor (i in 1:10) {\n    sum &lt;- sum + i\n    print(c(sum, i))\n    if (i &gt;= 5)\n        break\n}\n## [1] 1 1\n## [1] 3 2\n## [1] 6 3\n## [1] 10  4\n## [1] 15  5\nsum\n## [1] 15\n\n\nsum &lt;- 0\nfor (i in 1:10) {\n    if (i %% 2 == 0)\n        next # %% = 나머지\n    sum &lt;- sum + i\n    print(c(sum, i))\n}\n## [1] 1 1\n## [1] 4 3\n## [1] 9 5\n## [1] 16  7\n## [1] 25  9\nsum\n## [1] 25\n\n\napply(iris[, 1:4], 1, mean) # row 방향으로 함수 적용\n##   [1] 2.550 2.375 2.350 2.350 2.550 2.850 2.425 2.525 2.225 2.400 2.700 2.500\n##  [13] 2.325 2.125 2.800 3.000 2.750 2.575 2.875 2.675 2.675 2.675 2.350 2.650\n##  [25] 2.575 2.450 2.600 2.600 2.550 2.425 2.425 2.675 2.725 2.825 2.425 2.400\n##  [37] 2.625 2.500 2.225 2.550 2.525 2.100 2.275 2.675 2.800 2.375 2.675 2.350\n##  [49] 2.675 2.475 4.075 3.900 4.100 3.275 3.850 3.575 3.975 2.900 3.850 3.300\n##  [61] 2.875 3.650 3.300 3.775 3.350 3.900 3.650 3.400 3.600 3.275 3.925 3.550\n##  [73] 3.800 3.700 3.725 3.850 3.950 4.100 3.725 3.200 3.200 3.150 3.400 3.850\n##  [85] 3.600 3.875 4.000 3.575 3.500 3.325 3.425 3.775 3.400 2.900 3.450 3.525\n##  [97] 3.525 3.675 2.925 3.475 4.525 3.875 4.525 4.150 4.375 4.825 3.400 4.575\n## [109] 4.200 4.850 4.200 4.075 4.350 3.800 4.025 4.300 4.200 5.100 4.875 3.675\n## [121] 4.525 3.825 4.800 3.925 4.450 4.550 3.900 3.950 4.225 4.400 4.550 5.025\n## [133] 4.250 3.925 3.925 4.775 4.425 4.200 3.900 4.375 4.450 4.350 3.875 4.550\n## [145] 4.550 4.300 3.925 4.175 4.325 3.950\napply(iris[, 1:4], 2, mean) # col 방향으로 함수 적용\n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n##     5.843333     3.057333     3.758000     1.199333\n\nresult &lt;- c()\nfor (i in 1:4) {\n    iris_col &lt;- iris[, i]\n    iris_col_mean_temp &lt;- mean(iris_col)\n    result &lt;- c(result, iris_col_mean_temp)\n}\nresult\n## [1] 5.843333 3.057333 3.758000 1.199333\n\n\nmymax &lt;- function(x, y) {\n    num.max &lt;- x\n    if (y &gt; x) {\n        num.max &lt;- y\n    }\n    return(num.max)\n}\n\n\nmymax(10, 15)\n## [1] 15\na &lt;- mymax(20, 15)\nb &lt;- mymax(31, 45)\nprint(a + b)\n## [1] 65\n\n\nmydiv &lt;- function(x, y = 2) {\n    result &lt;- x / y\n    return(result)\n}\n\nmydiv(x = 10, y = 3) # 매개변수 이름과 매개변수값을 쌍으로 입력\n## [1] 3.333333\nmydiv(10, 3) # 매개변수값만 입력\n## [1] 3.333333\nmydiv(10) # x에 대한 값만 입력(y 값이 생략됨)\n## [1] 5\n\n\nmyfunc &lt;- function(x, y) {\n    val.sum &lt;- x + y\n    val.mul &lt;- x * y\n    return(list(sum = val.sum, mul = val.mul))\n}\n\nresult &lt;- myfunc(5, 8)\nresult\n## $sum\n## [1] 13\n## \n## $mul\n## [1] 40\ns &lt;- result$sum # 5, 8의 합\nm &lt;- result$mul # 5, 8의 곱\ncat('5+8=', s, '\\n')\n## 5+8= 13\ncat('5*8=', m, '\\n')\n## 5*8= 40\n\n\ngetwd()\n## [1] \"C:/Users/Hyunsoo Kim/Desktop/senior_grade/blog/my-quarto-website\"\n# source(\"myfunc.R\") # myfunc.R 안에 있는 함수 실행\n\na &lt;- mydiv(20, 4) # 함수 호출\nb &lt;- mydiv(30, 4) # 함수 호출\na + b\n## [1] 12.5\nmydiv(mydiv(20, 2), 5) # 함수 호출\n## [1] 2\n\n\nscore &lt;- c(76, 84, 69, 50, 95, 60, 82, 71, 88, 84)\nwhich(score == 69) # 성적이 69인 학생은 몇 번째에 있나\n## [1] 3\nwhich(score &gt;= 85) # 성적이 85 이상인 학생은 몇 번째에 있나\n## [1] 5 9\n\nmax(score) # 최고 점수는 몇 점인가\n## [1] 95\nwhich.max(score) # 최고 점수는 몇 번째에 있나\n## [1] 5\nscore[which.max(score)] # 최고 점수는 몇 점인가\n## [1] 95\n\nmin(score) # 최저 점수는 몇 점인가\n## [1] 50\nwhich.min(score) # 최저 점수는 몇 번째에 있나\n## [1] 4\nscore[which.min(score)] # 최저 점수는 몇 점인가\n## [1] 50\n\n\nscore &lt;- c(76, 84, 69, 50, 95, 60, 82, 71, 88, 84)\nidx &lt;- which(score &lt;= 60) # 성적이 60 이하인 값들의 인덱스\nidx\n## [1] 4 6\nscore[idx]\n## [1] 50 60\nscore[idx] &lt;- 61 # 성적이 60 이하인 값들은 61점으로 성적 상향 조정\nscore # 상향 조정된 성적 확인\n##  [1] 76 84 69 61 95 61 82 71 88 84\n\nidx &lt;- which(score &gt;= 80) # 성적이 80 이상인 값들의 인덱스\nidx\n## [1]  2  5  7  9 10\nscore[idx]\n## [1] 84 95 82 88 84\nscore.high &lt;- score[idx] # 성적이 80 이상인 값들만 추출하여 저장\nscore.high # score.high의 내용 확인\n## [1] 84 95 82 88 84\n\n\niris\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 1            5.1         3.5          1.4         0.2     setosa\n## 2            4.9         3.0          1.4         0.2     setosa\n## 3            4.7         3.2          1.3         0.2     setosa\n## 4            4.6         3.1          1.5         0.2     setosa\n## 5            5.0         3.6          1.4         0.2     setosa\n## 6            5.4         3.9          1.7         0.4     setosa\n## 7            4.6         3.4          1.4         0.3     setosa\n## 8            5.0         3.4          1.5         0.2     setosa\n## 9            4.4         2.9          1.4         0.2     setosa\n## 10           4.9         3.1          1.5         0.1     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 12           4.8         3.4          1.6         0.2     setosa\n## 13           4.8         3.0          1.4         0.1     setosa\n## 14           4.3         3.0          1.1         0.1     setosa\n## 15           5.8         4.0          1.2         0.2     setosa\n## 16           5.7         4.4          1.5         0.4     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 23           4.6         3.6          1.0         0.2     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 25           4.8         3.4          1.9         0.2     setosa\n## 26           5.0         3.0          1.6         0.2     setosa\n## 27           5.0         3.4          1.6         0.4     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 30           4.7         3.2          1.6         0.2     setosa\n## 31           4.8         3.1          1.6         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 34           5.5         4.2          1.4         0.2     setosa\n## 35           4.9         3.1          1.5         0.2     setosa\n## 36           5.0         3.2          1.2         0.2     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 38           4.9         3.6          1.4         0.1     setosa\n## 39           4.4         3.0          1.3         0.2     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 41           5.0         3.5          1.3         0.3     setosa\n## 42           4.5         2.3          1.3         0.3     setosa\n## 43           4.4         3.2          1.3         0.2     setosa\n## 44           5.0         3.5          1.6         0.6     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\n## 46           4.8         3.0          1.4         0.3     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 48           4.6         3.2          1.4         0.2     setosa\n## 49           5.3         3.7          1.5         0.2     setosa\n## 50           5.0         3.3          1.4         0.2     setosa\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 107          4.9         2.5          4.5         1.7  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 114          5.7         2.5          5.0         2.0  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 120          6.0         2.2          5.0         1.5  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 122          5.6         2.8          4.9         2.0  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 127          6.2         2.8          4.8         1.8  virginica\n## 128          6.1         3.0          4.9         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\niris$Petal.Length\n##   [1] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 1.5 1.6 1.4 1.1 1.2 1.5 1.3 1.4\n##  [19] 1.7 1.5 1.7 1.5 1.0 1.7 1.9 1.6 1.6 1.5 1.4 1.6 1.6 1.5 1.5 1.4 1.5 1.2\n##  [37] 1.3 1.4 1.3 1.5 1.3 1.3 1.3 1.6 1.9 1.4 1.6 1.4 1.5 1.4 4.7 4.5 4.9 4.0\n##  [55] 4.6 4.5 4.7 3.3 4.6 3.9 3.5 4.2 4.0 4.7 3.6 4.4 4.5 4.1 4.5 3.9 4.8 4.0\n##  [73] 4.9 4.7 4.3 4.4 4.8 5.0 4.5 3.5 3.8 3.7 3.9 5.1 4.5 4.5 4.7 4.4 4.1 4.0\n##  [91] 4.4 4.6 4.0 3.3 4.2 4.2 4.2 4.3 3.0 4.1 6.0 5.1 5.9 5.6 5.8 6.6 4.5 6.3\n## [109] 5.8 6.1 5.1 5.3 5.5 5.0 5.1 5.3 5.5 6.7 6.9 5.0 5.7 4.9 6.7 4.9 5.7 6.0\n## [127] 4.8 4.9 5.6 5.8 6.1 6.4 5.6 5.1 5.6 6.1 5.6 5.5 4.8 5.4 5.6 5.1 5.1 5.9\n## [145] 5.7 5.2 5.0 5.2 5.4 5.1\niris$Petal.Length &gt; 5.0\n##   [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n##  [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [97] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n## [109]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n## [121]  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n## [133]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n## [145]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\nwhich(iris$Petal.Length &gt; 5.0)\n##  [1]  84 101 102 103 104 105 106 108 109 110 111 112 113 115 116 117 118 119 121\n## [20] 123 125 126 129 130 131 132 133 134 135 136 137 138 140 141 142 143 144 145\n## [39] 146 148 149 150\n\niris$Petal.Length[iris$Petal.Length &gt; 5.0]\n##  [1] 5.1 6.0 5.1 5.9 5.6 5.8 6.6 6.3 5.8 6.1 5.1 5.3 5.5 5.1 5.3 5.5 6.7 6.9 5.7\n## [20] 6.7 5.7 6.0 5.6 5.8 6.1 6.4 5.6 5.1 5.6 6.1 5.6 5.5 5.4 5.6 5.1 5.1 5.9 5.7\n## [39] 5.2 5.2 5.4 5.1\n\nidx &lt;- which(iris$Petal.Length &gt; 5.0) # 꽃잎의 길이가 5.0 이상인 값들의 인덱스\nidx\n##  [1]  84 101 102 103 104 105 106 108 109 110 111 112 113 115 116 117 118 119 121\n## [20] 123 125 126 129 130 131 132 133 134 135 136 137 138 140 141 142 143 144 145\n## [39] 146 148 149 150\niris.big &lt;- iris[idx, ] # 인덱스에 해당하는 값만 추출하여 저장\niris.big\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\n\n\n# 1~4열의 값 중 5보다 큰 값의 행과 열의 위치\nwhich(iris[, 1:4] &gt; 5.0)\n##   [1]   1   6  11  15  16  17  18  19  20  21  22  24  28  29  32  33  34  37\n##  [19]  40  45  47  49  51  52  53  54  55  56  57  59  60  62  63  64  65  66\n##  [37]  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84\n##  [55]  85  86  87  88  89  90  91  92  93  95  96  97  98  99 100 101 102 103\n##  [73] 104 105 106 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122\n##  [91] 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140\n## [109] 141 142 143 144 145 146 147 148 149 150 384 401 402 403 404 405 406 408\n## [127] 409 410 411 412 413 415 416 417 418 419 421 423 425 426 429 430 431 432\n## [145] 433 434 435 436 437 438 440 441 442 443 444 445 446 448 449 450\nwhich(iris[, 1:4] &gt; 5.0, arr.ind = TRUE) # arr.ind = TRUE : 조건에 맞는 인덱스까지 반환\n##        row col\n##   [1,]   1   1\n##   [2,]   6   1\n##   [3,]  11   1\n##   [4,]  15   1\n##   [5,]  16   1\n##   [6,]  17   1\n##   [7,]  18   1\n##   [8,]  19   1\n##   [9,]  20   1\n##  [10,]  21   1\n##  [11,]  22   1\n##  [12,]  24   1\n##  [13,]  28   1\n##  [14,]  29   1\n##  [15,]  32   1\n##  [16,]  33   1\n##  [17,]  34   1\n##  [18,]  37   1\n##  [19,]  40   1\n##  [20,]  45   1\n##  [21,]  47   1\n##  [22,]  49   1\n##  [23,]  51   1\n##  [24,]  52   1\n##  [25,]  53   1\n##  [26,]  54   1\n##  [27,]  55   1\n##  [28,]  56   1\n##  [29,]  57   1\n##  [30,]  59   1\n##  [31,]  60   1\n##  [32,]  62   1\n##  [33,]  63   1\n##  [34,]  64   1\n##  [35,]  65   1\n##  [36,]  66   1\n##  [37,]  67   1\n##  [38,]  68   1\n##  [39,]  69   1\n##  [40,]  70   1\n##  [41,]  71   1\n##  [42,]  72   1\n##  [43,]  73   1\n##  [44,]  74   1\n##  [45,]  75   1\n##  [46,]  76   1\n##  [47,]  77   1\n##  [48,]  78   1\n##  [49,]  79   1\n##  [50,]  80   1\n##  [51,]  81   1\n##  [52,]  82   1\n##  [53,]  83   1\n##  [54,]  84   1\n##  [55,]  85   1\n##  [56,]  86   1\n##  [57,]  87   1\n##  [58,]  88   1\n##  [59,]  89   1\n##  [60,]  90   1\n##  [61,]  91   1\n##  [62,]  92   1\n##  [63,]  93   1\n##  [64,]  95   1\n##  [65,]  96   1\n##  [66,]  97   1\n##  [67,]  98   1\n##  [68,]  99   1\n##  [69,] 100   1\n##  [70,] 101   1\n##  [71,] 102   1\n##  [72,] 103   1\n##  [73,] 104   1\n##  [74,] 105   1\n##  [75,] 106   1\n##  [76,] 108   1\n##  [77,] 109   1\n##  [78,] 110   1\n##  [79,] 111   1\n##  [80,] 112   1\n##  [81,] 113   1\n##  [82,] 114   1\n##  [83,] 115   1\n##  [84,] 116   1\n##  [85,] 117   1\n##  [86,] 118   1\n##  [87,] 119   1\n##  [88,] 120   1\n##  [89,] 121   1\n##  [90,] 122   1\n##  [91,] 123   1\n##  [92,] 124   1\n##  [93,] 125   1\n##  [94,] 126   1\n##  [95,] 127   1\n##  [96,] 128   1\n##  [97,] 129   1\n##  [98,] 130   1\n##  [99,] 131   1\n## [100,] 132   1\n## [101,] 133   1\n## [102,] 134   1\n## [103,] 135   1\n## [104,] 136   1\n## [105,] 137   1\n## [106,] 138   1\n## [107,] 139   1\n## [108,] 140   1\n## [109,] 141   1\n## [110,] 142   1\n## [111,] 143   1\n## [112,] 144   1\n## [113,] 145   1\n## [114,] 146   1\n## [115,] 147   1\n## [116,] 148   1\n## [117,] 149   1\n## [118,] 150   1\n## [119,]  84   3\n## [120,] 101   3\n## [121,] 102   3\n## [122,] 103   3\n## [123,] 104   3\n## [124,] 105   3\n## [125,] 106   3\n## [126,] 108   3\n## [127,] 109   3\n## [128,] 110   3\n## [129,] 111   3\n## [130,] 112   3\n## [131,] 113   3\n## [132,] 115   3\n## [133,] 116   3\n## [134,] 117   3\n## [135,] 118   3\n## [136,] 119   3\n## [137,] 121   3\n## [138,] 123   3\n## [139,] 125   3\n## [140,] 126   3\n## [141,] 129   3\n## [142,] 130   3\n## [143,] 131   3\n## [144,] 132   3\n## [145,] 133   3\n## [146,] 134   3\n## [147,] 135   3\n## [148,] 136   3\n## [149,] 137   3\n## [150,] 138   3\n## [151,] 140   3\n## [152,] 141   3\n## [153,] 142   3\n## [154,] 143   3\n## [155,] 144   3\n## [156,] 145   3\n## [157,] 146   3\n## [158,] 148   3\n## [159,] 149   3\n## [160,] 150   3\n\nidx &lt;- which(iris[, 1:4] &gt; 5.0, arr.ind = TRUE)\niris[idx[, 1], ]\n##       Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 1              5.1         3.5          1.4         0.2     setosa\n## 6              5.4         3.9          1.7         0.4     setosa\n## 11             5.4         3.7          1.5         0.2     setosa\n## 15             5.8         4.0          1.2         0.2     setosa\n## 16             5.7         4.4          1.5         0.4     setosa\n## 17             5.4         3.9          1.3         0.4     setosa\n## 18             5.1         3.5          1.4         0.3     setosa\n## 19             5.7         3.8          1.7         0.3     setosa\n## 20             5.1         3.8          1.5         0.3     setosa\n## 21             5.4         3.4          1.7         0.2     setosa\n## 22             5.1         3.7          1.5         0.4     setosa\n## 24             5.1         3.3          1.7         0.5     setosa\n## 28             5.2         3.5          1.5         0.2     setosa\n## 29             5.2         3.4          1.4         0.2     setosa\n## 32             5.4         3.4          1.5         0.4     setosa\n## 33             5.2         4.1          1.5         0.1     setosa\n## 34             5.5         4.2          1.4         0.2     setosa\n## 37             5.5         3.5          1.3         0.2     setosa\n## 40             5.1         3.4          1.5         0.2     setosa\n## 45             5.1         3.8          1.9         0.4     setosa\n## 47             5.1         3.8          1.6         0.2     setosa\n## 49             5.3         3.7          1.5         0.2     setosa\n## 51             7.0         3.2          4.7         1.4 versicolor\n## 52             6.4         3.2          4.5         1.5 versicolor\n## 53             6.9         3.1          4.9         1.5 versicolor\n## 54             5.5         2.3          4.0         1.3 versicolor\n## 55             6.5         2.8          4.6         1.5 versicolor\n## 56             5.7         2.8          4.5         1.3 versicolor\n## 57             6.3         3.3          4.7         1.6 versicolor\n## 59             6.6         2.9          4.6         1.3 versicolor\n## 60             5.2         2.7          3.9         1.4 versicolor\n## 62             5.9         3.0          4.2         1.5 versicolor\n## 63             6.0         2.2          4.0         1.0 versicolor\n## 64             6.1         2.9          4.7         1.4 versicolor\n## 65             5.6         2.9          3.6         1.3 versicolor\n## 66             6.7         3.1          4.4         1.4 versicolor\n## 67             5.6         3.0          4.5         1.5 versicolor\n## 68             5.8         2.7          4.1         1.0 versicolor\n## 69             6.2         2.2          4.5         1.5 versicolor\n## 70             5.6         2.5          3.9         1.1 versicolor\n## 71             5.9         3.2          4.8         1.8 versicolor\n## 72             6.1         2.8          4.0         1.3 versicolor\n## 73             6.3         2.5          4.9         1.5 versicolor\n## 74             6.1         2.8          4.7         1.2 versicolor\n## 75             6.4         2.9          4.3         1.3 versicolor\n## 76             6.6         3.0          4.4         1.4 versicolor\n## 77             6.8         2.8          4.8         1.4 versicolor\n## 78             6.7         3.0          5.0         1.7 versicolor\n## 79             6.0         2.9          4.5         1.5 versicolor\n## 80             5.7         2.6          3.5         1.0 versicolor\n## 81             5.5         2.4          3.8         1.1 versicolor\n## 82             5.5         2.4          3.7         1.0 versicolor\n## 83             5.8         2.7          3.9         1.2 versicolor\n## 84             6.0         2.7          5.1         1.6 versicolor\n## 85             5.4         3.0          4.5         1.5 versicolor\n## 86             6.0         3.4          4.5         1.6 versicolor\n## 87             6.7         3.1          4.7         1.5 versicolor\n## 88             6.3         2.3          4.4         1.3 versicolor\n## 89             5.6         3.0          4.1         1.3 versicolor\n## 90             5.5         2.5          4.0         1.3 versicolor\n## 91             5.5         2.6          4.4         1.2 versicolor\n## 92             6.1         3.0          4.6         1.4 versicolor\n## 93             5.8         2.6          4.0         1.2 versicolor\n## 95             5.6         2.7          4.2         1.3 versicolor\n## 96             5.7         3.0          4.2         1.2 versicolor\n## 97             5.7         2.9          4.2         1.3 versicolor\n## 98             6.2         2.9          4.3         1.3 versicolor\n## 99             5.1         2.5          3.0         1.1 versicolor\n## 100            5.7         2.8          4.1         1.3 versicolor\n## 101            6.3         3.3          6.0         2.5  virginica\n## 102            5.8         2.7          5.1         1.9  virginica\n## 103            7.1         3.0          5.9         2.1  virginica\n## 104            6.3         2.9          5.6         1.8  virginica\n## 105            6.5         3.0          5.8         2.2  virginica\n## 106            7.6         3.0          6.6         2.1  virginica\n## 108            7.3         2.9          6.3         1.8  virginica\n## 109            6.7         2.5          5.8         1.8  virginica\n## 110            7.2         3.6          6.1         2.5  virginica\n## 111            6.5         3.2          5.1         2.0  virginica\n## 112            6.4         2.7          5.3         1.9  virginica\n## 113            6.8         3.0          5.5         2.1  virginica\n## 114            5.7         2.5          5.0         2.0  virginica\n## 115            5.8         2.8          5.1         2.4  virginica\n## 116            6.4         3.2          5.3         2.3  virginica\n## 117            6.5         3.0          5.5         1.8  virginica\n## 118            7.7         3.8          6.7         2.2  virginica\n## 119            7.7         2.6          6.9         2.3  virginica\n## 120            6.0         2.2          5.0         1.5  virginica\n## 121            6.9         3.2          5.7         2.3  virginica\n## 122            5.6         2.8          4.9         2.0  virginica\n## 123            7.7         2.8          6.7         2.0  virginica\n## 124            6.3         2.7          4.9         1.8  virginica\n## 125            6.7         3.3          5.7         2.1  virginica\n## 126            7.2         3.2          6.0         1.8  virginica\n## 127            6.2         2.8          4.8         1.8  virginica\n## 128            6.1         3.0          4.9         1.8  virginica\n## 129            6.4         2.8          5.6         2.1  virginica\n## 130            7.2         3.0          5.8         1.6  virginica\n## 131            7.4         2.8          6.1         1.9  virginica\n## 132            7.9         3.8          6.4         2.0  virginica\n## 133            6.4         2.8          5.6         2.2  virginica\n## 134            6.3         2.8          5.1         1.5  virginica\n## 135            6.1         2.6          5.6         1.4  virginica\n## 136            7.7         3.0          6.1         2.3  virginica\n## 137            6.3         3.4          5.6         2.4  virginica\n## 138            6.4         3.1          5.5         1.8  virginica\n## 139            6.0         3.0          4.8         1.8  virginica\n## 140            6.9         3.1          5.4         2.1  virginica\n## 141            6.7         3.1          5.6         2.4  virginica\n## 142            6.9         3.1          5.1         2.3  virginica\n## 143            5.8         2.7          5.1         1.9  virginica\n## 144            6.8         3.2          5.9         2.3  virginica\n## 145            6.7         3.3          5.7         2.5  virginica\n## 146            6.7         3.0          5.2         2.3  virginica\n## 147            6.3         2.5          5.0         1.9  virginica\n## 148            6.5         3.0          5.2         2.0  virginica\n## 149            6.2         3.4          5.4         2.3  virginica\n## 150            5.9         3.0          5.1         1.8  virginica\n## 84.1           6.0         2.7          5.1         1.6 versicolor\n## 101.1          6.3         3.3          6.0         2.5  virginica\n## 102.1          5.8         2.7          5.1         1.9  virginica\n## 103.1          7.1         3.0          5.9         2.1  virginica\n## 104.1          6.3         2.9          5.6         1.8  virginica\n## 105.1          6.5         3.0          5.8         2.2  virginica\n## 106.1          7.6         3.0          6.6         2.1  virginica\n## 108.1          7.3         2.9          6.3         1.8  virginica\n## 109.1          6.7         2.5          5.8         1.8  virginica\n## 110.1          7.2         3.6          6.1         2.5  virginica\n## 111.1          6.5         3.2          5.1         2.0  virginica\n## 112.1          6.4         2.7          5.3         1.9  virginica\n## 113.1          6.8         3.0          5.5         2.1  virginica\n## 115.1          5.8         2.8          5.1         2.4  virginica\n## 116.1          6.4         3.2          5.3         2.3  virginica\n## 117.1          6.5         3.0          5.5         1.8  virginica\n## 118.1          7.7         3.8          6.7         2.2  virginica\n## 119.1          7.7         2.6          6.9         2.3  virginica\n## 121.1          6.9         3.2          5.7         2.3  virginica\n## 123.1          7.7         2.8          6.7         2.0  virginica\n## 125.1          6.7         3.3          5.7         2.1  virginica\n## 126.1          7.2         3.2          6.0         1.8  virginica\n## 129.1          6.4         2.8          5.6         2.1  virginica\n## 130.1          7.2         3.0          5.8         1.6  virginica\n## 131.1          7.4         2.8          6.1         1.9  virginica\n## 132.1          7.9         3.8          6.4         2.0  virginica\n## 133.1          6.4         2.8          5.6         2.2  virginica\n## 134.1          6.3         2.8          5.1         1.5  virginica\n## 135.1          6.1         2.6          5.6         1.4  virginica\n## 136.1          7.7         3.0          6.1         2.3  virginica\n## 137.1          6.3         3.4          5.6         2.4  virginica\n## 138.1          6.4         3.1          5.5         1.8  virginica\n## 140.1          6.9         3.1          5.4         2.1  virginica\n## 141.1          6.7         3.1          5.6         2.4  virginica\n## 142.1          6.9         3.1          5.1         2.3  virginica\n## 143.1          5.8         2.7          5.1         1.9  virginica\n## 144.1          6.8         3.2          5.9         2.3  virginica\n## 145.1          6.7         3.3          5.7         2.5  virginica\n## 146.1          6.7         3.0          5.2         2.3  virginica\n## 148.1          6.5         3.0          5.2         2.0  virginica\n## 149.1          6.2         3.4          5.4         2.3  virginica\n## 150.1          5.9         3.0          5.1         1.8  virginica\n\niris[, 1:4][idx]\n##   [1] 5.1 5.4 5.4 5.8 5.7 5.4 5.1 5.7 5.1 5.4 5.1 5.1 5.2 5.2 5.4 5.2 5.5 5.5\n##  [19] 5.1 5.1 5.1 5.3 7.0 6.4 6.9 5.5 6.5 5.7 6.3 6.6 5.2 5.9 6.0 6.1 5.6 6.7\n##  [37] 5.6 5.8 6.2 5.6 5.9 6.1 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7 5.5 5.5 5.8 6.0\n##  [55] 5.4 6.0 6.7 6.3 5.6 5.5 5.5 6.1 5.8 5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 7.1\n##  [73] 6.3 6.5 7.6 7.3 6.7 7.2 6.5 6.4 6.8 5.7 5.8 6.4 6.5 7.7 7.7 6.0 6.9 5.6\n##  [91] 7.7 6.3 6.7 7.2 6.2 6.1 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 6.3 6.4 6.0 6.9\n## [109] 6.7 6.9 5.8 6.8 6.7 6.7 6.3 6.5 6.2 5.9 5.1 6.0 5.1 5.9 5.6 5.8 6.6 6.3\n## [127] 5.8 6.1 5.1 5.3 5.5 5.1 5.3 5.5 6.7 6.9 5.7 6.7 5.7 6.0 5.6 5.8 6.1 6.4\n## [145] 5.6 5.1 5.6 6.1 5.6 5.5 5.4 5.6 5.1 5.1 5.9 5.7 5.2 5.2 5.4 5.1\n\n\n\n\n\nfavorite &lt;- c('WINTER', 'SUMMER', 'SPRING', 'SUMMER', 'SUMMER',\n              'FALL', 'FALL', 'SUMMER', 'SPRING', 'SPRING')\nfavorite # favorite의 내용 출력\n##  [1] \"WINTER\" \"SUMMER\" \"SPRING\" \"SUMMER\" \"SUMMER\" \"FALL\"   \"FALL\"   \"SUMMER\"\n##  [9] \"SPRING\" \"SPRING\"\ntable(favorite) # 도수분포표 계산\n## favorite\n##   FALL SPRING SUMMER WINTER \n##      2      3      4      1\nlength(favorite)\n## [1] 10\ntable(favorite) / length(favorite) # 비율 출력\n## favorite\n##   FALL SPRING SUMMER WINTER \n##    0.2    0.3    0.4    0.1\n\n\nds &lt;- table(favorite)\nds\n## favorite\n##   FALL SPRING SUMMER WINTER \n##      2      3      4      1\nbarplot(ds, main = 'favorite season')\n\n\n\n\n\nds &lt;- table(favorite)\nds\n## favorite\n##   FALL SPRING SUMMER WINTER \n##      2      3      4      1\npie(ds, main = 'favorite season')\n\n\n\n\n\nfavorite.color &lt;- c(2, 3, 2, 1, 1, 2, 2, 1, 3, 2, 1, 3, 2, 1, 2)\nds &lt;- table(favorite.color)\nds\n## favorite.color\n## 1 2 3 \n## 5 7 3\nbarplot(ds, main = 'favorite color')\n\n\n\ncolors &lt;- c('green', 'red', 'blue')\nnames(ds) &lt;- colors # 자료값 1, 2, 3을 green, red, blue로 변경\nds\n## green   red  blue \n##     5     7     3\nbarplot(ds, main = 'favorite color', col = colors) # 색 지정 막대그래프\n\n\n\nbarplot(ds, main = 'favorite color', col = c('green', 'red', 'blue'))\npie(ds, main = 'favorite color', col = colors) # 색 지정 원그래프\n\n\n\n\n\nweight &lt;- c(60, 62, 64, 65, 68, 69)\nweight.heavy &lt;- c(weight, 120)\nweight\n## [1] 60 62 64 65 68 69\nweight.heavy\n## [1]  60  62  64  65  68  69 120\n\nmean(weight) # 평균\n## [1] 64.66667\nmean(weight.heavy) # 평균\n## [1] 72.57143\n\nmedian(weight) # 중앙값\n## [1] 64.5\nmedian(weight.heavy) # 중앙값\n## [1] 65\n\nmean(weight, trim = 0.2) # 절사평균(상하위 20% 제외)\n## [1] 64.75\nmean(weight.heavy, trim = 0.2) # 절사평균(상하위 20% 제외)\n## [1] 65.6\n\n\nmydata &lt;- c(60, 62, 64, 65, 68, 69, 120)\nquantile(mydata)\n##    0%   25%   50%   75%  100% \n##  60.0  63.0  65.0  68.5 120.0\nquantile(mydata, (0:10) / 10) # 10% 단위로 구간을 나누어 계산\n##    0%   10%   20%   30%   40%   50%   60%   70%   80%   90%  100% \n##  60.0  61.2  62.4  63.6  64.4  65.0  66.8  68.2  68.8  89.4 120.0\nsummary(mydata) # 최소값, 중앙값, 평균값, 3분위 값, 최대값\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   60.00   63.00   65.00   72.57   68.50  120.00\n\nmydata &lt;- 0:1000\nquantile(mydata)\n##   0%  25%  50%  75% 100% \n##    0  250  500  750 1000\nquantile(mydata, (0:10) / 10)\n##   0%  10%  20%  30%  40%  50%  60%  70%  80%  90% 100% \n##    0  100  200  300  400  500  600  700  800  900 1000\nsummary(mydata)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##       0     250     500     500     750    1000\n?quantile\n## starting httpd help server ... done\n\n\nmydata &lt;- c(60, 62, 64, 65, 68, 69, 120)\nvar(mydata) # 분산\n## [1] 447.2857\nsd(mydata) # 표준편차\n## [1] 21.14913\nrange(mydata) # 값의 범위\n## [1]  60 120\ndiff(range(mydata)) # 최대값, 최소값의 차이\n## [1] 60\n\n\ndist &lt;- cars[, 2] # 자동차 제동거리\nhist(dist,                            # 자료(data)\n     main = \"Histogram for 제동거리\", # 제목\n     xlab = \"제동거리\",               # x축 레이블\n     ylab = \"빈도수\",                 # y축 레이블\n     border = \"blue\",                 # 막대 테두리색\n     col = rainbow(10),               # 막대 색\n     las = 2,                         # x축 글씨 방향(0~3)\n     breaks = seq(0, 120, 10))        # 막대 개수 조절\n\n\n\n\n\ndist &lt;- cars[,2] # 자동차 제동거리(단위: 피트(ft))\nboxplot(dist, main = \"자동차 제동거리\") # ★★★★★\n\n\n\n\n\nboxplot.stats(dist)\n## $stats\n## [1]  2 26 36 56 93\n## \n## $n\n## [1] 50\n## \n## $conf\n## [1] 29.29663 42.70337\n## \n## $out\n## [1] 120\nboxplot.stats(dist)$stats\n## [1]  2 26 36 56 93\nboxplot.stats(dist)$stats[4]\n## [1] 56\n\n\nboxplot(Petal.Length ~ Species, data = iris, main = \"품종별 꽃잎의 길이\")\n\n\n\n\npar(mfrow = c(1, 3)) # 1*3 가상화면 분할\n\nbarplot(\n    table(mtcars$carb),\n    main = \"Barplot of Carburetors\",\n    xlab = \"#of carburetors\",\n    ylab = \"frequency\",\n    col = \"blue\"\n)\n\nbarplot(\n    table(mtcars$cyl),\n    main = \"Barplot of Cylender\",\n    xlab = \"#of cylender\",\n    ylab = \"frequency\",\n    col = \"red\"\n)\n\nbarplot(\n    table(mtcars$gear),\n    main = \"Barplot of Grar\",\n    xlab = \"#of gears\",\n    ylab = \"frequency\",\n    col = \"green\"\n)\n\n\n\n\npar(mfrow = c(1, 1)) # 가상화면 분할 해제\n\n\n\n\n\nwt &lt;- mtcars$wt                 # 중량 자료\nmpg &lt;- mtcars$mpg               # 연비 자료\nplot(wt, mpg,                   # 2개 변수(x축, y축)\n     main = \"중량-연비 그래프\", # 제목\n     xlab = \"중량\",             # x축 레이블\n     ylab = \"연비(MPG)\",        # y축 레이블\n     col = \"red\",               # point의 color\n     pch = 11)                  # point의 종류\n\n\n\n\n\nvars &lt;- c(\"mpg\", \"disp\", \"drat\", \"wt\") # 대상 변수(연비, 배기량, 후방차측 비율, 중량)\ntarget &lt;- mtcars[, vars]\nhead(target)\n##                    mpg disp drat    wt\n## Mazda RX4         21.0  160 3.90 2.620\n## Mazda RX4 Wag     21.0  160 3.90 2.875\n## Datsun 710        22.8  108 3.85 2.320\n## Hornet 4 Drive    21.4  258 3.08 3.215\n## Hornet Sportabout 18.7  360 3.15 3.440\n## Valiant           18.1  225 2.76 3.460\npairs(target, main = \"Multi Plots\")    # 대상 데이터\n\n\n\n\n\niris.2 &lt;- iris[, 3:4]              # 데이터 준비\npoint &lt;- as.numeric(iris$Species)  # 점의 모양\npoint                              # point 내용 출력\n##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n##  [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n##  [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3\n## [112] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n## [149] 3 3\ncolor &lt;- c(\"red\", \"green\", \"blue\") # 점의 컬러\nplot(iris.2,\n     main = \"Iris plot\",\n     pch = c(point),\n     col = color[point])\n\n\n\n\n\nbeers = c(5, 2, 9, 8, 3, 7, 3, 5, 3, 5) # 자료 입력\nbal &lt;- c(0.1, 0.03, 0.19, 0.12, 0.04, 0.0095, 0.07, 0.06, 0.02, 0.05)\ntbl &lt;- data.frame(beers, bal)           # 데이터프레임 생성\ntbl\n##    beers    bal\n## 1      5 0.1000\n## 2      2 0.0300\n## 3      9 0.1900\n## 4      8 0.1200\n## 5      3 0.0400\n## 6      7 0.0095\n## 7      3 0.0700\n## 8      5 0.0600\n## 9      3 0.0200\n## 10     5 0.0500\nplot(bal ~ beers, data = tbl)           # 산점도 plot(beers, bal)\nres &lt;- lm(bal ~ beers, data = tbl)      # 회귀식 도출\nabline(res)                             # 회귀선 그리기\n\n\n\ncor(beers, bal)                         # 상관계수 계산\n## [1] 0.6797025\n\n\ncor(iris[, 1:4]) # 4개 변수 간 상관성 분석\n##              Sepal.Length Sepal.Width Petal.Length Petal.Width\n## Sepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411\n## Sepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259\n## Petal.Length    0.8717538  -0.4284401    1.0000000   0.9628654\n## Petal.Width     0.8179411  -0.3661259    0.9628654   1.0000000\n\n\nmonth = 1:12 # 자료 입력\nlate = c(5, 8, 7, 9, 4, 6, 12, 13, 8, 6, 6, 4) # 자료 입력\nplot(month,                # x data\n     late,                 # y data\n     main = \"지각생 통계\", # 제목\n     type = \"l\",           # 그래프의 종류 선택(알파벳)\n     lty = 1,              # 선의 종류(line type) 선택\n     lwd = 1,              # 선의 굵기 선택\n     xlab = \"Month\",       # x축 레이블\n     ylab = \"Late cnt\")    # y축 레이블\n\n\n\n\n\nmonth = 1:12\nlate1 = c(5, 8, 7, 9, 4, 6, 12, 13, 8, 6, 6, 4)\nlate2 = c(4, 6, 5, 8, 7, 8, 10, 11, 6, 5, 7, 3)\nplot(month,                  # x data\n     late1,                  # y data\n     main = \"Late Students\",\n     type = \"b\",             # 그래프의 종류 선택(알파벳)\n     lty = 1,                # 선의 종류(line type) 선택\n     col = \"red\",            # 선의 색 선택\n     xlab = \"Month\",         # x축 레이블\n     ylab = \"Late cnt\",      # y축 레이블\n     ylim = c(1, 15))        # y축 값의 (하한, 상한)\n\nlines(month,                 # x data\n      late2,                 # y data\n      type = \"b\",            # 선의 종류(line type) 선택\n      col = \"blue\")          # 선의 색 선택\n\n\n\n\n\n## (1) 분석 대상 데이터셋 준비\n# install.packages(\"mlbench\")\nlibrary(mlbench)\ndata(\"BostonHousing\")\nmyds &lt;- BostonHousing[, c(\"crim\", \"rm\", \"dis\", \"tax\", \"medv\")]\n\n## (2) grp 변수 추가 ★★★★★\ngrp &lt;- c()\nfor (i in 1:nrow(myds)) {\n    # myds$medv 값에 따라 그룹 분류\n    if (myds$medv[i] &gt;= 25.0) {\n        grp[i] &lt;- \"H\"\n    } else if (myds$medv[i] &lt;= 17.0) {\n        grp[i] &lt;- \"L\"\n    } else {\n        grp[i] &lt;- \"M\"\n    }\n}\ngrp &lt;- factor(grp) # 문자 벡터를 팩터 타입으로 변경\ngrp &lt;- factor(grp, levels = c(\"H\", \"M\", \"L\")) # 레벨의 순서를 H, L, M -&gt; H, M, L\n\nmyds &lt;- data.frame(myds, grp) # myds에 grp 열 추가\n\n## (3) 데이터셋의 형태와 기본적인 내용 파악\nstr(myds)\n## 'data.frame':    506 obs. of  6 variables:\n##  $ crim: num  0.00632 0.02731 0.02729 0.03237 0.06905 ...\n##  $ rm  : num  6.58 6.42 7.18 7 7.15 ...\n##  $ dis : num  4.09 4.97 4.97 6.06 6.06 ...\n##  $ tax : num  296 242 242 222 222 222 311 311 311 311 ...\n##  $ medv: num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...\n##  $ grp : Factor w/ 3 levels \"H\",\"M\",\"L\": 2 2 1 1 1 1 2 1 3 2 ...\nhead(myds)\n##      crim    rm    dis tax medv grp\n## 1 0.00632 6.575 4.0900 296 24.0   M\n## 2 0.02731 6.421 4.9671 242 21.6   M\n## 3 0.02729 7.185 4.9671 242 34.7   H\n## 4 0.03237 6.998 6.0622 222 33.4   H\n## 5 0.06905 7.147 6.0622 222 36.2   H\n## 6 0.02985 6.430 6.0622 222 28.7   H\ntable(myds$grp) # 주택 가격 그룹별 분포\n## \n##   H   M   L \n## 132 247 127\n\n## (4) 히스토그램에 의한 관측값의 분포 확인\npar(mfrow = c(2, 3)) # 2*3 가상화면 분할\nfor (i in 1:5) {\n    hist(myds[, i], main = colnames(myds)[i], col = \"yellow\")\n}\npar(mfrow = c(1, 1)) # 2*3 가상화면 분할 해제\n\n\n\n\n## (5) 상자그림에 의한 관측값의 분포 확인\npar(mfrow = c(2, 3)) # 2*3 가상화면 분할\nfor (i in 1:5) {\n    boxplot(myds[, i], main = colnames(myds)[i])\n}\npar(mfrow = c(1, 1)) # 2*3 가상화면 분할 해제\n\n\n\n\n## (6) 그룹별 관측값 분포의 확인\nboxplot(myds$crim ~ myds$grp, main = \"1인당 범죄율\")\n\n\n\nboxplot(myds$rm ~ myds$grp, main = \"방의 개수\")\n\n\n\nboxplot(myds$dis ~ myds$grp, main = \"직업 센터까지의 거리\")\n\n\n\nboxplot(myds$tax ~ myds$grp, main = \"재산세율\")\n\n\n\n\n## (7) 다중 산점도를 통한 변수 간 상관 관계의 확인\npairs(myds[, -6]) # 6번째 열 제거(grp)\npairs(myds[, 1:5])\n\n\n\n\n## (8) 그룹 정보를 포함한 변수 간 상관 관계의 확인\npoint &lt;- as.integer(myds$grp) # 점의 모양 지정\ncolor &lt;- c(\"red\", \"green\", \"blue\") # 점의 색 지정\npairs(myds[, -6], pch = point, col = color[point])\n\n\n\n\n## (9) 변수 간 상관계수의 확인\ncor(myds[, -6])\n##            crim         rm        dis        tax       medv\n## crim  1.0000000 -0.2192467 -0.3796701  0.5827643 -0.3883046\n## rm   -0.2192467  1.0000000  0.2052462 -0.2920478  0.6953599\n## dis  -0.3796701  0.2052462  1.0000000 -0.5344316  0.2499287\n## tax   0.5827643 -0.2920478 -0.5344316  1.0000000 -0.4685359\n## medv -0.3883046  0.6953599  0.2499287 -0.4685359  1.0000000\ncor(myds[1:5])\n##            crim         rm        dis        tax       medv\n## crim  1.0000000 -0.2192467 -0.3796701  0.5827643 -0.3883046\n## rm   -0.2192467  1.0000000  0.2052462 -0.2920478  0.6953599\n## dis  -0.3796701  0.2052462  1.0000000 -0.5344316  0.2499287\n## tax   0.5827643 -0.2920478 -0.5344316  1.0000000 -0.4685359\n## medv -0.3883046  0.6953599  0.2499287 -0.4685359  1.0000000\n\n\n\n\n\nz &lt;- c(1, 2, 3, NA, 5, NA, 8)   # 결측값이 포함된 벡터 z\nsum(z)                          # 정상 계산이 안 됨\n## [1] NA\nis.na(z)                        # NA 여부 확인\n## [1] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE\nsum(is.na(z))                   # NA의 개수 확인\n## [1] 2\nsum(z, na.rm = TRUE)            # NA를 제외하고 합계를 계산\n## [1] 19\n\n\nz1 &lt;- c(1, 2, 3, NA, 5, NA, 8)          # 결측값이 포함된 벡터 z1\nz2 &lt;- c(5, 8, 1, NA, 3, NA, 7)          # 결측값이 포함된 벡터 z2\nz1[is.na(z1)] &lt;- 0                      # NA를 0으로 치환\nz1\n## [1] 1 2 3 0 5 0 8\n\nz1[is.na(z1)] &lt;- mean(z1, na.rm = TRUE) # NA를 z1의 평균값으로 치환\nz1\n## [1] 1 2 3 0 5 0 8\n\nz3 &lt;- as.vector(na.omit(z2))            # NA를 제거하고 새로운 벡터 생성\nz3\n## [1] 5 8 1 3 7\n\n\n# NA를 포함하는 test 데이터 생성\nx &lt;- iris\nx[1, 2] &lt;- NA\nx[1, 3] &lt;- NA\nx[2, 3] &lt;- NA\nx[3, 4] &lt;- NA\nhead(x)\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1          NA           NA         0.2  setosa\n## 2          4.9         3.0           NA         0.2  setosa\n## 3          4.7         3.2          1.3          NA  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\n\n\n# for문을 이용한 방법 ★★★★★\nfor (i in 1:ncol(x)) {\n    this.na &lt;- is.na(x[, i])\n    cat(colnames(x)[i], \"\\t\", sum(this.na), \"\\n\")\n}\n## Sepal.Length      0 \n## Sepal.Width   1 \n## Petal.Length      2 \n## Petal.Width   1 \n## Species   0\n\n# apply를 이용한 방법\ncol_na &lt;- function(y) {\n    return(sum(is.na(y)))\n}\n\nna_count &lt;- apply(x, 2, FUN = col_na)\nna_count\n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n##            0            1            2            1            0\n\n\nrowSums(is.na(x))           # 행별 NA의 개수\n##   [1] 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n##  [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n##  [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n## [112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n## [149] 0 0\nsum(rowSums(is.na(x)) &gt; 0)  # NA가 포함된 행의 개수\n## [1] 3\n\nsum(is.na(x))               # 데이터셋 전체에서 NA 개수\n## [1] 4\n\n\nhead(x)\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1          NA           NA         0.2  setosa\n## 2          4.9         3.0           NA         0.2  setosa\n## 3          4.7         3.2          1.3          NA  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\nx[!complete.cases(x), ]     # NA가 포함된 행들 출력\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1          NA           NA         0.2  setosa\n## 2          4.9         3.0           NA         0.2  setosa\n## 3          4.7         3.2          1.3          NA  setosa\ny &lt;- x[complete.cases(x), ] # NA가 포함된 행들 제거\nhead(y)                     # 새로운 데이터셋 y의 내용 확인\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\n## 7          4.6         3.4          1.4         0.3  setosa\n## 8          5.0         3.4          1.5         0.2  setosa\n## 9          4.4         2.9          1.4         0.2  setosa\n\n\nst &lt;- data.frame(state.x77)\nboxplot(st$Income)\n\n\n\nboxplot.stats(st$Income)\n## $stats\n## [1] 3098 3983 4519 4815 5348\n## \n## $n\n## [1] 50\n## \n## $conf\n## [1] 4333.093 4704.907\n## \n## $out\n## [1] 6315\n# stats (각 변수의 최소값, 1사분위수, 2사분위수, 3사분위수, 최대값이 저장되어 있는 행렬)\n# n (각 그룹마다의 관측값 수를 저장한 벡터)\n# conf (중앙값의 95% 신뢰구간, median+-1.58*IQR/(n)^0.5)\n# out (이상치)\nboxplot.stats(st$Income)$out\n## [1] 6315\n\n\nout.val &lt;- boxplot.stats(st$Income)$out     # 특이값 추출\n\nst$Income %in% out.val\n##  [1] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [49] FALSE FALSE\nst$Income == out.val\n##  [1] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [49] FALSE FALSE\n\nst$Income[st$Income %in% out.val] &lt;- NA     # 특이값을 NA로 대체\nst$Income[st$Income == out.val] &lt;- NA\n\nhead(st)\n##            Population Income Illiteracy Life.Exp Murder HS.Grad Frost   Area\n## Alabama          3615   3624        2.1    69.05   15.1    41.3    20  50708\n## Alaska            365     NA        1.5    69.31   11.3    66.7   152 566432\n## Arizona          2212   4530        1.8    70.55    7.8    58.1    15 113417\n## Arkansas         2110   3378        1.9    70.66   10.1    39.9    65  51945\n## California      21198   5114        1.1    71.71   10.3    62.6    20 156361\n## Colorado         2541   4884        0.7    72.06    6.8    63.9   166 103766\nnewdata &lt;- st[complete.cases(st), ]         # NA가 포함된 행 제거 ★★★★★\nhead(newdata)\n##             Population Income Illiteracy Life.Exp Murder HS.Grad Frost   Area\n## Alabama           3615   3624        2.1    69.05   15.1    41.3    20  50708\n## Arizona           2212   4530        1.8    70.55    7.8    58.1    15 113417\n## Arkansas          2110   3378        1.9    70.66   10.1    39.9    65  51945\n## California       21198   5114        1.1    71.71   10.3    62.6    20 156361\n## Colorado          2541   4884        0.7    72.06    6.8    63.9   166 103766\n## Connecticut       3100   5348        1.1    72.48    3.1    56.0   139   4862\n\n\nv1 &lt;- c(1, 7, 6, 8, 4, 2, 3)\norder(v1)\n## [1] 1 6 7 5 3 2 4\n\nv1 &lt;- sort(v1) # 오름차순\nv1\n## [1] 1 2 3 4 6 7 8\nv1[order(v1)]\n## [1] 1 2 3 4 6 7 8\n\nv2 &lt;- sort(v1, decreasing = T) # 내림차순\nv2\n## [1] 8 7 6 4 3 2 1\n\n\nhead(iris)\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\norder(iris$Sepal.Length)\n##   [1]  14   9  39  43  42   4   7  23  48   3  30  12  13  25  31  46   2  10\n##  [19]  35  38  58 107   5   8  26  27  36  41  44  50  61  94   1  18  20  22\n##  [37]  24  40  45  47  99  28  29  33  60  49   6  11  17  21  32  85  34  37\n##  [55]  54  81  82  90  91  65  67  70  89  95 122  16  19  56  80  96  97 100\n##  [73] 114  15  68  83  93 102 115 143  62  71 150  63  79  84  86 120 139  64\n##  [91]  72  74  92 128 135  69  98 127 149  57  73  88 101 104 124 134 137 147\n## [109]  52  75 112 116 129 133 138  55 105 111 117 148  59  76  66  78  87 109\n## [127] 125 141 145 146  77 113 144  53 121 140 142  51 103 110 126 130 108 131\n## [145] 106 118 119 123 136 132\niris[order(iris$Sepal.Length), ]                    # 오름차순으로 정렬\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 14           4.3         3.0          1.1         0.1     setosa\n## 9            4.4         2.9          1.4         0.2     setosa\n## 39           4.4         3.0          1.3         0.2     setosa\n## 43           4.4         3.2          1.3         0.2     setosa\n## 42           4.5         2.3          1.3         0.3     setosa\n## 4            4.6         3.1          1.5         0.2     setosa\n## 7            4.6         3.4          1.4         0.3     setosa\n## 23           4.6         3.6          1.0         0.2     setosa\n## 48           4.6         3.2          1.4         0.2     setosa\n## 3            4.7         3.2          1.3         0.2     setosa\n## 30           4.7         3.2          1.6         0.2     setosa\n## 12           4.8         3.4          1.6         0.2     setosa\n## 13           4.8         3.0          1.4         0.1     setosa\n## 25           4.8         3.4          1.9         0.2     setosa\n## 31           4.8         3.1          1.6         0.2     setosa\n## 46           4.8         3.0          1.4         0.3     setosa\n## 2            4.9         3.0          1.4         0.2     setosa\n## 10           4.9         3.1          1.5         0.1     setosa\n## 35           4.9         3.1          1.5         0.2     setosa\n## 38           4.9         3.6          1.4         0.1     setosa\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 107          4.9         2.5          4.5         1.7  virginica\n## 5            5.0         3.6          1.4         0.2     setosa\n## 8            5.0         3.4          1.5         0.2     setosa\n## 26           5.0         3.0          1.6         0.2     setosa\n## 27           5.0         3.4          1.6         0.4     setosa\n## 36           5.0         3.2          1.2         0.2     setosa\n## 41           5.0         3.5          1.3         0.3     setosa\n## 44           5.0         3.5          1.6         0.6     setosa\n## 50           5.0         3.3          1.4         0.2     setosa\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 1            5.1         3.5          1.4         0.2     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 28           5.2         3.5          1.5         0.2     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 49           5.3         3.7          1.5         0.2     setosa\n## 6            5.4         3.9          1.7         0.4     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 34           5.5         4.2          1.4         0.2     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 122          5.6         2.8          4.9         2.0  virginica\n## 16           5.7         4.4          1.5         0.4     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 114          5.7         2.5          5.0         2.0  virginica\n## 15           5.8         4.0          1.2         0.2     setosa\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 102          5.8         2.7          5.1         1.9  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 150          5.9         3.0          5.1         1.8  virginica\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 120          6.0         2.2          5.0         1.5  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 128          6.1         3.0          4.9         1.8  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 127          6.2         2.8          4.8         1.8  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 112          6.4         2.7          5.3         1.9  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 105          6.5         3.0          5.8         2.2  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 109          6.7         2.5          5.8         1.8  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 113          6.8         3.0          5.5         2.1  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 121          6.9         3.2          5.7         2.3  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 103          7.1         3.0          5.9         2.1  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\niris[order(iris$Sepal.Length, decreasing = T), ]    # 내림차순으로 정렬\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 132          7.9         3.8          6.4         2.0  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 121          6.9         3.2          5.7         2.3  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 113          6.8         3.0          5.5         2.1  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 109          6.7         2.5          5.8         1.8  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 105          6.5         3.0          5.8         2.2  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 112          6.4         2.7          5.3         1.9  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 127          6.2         2.8          4.8         1.8  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 128          6.1         3.0          4.9         1.8  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 120          6.0         2.2          5.0         1.5  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 150          5.9         3.0          5.1         1.8  virginica\n## 15           5.8         4.0          1.2         0.2     setosa\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 102          5.8         2.7          5.1         1.9  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 16           5.7         4.4          1.5         0.4     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 114          5.7         2.5          5.0         2.0  virginica\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 122          5.6         2.8          4.9         2.0  virginica\n## 34           5.5         4.2          1.4         0.2     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 6            5.4         3.9          1.7         0.4     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 49           5.3         3.7          1.5         0.2     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 1            5.1         3.5          1.4         0.2     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 5            5.0         3.6          1.4         0.2     setosa\n## 8            5.0         3.4          1.5         0.2     setosa\n## 26           5.0         3.0          1.6         0.2     setosa\n## 27           5.0         3.4          1.6         0.4     setosa\n## 36           5.0         3.2          1.2         0.2     setosa\n## 41           5.0         3.5          1.3         0.3     setosa\n## 44           5.0         3.5          1.6         0.6     setosa\n## 50           5.0         3.3          1.4         0.2     setosa\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 2            4.9         3.0          1.4         0.2     setosa\n## 10           4.9         3.1          1.5         0.1     setosa\n## 35           4.9         3.1          1.5         0.2     setosa\n## 38           4.9         3.6          1.4         0.1     setosa\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 107          4.9         2.5          4.5         1.7  virginica\n## 12           4.8         3.4          1.6         0.2     setosa\n## 13           4.8         3.0          1.4         0.1     setosa\n## 25           4.8         3.4          1.9         0.2     setosa\n## 31           4.8         3.1          1.6         0.2     setosa\n## 46           4.8         3.0          1.4         0.3     setosa\n## 3            4.7         3.2          1.3         0.2     setosa\n## 30           4.7         3.2          1.6         0.2     setosa\n## 4            4.6         3.1          1.5         0.2     setosa\n## 7            4.6         3.4          1.4         0.3     setosa\n## 23           4.6         3.6          1.0         0.2     setosa\n## 48           4.6         3.2          1.4         0.2     setosa\n## 42           4.5         2.3          1.3         0.3     setosa\n## 9            4.4         2.9          1.4         0.2     setosa\n## 39           4.4         3.0          1.3         0.2     setosa\n## 43           4.4         3.2          1.3         0.2     setosa\n## 14           4.3         3.0          1.1         0.1     setosa\n\niris.new &lt;- iris[order(iris$Sepal.Length), ]        # 정렬된 데이터를 저장\nhead(iris.new)\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 14          4.3         3.0          1.1         0.1  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 39          4.4         3.0          1.3         0.2  setosa\n## 43          4.4         3.2          1.3         0.2  setosa\n## 42          4.5         2.3          1.3         0.3  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\niris[order(iris$Species,-iris$Petal.Length, decreasing = T), ] # 정렬 기준이 2개\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 107          4.9         2.5          4.5         1.7  virginica\n## 127          6.2         2.8          4.8         1.8  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 122          5.6         2.8          4.9         2.0  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 128          6.1         3.0          4.9         1.8  virginica\n## 114          5.7         2.5          5.0         2.0  virginica\n## 120          6.0         2.2          5.0         1.5  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 101          6.3         3.3          6.0         2.5  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 23           4.6         3.6          1.0         0.2     setosa\n## 14           4.3         3.0          1.1         0.1     setosa\n## 15           5.8         4.0          1.2         0.2     setosa\n## 36           5.0         3.2          1.2         0.2     setosa\n## 3            4.7         3.2          1.3         0.2     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 39           4.4         3.0          1.3         0.2     setosa\n## 41           5.0         3.5          1.3         0.3     setosa\n## 42           4.5         2.3          1.3         0.3     setosa\n## 43           4.4         3.2          1.3         0.2     setosa\n## 1            5.1         3.5          1.4         0.2     setosa\n## 2            4.9         3.0          1.4         0.2     setosa\n## 5            5.0         3.6          1.4         0.2     setosa\n## 7            4.6         3.4          1.4         0.3     setosa\n## 9            4.4         2.9          1.4         0.2     setosa\n## 13           4.8         3.0          1.4         0.1     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 34           5.5         4.2          1.4         0.2     setosa\n## 38           4.9         3.6          1.4         0.1     setosa\n## 46           4.8         3.0          1.4         0.3     setosa\n## 48           4.6         3.2          1.4         0.2     setosa\n## 50           5.0         3.3          1.4         0.2     setosa\n## 4            4.6         3.1          1.5         0.2     setosa\n## 8            5.0         3.4          1.5         0.2     setosa\n## 10           4.9         3.1          1.5         0.1     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 16           5.7         4.4          1.5         0.4     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 35           4.9         3.1          1.5         0.2     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 49           5.3         3.7          1.5         0.2     setosa\n## 12           4.8         3.4          1.6         0.2     setosa\n## 26           5.0         3.0          1.6         0.2     setosa\n## 27           5.0         3.4          1.6         0.4     setosa\n## 30           4.7         3.2          1.6         0.2     setosa\n## 31           4.8         3.1          1.6         0.2     setosa\n## 44           5.0         3.5          1.6         0.6     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 6            5.4         3.9          1.7         0.4     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 25           4.8         3.4          1.9         0.2     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\niris[order(iris$Species, decreasing = T, iris$Petal.Length), ]\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 119          7.7         2.6          6.9         2.3  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 101          6.3         3.3          6.0         2.5  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\n## 114          5.7         2.5          5.0         2.0  virginica\n## 120          6.0         2.2          5.0         1.5  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 122          5.6         2.8          4.9         2.0  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 128          6.1         3.0          4.9         1.8  virginica\n## 127          6.2         2.8          4.8         1.8  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 107          4.9         2.5          4.5         1.7  virginica\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 25           4.8         3.4          1.9         0.2     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\n## 6            5.4         3.9          1.7         0.4     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 12           4.8         3.4          1.6         0.2     setosa\n## 26           5.0         3.0          1.6         0.2     setosa\n## 27           5.0         3.4          1.6         0.4     setosa\n## 30           4.7         3.2          1.6         0.2     setosa\n## 31           4.8         3.1          1.6         0.2     setosa\n## 44           5.0         3.5          1.6         0.6     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 4            4.6         3.1          1.5         0.2     setosa\n## 8            5.0         3.4          1.5         0.2     setosa\n## 10           4.9         3.1          1.5         0.1     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 16           5.7         4.4          1.5         0.4     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 35           4.9         3.1          1.5         0.2     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 49           5.3         3.7          1.5         0.2     setosa\n## 1            5.1         3.5          1.4         0.2     setosa\n## 2            4.9         3.0          1.4         0.2     setosa\n## 5            5.0         3.6          1.4         0.2     setosa\n## 7            4.6         3.4          1.4         0.3     setosa\n## 9            4.4         2.9          1.4         0.2     setosa\n## 13           4.8         3.0          1.4         0.1     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 34           5.5         4.2          1.4         0.2     setosa\n## 38           4.9         3.6          1.4         0.1     setosa\n## 46           4.8         3.0          1.4         0.3     setosa\n## 48           4.6         3.2          1.4         0.2     setosa\n## 50           5.0         3.3          1.4         0.2     setosa\n## 3            4.7         3.2          1.3         0.2     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 39           4.4         3.0          1.3         0.2     setosa\n## 41           5.0         3.5          1.3         0.3     setosa\n## 42           4.5         2.3          1.3         0.3     setosa\n## 43           4.4         3.2          1.3         0.2     setosa\n## 15           5.8         4.0          1.2         0.2     setosa\n## 36           5.0         3.2          1.2         0.2     setosa\n## 14           4.3         3.0          1.1         0.1     setosa\n## 23           4.6         3.6          1.0         0.2     setosa\n\n\nsp &lt;- split(iris, iris$Species) # 품종별로 데이터 분리\nsp                              # 분리 결과 확인\n## $setosa\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1           5.1         3.5          1.4         0.2  setosa\n## 2           4.9         3.0          1.4         0.2  setosa\n## 3           4.7         3.2          1.3         0.2  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\n## 5           5.0         3.6          1.4         0.2  setosa\n## 6           5.4         3.9          1.7         0.4  setosa\n## 7           4.6         3.4          1.4         0.3  setosa\n## 8           5.0         3.4          1.5         0.2  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\n## 11          5.4         3.7          1.5         0.2  setosa\n## 12          4.8         3.4          1.6         0.2  setosa\n## 13          4.8         3.0          1.4         0.1  setosa\n## 14          4.3         3.0          1.1         0.1  setosa\n## 15          5.8         4.0          1.2         0.2  setosa\n## 16          5.7         4.4          1.5         0.4  setosa\n## 17          5.4         3.9          1.3         0.4  setosa\n## 18          5.1         3.5          1.4         0.3  setosa\n## 19          5.7         3.8          1.7         0.3  setosa\n## 20          5.1         3.8          1.5         0.3  setosa\n## 21          5.4         3.4          1.7         0.2  setosa\n## 22          5.1         3.7          1.5         0.4  setosa\n## 23          4.6         3.6          1.0         0.2  setosa\n## 24          5.1         3.3          1.7         0.5  setosa\n## 25          4.8         3.4          1.9         0.2  setosa\n## 26          5.0         3.0          1.6         0.2  setosa\n## 27          5.0         3.4          1.6         0.4  setosa\n## 28          5.2         3.5          1.5         0.2  setosa\n## 29          5.2         3.4          1.4         0.2  setosa\n## 30          4.7         3.2          1.6         0.2  setosa\n## 31          4.8         3.1          1.6         0.2  setosa\n## 32          5.4         3.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\n## 35          4.9         3.1          1.5         0.2  setosa\n## 36          5.0         3.2          1.2         0.2  setosa\n## 37          5.5         3.5          1.3         0.2  setosa\n## 38          4.9         3.6          1.4         0.1  setosa\n## 39          4.4         3.0          1.3         0.2  setosa\n## 40          5.1         3.4          1.5         0.2  setosa\n## 41          5.0         3.5          1.3         0.3  setosa\n## 42          4.5         2.3          1.3         0.3  setosa\n## 43          4.4         3.2          1.3         0.2  setosa\n## 44          5.0         3.5          1.6         0.6  setosa\n## 45          5.1         3.8          1.9         0.4  setosa\n## 46          4.8         3.0          1.4         0.3  setosa\n## 47          5.1         3.8          1.6         0.2  setosa\n## 48          4.6         3.2          1.4         0.2  setosa\n## 49          5.3         3.7          1.5         0.2  setosa\n## 50          5.0         3.3          1.4         0.2  setosa\n## \n## $versicolor\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## \n## $virginica\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n## 101          6.3         3.3          6.0         2.5 virginica\n## 102          5.8         2.7          5.1         1.9 virginica\n## 103          7.1         3.0          5.9         2.1 virginica\n## 104          6.3         2.9          5.6         1.8 virginica\n## 105          6.5         3.0          5.8         2.2 virginica\n## 106          7.6         3.0          6.6         2.1 virginica\n## 107          4.9         2.5          4.5         1.7 virginica\n## 108          7.3         2.9          6.3         1.8 virginica\n## 109          6.7         2.5          5.8         1.8 virginica\n## 110          7.2         3.6          6.1         2.5 virginica\n## 111          6.5         3.2          5.1         2.0 virginica\n## 112          6.4         2.7          5.3         1.9 virginica\n## 113          6.8         3.0          5.5         2.1 virginica\n## 114          5.7         2.5          5.0         2.0 virginica\n## 115          5.8         2.8          5.1         2.4 virginica\n## 116          6.4         3.2          5.3         2.3 virginica\n## 117          6.5         3.0          5.5         1.8 virginica\n## 118          7.7         3.8          6.7         2.2 virginica\n## 119          7.7         2.6          6.9         2.3 virginica\n## 120          6.0         2.2          5.0         1.5 virginica\n## 121          6.9         3.2          5.7         2.3 virginica\n## 122          5.6         2.8          4.9         2.0 virginica\n## 123          7.7         2.8          6.7         2.0 virginica\n## 124          6.3         2.7          4.9         1.8 virginica\n## 125          6.7         3.3          5.7         2.1 virginica\n## 126          7.2         3.2          6.0         1.8 virginica\n## 127          6.2         2.8          4.8         1.8 virginica\n## 128          6.1         3.0          4.9         1.8 virginica\n## 129          6.4         2.8          5.6         2.1 virginica\n## 130          7.2         3.0          5.8         1.6 virginica\n## 131          7.4         2.8          6.1         1.9 virginica\n## 132          7.9         3.8          6.4         2.0 virginica\n## 133          6.4         2.8          5.6         2.2 virginica\n## 134          6.3         2.8          5.1         1.5 virginica\n## 135          6.1         2.6          5.6         1.4 virginica\n## 136          7.7         3.0          6.1         2.3 virginica\n## 137          6.3         3.4          5.6         2.4 virginica\n## 138          6.4         3.1          5.5         1.8 virginica\n## 139          6.0         3.0          4.8         1.8 virginica\n## 140          6.9         3.1          5.4         2.1 virginica\n## 141          6.7         3.1          5.6         2.4 virginica\n## 142          6.9         3.1          5.1         2.3 virginica\n## 143          5.8         2.7          5.1         1.9 virginica\n## 144          6.8         3.2          5.9         2.3 virginica\n## 145          6.7         3.3          5.7         2.5 virginica\n## 146          6.7         3.0          5.2         2.3 virginica\n## 147          6.3         2.5          5.0         1.9 virginica\n## 148          6.5         3.0          5.2         2.0 virginica\n## 149          6.2         3.4          5.4         2.3 virginica\n## 150          5.9         3.0          5.1         1.8 virginica\nsummary(sp)                     # 분리 결과 요약\n##            Length Class      Mode\n## setosa     5      data.frame list\n## versicolor 5      data.frame list\n## virginica  5      data.frame list\nsp$setosa                       # setosa 품종의 데이터 확인\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1           5.1         3.5          1.4         0.2  setosa\n## 2           4.9         3.0          1.4         0.2  setosa\n## 3           4.7         3.2          1.3         0.2  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\n## 5           5.0         3.6          1.4         0.2  setosa\n## 6           5.4         3.9          1.7         0.4  setosa\n## 7           4.6         3.4          1.4         0.3  setosa\n## 8           5.0         3.4          1.5         0.2  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\n## 11          5.4         3.7          1.5         0.2  setosa\n## 12          4.8         3.4          1.6         0.2  setosa\n## 13          4.8         3.0          1.4         0.1  setosa\n## 14          4.3         3.0          1.1         0.1  setosa\n## 15          5.8         4.0          1.2         0.2  setosa\n## 16          5.7         4.4          1.5         0.4  setosa\n## 17          5.4         3.9          1.3         0.4  setosa\n## 18          5.1         3.5          1.4         0.3  setosa\n## 19          5.7         3.8          1.7         0.3  setosa\n## 20          5.1         3.8          1.5         0.3  setosa\n## 21          5.4         3.4          1.7         0.2  setosa\n## 22          5.1         3.7          1.5         0.4  setosa\n## 23          4.6         3.6          1.0         0.2  setosa\n## 24          5.1         3.3          1.7         0.5  setosa\n## 25          4.8         3.4          1.9         0.2  setosa\n## 26          5.0         3.0          1.6         0.2  setosa\n## 27          5.0         3.4          1.6         0.4  setosa\n## 28          5.2         3.5          1.5         0.2  setosa\n## 29          5.2         3.4          1.4         0.2  setosa\n## 30          4.7         3.2          1.6         0.2  setosa\n## 31          4.8         3.1          1.6         0.2  setosa\n## 32          5.4         3.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\n## 35          4.9         3.1          1.5         0.2  setosa\n## 36          5.0         3.2          1.2         0.2  setosa\n## 37          5.5         3.5          1.3         0.2  setosa\n## 38          4.9         3.6          1.4         0.1  setosa\n## 39          4.4         3.0          1.3         0.2  setosa\n## 40          5.1         3.4          1.5         0.2  setosa\n## 41          5.0         3.5          1.3         0.3  setosa\n## 42          4.5         2.3          1.3         0.3  setosa\n## 43          4.4         3.2          1.3         0.2  setosa\n## 44          5.0         3.5          1.6         0.6  setosa\n## 45          5.1         3.8          1.9         0.4  setosa\n## 46          4.8         3.0          1.4         0.3  setosa\n## 47          5.1         3.8          1.6         0.2  setosa\n## 48          4.6         3.2          1.4         0.2  setosa\n## 49          5.3         3.7          1.5         0.2  setosa\n## 50          5.0         3.3          1.4         0.2  setosa\nsetosa &lt;- sp$setosa\n\n\nsubset(iris, Species == \"setosa\")\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1           5.1         3.5          1.4         0.2  setosa\n## 2           4.9         3.0          1.4         0.2  setosa\n## 3           4.7         3.2          1.3         0.2  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\n## 5           5.0         3.6          1.4         0.2  setosa\n## 6           5.4         3.9          1.7         0.4  setosa\n## 7           4.6         3.4          1.4         0.3  setosa\n## 8           5.0         3.4          1.5         0.2  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\n## 11          5.4         3.7          1.5         0.2  setosa\n## 12          4.8         3.4          1.6         0.2  setosa\n## 13          4.8         3.0          1.4         0.1  setosa\n## 14          4.3         3.0          1.1         0.1  setosa\n## 15          5.8         4.0          1.2         0.2  setosa\n## 16          5.7         4.4          1.5         0.4  setosa\n## 17          5.4         3.9          1.3         0.4  setosa\n## 18          5.1         3.5          1.4         0.3  setosa\n## 19          5.7         3.8          1.7         0.3  setosa\n## 20          5.1         3.8          1.5         0.3  setosa\n## 21          5.4         3.4          1.7         0.2  setosa\n## 22          5.1         3.7          1.5         0.4  setosa\n## 23          4.6         3.6          1.0         0.2  setosa\n## 24          5.1         3.3          1.7         0.5  setosa\n## 25          4.8         3.4          1.9         0.2  setosa\n## 26          5.0         3.0          1.6         0.2  setosa\n## 27          5.0         3.4          1.6         0.4  setosa\n## 28          5.2         3.5          1.5         0.2  setosa\n## 29          5.2         3.4          1.4         0.2  setosa\n## 30          4.7         3.2          1.6         0.2  setosa\n## 31          4.8         3.1          1.6         0.2  setosa\n## 32          5.4         3.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\n## 35          4.9         3.1          1.5         0.2  setosa\n## 36          5.0         3.2          1.2         0.2  setosa\n## 37          5.5         3.5          1.3         0.2  setosa\n## 38          4.9         3.6          1.4         0.1  setosa\n## 39          4.4         3.0          1.3         0.2  setosa\n## 40          5.1         3.4          1.5         0.2  setosa\n## 41          5.0         3.5          1.3         0.3  setosa\n## 42          4.5         2.3          1.3         0.3  setosa\n## 43          4.4         3.2          1.3         0.2  setosa\n## 44          5.0         3.5          1.6         0.6  setosa\n## 45          5.1         3.8          1.9         0.4  setosa\n## 46          4.8         3.0          1.4         0.3  setosa\n## 47          5.1         3.8          1.6         0.2  setosa\n## 48          4.6         3.2          1.4         0.2  setosa\n## 49          5.3         3.7          1.5         0.2  setosa\n## 50          5.0         3.3          1.4         0.2  setosa\nsubset(iris, Sepal.Length &gt; 7.5)\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n## 106          7.6         3.0          6.6         2.1 virginica\n## 118          7.7         3.8          6.7         2.2 virginica\n## 119          7.7         2.6          6.9         2.3 virginica\n## 123          7.7         2.8          6.7         2.0 virginica\n## 132          7.9         3.8          6.4         2.0 virginica\n## 136          7.7         3.0          6.1         2.3 virginica\nsubset(iris, Sepal.Length &gt; 5.1 &\n           Sepal.Width &gt; 3.9)\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 15          5.8         4.0          1.2         0.2  setosa\n## 16          5.7         4.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\nsubset(iris, Sepal.Length &gt; 5.1 |\n           Sepal.Width &gt; 3.9)\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 6            5.4         3.9          1.7         0.4     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 15           5.8         4.0          1.2         0.2     setosa\n## 16           5.7         4.4          1.5         0.4     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 34           5.5         4.2          1.4         0.2     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 49           5.3         3.7          1.5         0.2     setosa\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 114          5.7         2.5          5.0         2.0  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 120          6.0         2.2          5.0         1.5  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 122          5.6         2.8          4.9         2.0  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 127          6.2         2.8          4.8         1.8  virginica\n## 128          6.1         3.0          4.9         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\nsubset(iris, Sepal.Length &gt; 7.6,\n       select = c(Petal.Length, Petal.Width))\n##     Petal.Length Petal.Width\n## 118          6.7         2.2\n## 119          6.9         2.3\n## 123          6.7         2.0\n## 132          6.4         2.0\n## 136          6.1         2.3\n\n\nx &lt;- 1:10\nsample(x, size = 5, replace = FALSE) # 비복원추출\n## [1]  1  9  6  7 10\nsample(x, size = 5, replace = TRUE)\n## [1] 9 3 8 9 1\n\nx &lt;- 1:45\nsample(x, size = 6, replace = FALSE)\n## [1] 27 33 15 39 16  6\n\n\nidx &lt;- sample(1:nrow(iris), size = 50,\n              replace = FALSE)\niris.50 &lt;- iris[idx, ]  # 50개의 행 추출\ndim(iris.50)            # 행과 열의 개수 확인\n## [1] 50  5\nhead(iris.50)\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 17           5.4         3.9          1.3         0.4     setosa\n## 109          6.7         2.5          5.8         1.8  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n\n\nsample(1:20, size = 5)\n## [1]  9 14  2 16 15\nsample(1:20, size = 5)\n## [1]  7 16 15 14 13\nsample(1:20, size = 5)\n## [1]  1 15 20  9  5\n\n# 같은 값이 추출되도록 고정시키고 싶다면\n# set.seed() 함수를 이용하여 seed값을 지정해주면 된다.\nset.seed(100)\nsample(1:20, size = 5)\n## [1] 10  6 16 14 12\nset.seed(100)\nsample(1:20, size = 5)\n## [1] 10  6 16 14 12\nset.seed(100)\nsample(1:20, size = 5)\n## [1] 10  6 16 14 12\n\n\ncombn(1:5, 3) # 1~5에서 3개를 뽑는 조합\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n## [1,]    1    1    1    1    1    1    2    2    2     3\n## [2,]    2    2    2    3    3    4    3    3    4     4\n## [3,]    3    4    5    4    5    5    4    5    5     5\n\nx = c(\"red\", \"green\", \"blue\", \"black\", \"white\")\ncom &lt;- combn(x, 2) # x의 원소를 2개씩 뽑는 조합\ncom\n##      [,1]    [,2]   [,3]    [,4]    [,5]    [,6]    [,7]    [,8]    [,9]   \n## [1,] \"red\"   \"red\"  \"red\"   \"red\"   \"green\" \"green\" \"green\" \"blue\"  \"blue\" \n## [2,] \"green\" \"blue\" \"black\" \"white\" \"blue\"  \"black\" \"white\" \"black\" \"white\"\n##      [,10]  \n## [1,] \"black\"\n## [2,] \"white\"\n\nfor (i in 1:ncol(com)) {\n    # 조합을 출력\n    cat(com[, i], \"\\n\")\n}\n## red green \n## red blue \n## red black \n## red white \n## green blue \n## green black \n## green white \n## blue black \n## blue white \n## black white\n\n\n# aggregate(data, by = '기준이 되는 컬럼', FUN)\nagg &lt;- aggregate(iris[, -5], by = list(iris$Species), FUN = mean)\nagg\n##      Group.1 Sepal.Length Sepal.Width Petal.Length Petal.Width\n## 1     setosa        5.006       3.428        1.462       0.246\n## 2 versicolor        5.936       2.770        4.260       1.326\n## 3  virginica        6.588       2.974        5.552       2.026\n\n\n# aggregate는 데이터의 특정 컬럼을 기준으로 통계량을 구해주는 함수\nagg &lt;- aggregate(iris[, -5], by = list(표준편차 = iris$Species), FUN = sd)\nagg\n##     표준편차 Sepal.Length Sepal.Width Petal.Length Petal.Width\n## 1     setosa    0.3524897   0.3790644    0.1736640   0.1053856\n## 2 versicolor    0.5161711   0.3137983    0.4699110   0.1977527\n## 3  virginica    0.6358796   0.3224966    0.5518947   0.2746501\n\n\nhead(mtcars)\n##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\nagg &lt;- aggregate(mtcars, by = list(cyl = mtcars$cyl, vs = mtcars$vs), FUN = max)\nagg\n##   cyl vs  mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## 1   4  0 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n## 2   6  0 21.0   6 160.0 175 3.90 2.875 17.02  0  1    5    6\n## 3   8  0 19.2   8 472.0 335 4.22 5.424 18.00  0  1    5    8\n## 4   4  1 33.9   4 146.7 113 4.93 3.190 22.90  1  1    5    2\n## 5   6  1 21.4   6 258.0 123 3.92 3.460 20.22  1  0    4    4\n\nagg &lt;- aggregate(mtcars, by = list(cyl = mtcars$cyl, vs = mtcars$vs), FUN = mean)\nagg\n##   cyl vs      mpg cyl   disp       hp     drat       wt     qsec vs        am\n## 1   4  0 26.00000   4 120.30  91.0000 4.430000 2.140000 16.70000  0 1.0000000\n## 2   6  0 20.56667   6 155.00 131.6667 3.806667 2.755000 16.32667  0 1.0000000\n## 3   8  0 15.10000   8 353.10 209.2143 3.229286 3.999214 16.77214  0 0.1428571\n## 4   4  1 26.73000   4 103.62  81.8000 4.035000 2.300300 19.38100  1 0.7000000\n## 5   6  1 19.12500   6 204.55 115.2500 3.420000 3.388750 19.21500  1 0.0000000\n##       gear     carb\n## 1 5.000000 2.000000\n## 2 4.333333 4.666667\n## 3 3.285714 3.500000\n## 4 4.000000 1.500000\n## 5 3.500000 2.500000\n\n\nx &lt;- data.frame(name = c(\"a\", \"b\", \"c\"), math = c(90, 80, 40))\ny &lt;- data.frame(name = c(\"a\", \"b\", \"d\"), korean = c(75, 60, 90))\nx ; y\n##   name math\n## 1    a   90\n## 2    b   80\n## 3    c   40\n##   name korean\n## 1    a     75\n## 2    b     60\n## 3    d     90\n\n\nz &lt;- merge(x, y, by = c(\"name\"))\nz\n##   name math korean\n## 1    a   90     75\n## 2    b   80     60\n\n\nmerge(x, y, all.x = T)  # 첫 번째 데이터셋의 행들은 모두 표시되도록\n##   name math korean\n## 1    a   90     75\n## 2    b   80     60\n## 3    c   40     NA\nmerge(x, y, all.y = T)  # 두 번째 데이터셋의 행들은 모두 표시되도록\n##   name math korean\n## 1    a   90     75\n## 2    b   80     60\n## 3    d   NA     90\nmerge(x, y, all = T)    # 두 데이터셋의 모든 행들이 표시되도록\n##   name math korean\n## 1    a   90     75\n## 2    b   80     60\n## 3    c   40     NA\n## 4    d   NA     90\n\n\nx &lt;- data.frame(name = c(\"a\", \"b\", \"c\"), math = c(90, 80, 40))\ny &lt;- data.frame(sname = c(\"a\", \"b\", \"d\"), korean = c(75, 60, 90))\nx # 병합 기준 열의 이름이 name\n##   name math\n## 1    a   90\n## 2    b   80\n## 3    c   40\ny # 병합 기준 열의 이름이 sname\n##   sname korean\n## 1     a     75\n## 2     b     60\n## 3     d     90\nmerge(x, y, by.x = c(\"name\"), by.y = c(\"sname\"))\n##   name math korean\n## 1    a   90     75\n## 2    b   80     60"
  },
  {
    "objectID": "R_Basic.html#장.-변수와-벡터",
    "href": "R_Basic.html#장.-변수와-벡터",
    "title": "R Basic",
    "section": "",
    "text": "2 + 3  # 2 더하기 3\n## [1] 5\n(3 + 6) * 8\n## [1] 72\n2 ^ 3  # 2의 세제곱\n## [1] 8\n8 %% 3\n## [1] 2\n\n\n7 + 4\n## [1] 11\n\n\nlog(10) + 5 # 로그함수\n## [1] 7.302585\nsqrt(25) # 제곱근\n## [1] 5\nmax(5, 3, 2) # 가장 큰 값\n## [1] 5\n\n\na &lt;- 10\nb &lt;- 20\nc &lt;- a+b\nprint(c)\n## [1] 30\n\n\na &lt;- 125\na\n## [1] 125\nprint(a)\n## [1] 125\n\n\na &lt;- 10 # a에 숫자 저장\nb &lt;- 20\na + b # a+b의 결과 출력\n## [1] 30\na &lt;- \"A\" # a에 문자 저장\na + b # a+b의 결과 출력. 에러 발생\n## Error in a + b: non-numeric argument to binary operator\n\n\nx &lt;- c(1, 2, 3) # 숫자형 벡터\ny &lt;- c(\"a\", \"b\", \"c\") # 문자형 벡터\nz &lt;- c(TRUE, TRUE, FALSE, TRUE) # 논리형 벡터\nx ; y ;z\n## [1] 1 2 3\n## [1] \"a\" \"b\" \"c\"\n## [1]  TRUE  TRUE FALSE  TRUE\n\n\nw &lt;- c(1, 2, 3, \"a\", \"b\", \"c\")\nw\n## [1] \"1\" \"2\" \"3\" \"a\" \"b\" \"c\"\n\n\nv1 &lt;- 50:90\nv1\n##  [1] 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74\n## [26] 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90\nv2 &lt;- c(1, 2, 5, 50:90)\nv2\n##  [1]  1  2  5 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n## [26] 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90\n\n\nv3 &lt;- seq(1, 101, 3)\nv3\n##  [1]   1   4   7  10  13  16  19  22  25  28  31  34  37  40  43  46  49  52  55\n## [20]  58  61  64  67  70  73  76  79  82  85  88  91  94  97 100\nv4 &lt;- seq(0.1, 1.0, 0.1)\nv4\n##  [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n\n\nv5 &lt;- rep(1, times = 5) # 1을 5번 반복\nv5\n## [1] 1 1 1 1 1\nv6 &lt;- rep(1:5, times = 3) # 1에서 5까지 3번 반복\nv6\n##  [1] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5\nv7 &lt;- rep(c(1, 5, 9), times = 3) # 1, 5, 9를 3번 반복\nv7\n## [1] 1 5 9 1 5 9 1 5 9\nv8 &lt;- rep(1:5, each = 3) # 1에서 5를 각각 3번 반복\nv8\n##  [1] 1 1 1 2 2 2 3 3 3 4 4 4 5 5 5\n\nrep(1:3, each = 3, times = 3)\n##  [1] 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3\nrep(1:3, times = 3, each = 3)\n##  [1] 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3\n\n\nscore &lt;- c(90, 85, 70) # 성적\nscore\n## [1] 90 85 70\nnames(score) # score에 저장된 값들의 이름을 보이시오\n## NULL\nnames(score) &lt;- c(\"John\", \"Tom\", \"Jane\") # 값들에 이름을 부여\nnames(score) # score에 저장된 값들의 이름을 보이시오\n## [1] \"John\" \"Tom\"  \"Jane\"\nscore # 이름과 함께 값이 출력\n## John  Tom Jane \n##   90   85   70\n\n\nd &lt;- c(1, 4, 3, 7, 8)\nd[1]\n## [1] 1\nd[2]\n## [1] 4\nd[3]\n## [1] 3\nd[4]\n## [1] 7\nd[5]\n## [1] 8\nd[6]\n## [1] NA\nd[c(2, 4)]\n## [1] 4 7\n\n\nd &lt;- c(1, 4, 3, 7, 8)\nd[c(1, 3, 5)] # 1, 3, 5번째 값 출력\n## [1] 1 3 8\nd[1:3] # 처음 세 개의 값 출력\n## [1] 1 4 3\nd[seq(1, 5, 2)] # 홀수 번째 값 출력\n## [1] 1 3 8\nd[-2] # 2번째 값 제외하고 출력\n## [1] 1 3 7 8\nd[-c(3:5)] # 3~5번째 값은 제외하고 출력\n## [1] 1 4\n\n\nGNP &lt;- c(2000, 2450, 960)\nGNP\n## [1] 2000 2450  960\nnames(GNP) &lt;- c(\"Korea\", \"Japan\", \"Nepal\")\nGNP\n## Korea Japan Nepal \n##  2000  2450   960\nGNP[1]\n## Korea \n##  2000\nGNP[\"Korea\"]\n## Korea \n##  2000\nGNP_NEW &lt;- GNP[c(\"Korea\", \"Nepal\")]\nGNP_NEW\n## Korea Nepal \n##  2000   960\n\n\nv1 &lt;- c(1, 5, 7, 8, 9)\nv1\n## [1] 1 5 7 8 9\nv1[2] &lt;- 3 # v1의 2번째 값을 3으로 변경\nv1\n## [1] 1 3 7 8 9\nv1[c(1, 5)] &lt;- c(10, 20) # v1의 1, 5번째 값을 각각 10, 20으로 변경\nv1\n## [1] 10  3  7  8 20\n\n\nd &lt;- c(1, 4, 3, 7, 8)\n2 * d\n## [1]  2  8  6 14 16\nd - 5\n## [1] -4 -1 -2  2  3\n3 * d + 4\n## [1]  7 16 13 25 28\n\n\nx &lt;- c(1, 2, 3)\ny &lt;- c(4, 5, 6)\nx + y # 대응하는 원소끼리 더하여 출력\n## [1] 5 7 9\nx * y # 대응하는 원소끼리 곱하여 출력\n## [1]  4 10 18\nz &lt;- x + y # x, y를 더하여 z에 저장\nz\n## [1] 5 7 9\n\n\nd &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\nsum(d) # d에 포함된 값들의 합\n## [1] 55\nsum(2 * d) # d에 포함된 값들에 2를 곱한 후 합한 값\n## [1] 110\nlength(d) # d에 포함된 값들의 개수\n## [1] 10\nmean(d[1:5]) # 1~5번째 값들의 평균\n## [1] 3\nmax(d) # d에 포함된 값들의 최댓값\n## [1] 10\nmin(d) # d에 포함된 값들의 최솟값\n## [1] 1\nsort(d) # 오름차순 정렬\n##  [1]  1  2  3  4  5  6  7  8  9 10\nsort(d, decreasing = FALSE) # 오름차순 정렬\n##  [1]  1  2  3  4  5  6  7  8  9 10\nsort(d, decreasing = TRUE) # 내림차순 정렬\n##  [1] 10  9  8  7  6  5  4  3  2  1\nsort(d, TRUE) # 내림차순 정렬\n##  [1] 10  9  8  7  6  5  4  3  2  1\n\nv1 &lt;- median(d)\nv1\n## [1] 5.5\nv2 &lt;- sum(d) / length(d)\nv2\n## [1] 5.5\nmean(d)\n## [1] 5.5\n\n\nd &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9)\nd &gt;= 5\n## [1] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\nd[d &gt; 5] # 5보다 큰 값\n## [1] 6 7 8 9\nsum(d &gt; 5) # 5보다 큰 값의 개수를 출력\n## [1] 4\nsum(d[d &gt; 5]) # 5보다 큰 값의 합계를 출력\n## [1] 30\nd == 5\n## [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n\ncondi &lt;- d &gt; 5 & d &lt; 8 # 조건을 변수에 저장\nd[condi] # 조건에 맞는 값들을 선택\n## [1] 6 7\nd[d &gt; 5 & d &lt; 8]\n## [1] 6 7\n\n\nds &lt;- c(90, 85, 70, 84)\nmy.info &lt;- list(name = 'Tom', age = 60, status = TRUE, score = ds)\nmy.info # 리스트에 저장된 내용을 모두 출력\n## $name\n## [1] \"Tom\"\n## \n## $age\n## [1] 60\n## \n## $status\n## [1] TRUE\n## \n## $score\n## [1] 90 85 70 84\nmy.info[1] # 이름이랑 내용 다 출력\n## $name\n## [1] \"Tom\"\nmy.info[[1]] # 리스트의 첫 번째 값을 출력\n## [1] \"Tom\"\nmy.info$name # 리스트에서 값의 이름이 name인 값을 출력\n## [1] \"Tom\"\nmy.info[[4]] # 리스트의 네 번째 값을 출력\n## [1] 90 85 70 84\n\n\nbt &lt;- c('A', 'B', 'B', 'O', 'AB', 'A') # 문자형 벡터 bt 정의\nbt.new &lt;- factor(bt) # 팩터 bt.new 정의\nbt # 벡터 bt의 내용 출력\n## [1] \"A\"  \"B\"  \"B\"  \"O\"  \"AB\" \"A\"\nbt.new # 팩터 bt.new의 내용 출력\n## [1] A  B  B  O  AB A \n## Levels: A AB B O\nbt[5] # 벡터 bt의 5번째 값 출력\n## [1] \"AB\"\nbt.new[5] # 팩터 bt.new의 5번째 값 출력\n## [1] AB\n## Levels: A AB B O\nlevels(bt.new) # 팩터에 저장된 값의 종류를 출력\n## [1] \"A\"  \"AB\" \"B\"  \"O\"\nas.integer(bt.new) # 팩터의 문자값을 숫자로 바꾸어 출력\n## [1] 1 3 3 4 2 1\nbt.new[7] &lt;- 'B' # 팩터 bt.new의 7번째에 'B' 저장\nbt.new[8] &lt;- 'C' # 팩터 bt.new의 8번째에 'C' 저장\n## Warning in `[&lt;-.factor`(`*tmp*`, 8, value = \"C\"): invalid factor level, NA\n## generated\nbt.new # 팩터 bt.new의 내용 출력\n## [1] A    B    B    O    AB   A    B    &lt;NA&gt;\n## Levels: A AB B O"
  },
  {
    "objectID": "R_Basic.html#장.-매트릭스와-데이터프레임",
    "href": "R_Basic.html#장.-매트릭스와-데이터프레임",
    "title": "R Basic",
    "section": "",
    "text": "z &lt;- matrix(1:20, nrow = 4, ncol = 5)\nz # 매트릭스 z의 내용을 출력\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\n\n\nz2 &lt;- matrix(1:20, nrow = 4, ncol = 5, byrow = T)\nz2 # 매트릭스 z2의 내용을 출력\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    2    3    4    5\n## [2,]    6    7    8    9   10\n## [3,]   11   12   13   14   15\n## [4,]   16   17   18   19   20\n\nz &lt;- matrix(1:16, nrow = 4, ncol = 5)\n## Warning in matrix(1:16, nrow = 4, ncol = 5): data length [16] is not a\n## sub-multiple or multiple of the number of columns [5]\nz\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13    1\n## [2,]    2    6   10   14    2\n## [3,]    3    7   11   15    3\n## [4,]    4    8   12   16    4\n\n\nx &lt;- 1:4 # 벡터 x 생성\ny &lt;- 5:8 # 벡터 y 생성\nz &lt;- matrix(1:20, nrow = 4, ncol = 5) # 매트릭스 z 생성\n\nm1 &lt;- cbind(x, y) # x와 y를 열 방향으로 결합하여 매트릭스 생성\nm1 # 매트릭스 m1의 내용을 출력\n##      x y\n## [1,] 1 5\n## [2,] 2 6\n## [3,] 3 7\n## [4,] 4 8\nm2 &lt;- rbind(x, y) # x와 y를 행 방향으로 결합하여 매트릭스 생성\nm2 # 매트릭스 m2의 내용을 출력\n##   [,1] [,2] [,3] [,4]\n## x    1    2    3    4\n## y    5    6    7    8\nm3 &lt;- rbind(m2, x) # m2와 벡터 x를 행 방향으로 결합\nm3 # 매트릭스 m3의 내용을 출력\n##   [,1] [,2] [,3] [,4]\n## x    1    2    3    4\n## y    5    6    7    8\n## x    1    2    3    4\nm4 &lt;- cbind(z, x) # 매트릭스 z와 벡터 x를 열 방향으로 결합\nm4 # 매트릭스 m4의 내용을 출력\n##                   x\n## [1,] 1 5  9 13 17 1\n## [2,] 2 6 10 14 18 2\n## [3,] 3 7 11 15 19 3\n## [4,] 4 8 12 16 20 4\n\nx &lt;- 1:5\nm5 &lt;- cbind(z, x)\n## Warning in cbind(z, x): number of rows of result is not a multiple of vector\n## length (arg 2)\nm5\n##                   x\n## [1,] 1 5  9 13 17 1\n## [2,] 2 6 10 14 18 2\n## [3,] 3 7 11 15 19 3\n## [4,] 4 8 12 16 20 4\n\n\nz &lt;- matrix(1:20, nrow = 4, ncol = 5) # 매트릭스 z 생성\nz # 매트릭스 z의 내용 출력\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\n\nz[2, 3] # 2행 3열에 있는 값\n## [1] 10\nz[1, 4] # 1행 4열에 있는 값\n## [1] 13\nz[2, ] # 2행에 있는 모든 값\n## [1]  2  6 10 14 18\nz[, 4] # 4열에 있는 모든 값\n## [1] 13 14 15 16\nz[, ]\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\nz\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\n\n\nz &lt;- matrix(1:20, nrow = 4, ncol = 5) # 매트릭스 z 생성\nz # 매트릭스 z의 내용 출력\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\n\nz[2, 1:3] # 2행의 값 중 1~3열에 있는 값\n## [1]  2  6 10\nz[1, c(1, 2, 4)] # 1행의 값 중 1, 2, 4열에 있는 값\n## [1]  1  5 13\nz[1:2, ] # 1, 2행에 있는 모든 값\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\nz[, c(1, 4)] # 1, 4열에 있는 모든 값\n##      [,1] [,2]\n## [1,]    1   13\n## [2,]    2   14\n## [3,]    3   15\n## [4,]    4   16\n\n\nscore &lt;- matrix(c(90, 85, 69, 78,\n                  85, 96, 49, 95,\n                  90, 80, 70, 60),\n                nrow = 4,\n                ncol = 3)\nscore\n##      [,1] [,2] [,3]\n## [1,]   90   85   90\n## [2,]   85   96   80\n## [3,]   69   49   70\n## [4,]   78   95   60\nrownames(score) &lt;- c('John', 'Tom', 'Mark', 'Jane')\ncolnames(score) &lt;- c('English', 'Math', 'Science')\nscore\n##      English Math Science\n## John      90   85      90\n## Tom       85   96      80\n## Mark      69   49      70\n## Jane      78   95      60\n\n\nscore['John', 'Math'] # John의 수학 성적\n## [1] 85\nscore['Tom', c('Math', 'Science')] # Tom의 수학, 과학 성적\n##    Math Science \n##      96      80\nscore['Mark', ] # Mark의 모든 과목 성적\n## English    Math Science \n##      69      49      70\nscore[, 'English'] # 모든 학생의 영어 성적\n## John  Tom Mark Jane \n##   90   85   69   78\nrownames(score) # score의 행의 이름\n## [1] \"John\" \"Tom\"  \"Mark\" \"Jane\"\ncolnames(score) # score의 열의 이름\n## [1] \"English\" \"Math\"    \"Science\"\ncolnames(score)[2] # score의 열의 이름 중 두 번째 값\n## [1] \"Math\"\n\n\ncity &lt;- c(\"Seoul\", \"Tokyo\", \"Washington\") # 문자로 이루어진 벡터\nrank &lt;- c(1, 3, 2) # 숫자로 이루어진 벡터\ncity.info &lt;- data.frame(city, rank) # 데이터프레임 생성\ncity.info # city.info의 내용 출력\n##         city rank\n## 1      Seoul    1\n## 2      Tokyo    3\n## 3 Washington    2\n\n\n# iris\niris[, c(1:2)] # 1, 2열의 모든 데이터\n##     Sepal.Length Sepal.Width\n## 1            5.1         3.5\n## 2            4.9         3.0\n## 3            4.7         3.2\n## 4            4.6         3.1\n## 5            5.0         3.6\n## 6            5.4         3.9\n## 7            4.6         3.4\n## 8            5.0         3.4\n## 9            4.4         2.9\n## 10           4.9         3.1\n## 11           5.4         3.7\n## 12           4.8         3.4\n## 13           4.8         3.0\n## 14           4.3         3.0\n## 15           5.8         4.0\n## 16           5.7         4.4\n## 17           5.4         3.9\n## 18           5.1         3.5\n## 19           5.7         3.8\n## 20           5.1         3.8\n## 21           5.4         3.4\n## 22           5.1         3.7\n## 23           4.6         3.6\n## 24           5.1         3.3\n## 25           4.8         3.4\n## 26           5.0         3.0\n## 27           5.0         3.4\n## 28           5.2         3.5\n## 29           5.2         3.4\n## 30           4.7         3.2\n## 31           4.8         3.1\n## 32           5.4         3.4\n## 33           5.2         4.1\n## 34           5.5         4.2\n## 35           4.9         3.1\n## 36           5.0         3.2\n## 37           5.5         3.5\n## 38           4.9         3.6\n## 39           4.4         3.0\n## 40           5.1         3.4\n## 41           5.0         3.5\n## 42           4.5         2.3\n## 43           4.4         3.2\n## 44           5.0         3.5\n## 45           5.1         3.8\n## 46           4.8         3.0\n## 47           5.1         3.8\n## 48           4.6         3.2\n## 49           5.3         3.7\n## 50           5.0         3.3\n## 51           7.0         3.2\n## 52           6.4         3.2\n## 53           6.9         3.1\n## 54           5.5         2.3\n## 55           6.5         2.8\n## 56           5.7         2.8\n## 57           6.3         3.3\n## 58           4.9         2.4\n## 59           6.6         2.9\n## 60           5.2         2.7\n## 61           5.0         2.0\n## 62           5.9         3.0\n## 63           6.0         2.2\n## 64           6.1         2.9\n## 65           5.6         2.9\n## 66           6.7         3.1\n## 67           5.6         3.0\n## 68           5.8         2.7\n## 69           6.2         2.2\n## 70           5.6         2.5\n## 71           5.9         3.2\n## 72           6.1         2.8\n## 73           6.3         2.5\n## 74           6.1         2.8\n## 75           6.4         2.9\n## 76           6.6         3.0\n## 77           6.8         2.8\n## 78           6.7         3.0\n## 79           6.0         2.9\n## 80           5.7         2.6\n## 81           5.5         2.4\n## 82           5.5         2.4\n## 83           5.8         2.7\n## 84           6.0         2.7\n## 85           5.4         3.0\n## 86           6.0         3.4\n## 87           6.7         3.1\n## 88           6.3         2.3\n## 89           5.6         3.0\n## 90           5.5         2.5\n## 91           5.5         2.6\n## 92           6.1         3.0\n## 93           5.8         2.6\n## 94           5.0         2.3\n## 95           5.6         2.7\n## 96           5.7         3.0\n## 97           5.7         2.9\n## 98           6.2         2.9\n## 99           5.1         2.5\n## 100          5.7         2.8\n## 101          6.3         3.3\n## 102          5.8         2.7\n## 103          7.1         3.0\n## 104          6.3         2.9\n## 105          6.5         3.0\n## 106          7.6         3.0\n## 107          4.9         2.5\n## 108          7.3         2.9\n## 109          6.7         2.5\n## 110          7.2         3.6\n## 111          6.5         3.2\n## 112          6.4         2.7\n## 113          6.8         3.0\n## 114          5.7         2.5\n## 115          5.8         2.8\n## 116          6.4         3.2\n## 117          6.5         3.0\n## 118          7.7         3.8\n## 119          7.7         2.6\n## 120          6.0         2.2\n## 121          6.9         3.2\n## 122          5.6         2.8\n## 123          7.7         2.8\n## 124          6.3         2.7\n## 125          6.7         3.3\n## 126          7.2         3.2\n## 127          6.2         2.8\n## 128          6.1         3.0\n## 129          6.4         2.8\n## 130          7.2         3.0\n## 131          7.4         2.8\n## 132          7.9         3.8\n## 133          6.4         2.8\n## 134          6.3         2.8\n## 135          6.1         2.6\n## 136          7.7         3.0\n## 137          6.3         3.4\n## 138          6.4         3.1\n## 139          6.0         3.0\n## 140          6.9         3.1\n## 141          6.7         3.1\n## 142          6.9         3.1\n## 143          5.8         2.7\n## 144          6.8         3.2\n## 145          6.7         3.3\n## 146          6.7         3.0\n## 147          6.3         2.5\n## 148          6.5         3.0\n## 149          6.2         3.4\n## 150          5.9         3.0\niris[, c(1, 3, 5)] # 1, 3, 5열의 모든 데이터\n##     Sepal.Length Petal.Length    Species\n## 1            5.1          1.4     setosa\n## 2            4.9          1.4     setosa\n## 3            4.7          1.3     setosa\n## 4            4.6          1.5     setosa\n## 5            5.0          1.4     setosa\n## 6            5.4          1.7     setosa\n## 7            4.6          1.4     setosa\n## 8            5.0          1.5     setosa\n## 9            4.4          1.4     setosa\n## 10           4.9          1.5     setosa\n## 11           5.4          1.5     setosa\n## 12           4.8          1.6     setosa\n## 13           4.8          1.4     setosa\n## 14           4.3          1.1     setosa\n## 15           5.8          1.2     setosa\n## 16           5.7          1.5     setosa\n## 17           5.4          1.3     setosa\n## 18           5.1          1.4     setosa\n## 19           5.7          1.7     setosa\n## 20           5.1          1.5     setosa\n## 21           5.4          1.7     setosa\n## 22           5.1          1.5     setosa\n## 23           4.6          1.0     setosa\n## 24           5.1          1.7     setosa\n## 25           4.8          1.9     setosa\n## 26           5.0          1.6     setosa\n## 27           5.0          1.6     setosa\n## 28           5.2          1.5     setosa\n## 29           5.2          1.4     setosa\n## 30           4.7          1.6     setosa\n## 31           4.8          1.6     setosa\n## 32           5.4          1.5     setosa\n## 33           5.2          1.5     setosa\n## 34           5.5          1.4     setosa\n## 35           4.9          1.5     setosa\n## 36           5.0          1.2     setosa\n## 37           5.5          1.3     setosa\n## 38           4.9          1.4     setosa\n## 39           4.4          1.3     setosa\n## 40           5.1          1.5     setosa\n## 41           5.0          1.3     setosa\n## 42           4.5          1.3     setosa\n## 43           4.4          1.3     setosa\n## 44           5.0          1.6     setosa\n## 45           5.1          1.9     setosa\n## 46           4.8          1.4     setosa\n## 47           5.1          1.6     setosa\n## 48           4.6          1.4     setosa\n## 49           5.3          1.5     setosa\n## 50           5.0          1.4     setosa\n## 51           7.0          4.7 versicolor\n## 52           6.4          4.5 versicolor\n## 53           6.9          4.9 versicolor\n## 54           5.5          4.0 versicolor\n## 55           6.5          4.6 versicolor\n## 56           5.7          4.5 versicolor\n## 57           6.3          4.7 versicolor\n## 58           4.9          3.3 versicolor\n## 59           6.6          4.6 versicolor\n## 60           5.2          3.9 versicolor\n## 61           5.0          3.5 versicolor\n## 62           5.9          4.2 versicolor\n## 63           6.0          4.0 versicolor\n## 64           6.1          4.7 versicolor\n## 65           5.6          3.6 versicolor\n## 66           6.7          4.4 versicolor\n## 67           5.6          4.5 versicolor\n## 68           5.8          4.1 versicolor\n## 69           6.2          4.5 versicolor\n## 70           5.6          3.9 versicolor\n## 71           5.9          4.8 versicolor\n## 72           6.1          4.0 versicolor\n## 73           6.3          4.9 versicolor\n## 74           6.1          4.7 versicolor\n## 75           6.4          4.3 versicolor\n## 76           6.6          4.4 versicolor\n## 77           6.8          4.8 versicolor\n## 78           6.7          5.0 versicolor\n## 79           6.0          4.5 versicolor\n## 80           5.7          3.5 versicolor\n## 81           5.5          3.8 versicolor\n## 82           5.5          3.7 versicolor\n## 83           5.8          3.9 versicolor\n## 84           6.0          5.1 versicolor\n## 85           5.4          4.5 versicolor\n## 86           6.0          4.5 versicolor\n## 87           6.7          4.7 versicolor\n## 88           6.3          4.4 versicolor\n## 89           5.6          4.1 versicolor\n## 90           5.5          4.0 versicolor\n## 91           5.5          4.4 versicolor\n## 92           6.1          4.6 versicolor\n## 93           5.8          4.0 versicolor\n## 94           5.0          3.3 versicolor\n## 95           5.6          4.2 versicolor\n## 96           5.7          4.2 versicolor\n## 97           5.7          4.2 versicolor\n## 98           6.2          4.3 versicolor\n## 99           5.1          3.0 versicolor\n## 100          5.7          4.1 versicolor\n## 101          6.3          6.0  virginica\n## 102          5.8          5.1  virginica\n## 103          7.1          5.9  virginica\n## 104          6.3          5.6  virginica\n## 105          6.5          5.8  virginica\n## 106          7.6          6.6  virginica\n## 107          4.9          4.5  virginica\n## 108          7.3          6.3  virginica\n## 109          6.7          5.8  virginica\n## 110          7.2          6.1  virginica\n## 111          6.5          5.1  virginica\n## 112          6.4          5.3  virginica\n## 113          6.8          5.5  virginica\n## 114          5.7          5.0  virginica\n## 115          5.8          5.1  virginica\n## 116          6.4          5.3  virginica\n## 117          6.5          5.5  virginica\n## 118          7.7          6.7  virginica\n## 119          7.7          6.9  virginica\n## 120          6.0          5.0  virginica\n## 121          6.9          5.7  virginica\n## 122          5.6          4.9  virginica\n## 123          7.7          6.7  virginica\n## 124          6.3          4.9  virginica\n## 125          6.7          5.7  virginica\n## 126          7.2          6.0  virginica\n## 127          6.2          4.8  virginica\n## 128          6.1          4.9  virginica\n## 129          6.4          5.6  virginica\n## 130          7.2          5.8  virginica\n## 131          7.4          6.1  virginica\n## 132          7.9          6.4  virginica\n## 133          6.4          5.6  virginica\n## 134          6.3          5.1  virginica\n## 135          6.1          5.6  virginica\n## 136          7.7          6.1  virginica\n## 137          6.3          5.6  virginica\n## 138          6.4          5.5  virginica\n## 139          6.0          4.8  virginica\n## 140          6.9          5.4  virginica\n## 141          6.7          5.6  virginica\n## 142          6.9          5.1  virginica\n## 143          5.8          5.1  virginica\n## 144          6.8          5.9  virginica\n## 145          6.7          5.7  virginica\n## 146          6.7          5.2  virginica\n## 147          6.3          5.0  virginica\n## 148          6.5          5.2  virginica\n## 149          6.2          5.4  virginica\n## 150          5.9          5.1  virginica\niris[, c(\"Sepal.Length\", \"Species\")] # 1, 5열의 모든 데이터\n##     Sepal.Length    Species\n## 1            5.1     setosa\n## 2            4.9     setosa\n## 3            4.7     setosa\n## 4            4.6     setosa\n## 5            5.0     setosa\n## 6            5.4     setosa\n## 7            4.6     setosa\n## 8            5.0     setosa\n## 9            4.4     setosa\n## 10           4.9     setosa\n## 11           5.4     setosa\n## 12           4.8     setosa\n## 13           4.8     setosa\n## 14           4.3     setosa\n## 15           5.8     setosa\n## 16           5.7     setosa\n## 17           5.4     setosa\n## 18           5.1     setosa\n## 19           5.7     setosa\n## 20           5.1     setosa\n## 21           5.4     setosa\n## 22           5.1     setosa\n## 23           4.6     setosa\n## 24           5.1     setosa\n## 25           4.8     setosa\n## 26           5.0     setosa\n## 27           5.0     setosa\n## 28           5.2     setosa\n## 29           5.2     setosa\n## 30           4.7     setosa\n## 31           4.8     setosa\n## 32           5.4     setosa\n## 33           5.2     setosa\n## 34           5.5     setosa\n## 35           4.9     setosa\n## 36           5.0     setosa\n## 37           5.5     setosa\n## 38           4.9     setosa\n## 39           4.4     setosa\n## 40           5.1     setosa\n## 41           5.0     setosa\n## 42           4.5     setosa\n## 43           4.4     setosa\n## 44           5.0     setosa\n## 45           5.1     setosa\n## 46           4.8     setosa\n## 47           5.1     setosa\n## 48           4.6     setosa\n## 49           5.3     setosa\n## 50           5.0     setosa\n## 51           7.0 versicolor\n## 52           6.4 versicolor\n## 53           6.9 versicolor\n## 54           5.5 versicolor\n## 55           6.5 versicolor\n## 56           5.7 versicolor\n## 57           6.3 versicolor\n## 58           4.9 versicolor\n## 59           6.6 versicolor\n## 60           5.2 versicolor\n## 61           5.0 versicolor\n## 62           5.9 versicolor\n## 63           6.0 versicolor\n## 64           6.1 versicolor\n## 65           5.6 versicolor\n## 66           6.7 versicolor\n## 67           5.6 versicolor\n## 68           5.8 versicolor\n## 69           6.2 versicolor\n## 70           5.6 versicolor\n## 71           5.9 versicolor\n## 72           6.1 versicolor\n## 73           6.3 versicolor\n## 74           6.1 versicolor\n## 75           6.4 versicolor\n## 76           6.6 versicolor\n## 77           6.8 versicolor\n## 78           6.7 versicolor\n## 79           6.0 versicolor\n## 80           5.7 versicolor\n## 81           5.5 versicolor\n## 82           5.5 versicolor\n## 83           5.8 versicolor\n## 84           6.0 versicolor\n## 85           5.4 versicolor\n## 86           6.0 versicolor\n## 87           6.7 versicolor\n## 88           6.3 versicolor\n## 89           5.6 versicolor\n## 90           5.5 versicolor\n## 91           5.5 versicolor\n## 92           6.1 versicolor\n## 93           5.8 versicolor\n## 94           5.0 versicolor\n## 95           5.6 versicolor\n## 96           5.7 versicolor\n## 97           5.7 versicolor\n## 98           6.2 versicolor\n## 99           5.1 versicolor\n## 100          5.7 versicolor\n## 101          6.3  virginica\n## 102          5.8  virginica\n## 103          7.1  virginica\n## 104          6.3  virginica\n## 105          6.5  virginica\n## 106          7.6  virginica\n## 107          4.9  virginica\n## 108          7.3  virginica\n## 109          6.7  virginica\n## 110          7.2  virginica\n## 111          6.5  virginica\n## 112          6.4  virginica\n## 113          6.8  virginica\n## 114          5.7  virginica\n## 115          5.8  virginica\n## 116          6.4  virginica\n## 117          6.5  virginica\n## 118          7.7  virginica\n## 119          7.7  virginica\n## 120          6.0  virginica\n## 121          6.9  virginica\n## 122          5.6  virginica\n## 123          7.7  virginica\n## 124          6.3  virginica\n## 125          6.7  virginica\n## 126          7.2  virginica\n## 127          6.2  virginica\n## 128          6.1  virginica\n## 129          6.4  virginica\n## 130          7.2  virginica\n## 131          7.4  virginica\n## 132          7.9  virginica\n## 133          6.4  virginica\n## 134          6.3  virginica\n## 135          6.1  virginica\n## 136          7.7  virginica\n## 137          6.3  virginica\n## 138          6.4  virginica\n## 139          6.0  virginica\n## 140          6.9  virginica\n## 141          6.7  virginica\n## 142          6.9  virginica\n## 143          5.8  virginica\n## 144          6.8  virginica\n## 145          6.7  virginica\n## 146          6.7  virginica\n## 147          6.3  virginica\n## 148          6.5  virginica\n## 149          6.2  virginica\n## 150          5.9  virginica\niris[1:5, ] # 1~5행의 모든 데이터\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\niris[1:5, c(1, 3)] # 1~5행의 데이터 중 1, 3열의 데이터\n##   Sepal.Length Petal.Length\n## 1          5.1          1.4\n## 2          4.9          1.4\n## 3          4.7          1.3\n## 4          4.6          1.5\n## 5          5.0          1.4\n\n\ndim(iris) # 행과 열의 개수 출력\n## [1] 150   5\nnrow(iris) # 행의 개수 출력\n## [1] 150\nncol(iris) # 열의 개수 출력\n## [1] 5\ncolnames(iris) # 열 이름 출력, names()와 결과 동일\n## [1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"\nhead(iris) # 데이터셋의 앞부분 일부 출력\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\ntail(iris) # 데이터셋의 뒷부분 일부 출력\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n## 145          6.7         3.3          5.7         2.5 virginica\n## 146          6.7         3.0          5.2         2.3 virginica\n## 147          6.3         2.5          5.0         1.9 virginica\n## 148          6.5         3.0          5.2         2.0 virginica\n## 149          6.2         3.4          5.4         2.3 virginica\n## 150          5.9         3.0          5.1         1.8 virginica\n\nhead(iris, 10)\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1           5.1         3.5          1.4         0.2  setosa\n## 2           4.9         3.0          1.4         0.2  setosa\n## 3           4.7         3.2          1.3         0.2  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\n## 5           5.0         3.6          1.4         0.2  setosa\n## 6           5.4         3.9          1.7         0.4  setosa\n## 7           4.6         3.4          1.4         0.3  setosa\n## 8           5.0         3.4          1.5         0.2  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\ntail(iris, 20)\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n## 131          7.4         2.8          6.1         1.9 virginica\n## 132          7.9         3.8          6.4         2.0 virginica\n## 133          6.4         2.8          5.6         2.2 virginica\n## 134          6.3         2.8          5.1         1.5 virginica\n## 135          6.1         2.6          5.6         1.4 virginica\n## 136          7.7         3.0          6.1         2.3 virginica\n## 137          6.3         3.4          5.6         2.4 virginica\n## 138          6.4         3.1          5.5         1.8 virginica\n## 139          6.0         3.0          4.8         1.8 virginica\n## 140          6.9         3.1          5.4         2.1 virginica\n## 141          6.7         3.1          5.6         2.4 virginica\n## 142          6.9         3.1          5.1         2.3 virginica\n## 143          5.8         2.7          5.1         1.9 virginica\n## 144          6.8         3.2          5.9         2.3 virginica\n## 145          6.7         3.3          5.7         2.5 virginica\n## 146          6.7         3.0          5.2         2.3 virginica\n## 147          6.3         2.5          5.0         1.9 virginica\n## 148          6.5         3.0          5.2         2.0 virginica\n## 149          6.2         3.4          5.4         2.3 virginica\n## 150          5.9         3.0          5.1         1.8 virginica\n\n\nstr(iris) # 데이터셋 요약 정보 보기\n## 'data.frame':    150 obs. of  5 variables:\n##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n##  $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\niris[, 5] # 품종 데이터 보기\n##   [1] setosa     setosa     setosa     setosa     setosa     setosa    \n##   [7] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [13] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [19] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [25] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [31] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [37] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [43] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [49] setosa     setosa     versicolor versicolor versicolor versicolor\n##  [55] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [61] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [67] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [73] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [79] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [85] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [91] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [97] versicolor versicolor versicolor versicolor virginica  virginica \n## [103] virginica  virginica  virginica  virginica  virginica  virginica \n## [109] virginica  virginica  virginica  virginica  virginica  virginica \n## [115] virginica  virginica  virginica  virginica  virginica  virginica \n## [121] virginica  virginica  virginica  virginica  virginica  virginica \n## [127] virginica  virginica  virginica  virginica  virginica  virginica \n## [133] virginica  virginica  virginica  virginica  virginica  virginica \n## [139] virginica  virginica  virginica  virginica  virginica  virginica \n## [145] virginica  virginica  virginica  virginica  virginica  virginica \n## Levels: setosa versicolor virginica\nunique(iris[, 5]) # 품종의 종류 보기(중복 제거)\n## [1] setosa     versicolor virginica \n## Levels: setosa versicolor virginica\ntable(iris[, \"Species\"]) # 품종의 종류별 행의 개수 세기\n## \n##     setosa versicolor  virginica \n##         50         50         50\n\n\niris[, -5]\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width\n## 1            5.1         3.5          1.4         0.2\n## 2            4.9         3.0          1.4         0.2\n## 3            4.7         3.2          1.3         0.2\n## 4            4.6         3.1          1.5         0.2\n## 5            5.0         3.6          1.4         0.2\n## 6            5.4         3.9          1.7         0.4\n## 7            4.6         3.4          1.4         0.3\n## 8            5.0         3.4          1.5         0.2\n## 9            4.4         2.9          1.4         0.2\n## 10           4.9         3.1          1.5         0.1\n## 11           5.4         3.7          1.5         0.2\n## 12           4.8         3.4          1.6         0.2\n## 13           4.8         3.0          1.4         0.1\n## 14           4.3         3.0          1.1         0.1\n## 15           5.8         4.0          1.2         0.2\n## 16           5.7         4.4          1.5         0.4\n## 17           5.4         3.9          1.3         0.4\n## 18           5.1         3.5          1.4         0.3\n## 19           5.7         3.8          1.7         0.3\n## 20           5.1         3.8          1.5         0.3\n## 21           5.4         3.4          1.7         0.2\n## 22           5.1         3.7          1.5         0.4\n## 23           4.6         3.6          1.0         0.2\n## 24           5.1         3.3          1.7         0.5\n## 25           4.8         3.4          1.9         0.2\n## 26           5.0         3.0          1.6         0.2\n## 27           5.0         3.4          1.6         0.4\n## 28           5.2         3.5          1.5         0.2\n## 29           5.2         3.4          1.4         0.2\n## 30           4.7         3.2          1.6         0.2\n## 31           4.8         3.1          1.6         0.2\n## 32           5.4         3.4          1.5         0.4\n## 33           5.2         4.1          1.5         0.1\n## 34           5.5         4.2          1.4         0.2\n## 35           4.9         3.1          1.5         0.2\n## 36           5.0         3.2          1.2         0.2\n## 37           5.5         3.5          1.3         0.2\n## 38           4.9         3.6          1.4         0.1\n## 39           4.4         3.0          1.3         0.2\n## 40           5.1         3.4          1.5         0.2\n## 41           5.0         3.5          1.3         0.3\n## 42           4.5         2.3          1.3         0.3\n## 43           4.4         3.2          1.3         0.2\n## 44           5.0         3.5          1.6         0.6\n## 45           5.1         3.8          1.9         0.4\n## 46           4.8         3.0          1.4         0.3\n## 47           5.1         3.8          1.6         0.2\n## 48           4.6         3.2          1.4         0.2\n## 49           5.3         3.7          1.5         0.2\n## 50           5.0         3.3          1.4         0.2\n## 51           7.0         3.2          4.7         1.4\n## 52           6.4         3.2          4.5         1.5\n## 53           6.9         3.1          4.9         1.5\n## 54           5.5         2.3          4.0         1.3\n## 55           6.5         2.8          4.6         1.5\n## 56           5.7         2.8          4.5         1.3\n## 57           6.3         3.3          4.7         1.6\n## 58           4.9         2.4          3.3         1.0\n## 59           6.6         2.9          4.6         1.3\n## 60           5.2         2.7          3.9         1.4\n## 61           5.0         2.0          3.5         1.0\n## 62           5.9         3.0          4.2         1.5\n## 63           6.0         2.2          4.0         1.0\n## 64           6.1         2.9          4.7         1.4\n## 65           5.6         2.9          3.6         1.3\n## 66           6.7         3.1          4.4         1.4\n## 67           5.6         3.0          4.5         1.5\n## 68           5.8         2.7          4.1         1.0\n## 69           6.2         2.2          4.5         1.5\n## 70           5.6         2.5          3.9         1.1\n## 71           5.9         3.2          4.8         1.8\n## 72           6.1         2.8          4.0         1.3\n## 73           6.3         2.5          4.9         1.5\n## 74           6.1         2.8          4.7         1.2\n## 75           6.4         2.9          4.3         1.3\n## 76           6.6         3.0          4.4         1.4\n## 77           6.8         2.8          4.8         1.4\n## 78           6.7         3.0          5.0         1.7\n## 79           6.0         2.9          4.5         1.5\n## 80           5.7         2.6          3.5         1.0\n## 81           5.5         2.4          3.8         1.1\n## 82           5.5         2.4          3.7         1.0\n## 83           5.8         2.7          3.9         1.2\n## 84           6.0         2.7          5.1         1.6\n## 85           5.4         3.0          4.5         1.5\n## 86           6.0         3.4          4.5         1.6\n## 87           6.7         3.1          4.7         1.5\n## 88           6.3         2.3          4.4         1.3\n## 89           5.6         3.0          4.1         1.3\n## 90           5.5         2.5          4.0         1.3\n## 91           5.5         2.6          4.4         1.2\n## 92           6.1         3.0          4.6         1.4\n## 93           5.8         2.6          4.0         1.2\n## 94           5.0         2.3          3.3         1.0\n## 95           5.6         2.7          4.2         1.3\n## 96           5.7         3.0          4.2         1.2\n## 97           5.7         2.9          4.2         1.3\n## 98           6.2         2.9          4.3         1.3\n## 99           5.1         2.5          3.0         1.1\n## 100          5.7         2.8          4.1         1.3\n## 101          6.3         3.3          6.0         2.5\n## 102          5.8         2.7          5.1         1.9\n## 103          7.1         3.0          5.9         2.1\n## 104          6.3         2.9          5.6         1.8\n## 105          6.5         3.0          5.8         2.2\n## 106          7.6         3.0          6.6         2.1\n## 107          4.9         2.5          4.5         1.7\n## 108          7.3         2.9          6.3         1.8\n## 109          6.7         2.5          5.8         1.8\n## 110          7.2         3.6          6.1         2.5\n## 111          6.5         3.2          5.1         2.0\n## 112          6.4         2.7          5.3         1.9\n## 113          6.8         3.0          5.5         2.1\n## 114          5.7         2.5          5.0         2.0\n## 115          5.8         2.8          5.1         2.4\n## 116          6.4         3.2          5.3         2.3\n## 117          6.5         3.0          5.5         1.8\n## 118          7.7         3.8          6.7         2.2\n## 119          7.7         2.6          6.9         2.3\n## 120          6.0         2.2          5.0         1.5\n## 121          6.9         3.2          5.7         2.3\n## 122          5.6         2.8          4.9         2.0\n## 123          7.7         2.8          6.7         2.0\n## 124          6.3         2.7          4.9         1.8\n## 125          6.7         3.3          5.7         2.1\n## 126          7.2         3.2          6.0         1.8\n## 127          6.2         2.8          4.8         1.8\n## 128          6.1         3.0          4.9         1.8\n## 129          6.4         2.8          5.6         2.1\n## 130          7.2         3.0          5.8         1.6\n## 131          7.4         2.8          6.1         1.9\n## 132          7.9         3.8          6.4         2.0\n## 133          6.4         2.8          5.6         2.2\n## 134          6.3         2.8          5.1         1.5\n## 135          6.1         2.6          5.6         1.4\n## 136          7.7         3.0          6.1         2.3\n## 137          6.3         3.4          5.6         2.4\n## 138          6.4         3.1          5.5         1.8\n## 139          6.0         3.0          4.8         1.8\n## 140          6.9         3.1          5.4         2.1\n## 141          6.7         3.1          5.6         2.4\n## 142          6.9         3.1          5.1         2.3\n## 143          5.8         2.7          5.1         1.9\n## 144          6.8         3.2          5.9         2.3\n## 145          6.7         3.3          5.7         2.5\n## 146          6.7         3.0          5.2         2.3\n## 147          6.3         2.5          5.0         1.9\n## 148          6.5         3.0          5.2         2.0\n## 149          6.2         3.4          5.4         2.3\n## 150          5.9         3.0          5.1         1.8\ncolSums(iris[, -5]) # 열별 합계\n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n##        876.5        458.6        563.7        179.9\ncolMeans(iris[, -5]) # 열별 평균\n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n##     5.843333     3.057333     3.758000     1.199333\nrowSums(iris[, -5]) # 행별 합계\n##   [1] 10.2  9.5  9.4  9.4 10.2 11.4  9.7 10.1  8.9  9.6 10.8 10.0  9.3  8.5 11.2\n##  [16] 12.0 11.0 10.3 11.5 10.7 10.7 10.7  9.4 10.6 10.3  9.8 10.4 10.4 10.2  9.7\n##  [31]  9.7 10.7 10.9 11.3  9.7  9.6 10.5 10.0  8.9 10.2 10.1  8.4  9.1 10.7 11.2\n##  [46]  9.5 10.7  9.4 10.7  9.9 16.3 15.6 16.4 13.1 15.4 14.3 15.9 11.6 15.4 13.2\n##  [61] 11.5 14.6 13.2 15.1 13.4 15.6 14.6 13.6 14.4 13.1 15.7 14.2 15.2 14.8 14.9\n##  [76] 15.4 15.8 16.4 14.9 12.8 12.8 12.6 13.6 15.4 14.4 15.5 16.0 14.3 14.0 13.3\n##  [91] 13.7 15.1 13.6 11.6 13.8 14.1 14.1 14.7 11.7 13.9 18.1 15.5 18.1 16.6 17.5\n## [106] 19.3 13.6 18.3 16.8 19.4 16.8 16.3 17.4 15.2 16.1 17.2 16.8 20.4 19.5 14.7\n## [121] 18.1 15.3 19.2 15.7 17.8 18.2 15.6 15.8 16.9 17.6 18.2 20.1 17.0 15.7 15.7\n## [136] 19.1 17.7 16.8 15.6 17.5 17.8 17.4 15.5 18.2 18.2 17.2 15.7 16.7 17.3 15.8\nrowMeans(iris[, -5]) # 행별 평균\n##   [1] 2.550 2.375 2.350 2.350 2.550 2.850 2.425 2.525 2.225 2.400 2.700 2.500\n##  [13] 2.325 2.125 2.800 3.000 2.750 2.575 2.875 2.675 2.675 2.675 2.350 2.650\n##  [25] 2.575 2.450 2.600 2.600 2.550 2.425 2.425 2.675 2.725 2.825 2.425 2.400\n##  [37] 2.625 2.500 2.225 2.550 2.525 2.100 2.275 2.675 2.800 2.375 2.675 2.350\n##  [49] 2.675 2.475 4.075 3.900 4.100 3.275 3.850 3.575 3.975 2.900 3.850 3.300\n##  [61] 2.875 3.650 3.300 3.775 3.350 3.900 3.650 3.400 3.600 3.275 3.925 3.550\n##  [73] 3.800 3.700 3.725 3.850 3.950 4.100 3.725 3.200 3.200 3.150 3.400 3.850\n##  [85] 3.600 3.875 4.000 3.575 3.500 3.325 3.425 3.775 3.400 2.900 3.450 3.525\n##  [97] 3.525 3.675 2.925 3.475 4.525 3.875 4.525 4.150 4.375 4.825 3.400 4.575\n## [109] 4.200 4.850 4.200 4.075 4.350 3.800 4.025 4.300 4.200 5.100 4.875 3.675\n## [121] 4.525 3.825 4.800 3.925 4.450 4.550 3.900 3.950 4.225 4.400 4.550 5.025\n## [133] 4.250 3.925 3.925 4.775 4.425 4.200 3.900 4.375 4.450 4.350 3.875 4.550\n## [145] 4.550 4.300 3.925 4.175 4.325 3.950\n\n\nz &lt;- matrix(1:20, nrow = 4, ncol = 5)\nz\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\nt(z) # 행과열 방향 전환\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    2    3    4\n## [2,]    5    6    7    8\n## [3,]    9   10   11   12\n## [4,]   13   14   15   16\n## [5,]   17   18   19   20\n\n\nIR.1 &lt;- subset(iris, Species == \"setosa\")\nIR.1\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1           5.1         3.5          1.4         0.2  setosa\n## 2           4.9         3.0          1.4         0.2  setosa\n## 3           4.7         3.2          1.3         0.2  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\n## 5           5.0         3.6          1.4         0.2  setosa\n## 6           5.4         3.9          1.7         0.4  setosa\n## 7           4.6         3.4          1.4         0.3  setosa\n## 8           5.0         3.4          1.5         0.2  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\n## 11          5.4         3.7          1.5         0.2  setosa\n## 12          4.8         3.4          1.6         0.2  setosa\n## 13          4.8         3.0          1.4         0.1  setosa\n## 14          4.3         3.0          1.1         0.1  setosa\n## 15          5.8         4.0          1.2         0.2  setosa\n## 16          5.7         4.4          1.5         0.4  setosa\n## 17          5.4         3.9          1.3         0.4  setosa\n## 18          5.1         3.5          1.4         0.3  setosa\n## 19          5.7         3.8          1.7         0.3  setosa\n## 20          5.1         3.8          1.5         0.3  setosa\n## 21          5.4         3.4          1.7         0.2  setosa\n## 22          5.1         3.7          1.5         0.4  setosa\n## 23          4.6         3.6          1.0         0.2  setosa\n## 24          5.1         3.3          1.7         0.5  setosa\n## 25          4.8         3.4          1.9         0.2  setosa\n## 26          5.0         3.0          1.6         0.2  setosa\n## 27          5.0         3.4          1.6         0.4  setosa\n## 28          5.2         3.5          1.5         0.2  setosa\n## 29          5.2         3.4          1.4         0.2  setosa\n## 30          4.7         3.2          1.6         0.2  setosa\n## 31          4.8         3.1          1.6         0.2  setosa\n## 32          5.4         3.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\n## 35          4.9         3.1          1.5         0.2  setosa\n## 36          5.0         3.2          1.2         0.2  setosa\n## 37          5.5         3.5          1.3         0.2  setosa\n## 38          4.9         3.6          1.4         0.1  setosa\n## 39          4.4         3.0          1.3         0.2  setosa\n## 40          5.1         3.4          1.5         0.2  setosa\n## 41          5.0         3.5          1.3         0.3  setosa\n## 42          4.5         2.3          1.3         0.3  setosa\n## 43          4.4         3.2          1.3         0.2  setosa\n## 44          5.0         3.5          1.6         0.6  setosa\n## 45          5.1         3.8          1.9         0.4  setosa\n## 46          4.8         3.0          1.4         0.3  setosa\n## 47          5.1         3.8          1.6         0.2  setosa\n## 48          4.6         3.2          1.4         0.2  setosa\n## 49          5.3         3.7          1.5         0.2  setosa\n## 50          5.0         3.3          1.4         0.2  setosa\n\nIR.2 &lt;- subset(iris, Sepal.Length &gt; 5.0 & Sepal.Width &gt; 4.0)\nIR.2\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 16          5.7         4.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\nIR.2[, c(2, 4)] # 2, 4열의 값만 추출\n##    Sepal.Width Petal.Width\n## 16         4.4         0.4\n## 33         4.1         0.1\n## 34         4.2         0.2\n\nIR.3 &lt;- subset(iris, Sepal.Length &gt; 5.0 | Sepal.Width &gt; 4.0)\nIR.3\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 1            5.1         3.5          1.4         0.2     setosa\n## 6            5.4         3.9          1.7         0.4     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 15           5.8         4.0          1.2         0.2     setosa\n## 16           5.7         4.4          1.5         0.4     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 34           5.5         4.2          1.4         0.2     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 49           5.3         3.7          1.5         0.2     setosa\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 114          5.7         2.5          5.0         2.0  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 120          6.0         2.2          5.0         1.5  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 122          5.6         2.8          4.9         2.0  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 127          6.2         2.8          4.8         1.8  virginica\n## 128          6.1         3.0          4.9         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\n\n\na &lt;- matrix(1:20, 4, 5)\nb &lt;- matrix(21:40, 4, 5)\na ; b\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   21   25   29   33   37\n## [2,]   22   26   30   34   38\n## [3,]   23   27   31   35   39\n## [4,]   24   28   32   36   40\n\n2 * a # 매트릭스 a에 저장된 값들에 2를 곱하기\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    2   10   18   26   34\n## [2,]    4   12   20   28   36\n## [3,]    6   14   22   30   38\n## [4,]    8   16   24   32   40\nb - 5\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   16   20   24   28   32\n## [2,]   17   21   25   29   33\n## [3,]   18   22   26   30   34\n## [4,]   19   23   27   31   35\n2 * a + 3 * b\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   65   85  105  125  145\n## [2,]   70   90  110  130  150\n## [3,]   75   95  115  135  155\n## [4,]   80  100  120  140  160\n\na + b\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   22   30   38   46   54\n## [2,]   24   32   40   48   56\n## [3,]   26   34   42   50   58\n## [4,]   28   36   44   52   60\nb - a\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   20   20   20   20   20\n## [2,]   20   20   20   20   20\n## [3,]   20   20   20   20   20\n## [4,]   20   20   20   20   20\nb / a\n##           [,1]     [,2]     [,3]     [,4]     [,5]\n## [1,] 21.000000 5.000000 3.222222 2.538462 2.176471\n## [2,] 11.000000 4.333333 3.000000 2.428571 2.111111\n## [3,]  7.666667 3.857143 2.818182 2.333333 2.052632\n## [4,]  6.000000 3.500000 2.666667 2.250000 2.000000\na * b\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   21  125  261  429  629\n## [2,]   44  156  300  476  684\n## [3,]   69  189  341  525  741\n## [4,]   96  224  384  576  800\n\na &lt;- a * 3\na\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    3   15   27   39   51\n## [2,]    6   18   30   42   54\n## [3,]    9   21   33   45   57\n## [4,]   12   24   36   48   60\nb &lt;- b - 5\nb\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   16   20   24   28   32\n## [2,]   17   21   25   29   33\n## [3,]   18   22   26   30   34\n## [4,]   19   23   27   31   35\n\n\nclass(iris) # iris 데이터셋의 자료구조 확인\n## [1] \"data.frame\"\nclass(state.x77) # state.x77 데이터셋의 자료구조 확인\n## [1] \"matrix\" \"array\"\nis.matrix(iris) # 데이터셋이 매트릭스인지를 확인하는 함수\n## [1] FALSE\nis.data.frame(iris) # 데이터셋이 데이터프레임인지를 확인하는 함수\n## [1] TRUE\nis.matrix(state.x77)\n## [1] TRUE\nis.data.frame(state.x77)\n## [1] FALSE\n\n\n# 매트릭스를 데이터프레임으로 변환\nst &lt;- data.frame(state.x77)\nhead(st)\n##            Population Income Illiteracy Life.Exp Murder HS.Grad Frost   Area\n## Alabama          3615   3624        2.1    69.05   15.1    41.3    20  50708\n## Alaska            365   6315        1.5    69.31   11.3    66.7   152 566432\n## Arizona          2212   4530        1.8    70.55    7.8    58.1    15 113417\n## Arkansas         2110   3378        1.9    70.66   10.1    39.9    65  51945\n## California      21198   5114        1.1    71.71   10.3    62.6    20 156361\n## Colorado         2541   4884        0.7    72.06    6.8    63.9   166 103766\nclass(st)\n## [1] \"data.frame\"\n\n\niris[, \"Species\"] # 결과=벡터. 매트릭스와 데이터프레임 모두 가능\n##   [1] setosa     setosa     setosa     setosa     setosa     setosa    \n##   [7] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [13] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [19] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [25] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [31] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [37] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [43] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [49] setosa     setosa     versicolor versicolor versicolor versicolor\n##  [55] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [61] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [67] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [73] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [79] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [85] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [91] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [97] versicolor versicolor versicolor versicolor virginica  virginica \n## [103] virginica  virginica  virginica  virginica  virginica  virginica \n## [109] virginica  virginica  virginica  virginica  virginica  virginica \n## [115] virginica  virginica  virginica  virginica  virginica  virginica \n## [121] virginica  virginica  virginica  virginica  virginica  virginica \n## [127] virginica  virginica  virginica  virginica  virginica  virginica \n## [133] virginica  virginica  virginica  virginica  virginica  virginica \n## [139] virginica  virginica  virginica  virginica  virginica  virginica \n## [145] virginica  virginica  virginica  virginica  virginica  virginica \n## Levels: setosa versicolor virginica\niris[, 5] # 결과=벡터. 매트릭스와 데이터프레임 모두 가능\n##   [1] setosa     setosa     setosa     setosa     setosa     setosa    \n##   [7] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [13] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [19] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [25] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [31] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [37] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [43] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [49] setosa     setosa     versicolor versicolor versicolor versicolor\n##  [55] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [61] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [67] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [73] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [79] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [85] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [91] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [97] versicolor versicolor versicolor versicolor virginica  virginica \n## [103] virginica  virginica  virginica  virginica  virginica  virginica \n## [109] virginica  virginica  virginica  virginica  virginica  virginica \n## [115] virginica  virginica  virginica  virginica  virginica  virginica \n## [121] virginica  virginica  virginica  virginica  virginica  virginica \n## [127] virginica  virginica  virginica  virginica  virginica  virginica \n## [133] virginica  virginica  virginica  virginica  virginica  virginica \n## [139] virginica  virginica  virginica  virginica  virginica  virginica \n## [145] virginica  virginica  virginica  virginica  virginica  virginica \n## Levels: setosa versicolor virginica\niris[\"Species\"] # 결과=데이터프레임. 데이터프레임만 가능\n##        Species\n## 1       setosa\n## 2       setosa\n## 3       setosa\n## 4       setosa\n## 5       setosa\n## 6       setosa\n## 7       setosa\n## 8       setosa\n## 9       setosa\n## 10      setosa\n## 11      setosa\n## 12      setosa\n## 13      setosa\n## 14      setosa\n## 15      setosa\n## 16      setosa\n## 17      setosa\n## 18      setosa\n## 19      setosa\n## 20      setosa\n## 21      setosa\n## 22      setosa\n## 23      setosa\n## 24      setosa\n## 25      setosa\n## 26      setosa\n## 27      setosa\n## 28      setosa\n## 29      setosa\n## 30      setosa\n## 31      setosa\n## 32      setosa\n## 33      setosa\n## 34      setosa\n## 35      setosa\n## 36      setosa\n## 37      setosa\n## 38      setosa\n## 39      setosa\n## 40      setosa\n## 41      setosa\n## 42      setosa\n## 43      setosa\n## 44      setosa\n## 45      setosa\n## 46      setosa\n## 47      setosa\n## 48      setosa\n## 49      setosa\n## 50      setosa\n## 51  versicolor\n## 52  versicolor\n## 53  versicolor\n## 54  versicolor\n## 55  versicolor\n## 56  versicolor\n## 57  versicolor\n## 58  versicolor\n## 59  versicolor\n## 60  versicolor\n## 61  versicolor\n## 62  versicolor\n## 63  versicolor\n## 64  versicolor\n## 65  versicolor\n## 66  versicolor\n## 67  versicolor\n## 68  versicolor\n## 69  versicolor\n## 70  versicolor\n## 71  versicolor\n## 72  versicolor\n## 73  versicolor\n## 74  versicolor\n## 75  versicolor\n## 76  versicolor\n## 77  versicolor\n## 78  versicolor\n## 79  versicolor\n## 80  versicolor\n## 81  versicolor\n## 82  versicolor\n## 83  versicolor\n## 84  versicolor\n## 85  versicolor\n## 86  versicolor\n## 87  versicolor\n## 88  versicolor\n## 89  versicolor\n## 90  versicolor\n## 91  versicolor\n## 92  versicolor\n## 93  versicolor\n## 94  versicolor\n## 95  versicolor\n## 96  versicolor\n## 97  versicolor\n## 98  versicolor\n## 99  versicolor\n## 100 versicolor\n## 101  virginica\n## 102  virginica\n## 103  virginica\n## 104  virginica\n## 105  virginica\n## 106  virginica\n## 107  virginica\n## 108  virginica\n## 109  virginica\n## 110  virginica\n## 111  virginica\n## 112  virginica\n## 113  virginica\n## 114  virginica\n## 115  virginica\n## 116  virginica\n## 117  virginica\n## 118  virginica\n## 119  virginica\n## 120  virginica\n## 121  virginica\n## 122  virginica\n## 123  virginica\n## 124  virginica\n## 125  virginica\n## 126  virginica\n## 127  virginica\n## 128  virginica\n## 129  virginica\n## 130  virginica\n## 131  virginica\n## 132  virginica\n## 133  virginica\n## 134  virginica\n## 135  virginica\n## 136  virginica\n## 137  virginica\n## 138  virginica\n## 139  virginica\n## 140  virginica\n## 141  virginica\n## 142  virginica\n## 143  virginica\n## 144  virginica\n## 145  virginica\n## 146  virginica\n## 147  virginica\n## 148  virginica\n## 149  virginica\n## 150  virginica\niris[5] # 결과=데이터프레임. 데이터프레임만 가능\n##        Species\n## 1       setosa\n## 2       setosa\n## 3       setosa\n## 4       setosa\n## 5       setosa\n## 6       setosa\n## 7       setosa\n## 8       setosa\n## 9       setosa\n## 10      setosa\n## 11      setosa\n## 12      setosa\n## 13      setosa\n## 14      setosa\n## 15      setosa\n## 16      setosa\n## 17      setosa\n## 18      setosa\n## 19      setosa\n## 20      setosa\n## 21      setosa\n## 22      setosa\n## 23      setosa\n## 24      setosa\n## 25      setosa\n## 26      setosa\n## 27      setosa\n## 28      setosa\n## 29      setosa\n## 30      setosa\n## 31      setosa\n## 32      setosa\n## 33      setosa\n## 34      setosa\n## 35      setosa\n## 36      setosa\n## 37      setosa\n## 38      setosa\n## 39      setosa\n## 40      setosa\n## 41      setosa\n## 42      setosa\n## 43      setosa\n## 44      setosa\n## 45      setosa\n## 46      setosa\n## 47      setosa\n## 48      setosa\n## 49      setosa\n## 50      setosa\n## 51  versicolor\n## 52  versicolor\n## 53  versicolor\n## 54  versicolor\n## 55  versicolor\n## 56  versicolor\n## 57  versicolor\n## 58  versicolor\n## 59  versicolor\n## 60  versicolor\n## 61  versicolor\n## 62  versicolor\n## 63  versicolor\n## 64  versicolor\n## 65  versicolor\n## 66  versicolor\n## 67  versicolor\n## 68  versicolor\n## 69  versicolor\n## 70  versicolor\n## 71  versicolor\n## 72  versicolor\n## 73  versicolor\n## 74  versicolor\n## 75  versicolor\n## 76  versicolor\n## 77  versicolor\n## 78  versicolor\n## 79  versicolor\n## 80  versicolor\n## 81  versicolor\n## 82  versicolor\n## 83  versicolor\n## 84  versicolor\n## 85  versicolor\n## 86  versicolor\n## 87  versicolor\n## 88  versicolor\n## 89  versicolor\n## 90  versicolor\n## 91  versicolor\n## 92  versicolor\n## 93  versicolor\n## 94  versicolor\n## 95  versicolor\n## 96  versicolor\n## 97  versicolor\n## 98  versicolor\n## 99  versicolor\n## 100 versicolor\n## 101  virginica\n## 102  virginica\n## 103  virginica\n## 104  virginica\n## 105  virginica\n## 106  virginica\n## 107  virginica\n## 108  virginica\n## 109  virginica\n## 110  virginica\n## 111  virginica\n## 112  virginica\n## 113  virginica\n## 114  virginica\n## 115  virginica\n## 116  virginica\n## 117  virginica\n## 118  virginica\n## 119  virginica\n## 120  virginica\n## 121  virginica\n## 122  virginica\n## 123  virginica\n## 124  virginica\n## 125  virginica\n## 126  virginica\n## 127  virginica\n## 128  virginica\n## 129  virginica\n## 130  virginica\n## 131  virginica\n## 132  virginica\n## 133  virginica\n## 134  virginica\n## 135  virginica\n## 136  virginica\n## 137  virginica\n## 138  virginica\n## 139  virginica\n## 140  virginica\n## 141  virginica\n## 142  virginica\n## 143  virginica\n## 144  virginica\n## 145  virginica\n## 146  virginica\n## 147  virginica\n## 148  virginica\n## 149  virginica\n## 150  virginica\niris$Species # 결과=벡터. 데이터프레임만 가능\n##   [1] setosa     setosa     setosa     setosa     setosa     setosa    \n##   [7] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [13] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [19] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [25] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [31] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [37] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [43] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [49] setosa     setosa     versicolor versicolor versicolor versicolor\n##  [55] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [61] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [67] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [73] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [79] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [85] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [91] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [97] versicolor versicolor versicolor versicolor virginica  virginica \n## [103] virginica  virginica  virginica  virginica  virginica  virginica \n## [109] virginica  virginica  virginica  virginica  virginica  virginica \n## [115] virginica  virginica  virginica  virginica  virginica  virginica \n## [121] virginica  virginica  virginica  virginica  virginica  virginica \n## [127] virginica  virginica  virginica  virginica  virginica  virginica \n## [133] virginica  virginica  virginica  virginica  virginica  virginica \n## [139] virginica  virginica  virginica  virginica  virginica  virginica \n## [145] virginica  virginica  virginica  virginica  virginica  virginica \n## Levels: setosa versicolor virginica\n\n\ngetwd()\n## [1] \"C:/Users/Hyunsoo Kim/Desktop/senior_grade/blog/my-quarto-website\"\n# setwd(\"G:/내 드라이브/202202/R_Basic/data\") # 작업 폴더 지정\nair &lt;- read.csv(\"./R_Basic/data/airquality.csv\", header = T) # .csv 파일 읽기\nhead(air)\n##                                    version.https...git.lfs.github.com.spec.v1\n## 1 oid sha256:6fdc84af524856a54abe063336bfea6511e9fb5dfcd2ec6e1dfa9e1e4d8c7357\n## 2                                                                   size 3044\n\n\nmy.iris &lt;- subset(iris, Species = 'Setosa') # Setosa 품종 데이터만 추출\n## Warning: In subset.data.frame(iris, Species = \"Setosa\") :\n##  extra argument 'Species' will be disregarded\nwrite.csv(my.iris, \"./R_Basic/data/my_iris_1.csv\") # .csv 파일에 저장하기"
  },
  {
    "objectID": "R_Basic.html#장.-조건문-반복문-함수",
    "href": "R_Basic.html#장.-조건문-반복문-함수",
    "title": "R Basic",
    "section": "",
    "text": "job.type &lt;- 'A'\nif (job.type == 'B') {\n    bonus &lt;- 200 # 직무 유형이 B일 때 실행\n} else {\n    bonus &lt;- 100 # 직무 유형이 B가 아닌 나머지 경우 실행\n}\nprint(bonus)\n## [1] 100\n\n\njob.type &lt;- 'B'\nbonus &lt;- 100\nif (job.type == 'A') {\n    bonus &lt;- 200 # 직무 유형이 A일 때 실행\n}\nprint(bonus)\n## [1] 100\n\n\nscore &lt;- 85\n\nif (score &gt; 90) {\n    grade &lt;- 'A'\n} else if (score &gt; 80) {\n    grade &lt;- 'B'\n} else if (score &gt; 70) {\n    grade &lt;- 'C'\n} else if (score &gt; 60) {\n    grade &lt;- 'D'\n} else {\n    grade &lt;- 'F'\n}\n\nprint(grade)\n## [1] \"B\"\n\n\na &lt;- 10\nb &lt;- 20\nif (a &gt; 5 & b &gt; 5) {    # and 사용\n    print(a + b)\n}\n## [1] 30\n\nif (a &gt; 5 | b &gt; 30) {   # or 사용\n    print(a * b)\n}\n## [1] 200\n\nif (a &gt; 5 & b &gt; 30) {\n    print(a * b)\n}\n\nif (a &gt; 20 | b &gt; 30) {\n    print(a * b)\n}\n\nif (a &gt; 20 & b &gt; 15) {\n    print(a * b)\n}\n\nr_basic &lt;- 70\npython_basic &lt;- 82\n\nif (r_basic &gt; 80 & python_basic &gt; 80) {\n    grade &lt;- \"Excellent\"\n} else {\n    grade &lt;- \"Good\"\n}\ngrade\n## [1] \"Good\"\n\n\na &lt;- 10\nb &lt;- 20\n\nif (a &gt; b) {\n    c &lt;- a\n} else {\n    c &lt;- b\n}\nprint(c)\n## [1] 20\n\na &lt;- 10\nb &lt;- 20\n\nc &lt;- ifelse(a &gt; b, a, b)\nprint(c)\n## [1] 20\n\n\nfor(i in 1:5) {\n    print('*')\n}\n## [1] \"*\"\n## [1] \"*\"\n## [1] \"*\"\n## [1] \"*\"\n## [1] \"*\"\n\nfor (i in 1:5) {\n    print(i)\n}\n## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\n\nfor (i in 1:5) {\n    a &lt;- i * 2\n    print(a)\n}\n## [1] 2\n## [1] 4\n## [1] 6\n## [1] 8\n## [1] 10\n\n# for (i in 1:10000) {\n#     a &lt;- i * 2\n#     print(a)\n# }\n\n# for (i in 1:10000) {\n#     a &lt;- i * 2 / 1521 + 10000\n#     print(a)\n# }\n\n\nfor (i in 6:10) {\n    print(i)\n}\n## [1] 6\n## [1] 7\n## [1] 8\n## [1] 9\n## [1] 10\n\n\nfor(i in 1:9) {\n    cat('2 *', i, '=', 2 * i, '\\n')\n}\n## 2 * 1 = 2 \n## 2 * 2 = 4 \n## 2 * 3 = 6 \n## 2 * 4 = 8 \n## 2 * 5 = 10 \n## 2 * 6 = 12 \n## 2 * 7 = 14 \n## 2 * 8 = 16 \n## 2 * 9 = 18\n\nfor (i in 1:9) {\n    cat('2 *', i, '=', 2 * i)\n}\n## 2 * 1 = 22 * 2 = 42 * 3 = 62 * 4 = 82 * 5 = 102 * 6 = 122 * 7 = 142 * 8 = 162 * 9 = 18\n\nfor (i in 1:9) {\n    j &lt;- i:10\n    print(j)\n}\n##  [1]  1  2  3  4  5  6  7  8  9 10\n## [1]  2  3  4  5  6  7  8  9 10\n## [1]  3  4  5  6  7  8  9 10\n## [1]  4  5  6  7  8  9 10\n## [1]  5  6  7  8  9 10\n## [1]  6  7  8  9 10\n## [1]  7  8  9 10\n## [1]  8  9 10\n## [1]  9 10\n\n\nfor(i in 1:20) {\n    if (i %% 2 == 0) {  # 짝수인지 확인\n        print(i)\n    }\n}\n## [1] 2\n## [1] 4\n## [1] 6\n## [1] 8\n## [1] 10\n## [1] 12\n## [1] 14\n## [1] 16\n## [1] 18\n## [1] 20\n\n\nsum &lt;- 0\nfor (i in 1:100) {\n    sum &lt;- sum + i  # sum에 i 값을 누적\n}\nprint(sum)\n## [1] 5050\n\nsum &lt;- 0\nfor (i in 1:100) {\n    sum &lt;- sum + i\n    print(c(sum, i))\n}\n## [1] 1 1\n## [1] 3 2\n## [1] 6 3\n## [1] 10  4\n## [1] 15  5\n## [1] 21  6\n## [1] 28  7\n## [1] 36  8\n## [1] 45  9\n## [1] 55 10\n## [1] 66 11\n## [1] 78 12\n## [1] 91 13\n## [1] 105  14\n## [1] 120  15\n## [1] 136  16\n## [1] 153  17\n## [1] 171  18\n## [1] 190  19\n## [1] 210  20\n## [1] 231  21\n## [1] 253  22\n## [1] 276  23\n## [1] 300  24\n## [1] 325  25\n## [1] 351  26\n## [1] 378  27\n## [1] 406  28\n## [1] 435  29\n## [1] 465  30\n## [1] 496  31\n## [1] 528  32\n## [1] 561  33\n## [1] 595  34\n## [1] 630  35\n## [1] 666  36\n## [1] 703  37\n## [1] 741  38\n## [1] 780  39\n## [1] 820  40\n## [1] 861  41\n## [1] 903  42\n## [1] 946  43\n## [1] 990  44\n## [1] 1035   45\n## [1] 1081   46\n## [1] 1128   47\n## [1] 1176   48\n## [1] 1225   49\n## [1] 1275   50\n## [1] 1326   51\n## [1] 1378   52\n## [1] 1431   53\n## [1] 1485   54\n## [1] 1540   55\n## [1] 1596   56\n## [1] 1653   57\n## [1] 1711   58\n## [1] 1770   59\n## [1] 1830   60\n## [1] 1891   61\n## [1] 1953   62\n## [1] 2016   63\n## [1] 2080   64\n## [1] 2145   65\n## [1] 2211   66\n## [1] 2278   67\n## [1] 2346   68\n## [1] 2415   69\n## [1] 2485   70\n## [1] 2556   71\n## [1] 2628   72\n## [1] 2701   73\n## [1] 2775   74\n## [1] 2850   75\n## [1] 2926   76\n## [1] 3003   77\n## [1] 3081   78\n## [1] 3160   79\n## [1] 3240   80\n## [1] 3321   81\n## [1] 3403   82\n## [1] 3486   83\n## [1] 3570   84\n## [1] 3655   85\n## [1] 3741   86\n## [1] 3828   87\n## [1] 3916   88\n## [1] 4005   89\n## [1] 4095   90\n## [1] 4186   91\n## [1] 4278   92\n## [1] 4371   93\n## [1] 4465   94\n## [1] 4560   95\n## [1] 4656   96\n## [1] 4753   97\n## [1] 4851   98\n## [1] 4950   99\n## [1] 5050  100\nprint(sum)\n## [1] 5050\n\nsum &lt;- 0\nfor (i in 1:100) {\n    print(c(sum, i))\n    sum &lt;- sum + i\n}\n## [1] 0 1\n## [1] 1 2\n## [1] 3 3\n## [1] 6 4\n## [1] 10  5\n## [1] 15  6\n## [1] 21  7\n## [1] 28  8\n## [1] 36  9\n## [1] 45 10\n## [1] 55 11\n## [1] 66 12\n## [1] 78 13\n## [1] 91 14\n## [1] 105  15\n## [1] 120  16\n## [1] 136  17\n## [1] 153  18\n## [1] 171  19\n## [1] 190  20\n## [1] 210  21\n## [1] 231  22\n## [1] 253  23\n## [1] 276  24\n## [1] 300  25\n## [1] 325  26\n## [1] 351  27\n## [1] 378  28\n## [1] 406  29\n## [1] 435  30\n## [1] 465  31\n## [1] 496  32\n## [1] 528  33\n## [1] 561  34\n## [1] 595  35\n## [1] 630  36\n## [1] 666  37\n## [1] 703  38\n## [1] 741  39\n## [1] 780  40\n## [1] 820  41\n## [1] 861  42\n## [1] 903  43\n## [1] 946  44\n## [1] 990  45\n## [1] 1035   46\n## [1] 1081   47\n## [1] 1128   48\n## [1] 1176   49\n## [1] 1225   50\n## [1] 1275   51\n## [1] 1326   52\n## [1] 1378   53\n## [1] 1431   54\n## [1] 1485   55\n## [1] 1540   56\n## [1] 1596   57\n## [1] 1653   58\n## [1] 1711   59\n## [1] 1770   60\n## [1] 1830   61\n## [1] 1891   62\n## [1] 1953   63\n## [1] 2016   64\n## [1] 2080   65\n## [1] 2145   66\n## [1] 2211   67\n## [1] 2278   68\n## [1] 2346   69\n## [1] 2415   70\n## [1] 2485   71\n## [1] 2556   72\n## [1] 2628   73\n## [1] 2701   74\n## [1] 2775   75\n## [1] 2850   76\n## [1] 2926   77\n## [1] 3003   78\n## [1] 3081   79\n## [1] 3160   80\n## [1] 3240   81\n## [1] 3321   82\n## [1] 3403   83\n## [1] 3486   84\n## [1] 3570   85\n## [1] 3655   86\n## [1] 3741   87\n## [1] 3828   88\n## [1] 3916   89\n## [1] 4005   90\n## [1] 4095   91\n## [1] 4186   92\n## [1] 4278   93\n## [1] 4371   94\n## [1] 4465   95\n## [1] 4560   96\n## [1] 4656   97\n## [1] 4753   98\n## [1] 4851   99\n## [1] 4950  100\nprint(sum)\n## [1] 5050\n\n\nnorow &lt;- nrow(iris)                             # iris의 행의 수\nmylabel &lt;- c()                                  # 비어 있는 벡터 선언\nfor (i in 1:norow) {\n    if (iris$Petal.Length[i] &lt;= 1.6) {          # 꽃잎의 길이에 따라 레이블 결정\n        mylabel[i] &lt;- 'L'\n    } else if (iris$Petal.Length[i] &gt;= 5.1) {\n        mylabel[i] &lt;- 'H'\n    } else {\n        mylabel[i] &lt;- 'M'\n    }\n    print(c(iris$Petal.Length[i], mylabel))\n}\n## [1] \"1.4\" \"L\"  \n## [1] \"1.4\" \"L\"   \"L\"  \n## [1] \"1.3\" \"L\"   \"L\"   \"L\"  \n## [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"  \n## [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [1] \"1.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"  \n## [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"  \n## [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"  \n##  [1] \"1.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"  \n##  [1] \"1.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"  \n##  [1] \"1.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"  \n##  [1] \"1\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\"\n##  [1] \"1.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"  \n##  [1] \"1.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"  \n##  [1] \"4.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"  \n##  [1] \"4.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\"\n##  [1] \"4.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"  \n##  [1] \"3.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"  \n##  [1] \"4.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"  \n##  [1] \"4\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [58] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [1] \"4.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [58] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [1] \"4.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"  \n##  [1] \"4.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"  \n##  [1] \"4.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"5\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [58] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [77] \"M\" \"M\" \"M\"\n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"  \n##  [1] \"4.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [58] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [77] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [1] \"4.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [58] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [77] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [1] \"3.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [97] \"M\"  \n##  [1] \"4.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [97] \"M\"   \"M\"  \n##  [1] \"4.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [97] \"M\"   \"M\"   \"M\"  \n##   [1] \"3\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##   [1] \"4.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##   [1] \"6\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\"\n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"  \n##   [1] \"5.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"6.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n##   [1] \"6.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"  \n##   [1] \"5.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"  \n##   [1] \"6.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"  \n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n## [109] \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"  \n##   [1] \"5.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"  \n##   [1] \"5.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"6.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"6.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n## [109] \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n##   [1] \"5.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"  \n##   [1] \"4.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"  \n##   [1] \"6.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"  \n##   [1] \"4.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"  \n##   [1] \"5.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"  \n##   [1] \"6\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n## [109] \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"M\" \"H\" \"M\" \"H\"\n## [127] \"H\"\n##   [1] \"4.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"  \n##   [1] \"4.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"  \n##   [1] \"5.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"  \n##   [1] \"6.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"6.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"  \n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"6.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"4.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n##   [1] \"5.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"  \n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"  \n##   [1] \"5.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"   \"H\"  \n##   [1] \"5.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"   \"H\"   \"H\"  \n##   [1] \"5\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n## [109] \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"M\" \"H\" \"M\" \"H\"\n## [127] \"H\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\"\n## [145] \"H\" \"H\" \"H\" \"M\"\n##   [1] \"5.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"   \"H\"   \"H\"   \"M\"   \"H\"  \n##   [1] \"5.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"  \n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"\nprint(mylabel)                                  # 레이블 출력\n##   [1] \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\"\n## [109] \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"M\" \"H\" \"M\" \"H\" \"H\"\n## [127] \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\"\n## [145] \"H\" \"H\" \"M\" \"H\" \"H\" \"H\"\nnewds &lt;- data.frame(iris$Petal.Length, mylabel) # 꽃잎의 길이와 레이블 결합\nhead(newds)                                     # 새로운 데이터셋 내용 출력\n##   iris.Petal.Length mylabel\n## 1               1.4       L\n## 2               1.4       L\n## 3               1.3       L\n## 4               1.5       L\n## 5               1.4       L\n## 6               1.7       M\n\n\nsum &lt;- 0\ni &lt;- 1\nwhile (i &lt;= 100) {\n    sum &lt;- sum + i      # sum에 i 값을 누적\n    i &lt;- i + 1          # i 값을 1 증가시킴\n    print(c(sum, i))\n}\n## [1] 1 2\n## [1] 3 3\n## [1] 6 4\n## [1] 10  5\n## [1] 15  6\n## [1] 21  7\n## [1] 28  8\n## [1] 36  9\n## [1] 45 10\n## [1] 55 11\n## [1] 66 12\n## [1] 78 13\n## [1] 91 14\n## [1] 105  15\n## [1] 120  16\n## [1] 136  17\n## [1] 153  18\n## [1] 171  19\n## [1] 190  20\n## [1] 210  21\n## [1] 231  22\n## [1] 253  23\n## [1] 276  24\n## [1] 300  25\n## [1] 325  26\n## [1] 351  27\n## [1] 378  28\n## [1] 406  29\n## [1] 435  30\n## [1] 465  31\n## [1] 496  32\n## [1] 528  33\n## [1] 561  34\n## [1] 595  35\n## [1] 630  36\n## [1] 666  37\n## [1] 703  38\n## [1] 741  39\n## [1] 780  40\n## [1] 820  41\n## [1] 861  42\n## [1] 903  43\n## [1] 946  44\n## [1] 990  45\n## [1] 1035   46\n## [1] 1081   47\n## [1] 1128   48\n## [1] 1176   49\n## [1] 1225   50\n## [1] 1275   51\n## [1] 1326   52\n## [1] 1378   53\n## [1] 1431   54\n## [1] 1485   55\n## [1] 1540   56\n## [1] 1596   57\n## [1] 1653   58\n## [1] 1711   59\n## [1] 1770   60\n## [1] 1830   61\n## [1] 1891   62\n## [1] 1953   63\n## [1] 2016   64\n## [1] 2080   65\n## [1] 2145   66\n## [1] 2211   67\n## [1] 2278   68\n## [1] 2346   69\n## [1] 2415   70\n## [1] 2485   71\n## [1] 2556   72\n## [1] 2628   73\n## [1] 2701   74\n## [1] 2775   75\n## [1] 2850   76\n## [1] 2926   77\n## [1] 3003   78\n## [1] 3081   79\n## [1] 3160   80\n## [1] 3240   81\n## [1] 3321   82\n## [1] 3403   83\n## [1] 3486   84\n## [1] 3570   85\n## [1] 3655   86\n## [1] 3741   87\n## [1] 3828   88\n## [1] 3916   89\n## [1] 4005   90\n## [1] 4095   91\n## [1] 4186   92\n## [1] 4278   93\n## [1] 4371   94\n## [1] 4465   95\n## [1] 4560   96\n## [1] 4656   97\n## [1] 4753   98\n## [1] 4851   99\n## [1] 4950  100\n## [1] 5050  101\nprint(sum)\n## [1] 5050\n\n#---------------------------------------#\n# 오류 없이 계속 실행됨\n# sum &lt;- 0\n# i &lt;- 1\n# while(i &gt;= 1) {\n#   sum &lt;- sum + i # sum에 i 값을 누적\n#   i &lt;- i + 1 # i 값을 1 증가시킴\n#   print(c(sum,i))\n# }\n# print(sum)\n#---------------------------------------#\n\n\nsum &lt;- 0\nfor (i in 1:10) {\n    sum &lt;- sum + i\n    print(c(sum, i))\n    if (i &gt;= 5)\n        break\n}\n## [1] 1 1\n## [1] 3 2\n## [1] 6 3\n## [1] 10  4\n## [1] 15  5\nsum\n## [1] 15\n\n\nsum &lt;- 0\nfor (i in 1:10) {\n    if (i %% 2 == 0)\n        next # %% = 나머지\n    sum &lt;- sum + i\n    print(c(sum, i))\n}\n## [1] 1 1\n## [1] 4 3\n## [1] 9 5\n## [1] 16  7\n## [1] 25  9\nsum\n## [1] 25\n\n\napply(iris[, 1:4], 1, mean) # row 방향으로 함수 적용\n##   [1] 2.550 2.375 2.350 2.350 2.550 2.850 2.425 2.525 2.225 2.400 2.700 2.500\n##  [13] 2.325 2.125 2.800 3.000 2.750 2.575 2.875 2.675 2.675 2.675 2.350 2.650\n##  [25] 2.575 2.450 2.600 2.600 2.550 2.425 2.425 2.675 2.725 2.825 2.425 2.400\n##  [37] 2.625 2.500 2.225 2.550 2.525 2.100 2.275 2.675 2.800 2.375 2.675 2.350\n##  [49] 2.675 2.475 4.075 3.900 4.100 3.275 3.850 3.575 3.975 2.900 3.850 3.300\n##  [61] 2.875 3.650 3.300 3.775 3.350 3.900 3.650 3.400 3.600 3.275 3.925 3.550\n##  [73] 3.800 3.700 3.725 3.850 3.950 4.100 3.725 3.200 3.200 3.150 3.400 3.850\n##  [85] 3.600 3.875 4.000 3.575 3.500 3.325 3.425 3.775 3.400 2.900 3.450 3.525\n##  [97] 3.525 3.675 2.925 3.475 4.525 3.875 4.525 4.150 4.375 4.825 3.400 4.575\n## [109] 4.200 4.850 4.200 4.075 4.350 3.800 4.025 4.300 4.200 5.100 4.875 3.675\n## [121] 4.525 3.825 4.800 3.925 4.450 4.550 3.900 3.950 4.225 4.400 4.550 5.025\n## [133] 4.250 3.925 3.925 4.775 4.425 4.200 3.900 4.375 4.450 4.350 3.875 4.550\n## [145] 4.550 4.300 3.925 4.175 4.325 3.950\napply(iris[, 1:4], 2, mean) # col 방향으로 함수 적용\n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n##     5.843333     3.057333     3.758000     1.199333\n\nresult &lt;- c()\nfor (i in 1:4) {\n    iris_col &lt;- iris[, i]\n    iris_col_mean_temp &lt;- mean(iris_col)\n    result &lt;- c(result, iris_col_mean_temp)\n}\nresult\n## [1] 5.843333 3.057333 3.758000 1.199333\n\n\nmymax &lt;- function(x, y) {\n    num.max &lt;- x\n    if (y &gt; x) {\n        num.max &lt;- y\n    }\n    return(num.max)\n}\n\n\nmymax(10, 15)\n## [1] 15\na &lt;- mymax(20, 15)\nb &lt;- mymax(31, 45)\nprint(a + b)\n## [1] 65\n\n\nmydiv &lt;- function(x, y = 2) {\n    result &lt;- x / y\n    return(result)\n}\n\nmydiv(x = 10, y = 3) # 매개변수 이름과 매개변수값을 쌍으로 입력\n## [1] 3.333333\nmydiv(10, 3) # 매개변수값만 입력\n## [1] 3.333333\nmydiv(10) # x에 대한 값만 입력(y 값이 생략됨)\n## [1] 5\n\n\nmyfunc &lt;- function(x, y) {\n    val.sum &lt;- x + y\n    val.mul &lt;- x * y\n    return(list(sum = val.sum, mul = val.mul))\n}\n\nresult &lt;- myfunc(5, 8)\nresult\n## $sum\n## [1] 13\n## \n## $mul\n## [1] 40\ns &lt;- result$sum # 5, 8의 합\nm &lt;- result$mul # 5, 8의 곱\ncat('5+8=', s, '\\n')\n## 5+8= 13\ncat('5*8=', m, '\\n')\n## 5*8= 40\n\n\ngetwd()\n## [1] \"C:/Users/Hyunsoo Kim/Desktop/senior_grade/blog/my-quarto-website\"\n# source(\"myfunc.R\") # myfunc.R 안에 있는 함수 실행\n\na &lt;- mydiv(20, 4) # 함수 호출\nb &lt;- mydiv(30, 4) # 함수 호출\na + b\n## [1] 12.5\nmydiv(mydiv(20, 2), 5) # 함수 호출\n## [1] 2\n\n\nscore &lt;- c(76, 84, 69, 50, 95, 60, 82, 71, 88, 84)\nwhich(score == 69) # 성적이 69인 학생은 몇 번째에 있나\n## [1] 3\nwhich(score &gt;= 85) # 성적이 85 이상인 학생은 몇 번째에 있나\n## [1] 5 9\n\nmax(score) # 최고 점수는 몇 점인가\n## [1] 95\nwhich.max(score) # 최고 점수는 몇 번째에 있나\n## [1] 5\nscore[which.max(score)] # 최고 점수는 몇 점인가\n## [1] 95\n\nmin(score) # 최저 점수는 몇 점인가\n## [1] 50\nwhich.min(score) # 최저 점수는 몇 번째에 있나\n## [1] 4\nscore[which.min(score)] # 최저 점수는 몇 점인가\n## [1] 50\n\n\nscore &lt;- c(76, 84, 69, 50, 95, 60, 82, 71, 88, 84)\nidx &lt;- which(score &lt;= 60) # 성적이 60 이하인 값들의 인덱스\nidx\n## [1] 4 6\nscore[idx]\n## [1] 50 60\nscore[idx] &lt;- 61 # 성적이 60 이하인 값들은 61점으로 성적 상향 조정\nscore # 상향 조정된 성적 확인\n##  [1] 76 84 69 61 95 61 82 71 88 84\n\nidx &lt;- which(score &gt;= 80) # 성적이 80 이상인 값들의 인덱스\nidx\n## [1]  2  5  7  9 10\nscore[idx]\n## [1] 84 95 82 88 84\nscore.high &lt;- score[idx] # 성적이 80 이상인 값들만 추출하여 저장\nscore.high # score.high의 내용 확인\n## [1] 84 95 82 88 84\n\n\niris\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 1            5.1         3.5          1.4         0.2     setosa\n## 2            4.9         3.0          1.4         0.2     setosa\n## 3            4.7         3.2          1.3         0.2     setosa\n## 4            4.6         3.1          1.5         0.2     setosa\n## 5            5.0         3.6          1.4         0.2     setosa\n## 6            5.4         3.9          1.7         0.4     setosa\n## 7            4.6         3.4          1.4         0.3     setosa\n## 8            5.0         3.4          1.5         0.2     setosa\n## 9            4.4         2.9          1.4         0.2     setosa\n## 10           4.9         3.1          1.5         0.1     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 12           4.8         3.4          1.6         0.2     setosa\n## 13           4.8         3.0          1.4         0.1     setosa\n## 14           4.3         3.0          1.1         0.1     setosa\n## 15           5.8         4.0          1.2         0.2     setosa\n## 16           5.7         4.4          1.5         0.4     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 23           4.6         3.6          1.0         0.2     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 25           4.8         3.4          1.9         0.2     setosa\n## 26           5.0         3.0          1.6         0.2     setosa\n## 27           5.0         3.4          1.6         0.4     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 30           4.7         3.2          1.6         0.2     setosa\n## 31           4.8         3.1          1.6         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 34           5.5         4.2          1.4         0.2     setosa\n## 35           4.9         3.1          1.5         0.2     setosa\n## 36           5.0         3.2          1.2         0.2     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 38           4.9         3.6          1.4         0.1     setosa\n## 39           4.4         3.0          1.3         0.2     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 41           5.0         3.5          1.3         0.3     setosa\n## 42           4.5         2.3          1.3         0.3     setosa\n## 43           4.4         3.2          1.3         0.2     setosa\n## 44           5.0         3.5          1.6         0.6     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\n## 46           4.8         3.0          1.4         0.3     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 48           4.6         3.2          1.4         0.2     setosa\n## 49           5.3         3.7          1.5         0.2     setosa\n## 50           5.0         3.3          1.4         0.2     setosa\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 107          4.9         2.5          4.5         1.7  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 114          5.7         2.5          5.0         2.0  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 120          6.0         2.2          5.0         1.5  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 122          5.6         2.8          4.9         2.0  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 127          6.2         2.8          4.8         1.8  virginica\n## 128          6.1         3.0          4.9         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\niris$Petal.Length\n##   [1] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 1.5 1.6 1.4 1.1 1.2 1.5 1.3 1.4\n##  [19] 1.7 1.5 1.7 1.5 1.0 1.7 1.9 1.6 1.6 1.5 1.4 1.6 1.6 1.5 1.5 1.4 1.5 1.2\n##  [37] 1.3 1.4 1.3 1.5 1.3 1.3 1.3 1.6 1.9 1.4 1.6 1.4 1.5 1.4 4.7 4.5 4.9 4.0\n##  [55] 4.6 4.5 4.7 3.3 4.6 3.9 3.5 4.2 4.0 4.7 3.6 4.4 4.5 4.1 4.5 3.9 4.8 4.0\n##  [73] 4.9 4.7 4.3 4.4 4.8 5.0 4.5 3.5 3.8 3.7 3.9 5.1 4.5 4.5 4.7 4.4 4.1 4.0\n##  [91] 4.4 4.6 4.0 3.3 4.2 4.2 4.2 4.3 3.0 4.1 6.0 5.1 5.9 5.6 5.8 6.6 4.5 6.3\n## [109] 5.8 6.1 5.1 5.3 5.5 5.0 5.1 5.3 5.5 6.7 6.9 5.0 5.7 4.9 6.7 4.9 5.7 6.0\n## [127] 4.8 4.9 5.6 5.8 6.1 6.4 5.6 5.1 5.6 6.1 5.6 5.5 4.8 5.4 5.6 5.1 5.1 5.9\n## [145] 5.7 5.2 5.0 5.2 5.4 5.1\niris$Petal.Length &gt; 5.0\n##   [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n##  [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [97] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n## [109]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n## [121]  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n## [133]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n## [145]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\nwhich(iris$Petal.Length &gt; 5.0)\n##  [1]  84 101 102 103 104 105 106 108 109 110 111 112 113 115 116 117 118 119 121\n## [20] 123 125 126 129 130 131 132 133 134 135 136 137 138 140 141 142 143 144 145\n## [39] 146 148 149 150\n\niris$Petal.Length[iris$Petal.Length &gt; 5.0]\n##  [1] 5.1 6.0 5.1 5.9 5.6 5.8 6.6 6.3 5.8 6.1 5.1 5.3 5.5 5.1 5.3 5.5 6.7 6.9 5.7\n## [20] 6.7 5.7 6.0 5.6 5.8 6.1 6.4 5.6 5.1 5.6 6.1 5.6 5.5 5.4 5.6 5.1 5.1 5.9 5.7\n## [39] 5.2 5.2 5.4 5.1\n\nidx &lt;- which(iris$Petal.Length &gt; 5.0) # 꽃잎의 길이가 5.0 이상인 값들의 인덱스\nidx\n##  [1]  84 101 102 103 104 105 106 108 109 110 111 112 113 115 116 117 118 119 121\n## [20] 123 125 126 129 130 131 132 133 134 135 136 137 138 140 141 142 143 144 145\n## [39] 146 148 149 150\niris.big &lt;- iris[idx, ] # 인덱스에 해당하는 값만 추출하여 저장\niris.big\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\n\n\n# 1~4열의 값 중 5보다 큰 값의 행과 열의 위치\nwhich(iris[, 1:4] &gt; 5.0)\n##   [1]   1   6  11  15  16  17  18  19  20  21  22  24  28  29  32  33  34  37\n##  [19]  40  45  47  49  51  52  53  54  55  56  57  59  60  62  63  64  65  66\n##  [37]  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84\n##  [55]  85  86  87  88  89  90  91  92  93  95  96  97  98  99 100 101 102 103\n##  [73] 104 105 106 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122\n##  [91] 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140\n## [109] 141 142 143 144 145 146 147 148 149 150 384 401 402 403 404 405 406 408\n## [127] 409 410 411 412 413 415 416 417 418 419 421 423 425 426 429 430 431 432\n## [145] 433 434 435 436 437 438 440 441 442 443 444 445 446 448 449 450\nwhich(iris[, 1:4] &gt; 5.0, arr.ind = TRUE) # arr.ind = TRUE : 조건에 맞는 인덱스까지 반환\n##        row col\n##   [1,]   1   1\n##   [2,]   6   1\n##   [3,]  11   1\n##   [4,]  15   1\n##   [5,]  16   1\n##   [6,]  17   1\n##   [7,]  18   1\n##   [8,]  19   1\n##   [9,]  20   1\n##  [10,]  21   1\n##  [11,]  22   1\n##  [12,]  24   1\n##  [13,]  28   1\n##  [14,]  29   1\n##  [15,]  32   1\n##  [16,]  33   1\n##  [17,]  34   1\n##  [18,]  37   1\n##  [19,]  40   1\n##  [20,]  45   1\n##  [21,]  47   1\n##  [22,]  49   1\n##  [23,]  51   1\n##  [24,]  52   1\n##  [25,]  53   1\n##  [26,]  54   1\n##  [27,]  55   1\n##  [28,]  56   1\n##  [29,]  57   1\n##  [30,]  59   1\n##  [31,]  60   1\n##  [32,]  62   1\n##  [33,]  63   1\n##  [34,]  64   1\n##  [35,]  65   1\n##  [36,]  66   1\n##  [37,]  67   1\n##  [38,]  68   1\n##  [39,]  69   1\n##  [40,]  70   1\n##  [41,]  71   1\n##  [42,]  72   1\n##  [43,]  73   1\n##  [44,]  74   1\n##  [45,]  75   1\n##  [46,]  76   1\n##  [47,]  77   1\n##  [48,]  78   1\n##  [49,]  79   1\n##  [50,]  80   1\n##  [51,]  81   1\n##  [52,]  82   1\n##  [53,]  83   1\n##  [54,]  84   1\n##  [55,]  85   1\n##  [56,]  86   1\n##  [57,]  87   1\n##  [58,]  88   1\n##  [59,]  89   1\n##  [60,]  90   1\n##  [61,]  91   1\n##  [62,]  92   1\n##  [63,]  93   1\n##  [64,]  95   1\n##  [65,]  96   1\n##  [66,]  97   1\n##  [67,]  98   1\n##  [68,]  99   1\n##  [69,] 100   1\n##  [70,] 101   1\n##  [71,] 102   1\n##  [72,] 103   1\n##  [73,] 104   1\n##  [74,] 105   1\n##  [75,] 106   1\n##  [76,] 108   1\n##  [77,] 109   1\n##  [78,] 110   1\n##  [79,] 111   1\n##  [80,] 112   1\n##  [81,] 113   1\n##  [82,] 114   1\n##  [83,] 115   1\n##  [84,] 116   1\n##  [85,] 117   1\n##  [86,] 118   1\n##  [87,] 119   1\n##  [88,] 120   1\n##  [89,] 121   1\n##  [90,] 122   1\n##  [91,] 123   1\n##  [92,] 124   1\n##  [93,] 125   1\n##  [94,] 126   1\n##  [95,] 127   1\n##  [96,] 128   1\n##  [97,] 129   1\n##  [98,] 130   1\n##  [99,] 131   1\n## [100,] 132   1\n## [101,] 133   1\n## [102,] 134   1\n## [103,] 135   1\n## [104,] 136   1\n## [105,] 137   1\n## [106,] 138   1\n## [107,] 139   1\n## [108,] 140   1\n## [109,] 141   1\n## [110,] 142   1\n## [111,] 143   1\n## [112,] 144   1\n## [113,] 145   1\n## [114,] 146   1\n## [115,] 147   1\n## [116,] 148   1\n## [117,] 149   1\n## [118,] 150   1\n## [119,]  84   3\n## [120,] 101   3\n## [121,] 102   3\n## [122,] 103   3\n## [123,] 104   3\n## [124,] 105   3\n## [125,] 106   3\n## [126,] 108   3\n## [127,] 109   3\n## [128,] 110   3\n## [129,] 111   3\n## [130,] 112   3\n## [131,] 113   3\n## [132,] 115   3\n## [133,] 116   3\n## [134,] 117   3\n## [135,] 118   3\n## [136,] 119   3\n## [137,] 121   3\n## [138,] 123   3\n## [139,] 125   3\n## [140,] 126   3\n## [141,] 129   3\n## [142,] 130   3\n## [143,] 131   3\n## [144,] 132   3\n## [145,] 133   3\n## [146,] 134   3\n## [147,] 135   3\n## [148,] 136   3\n## [149,] 137   3\n## [150,] 138   3\n## [151,] 140   3\n## [152,] 141   3\n## [153,] 142   3\n## [154,] 143   3\n## [155,] 144   3\n## [156,] 145   3\n## [157,] 146   3\n## [158,] 148   3\n## [159,] 149   3\n## [160,] 150   3\n\nidx &lt;- which(iris[, 1:4] &gt; 5.0, arr.ind = TRUE)\niris[idx[, 1], ]\n##       Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 1              5.1         3.5          1.4         0.2     setosa\n## 6              5.4         3.9          1.7         0.4     setosa\n## 11             5.4         3.7          1.5         0.2     setosa\n## 15             5.8         4.0          1.2         0.2     setosa\n## 16             5.7         4.4          1.5         0.4     setosa\n## 17             5.4         3.9          1.3         0.4     setosa\n## 18             5.1         3.5          1.4         0.3     setosa\n## 19             5.7         3.8          1.7         0.3     setosa\n## 20             5.1         3.8          1.5         0.3     setosa\n## 21             5.4         3.4          1.7         0.2     setosa\n## 22             5.1         3.7          1.5         0.4     setosa\n## 24             5.1         3.3          1.7         0.5     setosa\n## 28             5.2         3.5          1.5         0.2     setosa\n## 29             5.2         3.4          1.4         0.2     setosa\n## 32             5.4         3.4          1.5         0.4     setosa\n## 33             5.2         4.1          1.5         0.1     setosa\n## 34             5.5         4.2          1.4         0.2     setosa\n## 37             5.5         3.5          1.3         0.2     setosa\n## 40             5.1         3.4          1.5         0.2     setosa\n## 45             5.1         3.8          1.9         0.4     setosa\n## 47             5.1         3.8          1.6         0.2     setosa\n## 49             5.3         3.7          1.5         0.2     setosa\n## 51             7.0         3.2          4.7         1.4 versicolor\n## 52             6.4         3.2          4.5         1.5 versicolor\n## 53             6.9         3.1          4.9         1.5 versicolor\n## 54             5.5         2.3          4.0         1.3 versicolor\n## 55             6.5         2.8          4.6         1.5 versicolor\n## 56             5.7         2.8          4.5         1.3 versicolor\n## 57             6.3         3.3          4.7         1.6 versicolor\n## 59             6.6         2.9          4.6         1.3 versicolor\n## 60             5.2         2.7          3.9         1.4 versicolor\n## 62             5.9         3.0          4.2         1.5 versicolor\n## 63             6.0         2.2          4.0         1.0 versicolor\n## 64             6.1         2.9          4.7         1.4 versicolor\n## 65             5.6         2.9          3.6         1.3 versicolor\n## 66             6.7         3.1          4.4         1.4 versicolor\n## 67             5.6         3.0          4.5         1.5 versicolor\n## 68             5.8         2.7          4.1         1.0 versicolor\n## 69             6.2         2.2          4.5         1.5 versicolor\n## 70             5.6         2.5          3.9         1.1 versicolor\n## 71             5.9         3.2          4.8         1.8 versicolor\n## 72             6.1         2.8          4.0         1.3 versicolor\n## 73             6.3         2.5          4.9         1.5 versicolor\n## 74             6.1         2.8          4.7         1.2 versicolor\n## 75             6.4         2.9          4.3         1.3 versicolor\n## 76             6.6         3.0          4.4         1.4 versicolor\n## 77             6.8         2.8          4.8         1.4 versicolor\n## 78             6.7         3.0          5.0         1.7 versicolor\n## 79             6.0         2.9          4.5         1.5 versicolor\n## 80             5.7         2.6          3.5         1.0 versicolor\n## 81             5.5         2.4          3.8         1.1 versicolor\n## 82             5.5         2.4          3.7         1.0 versicolor\n## 83             5.8         2.7          3.9         1.2 versicolor\n## 84             6.0         2.7          5.1         1.6 versicolor\n## 85             5.4         3.0          4.5         1.5 versicolor\n## 86             6.0         3.4          4.5         1.6 versicolor\n## 87             6.7         3.1          4.7         1.5 versicolor\n## 88             6.3         2.3          4.4         1.3 versicolor\n## 89             5.6         3.0          4.1         1.3 versicolor\n## 90             5.5         2.5          4.0         1.3 versicolor\n## 91             5.5         2.6          4.4         1.2 versicolor\n## 92             6.1         3.0          4.6         1.4 versicolor\n## 93             5.8         2.6          4.0         1.2 versicolor\n## 95             5.6         2.7          4.2         1.3 versicolor\n## 96             5.7         3.0          4.2         1.2 versicolor\n## 97             5.7         2.9          4.2         1.3 versicolor\n## 98             6.2         2.9          4.3         1.3 versicolor\n## 99             5.1         2.5          3.0         1.1 versicolor\n## 100            5.7         2.8          4.1         1.3 versicolor\n## 101            6.3         3.3          6.0         2.5  virginica\n## 102            5.8         2.7          5.1         1.9  virginica\n## 103            7.1         3.0          5.9         2.1  virginica\n## 104            6.3         2.9          5.6         1.8  virginica\n## 105            6.5         3.0          5.8         2.2  virginica\n## 106            7.6         3.0          6.6         2.1  virginica\n## 108            7.3         2.9          6.3         1.8  virginica\n## 109            6.7         2.5          5.8         1.8  virginica\n## 110            7.2         3.6          6.1         2.5  virginica\n## 111            6.5         3.2          5.1         2.0  virginica\n## 112            6.4         2.7          5.3         1.9  virginica\n## 113            6.8         3.0          5.5         2.1  virginica\n## 114            5.7         2.5          5.0         2.0  virginica\n## 115            5.8         2.8          5.1         2.4  virginica\n## 116            6.4         3.2          5.3         2.3  virginica\n## 117            6.5         3.0          5.5         1.8  virginica\n## 118            7.7         3.8          6.7         2.2  virginica\n## 119            7.7         2.6          6.9         2.3  virginica\n## 120            6.0         2.2          5.0         1.5  virginica\n## 121            6.9         3.2          5.7         2.3  virginica\n## 122            5.6         2.8          4.9         2.0  virginica\n## 123            7.7         2.8          6.7         2.0  virginica\n## 124            6.3         2.7          4.9         1.8  virginica\n## 125            6.7         3.3          5.7         2.1  virginica\n## 126            7.2         3.2          6.0         1.8  virginica\n## 127            6.2         2.8          4.8         1.8  virginica\n## 128            6.1         3.0          4.9         1.8  virginica\n## 129            6.4         2.8          5.6         2.1  virginica\n## 130            7.2         3.0          5.8         1.6  virginica\n## 131            7.4         2.8          6.1         1.9  virginica\n## 132            7.9         3.8          6.4         2.0  virginica\n## 133            6.4         2.8          5.6         2.2  virginica\n## 134            6.3         2.8          5.1         1.5  virginica\n## 135            6.1         2.6          5.6         1.4  virginica\n## 136            7.7         3.0          6.1         2.3  virginica\n## 137            6.3         3.4          5.6         2.4  virginica\n## 138            6.4         3.1          5.5         1.8  virginica\n## 139            6.0         3.0          4.8         1.8  virginica\n## 140            6.9         3.1          5.4         2.1  virginica\n## 141            6.7         3.1          5.6         2.4  virginica\n## 142            6.9         3.1          5.1         2.3  virginica\n## 143            5.8         2.7          5.1         1.9  virginica\n## 144            6.8         3.2          5.9         2.3  virginica\n## 145            6.7         3.3          5.7         2.5  virginica\n## 146            6.7         3.0          5.2         2.3  virginica\n## 147            6.3         2.5          5.0         1.9  virginica\n## 148            6.5         3.0          5.2         2.0  virginica\n## 149            6.2         3.4          5.4         2.3  virginica\n## 150            5.9         3.0          5.1         1.8  virginica\n## 84.1           6.0         2.7          5.1         1.6 versicolor\n## 101.1          6.3         3.3          6.0         2.5  virginica\n## 102.1          5.8         2.7          5.1         1.9  virginica\n## 103.1          7.1         3.0          5.9         2.1  virginica\n## 104.1          6.3         2.9          5.6         1.8  virginica\n## 105.1          6.5         3.0          5.8         2.2  virginica\n## 106.1          7.6         3.0          6.6         2.1  virginica\n## 108.1          7.3         2.9          6.3         1.8  virginica\n## 109.1          6.7         2.5          5.8         1.8  virginica\n## 110.1          7.2         3.6          6.1         2.5  virginica\n## 111.1          6.5         3.2          5.1         2.0  virginica\n## 112.1          6.4         2.7          5.3         1.9  virginica\n## 113.1          6.8         3.0          5.5         2.1  virginica\n## 115.1          5.8         2.8          5.1         2.4  virginica\n## 116.1          6.4         3.2          5.3         2.3  virginica\n## 117.1          6.5         3.0          5.5         1.8  virginica\n## 118.1          7.7         3.8          6.7         2.2  virginica\n## 119.1          7.7         2.6          6.9         2.3  virginica\n## 121.1          6.9         3.2          5.7         2.3  virginica\n## 123.1          7.7         2.8          6.7         2.0  virginica\n## 125.1          6.7         3.3          5.7         2.1  virginica\n## 126.1          7.2         3.2          6.0         1.8  virginica\n## 129.1          6.4         2.8          5.6         2.1  virginica\n## 130.1          7.2         3.0          5.8         1.6  virginica\n## 131.1          7.4         2.8          6.1         1.9  virginica\n## 132.1          7.9         3.8          6.4         2.0  virginica\n## 133.1          6.4         2.8          5.6         2.2  virginica\n## 134.1          6.3         2.8          5.1         1.5  virginica\n## 135.1          6.1         2.6          5.6         1.4  virginica\n## 136.1          7.7         3.0          6.1         2.3  virginica\n## 137.1          6.3         3.4          5.6         2.4  virginica\n## 138.1          6.4         3.1          5.5         1.8  virginica\n## 140.1          6.9         3.1          5.4         2.1  virginica\n## 141.1          6.7         3.1          5.6         2.4  virginica\n## 142.1          6.9         3.1          5.1         2.3  virginica\n## 143.1          5.8         2.7          5.1         1.9  virginica\n## 144.1          6.8         3.2          5.9         2.3  virginica\n## 145.1          6.7         3.3          5.7         2.5  virginica\n## 146.1          6.7         3.0          5.2         2.3  virginica\n## 148.1          6.5         3.0          5.2         2.0  virginica\n## 149.1          6.2         3.4          5.4         2.3  virginica\n## 150.1          5.9         3.0          5.1         1.8  virginica\n\niris[, 1:4][idx]\n##   [1] 5.1 5.4 5.4 5.8 5.7 5.4 5.1 5.7 5.1 5.4 5.1 5.1 5.2 5.2 5.4 5.2 5.5 5.5\n##  [19] 5.1 5.1 5.1 5.3 7.0 6.4 6.9 5.5 6.5 5.7 6.3 6.6 5.2 5.9 6.0 6.1 5.6 6.7\n##  [37] 5.6 5.8 6.2 5.6 5.9 6.1 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7 5.5 5.5 5.8 6.0\n##  [55] 5.4 6.0 6.7 6.3 5.6 5.5 5.5 6.1 5.8 5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 7.1\n##  [73] 6.3 6.5 7.6 7.3 6.7 7.2 6.5 6.4 6.8 5.7 5.8 6.4 6.5 7.7 7.7 6.0 6.9 5.6\n##  [91] 7.7 6.3 6.7 7.2 6.2 6.1 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 6.3 6.4 6.0 6.9\n## [109] 6.7 6.9 5.8 6.8 6.7 6.7 6.3 6.5 6.2 5.9 5.1 6.0 5.1 5.9 5.6 5.8 6.6 6.3\n## [127] 5.8 6.1 5.1 5.3 5.5 5.1 5.3 5.5 6.7 6.9 5.7 6.7 5.7 6.0 5.6 5.8 6.1 6.4\n## [145] 5.6 5.1 5.6 6.1 5.6 5.5 5.4 5.6 5.1 5.1 5.9 5.7 5.2 5.2 5.4 5.1"
  },
  {
    "objectID": "R_Basic.html#장.-단일변수-자료의-탐색",
    "href": "R_Basic.html#장.-단일변수-자료의-탐색",
    "title": "R Basic",
    "section": "",
    "text": "favorite &lt;- c('WINTER', 'SUMMER', 'SPRING', 'SUMMER', 'SUMMER',\n              'FALL', 'FALL', 'SUMMER', 'SPRING', 'SPRING')\nfavorite # favorite의 내용 출력\n##  [1] \"WINTER\" \"SUMMER\" \"SPRING\" \"SUMMER\" \"SUMMER\" \"FALL\"   \"FALL\"   \"SUMMER\"\n##  [9] \"SPRING\" \"SPRING\"\ntable(favorite) # 도수분포표 계산\n## favorite\n##   FALL SPRING SUMMER WINTER \n##      2      3      4      1\nlength(favorite)\n## [1] 10\ntable(favorite) / length(favorite) # 비율 출력\n## favorite\n##   FALL SPRING SUMMER WINTER \n##    0.2    0.3    0.4    0.1\n\n\nds &lt;- table(favorite)\nds\n## favorite\n##   FALL SPRING SUMMER WINTER \n##      2      3      4      1\nbarplot(ds, main = 'favorite season')\n\n\n\n\n\nds &lt;- table(favorite)\nds\n## favorite\n##   FALL SPRING SUMMER WINTER \n##      2      3      4      1\npie(ds, main = 'favorite season')\n\n\n\n\n\nfavorite.color &lt;- c(2, 3, 2, 1, 1, 2, 2, 1, 3, 2, 1, 3, 2, 1, 2)\nds &lt;- table(favorite.color)\nds\n## favorite.color\n## 1 2 3 \n## 5 7 3\nbarplot(ds, main = 'favorite color')\n\n\n\ncolors &lt;- c('green', 'red', 'blue')\nnames(ds) &lt;- colors # 자료값 1, 2, 3을 green, red, blue로 변경\nds\n## green   red  blue \n##     5     7     3\nbarplot(ds, main = 'favorite color', col = colors) # 색 지정 막대그래프\n\n\n\nbarplot(ds, main = 'favorite color', col = c('green', 'red', 'blue'))\npie(ds, main = 'favorite color', col = colors) # 색 지정 원그래프\n\n\n\n\n\nweight &lt;- c(60, 62, 64, 65, 68, 69)\nweight.heavy &lt;- c(weight, 120)\nweight\n## [1] 60 62 64 65 68 69\nweight.heavy\n## [1]  60  62  64  65  68  69 120\n\nmean(weight) # 평균\n## [1] 64.66667\nmean(weight.heavy) # 평균\n## [1] 72.57143\n\nmedian(weight) # 중앙값\n## [1] 64.5\nmedian(weight.heavy) # 중앙값\n## [1] 65\n\nmean(weight, trim = 0.2) # 절사평균(상하위 20% 제외)\n## [1] 64.75\nmean(weight.heavy, trim = 0.2) # 절사평균(상하위 20% 제외)\n## [1] 65.6\n\n\nmydata &lt;- c(60, 62, 64, 65, 68, 69, 120)\nquantile(mydata)\n##    0%   25%   50%   75%  100% \n##  60.0  63.0  65.0  68.5 120.0\nquantile(mydata, (0:10) / 10) # 10% 단위로 구간을 나누어 계산\n##    0%   10%   20%   30%   40%   50%   60%   70%   80%   90%  100% \n##  60.0  61.2  62.4  63.6  64.4  65.0  66.8  68.2  68.8  89.4 120.0\nsummary(mydata) # 최소값, 중앙값, 평균값, 3분위 값, 최대값\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   60.00   63.00   65.00   72.57   68.50  120.00\n\nmydata &lt;- 0:1000\nquantile(mydata)\n##   0%  25%  50%  75% 100% \n##    0  250  500  750 1000\nquantile(mydata, (0:10) / 10)\n##   0%  10%  20%  30%  40%  50%  60%  70%  80%  90% 100% \n##    0  100  200  300  400  500  600  700  800  900 1000\nsummary(mydata)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##       0     250     500     500     750    1000\n?quantile\n## starting httpd help server ... done\n\n\nmydata &lt;- c(60, 62, 64, 65, 68, 69, 120)\nvar(mydata) # 분산\n## [1] 447.2857\nsd(mydata) # 표준편차\n## [1] 21.14913\nrange(mydata) # 값의 범위\n## [1]  60 120\ndiff(range(mydata)) # 최대값, 최소값의 차이\n## [1] 60\n\n\ndist &lt;- cars[, 2] # 자동차 제동거리\nhist(dist,                            # 자료(data)\n     main = \"Histogram for 제동거리\", # 제목\n     xlab = \"제동거리\",               # x축 레이블\n     ylab = \"빈도수\",                 # y축 레이블\n     border = \"blue\",                 # 막대 테두리색\n     col = rainbow(10),               # 막대 색\n     las = 2,                         # x축 글씨 방향(0~3)\n     breaks = seq(0, 120, 10))        # 막대 개수 조절\n\n\n\n\n\ndist &lt;- cars[,2] # 자동차 제동거리(단위: 피트(ft))\nboxplot(dist, main = \"자동차 제동거리\") # ★★★★★\n\n\n\n\n\nboxplot.stats(dist)\n## $stats\n## [1]  2 26 36 56 93\n## \n## $n\n## [1] 50\n## \n## $conf\n## [1] 29.29663 42.70337\n## \n## $out\n## [1] 120\nboxplot.stats(dist)$stats\n## [1]  2 26 36 56 93\nboxplot.stats(dist)$stats[4]\n## [1] 56\n\n\nboxplot(Petal.Length ~ Species, data = iris, main = \"품종별 꽃잎의 길이\")\n\n\n\n\npar(mfrow = c(1, 3)) # 1*3 가상화면 분할\n\nbarplot(\n    table(mtcars$carb),\n    main = \"Barplot of Carburetors\",\n    xlab = \"#of carburetors\",\n    ylab = \"frequency\",\n    col = \"blue\"\n)\n\nbarplot(\n    table(mtcars$cyl),\n    main = \"Barplot of Cylender\",\n    xlab = \"#of cylender\",\n    ylab = \"frequency\",\n    col = \"red\"\n)\n\nbarplot(\n    table(mtcars$gear),\n    main = \"Barplot of Grar\",\n    xlab = \"#of gears\",\n    ylab = \"frequency\",\n    col = \"green\"\n)\n\n\n\n\npar(mfrow = c(1, 1)) # 가상화면 분할 해제"
  },
  {
    "objectID": "R_Basic.html#장.-다중변수-자료의-탐색",
    "href": "R_Basic.html#장.-다중변수-자료의-탐색",
    "title": "R Basic",
    "section": "",
    "text": "wt &lt;- mtcars$wt                 # 중량 자료\nmpg &lt;- mtcars$mpg               # 연비 자료\nplot(wt, mpg,                   # 2개 변수(x축, y축)\n     main = \"중량-연비 그래프\", # 제목\n     xlab = \"중량\",             # x축 레이블\n     ylab = \"연비(MPG)\",        # y축 레이블\n     col = \"red\",               # point의 color\n     pch = 11)                  # point의 종류\n\n\n\n\n\nvars &lt;- c(\"mpg\", \"disp\", \"drat\", \"wt\") # 대상 변수(연비, 배기량, 후방차측 비율, 중량)\ntarget &lt;- mtcars[, vars]\nhead(target)\n##                    mpg disp drat    wt\n## Mazda RX4         21.0  160 3.90 2.620\n## Mazda RX4 Wag     21.0  160 3.90 2.875\n## Datsun 710        22.8  108 3.85 2.320\n## Hornet 4 Drive    21.4  258 3.08 3.215\n## Hornet Sportabout 18.7  360 3.15 3.440\n## Valiant           18.1  225 2.76 3.460\npairs(target, main = \"Multi Plots\")    # 대상 데이터\n\n\n\n\n\niris.2 &lt;- iris[, 3:4]              # 데이터 준비\npoint &lt;- as.numeric(iris$Species)  # 점의 모양\npoint                              # point 내용 출력\n##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n##  [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n##  [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3\n## [112] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n## [149] 3 3\ncolor &lt;- c(\"red\", \"green\", \"blue\") # 점의 컬러\nplot(iris.2,\n     main = \"Iris plot\",\n     pch = c(point),\n     col = color[point])\n\n\n\n\n\nbeers = c(5, 2, 9, 8, 3, 7, 3, 5, 3, 5) # 자료 입력\nbal &lt;- c(0.1, 0.03, 0.19, 0.12, 0.04, 0.0095, 0.07, 0.06, 0.02, 0.05)\ntbl &lt;- data.frame(beers, bal)           # 데이터프레임 생성\ntbl\n##    beers    bal\n## 1      5 0.1000\n## 2      2 0.0300\n## 3      9 0.1900\n## 4      8 0.1200\n## 5      3 0.0400\n## 6      7 0.0095\n## 7      3 0.0700\n## 8      5 0.0600\n## 9      3 0.0200\n## 10     5 0.0500\nplot(bal ~ beers, data = tbl)           # 산점도 plot(beers, bal)\nres &lt;- lm(bal ~ beers, data = tbl)      # 회귀식 도출\nabline(res)                             # 회귀선 그리기\n\n\n\ncor(beers, bal)                         # 상관계수 계산\n## [1] 0.6797025\n\n\ncor(iris[, 1:4]) # 4개 변수 간 상관성 분석\n##              Sepal.Length Sepal.Width Petal.Length Petal.Width\n## Sepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411\n## Sepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259\n## Petal.Length    0.8717538  -0.4284401    1.0000000   0.9628654\n## Petal.Width     0.8179411  -0.3661259    0.9628654   1.0000000\n\n\nmonth = 1:12 # 자료 입력\nlate = c(5, 8, 7, 9, 4, 6, 12, 13, 8, 6, 6, 4) # 자료 입력\nplot(month,                # x data\n     late,                 # y data\n     main = \"지각생 통계\", # 제목\n     type = \"l\",           # 그래프의 종류 선택(알파벳)\n     lty = 1,              # 선의 종류(line type) 선택\n     lwd = 1,              # 선의 굵기 선택\n     xlab = \"Month\",       # x축 레이블\n     ylab = \"Late cnt\")    # y축 레이블\n\n\n\n\n\nmonth = 1:12\nlate1 = c(5, 8, 7, 9, 4, 6, 12, 13, 8, 6, 6, 4)\nlate2 = c(4, 6, 5, 8, 7, 8, 10, 11, 6, 5, 7, 3)\nplot(month,                  # x data\n     late1,                  # y data\n     main = \"Late Students\",\n     type = \"b\",             # 그래프의 종류 선택(알파벳)\n     lty = 1,                # 선의 종류(line type) 선택\n     col = \"red\",            # 선의 색 선택\n     xlab = \"Month\",         # x축 레이블\n     ylab = \"Late cnt\",      # y축 레이블\n     ylim = c(1, 15))        # y축 값의 (하한, 상한)\n\nlines(month,                 # x data\n      late2,                 # y data\n      type = \"b\",            # 선의 종류(line type) 선택\n      col = \"blue\")          # 선의 색 선택\n\n\n\n\n\n## (1) 분석 대상 데이터셋 준비\n# install.packages(\"mlbench\")\nlibrary(mlbench)\ndata(\"BostonHousing\")\nmyds &lt;- BostonHousing[, c(\"crim\", \"rm\", \"dis\", \"tax\", \"medv\")]\n\n## (2) grp 변수 추가 ★★★★★\ngrp &lt;- c()\nfor (i in 1:nrow(myds)) {\n    # myds$medv 값에 따라 그룹 분류\n    if (myds$medv[i] &gt;= 25.0) {\n        grp[i] &lt;- \"H\"\n    } else if (myds$medv[i] &lt;= 17.0) {\n        grp[i] &lt;- \"L\"\n    } else {\n        grp[i] &lt;- \"M\"\n    }\n}\ngrp &lt;- factor(grp) # 문자 벡터를 팩터 타입으로 변경\ngrp &lt;- factor(grp, levels = c(\"H\", \"M\", \"L\")) # 레벨의 순서를 H, L, M -&gt; H, M, L\n\nmyds &lt;- data.frame(myds, grp) # myds에 grp 열 추가\n\n## (3) 데이터셋의 형태와 기본적인 내용 파악\nstr(myds)\n## 'data.frame':    506 obs. of  6 variables:\n##  $ crim: num  0.00632 0.02731 0.02729 0.03237 0.06905 ...\n##  $ rm  : num  6.58 6.42 7.18 7 7.15 ...\n##  $ dis : num  4.09 4.97 4.97 6.06 6.06 ...\n##  $ tax : num  296 242 242 222 222 222 311 311 311 311 ...\n##  $ medv: num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...\n##  $ grp : Factor w/ 3 levels \"H\",\"M\",\"L\": 2 2 1 1 1 1 2 1 3 2 ...\nhead(myds)\n##      crim    rm    dis tax medv grp\n## 1 0.00632 6.575 4.0900 296 24.0   M\n## 2 0.02731 6.421 4.9671 242 21.6   M\n## 3 0.02729 7.185 4.9671 242 34.7   H\n## 4 0.03237 6.998 6.0622 222 33.4   H\n## 5 0.06905 7.147 6.0622 222 36.2   H\n## 6 0.02985 6.430 6.0622 222 28.7   H\ntable(myds$grp) # 주택 가격 그룹별 분포\n## \n##   H   M   L \n## 132 247 127\n\n## (4) 히스토그램에 의한 관측값의 분포 확인\npar(mfrow = c(2, 3)) # 2*3 가상화면 분할\nfor (i in 1:5) {\n    hist(myds[, i], main = colnames(myds)[i], col = \"yellow\")\n}\npar(mfrow = c(1, 1)) # 2*3 가상화면 분할 해제\n\n\n\n\n## (5) 상자그림에 의한 관측값의 분포 확인\npar(mfrow = c(2, 3)) # 2*3 가상화면 분할\nfor (i in 1:5) {\n    boxplot(myds[, i], main = colnames(myds)[i])\n}\npar(mfrow = c(1, 1)) # 2*3 가상화면 분할 해제\n\n\n\n\n## (6) 그룹별 관측값 분포의 확인\nboxplot(myds$crim ~ myds$grp, main = \"1인당 범죄율\")\n\n\n\nboxplot(myds$rm ~ myds$grp, main = \"방의 개수\")\n\n\n\nboxplot(myds$dis ~ myds$grp, main = \"직업 센터까지의 거리\")\n\n\n\nboxplot(myds$tax ~ myds$grp, main = \"재산세율\")\n\n\n\n\n## (7) 다중 산점도를 통한 변수 간 상관 관계의 확인\npairs(myds[, -6]) # 6번째 열 제거(grp)\npairs(myds[, 1:5])\n\n\n\n\n## (8) 그룹 정보를 포함한 변수 간 상관 관계의 확인\npoint &lt;- as.integer(myds$grp) # 점의 모양 지정\ncolor &lt;- c(\"red\", \"green\", \"blue\") # 점의 색 지정\npairs(myds[, -6], pch = point, col = color[point])\n\n\n\n\n## (9) 변수 간 상관계수의 확인\ncor(myds[, -6])\n##            crim         rm        dis        tax       medv\n## crim  1.0000000 -0.2192467 -0.3796701  0.5827643 -0.3883046\n## rm   -0.2192467  1.0000000  0.2052462 -0.2920478  0.6953599\n## dis  -0.3796701  0.2052462  1.0000000 -0.5344316  0.2499287\n## tax   0.5827643 -0.2920478 -0.5344316  1.0000000 -0.4685359\n## medv -0.3883046  0.6953599  0.2499287 -0.4685359  1.0000000\ncor(myds[1:5])\n##            crim         rm        dis        tax       medv\n## crim  1.0000000 -0.2192467 -0.3796701  0.5827643 -0.3883046\n## rm   -0.2192467  1.0000000  0.2052462 -0.2920478  0.6953599\n## dis  -0.3796701  0.2052462  1.0000000 -0.5344316  0.2499287\n## tax   0.5827643 -0.2920478 -0.5344316  1.0000000 -0.4685359\n## medv -0.3883046  0.6953599  0.2499287 -0.4685359  1.0000000"
  },
  {
    "objectID": "R_Basic.html#장.-데이터-전처리",
    "href": "R_Basic.html#장.-데이터-전처리",
    "title": "R Basic",
    "section": "",
    "text": "z &lt;- c(1, 2, 3, NA, 5, NA, 8)   # 결측값이 포함된 벡터 z\nsum(z)                          # 정상 계산이 안 됨\n## [1] NA\nis.na(z)                        # NA 여부 확인\n## [1] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE\nsum(is.na(z))                   # NA의 개수 확인\n## [1] 2\nsum(z, na.rm = TRUE)            # NA를 제외하고 합계를 계산\n## [1] 19\n\n\nz1 &lt;- c(1, 2, 3, NA, 5, NA, 8)          # 결측값이 포함된 벡터 z1\nz2 &lt;- c(5, 8, 1, NA, 3, NA, 7)          # 결측값이 포함된 벡터 z2\nz1[is.na(z1)] &lt;- 0                      # NA를 0으로 치환\nz1\n## [1] 1 2 3 0 5 0 8\n\nz1[is.na(z1)] &lt;- mean(z1, na.rm = TRUE) # NA를 z1의 평균값으로 치환\nz1\n## [1] 1 2 3 0 5 0 8\n\nz3 &lt;- as.vector(na.omit(z2))            # NA를 제거하고 새로운 벡터 생성\nz3\n## [1] 5 8 1 3 7\n\n\n# NA를 포함하는 test 데이터 생성\nx &lt;- iris\nx[1, 2] &lt;- NA\nx[1, 3] &lt;- NA\nx[2, 3] &lt;- NA\nx[3, 4] &lt;- NA\nhead(x)\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1          NA           NA         0.2  setosa\n## 2          4.9         3.0           NA         0.2  setosa\n## 3          4.7         3.2          1.3          NA  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\n\n\n# for문을 이용한 방법 ★★★★★\nfor (i in 1:ncol(x)) {\n    this.na &lt;- is.na(x[, i])\n    cat(colnames(x)[i], \"\\t\", sum(this.na), \"\\n\")\n}\n## Sepal.Length      0 \n## Sepal.Width   1 \n## Petal.Length      2 \n## Petal.Width   1 \n## Species   0\n\n# apply를 이용한 방법\ncol_na &lt;- function(y) {\n    return(sum(is.na(y)))\n}\n\nna_count &lt;- apply(x, 2, FUN = col_na)\nna_count\n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n##            0            1            2            1            0\n\n\nrowSums(is.na(x))           # 행별 NA의 개수\n##   [1] 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n##  [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n##  [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n## [112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n## [149] 0 0\nsum(rowSums(is.na(x)) &gt; 0)  # NA가 포함된 행의 개수\n## [1] 3\n\nsum(is.na(x))               # 데이터셋 전체에서 NA 개수\n## [1] 4\n\n\nhead(x)\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1          NA           NA         0.2  setosa\n## 2          4.9         3.0           NA         0.2  setosa\n## 3          4.7         3.2          1.3          NA  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\nx[!complete.cases(x), ]     # NA가 포함된 행들 출력\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1          NA           NA         0.2  setosa\n## 2          4.9         3.0           NA         0.2  setosa\n## 3          4.7         3.2          1.3          NA  setosa\ny &lt;- x[complete.cases(x), ] # NA가 포함된 행들 제거\nhead(y)                     # 새로운 데이터셋 y의 내용 확인\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\n## 7          4.6         3.4          1.4         0.3  setosa\n## 8          5.0         3.4          1.5         0.2  setosa\n## 9          4.4         2.9          1.4         0.2  setosa\n\n\nst &lt;- data.frame(state.x77)\nboxplot(st$Income)\n\n\n\nboxplot.stats(st$Income)\n## $stats\n## [1] 3098 3983 4519 4815 5348\n## \n## $n\n## [1] 50\n## \n## $conf\n## [1] 4333.093 4704.907\n## \n## $out\n## [1] 6315\n# stats (각 변수의 최소값, 1사분위수, 2사분위수, 3사분위수, 최대값이 저장되어 있는 행렬)\n# n (각 그룹마다의 관측값 수를 저장한 벡터)\n# conf (중앙값의 95% 신뢰구간, median+-1.58*IQR/(n)^0.5)\n# out (이상치)\nboxplot.stats(st$Income)$out\n## [1] 6315\n\n\nout.val &lt;- boxplot.stats(st$Income)$out     # 특이값 추출\n\nst$Income %in% out.val\n##  [1] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [49] FALSE FALSE\nst$Income == out.val\n##  [1] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [49] FALSE FALSE\n\nst$Income[st$Income %in% out.val] &lt;- NA     # 특이값을 NA로 대체\nst$Income[st$Income == out.val] &lt;- NA\n\nhead(st)\n##            Population Income Illiteracy Life.Exp Murder HS.Grad Frost   Area\n## Alabama          3615   3624        2.1    69.05   15.1    41.3    20  50708\n## Alaska            365     NA        1.5    69.31   11.3    66.7   152 566432\n## Arizona          2212   4530        1.8    70.55    7.8    58.1    15 113417\n## Arkansas         2110   3378        1.9    70.66   10.1    39.9    65  51945\n## California      21198   5114        1.1    71.71   10.3    62.6    20 156361\n## Colorado         2541   4884        0.7    72.06    6.8    63.9   166 103766\nnewdata &lt;- st[complete.cases(st), ]         # NA가 포함된 행 제거 ★★★★★\nhead(newdata)\n##             Population Income Illiteracy Life.Exp Murder HS.Grad Frost   Area\n## Alabama           3615   3624        2.1    69.05   15.1    41.3    20  50708\n## Arizona           2212   4530        1.8    70.55    7.8    58.1    15 113417\n## Arkansas          2110   3378        1.9    70.66   10.1    39.9    65  51945\n## California       21198   5114        1.1    71.71   10.3    62.6    20 156361\n## Colorado          2541   4884        0.7    72.06    6.8    63.9   166 103766\n## Connecticut       3100   5348        1.1    72.48    3.1    56.0   139   4862\n\n\nv1 &lt;- c(1, 7, 6, 8, 4, 2, 3)\norder(v1)\n## [1] 1 6 7 5 3 2 4\n\nv1 &lt;- sort(v1) # 오름차순\nv1\n## [1] 1 2 3 4 6 7 8\nv1[order(v1)]\n## [1] 1 2 3 4 6 7 8\n\nv2 &lt;- sort(v1, decreasing = T) # 내림차순\nv2\n## [1] 8 7 6 4 3 2 1\n\n\nhead(iris)\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\norder(iris$Sepal.Length)\n##   [1]  14   9  39  43  42   4   7  23  48   3  30  12  13  25  31  46   2  10\n##  [19]  35  38  58 107   5   8  26  27  36  41  44  50  61  94   1  18  20  22\n##  [37]  24  40  45  47  99  28  29  33  60  49   6  11  17  21  32  85  34  37\n##  [55]  54  81  82  90  91  65  67  70  89  95 122  16  19  56  80  96  97 100\n##  [73] 114  15  68  83  93 102 115 143  62  71 150  63  79  84  86 120 139  64\n##  [91]  72  74  92 128 135  69  98 127 149  57  73  88 101 104 124 134 137 147\n## [109]  52  75 112 116 129 133 138  55 105 111 117 148  59  76  66  78  87 109\n## [127] 125 141 145 146  77 113 144  53 121 140 142  51 103 110 126 130 108 131\n## [145] 106 118 119 123 136 132\niris[order(iris$Sepal.Length), ]                    # 오름차순으로 정렬\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 14           4.3         3.0          1.1         0.1     setosa\n## 9            4.4         2.9          1.4         0.2     setosa\n## 39           4.4         3.0          1.3         0.2     setosa\n## 43           4.4         3.2          1.3         0.2     setosa\n## 42           4.5         2.3          1.3         0.3     setosa\n## 4            4.6         3.1          1.5         0.2     setosa\n## 7            4.6         3.4          1.4         0.3     setosa\n## 23           4.6         3.6          1.0         0.2     setosa\n## 48           4.6         3.2          1.4         0.2     setosa\n## 3            4.7         3.2          1.3         0.2     setosa\n## 30           4.7         3.2          1.6         0.2     setosa\n## 12           4.8         3.4          1.6         0.2     setosa\n## 13           4.8         3.0          1.4         0.1     setosa\n## 25           4.8         3.4          1.9         0.2     setosa\n## 31           4.8         3.1          1.6         0.2     setosa\n## 46           4.8         3.0          1.4         0.3     setosa\n## 2            4.9         3.0          1.4         0.2     setosa\n## 10           4.9         3.1          1.5         0.1     setosa\n## 35           4.9         3.1          1.5         0.2     setosa\n## 38           4.9         3.6          1.4         0.1     setosa\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 107          4.9         2.5          4.5         1.7  virginica\n## 5            5.0         3.6          1.4         0.2     setosa\n## 8            5.0         3.4          1.5         0.2     setosa\n## 26           5.0         3.0          1.6         0.2     setosa\n## 27           5.0         3.4          1.6         0.4     setosa\n## 36           5.0         3.2          1.2         0.2     setosa\n## 41           5.0         3.5          1.3         0.3     setosa\n## 44           5.0         3.5          1.6         0.6     setosa\n## 50           5.0         3.3          1.4         0.2     setosa\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 1            5.1         3.5          1.4         0.2     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 28           5.2         3.5          1.5         0.2     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 49           5.3         3.7          1.5         0.2     setosa\n## 6            5.4         3.9          1.7         0.4     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 34           5.5         4.2          1.4         0.2     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 122          5.6         2.8          4.9         2.0  virginica\n## 16           5.7         4.4          1.5         0.4     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 114          5.7         2.5          5.0         2.0  virginica\n## 15           5.8         4.0          1.2         0.2     setosa\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 102          5.8         2.7          5.1         1.9  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 150          5.9         3.0          5.1         1.8  virginica\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 120          6.0         2.2          5.0         1.5  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 128          6.1         3.0          4.9         1.8  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 127          6.2         2.8          4.8         1.8  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 112          6.4         2.7          5.3         1.9  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 105          6.5         3.0          5.8         2.2  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 109          6.7         2.5          5.8         1.8  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 113          6.8         3.0          5.5         2.1  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 121          6.9         3.2          5.7         2.3  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 103          7.1         3.0          5.9         2.1  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\niris[order(iris$Sepal.Length, decreasing = T), ]    # 내림차순으로 정렬\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 132          7.9         3.8          6.4         2.0  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 121          6.9         3.2          5.7         2.3  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 113          6.8         3.0          5.5         2.1  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 109          6.7         2.5          5.8         1.8  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 105          6.5         3.0          5.8         2.2  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 112          6.4         2.7          5.3         1.9  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 127          6.2         2.8          4.8         1.8  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 128          6.1         3.0          4.9         1.8  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 120          6.0         2.2          5.0         1.5  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 150          5.9         3.0          5.1         1.8  virginica\n## 15           5.8         4.0          1.2         0.2     setosa\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 102          5.8         2.7          5.1         1.9  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 16           5.7         4.4          1.5         0.4     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 114          5.7         2.5          5.0         2.0  virginica\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 122          5.6         2.8          4.9         2.0  virginica\n## 34           5.5         4.2          1.4         0.2     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 6            5.4         3.9          1.7         0.4     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 49           5.3         3.7          1.5         0.2     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 1            5.1         3.5          1.4         0.2     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 5            5.0         3.6          1.4         0.2     setosa\n## 8            5.0         3.4          1.5         0.2     setosa\n## 26           5.0         3.0          1.6         0.2     setosa\n## 27           5.0         3.4          1.6         0.4     setosa\n## 36           5.0         3.2          1.2         0.2     setosa\n## 41           5.0         3.5          1.3         0.3     setosa\n## 44           5.0         3.5          1.6         0.6     setosa\n## 50           5.0         3.3          1.4         0.2     setosa\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 2            4.9         3.0          1.4         0.2     setosa\n## 10           4.9         3.1          1.5         0.1     setosa\n## 35           4.9         3.1          1.5         0.2     setosa\n## 38           4.9         3.6          1.4         0.1     setosa\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 107          4.9         2.5          4.5         1.7  virginica\n## 12           4.8         3.4          1.6         0.2     setosa\n## 13           4.8         3.0          1.4         0.1     setosa\n## 25           4.8         3.4          1.9         0.2     setosa\n## 31           4.8         3.1          1.6         0.2     setosa\n## 46           4.8         3.0          1.4         0.3     setosa\n## 3            4.7         3.2          1.3         0.2     setosa\n## 30           4.7         3.2          1.6         0.2     setosa\n## 4            4.6         3.1          1.5         0.2     setosa\n## 7            4.6         3.4          1.4         0.3     setosa\n## 23           4.6         3.6          1.0         0.2     setosa\n## 48           4.6         3.2          1.4         0.2     setosa\n## 42           4.5         2.3          1.3         0.3     setosa\n## 9            4.4         2.9          1.4         0.2     setosa\n## 39           4.4         3.0          1.3         0.2     setosa\n## 43           4.4         3.2          1.3         0.2     setosa\n## 14           4.3         3.0          1.1         0.1     setosa\n\niris.new &lt;- iris[order(iris$Sepal.Length), ]        # 정렬된 데이터를 저장\nhead(iris.new)\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 14          4.3         3.0          1.1         0.1  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 39          4.4         3.0          1.3         0.2  setosa\n## 43          4.4         3.2          1.3         0.2  setosa\n## 42          4.5         2.3          1.3         0.3  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\niris[order(iris$Species,-iris$Petal.Length, decreasing = T), ] # 정렬 기준이 2개\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 107          4.9         2.5          4.5         1.7  virginica\n## 127          6.2         2.8          4.8         1.8  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 122          5.6         2.8          4.9         2.0  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 128          6.1         3.0          4.9         1.8  virginica\n## 114          5.7         2.5          5.0         2.0  virginica\n## 120          6.0         2.2          5.0         1.5  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 101          6.3         3.3          6.0         2.5  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 23           4.6         3.6          1.0         0.2     setosa\n## 14           4.3         3.0          1.1         0.1     setosa\n## 15           5.8         4.0          1.2         0.2     setosa\n## 36           5.0         3.2          1.2         0.2     setosa\n## 3            4.7         3.2          1.3         0.2     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 39           4.4         3.0          1.3         0.2     setosa\n## 41           5.0         3.5          1.3         0.3     setosa\n## 42           4.5         2.3          1.3         0.3     setosa\n## 43           4.4         3.2          1.3         0.2     setosa\n## 1            5.1         3.5          1.4         0.2     setosa\n## 2            4.9         3.0          1.4         0.2     setosa\n## 5            5.0         3.6          1.4         0.2     setosa\n## 7            4.6         3.4          1.4         0.3     setosa\n## 9            4.4         2.9          1.4         0.2     setosa\n## 13           4.8         3.0          1.4         0.1     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 34           5.5         4.2          1.4         0.2     setosa\n## 38           4.9         3.6          1.4         0.1     setosa\n## 46           4.8         3.0          1.4         0.3     setosa\n## 48           4.6         3.2          1.4         0.2     setosa\n## 50           5.0         3.3          1.4         0.2     setosa\n## 4            4.6         3.1          1.5         0.2     setosa\n## 8            5.0         3.4          1.5         0.2     setosa\n## 10           4.9         3.1          1.5         0.1     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 16           5.7         4.4          1.5         0.4     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 35           4.9         3.1          1.5         0.2     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 49           5.3         3.7          1.5         0.2     setosa\n## 12           4.8         3.4          1.6         0.2     setosa\n## 26           5.0         3.0          1.6         0.2     setosa\n## 27           5.0         3.4          1.6         0.4     setosa\n## 30           4.7         3.2          1.6         0.2     setosa\n## 31           4.8         3.1          1.6         0.2     setosa\n## 44           5.0         3.5          1.6         0.6     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 6            5.4         3.9          1.7         0.4     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 25           4.8         3.4          1.9         0.2     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\niris[order(iris$Species, decreasing = T, iris$Petal.Length), ]\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 119          7.7         2.6          6.9         2.3  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 101          6.3         3.3          6.0         2.5  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\n## 114          5.7         2.5          5.0         2.0  virginica\n## 120          6.0         2.2          5.0         1.5  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 122          5.6         2.8          4.9         2.0  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 128          6.1         3.0          4.9         1.8  virginica\n## 127          6.2         2.8          4.8         1.8  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 107          4.9         2.5          4.5         1.7  virginica\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 25           4.8         3.4          1.9         0.2     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\n## 6            5.4         3.9          1.7         0.4     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 12           4.8         3.4          1.6         0.2     setosa\n## 26           5.0         3.0          1.6         0.2     setosa\n## 27           5.0         3.4          1.6         0.4     setosa\n## 30           4.7         3.2          1.6         0.2     setosa\n## 31           4.8         3.1          1.6         0.2     setosa\n## 44           5.0         3.5          1.6         0.6     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 4            4.6         3.1          1.5         0.2     setosa\n## 8            5.0         3.4          1.5         0.2     setosa\n## 10           4.9         3.1          1.5         0.1     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 16           5.7         4.4          1.5         0.4     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 35           4.9         3.1          1.5         0.2     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 49           5.3         3.7          1.5         0.2     setosa\n## 1            5.1         3.5          1.4         0.2     setosa\n## 2            4.9         3.0          1.4         0.2     setosa\n## 5            5.0         3.6          1.4         0.2     setosa\n## 7            4.6         3.4          1.4         0.3     setosa\n## 9            4.4         2.9          1.4         0.2     setosa\n## 13           4.8         3.0          1.4         0.1     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 34           5.5         4.2          1.4         0.2     setosa\n## 38           4.9         3.6          1.4         0.1     setosa\n## 46           4.8         3.0          1.4         0.3     setosa\n## 48           4.6         3.2          1.4         0.2     setosa\n## 50           5.0         3.3          1.4         0.2     setosa\n## 3            4.7         3.2          1.3         0.2     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 39           4.4         3.0          1.3         0.2     setosa\n## 41           5.0         3.5          1.3         0.3     setosa\n## 42           4.5         2.3          1.3         0.3     setosa\n## 43           4.4         3.2          1.3         0.2     setosa\n## 15           5.8         4.0          1.2         0.2     setosa\n## 36           5.0         3.2          1.2         0.2     setosa\n## 14           4.3         3.0          1.1         0.1     setosa\n## 23           4.6         3.6          1.0         0.2     setosa\n\n\nsp &lt;- split(iris, iris$Species) # 품종별로 데이터 분리\nsp                              # 분리 결과 확인\n## $setosa\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1           5.1         3.5          1.4         0.2  setosa\n## 2           4.9         3.0          1.4         0.2  setosa\n## 3           4.7         3.2          1.3         0.2  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\n## 5           5.0         3.6          1.4         0.2  setosa\n## 6           5.4         3.9          1.7         0.4  setosa\n## 7           4.6         3.4          1.4         0.3  setosa\n## 8           5.0         3.4          1.5         0.2  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\n## 11          5.4         3.7          1.5         0.2  setosa\n## 12          4.8         3.4          1.6         0.2  setosa\n## 13          4.8         3.0          1.4         0.1  setosa\n## 14          4.3         3.0          1.1         0.1  setosa\n## 15          5.8         4.0          1.2         0.2  setosa\n## 16          5.7         4.4          1.5         0.4  setosa\n## 17          5.4         3.9          1.3         0.4  setosa\n## 18          5.1         3.5          1.4         0.3  setosa\n## 19          5.7         3.8          1.7         0.3  setosa\n## 20          5.1         3.8          1.5         0.3  setosa\n## 21          5.4         3.4          1.7         0.2  setosa\n## 22          5.1         3.7          1.5         0.4  setosa\n## 23          4.6         3.6          1.0         0.2  setosa\n## 24          5.1         3.3          1.7         0.5  setosa\n## 25          4.8         3.4          1.9         0.2  setosa\n## 26          5.0         3.0          1.6         0.2  setosa\n## 27          5.0         3.4          1.6         0.4  setosa\n## 28          5.2         3.5          1.5         0.2  setosa\n## 29          5.2         3.4          1.4         0.2  setosa\n## 30          4.7         3.2          1.6         0.2  setosa\n## 31          4.8         3.1          1.6         0.2  setosa\n## 32          5.4         3.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\n## 35          4.9         3.1          1.5         0.2  setosa\n## 36          5.0         3.2          1.2         0.2  setosa\n## 37          5.5         3.5          1.3         0.2  setosa\n## 38          4.9         3.6          1.4         0.1  setosa\n## 39          4.4         3.0          1.3         0.2  setosa\n## 40          5.1         3.4          1.5         0.2  setosa\n## 41          5.0         3.5          1.3         0.3  setosa\n## 42          4.5         2.3          1.3         0.3  setosa\n## 43          4.4         3.2          1.3         0.2  setosa\n## 44          5.0         3.5          1.6         0.6  setosa\n## 45          5.1         3.8          1.9         0.4  setosa\n## 46          4.8         3.0          1.4         0.3  setosa\n## 47          5.1         3.8          1.6         0.2  setosa\n## 48          4.6         3.2          1.4         0.2  setosa\n## 49          5.3         3.7          1.5         0.2  setosa\n## 50          5.0         3.3          1.4         0.2  setosa\n## \n## $versicolor\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## \n## $virginica\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n## 101          6.3         3.3          6.0         2.5 virginica\n## 102          5.8         2.7          5.1         1.9 virginica\n## 103          7.1         3.0          5.9         2.1 virginica\n## 104          6.3         2.9          5.6         1.8 virginica\n## 105          6.5         3.0          5.8         2.2 virginica\n## 106          7.6         3.0          6.6         2.1 virginica\n## 107          4.9         2.5          4.5         1.7 virginica\n## 108          7.3         2.9          6.3         1.8 virginica\n## 109          6.7         2.5          5.8         1.8 virginica\n## 110          7.2         3.6          6.1         2.5 virginica\n## 111          6.5         3.2          5.1         2.0 virginica\n## 112          6.4         2.7          5.3         1.9 virginica\n## 113          6.8         3.0          5.5         2.1 virginica\n## 114          5.7         2.5          5.0         2.0 virginica\n## 115          5.8         2.8          5.1         2.4 virginica\n## 116          6.4         3.2          5.3         2.3 virginica\n## 117          6.5         3.0          5.5         1.8 virginica\n## 118          7.7         3.8          6.7         2.2 virginica\n## 119          7.7         2.6          6.9         2.3 virginica\n## 120          6.0         2.2          5.0         1.5 virginica\n## 121          6.9         3.2          5.7         2.3 virginica\n## 122          5.6         2.8          4.9         2.0 virginica\n## 123          7.7         2.8          6.7         2.0 virginica\n## 124          6.3         2.7          4.9         1.8 virginica\n## 125          6.7         3.3          5.7         2.1 virginica\n## 126          7.2         3.2          6.0         1.8 virginica\n## 127          6.2         2.8          4.8         1.8 virginica\n## 128          6.1         3.0          4.9         1.8 virginica\n## 129          6.4         2.8          5.6         2.1 virginica\n## 130          7.2         3.0          5.8         1.6 virginica\n## 131          7.4         2.8          6.1         1.9 virginica\n## 132          7.9         3.8          6.4         2.0 virginica\n## 133          6.4         2.8          5.6         2.2 virginica\n## 134          6.3         2.8          5.1         1.5 virginica\n## 135          6.1         2.6          5.6         1.4 virginica\n## 136          7.7         3.0          6.1         2.3 virginica\n## 137          6.3         3.4          5.6         2.4 virginica\n## 138          6.4         3.1          5.5         1.8 virginica\n## 139          6.0         3.0          4.8         1.8 virginica\n## 140          6.9         3.1          5.4         2.1 virginica\n## 141          6.7         3.1          5.6         2.4 virginica\n## 142          6.9         3.1          5.1         2.3 virginica\n## 143          5.8         2.7          5.1         1.9 virginica\n## 144          6.8         3.2          5.9         2.3 virginica\n## 145          6.7         3.3          5.7         2.5 virginica\n## 146          6.7         3.0          5.2         2.3 virginica\n## 147          6.3         2.5          5.0         1.9 virginica\n## 148          6.5         3.0          5.2         2.0 virginica\n## 149          6.2         3.4          5.4         2.3 virginica\n## 150          5.9         3.0          5.1         1.8 virginica\nsummary(sp)                     # 분리 결과 요약\n##            Length Class      Mode\n## setosa     5      data.frame list\n## versicolor 5      data.frame list\n## virginica  5      data.frame list\nsp$setosa                       # setosa 품종의 데이터 확인\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1           5.1         3.5          1.4         0.2  setosa\n## 2           4.9         3.0          1.4         0.2  setosa\n## 3           4.7         3.2          1.3         0.2  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\n## 5           5.0         3.6          1.4         0.2  setosa\n## 6           5.4         3.9          1.7         0.4  setosa\n## 7           4.6         3.4          1.4         0.3  setosa\n## 8           5.0         3.4          1.5         0.2  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\n## 11          5.4         3.7          1.5         0.2  setosa\n## 12          4.8         3.4          1.6         0.2  setosa\n## 13          4.8         3.0          1.4         0.1  setosa\n## 14          4.3         3.0          1.1         0.1  setosa\n## 15          5.8         4.0          1.2         0.2  setosa\n## 16          5.7         4.4          1.5         0.4  setosa\n## 17          5.4         3.9          1.3         0.4  setosa\n## 18          5.1         3.5          1.4         0.3  setosa\n## 19          5.7         3.8          1.7         0.3  setosa\n## 20          5.1         3.8          1.5         0.3  setosa\n## 21          5.4         3.4          1.7         0.2  setosa\n## 22          5.1         3.7          1.5         0.4  setosa\n## 23          4.6         3.6          1.0         0.2  setosa\n## 24          5.1         3.3          1.7         0.5  setosa\n## 25          4.8         3.4          1.9         0.2  setosa\n## 26          5.0         3.0          1.6         0.2  setosa\n## 27          5.0         3.4          1.6         0.4  setosa\n## 28          5.2         3.5          1.5         0.2  setosa\n## 29          5.2         3.4          1.4         0.2  setosa\n## 30          4.7         3.2          1.6         0.2  setosa\n## 31          4.8         3.1          1.6         0.2  setosa\n## 32          5.4         3.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\n## 35          4.9         3.1          1.5         0.2  setosa\n## 36          5.0         3.2          1.2         0.2  setosa\n## 37          5.5         3.5          1.3         0.2  setosa\n## 38          4.9         3.6          1.4         0.1  setosa\n## 39          4.4         3.0          1.3         0.2  setosa\n## 40          5.1         3.4          1.5         0.2  setosa\n## 41          5.0         3.5          1.3         0.3  setosa\n## 42          4.5         2.3          1.3         0.3  setosa\n## 43          4.4         3.2          1.3         0.2  setosa\n## 44          5.0         3.5          1.6         0.6  setosa\n## 45          5.1         3.8          1.9         0.4  setosa\n## 46          4.8         3.0          1.4         0.3  setosa\n## 47          5.1         3.8          1.6         0.2  setosa\n## 48          4.6         3.2          1.4         0.2  setosa\n## 49          5.3         3.7          1.5         0.2  setosa\n## 50          5.0         3.3          1.4         0.2  setosa\nsetosa &lt;- sp$setosa\n\n\nsubset(iris, Species == \"setosa\")\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1           5.1         3.5          1.4         0.2  setosa\n## 2           4.9         3.0          1.4         0.2  setosa\n## 3           4.7         3.2          1.3         0.2  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\n## 5           5.0         3.6          1.4         0.2  setosa\n## 6           5.4         3.9          1.7         0.4  setosa\n## 7           4.6         3.4          1.4         0.3  setosa\n## 8           5.0         3.4          1.5         0.2  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\n## 11          5.4         3.7          1.5         0.2  setosa\n## 12          4.8         3.4          1.6         0.2  setosa\n## 13          4.8         3.0          1.4         0.1  setosa\n## 14          4.3         3.0          1.1         0.1  setosa\n## 15          5.8         4.0          1.2         0.2  setosa\n## 16          5.7         4.4          1.5         0.4  setosa\n## 17          5.4         3.9          1.3         0.4  setosa\n## 18          5.1         3.5          1.4         0.3  setosa\n## 19          5.7         3.8          1.7         0.3  setosa\n## 20          5.1         3.8          1.5         0.3  setosa\n## 21          5.4         3.4          1.7         0.2  setosa\n## 22          5.1         3.7          1.5         0.4  setosa\n## 23          4.6         3.6          1.0         0.2  setosa\n## 24          5.1         3.3          1.7         0.5  setosa\n## 25          4.8         3.4          1.9         0.2  setosa\n## 26          5.0         3.0          1.6         0.2  setosa\n## 27          5.0         3.4          1.6         0.4  setosa\n## 28          5.2         3.5          1.5         0.2  setosa\n## 29          5.2         3.4          1.4         0.2  setosa\n## 30          4.7         3.2          1.6         0.2  setosa\n## 31          4.8         3.1          1.6         0.2  setosa\n## 32          5.4         3.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\n## 35          4.9         3.1          1.5         0.2  setosa\n## 36          5.0         3.2          1.2         0.2  setosa\n## 37          5.5         3.5          1.3         0.2  setosa\n## 38          4.9         3.6          1.4         0.1  setosa\n## 39          4.4         3.0          1.3         0.2  setosa\n## 40          5.1         3.4          1.5         0.2  setosa\n## 41          5.0         3.5          1.3         0.3  setosa\n## 42          4.5         2.3          1.3         0.3  setosa\n## 43          4.4         3.2          1.3         0.2  setosa\n## 44          5.0         3.5          1.6         0.6  setosa\n## 45          5.1         3.8          1.9         0.4  setosa\n## 46          4.8         3.0          1.4         0.3  setosa\n## 47          5.1         3.8          1.6         0.2  setosa\n## 48          4.6         3.2          1.4         0.2  setosa\n## 49          5.3         3.7          1.5         0.2  setosa\n## 50          5.0         3.3          1.4         0.2  setosa\nsubset(iris, Sepal.Length &gt; 7.5)\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n## 106          7.6         3.0          6.6         2.1 virginica\n## 118          7.7         3.8          6.7         2.2 virginica\n## 119          7.7         2.6          6.9         2.3 virginica\n## 123          7.7         2.8          6.7         2.0 virginica\n## 132          7.9         3.8          6.4         2.0 virginica\n## 136          7.7         3.0          6.1         2.3 virginica\nsubset(iris, Sepal.Length &gt; 5.1 &\n           Sepal.Width &gt; 3.9)\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 15          5.8         4.0          1.2         0.2  setosa\n## 16          5.7         4.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\nsubset(iris, Sepal.Length &gt; 5.1 |\n           Sepal.Width &gt; 3.9)\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 6            5.4         3.9          1.7         0.4     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 15           5.8         4.0          1.2         0.2     setosa\n## 16           5.7         4.4          1.5         0.4     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 34           5.5         4.2          1.4         0.2     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 49           5.3         3.7          1.5         0.2     setosa\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 114          5.7         2.5          5.0         2.0  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 120          6.0         2.2          5.0         1.5  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 122          5.6         2.8          4.9         2.0  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 127          6.2         2.8          4.8         1.8  virginica\n## 128          6.1         3.0          4.9         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\nsubset(iris, Sepal.Length &gt; 7.6,\n       select = c(Petal.Length, Petal.Width))\n##     Petal.Length Petal.Width\n## 118          6.7         2.2\n## 119          6.9         2.3\n## 123          6.7         2.0\n## 132          6.4         2.0\n## 136          6.1         2.3\n\n\nx &lt;- 1:10\nsample(x, size = 5, replace = FALSE) # 비복원추출\n## [1]  1  9  6  7 10\nsample(x, size = 5, replace = TRUE)\n## [1] 9 3 8 9 1\n\nx &lt;- 1:45\nsample(x, size = 6, replace = FALSE)\n## [1] 27 33 15 39 16  6\n\n\nidx &lt;- sample(1:nrow(iris), size = 50,\n              replace = FALSE)\niris.50 &lt;- iris[idx, ]  # 50개의 행 추출\ndim(iris.50)            # 행과 열의 개수 확인\n## [1] 50  5\nhead(iris.50)\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 17           5.4         3.9          1.3         0.4     setosa\n## 109          6.7         2.5          5.8         1.8  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n\n\nsample(1:20, size = 5)\n## [1]  9 14  2 16 15\nsample(1:20, size = 5)\n## [1]  7 16 15 14 13\nsample(1:20, size = 5)\n## [1]  1 15 20  9  5\n\n# 같은 값이 추출되도록 고정시키고 싶다면\n# set.seed() 함수를 이용하여 seed값을 지정해주면 된다.\nset.seed(100)\nsample(1:20, size = 5)\n## [1] 10  6 16 14 12\nset.seed(100)\nsample(1:20, size = 5)\n## [1] 10  6 16 14 12\nset.seed(100)\nsample(1:20, size = 5)\n## [1] 10  6 16 14 12\n\n\ncombn(1:5, 3) # 1~5에서 3개를 뽑는 조합\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n## [1,]    1    1    1    1    1    1    2    2    2     3\n## [2,]    2    2    2    3    3    4    3    3    4     4\n## [3,]    3    4    5    4    5    5    4    5    5     5\n\nx = c(\"red\", \"green\", \"blue\", \"black\", \"white\")\ncom &lt;- combn(x, 2) # x의 원소를 2개씩 뽑는 조합\ncom\n##      [,1]    [,2]   [,3]    [,4]    [,5]    [,6]    [,7]    [,8]    [,9]   \n## [1,] \"red\"   \"red\"  \"red\"   \"red\"   \"green\" \"green\" \"green\" \"blue\"  \"blue\" \n## [2,] \"green\" \"blue\" \"black\" \"white\" \"blue\"  \"black\" \"white\" \"black\" \"white\"\n##      [,10]  \n## [1,] \"black\"\n## [2,] \"white\"\n\nfor (i in 1:ncol(com)) {\n    # 조합을 출력\n    cat(com[, i], \"\\n\")\n}\n## red green \n## red blue \n## red black \n## red white \n## green blue \n## green black \n## green white \n## blue black \n## blue white \n## black white\n\n\n# aggregate(data, by = '기준이 되는 컬럼', FUN)\nagg &lt;- aggregate(iris[, -5], by = list(iris$Species), FUN = mean)\nagg\n##      Group.1 Sepal.Length Sepal.Width Petal.Length Petal.Width\n## 1     setosa        5.006       3.428        1.462       0.246\n## 2 versicolor        5.936       2.770        4.260       1.326\n## 3  virginica        6.588       2.974        5.552       2.026\n\n\n# aggregate는 데이터의 특정 컬럼을 기준으로 통계량을 구해주는 함수\nagg &lt;- aggregate(iris[, -5], by = list(표준편차 = iris$Species), FUN = sd)\nagg\n##     표준편차 Sepal.Length Sepal.Width Petal.Length Petal.Width\n## 1     setosa    0.3524897   0.3790644    0.1736640   0.1053856\n## 2 versicolor    0.5161711   0.3137983    0.4699110   0.1977527\n## 3  virginica    0.6358796   0.3224966    0.5518947   0.2746501\n\n\nhead(mtcars)\n##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\nagg &lt;- aggregate(mtcars, by = list(cyl = mtcars$cyl, vs = mtcars$vs), FUN = max)\nagg\n##   cyl vs  mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## 1   4  0 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n## 2   6  0 21.0   6 160.0 175 3.90 2.875 17.02  0  1    5    6\n## 3   8  0 19.2   8 472.0 335 4.22 5.424 18.00  0  1    5    8\n## 4   4  1 33.9   4 146.7 113 4.93 3.190 22.90  1  1    5    2\n## 5   6  1 21.4   6 258.0 123 3.92 3.460 20.22  1  0    4    4\n\nagg &lt;- aggregate(mtcars, by = list(cyl = mtcars$cyl, vs = mtcars$vs), FUN = mean)\nagg\n##   cyl vs      mpg cyl   disp       hp     drat       wt     qsec vs        am\n## 1   4  0 26.00000   4 120.30  91.0000 4.430000 2.140000 16.70000  0 1.0000000\n## 2   6  0 20.56667   6 155.00 131.6667 3.806667 2.755000 16.32667  0 1.0000000\n## 3   8  0 15.10000   8 353.10 209.2143 3.229286 3.999214 16.77214  0 0.1428571\n## 4   4  1 26.73000   4 103.62  81.8000 4.035000 2.300300 19.38100  1 0.7000000\n## 5   6  1 19.12500   6 204.55 115.2500 3.420000 3.388750 19.21500  1 0.0000000\n##       gear     carb\n## 1 5.000000 2.000000\n## 2 4.333333 4.666667\n## 3 3.285714 3.500000\n## 4 4.000000 1.500000\n## 5 3.500000 2.500000\n\n\nx &lt;- data.frame(name = c(\"a\", \"b\", \"c\"), math = c(90, 80, 40))\ny &lt;- data.frame(name = c(\"a\", \"b\", \"d\"), korean = c(75, 60, 90))\nx ; y\n##   name math\n## 1    a   90\n## 2    b   80\n## 3    c   40\n##   name korean\n## 1    a     75\n## 2    b     60\n## 3    d     90\n\n\nz &lt;- merge(x, y, by = c(\"name\"))\nz\n##   name math korean\n## 1    a   90     75\n## 2    b   80     60\n\n\nmerge(x, y, all.x = T)  # 첫 번째 데이터셋의 행들은 모두 표시되도록\n##   name math korean\n## 1    a   90     75\n## 2    b   80     60\n## 3    c   40     NA\nmerge(x, y, all.y = T)  # 두 번째 데이터셋의 행들은 모두 표시되도록\n##   name math korean\n## 1    a   90     75\n## 2    b   80     60\n## 3    d   NA     90\nmerge(x, y, all = T)    # 두 데이터셋의 모든 행들이 표시되도록\n##   name math korean\n## 1    a   90     75\n## 2    b   80     60\n## 3    c   40     NA\n## 4    d   NA     90\n\n\nx &lt;- data.frame(name = c(\"a\", \"b\", \"c\"), math = c(90, 80, 40))\ny &lt;- data.frame(sname = c(\"a\", \"b\", \"d\"), korean = c(75, 60, 90))\nx # 병합 기준 열의 이름이 name\n##   name math\n## 1    a   90\n## 2    b   80\n## 3    c   40\ny # 병합 기준 열의 이름이 sname\n##   sname korean\n## 1     a     75\n## 2     b     60\n## 3     d     90\nmerge(x, y, by.x = c(\"name\"), by.y = c(\"sname\"))\n##   name math korean\n## 1    a   90     75\n## 2    b   80     60"
  }
]