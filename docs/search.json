[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_1ìœ í˜•.html",
    "href": "BigData_Analysis/ì‹¤ê¸°_1ìœ í˜•.html",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 1ìœ í˜• ë¬¸ì œí’€ì´",
    "section": "",
    "text": "ì‹¤ê¸° 1 ìœ í˜•"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_1ìœ í˜•.html#ë°ì´í„°-ë‹¤ë£¨ê¸°-ìœ í˜•",
    "href": "BigData_Analysis/ì‹¤ê¸°_1ìœ í˜•.html#ë°ì´í„°-ë‹¤ë£¨ê¸°-ìœ í˜•",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 1ìœ í˜• ë¬¸ì œí’€ì´",
    "section": "âœ… ë°ì´í„° ë‹¤ë£¨ê¸° ìœ í˜•",
    "text": "âœ… ë°ì´í„° ë‹¤ë£¨ê¸° ìœ í˜•\n\n\në°ì´í„° íƒ€ì…(object, int, float, bool ë“±)\nê¸°ì´ˆí†µê³„ëŸ‰(í‰ê· , ì¤‘ì•™ê°’, ì‚¬ë¶„ìœ„ìˆ˜, IQR, í‘œì¤€í¸ì°¨ ë“±)\në°ì´í„° ì¸ë±ì‹±, í•„í„°ë§, ì •ë ¬, ë³€ê²½ ë“±\nì¤‘ë³µê°’, ê²°ì¸¡ì¹˜, ì´ìƒì¹˜ ì²˜ë¦¬ (ì œê±° or ëŒ€ì²´)\në°ì´í„° Scaling (ë°ì´í„° í‘œì¤€í™”(z), ë°ì´í„° ì •ê·œí™”(min-max))\në°ì´í„° í•©ì¹˜ê¸°\në‚ ì§œ/ì‹œê°„ ë°ì´í„°, index ë‹¤ë£¨ê¸°"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_1ìœ í˜•.html#í•µì‹¬ë¬¸ì œ-27ê°œ-110",
    "href": "BigData_Analysis/ì‹¤ê¸°_1ìœ í˜•.html#í•µì‹¬ë¬¸ì œ-27ê°œ-110",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 1ìœ í˜• ë¬¸ì œí’€ì´",
    "section": "âœ… í•µì‹¬ë¬¸ì œ 27ê°œ (1~10)",
    "text": "âœ… í•µì‹¬ë¬¸ì œ 27ê°œ (1~10)\n\n# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\nimport pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"mtcars.txt\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      Hornet Sportabout\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\n# ë¬¸ì œ 1\n# mpg ë³€ìˆ˜ì˜ ì œ 1ì‚¬ë¶„ìœ„ìˆ˜ë¥¼ êµ¬í•˜ê³  ì •ìˆ˜ê°’ìœ¼ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (í’€ì´)\nQ1 = df['mpg'].quantile(.25)\nprint(round(Q1))\n\n15\n\n\n\n# ë¬¸ì œ 2\n# mpg ê°’ì´ 19ì´ìƒ 21ì´í•˜ì¸ ë°ì´í„°ì˜ ìˆ˜ë¥¼ êµ¬í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (í’€ì´ 1)\ncond1 = (df[(df['mpg']>=19) & (df['mpg']<=21)])\nprint(len(cond1))\n\n# (í’€ì´ 2)\n# cond1 = (df['mpg']>=19)\n# cond2 = (df['mpg']<=21)\n# print(len(df[cond1&cond2]))\n\n5\n\n\n\n# ë¬¸ì œ 3\n# hp ë³€ìˆ˜ì˜ IQR ê°’ì„ êµ¬í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (í’€ì´)\nQ1 = df['hp'].quantile(.25)\nQ3 = df['hp'].quantile(.75)\nIQR = Q3 - Q1\nprint(IQR)\n\n83.5\n\n\n\n# ë¬¸ì œ 4 \n# wt ë³€ìˆ˜ì˜ ìƒìœ„ 10ê°œ ê°’ì˜ ì´í•©ì„ êµ¬í•˜ì—¬ ì†Œìˆ˜ì ì„ ë²„ë¦¬ê³  ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (í’€ì´)\ntop10 = df.sort_values('wt', ascending=False)\nsum_top10 = sum(top10['wt'].head(10))\nprint(int(sum_top10))\n\n# top_10 = df.sort_values('wt',ascending=False).head(10)\n# print(int(sum(top_10['wt']))) #ì£¼ì˜: ì†Œìˆ˜ì  ë°˜ì˜¬ë¦¼ì´ ì•„ë‹ˆë¼ ë²„ë¦¬ëŠ” ë¬¸ì œ\n\n42\n\n\n\n# ë¬¸ì œ 5\n# ì „ì²´ ìë™ì°¨ì—ì„œ cylê°€ 6ì¸ ë¹„ìœ¨ì´ ì–¼ë§ˆì¸ì§€ ì†Œìˆ˜ì  ì²«ì§¸ì§œë¦¬ê¹Œì§€ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\ncond1 = len(df[df['cyl']==6])\ncond2 = len(df['cyl'])\nprint(round(cond1/cond2, 1))\n\n0.2\n\n\n\n# ë¬¸ì œ 6\n# ì²«ë²ˆì§¸ í–‰ë¶€í„° ìˆœì„œëŒ€ë¡œ 10ê°œ ë½‘ì€ í›„ mpg ì—´ì˜ í‰ê· ê°’ì„ ë°˜ì˜¬ë¦¼í•˜ì—¬ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (í’€ì´)\ncond1 = df.head(10) # df[:10], df.loc[0:9]\nmean_mpg = cond1['mpg'].mean()\nprint(round(mean_mpg))\n\n20\n\n\n\n# ë¬¸ì œ 7\n# ì²«ë²ˆì§¸ í–‰ë¶€í„° ìˆœì„œëŒ€ë¡œ 50% ê¹Œì§€ ë°ì´í„°ë¥¼ ë½‘ì•„ wt ë³€ìˆ˜ì˜ ì¤‘ì•™ê°’ì„ êµ¬í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (í’€ì´)\n# 50% ë°ì´í„°ì— í•´ë‹¹í•˜ëŠ” í–‰ì˜ ìˆ˜ (ì •ìˆ˜ë¡œ êµ¬í•˜ê¸°)\np50 = int(len(df)*0.5)\ndf50 = df[:p50]\nprint(df50['wt'].median())\n\n# len(df)/2\n# cond1 = df[:17]\n# cond2 = cond1['wt'].median()\n# print(cond2)\n\n3.44\n\n\n\n# ë¬¸ì œ 8\n# ê²°ì¸¡ê°’ì´ ìˆëŠ” ë°ì´í„°ì˜ ìˆ˜ë¥¼ ì¶œë ¥í•˜ì‹œì˜¤\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ì œí’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3.0\n      300\n    \n    \n      1\n      20220105\n      B\n      NaN\n      400\n    \n    \n      2\n      None\n      None\n      5.0\n      500\n    \n    \n      3\n      20230127\n      B\n      10.0\n      600\n    \n    \n      4\n      20220203\n      A\n      10.0\n      400\n    \n  \n\n\n\n\n\n# (í’€ì´)\nm_value = df.isnull().sum()\nprint(m_value.sum())\n\n5\n\n\n\n# ë¬¸ì œ 9 \n# 'íŒë§¤ìˆ˜' ì»¬ëŸ¼ì˜ ê²°ì¸¡ê°’ì„ íŒë§¤ìˆ˜ì˜ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´í•˜ê³  íŒë§¤ìˆ˜ì˜ \n#  í‰ê· ê°’ì„ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\n\n\n# (í’€ì´)\ndf[df['íŒë§¤ìˆ˜'].isnull()]\nmedian = df['íŒë§¤ìˆ˜'].median()\ndf['íŒë§¤ìˆ˜'] = df['íŒë§¤ìˆ˜'].fillna(median)\nmean = df['íŒë§¤ìˆ˜'].mean()\nprint(int(mean))\n\n15\n\n\n\n# ë¬¸ì œ 10\n# íŒë§¤ìˆ˜ ì»¬ëŸ¼ì— ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ì„ ì œê±°í•˜ê³ \n# ì²«ë²ˆì§¸ í–‰ë¶€í„° ìˆœì„œëŒ€ë¡œ 50%ê¹Œì§€ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬\n# íŒë§¤ìˆ˜ ë³€ìˆ˜ì˜ Q1(ì œ1ì‚¬ë¶„ìœ„ìˆ˜) ê°’ì„ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\n\n\n# (í’€ì´)\n# ê²°ì¸¡ì¹˜ ì‚­ì œ(í–‰ ê¸°ì¤€)\ndf = df['íŒë§¤ìˆ˜'].dropna()\n# ì²«ë²ˆì§¸ í–‰ë¶€í„° ìˆœì„œëŒ€ë¡œ 50%ê¹Œì§€ì˜ ë°ì´í„° ì¶”ì¶œ\nper50 = int(len(df)*0.5)\ndf = df[:per50]\n# ê°’êµ¬í•˜ê¸°\nprint(round(df.quantile(.25)))\n\n# df[df['íŒë§¤ìˆ˜'].isnull()]\n# df['íŒë§¤ìˆ˜'] = df['íŒë§¤ìˆ˜'].dropna()\n# int(len(df['íŒë§¤ìˆ˜'])/2)\n# cond1 = df['íŒë§¤ìˆ˜'].head(6)\n# Q1 = cond1.quantile(.25)\n# print(int(Q1))\n\n5"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_1ìœ í˜•.html#í•µì‹¬ë¬¸ì œ-27ê°œ-1120",
    "href": "BigData_Analysis/ì‹¤ê¸°_1ìœ í˜•.html#í•µì‹¬ë¬¸ì œ-27ê°œ-1120",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 1ìœ í˜• ë¬¸ì œí’€ì´",
    "section": "âœ… í•µì‹¬ë¬¸ì œ 27ê°œ (11~20)",
    "text": "âœ… í•µì‹¬ë¬¸ì œ 27ê°œ (11~20)\n\ndf = pd.read_csv('mtcars.txt')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      Hornet Sportabout\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\n# ë¬¸ì œ 11\n# cylê°€ 4ì¸ ìë™ì°¨ì™€ 6ì¸ ìë™ì°¨ ê·¸ë£¹ì˜ mpg í‰ê· ê°’ì´ ì°¨ì´ë¥¼ ì ˆëŒ€ê°’ìœ¼ë¡œ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\ncyl_4 = df[df['cyl']==4]\ncyl_4_mean = cyl_4['mpg'].mean()\n\ncyl_6 = df[df['cyl']==6]\ncyl_6_mean = cyl_6['mpg'].mean()\n\nprint(round(abs(cyl_4_mean - cyl_6_mean)))\n\n7\n\n\n\n# ë¬¸ì œ 12\n# hp ë³€ìˆ˜ì— ëŒ€í•´ ë°ì´í„°í‘œì¤€í™”(Z-score)ë¥¼ ì§„í–‰í•˜ê³  ì´ìƒì¹˜ì˜ ìˆ˜ë¥¼ êµ¬í•˜ì‹œì˜¤\n# (ë‹¨, ì´ìƒì¹˜ëŠ” Zê°’ì´ 1.5ë¥¼ ì´ˆê³¼í•˜ê±°ë‚˜ -1.5ë¯¸ë§Œì¸ ê°’ì´ë‹¤)\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\n# Z = (X-í‰ê· ) / í‘œì¤€í¸ì°¨\nstd = df['hp'].std()\nmean_hp = df['hp'].mean()\ndf['zscore'] = (df['hp']-mean_hp)/std\n\ncond1 = (df['zscore']>1.5)\ncond2 = (df['zscore']<-1.5)\nprint(len(df[cond1] + df[cond2]))\n\n2\n\n\n\n# ë¬¸ì œ 13\n# mpg ì»¬ëŸ¼ì„ ìµœì†ŒìµœëŒ€ Scalingì„ ì§„í–‰í•œ í›„ 0.7ë³´ë‹¤ í° ê°’ì„ ê°€ì§€ëŠ” ë ˆì½”ë“œ ìˆ˜ë¥¼ êµ¬í•˜ë¼\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\nfrom sklearn.preprocessing import MinMaxScaler\nmscaler = MinMaxScaler()\ndf['mpg'] = mscaler.fit_transform(df[['mpg']])\nprint(len(df[df['mpg']>0.7]))\n\n# ê³µì‹ : (x-min) / (max-min)\n# min_mpg = df['mpg'].min()\n# max_mpg = df['mpg'].max()\n# df['mpg'] = (df['mpg'] - min_mpg)/(max_mpg - min_mpg)\n# cond1 = (df['mpg']>0.7)\n# print(len(df[cond1]))\n\n5\n\n\n\n# ë¬¸ì œ 14\n# wt ì»¬ëŸ¼ì— ëŒ€í•´ ìƒìê·¸ë¦¼ ê¸°ì¤€ìœ¼ë¡œ ì´ìƒì¹˜ì˜ ê°œìˆ˜ë¥¼ êµ¬í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\n# ì´ìƒì¹˜ : Q1, Q3 ë¡œë¶€í„° 1.5*IQRì„ ë„˜ì–´ê°€ëŠ” ê°’\nQ1 = df['wt'].quantile(.25)\nQ3 = df['wt'].quantile(.75)\nIQR = Q3-Q1\n\nupper = Q3 + (1.5*IQR)\nlower = Q1 - (1.5*IQR)\n\ncond1 = (df['wt'] > upper)\ncond2 = (df['wt'] < lower)\n\nprint(len(df[cond1] + df[cond2]))\n\n3\n\n\n\n# ë¬¸ì œ 15\n# íŒë§¤ìˆ˜ ì»¬ëŸ¼ì˜ ê²°ì¸¡ì¹˜ë¥¼ ìµœì†Œê°’ìœ¼ë¡œ ëŒ€ì²´í•˜ê³ \n# ê²°ì¸¡ì¹˜ê°€ ìˆì„ ë•Œì™€ ìµœì†Œê°’ìœ¼ë¡œ ëŒ€ì²´í–ˆì„ ë•Œ\n# í‰ê· ê°’ì˜ ì°¨ì´ë¥¼ ì ˆëŒ€ê°’ìœ¼ë¡œ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ì œí’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3.0\n      300\n    \n    \n      1\n      20220105\n      B\n      NaN\n      400\n    \n    \n      2\n      None\n      None\n      5.0\n      500\n    \n    \n      3\n      20230127\n      B\n      10.0\n      600\n    \n    \n      4\n      20220203\n      A\n      10.0\n      400\n    \n    \n      5\n      20220205\n      None\n      10.0\n      500\n    \n    \n      6\n      20230210\n      A\n      15.0\n      500\n    \n    \n      7\n      20230223\n      B\n      15.0\n      600\n    \n    \n      8\n      20230312\n      A\n      20.0\n      600\n    \n    \n      9\n      20230422\n      B\n      NaN\n      700\n    \n    \n      10\n      20220505\n      A\n      30.0\n      600\n    \n    \n      11\n      20230511\n      A\n      40.0\n      600\n    \n  \n\n\n\n\n\n# (í’€ì´)\n# ë°ì´í„° ë³µì‚¬\ndf2 = df.copy()\n# ìµœì†Œê°’ìœ¼ë¡œ ê²°ì¸¡ì¹˜ ëŒ€ì²´\nmin = df['íŒë§¤ìˆ˜'].min()\ndf2['íŒë§¤ìˆ˜'] = df2['íŒë§¤ìˆ˜'].fillna(min)\n\n# ê²°ì¸¡ì¹˜ê°€ ìˆì„ë•Œ, ëŒ€ì²´í–ˆì„ë•Œ í‰ê· \nm_yes = df['íŒë§¤ìˆ˜'].mean()\nm_no = df2['íŒë§¤ìˆ˜'].mean()\n\nprint(round(abs(m_yes - m_no)))\n\n2\n\n\n\n# ë¬¸ì œ 16\n# vsë³€ìˆ˜ê°€ 0ì´ ì•„ë‹Œ ì°¨ëŸ‰ ì¤‘ì— mpg ê°’ì´ ê°€ì¥ í° ì°¨ëŸ‰ì˜ hp ê°’ì„ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\ncond1 = df[df['vs']!=0]\n# mpg ê°’ì´ ê°€ì¥ í° ì°¨ëŸ‰(ë‚´ë¦¼ì°¨ìˆœ)\ncond1 = cond1.sort_values('mpg',ascending=False)\n# cond1\nprint(cond1['hp'].iloc[0])\n\n65\n\n\n\n# ë¬¸ì œ 17\n# gear ë³€ìˆ˜ê°’ì´ 3, 4ì¸ ë‘ ê·¸ë£¹ì˜ hp í‘œì¤€í¸ì°¨ê°’ì˜ ì°¨ì´ë¥¼ ì ˆëŒ€ê°’ìœ¼ë¡œ \n# ì†Œìˆ˜ì  ì²«ì§¸ìë¦¬ê¹Œì§€ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\n# ë‘ê°œ ê·¸ë£¹ìœ¼ë¡œ í•„í„°ë§\ncond1 = df[df['gear']==3]\ncond2 = df[df['gear']==4]\n# std êµ¬í•˜ê¸°\nstd3 = cond1['hp'].std()\nstd4 = cond2['hp'].std()\n\nprint(round(abs(std3 - std4),1))\n\n21.8\n\n\n\n# ë¬¸ì œ 18\n# gear ë³€ìˆ˜ì˜ ê°‘ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ mpg í‰ê· ê°’ì„ ì‚°ì¶œí•˜ê³ \n# í‰ê· ê°’ì´ ë†’ì€ ê·¸ë£¹ì˜ mpg ì œ3ì‚¬ë¶„ìœ„ìˆ˜ ê°’ì„ êµ¬í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\ndf['gear'].unique()\ngear3 = df[df['gear']==3]\ngear4 = df[df['gear']==4]\ngear5 = df[df['gear']==5]\n\nm_3 = gear3['mpg'].mean()\nm_4 = gear4['mpg'].mean()\nm_5 = gear5['mpg'].mean()\n\n(m_3, m_4, m_5) # m_4ì˜ í‰ê· ê°’ì´ ê°€ì¥ í¼\nQ3 = gear4['mpg'].quantile(.75)\nprint(Q3)\n\n28.075\n\n\n\n# (í’€ì´_v2)\n# gear, mpg ë³€ìˆ˜ë§Œ í•„í„°ë§\ndf = df.loc[:, ['gear', 'mpg']]\n\n# gear ë³€ìˆ˜ë¡œ ê·¸ë£¹í™”í•˜ì—¬ mpg í‰ê· ê°’ ë³´ê¸°\n# print(df.groupby('gear').mean()) # gear 4ê·¸ë£¹ì´ ì œì¼ ë†’ìŒ\n\n# gear=4ì¸ ê·¸ë£¹ì˜ mpg Q3 êµ¬í•˜ê¸°\ngear4 = df[df['gear']==4]\nprint(gear4['mpg'].quantile(.75))\n\n28.075\n\n\n\n# ë¬¸ì œ 19\n# hp í•­ëª©ì˜ ìƒìœ„ 7ë²ˆì§¸ ê°’ìœ¼ë¡œ ìƒìœ„ 7ê°œ ê°’ì„ ë³€í™˜í•œ í›„,\n# hpê°€ 150 ì´ìƒì¸ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ hpì˜ í‰ê· ê°’ì„ ë°˜ì˜¬ë¦¼í•˜ì—¬ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\n# hp ì—´ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬\ndf = df.sort_values('hp', ascending=False)\n\n# ì¸ë±ìŠ¤ ì´ˆê¸°í™” - ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬ì‹œ ìµœì´ˆì˜ ì¸ë±ìŠ¤ë¡œ ìˆê¸°ì—\ndf = df.reset_index(drop=True) # drop=True ëŠ” ê¸°ì¡´ index ì‚­ì œ\n# print(df)\n\n# hp ìƒìœ„ 7ë²ˆì§¸ ê°’ í™•ì¸\ntop7 = df['hp'].loc[6] # top7 = 205\n\n# np.where í™œìš©\nimport numpy as np\ndf['hp'] = np.where(df['hp'] >= 205, top7, df['hp'])\n# np.where(ì¡°ê±´, ì¡°ê±´ì— í•´ë‹¹í•  ë•Œ ê°’, ê·¸ë ‡ì§€ ì•Šì„ ë•Œ ê°’)\n\n# hp 150ì´ìƒì¸ ë°ì´í„°\ncond1 = (df['hp']>=150)\ndf = df[cond1]\nprint(round(df['hp'].mean()))\n\n187\n\n\n\n# ë¬¸ì œ 20\n# car ë³€ìˆ˜ì— Merc ë¬´êµ¬ê°€ í¬í•¨ëœ ìë™ì°¨ì˜ mpg í‰ê· ê°’ì„ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\ndf2 = df[df['car'].str.contains('Merc')] # ë¬¸ìì—´ ì¶”ì¶œ ì•Œì•„ë‘ê¸°\nprint(round(df2['mpg'].mean()))\n\n# ì‹œí—˜í™˜ê²½ì—ì„œ ë‹µêµ¬í•˜ëŠ” ë°©ë²•(reset_index() ì‚¬ìš© í›„)\n# ì‹œí—˜ì—ì„œëŠ” carê°€ indexë¡œ ë˜ì–´ ìˆìŒ\n# df.reset_index(inplace=True)\n# df2 = df[df['index'].str.contains(\"Merc\")] \n# print(round(df2['mpg'].mean()))\n\n19"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_1ìœ í˜•.html#í•µì‹¬ë¬¸ì œ-27ê°œ-2127",
    "href": "BigData_Analysis/ì‹¤ê¸°_1ìœ í˜•.html#í•µì‹¬ë¬¸ì œ-27ê°œ-2127",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 1ìœ í˜• ë¬¸ì œí’€ì´",
    "section": "âœ… í•µì‹¬ë¬¸ì œ 27ê°œ (21~27)",
    "text": "âœ… í•µì‹¬ë¬¸ì œ 27ê°œ (21~27)\n\n# ë¬¸ì œ 21\n# 22ë…„ 1ë¶„ê¸° Aì œí’ˆì˜ ë§¤ì¶œì•¡ì„ êµ¬í•˜ì‹œì˜¤\n# (ë§¤ì¶œì•¡ = íŒë§¤ìˆ˜*ê°œë‹¹ìˆ˜ìµ)\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ì œí’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# # (í’€ì´)\n# cond1 = df[df['ë‚ ì§œ']<='20220431']\n# cond2 = cond1[cond1['ì œí’ˆ']=='A']\n# cond2['ë§¤ì¶œì•¡'] = (cond2['íŒë§¤ìˆ˜']*cond2['ê°œë‹¹ìˆ˜ìµ'])\n# cond2\n\n# print(cond2['ë§¤ì¶œì•¡'].sum())\n\n\n# (í’€ì´_v2)\n# ë°ì´í„° íƒ€ì… datetimeìœ¼ë¡œ ë³€ê²½\ndf['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])\n\n# ë…„, ì›”, ì¼ ë³€ìˆ˜(ì—´) ì¶”ê°€í•˜ê¸°\ndf['year'] = df['ë‚ ì§œ'].dt.year\ndf['month'] = df['ë‚ ì§œ'].dt.month\ndf['day'] = df['ë‚ ì§œ'].dt.day\n\n# ë§¤ì¶œì•¡ ë³€ìˆ˜ ì¶”ê°€í•˜ê¸°\ndf['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\n# 22ë…„ìœ¼ë¡œ í•„í„°ë§\ndf = df[df['year']==2022]\ndf.head()\n\n# 1,2,3ì›” ë§¤ì¶œì•¡ ê³„ì‚°\nm1 = df[df['month']==1]['ë§¤ì¶œì•¡'].sum()\nm2 = df[df['month']==2]['ë§¤ì¶œì•¡'].sum()\nm3 = df[df['month']==3]['ë§¤ì¶œì•¡'].sum()\nprint(m1+m2+m3)\n\n11900\n\n\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['year'] = df['ë‚ ì§œ'].dt.year\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['month'] = df['ë‚ ì§œ'].dt.month\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['day'] = df['ë‚ ì§œ'].dt.day\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\n\n\n# ë¬¸ì œ 22\n# 22ë…„ê³¼ 23ë…„ì˜ ì´ ë§¤ì¶œì•¡ ì°¨ì´ë¥¼ ì ˆëŒ€ê°’ìœ¼ë¡œ êµ¬í•˜ì‹œì˜¤\n# (ë§¤ì¶œì•¡ = íŒë§¤ìˆ˜*ê°œë‹¹ìˆ˜ìµ)\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ì œí’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# (í’€ì´)\n# ë°ì´í„° íƒ€ì… datetimeìœ¼ë¡œ ë³€ê²½\ndf['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])\n\n# ë…„ ë³€ìˆ˜(ì—´) ì¶”ê°€í•˜ê¸°\ndf['year'] = df['ë‚ ì§œ'].dt.year\n\n# ë§¤ì¶œì•¡ ë³€ìˆ˜ ì¶”ê°€í•˜ê¸°\ndf['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\ncond22 = df[df['year']==2022]\ncond23 = df[df['year']==2023]\n\nprint(abs((cond22['ë§¤ì¶œì•¡'].sum()) - (cond23['ë§¤ì¶œì•¡'].sum())))\n\n48600\n\n\n\n# ë¬¸ì œ 23\n# 23ë…„ ì´ ë§¤ì¶œì•¡ì´ í° ì œí’ˆì˜ 23ë…„ íŒë§¤ìˆ˜ë¥¼ êµ¬í•˜ì‹œì˜¤\n# (ë§¤ì¶œì•¡ = íŒë§¤ìˆ˜*ê°œë‹¹ìˆ˜ìµ)\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ì œí’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# ë°ì´í„° íƒ€ì… datetimeìœ¼ë¡œ ë³€ê²½\ndf['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])\n\n# ë…„ ë³€ìˆ˜(ì—´) ì¶”ê°€í•˜ê¸°\ndf['year'] = df['ë‚ ì§œ'].dt.year\n\n# ë§¤ì¶œì•¡ ë³€ìˆ˜ ì¶”ê°€í•˜ê¸°\ndf['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\ndf = df[df['year']==2023]\n# df.head()\n\n# 23ë…„ A ë§¤ì¶œì•¡ê³¼ B ë§¤ì¶œì•¡ ë³„ë„ë¡œ êµ¬í•˜ê¸°\n# A ë§¤ì¶œì•¡\ndf_a = df[df['ì œí’ˆ']=='A']\na_sales = df_a['ë§¤ì¶œì•¡'].sum()\n# print(a_sales) # 46000\n\n# B ë§¤ì¶œì•¡\ndf_b = df[df['ì œí’ˆ']=='B']\nb_sales = df_b['ë§¤ì¶œì•¡'].sum()\n# print(b_sales) # 32500\n\n# A ì œí’ˆì˜ 23ë…„ íŒë§¤ìˆ˜\na_sum = df_a['íŒë§¤ìˆ˜'].sum()\nprint(a_sum)\n\n80\n\n\n\n# ë¬¸ì œ 24\n# ë§¤ì¶œì•¡ì´ 4ì²œì› ì´ˆê³¼, 1ë§Œì› ë¯¸ë§Œì¸ ë°ì´í„° ìˆ˜ë¥¼ ì¶œë ¥í•˜ì‹œì˜¤\n# (ë§¤ì¶œì•¡ = íŒë§¤ìˆ˜*ê°œë‹¹ìˆ˜ìµ)\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ì œí’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# (í’€ì´)\ndf['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\ncond1 = df['ë§¤ì¶œì•¡']>4000\ncond2 = df['ë§¤ì¶œì•¡']<10000\n\ndf = df[cond1&cond2]\nprint(len(df))\n\n4\n\n\n\n# ë¬¸ì œ 25\n# 23ë…„ 9ì›” 24ì¼ 16:00~22:00 ì‚¬ì´ì— ì „ì²´ ì œí’ˆì˜ íŒë§¤ìˆ˜ë¥¼ êµ¬í•˜ì‹œì˜¤\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë¬¼í’ˆ' : ['A','B','A','B','A','B','A'],\n    'íŒë§¤ìˆ˜' : [5,10,15,15,20,25,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [500,600,500,600,600,700,600]})\ntime = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf['time'] = time\ndf = df[['time', 'ë¬¼í’ˆ', 'íŒë§¤ìˆ˜', 'ê°œë‹¹ìˆ˜ìµ']]\ndf\n\n\n\n\n\n  \n    \n      \n      time\n      ë¬¼í’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      1\n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2\n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      3\n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      4\n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      5\n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      6\n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# (í’€ì´)\n# ì»¬ëŸ¼ê³¼ ì¸ë±ìŠ¤ì— ë‘˜ë‹¤ time ë³€ìˆ˜ë¥¼ ë†“ê³  í’€ì´\n# ë°ì´í„° íƒ€ì… datetimeìœ¼ë¡œ ë³€ê²½ (í•­ìƒ df.info()ë¡œ ë°ì´í„° íƒ€ì… í™•ì¸í•  ê²ƒ)\ndf['time'] = pd.to_datetime(df['time'])\n\n# index ìƒˆë¡œ ì§€ì •\ndf = df.set_index('time', drop=False) # defaultëŠ” True\n\n# 9ì›” 24ì¼ í•„í„°ë§ // df['ë³€ìˆ˜'].between( , )\ndf_after = df[df['time'].between('2023-09-24', '2023-09-25')] # 25ì¼ì€ ë¯¸í¬í•¨\n\n# ì‹œê°„ í•„í„°ë§ 16:00 ~ 22:00 (ì£¼ì˜: ì‹œê°„ì´ indexì— ìœ„ì¹˜í•´ì•¼ í•¨)\ndf = df_after.between_time(start_time='16:00', end_time='22:00') # í¬í•¨ ê¸°ì¤€\n\n# ì „ì²´ íŒë§¤ìˆ˜ êµ¬í•˜ê¸°\nprint(df['íŒë§¤ìˆ˜'].sum())\n\n25\n\n\n\n# (í’€ì´2) // ìœ„ì— df ìƒˆë¡œ ë¶ˆëŸ¬ì™€ì„œ ì‹¤í–‰í•´ë³´ê¸°\n# ë°ì´í„° íƒ€ì… datetimeìœ¼ë¡œ ë³€ê²½\ndf['time'] = pd.to_datetime(df['time'])\n\n# index ìƒˆë¡œ ì§€ì •\ndf = df.set_index('time')\n\n# loc í•¨ìˆ˜ë¡œ í•„í„°ë§\ndf = df.loc[(df.index >= '2023-09-24 16:00:00') & (df.index <= '2023-09-24 22:00:00')]\n\n# ì „ì²´ íŒë§¤ìˆ˜ êµ¬í•˜ê¸°\nprint(df['íŒë§¤ìˆ˜'].sum())\n\n25\n\n\n\n# ë¬¸ì œ 26\n# 9ì›” 25ì¼ 00:10~12:00ê¹Œì§€ì˜ Bë¬¼í’ˆì˜ ë§¤ì¶œì•¡ ì´í•©ì„ êµ¬í•˜ì‹œì˜¤\n# (ë§¤ì¶œì•¡ = íŒë§¤ìˆ˜*ê°œë‹¹ìˆ˜ìµ)\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë¬¼í’ˆ' : ['A','B','A','B','A','B','A'],\n    'íŒë§¤ìˆ˜' : [5,10,15,15,20,25,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [500,600,500,600,600,700,600]})\ndf['time'] = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf = df[['time', 'ë¬¼í’ˆ', 'íŒë§¤ìˆ˜', 'ê°œë‹¹ìˆ˜ìµ']]\ndf = df.set_index('time',drop=True)\ndf\n\n\n\n\n\n  \n    \n      \n      ë¬¼í’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n    \n      time\n      \n      \n      \n    \n  \n  \n    \n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# (í’€ì´)\n# ë§¤ì¶œì•¡ ë³€ìˆ˜ ì¶”ê°€\ndf['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\n# loc í•¨ìˆ˜ë¡œ í•„í„°ë§\ndf = df.loc[(df.index >= '2023-09-25 00:10:00') & (df.index <= '2023-09-25 12:00:00')]\n\n# B ë¬¼í’ˆì˜ ë§¤ì¶œì•¡ ì´í•©\ncond1 = df[df['ë¬¼í’ˆ']=='B']\nprint(cond1['ë§¤ì¶œì•¡'].sum())\n\n26500\n\n\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\1645811142.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\n\n\n# ë¬¸ì œ 27\n# 9ì›” 24ì¼ 12:00~24:00ê¹Œì§€ì˜ Aë¬¼í’ˆì˜ ë§¤ì¶œì•¡ ì´í•©ì„ êµ¬í•˜ì‹œì˜¤\n# (ë§¤ì¶œì•¡ = íŒë§¤ìˆ˜*ê°œë‹¹ìˆ˜ìµ)\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë¬¼í’ˆ' : ['A','B','A','B','A','B','A'],\n    'íŒë§¤ìˆ˜' : [5,10,15,15,20,25,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [500,600,500,600,600,700,600]})\ndf['time'] = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf = df[['time', 'ë¬¼í’ˆ', 'íŒë§¤ìˆ˜', 'ê°œë‹¹ìˆ˜ìµ']]\ndf = df.set_index('time',drop=True)\ndf\n\n\n\n\n\n  \n    \n      \n      ë¬¼í’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n    \n      time\n      \n      \n      \n    \n  \n  \n    \n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# (í’€ì´)\n# ë§¤ì¶œì•¡ ë³€ìˆ˜ ì¶”ê°€\ndf['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\n# loc í•¨ìˆ˜ë¡œ í•„í„°ë§\ndf = df.loc[(df.index >= '2023-09-24 12:00:00') & (df.index < '2023-09-25 00:00:00')]\n\n# A ë¬¼í’ˆì˜ ë§¤ì¶œì•¡ ì´í•©\ncond1 = df[df['ë¬¼í’ˆ']=='A']\nprint(cond1['ë§¤ì¶œì•¡'].sum())\n\n10000"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_1.html",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_1.html",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(1)",
    "section": "",
    "text": "ì‹¤ê¸° 2 ìœ í˜•(1)\nì§€ë„í•™ìŠµë§Œ í˜„ì¬ê¹Œì§€ ë‚˜ì˜´"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_1.html#ì œ2ìœ í˜•_ì—°ìŠµí•˜ê¸°_ì™€ì¸ì¢…ë¥˜ë¶„ë¥˜",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_1.html#ì œ2ìœ í˜•_ì—°ìŠµí•˜ê¸°_ì™€ì¸ì¢…ë¥˜ë¶„ë¥˜",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(1)",
    "section": "ì œ2ìœ í˜•_ì—°ìŠµí•˜ê¸°_ì™€ì¸ì¢…ë¥˜ë¶„ë¥˜",
    "text": "ì œ2ìœ í˜•_ì—°ìŠµí•˜ê¸°_ì™€ì¸ì¢…ë¥˜ë¶„ë¥˜"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_1.html#ë°ì´í„°-ë¶„ì„-ë¶„ì„œ",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_1.html#ë°ì´í„°-ë¶„ì„-ë¶„ì„œ",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(1)",
    "section": "âœ… ë°ì´í„° ë¶„ì„ ë¶„ì„œ",
    "text": "âœ… ë°ì´í„° ë¶„ì„ ë¶„ì„œ\n\n1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë°ì´í„° í™•ì¸\n\n\n2. ë°ì´í„° íƒìƒ‰(EDA)\n\n\n3. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¶„ë¦¬\n\n\n4. ëª¨ë¸ë§ ë° ì„±ëŠ¥í‰ê°€\n\n\n5. ì˜ˆì¸¡ê°’ ì œì¶œ"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_1.html#ë¼ì´ë¸ŒëŸ¬ë¦¬-ë°-ë°ì´í„°-í™•ì¸-1",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_1.html#ë¼ì´ë¸ŒëŸ¬ë¦¬-ë°-ë°ì´í„°-í™•ì¸-1",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(1)",
    "section": "âœ… 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë°ì´í„° í™•ì¸",
    "text": "âœ… 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë°ì´í„° í™•ì¸\n\nimport pandas as pd\nimport numpy as np\n\n\n####### ì‹¤ê¸°í™˜ê²½ ë³µì‚¬ ì˜ì—­ #######\nimport pandas as pd\nimport numpy as np\n# ì‹¤ê¸° ì‹œí—˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ì„¸íŒ…í•˜ê¸° (ìˆ˜ì •ê¸ˆì§€)\n\nfrom sklearn.datasets import load_wine\n# ì™€ì¸ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤\nwine = load_wine()\nx = pd.DataFrame(wine.data, columns=wine.feature_names)\ny = pd.DataFrame(wine.target)\n\n# ì‹¤ê¸° ì‹œí—˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ì„¸íŒ…í•˜ê¸° (ìˆ˜ì •ê¸ˆì§€)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2,\n                                                   stratify=y,\n                                                   random_state=2023)\nx_test = pd.DataFrame(x_test)\nx_train = pd.DataFrame(x_train)\ny_train = pd.DataFrame(y_train)\n\nx_test.reset_index()\ny_train.columns = ['target']\n####### ì‹¤ê¸°í™˜ê²½ ë³µì‚¬ ì˜ì—­ #######\n\nğŸ· ì™€ì¸ì˜ ì¢…ë¥˜ë¥¼ ë¶„ë¥˜í•´ë³´ì\n\në°ì´í„°ì˜ ê²°ì¸¡ì¹˜, ì´ìƒì¹˜ì— ëŒ€í•´ ì²˜ë¦¬í•˜ê³ \në¶„ë¥˜ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì •í™•ë„, F1 score, AUC ê°’ì„ ì‚°ì¶œí•˜ì‹œì˜¤\nì œì¶œì€ result ë³€ìˆ˜ì— ë‹´ì•„ ì–‘ì‹ì— ë§ê²Œ ì œì¶œí•˜ì‹œì˜¤\n\n\n# ë°ì´í„° ì„¤ëª…\n# print(wine.DESCR)"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_1.html#ë°ì´í„°-íƒìƒ‰eda-1",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_1.html#ë°ì´í„°-íƒìƒ‰eda-1",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(1)",
    "section": "âœ… 2. ë°ì´í„° íƒìƒ‰(EDA)",
    "text": "âœ… 2. ë°ì´í„° íƒìƒ‰(EDA)\n\n# ë°ì´í„° í–‰/ì—´ í™•ì¸\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\n\n(142, 13)\n(36, 13)\n(142, 1)\n\n\n\n# ì´ˆê¸° ë°ì´í„° í™•ì¸\nprint(x_train.head(3))\nprint(x_test.head(3))\nprint(y_train.head(3))\n\n     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n52     13.82        1.75  2.42               14.0      111.0           3.88   \n146    13.88        5.04  2.23               20.0       80.0           0.98   \n44     13.05        1.77  2.10               17.0      107.0           3.00   \n\n     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n52         3.74                  0.32             1.87             7.05  1.01   \n146        0.34                  0.40             0.68             4.90  0.58   \n44         3.00                  0.28             2.03             5.04  0.88   \n\n     od280/od315_of_diluted_wines  proline  \n52                           3.26   1190.0  \n146                          1.33    415.0  \n44                           3.35    885.0  \n     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n168    13.58        2.58  2.69               24.5      105.0           1.55   \n144    12.25        3.88  2.20               18.5      112.0           1.38   \n151    12.79        2.67  2.48               22.0      112.0           1.48   \n\n     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n168        0.84                  0.39             1.54             8.66  0.74   \n144        0.78                  0.29             1.14             8.21  0.65   \n151        1.36                  0.24             1.26            10.80  0.48   \n\n     od280/od315_of_diluted_wines  proline  \n168                          1.80    750.0  \n144                          2.00    855.0  \n151                          1.47    480.0  \n     target\n52        0\n146       2\n44        0\n\n\n\n# ë³€ìˆ˜ëª…ê³¼ ë°ì´í„° íƒ€ì…ì´ ë§¤ì¹­ì´ ë˜ëŠ”ì§€, ê²°ì¸¡ì¹˜ê°€ ìˆëŠ”ì§€ í™•ì¸\nprint(x_train.info())\nprint(x_test.info())\nprint(y_train.info())\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 142 entries, 52 to 115\nData columns (total 13 columns):\n #   Column                        Non-Null Count  Dtype  \n---  ------                        --------------  -----  \n 0   alcohol                       142 non-null    float64\n 1   malic_acid                    142 non-null    float64\n 2   ash                           142 non-null    float64\n 3   alcalinity_of_ash             142 non-null    float64\n 4   magnesium                     142 non-null    float64\n 5   total_phenols                 142 non-null    float64\n 6   flavanoids                    142 non-null    float64\n 7   nonflavanoid_phenols          142 non-null    float64\n 8   proanthocyanins               142 non-null    float64\n 9   color_intensity               142 non-null    float64\n 10  hue                           142 non-null    float64\n 11  od280/od315_of_diluted_wines  142 non-null    float64\n 12  proline                       142 non-null    float64\ndtypes: float64(13)\nmemory usage: 15.5 KB\nNone\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 36 entries, 168 to 55\nData columns (total 13 columns):\n #   Column                        Non-Null Count  Dtype  \n---  ------                        --------------  -----  \n 0   alcohol                       36 non-null     float64\n 1   malic_acid                    36 non-null     float64\n 2   ash                           36 non-null     float64\n 3   alcalinity_of_ash             36 non-null     float64\n 4   magnesium                     36 non-null     float64\n 5   total_phenols                 36 non-null     float64\n 6   flavanoids                    36 non-null     float64\n 7   nonflavanoid_phenols          36 non-null     float64\n 8   proanthocyanins               36 non-null     float64\n 9   color_intensity               36 non-null     float64\n 10  hue                           36 non-null     float64\n 11  od280/od315_of_diluted_wines  36 non-null     float64\n 12  proline                       36 non-null     float64\ndtypes: float64(13)\nmemory usage: 3.9 KB\nNone\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 142 entries, 52 to 115\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype\n---  ------  --------------  -----\n 0   target  142 non-null    int32\ndtypes: int32(1)\nmemory usage: 1.7 KB\nNone\n\n\n\n# x_trainê³¼ x_test ë°ì´í„°ì˜ ê¸°ì´ˆí†µê³„ëŸ‰ì„ ì˜ ë¹„êµí•´ë³´ì„¸ìš”\nprint(x_train.describe())\nprint(x_test.describe())\nprint(y_train.describe())\n\n          alcohol  malic_acid         ash  alcalinity_of_ash   magnesium  \\\ncount  142.000000  142.000000  142.000000         142.000000  142.000000   \nmean    13.025915    2.354296    2.340211          19.354225   98.732394   \nstd      0.812423    1.142722    0.279910           3.476825   13.581859   \nmin     11.030000    0.740000    1.360000          10.600000   70.000000   \n25%     12.370000    1.610000    2.190000          16.800000   88.000000   \n50%     13.050000    1.820000    2.320000          19.300000   97.000000   \n75%     13.685000    3.115000    2.510000          21.500000  106.750000   \nmax     14.830000    5.800000    3.230000          30.000000  151.000000   \n\n       total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\ncount     142.000000  142.000000            142.000000       142.000000   \nmean        2.303592    2.043592              0.361479         1.575070   \nstd         0.633955    1.033597              0.124627         0.576798   \nmin         0.980000    0.340000              0.140000         0.410000   \n25%         1.757500    1.227500              0.270000         1.242500   \n50%         2.335000    2.100000              0.325000         1.555000   \n75%         2.800000    2.917500              0.437500         1.950000   \nmax         3.880000    5.080000              0.630000         3.580000   \n\n       color_intensity         hue  od280/od315_of_diluted_wines      proline  \ncount       142.000000  142.000000                    142.000000   142.000000  \nmean          5.005070    0.950394                      2.603592   742.112676  \nstd           2.150186    0.220736                      0.709751   317.057395  \nmin           1.280000    0.540000                      1.270000   290.000000  \n25%           3.300000    0.782500                      1.922500   496.250000  \n50%           4.850000    0.960000                      2.780000   660.000000  \n75%           6.122500    1.097500                      3.170000   981.250000  \nmax          13.000000    1.710000                      3.920000  1680.000000  \n         alcohol  malic_acid        ash  alcalinity_of_ash   magnesium  \\\ncount  36.000000   36.000000  36.000000           36.00000   36.000000   \nmean   12.900833    2.265556   2.470278           20.05000  103.722222   \nstd     0.813112    1.021943   0.226066            2.70275   16.371772   \nmin    11.640000    0.890000   2.000000           14.60000   84.000000   \n25%    12.230000    1.592500   2.300000           18.00000   91.500000   \n50%    12.835000    1.885000   2.470000           19.50000  101.000000   \n75%    13.635000    2.792500   2.605000           21.70000  112.000000   \nmax    14.390000    4.950000   3.220000           26.50000  162.000000   \n\n       total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\ncount      36.000000   36.000000             36.000000        36.000000   \nmean        2.261667    1.972778              0.363333         1.653333   \nstd         0.600259    0.858882              0.125516         0.558012   \nmin         1.350000    0.660000              0.130000         0.840000   \n25%         1.715000    1.175000              0.267500         1.320000   \n50%         2.420000    2.175000              0.395000         1.550000   \n75%         2.602500    2.682500              0.435000         1.972500   \nmax         3.850000    3.490000              0.660000         3.280000   \n\n       color_intensity        hue  od280/od315_of_diluted_wines     proline  \ncount        36.000000  36.000000                     36.000000    36.00000  \nmean          5.267222   0.985278                      2.643611   765.75000  \nstd           2.915076   0.258694                      0.720100   309.94851  \nmin           2.080000   0.480000                      1.290000   278.00000  \n25%           2.875000   0.787500                      2.037500   542.50000  \n50%           4.325000   0.985000                      2.790000   682.50000  \n75%           6.900000   1.167500                      3.192500   996.25000  \nmax          11.750000   1.450000                      4.000000  1480.00000  \n           target\ncount  142.000000\nmean     0.936620\nstd      0.773816\nmin      0.000000\n25%      0.000000\n50%      1.000000\n75%      2.000000\nmax      2.000000\n\n\n\n# y ë°ì´í„°ë„ êµ¬ì²´ì ìœ¼ë¡œ ì‚´í´ë³¼ í•„ìš”ìˆìŒ\nprint(y_train.head())\n\nprint(y_train.value_counts())\n\n     target\n52        0\n146       2\n44        0\n67        1\n43        0\ntarget\n1         57\n0         47\n2         38\ndtype: int64"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_1.html#ë°ì´í„°-ì „ì²˜ë¦¬-ë°-ë¶„ë¦¬-1",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_1.html#ë°ì´í„°-ì „ì²˜ë¦¬-ë°-ë¶„ë¦¬-1",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(1)",
    "section": "âœ… 3. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¶„ë¦¬",
    "text": "âœ… 3. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¶„ë¦¬\n\n1) ê²°ì¸¡ì¹˜, 2) ì´ìƒì¹˜, 3) ë³€ìˆ˜ ì²˜ë¦¬í•˜ê¸°\n\n# ê²°ì¸¡ì¹˜ í™•ì¸\nprint(x_train.isnull().sum())\nprint(x_test.isnull().sum())\nprint(y_train.isnull().sum())\n\nalcohol                         0\nmalic_acid                      0\nash                             0\nalcalinity_of_ash               0\nmagnesium                       0\ntotal_phenols                   0\nflavanoids                      0\nnonflavanoid_phenols            0\nproanthocyanins                 0\ncolor_intensity                 0\nhue                             0\nod280/od315_of_diluted_wines    0\nproline                         0\ndtype: int64\nalcohol                         0\nmalic_acid                      0\nash                             0\nalcalinity_of_ash               0\nmagnesium                       0\ntotal_phenols                   0\nflavanoids                      0\nnonflavanoid_phenols            0\nproanthocyanins                 0\ncolor_intensity                 0\nhue                             0\nod280/od315_of_diluted_wines    0\nproline                         0\ndtype: int64\ntarget    0\ndtype: int64\n\n\n\n# ê²°ì¸¡ì¹˜ ì œê±°\n# df = df.dropna()\n# print(df)\n\n# ì°¸ê³ ì‚¬í•­\n# print(df.dropna().shape) # í–‰ê¸°ì¤€ìœ¼ë¡œ ì‚­ì œ\n\n\n# ê²°ì¸¡ì¹˜ ëŒ€ì²´(í‰ê· ê°’, ì¤‘ì•™ê°’, ìµœë¹ˆê°’)\n# ì—°ì†í˜• ë³€ìˆ˜ : ì¤‘ì•™ê°’, í‰ê· ê°’\n#     - df['ë³€ìˆ˜ëª…'].median()\n#     - df['ë³€ìˆ˜ëª…'].mean()\n# ë²”ì£¼í˜• ë³€ìˆ˜ : ìµœë¹ˆê°’\n\n# df['ë³€ìˆ˜ëª…'] = df['ë³€ìˆ˜ëª…'].fillna(ëŒ€ì²´í•  ê°’)\n\n\n# ì´ìƒì¹˜ ëŒ€ì²´(ì˜ˆì‹œ)\n# df['ë³€ìˆ˜ëª…'] = np.where(df['ë³€ìˆ˜ëª…']>=5, ëŒ€ì²´í•  ê°’, df['ë³€ìˆ˜ëª…'])\n\n\n# ë³€ìˆ˜ì²˜ë¦¬\n\n# ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ ì œê±°\n# df = df.drop(columns = ['ë³€ìˆ˜1', 'ë³€ìˆ˜2'])\n# df = df.drop(['ë³€ìˆ˜1', 'ë³€ìˆ˜2'], axis=1)\n\n# í•„ìš”ì‹œ ë³€ìˆ˜ ì¶”ê°€(íŒŒìƒë³€ìˆ˜ ìƒì„±)\n# df['íŒŒìƒë³€ìˆ˜ëª…'] = df['A'] * df['B'] (íŒŒìƒë³€ìˆ˜ ìƒì„± ìˆ˜ì‹)\n\n# ì›í•«ì¸ì½”ë”©(ê°€ë³€ìˆ˜ ì²˜ë¦¬)\n# x_train = pd.get_dummies(x_train)\n# x_test = pd.get_dummies(x_test)\n# print(x_train.info())\n# print(x_test.info())\n\n\n\në°ì´í„° ë¶„ë¦¬\n\n# ë°ì´í„°ë¥¼ í›ˆë ¨ ì„¸íŠ¸ì™€ ê²€ì¦ìš© ì„¸íŠ¸ë¡œ ë¶„í•  (80% í›ˆë ¨, 20% ê²€ì¦ìš©)\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train['target'],\n                                                   test_size = 0.2,\n                                                   stratify = y_train['target'],\n                                                   random_state = 2023)\nprint(x_train.shape)\nprint(x_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)\n\n(113, 13)\n(29, 13)\n(113,)\n(29,)"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_1.html#ëª¨ë¸ë§-ë°-ì„±ëŠ¥í‰ê°€-1",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_1.html#ëª¨ë¸ë§-ë°-ì„±ëŠ¥í‰ê°€-1",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(1)",
    "section": "âœ… 4. ëª¨ë¸ë§ ë° ì„±ëŠ¥í‰ê°€",
    "text": "âœ… 4. ëª¨ë¸ë§ ë° ì„±ëŠ¥í‰ê°€\n\n# ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ ì‚¬ìš© (ì°¸ê³ : íšŒê·€ëª¨ë¸ì€ RandomForestRegressor)\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nmodel.fit(x_train, y_train)\n\nRandomForestClassifier()\n\n\n\n# ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\ny_pred = model.predict(x_val)\n\n\n# ëª¨ë¸ ì„±ëŠ˜ í‰ê°€ (ì •í™•ë„, F1 score, ë¯¼ê°ë„, íŠ¹ì´ë„ ë“±)\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score\nacc = accuracy_score(y_val, y_pred)              # (ì‹¤ì œê°’, ì˜ˆì¸¡ê°’)\nf1 = f1_score(y_val, y_pred, average = 'macro')  # (ì‹¤ì œê°’, ì˜ˆì¸¡ê°’)\n# auc = roc_auc_score(y_val, y_pred)             # (ì‹¤ì œê°’, ì˜ˆì¸¡ê°’)  * AUCëŠ” ì´ì§„ë¶„ë¥˜\n\n\n# ì •í™•ë„(Accuracy)\nprint(acc)\n\n1.0\n\n\n\n# macro f1 score\nprint(f1)\n\n1.0"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_1.html#ì˜ˆì¸¡ê°’-ì œì¶œ-1",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_1.html#ì˜ˆì¸¡ê°’-ì œì¶œ-1",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(1)",
    "section": "âœ… 5. ì˜ˆì¸¡ê°’ ì œì¶œ",
    "text": "âœ… 5. ì˜ˆì¸¡ê°’ ì œì¶œ\n\n(ì£¼ì˜) test ì…‹ì„ ëª¨ë¸ì— ë„£ì–´ ë‚˜ì˜¨ ì˜ˆì¸¡ê°’ì„ ì œì¶œí•´ì•¼í•¨\n\n# (ì‹¤ê¸°ì‹œí—˜ ì•ˆë‚´ì‚¬í•­)\n# ì•„ë˜ ì½”ë“œ ì˜ˆì¸¡ë³€ìˆ˜ì™€ ìˆ˜í—˜ë²ˆí˜¸ë¥¼ ê°œì¸ë³„ë¡œ ë³€ê²½í•˜ì—¬ í™œìš©\n# pd.DataFrame({ 'result':y_result }).to_csv('ìˆ˜í—˜ë²ˆí˜¸_csv', index=False)\n\n# ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\n\n# 1. íŠ¹ì • í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜í•  ê²½ìš° (predict)\ny_result = model.predict(x_test)\nprint(y_result[:5])\n\n# 2. íŠ¹ì • í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜ë  í™•ë¥ ì„ êµ¬í•  ê²½ìš° (predict_proba)\ny_result_prob = model.predict_proba(x_test)\nprint(y_result_prob[:5])\n\n# ì´í•´í•´ë³´ê¸°\nresult_prob = pd.DataFrame({\n    'result' : y_result,\n    'prob_0' : y_result_prob[:, 0],\n    'prob_1' : y_result_prob[:, 1],\n    'prob_2' : y_result_prob[:, 2],\n})\n# Class 0ì¼ í™•ë¥  : y_result_prob[:, 0]\n# Class 1ì¼ í™•ë¥  : y_result_prob[:, 1]\n# Class 2ì¼ í™•ë¥  : y_result_prob[:, 2]\n\nprint(result_prob[:5])\n\n[2 2 2 0 1]\n[[0.01 0.05 0.94]\n [0.13 0.17 0.7 ]\n [0.   0.08 0.92]\n [0.99 0.01 0.  ]\n [0.08 0.81 0.11]]\n   result  prob_0  prob_1  prob_2\n0       2    0.01    0.05    0.94\n1       2    0.13    0.17    0.70\n2       2    0.00    0.08    0.92\n3       0    0.99    0.01    0.00\n4       1    0.08    0.81    0.11"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_2.html",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_2.html",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(2)",
    "section": "",
    "text": "ì‹¤ê¸° 2 ìœ í˜•(2)"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_2.html#ë°ì´í„°-ë¶„ì„-ìˆœì„œ",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_2.html#ë°ì´í„°-ë¶„ì„-ìˆœì„œ",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(2)",
    "section": "âœ… ë°ì´í„° ë¶„ì„ ìˆœì„œ",
    "text": "âœ… ë°ì´í„° ë¶„ì„ ìˆœì„œ\n\n1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë°ì´í„° í™•ì¸\n\n\n2. ë°ì´í„° íƒìƒ‰(EDA)\n\n\n3. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¶„ë¦¬\n\n\n4. ëª¨ë¸ë§ ë° ì„±ëŠ¥í‰ê°€\n\n\n5. ì˜ˆì¸¡ê°’ ì œì¶œ"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_2.html#ë¼ì´ë¸ŒëŸ¬ë¦¬-ë°-ë°ì´í„°-í™•ì¸-1",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_2.html#ë¼ì´ë¸ŒëŸ¬ë¦¬-ë°-ë°ì´í„°-í™•ì¸-1",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(2)",
    "section": "âœ… 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë°ì´í„° í™•ì¸",
    "text": "âœ… 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë°ì´í„° í™•ì¸\n\nimport pandas as pd\nimport numpy as np\n\n\n###### ì‹¤ê¸°í™˜ê²½ ë³µì‚¬ ì˜ì—­ ######\nimport pandas as pd\nimport numpy as np\n# ì‹¤ê¸° ì‹œí—˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ì…‹íŒ…í•˜ê¸° (ìˆ˜ì •ê¸ˆì§€)\nfrom sklearn.datasets import load_diabetes\n# diabetes ë°ì´í„°ì…‹ ë¡œë“œ\ndiabetes = load_diabetes()\nx = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\ny = pd.DataFrame(diabetes.target)\n\n# ì‹¤ì‹œ ì‹œí—˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ì…‹íŒ…í•˜ê¸° (ìˆ˜ì •ê¸ˆì§€)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2,\n                                                   random_state = 2023)\n\nx_test = pd.DataFrame(x_test.reset_index())\nx_train = pd.DataFrame(x_train.reset_index())\ny_train = pd.DataFrame(y_train.reset_index())\n\nx_test.rename(columns = {'index':'cust_id'}, inplace=True)\nx_train.rename(columns = {'index':'cust_id'}, inplace=True)\ny_train.columns = ['cust_id', 'target']\n###### ì‹¤ê¸°í™˜ê²½ ë³µì‚¬ ì˜ì—­ ######\n\n\n# from sklearn.datasets import load_diabetes\n# diabetes = load_diabetes()\n# x = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n# y = pd.DataFrame(diabetes.target)\n\n# from sklearn.model_selection import train_test_split\n# x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,\n#                                                 random_state=2023)\n\n# x_test = pd.DataFrame(x_test.reset_index())\n# x_train = pd.DataFrame(x_train.reset_index())\n# y_train = pd.DataFrame(y_train.reset_index())\n\n# x_test.rename(columns={'index':'cust_id'},inplace=True)\n# x_train.rename(columns={'index':'cust_id'},inplace=True)\n# y_train.columns=['cust']\n\n\nğŸ¥ ë‹¹ë‡¨ë³‘ í™˜ìì˜ ì§ˆë³‘ ì§„í–‰ì •ë„ë¥¼ ì˜ˆì¸¡í•´ë³´ì\n\n\n- ë°ì´í„°ì˜ ê²°ì¸¡ì¹˜, ì´ìƒì¹˜, ë³€ìˆ˜ë“¤ì— ëŒ€í•´ ì „ì²˜ë¦¬í•˜ê³ \n\n\n- íšŒê·€ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ Rsq, MSE ê°’ì„ ì‚°ì¶œí•˜ì‹œì˜¤\n\n\n- ì œì¶œì€ cust_id, target ë³€ìˆ˜ë¥¼ ê°€ì§„ dataframe í˜•íƒœë¡œ ì œì¶œí•˜ì‹œì˜¤\n\n# ë°ì´í„° ì„¤ëª…\nprint(diabetes.DESCR)\n\n.. _diabetes_dataset:\n\nDiabetes dataset\n----------------\n\nTen baseline variables, age, sex, body mass index, average blood\npressure, and six blood serum measurements were obtained for each of n =\n442 diabetes patients, as well as the response of interest, a\nquantitative measure of disease progression one year after baseline.\n\n**Data Set Characteristics:**\n\n  :Number of Instances: 442\n\n  :Number of Attributes: First 10 columns are numeric predictive values\n\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n\n  :Attribute Information:\n      - age     age in years\n      - sex\n      - bmi     body mass index\n      - bp      average blood pressure\n      - s1      tc, total serum cholesterol\n      - s2      ldl, low-density lipoproteins\n      - s3      hdl, high-density lipoproteins\n      - s4      tch, total cholesterol / HDL\n      - s5      ltg, possibly log of serum triglycerides level\n      - s6      glu, blood sugar level\n\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n\nSource URL:\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n\nFor more information see:\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_2.html#ë°ì´í„°-íƒìƒ‰eda-1",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_2.html#ë°ì´í„°-íƒìƒ‰eda-1",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(2)",
    "section": "âœ… 2. ë°ì´í„° íƒìƒ‰(EDA)",
    "text": "âœ… 2. ë°ì´í„° íƒìƒ‰(EDA)\n\n# ë°ì´í„°ì˜ í–‰/ì—´ í™•ì¸\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\n\n(353, 11)\n(89, 11)\n(353, 2)\n\n\n\n# ì´ˆê¸° ë°ì´í„° í™•ì¸\n\nprint(x_train.head(3))\nprint(x_test.head(3))\nprint(y_train.head(3))\n\n   cust_id       age       sex       bmi        bp        s1        s2  \\\n0        4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596   \n1      318  0.088931 -0.044642  0.006728  0.025315  0.030078  0.008707   \n2      301 -0.001882  0.050680 -0.024529  0.052858  0.027326  0.030001   \n\n         s3        s4        s5        s6  \n0  0.008142 -0.002592 -0.031991 -0.046641  \n1  0.063367 -0.039493  0.009436  0.032059  \n2  0.030232 -0.002592 -0.021394  0.036201  \n   cust_id       age       sex       bmi        bp        s1        s2  \\\n0      280  0.009016  0.050680  0.018584  0.039087  0.017694  0.010586   \n1      412  0.074401 -0.044642  0.085408  0.063187  0.014942  0.013091   \n2       68  0.038076  0.050680 -0.029918 -0.040099 -0.033216 -0.024174   \n\n         s3        s4        s5        s6  \n0  0.019187 -0.002592  0.016305 -0.017646  \n1  0.015505 -0.002592  0.006209  0.085907  \n2 -0.010266 -0.002592 -0.012908  0.003064  \n   cust_id  target\n0        4   135.0\n1      318   109.0\n2      301    65.0\n\n\n\n# ë³€ìˆ˜ëª…ê³¼ ë°ì´í„° íƒ€ì…ì´ ë§¤ì¹­ì´ ë˜ëŠ”ì§€, ê²°ì¸¡ì¹˜ê°€ ìˆëŠ”ì§€ í™•ì¸í•´ë³´ì„¸ìš”\n\nprint(x_train.info())\nprint(x_test.info())\nprint(y_train.info())\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 353 entries, 0 to 352\nData columns (total 11 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   cust_id  353 non-null    int64  \n 1   age      353 non-null    float64\n 2   sex      353 non-null    float64\n 3   bmi      353 non-null    float64\n 4   bp       353 non-null    float64\n 5   s1       353 non-null    float64\n 6   s2       353 non-null    float64\n 7   s3       353 non-null    float64\n 8   s4       353 non-null    float64\n 9   s5       353 non-null    float64\n 10  s6       353 non-null    float64\ndtypes: float64(10), int64(1)\nmemory usage: 30.5 KB\nNone\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 89 entries, 0 to 88\nData columns (total 11 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   cust_id  89 non-null     int64  \n 1   age      89 non-null     float64\n 2   sex      89 non-null     float64\n 3   bmi      89 non-null     float64\n 4   bp       89 non-null     float64\n 5   s1       89 non-null     float64\n 6   s2       89 non-null     float64\n 7   s3       89 non-null     float64\n 8   s4       89 non-null     float64\n 9   s5       89 non-null     float64\n 10  s6       89 non-null     float64\ndtypes: float64(10), int64(1)\nmemory usage: 7.8 KB\nNone\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 353 entries, 0 to 352\nData columns (total 2 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   cust_id  353 non-null    int64  \n 1   target   353 non-null    float64\ndtypes: float64(1), int64(1)\nmemory usage: 5.6 KB\nNone\n\n\n\n# x_train ê³¼ x_test ë°ì´í„°ì˜ ê¸°ì´ˆí†µê³„ëŸ‰ì„ ì˜ ë¹„êµí•´ë³´ì„¸ìš”\n\nprint(x_train.describe())\nprint(x_test.describe())\nprint(y_train.describe())\n\n          cust_id         age         sex         bmi          bp          s1  \\\ncount  353.000000  353.000000  353.000000  353.000000  353.000000  353.000000   \nmean   212.634561    0.000804    0.000724    0.000640   -0.000326    0.001179   \nstd    126.668903    0.047617    0.047673    0.048141    0.046585    0.047891   \nmin      0.000000   -0.107226   -0.044642   -0.084886   -0.112400   -0.126781   \n25%    105.000000   -0.038207   -0.044642   -0.035307   -0.033214   -0.033216   \n50%    210.000000    0.005383   -0.044642   -0.006206   -0.005671   -0.002945   \n75%    322.000000    0.038076    0.050680    0.030440    0.032201    0.027326   \nmax    441.000000    0.110727    0.050680    0.170555    0.125158    0.153914   \n\n               s2          s3          s4          s5          s6  \ncount  353.000000  353.000000  353.000000  353.000000  353.000000  \nmean     0.001110   -0.000452    0.000901    0.001446    0.000589  \nstd      0.048248    0.048600    0.048045    0.047160    0.048122  \nmin     -0.115613   -0.102307   -0.076395   -0.126097   -0.137767  \n25%     -0.029184   -0.039719   -0.039493   -0.033249   -0.034215  \n50%     -0.001314   -0.006584   -0.002592    0.000271    0.003064  \n75%      0.031567    0.030232    0.034309    0.033657    0.032059  \nmax      0.198788    0.181179    0.185234    0.133599    0.135612  \n          cust_id        age        sex        bmi         bp         s1  \\\ncount   89.000000  89.000000  89.000000  89.000000  89.000000  89.000000   \nmean   251.696629  -0.003188  -0.002871  -0.002537   0.001292  -0.004676   \nstd    127.901365   0.047761   0.047563   0.045665   0.051777   0.046493   \nmin      9.000000  -0.099961  -0.044642  -0.090275  -0.108957  -0.091006   \n25%    148.000000  -0.034575  -0.044642  -0.030996  -0.036656  -0.037344   \n50%    280.000000  -0.001882  -0.044642  -0.009439  -0.005671  -0.009825   \n75%    366.000000   0.030811   0.050680   0.034751   0.042530   0.031454   \nmax    436.000000   0.096197   0.050680   0.137143   0.132044   0.119515   \n\n              s2         s3         s4         s5         s6  \ncount  89.000000  89.000000  89.000000  89.000000  89.000000  \nmean   -0.004401   0.001792  -0.003575  -0.005737  -0.002334  \nstd     0.045030   0.043723   0.045980   0.049252   0.045757  \nmin    -0.089935  -0.080217  -0.076395  -0.104365  -0.129483  \n25%    -0.030437  -0.028674  -0.039493  -0.038459  -0.030072  \n50%    -0.014153  -0.002903  -0.002592  -0.014956  -0.005220  \n75%     0.020607   0.022869   0.003312   0.024053   0.019633  \nmax     0.130208   0.122273   0.141322   0.133599   0.135612  \n          cust_id      target\ncount  353.000000  353.000000\nmean   212.634561  152.943343\nstd    126.668903   75.324692\nmin      0.000000   37.000000\n25%    105.000000   90.000000\n50%    210.000000  141.000000\n75%    322.000000  208.000000\nmax    441.000000  346.000000\n\n\n\n# y ë°ì´í„°ë„ êµ¬ì²´ì ìœ¼ë¡œ ì‚´í´ë³´ì„¸ìš”\nprint(y_train.head())\n\n   cust_id  target\n0        4   135.0\n1      318   109.0\n2      301    65.0\n3      189    79.0\n4      288    80.0\n\n\n\n# y ë°ì´í„°ë„ êµ¬ì²´ì ìœ¼ë¡œ ì‚´í´ë³´ì„¸ìš”\nprint(y_train.describe())\n\n          cust_id      target\ncount  353.000000  353.000000\nmean   212.634561  152.943343\nstd    126.668903   75.324692\nmin      0.000000   37.000000\n25%    105.000000   90.000000\n50%    210.000000  141.000000\n75%    322.000000  208.000000\nmax    441.000000  346.000000"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_2.html#ë°ì´í„°-ì „ì²˜ë¦¬-ë°-ë¶„ë¦¬-1",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_2.html#ë°ì´í„°-ì „ì²˜ë¦¬-ë°-ë¶„ë¦¬-1",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(2)",
    "section": "âœ… 3. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¶„ë¦¬",
    "text": "âœ… 3. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¶„ë¦¬\n\n1) ê²°ì¸¡ì¹˜, 2) ì´ìƒì¹˜, 3) ë³€ìˆ˜ì²˜ë¦¬í•˜ê¸°\n\n# ê²°ì¸¡ì¹˜ í™•ì¸\n\nprint(x_train.isnull().sum())\nprint(x_test.isnull().sum())\nprint(y_train.isnull().sum())\n\ncust_id    0\nage        0\nsex        0\nbmi        0\nbp         0\ns1         0\ns2         0\ns3         0\ns4         0\ns5         0\ns6         0\ndtype: int64\ncust_id    0\nage        0\nsex        0\nbmi        0\nbp         0\ns1         0\ns2         0\ns3         0\ns4         0\ns5         0\ns6         0\ndtype: int64\ncust_id    0\ntarget     0\ndtype: int64\n\n\n\n# ê²°ì¸¡ì¹˜ ì œê±°\n# df = df.dropna()\n# print(df)\n\n# ì°¸ê³ ì‚¬í•­\n# print(df.dropna().shape) # í–‰ ê¸°ì¤€ìœ¼ë¡œ ì‚­ì œ\n\n\n# ê²°ì¸¡ì¹˜ ëŒ€ì²´ (í‰ê· ê°’, ì¤‘ì•™ê°’, ìµœë¹ˆê°’)\n# ì—°ì†í˜• ë³€ìˆ˜ :  ì¤‘ì•™ê°’, í‰ê· ê°’\n# df['ë³€ìˆ˜ëª…'].median()\n# df['ë³€ìˆ˜ëª…'].mean()\n\n# ë²”ì£¼í˜• ë³€ìˆ˜ :  ìµœë¹ˆê°’\n# df['ë³€ìˆ˜ëª…'].mode()\n\n\n# df['ë³€ìˆ˜ëª…'] = df['ë³€ìˆ˜ëª…'].fillna(ëŒ€ì²´í• ê°’)\n\n\n# ì´ìƒì¹˜ ëŒ€ì²´\n# (ì°¸ê³ ) df['ë³€ìˆ˜ëª…'] = np.where(df['ë³€ìˆ˜ëª…'] >= 5, ëŒ€ì²´í•  ê°’, df['ë³€ìˆ˜ëª…'])\n\n\n# ë³€ìˆ˜ì²˜ë¦¬\n\n# ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ ì œê±°\n# df = df.drop(columns = ['ë³€ìˆ˜1', 'ë³€ìˆ˜2'])\n# df = df.drop(['ë³€ìˆ˜1', 'ë³€ìˆ˜2'], axis=1)\n\n# í•„ìš”ì‹œ ë³€ìˆ˜ ì¶”ê°€(íŒŒìƒë³€ìˆ˜ ìƒì„±)\n# df['íŒŒìƒë³€ìˆ˜ëª…'] = df['A'] * df['B'] (íŒŒìƒë³€ìˆ˜ ìƒì„± ìˆ˜ì‹)\n\n# ì›í•«ì¸ì½”ë”©(ê°€ë³€ìˆ˜ ì²˜ë¦¬)\n# x_train = pd.get_dummies(x_train)\n# x_test = pd.get_dummies(x_test)\n\n# print(x_train.info())\n# print(x_test.info())\n\n\n# ë³€ìˆ˜ì²˜ë¦¬\n\n# ë¶ˆí•„ìš”í•œ ë³€ìˆ˜(columns) ì œê±°\n# cust_id ëŠ” ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ì´ë¯€ë¡œ ì œê±°í•©ë‹ˆë‹¤.\n# ë‹¨, testì…‹ì˜ cust_idê°€ ë‚˜ì¤‘ì— ì œì¶œì´ í•„ìš”í•˜ê¸° ë–„ë¬¸ì— ë³„ë„ë¡œ ì €ì¥\n\ncust_id = x_test['cust_id'].copy()\n\n# ê° ë°ì´í„°ì—ì„œ cust_id ë³€ìˆ˜ ì œê±°\nx_train = x_train.drop(columns = ['cust_id']) # drop(columns = ['ë³€ìˆ˜1', 'ë³€ìˆ˜2'])\nx_test = x_test.drop(columns = ['cust_id'])\n\n\n\në°ì´í„° ë¶„ë¦¬\n\n# ë°ì´í„°ë¥¼ í›ˆë ¨ ì„¸íŠ¸ì™€ ê²€ì¦ìš© ì„¸íŠ¸ë¡œ ë¶„í•  (80% í›ˆë ¨, 20% ê²€ì¦ìš©)\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train['target'],\n                                                   test_size = 0.2,\n                                                   random_state = 23)\n# ë¶„ë¥˜ ëª¨ë¸ì—ì„œëŠ” ì¸µí™”(starify)ë¥¼ í•  í•„ìš”ê°€ ì—†ë‹¤. ì—°ì†í˜•ì¸ ê²½ìš°ì—ë§Œ ì‚¬ìš©\n\nprint(x_train.shape)\nprint(x_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)\n\n(282, 10)\n(71, 10)\n(282,)\n(71,)"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_2.html#ëª¨ë¸ë§-ë°-ì„±ëŠ¥í‰ê°€-1",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_2.html#ëª¨ë¸ë§-ë°-ì„±ëŠ¥í‰ê°€-1",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(2)",
    "section": "âœ… 4. ëª¨ë¸ë§ ë° ì„±ëŠ¥í‰ê°€",
    "text": "âœ… 4. ëª¨ë¸ë§ ë° ì„±ëŠ¥í‰ê°€\n\n# ëœë¤í¬ë ˆìŠ¤ ëª¨ë¸ ì‚¬ìš© (ì°¸ê³  : ë¶„ë¥˜ëª¨ë¸ì€ RandomForestClassifier)\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(random_state = 2023)\nmodel.fit(x_train, y_train)\n\nRandomForestRegressor(random_state=2023)\n\n\n\n# ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\ny_pred = model.predict(x_val)\n\n\n# ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ (í‰ê·  ì œê³± ì˜¤ì°¨ ë° R-squared)\nfrom sklearn.metrics import mean_squared_error, r2_score\nmse = mean_squared_error(y_val, y_pred) # (ì‹¤ì œê°’, ì˜ˆì¸¡ê°’)\nr2 = r2_score(y_val, y_pred) # (ì‹¤ì œê°’, ì˜ˆì¸¡ê°’)\n\n\n# MSE(mean_squared_error, í‰ê·  ì œê³± ì˜¤ì°¨)\nprint(mse)\n\n2571.6276845070424\n\n\n\n# R2 score(R-squared)\nprint(r2)\n\n0.5235611874726152\n\n\n\n# RMSE (root mean squared error)\nrmse = mse ** 0.5\nprint(rmse)\n\n50.71121852713699"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_2.html#ì˜ˆì¸¡ê°’-ì œì¶œ-1",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_2.html#ì˜ˆì¸¡ê°’-ì œì¶œ-1",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(2)",
    "section": "âœ… 5. ì˜ˆì¸¡ê°’ ì œì¶œ",
    "text": "âœ… 5. ì˜ˆì¸¡ê°’ ì œì¶œ\n\n(ì£¼ì˜) x_test ë¥¼ ëª¨ë¸ì— ë„£ì–´ ë‚˜ì˜¨ ì˜ˆì¸¡ê°’ì„ ì œì¶œí•´ì•¼í•¨\n\n#(ì‹¤ê¸°ì‹œí—˜ ì•ˆë‚´ì‚¬í•­)\n# ì•„ë˜ ì½”ë“œ ì˜ˆì¸¡ë³€ìˆ˜ì™€ ìˆ˜í—˜ë²ˆí˜¸ë¥¼ ê°œì¸ë³„ë¡œ ë³€ê²½í•˜ì—¬ í™œìš©\n# pd.DataFrame({'cust_id':cust_id, 'target':y_result}).to_csv('0030000.csv', index=False)\n\n# ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\ny_result = model.predict(x_test)\nresult = pd.DataFrame({'cust_id':cust_id, 'target':y_result})\nprint(result[:5])\n\n   cust_id  target\n0      280  186.51\n1      412  255.92\n2       68   77.97\n3      324  184.22\n4      101  111.14\n\n\n\n# â˜…tip : ë°ì´í„°ë¥¼ ì €ì¥í•œ ë‹¤ìŒ ë¶ˆëŸ¬ì™€ì„œ ì œëŒ€ë¡œ ì œì¶œí–ˆëŠ”ì§€ í™•ì¸í•´ë³´ì\n# pd.DataFrame({'result':y_result}).to_csv('ìˆ˜í—˜ë²ˆí˜¸.csv', index=False)\n# df2 = pd.read_csv(\"ìˆ˜í—˜ë²ˆí˜¸.csv\")\n# print(df2.head())"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_3.html",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_3.html",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(3)",
    "section": "",
    "text": "ì‹¤ê¸° 2 ìœ í˜•(3)"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_3.html#ë°ì´í„°-ë¶„ì„-ìˆœì„œ",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_3.html#ë°ì´í„°-ë¶„ì„-ìˆœì„œ",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(3)",
    "section": "âœ… ë°ì´í„° ë¶„ì„ ìˆœì„œ",
    "text": "âœ… ë°ì´í„° ë¶„ì„ ìˆœì„œ\n\n1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë°ì´í„° í™•ì¸\n\n\n2. ë°ì´í„° íƒìƒ‰(EDA)\n\n\n3. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¶„ë¦¬\n\n\n4. ëª¨ë¸ë§ ë° ì„±ëŠ¥í‰ê°€\n\n\n5. ì˜ˆì¸¡ê°’ ì œì¶œ"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_3.html#ë¼ì´ë¸ŒëŸ¬ë¦¬-ë°-ë°ì´í„°-í™•ì¸-1",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_3.html#ë¼ì´ë¸ŒëŸ¬ë¦¬-ë°-ë°ì´í„°-í™•ì¸-1",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(3)",
    "section": "âœ… 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë°ì´í„° í™•ì¸",
    "text": "âœ… 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë°ì´í„° í™•ì¸\n\nimport pandas as pd\nimport numpy as np\n\n\n###### ì‹¤ê¸°í™˜ê²½ ë³µì‚¬ ì˜ì—­ ######\n# ì‹¤ê¸° ì‹œí—˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ì…‹íŒ…í•˜ê¸° (ìˆ˜ì •ê¸ˆì§€)\n\nimport seaborn as sns\n# tips ë°ì´í„°ì…‹ ë¡œë“œ\ndf = sns.load_dataset('tips')\n\nx = df.drop(['tip'], axis=1)\ny = df['tip']\n\n\n# ì‹¤ì‹œ ì‹œí—˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ì…‹íŒ…í•˜ê¸° (ìˆ˜ì •ê¸ˆì§€)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2,\n                                                   random_state = 2023)\n\nx_test = pd.DataFrame(x_test.reset_index())\nx_train = pd.DataFrame(x_train.reset_index())\ny_train = pd.DataFrame(y_train.reset_index())\n\nx_test.rename(columns = {'index':'cust_id'}, inplace=True)\nx_train.rename(columns = {'index':'cust_id'}, inplace=True)\ny_train.columns = ['cust_id', 'target']\n###### ì‹¤ê¸°í™˜ê²½ ë³µì‚¬ ì˜ì—­ ######"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_3.html#ë ˆìŠ¤í† ë‘ì˜-ì˜ˆì¸¡-ë¬¸ì œ",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_3.html#ë ˆìŠ¤í† ë‘ì˜-ì˜ˆì¸¡-ë¬¸ì œ",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(3)",
    "section": "ë ˆìŠ¤í† ë‘ì˜ ì˜ˆì¸¡ ë¬¸ì œ",
    "text": "ë ˆìŠ¤í† ë‘ì˜ ì˜ˆì¸¡ ë¬¸ì œ"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_3.html#ë°ì´í„°ì˜-ê²°ì¸¡ì¹˜-ì´ìƒì¹˜-ë³€ìˆ˜ì—-ëŒ€í•´-ì²˜ë¦¬í•˜ê³ ",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_3.html#ë°ì´í„°ì˜-ê²°ì¸¡ì¹˜-ì´ìƒì¹˜-ë³€ìˆ˜ì—-ëŒ€í•´-ì²˜ë¦¬í•˜ê³ ",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(3)",
    "section": "- ë°ì´í„°ì˜ ê²°ì¸¡ì¹˜, ì´ìƒì¹˜, ë³€ìˆ˜ì— ëŒ€í•´ ì²˜ë¦¬í•˜ê³ ",
    "text": "- ë°ì´í„°ì˜ ê²°ì¸¡ì¹˜, ì´ìƒì¹˜, ë³€ìˆ˜ì— ëŒ€í•´ ì²˜ë¦¬í•˜ê³ "
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_3.html#íšŒê·€ëª¨ë¸ì„-ì‚¬ìš©í•˜ì—¬-rsq-mse-ê°’ì„-ì‚°ì¶œí•˜ì‹œì˜¤",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_3.html#íšŒê·€ëª¨ë¸ì„-ì‚¬ìš©í•˜ì—¬-rsq-mse-ê°’ì„-ì‚°ì¶œí•˜ì‹œì˜¤",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(3)",
    "section": "- íšŒê·€ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ Rsq, MSE ê°’ì„ ì‚°ì¶œí•˜ì‹œì˜¤",
    "text": "- íšŒê·€ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ Rsq, MSE ê°’ì„ ì‚°ì¶œí•˜ì‹œì˜¤"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_3.html#ë°ì´í„°ì…‹-ì„¤ëª…",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_3.html#ë°ì´í„°ì…‹-ì„¤ëª…",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(3)",
    "section": "ë°ì´í„°ì…‹ ì„¤ëª…",
    "text": "ë°ì´í„°ì…‹ ì„¤ëª…\n\n\ntotal_bill(): ì†ë‹˜ì˜ ì‹ì‚¬ ì´ ì²­êµ¬ì•¡\ntip(íŒ): íŒì˜ ì–‘\nsex(ì„±ë³„): ì†ë‹˜ì˜ ì„±ë³„\nsmoker(í¡ì—°ì): ì†ë‹˜ì˜ í¡ì—° ì—¬ë¶€(â€œYesâ€ ë˜ëŠ” â€œNoâ€)\nday(ìš”ì¼): ì‹ì‚¬ê°€ ì´ë£¨ì–´ì§„ ìš”ì¼\ntime(ì‹œê°„): ì ì‹¬ ë˜ëŠ” ì €ë… ì¤‘ ì–¸ì œ ì‹ì‚¬ê°€ ì´ë£¨ì–´ì¡ŒëŠ”ì§€\nsize(ì¸ì› ìˆ˜): ì‹ì‚¬ì— ì°¸ì„í•œ ì¸ì› ìˆ˜"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_3.html#ë°ì´í„°-íƒìƒ‰eda-1",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_3.html#ë°ì´í„°-íƒìƒ‰eda-1",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(3)",
    "section": "âœ… 2. ë°ì´í„° íƒìƒ‰(EDA)",
    "text": "âœ… 2. ë°ì´í„° íƒìƒ‰(EDA)\n\n# ë°ì´í„°ì˜ í–‰/ì—´ í™•ì¸\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\n\n(195, 7)\n(49, 7)\n(195, 2)\n\n\n\n# ì´ˆê¸° ë°ì´í„° í™•ì¸\n\nprint(x_train.head(3))\nprint(x_test.head(3))\nprint(y_train.head(3))\n\n   cust_id  total_bill     sex smoker  day    time  size\n0      158       13.39  Female     No  Sun  Dinner     2\n1      186       20.90  Female    Yes  Sun  Dinner     3\n2       21       20.29  Female     No  Sat  Dinner     2\n   cust_id  total_bill     sex smoker  day    time  size\n0      154       19.77    Male     No  Sun  Dinner     4\n1        4       24.59  Female     No  Sun  Dinner     4\n2       30        9.55    Male     No  Sat  Dinner     2\n   cust_id  target\n0      158    2.61\n1      186    3.50\n2       21    2.75\n\n\n\n# ë³€ìˆ˜ëª…ê³¼ ë°ì´í„° íƒ€ì…ì´ ë§¤ì¹­ì´ ë˜ëŠ”ì§€, ê²°ì¸¡ì¹˜ê°€ ìˆëŠ”ì§€ í™•ì¸í•´ë³´ì„¸ìš”\n\nprint(x_train.info())\nprint(x_test.info())\nprint(y_train.info())\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 195 entries, 0 to 194\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   cust_id     195 non-null    int64   \n 1   total_bill  195 non-null    float64 \n 2   sex         195 non-null    category\n 3   smoker      195 non-null    category\n 4   day         195 non-null    category\n 5   time        195 non-null    category\n 6   size        195 non-null    int64   \ndtypes: category(4), float64(1), int64(2)\nmemory usage: 6.0 KB\nNone\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 49 entries, 0 to 48\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   cust_id     49 non-null     int64   \n 1   total_bill  49 non-null     float64 \n 2   sex         49 non-null     category\n 3   smoker      49 non-null     category\n 4   day         49 non-null     category\n 5   time        49 non-null     category\n 6   size        49 non-null     int64   \ndtypes: category(4), float64(1), int64(2)\nmemory usage: 2.0 KB\nNone\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 195 entries, 0 to 194\nData columns (total 2 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   cust_id  195 non-null    int64  \n 1   target   195 non-null    float64\ndtypes: float64(1), int64(1)\nmemory usage: 3.2 KB\nNone\n\n\n\n# x_train ê³¼ x_test ë°ì´í„°ì˜ ê¸°ì´ˆí†µê³„ëŸ‰ì„ ì˜ ë¹„êµí•´ë³´ì„¸ìš”\n\nprint(x_train.describe())\nprint(x_test.describe())\nprint(y_train.describe())\n\n          cust_id  total_bill        size\ncount  195.000000  195.000000  195.000000\nmean   122.056410   20.054667    2.543590\nstd     70.668034    8.961645    0.942631\nmin      0.000000    3.070000    1.000000\n25%     59.500000   13.420000    2.000000\n50%    121.000000   17.920000    2.000000\n75%    182.500000   24.395000    3.000000\nmax    243.000000   50.810000    6.000000\n          cust_id  total_bill       size\ncount   49.000000   49.000000  49.000000\nmean   119.285714   18.716531   2.673469\nstd     70.918674    8.669864   0.987162\nmin      2.000000    5.750000   2.000000\n25%     62.000000   12.740000   2.000000\n50%    123.000000   16.660000   2.000000\n75%    180.000000   21.010000   3.000000\nmax    239.000000   44.300000   6.000000\n          cust_id      target\ncount  195.000000  195.000000\nmean   122.056410    3.021692\nstd     70.668034    1.402690\nmin      0.000000    1.000000\n25%     59.500000    2.000000\n50%    121.000000    2.920000\n75%    182.500000    3.530000\nmax    243.000000   10.000000\n\n\n\n# object, category ë°ì´í„°ë„ ì¶”ê°€ í™•ì¸\n# print(x_trian.describe(include='object'))\n# print(x_test.describe(include='object'))\n\nprint(x_train.describe(include='category'))\nprint(x_test.describe(include='category'))\n\n         sex smoker  day    time\ncount    195    195  195     195\nunique     2      2    4       2\ntop     Male     No  Sat  Dinner\nfreq     125    120   71     142\n         sex smoker  day    time\ncount     49     49   49      49\nunique     2      2    4       2\ntop     Male     No  Sat  Dinner\nfreq      32     31   16      34\n\n\n\n# yë°ì´í„°ë„ êµ¬ì²´ì ìœ¼ë¡œ ì‚´í´ë³´ì„¸ìš”\nprint(y_train.head())\n\n   cust_id  target\n0      158    2.61\n1      186    3.50\n2       21    2.75\n3       74    2.20\n4       43    1.32\n\n\n\n# yë°ì´í„°ë„ êµ¬ì²´ì ìœ¼ë¡œ ì‚´í´ë³´ì„¸ìš”\nprint(y_train.describe().T)\n\n         count        mean        std  min   25%     50%     75%    max\ncust_id  195.0  122.056410  70.668034  0.0  59.5  121.00  182.50  243.0\ntarget   195.0    3.021692   1.402690  1.0   2.0    2.92    3.53   10.0"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_3.html#ë°ì´í„°-ì „ì²˜ë¦¬-ë°-ë¶„ë¦¬-1",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_3.html#ë°ì´í„°-ì „ì²˜ë¦¬-ë°-ë¶„ë¦¬-1",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(3)",
    "section": "âœ… 3. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¶„ë¦¬",
    "text": "âœ… 3. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¶„ë¦¬\n\n1) ê²°ì¸¡ì¹˜, 2) ì´ìƒì¹˜, 3) ë³€ìˆ˜ ì²˜ë¦¬í•˜ê¸°\n\n# ê²°ì¸¡ì¹˜ í™•ì¸\n\nprint(x_train.isnull().sum())\nprint(x_test.isnull().sum())\nprint(y_train.isnull().sum())\n\ncust_id       0\ntotal_bill    0\nsex           0\nsmoker        0\nday           0\ntime          0\nsize          0\ndtype: int64\ncust_id       0\ntotal_bill    0\nsex           0\nsmoker        0\nday           0\ntime          0\nsize          0\ndtype: int64\ncust_id    0\ntarget     0\ndtype: int64\n\n\n\n# ê²°ì¸¡ì¹˜ ì œê±°\n# df = df.dropna()\n# print(df)\n\n# ì°¸ê³ ì‚¬í•­\n# print(df.dropna().shape) # í–‰ ê¸°ì¤€ìœ¼ë¡œ ì‚­ì œ\n\n\n# ê²°ì¸¡ì¹˜ ëŒ€ì²´ (í‰ê· ê°’, ì¤‘ì•™ê°’, ìµœë¹ˆê°’)\n# ì—°ì†í˜• ë³€ìˆ˜ :  ì¤‘ì•™ê°’, í‰ê· ê°’\n# df['ë³€ìˆ˜ëª…'].median()\n# df['ë³€ìˆ˜ëª…'].mean()\n\n# ë²”ì£¼í˜• ë³€ìˆ˜ :  ìµœë¹ˆê°’\n# df['ë³€ìˆ˜ëª…'].mode()\n\n\n# df['ë³€ìˆ˜ëª…'] = df['ë³€ìˆ˜ëª…'].fillna(ëŒ€ì²´í• ê°’)\n\n\n# ì´ìƒì¹˜ ëŒ€ì²´\n# (ì°¸ê³ ) df['ë³€ìˆ˜ëª…'] = np.where(df['ë³€ìˆ˜ëª…'] >= 5, ëŒ€ì²´í•  ê°’, df['ë³€ìˆ˜ëª…'])\n\n\n# ë³€ìˆ˜ì²˜ë¦¬\n\n# ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ ì œê±°\n# df = df.drop(columns = ['ë³€ìˆ˜1', 'ë³€ìˆ˜2'])\n# df = df.drop(['ë³€ìˆ˜1', 'ë³€ìˆ˜2'], axis=1)\n\n# í•„ìš”ì‹œ ë³€ìˆ˜ ì¶”ê°€(íŒŒìƒë³€ìˆ˜ ìƒì„±)\n# df['íŒŒìƒë³€ìˆ˜ëª…'] = df['A'] * df['B'] (íŒŒìƒë³€ìˆ˜ ìƒì„± ìˆ˜ì‹)\n\n# ì›í•«ì¸ì½”ë”©(ê°€ë³€ìˆ˜ ì²˜ë¦¬)\n# x_train = pd.get_dummies(x_train)\n# x_test = pd.get_dummies(x_test)\n\n# print(x_train.info())\n# print(x_test.info())\n\n\n# ë³€ìˆ˜ì²˜ë¦¬\n\n# ë¶ˆí•„ìš”í•œ ë³€ìˆ˜(columns) ì œê±°\n# cust_id ëŠ” ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ì´ë¯€ë¡œ ì œê±°í•©ë‹ˆë‹¤.\n# ë‹¨, testì…‹ì˜ cust_idê°€ ë‚˜ì¤‘ì— ì œì¶œì´ í•„ìš”í•˜ê¸° ë–„ë¬¸ì— ë³„ë„ë¡œ ì €ì¥\n\ncust_id = x_test['cust_id'].copy()\n\n# ê° ë°ì´í„°ì—ì„œ cust_id ë³€ìˆ˜ ì œê±°\nx_train = x_train.drop(columns = ['cust_id']) # drop(columns = ['ë³€ìˆ˜1', 'ë³€ìˆ˜2'])\nx_test = x_test.drop(columns = ['cust_id'])\n\n\n# ë³€ìˆ˜ì²˜ë¦¬(ì›í•«ì¸ì½”ë”©)\nprint(x_train.info())\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 195 entries, 0 to 194\nData columns (total 6 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   total_bill  195 non-null    float64 \n 1   sex         195 non-null    category\n 2   smoker      195 non-null    category\n 3   day         195 non-null    category\n 4   time        195 non-null    category\n 5   size        195 non-null    int64   \ndtypes: category(4), float64(1), int64(1)\nmemory usage: 4.5 KB\nNone\n\n\n\n# ë³€ìˆ˜ì²˜ë¦¬(ì›í•«ì¸ì½”ë”©)\nx_train = pd.get_dummies(x_train)\nx_test = pd.get_dummies(x_test)\n\nprint(x_train.info())\nprint(x_test.info())\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 195 entries, 0 to 194\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   total_bill   195 non-null    float64\n 1   size         195 non-null    int64  \n 2   sex_Male     195 non-null    uint8  \n 3   sex_Female   195 non-null    uint8  \n 4   smoker_Yes   195 non-null    uint8  \n 5   smoker_No    195 non-null    uint8  \n 6   day_Thur     195 non-null    uint8  \n 7   day_Fri      195 non-null    uint8  \n 8   day_Sat      195 non-null    uint8  \n 9   day_Sun      195 non-null    uint8  \n 10  time_Lunch   195 non-null    uint8  \n 11  time_Dinner  195 non-null    uint8  \ndtypes: float64(1), int64(1), uint8(10)\nmemory usage: 5.1 KB\nNone\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 49 entries, 0 to 48\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   total_bill   49 non-null     float64\n 1   size         49 non-null     int64  \n 2   sex_Male     49 non-null     uint8  \n 3   sex_Female   49 non-null     uint8  \n 4   smoker_Yes   49 non-null     uint8  \n 5   smoker_No    49 non-null     uint8  \n 6   day_Thur     49 non-null     uint8  \n 7   day_Fri      49 non-null     uint8  \n 8   day_Sat      49 non-null     uint8  \n 9   day_Sun      49 non-null     uint8  \n 10  time_Lunch   49 non-null     uint8  \n 11  time_Dinner  49 non-null     uint8  \ndtypes: float64(1), int64(1), uint8(10)\nmemory usage: 1.4 KB\nNone\n\n\n\n\në°ì´í„° ë¶„ë¦¬\n\n# ë°ì´í„°ë¥¼ í›ˆë ¨ ì„¸íŠ¸ì™€ ê²€ì¦ìš© ì„¸íŠ¸ë¡œ ë¶„í•  (80% í›ˆë ¨, 20% ê²€ì¦ìš©)\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train['target'],\n                                                   test_size = 0.2,\n                                                   random_state = 23)\n# ë¶„ë¥˜ ëª¨ë¸ì—ì„œëŠ” ì¸µí™”(starify)ë¥¼ í•  í•„ìš”ê°€ ì—†ë‹¤. ì—°ì†í˜•ì¸ ê²½ìš°ì—ë§Œ ì‚¬ìš©\n\nprint(x_train.shape)\nprint(x_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)\n\n(156, 12)\n(39, 12)\n(156,)\n(39,)"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_3.html#ëª¨ë¸ë§-ë°‘-ì„±ëŠ¥í‰ê°€",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_3.html#ëª¨ë¸ë§-ë°‘-ì„±ëŠ¥í‰ê°€",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(3)",
    "section": "âœ… 4. ëª¨ë¸ë§ ë°‘ ì„±ëŠ¥í‰ê°€",
    "text": "âœ… 4. ëª¨ë¸ë§ ë°‘ ì„±ëŠ¥í‰ê°€\n\n# ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ ì‚¬ìš© (ì°¸ê³ : ë¶„ë¥˜ëª¨ë¸ì€ RandomForestClassifier)\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(random_state=2023)\nmodel.fit(x_train, y_train)\n\nRandomForestRegressor(random_state=2023)\n\n\n\n# ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\ny_pred = model.predict(x_val)\n\n\n# ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ (R-squared, MSE ë“±)\nfrom sklearn.metrics import r2_score, mean_squared_error\nr2 = r2_score(y_val, y_pred)            # (ì‹¤ì œê°’, ì˜ˆì¸¡ê°’)\nmse = mean_squared_error(y_val, y_pred) # (ì‹¤ì œê°’, ì˜ˆì¸¡ê°’)\n\n\n# MSE\nprint(mse)\n\n0.9812277338461534\n\n\n\n# RMSE\nrmse = mse**0.5\nprint(rmse)\n\n0.990569398803614\n\n\n\n# R2 score(R-squared)\nprint(r2)\n\n0.4286497615634072"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_3.html#ì˜ˆì¸¡ê°’-ì œì¶œ-1",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_3.html#ì˜ˆì¸¡ê°’-ì œì¶œ-1",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(3)",
    "section": "âœ… 5. ì˜ˆì¸¡ê°’ ì œì¶œ",
    "text": "âœ… 5. ì˜ˆì¸¡ê°’ ì œì¶œ\n\n(ì£¼ì˜) x_test ë¥¼ ëª¨ë¸ì— ë„£ì–´ ë‚˜ì˜¨ ì˜ˆì¸¡ê°’ì„ ì œì¶œí•´ì•¼í•¨\n\n#(ì‹¤ê¸°ì‹œí—˜ ì•ˆë‚´ì‚¬í•­)\n# ì•„ë˜ ì½”ë“œ ì˜ˆì¸¡ë³€ìˆ˜ì™€ ìˆ˜í—˜ë²ˆí˜¸ë¥¼ ê°œì¸ë³„ë¡œ ë³€ê²½í•˜ì—¬ í™œìš©\n# pd.DataFrame({'cust_id':cust_id, 'target':y_result}).to_csv('0030000.csv', index=False)\n\n# ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\ny_result = model.predict(x_test)\nresult = pd.DataFrame({'cust_id':cust_id, 'target':y_result})\nprint(result[:5])\n\n   cust_id  target\n0      154  3.2266\n1        4  4.1160\n2       30  1.8966\n3       75  1.8735\n4       33  3.0267\n\n\n\n# â˜…tip : ë°ì´í„°ë¥¼ ì €ì¥í•œ ë‹¤ìŒ ë¶ˆëŸ¬ì™€ì„œ ì œëŒ€ë¡œ ì œì¶œí–ˆëŠ”ì§€ í™•ì¸í•´ë³´ì\n# pd.DataFrame({'result':y_result}).to_csv('ìˆ˜í—˜ë²ˆí˜¸.csv', index=False)\n# df2 = pd.read_csv(\"ìˆ˜í—˜ë²ˆí˜¸.csv\")\n# print(df2.head())"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_4.html",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_4.html",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(4)",
    "section": "",
    "text": "ì‹¤ê¸° 2 ìœ í˜•(4)"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_4.html#ë°ì´í„°-ë¶„ì„-ìˆœì„œ",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_4.html#ë°ì´í„°-ë¶„ì„-ìˆœì„œ",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(4)",
    "section": "âœ… ë°ì´í„° ë¶„ì„ ìˆœì„œ",
    "text": "âœ… ë°ì´í„° ë¶„ì„ ìˆœì„œ\n\n1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë°ì´í„° í™•ì¸\n\n\n2. ë°ì´í„° íƒìƒ‰(EDA)\n\n\n3. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¶„ë¦¬\n\n\n4. ëª¨ë¸ë§ ë° ì„±ëŠ¥í‰ê°€\n\n\n5. ì˜ˆì¸¡ê°’ ì œì¶œ"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_4.html#ë¼ì´ë¸ŒëŸ¬ë¦¬-ë°-ë°ì´í„°-í™•ì¸-1",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_4.html#ë¼ì´ë¸ŒëŸ¬ë¦¬-ë°-ë°ì´í„°-í™•ì¸-1",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(4)",
    "section": "âœ… 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë°ì´í„° í™•ì¸",
    "text": "âœ… 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë°ì´í„° í™•ì¸\n\nimport pandas as pd\nimport numpy as np\n\n\n###############  ì‹¤ê¸°í™˜ê²½ ë³µì‚¬ ì˜ì—­  ###############\nimport pandas as pd\nimport numpy as np\n# ì‹¤ê¸° ì‹œí—˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ì…‹íŒ…í•˜ê¸° (ìˆ˜ì •ê¸ˆì§€)\nfrom sklearn.datasets import load_iris\n# Iris ë°ì´í„°ì…‹ì„ ë¡œë“œ\niris = load_iris()\nx = pd.DataFrame(iris.data, columns=['sepal_length', 'sepal_width',\n                                     'petal_length', 'petal_width'])\ny = iris.target   # 'setosa'=0, 'versicolor'=1, 'virginica'=2\ny = np.where(y>0, 1, 0) # setosa ì¢…ì€ 0, ë‚˜ë¨¸ì§€ ì¢…ì€ 1ë¡œ ë³€ê²½ // ì´ì§„ë¶„ë¥˜\n\n# ì‹¤ê¸° ì‹œí—˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ì…‹íŒ…í•˜ê¸° (ìˆ˜ì •ê¸ˆì§€)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, \n                                                    stratify=y,\n                                                    random_state=2023)\n\nx_test = pd.DataFrame(x_test)\nx_train = pd.DataFrame(x_train)\ny_train = pd.DataFrame(y_train)\ny_train.columns = ['species']\n\n# ê²°ì¸¡ì¹˜ ì‚½ì…\nx_test['sepal_length'].iloc[0] = None  \nx_train['sepal_length'].iloc[0] = None\n# ì´ìƒì¹˜ ì‚½ì…\nx_train['sepal_width'].iloc[0] = 150\n###############  ì‹¤ê¸°í™˜ê²½ ë³µì‚¬ ì˜ì—­  ###############"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_4.html#ë¶“ê½ƒirisì˜-ì¢…speciesì„-ë¶„ë¥˜í•´ë³´ì",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_4.html#ë¶“ê½ƒirisì˜-ì¢…speciesì„-ë¶„ë¥˜í•´ë³´ì",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(4)",
    "section": "ë¶“ê½ƒ(iris)ì˜ ì¢…(Species)ì„ ë¶„ë¥˜í•´ë³´ì",
    "text": "ë¶“ê½ƒ(iris)ì˜ ì¢…(Species)ì„ ë¶„ë¥˜í•´ë³´ì"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_4.html#ë°ì´í„°ì˜-ê²°ì¸¡ì¹˜-ì´ìƒì¹˜ì—-ëŒ€í•´-ì²˜ë¦¬í•˜ê³ ",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_4.html#ë°ì´í„°ì˜-ê²°ì¸¡ì¹˜-ì´ìƒì¹˜ì—-ëŒ€í•´-ì²˜ë¦¬í•˜ê³ ",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(4)",
    "section": "- ë°ì´í„°ì˜ ê²°ì¸¡ì¹˜, ì´ìƒì¹˜ì— ëŒ€í•´ ì²˜ë¦¬í•˜ê³ ",
    "text": "- ë°ì´í„°ì˜ ê²°ì¸¡ì¹˜, ì´ìƒì¹˜ì— ëŒ€í•´ ì²˜ë¦¬í•˜ê³ "
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_4.html#ë¶„ë¥˜ëª¨ë¸ì„-ì‚¬ìš©í•˜ì—¬-ì •í™•ë„-f1-score-auc-ê°’ì„-ì‚°ì¶œí•˜ì‹œì˜¤",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_4.html#ë¶„ë¥˜ëª¨ë¸ì„-ì‚¬ìš©í•˜ì—¬-ì •í™•ë„-f1-score-auc-ê°’ì„-ì‚°ì¶œí•˜ì‹œì˜¤",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(4)",
    "section": "- ë¶„ë¥˜ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì •í™•ë„, F1 score, AUC ê°’ì„ ì‚°ì¶œí•˜ì‹œì˜¤",
    "text": "- ë¶„ë¥˜ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì •í™•ë„, F1 score, AUC ê°’ì„ ì‚°ì¶œí•˜ì‹œì˜¤"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_4.html#ì œì¶œì€-result-ë³€ìˆ˜ì—-ë‹´ì•„-ì–‘ì‹ì—-ë§ê²Œ-ì œì¶œí•˜ì‹œì˜¤",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_4.html#ì œì¶œì€-result-ë³€ìˆ˜ì—-ë‹´ì•„-ì–‘ì‹ì—-ë§ê²Œ-ì œì¶œí•˜ì‹œì˜¤",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(4)",
    "section": "- ì œì¶œì€ result ë³€ìˆ˜ì— ë‹´ì•„ ì–‘ì‹ì— ë§ê²Œ ì œì¶œí•˜ì‹œì˜¤",
    "text": "- ì œì¶œì€ result ë³€ìˆ˜ì— ë‹´ì•„ ì–‘ì‹ì— ë§ê²Œ ì œì¶œí•˜ì‹œì˜¤\n\n# ë°ì´í„° ì„¤ëª…\nprint(iris.DESCR)\n\n.. _iris_dataset:\n\nIris plants dataset\n--------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 150 (50 in each of three classes)\n    :Number of Attributes: 4 numeric, predictive attributes and the class\n    :Attribute Information:\n        - sepal length in cm\n        - sepal width in cm\n        - petal length in cm\n        - petal width in cm\n        - class:\n                - Iris-Setosa\n                - Iris-Versicolour\n                - Iris-Virginica\n                \n    :Summary Statistics:\n\n    ============== ==== ==== ======= ===== ====================\n                    Min  Max   Mean    SD   Class Correlation\n    ============== ==== ==== ======= ===== ====================\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n    ============== ==== ==== ======= ===== ====================\n\n    :Missing Attribute Values: None\n    :Class Distribution: 33.3% for each of 3 classes.\n    :Creator: R.A. Fisher\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n    :Date: July, 1988\n\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\nfrom Fisher's paper. Note that it's the same as in R, but not as in the UCI\nMachine Learning Repository, which has two wrong data points.\n\nThis is perhaps the best known database to be found in the\npattern recognition literature.  Fisher's paper is a classic in the field and\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\ndata set contains 3 classes of 50 instances each, where each class refers to a\ntype of iris plant.  One class is linearly separable from the other 2; the\nlatter are NOT linearly separable from each other.\n\n.. topic:: References\n\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n     Mathematical Statistics\" (John Wiley, NY, 1950).\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n     Structure and Classification Rule for Recognition in Partially Exposed\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n     on Information Theory, May 1972, 431-433.\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n     conceptual clustering system finds 3 classes in the data.\n   - Many, many more ..."
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_4.html#ë°ì´í„°-íƒìƒ‰eda-1",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_4.html#ë°ì´í„°-íƒìƒ‰eda-1",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(4)",
    "section": "âœ… 2. ë°ì´í„° íƒìƒ‰(EDA)",
    "text": "âœ… 2. ë°ì´í„° íƒìƒ‰(EDA)\n\n# ë°ì´í„°ì˜ í–‰/ì—´ í™•ì¸\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\n\n(120, 4)\n(30, 4)\n(120, 1)\n\n\n\n# ì´ˆê¸° ë°ì´í„° í™•ì¸\n\nprint(x_train.head(3))\nprint(x_test.head(3))\nprint(y_train.head(3))\n\n    sepal_length  sepal_width  petal_length  petal_width\n2            NaN        150.0           1.3          0.2\n49           5.0          3.3           1.4          0.2\n66           5.6          3.0           4.5          1.5\n     sepal_length  sepal_width  petal_length  petal_width\n93            NaN          2.3           3.3          1.0\n69            5.6          2.5           3.9          1.1\n137           6.4          3.1           5.5          1.8\n   species\n0        0\n1        0\n2        1\n\n\n\n# ë³€ìˆ˜ëª…ê³¼ ë°ì´í„° íƒ€ì…ì´ ë§¤ì¹­ì´ ë˜ëŠ”ì§€, ê²°ì¸¡ì¹˜ê°€ ìˆëŠ”ì§€ í™•ì¸\n\nprint(x_train.info())\nprint(x_test.info())\nprint(y_train.info()) # í˜„ì¬ train, testì— ê²°ì¸¡ì¹˜ë¥¼ í•˜ë‚˜ì”© ë„£ì—ˆê¸°ì— í™•ì¸ ê°€ëŠ¥\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 120 entries, 2 to 44\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   sepal_length  119 non-null    float64\n 1   sepal_width   120 non-null    float64\n 2   petal_length  120 non-null    float64\n 3   petal_width   120 non-null    float64\ndtypes: float64(4)\nmemory usage: 4.7 KB\nNone\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 30 entries, 93 to 55\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   sepal_length  29 non-null     float64\n 1   sepal_width   30 non-null     float64\n 2   petal_length  30 non-null     float64\n 3   petal_width   30 non-null     float64\ndtypes: float64(4)\nmemory usage: 1.2 KB\nNone\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 120 entries, 0 to 119\nData columns (total 1 columns):\n #   Column   Non-Null Count  Dtype\n---  ------   --------------  -----\n 0   species  120 non-null    int32\ndtypes: int32(1)\nmemory usage: 608.0 bytes\nNone\n\n\n\n# x_train ê³¼ x_test ë°ì´í„°ì˜ ê¸°ì´ˆí†µê³„ëŸ‰ì„ ì˜ ë¹„êµí•´ë³´ì„¸ìš”\n\nprint(x_train.describe())\nprint(x_test.describe())\nprint(y_train.describe())\n\n       sepal_length  sepal_width  petal_length  petal_width\ncount    119.000000     120.0000    120.000000   120.000000\nmean       5.920168       4.2950      3.816667     1.226667\nstd        0.841667      13.4191      1.798848     0.780512\nmin        4.300000       2.2000      1.100000     0.100000\n25%        5.150000       2.8000      1.575000     0.300000\n50%        6.000000       3.0000      4.400000     1.350000\n75%        6.500000       3.4000      5.225000     1.800000\nmax        7.900000     150.0000      6.900000     2.500000\n       sepal_length  sepal_width  petal_length  petal_width\ncount     29.000000    30.000000     30.000000     30.00000\nmean       5.596552     3.000000      3.523333      1.09000\nstd        0.709367     0.522593      1.631518      0.68549\nmin        4.600000     2.000000      1.000000      0.10000\n25%        5.000000     2.625000      1.600000      0.35000\n50%        5.500000     3.000000      4.050000      1.15000\n75%        5.900000     3.300000      4.925000      1.57500\nmax        7.600000     4.200000      6.600000      2.30000\n          species\ncount  120.000000\nmean     0.666667\nstd      0.473381\nmin      0.000000\n25%      0.000000\n50%      1.000000\n75%      1.000000\nmax      1.000000\n\n\n\n# y ë°ì´í„°ë„ êµ¬ì²´ì ìœ¼ë¡œ ì‚´í´ë³´ì„¸ìš”\nprint(y_train.head())\n\n   species\n0        0\n1        0\n2        1\n3        1\n4        1\n\n\n\n# y ë°ì´í„°ë„ êµ¬ì²´ì ìœ¼ë¡œ ì‚´í´ë³´ì„¸ìš”\nprint(y_train.value_counts())\n\nspecies\n1          80\n0          40\ndtype: int64"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_4.html#ë°ì´í„°-ì „ì²˜ë¦¬-ë°-ë¶„ë¦¬-1",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_4.html#ë°ì´í„°-ì „ì²˜ë¦¬-ë°-ë¶„ë¦¬-1",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(4)",
    "section": "âœ… 3. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¶„ë¦¬",
    "text": "âœ… 3. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¶„ë¦¬\n\n1) ê²°ì¸¡ì¹˜, 2) ì´ìƒì¹˜, 3) ë³€ìˆ˜ ì²˜ë¦¬í•˜ê¸°\n\n# ê²°ì¸¡ì¹˜ í™•ì¸\nprint(x_train.isnull().sum())\nprint(x_test.isnull().sum())\nprint(y_train.isnull().sum())\n\nsepal_length    1\nsepal_width     0\npetal_length    0\npetal_width     0\ndtype: int64\nsepal_length    1\nsepal_width     0\npetal_length    0\npetal_width     0\ndtype: int64\nspecies    0\ndtype: int64\n\n\n\n# ê²°ì¸¡ì¹˜ ì œê±° // ë°ì´í„°ì˜ ìˆ˜ê°€ ë§ì€ ê²½ìš°\n# df = df.dropna()\n# print(df)\n\n# ì°¸ê³ ì‚¬í•­\n# print(df.dropna().shape) # í–‰ ê¸°ì¤€ìœ¼ë¡œ ì‚­ì œ\n\n\n# ê²°ì¸¡ì¹˜ ëŒ€ì²´(í‰ê· ê°’, ì¤‘ì•™ê°’, ìµœë¹ˆê°’)\n\n# ì—°ì†í˜• ë³€ìˆ˜ : ì¤‘ì•™ê°’, í‰ê· ê°’\n#  - df['ë³€ìˆ˜ëª…'].median()\n#  - df['ë³€ìˆ˜ëª…'].mean()\n\n# ë²”ìˆ˜í˜• ë³€ìˆ˜ :  ìµœë¹ˆê°’\n\n# df['ë³€ìˆ˜ëª…'] = df['ë³€ìˆ˜ëª…'].fillna(ëŒ€ì²´í•  ê°’)\n\n\n# ê²°ì¸¡ì¹˜ ëŒ€ì²´(ì¤‘ì•™ê°’)\n# ** ì£¼ì˜ì‚¬í•­ : train ë°ì´í„°ì˜ ì¤‘ì•™ê°’ìœ¼ë¡œ test ë°ì´í„°ë„ ë³€ê²½í•´ì¤˜ì•¼ í•¨ **\n\nmedian = x_train['sepal_length'].median()\nx_train['sepal_length'] = x_train['sepal_length'].fillna(median)\nx_test['sepal_length'] = x_test['sepal_length'].fillna(median)\n\n\n# ì´ìƒì¹˜ í™•ì¸\ncond1 = (x_train['sepal_width']>=10)\nprint(len(x_train[cond1]))\n\n1\n\n\n\n# ì´ìƒì¹˜ ëŒ€ì²´\n# (ì°¸ê³ ) df['ë³€ìˆ˜ëª…'] = np.where( df['ë³€ìˆ˜ëª…'] >= 5, ëŒ€ì²´í•  ê°’, df['ë³€ìˆ˜ëª…'])\n\n# ì˜ˆë¥¼ ë“¤ì–´ 'sepal_width' ê°’ì´ 10ì´ ë„˜ìœ¼ë©´ ì´ìƒì¹˜ë¼ê³  ê°€ì •í•´ë³¸ë‹¤ëª…\n# ì´ìƒì¹˜ë¥¼ ì œì™¸í•œ Max ê°’ì„ êµ¬í•´ì„œ ëŒ€ì²´í•´ë³´ì\ncond1 = (x_train['sepal_width'] <= 10)\nmax_sw = x_train[cond1]['sepal_width'].max()\nprint(max_sw)\n\nx_train['sepal_width'] = np.where(x_train['sepal_width'] >=10, max_sw,\n                                 x_train['sepal_width'])\nprint(x_train.describe())\n\n4.4\n       sepal_length  sepal_width  petal_length  petal_width\ncount    120.000000   120.000000    120.000000   120.000000\nmean       5.920833     3.081667      3.816667     1.226667\nstd        0.838155     0.429966      1.798848     0.780512\nmin        4.300000     2.200000      1.100000     0.100000\n25%        5.175000     2.800000      1.575000     0.300000\n50%        6.000000     3.000000      4.400000     1.350000\n75%        6.500000     3.400000      5.225000     1.800000\nmax        7.900000     4.400000      6.900000     2.500000\n\n\n\n# ë³€ìˆ˜ì²˜ë¦¬\n\n# ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ ì œê±°\n# df = df.drop(columns = ['ë³€ìˆ˜1', 'ë³€ìˆ˜2'])\n# df = df.drop(['ë³€ìˆ˜1', 'ë³€ìˆ˜2'], axis = 1)\n\n# í•„ìš”ì‹œ ë³€ìˆ˜ ì¶”ê°€(íŒŒìƒë³€ìˆ˜ ìƒì„±)\n# df['íŒŒìƒë³€ìˆ˜ëª…'] = df['A'] * df['B'] (íŒŒìƒë³€ìˆ˜ ìƒì„± ìˆ˜ì‹)\n\n# ì›í•«ì¸ì½”ë”©(ê°€ë³€ìˆ˜ ì²˜ë¦¬)\n# x_train = pd.get_dummies(x_train)\n# x_test = pd.get_dummies(x_test)\n# print(x_train.info())\n# print(x_test.info())\n\n\n\në°ì´í„° ë¶„ë¦¬\n\n#  ë°ì´í„°ë¥¼ í›ˆë ¨ ì„¸íŠ¸ì™€ ê²€ì¦ìš© ì„¸íŠ¸ë¡œ ë¶„í•  (80% í›ˆë ¨, 20% ê²€ì¦)\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_train,\n                                                  y_train['species'],\n                                                  test_size=0.2,\n                                                  stratify = y_train['species'],\n                                                  random_state = 2023)\nprint(x_train.shape)\nprint(x_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)\n\n(96, 4)\n(24, 4)\n(96,)\n(24,)"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_4.html#ëª¨ë¸ë§-ë°-ì„±ëŠ¥í‰ê°€-1",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_4.html#ëª¨ë¸ë§-ë°-ì„±ëŠ¥í‰ê°€-1",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(4)",
    "section": "âœ… 4. ëª¨ë¸ë§ ë° ì„±ëŠ¥í‰ê°€",
    "text": "âœ… 4. ëª¨ë¸ë§ ë° ì„±ëŠ¥í‰ê°€\n\n# ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ ì‚¬ìš© (ì°¸ê³  : íšŒê·€ëª¨ë¸ì€ RandomForestRegressor)\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nmodel.fit(x_train, y_train)\n\nRandomForestClassifier()\n\n\n\n# ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\ny_pred = model.predict(x_val)\n\n\n# ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ (accuracy, f1 score, AUC ë“±)\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score\nacc = accuracy_score(y_val, y_pred)  # (ì‹¤ì œê°’, ì˜ˆì¸¡ê°’)\nf1 = f1_score(y_val, y_pred)         # (ì‹¤ì œê°’, ì˜ˆì¸¡ê°’)\n# ë‹¤ì¤‘ë¶„ë¥˜ì¸ ê²½ìš° f1  f1_score(y_val, y_pred, average = 'macro')\nauc = roc_auc_score(y_val, y_pred)   # (ì‹¤ì œê°’, ì˜ˆì¸¡ê°’)\n\n\n# ì •í™•ë„(Accuracy)\nprint(acc)\n\n1.0\n\n\n\n# F1 Score\nprint(f1)\n\n1.0\n\n\n\n# AUC\nprint(auc)\n\n1.0\n\n\n\n# ì°¸ê³ ì‚¬í•­\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_val, y_pred)\nprint(cm)\n\n# #####  ì˜ˆì¸¡\n# #####  0  1\n# ì‹¤ì œ 0 TN FP\n# ì‹¤ì œ 1 FN TP\n\n[[ 8  0]\n [ 0 16]]"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_4.html#ì˜ˆì¸¡ê°’-ì œì¶œ-1",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_4.html#ì˜ˆì¸¡ê°’-ì œì¶œ-1",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(4)",
    "section": "âœ… 5. ì˜ˆì¸¡ê°’ ì œì¶œ",
    "text": "âœ… 5. ì˜ˆì¸¡ê°’ ì œì¶œ\n\n(ì£¼ì˜) x_test ë¥¼ ëª¨ë¸ì— ë„£ì–´ ë‚˜ì˜¨ ì˜ˆì¸¡ê°’ì„ ì œì¶œí•´ì•¼ í•¨\n\n#(ì‹¤ê¸°ì‹œí—˜ ì•ˆë‚´ì‚¬í•­)\n# ì•„ë˜ ì½”ë“œ ì˜ˆì¸¡ë³€ìˆ˜ì™€ ìˆ˜í—˜ë²ˆí˜¸ë¥¼ ê°œì¸ë³„ë¡œ ë³€ê²½í•˜ì—¬ í™œìš©\n# pd.DataFrame({'cust_id':cust_id, 'target':y_result}).to_csv('0030000.csv', index=False)\n\n# ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\n\n# 1. íŠ¹ì • í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜í•  ê²½ìš° (predict)\ny_result = model.predict(x_test)\nprint(y_result[:5])\n\n# 2. íŠ¹ì • í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜ë  í™•ë¥ ì„ êµ¬í•  ê²½ìš° (predict_proba)\ny_result_prob = model.predict_proba(x_test)\nprint(y_result_prob[:5])\n\n# ì´í•´í•´ë³´ê¸°\nresult_prob = pd.DataFrame({\n    'result' : y_result,\n    'prob_0' : y_result_prob[:,0]\n})\n# setosa ì¼ í™•ë¥  : y_result_prob[:, 0]\n# ê·¸ ì™¸ ì¢…ì¼ í™•ë¥  : y_result_prob[:, 1]\n\nprint(result_prob[:5])\n\n[1 1 1 0 1]\n[[0.   1.  ]\n [0.   1.  ]\n [0.   1.  ]\n [1.   0.  ]\n [0.04 0.96]]\n   result  prob_0\n0       1    0.00\n1       1    0.00\n2       1    0.00\n3       0    1.00\n4       1    0.04\n\n\n\n# â˜…tip : ë°ì´í„°ë¥¼ ì €ì¥í•œ ë‹¤ìŒ ë¶ˆëŸ¬ì™€ì„œ ì œëŒ€ë¡œ ì œì¶œí–ˆëŠ”ì§€ í™•ì¸í•´ë³´ì\n# pd.DataFrame({'result':y_result}).to_csv('ìˆ˜í—˜ë²ˆí˜¸.csv', index=False)\n# df2 = pd.read_csv(\"ìˆ˜í—˜ë²ˆí˜¸.csv\")\n# print(df2.head())"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_5.html",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_5.html",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(5)",
    "section": "",
    "text": "ì‹¤ê¸° 2 ìœ í˜•(5)"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_5.html#ë°ì´í„°-ë¶„ì„-ìˆœì„œ",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_5.html#ë°ì´í„°-ë¶„ì„-ìˆœì„œ",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(5)",
    "section": "âœ… ë°ì´í„° ë¶„ì„ ìˆœì„œ",
    "text": "âœ… ë°ì´í„° ë¶„ì„ ìˆœì„œ\n\n1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë°ì´í„° í™•ì¸\n\n\n2. ë°ì´í„° íƒìƒ‰(EDA)\n\n\n3. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¶„ë¦¬\n\n\n4. ëª¨ë¸ë§ ë° ì„±ëŠ¥í‰ê°€\n\n\n5. ì˜ˆì¸¡ê°’ ì œì¶œ"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_5.html#ë¼ì´ë¸ŒëŸ¬ë¦¬-ë°-ë°ì´í„°-í™•ì¸-1",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_5.html#ë¼ì´ë¸ŒëŸ¬ë¦¬-ë°-ë°ì´í„°-í™•ì¸-1",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(5)",
    "section": "âœ… 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë°ì´í„° í™•ì¸",
    "text": "âœ… 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë°ì´í„° í™•ì¸\n\nimport pandas as pd\nimport numpy as np\n\n\n###############  ë³µì‚¬ ì˜ì—­  ###############\n# ì‹¤ê¸° ì‹œí—˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ì…‹íŒ…í•˜ê¸° (ìˆ˜ì •ê¸ˆì§€)\n\n# Seabornì˜ ë‚´ì¥ íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\nimport seaborn as sns\ndf = sns.load_dataset('titanic')\n\nx = df.drop('survived', axis=1)\ny = df['survived']\n\n# ì‹¤ê¸° ì‹œí—˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ì…‹íŒ…í•˜ê¸° (ìˆ˜ì •ê¸ˆì§€)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, \n                                                    stratify=y,\n                                                    random_state=2023)\n\nx_test = pd.DataFrame(x_test)\nx_train = pd.DataFrame(x_train)\ny_train = pd.DataFrame(y_train)\ny_test = pd.DataFrame(y_test) # í‰ê°€ìš©\n\nx_test.reset_index()\ny_train.columns = ['target'] \ny_test.columns = ['target'] \n###############  ë³µì‚¬ ì˜ì—­  ###############\n\n\níƒ€ì´íƒ€ë‹‰ ìƒì¡´ì ì˜ˆì¸¡ ë¬¸ì œ\n\n\n- ë°ì´í„°ì˜ ê²°ì¸¡ì¹˜, ì¤‘ë³µ ë³€ìˆ˜ê°’ì— ëŒ€í•´ ì²˜ë¦¬í•˜ê³ \n\n\n- ë¶„ë¥˜ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ Accuracy, F1 score, AUC ê°’ì„ ì‚°ì¶œí•˜ì‹œì˜¤.\n\n\në°ì´í„° ì„¤ëª…\n\n\nsurvival : 0 = No, 1 = Yes -pclass : ê°ì‹¤ ë“±ê¸‰(1,2,3) -sex : ì„±ë³„ -age : ë‚˜ì´ -sibsp : íƒ€ì´íƒ€ë‹‰í˜¸ì— íƒ‘ìŠ¹í•œ í˜•ì œ/ë°°ìš°ìì˜ ìˆ˜ -parch : íƒ€ì´íƒ€ë‹‰í˜¸ì— íƒ‘ìŠ¹í•œ ë¶€ëª¨/ìë…€ì˜ ìˆ˜ -fare : ìš”ê¸ˆ -embarked : íƒ‘ìŠ¹ì§€ ì´ë¦„(C, Q, S) Cherbourg / Queenstown / Southampton -(ì¤‘ë³µ)class : ê°ì‹¤ ë“±ê¸‰(First, Second, Third) -who : man, women, child -adult_male : ì„±ì¸ë‚¨ìì¸ì§€ ì—¬ë¶€(True=ì„±ì¸ë‚¨ì, False ê·¸ì™¸) -deck : ì„ ì‹¤ë²ˆí˜¸ ì²« ì•ŒíŒŒë²³(A,B,C,D,E,F,G) -(ì¤‘ë³µ) embark_town : íƒ‘ìŠ¹ì§€ ì´ë¦„(Cherbourg, Queenstown, Southampton) -(ì¤‘ë³µ) alive : ìƒì¡´ì—¬ë¶€(no:ì‚¬ë§, yes:ìƒì¡´) -alone : í˜¼ì íƒ‘ìŠ¹í–ˆëŠ”ì§€ ì—¬ë¶€(True=í˜¼ì, False=ê°€ì¡±ê³¼ í•¨ê»˜)"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_5.html#ë°ì´í„°-íƒìƒ‰eda-1",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_5.html#ë°ì´í„°-íƒìƒ‰eda-1",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(5)",
    "section": "âœ… 2. ë°ì´í„° íƒìƒ‰(EDA)",
    "text": "âœ… 2. ë°ì´í„° íƒìƒ‰(EDA)\n\n# ë°ì´í„°ì˜ í–‰/ì—´ í™•ì¸\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\n\n(712, 14)\n(179, 14)\n(712, 1)\n\n\n\n# ì´ˆê¸° ë°ì´í„° í™•ì¸\n\nprint(x_train.head())\nprint(x_test.head())\nprint(y_train.head())\n\n     pclass     sex   age  sibsp  parch   fare embarked   class    who  \\\n3         1  female  35.0      1      0  53.10        S   First  woman   \n517       3    male   NaN      0      0  24.15        Q   Third    man   \n861       2    male  21.0      1      0  11.50        S  Second    man   \n487       1    male  58.0      0      0  29.70        C   First    man   \n58        2  female   5.0      1      2  27.75        S  Second  child   \n\n     adult_male deck  embark_town alive  alone  \n3         False    C  Southampton   yes  False  \n517        True  NaN   Queenstown    no   True  \n861        True  NaN  Southampton    no  False  \n487        True    B    Cherbourg    no   True  \n58        False  NaN  Southampton   yes  False  \n     pclass     sex   age  sibsp  parch      fare embarked   class    who  \\\n800       2    male  34.0      0      0   13.0000        S  Second    man   \n341       1  female  24.0      3      2  263.0000        S   First  woman   \n413       2    male   NaN      0      0    0.0000        S  Second    man   \n575       3    male  19.0      0      0   14.5000        S   Third    man   \n202       3    male  34.0      0      0    6.4958        S   Third    man   \n\n     adult_male deck  embark_town alive  alone  \n800        True  NaN  Southampton    no   True  \n341       False    C  Southampton   yes  False  \n413        True  NaN  Southampton    no   True  \n575        True  NaN  Southampton    no   True  \n202        True  NaN  Southampton    no   True  \n     target\n3         1\n517       0\n861       0\n487       0\n58        1\n\n\n\n# ë³€ìˆ˜ëª…ê³¼ ë°ì´í„° íƒ€ì…ì´ ë§¤ì¹­ì´ ë˜ëŠ”ì§€, ê²°ì¸¡ì¹˜ê°€ ìˆëŠ”ì§€ í™•ì¸í•´ë³´ì„¸ìš”\n\nprint(x_train.info())\nprint(x_test.info())\nprint(y_train.info())\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 712 entries, 3 to 608\nData columns (total 14 columns):\n #   Column       Non-Null Count  Dtype   \n---  ------       --------------  -----   \n 0   pclass       712 non-null    int64   \n 1   sex          712 non-null    object  \n 2   age          579 non-null    float64 \n 3   sibsp        712 non-null    int64   \n 4   parch        712 non-null    int64   \n 5   fare         712 non-null    float64 \n 6   embarked     710 non-null    object  \n 7   class        712 non-null    category\n 8   who          712 non-null    object  \n 9   adult_male   712 non-null    bool    \n 10  deck         164 non-null    category\n 11  embark_town  710 non-null    object  \n 12  alive        712 non-null    object  \n 13  alone        712 non-null    bool    \ndtypes: bool(2), category(2), float64(2), int64(3), object(5)\nmemory usage: 64.4+ KB\nNone\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 179 entries, 800 to 410\nData columns (total 14 columns):\n #   Column       Non-Null Count  Dtype   \n---  ------       --------------  -----   \n 0   pclass       179 non-null    int64   \n 1   sex          179 non-null    object  \n 2   age          135 non-null    float64 \n 3   sibsp        179 non-null    int64   \n 4   parch        179 non-null    int64   \n 5   fare         179 non-null    float64 \n 6   embarked     179 non-null    object  \n 7   class        179 non-null    category\n 8   who          179 non-null    object  \n 9   adult_male   179 non-null    bool    \n 10  deck         39 non-null     category\n 11  embark_town  179 non-null    object  \n 12  alive        179 non-null    object  \n 13  alone        179 non-null    bool    \ndtypes: bool(2), category(2), float64(2), int64(3), object(5)\nmemory usage: 16.6+ KB\nNone\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 712 entries, 3 to 608\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype\n---  ------  --------------  -----\n 0   target  712 non-null    int64\ndtypes: int64(1)\nmemory usage: 11.1 KB\nNone\n\n\n\n# x_train ê³¼ x_test ë°ì´í„°ì˜ ê¸°ì´ˆí†µê³„ëŸ‰ì„ ì˜ ë¹„êµí•´ë³´ì„¸ìš”\n\nprint(x_train.describe())\nprint(x_test.describe())\nprint(y_train.describe())\n\n           pclass         age       sibsp       parch        fare\ncount  712.000000  579.000000  712.000000  712.000000  712.000000\nmean     2.307584   29.479568    0.518258    0.372191   31.741836\nstd      0.834926   14.355304    1.094522    0.792341   45.403910\nmin      1.000000    0.420000    0.000000    0.000000    0.000000\n25%      2.000000   20.000000    0.000000    0.000000    7.895800\n50%      3.000000   28.000000    0.000000    0.000000   14.454200\n75%      3.000000   38.000000    1.000000    0.000000   31.275000\nmax      3.000000   74.000000    8.000000    6.000000  512.329200\n           pclass         age       sibsp       parch        fare\ncount  179.000000  135.000000  179.000000  179.000000  179.000000\nmean     2.312849   30.640741    0.541899    0.418994   34.043364\nstd      0.842950   15.258427    1.137797    0.859760   64.097184\nmin      1.000000    1.000000    0.000000    0.000000    0.000000\n25%      2.000000   22.000000    0.000000    0.000000    7.925000\n50%      3.000000   29.000000    0.000000    0.000000   14.500000\n75%      3.000000   39.000000    1.000000    0.000000   30.285400\nmax      3.000000   80.000000    8.000000    5.000000  512.329200\n           target\ncount  712.000000\nmean     0.383427\nstd      0.486563\nmin      0.000000\n25%      0.000000\n50%      0.000000\n75%      1.000000\nmax      1.000000\n\n\n\n# object, category ë°ì´í„°ë„ ì¶”ê°€í™•ì¸\n\nprint(x_train.describe(include=\"object\"))\nprint(x_test.describe(include=\"object\"))\n\nprint(x_train.describe(include=\"category\"))\nprint(x_test.describe(include=\"category\"))\n\n         sex embarked  who  embark_town alive\ncount    712      710  712          710   712\nunique     2        3    3            3     2\ntop     male        S  man  Southampton    no\nfreq     469      518  432          518   439\n         sex embarked  who  embark_town alive\ncount    179      179  179          179   179\nunique     2        3    3            3     2\ntop     male        S  man  Southampton    no\nfreq     108      126  105          126   110\n        class deck\ncount     712  164\nunique      3    7\ntop     Third    C\nfreq      391   47\n        class deck\ncount     179   39\nunique      3    7\ntop     Third    C\nfreq      100   12\n\n\n\n# y ë°ì´í„°ë„ êµ¬ì²´ì ìœ¼ë¡œ ì‚´í´ë³´ì„¸ìš”\nprint(y_train.head())\n\n     target\n3         1\n517       0\n861       0\n487       0\n58        1\n\n\n\n# y ë°ì´í„°ë„ êµ¬ì²´ì ìœ¼ë¡œ ì‚´í´ë³´ì„¸ìš”\nprint(y_train.value_counts())\n\ntarget\n0         439\n1         273\ndtype: int64"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_5.html#ë°ì´í„°-ì „ì²˜ë¦¬-ë°-ë¶„ë¦¬-1",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_5.html#ë°ì´í„°-ì „ì²˜ë¦¬-ë°-ë¶„ë¦¬-1",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(5)",
    "section": "âœ… 3. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¶„ë¦¬",
    "text": "âœ… 3. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¶„ë¦¬\n\n1) ê²°ì¸¡ì¹˜, 2) ì´ìƒì¹˜, 3) ë³€ìˆ˜ ì²˜ë¦¬í•˜ê¸°\n\n# ê²°ì¸¡ì¹˜ í™•ì¸\n\nprint(x_train.isnull().sum())\nprint(x_test.isnull().sum())\nprint(y_train.isnull().sum())\n\npclass           0\nsex              0\nage            133\nsibsp            0\nparch            0\nfare             0\nembarked         2\nclass            0\nwho              0\nadult_male       0\ndeck           548\nembark_town      2\nalive            0\nalone            0\ndtype: int64\npclass           0\nsex              0\nage             44\nsibsp            0\nparch            0\nfare             0\nembarked         0\nclass            0\nwho              0\nadult_male       0\ndeck           140\nembark_town      0\nalive            0\nalone            0\ndtype: int64\ntarget    0\ndtype: int64\n\n\n\n# ê²°ì¸¡ì¹˜ ì œê±°\n# df = df.dropna()\n# print(df)\n\n# ì°¸ê³ ì‚¬í•­\n# print(df.dropna().shape) # í–‰ ê¸°ì¤€ìœ¼ë¡œ ì‚­ì œ\n\n\n# ê²°ì¸¡ì¹˜ ëŒ€ì²´\n# x_train(712, 14) : age(133), embarked(2), deck(548), embark_town(2)\n# x_test(179, 14) : age(44), deck(140)\n\n# ë³€ìˆ˜ ì œê±°\n# (ì¤‘ë³µ)class\n# (ì¤‘ë³µ) embark_town\n# (ì¤‘ë³µ) alive \n# (ê²°ì¸¡ì¹˜ ë‹¤ìˆ˜) deck\n\n\n# ì¤‘ë³µë³€ìˆ˜ ì œê±°\nx_train = x_train.drop(['class', 'embark_town', 'alive', 'deck'], axis=1)\nx_test = x_test.drop(['class', 'embark_town', 'alive', 'deck'], axis=1)\n\n\n# ë³€ìˆ˜ ì œê±° í™•ì¸\nprint(x_train.info())\nprint(x_test.info())\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 712 entries, 3 to 608\nData columns (total 10 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   pclass      712 non-null    int64  \n 1   sex         712 non-null    object \n 2   age         579 non-null    float64\n 3   sibsp       712 non-null    int64  \n 4   parch       712 non-null    int64  \n 5   fare        712 non-null    float64\n 6   embarked    710 non-null    object \n 7   who         712 non-null    object \n 8   adult_male  712 non-null    bool   \n 9   alone       712 non-null    bool   \ndtypes: bool(2), float64(2), int64(3), object(3)\nmemory usage: 51.5+ KB\nNone\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 179 entries, 800 to 410\nData columns (total 10 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   pclass      179 non-null    int64  \n 1   sex         179 non-null    object \n 2   age         135 non-null    float64\n 3   sibsp       179 non-null    int64  \n 4   parch       179 non-null    int64  \n 5   fare        179 non-null    float64\n 6   embarked    179 non-null    object \n 7   who         179 non-null    object \n 8   adult_male  179 non-null    bool   \n 9   alone       179 non-null    bool   \ndtypes: bool(2), float64(2), int64(3), object(3)\nmemory usage: 12.9+ KB\nNone\n\n\n\n# ê²°ì¸¡ì¹˜ ëŒ€ì²´\n# x_train(712, 14) : age(133), embarked(2)\n# x_test(179, 14) : age(44)\n\n# age ë³€ìˆ˜ \nmed_age = x_train['age'].median()\nx_train['age'] = x_train['age'].fillna(med_age)\nx_test['age'] = x_test['age'].fillna(med_age) # train dataì˜ ì¤‘ì•™ê°’ìœ¼ë¡œ \n\n# embarked(object íƒ€ì…ì€ ëŒ€ì²´ì‹œ ì£¼ë¡œ ìµœë¹ˆê°’ìœ¼ë¡œ)\nmode_et = x_train['embarked'].mode()\nx_train['embarked'] = x_train['embarked'].fillna(mode_et[0]) # ìµœë¹ˆê°’ [0] ì£¼ì˜\n\n\n# ê²°ì¸¡ì¹˜ ëŒ€ì²´ ì—¬ë¶€ í™•ì¸\nprint(x_train.isnull().sum())\nprint(x_test.isnull().sum())\n\npclass        0\nsex           0\nage           0\nsibsp         0\nparch         0\nfare          0\nembarked      0\nwho           0\nadult_male    0\nalone         0\ndtype: int64\npclass        0\nsex           0\nage           0\nsibsp         0\nparch         0\nfare          0\nembarked      0\nwho           0\nadult_male    0\nalone         0\ndtype: int64\n\n\n\n# ë³€ìˆ˜ì²˜ë¦¬(ì›í•«ì¸ì½”ë”©) - object ë³€ìˆ˜ë§Œ ì ìš©ë¨\nx_train = pd.get_dummies(x_train)\nx_test = pd.get_dummies(x_test)\n\nprint(x_train.info())\nprint(x_test.info())\n\n# advanced ë²„ì „ ì‚¬ìš©\nx_train_ad = x_train.copy()\nx_test_ad = x_test.copy()\ny_train_ad = y_train.copy()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 712 entries, 3 to 608\nData columns (total 15 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   pclass      712 non-null    int64  \n 1   age         712 non-null    float64\n 2   sibsp       712 non-null    int64  \n 3   parch       712 non-null    int64  \n 4   fare        712 non-null    float64\n 5   adult_male  712 non-null    bool   \n 6   alone       712 non-null    bool   \n 7   sex_female  712 non-null    uint8  \n 8   sex_male    712 non-null    uint8  \n 9   embarked_C  712 non-null    uint8  \n 10  embarked_Q  712 non-null    uint8  \n 11  embarked_S  712 non-null    uint8  \n 12  who_child   712 non-null    uint8  \n 13  who_man     712 non-null    uint8  \n 14  who_woman   712 non-null    uint8  \ndtypes: bool(2), float64(2), int64(3), uint8(8)\nmemory usage: 40.3 KB\nNone\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 179 entries, 800 to 410\nData columns (total 15 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   pclass      179 non-null    int64  \n 1   age         179 non-null    float64\n 2   sibsp       179 non-null    int64  \n 3   parch       179 non-null    int64  \n 4   fare        179 non-null    float64\n 5   adult_male  179 non-null    bool   \n 6   alone       179 non-null    bool   \n 7   sex_female  179 non-null    uint8  \n 8   sex_male    179 non-null    uint8  \n 9   embarked_C  179 non-null    uint8  \n 10  embarked_Q  179 non-null    uint8  \n 11  embarked_S  179 non-null    uint8  \n 12  who_child   179 non-null    uint8  \n 13  who_man     179 non-null    uint8  \n 14  who_woman   179 non-null    uint8  \ndtypes: bool(2), float64(2), int64(3), uint8(8)\nmemory usage: 10.1 KB\nNone\n\n\n\n# (ì°¸ê³ ì‚¬í•­)ì›í•«ì¸ì½”ë”© í›„ ë³€ìˆ˜ì˜ ìˆ˜ê°€ ë‹¤ë¥¸ ê²½ìš°\n# => x_testì˜ ë³€ìˆ˜ì˜ ìˆ˜ê°€ x_train ë³´ë‹¤ ë§ì€ ê²½ìš° (í˜¹ì€ ê·¸ ë°˜ëŒ€ì¸ ê²½ìš°)\n# ì›í•«ì¸ì½”ë”© í›„ Feature ìˆ˜ê°€ ë‹¤ë¥¼ ê²½ìš°\n# x_train = pd.get_dummies(x_train)\n# x_test = pd.get_dummies(x_test)\n# x_train.info()\n# x_test.info()\n# í•´ê²°ë°©ë²•(x_testì˜ ë³€ìˆ˜ê°€ ìˆ˜ê°€ ë” ë§ì€ ê²½ìš°ì˜ ì½”ë“œ)\n# x_train = x_train.reindex(columns = x_test.columns, fill_value=0)\n# x_train.info()\n\n\n\në°ì´í„° ë¶„ë¦¬\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_train,\n                                                 y_train['target'],\n                                                 test_size = 0.2,\n                                                 stratify = y_train['target'],\n                                                 random_state = 2023)\nprint(x_train.shape)\nprint(x_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)\n\n(569, 15)\n(143, 15)\n(569,)\n(143,)"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_5.html#ëª¨ë¸ë§-ë°-ì„±ëŠ¥í‰ê°€-1",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_5.html#ëª¨ë¸ë§-ë°-ì„±ëŠ¥í‰ê°€-1",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(5)",
    "section": "âœ… 4. ëª¨ë¸ë§ ë° ì„±ëŠ¥í‰ê°€",
    "text": "âœ… 4. ëª¨ë¸ë§ ë° ì„±ëŠ¥í‰ê°€\n\n# ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ ì‚¬ìš© (ì°¸ê³ : íšŒê·€ëª¨ë¸ì€ RandomForestRegressor)\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(random_state = 2023)\nmodel.fit(x_train, y_train)\n\nRandomForestClassifier(random_state=2023)\n\n\n\n# ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\ny_pred = model.predict(x_val)\n\n\n# ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ (accuracy, f1 score, AUC ë“±)\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score\nacc = accuracy_score(y_val, y_pred)  # (ì‹¤ì œê°’, ì˜ˆì¸¡ê°’)\nf1 = f1_score(y_val, y_pred)         # (ì‹¤ì œê°’, ì˜ˆì¸¡ê°’)\nauc = roc_auc_score(y_val, y_pred)   # (ì‹¤ì œê°’, ì˜ˆì¸¡ê°’)\n\n\nprint(acc) # ì •í™•ë„(Accuracy)\nprint(f1) # f1 score\nprint(auc) # AUC\n\n0.8531468531468531\n0.8108108108108109\n0.8465909090909092\n\n\n\n# ì°¸ê³ ì‚¬í•­\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_val, y_pred)\nprint(cm)\n\n# ####    ì˜ˆì¸¡\n# ####   0  1\n# ì‹¤ì œ 0 TN FP\n# ì‹¤ì œ 1 FN TP\n\n[[77 11]\n [10 45]]\n\n\n\nì‹¤ì œ test ì…‹ìœ¼ë¡œ ì„±ëŠ¥í‰ê°€ë¥¼ í•œë‹¤ë©´?\n\n# ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\ny_pred_f = model.predict(x_test)\n# ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ (ì •í™•ë„, F1 score, AUC)\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\nacc_f = accuracy_score(y_test, y_pred_f) # (ì‹¤ì œê°’, ì˜ˆì¸¡ê°’)\nf1_f = f1_score(y_test, y_pred_f) # (ì‹¤ì œê°’, ì˜ˆì¸¡ê°’)\nauc_f = roc_auc_score(y_test, y_pred_f) # (ì‹¤ì œê°’,ì˜ˆì¸¡ê°’)\n\n\nprint(acc_f) # ì •í™•ë„(Accuracy)\nprint(f1_f) #f1 score\nprint(auc_f) #AUC\n\n0.7821229050279329\n0.7153284671532847\n0.7687088274044797"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_5.html#ì˜ˆì¸¡ê°’-ì œì¶œ-1",
    "href": "BigData_Analysis/ì‹¤ê¸°_2ìœ í˜•_5.html#ì˜ˆì¸¡ê°’-ì œì¶œ-1",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 2ìœ í˜• ë¬¸ì œí’€ì´(5)",
    "section": "âœ… 5. ì˜ˆì¸¡ê°’ ì œì¶œ",
    "text": "âœ… 5. ì˜ˆì¸¡ê°’ ì œì¶œ\n\n(ì£¼ì˜) x_test ë¥¼ ëª¨ë¸ì— ë„£ì–´ ë‚˜ì˜¨ ì˜ˆì¸¡ê°’ì„ ì œì¶œí•´ì•¼ í•¨\n\n#(ì‹¤ê¸°ì‹œí—˜ ì•ˆë‚´ì‚¬í•­)\n# ì•„ë˜ ì½”ë“œ ì˜ˆì¸¡ë³€ìˆ˜ì™€ ìˆ˜í—˜ë²ˆí˜¸ë¥¼ ê°œì¸ë³„ë¡œ ë³€ê²½í•˜ì—¬ í™œìš©\n# pd.DataFrame({'cust_id':cust_id, 'target':y_result}).to_csv('0030000.csv', index=False)\n\n# ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\n\n# 1. íŠ¹ì • í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜í•  ê²½ìš° (predict)\ny_result = model.predict(x_test)\nprint(y_result[:5])\n\n# 2. íŠ¹ì • í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜ë  í™•ë¥ ì„ êµ¬í•  ê²½ìš° (predict_proba)\ny_result_prob = model.predict_proba(x_test)\nprint(y_result_prob[:5])\n\n# ì´í•´í•´ë³´ê¸°\nresult_prob = pd.DataFrame({\n    'result' : y_result,\n    'prob_0' : y_result_prob[:,0]\n})\n# setosa ì¼ í™•ë¥  : y_result_prob[:, 0]\n# ê·¸ ì™¸ ì¢…ì¼ í™•ë¥  : y_result_prob[:, 1]\n\nprint(result_prob[:5])\n\n[1 1 0 0 0]\n[[0.32 0.68]\n [0.24 0.76]\n [1.   0.  ]\n [0.93 0.07]\n [0.93 0.07]]\n   result  prob_0\n0       1    0.32\n1       1    0.24\n2       0    1.00\n3       0    0.93\n4       0    0.93\n\n\n\n# â˜…tip : ë°ì´í„°ë¥¼ ì €ì¥í•œ ë‹¤ìŒ ë¶ˆëŸ¬ì™€ì„œ ì œëŒ€ë¡œ ì œì¶œí–ˆëŠ”ì§€ í™•ì¸í•´ë³´ì\n# pd.DataFrame({'result':y_result}).to_csv('ìˆ˜í—˜ë²ˆí˜¸.csv', index=False)\n# df2 = pd.read_csv(\"ìˆ˜í—˜ë²ˆí˜¸.csv\")\n# print(df2.head())"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_1.html",
    "href": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_1.html",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 3ìœ í˜• ë¬¸ì œí’€ì´(1)",
    "section": "",
    "text": "ì‹¤ê¸° 3 ìœ í˜•(1)"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_1.html#ê²€ì •ë°©ë²•",
    "href": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_1.html#ê²€ì •ë°©ë²•",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 3ìœ í˜• ë¬¸ì œí’€ì´(1)",
    "section": "âœ… ê²€ì •ë°©ë²•",
    "text": "âœ… ê²€ì •ë°©ë²•\n\n1) (ì •ê·œì„±o) ë‹¨ì¼í‘œë³¸ tê²€ì •(1sample t-test)\n\n\n2) (ì •ê·œì„±x) ìœŒì½•ìŠ¨ ë¶€í˜¸ìˆœìœ„ ê²€ì •"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_1.html#ê°€ì„¤ê²€ì •-ìˆœì„œì¤‘ìš”",
    "href": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_1.html#ê°€ì„¤ê²€ì •-ìˆœì„œì¤‘ìš”",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 3ìœ í˜• ë¬¸ì œí’€ì´(1)",
    "section": "âœ… ê°€ì„¤ê²€ì • ìˆœì„œ(ì¤‘ìš”!!)",
    "text": "âœ… ê°€ì„¤ê²€ì • ìˆœì„œ(ì¤‘ìš”!!)\n\n\nê°€ì„¤ê²€ì •\nìœ ì˜ìˆ˜ì¤€ í™•ì¸\nì •ê·œì„± ê²€ì •\nê²€ì •ì‹¤ì‹œ(í†µê³„ëŸ‰, p-value í™•ì¸)\nê·€ë¬´ê°€ì„¤ ê¸°ê°ì—¬ë¶€ ê²°ì •(ì±„íƒ/ê¸°ê°)"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_1.html#ë°ì´í„°-ë¶ˆëŸ¬ì˜¤ê¸°",
    "href": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_1.html#ë°ì´í„°-ë¶ˆëŸ¬ì˜¤ê¸°",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 3ìœ í˜• ë¬¸ì œí’€ì´(1)",
    "section": "âœ… ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°",
    "text": "âœ… ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n\nimport pandas as pd\nimport numpy as np\n\n\n# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° mtcars\ndf = pd.read_csv('mtcars.txt')\ndf.head(3)\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n  \n\n\n\n\n\nì˜ˆì œë¬¸ì œ\n\n\n1. mtcars ë°ì´í„°ì…‹ì˜ mpgì—´ ë°ì´í„°ì˜ í‰ê· ì´ 20ê³¼ ê°™ë‹¤ê³  í•  ìˆ˜ ìˆëŠ”ì§€ ê²€ì •í•˜ì‹œì˜¤. (ìœ ì˜ìˆ˜ì¤€ 5%)\n\nimport scipy.stats as stats\nfrom scipy.stats import shapiro\n\n\n# 1. ê°€ì„¤ê²€ì •\n# H_0 : mpg ì—´ì˜ í‰ê· ì´ 20ê³¼ ê°™ë‹¤\n# H_1 : mpg ì—´ì˜ í‰ê· ì´ 20ê³¼ ê°™ì§€ ì•Šë‹¤\n\n\n# 2. ìœ ì˜ìˆ˜ì¤€ í™•ì¸ : ìœ ì˜ìˆ˜ì¤€ 5%ë¡œ í™•ì¸\n\n\n# 3. ì •ê·œì„± ê²€ì •\n# H_0(ê·€ë¬´ê°€ì„¤) : ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤\n# H_1(ëŒ€ë¦½ê°€ì„¤) : ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•ŠëŠ”ë‹¤\nstatistic, pvalue = stats.shapiro(df['mpg'])\nprint(round(statistic, 4), round(pvalue, 4))\n\nresult = stats.shapiro(df['mpg'])\nprint(result)\n\n0.9476 0.1229\nShapiroResult(statistic=0.9475648403167725, pvalue=0.1228824257850647)\n\n\n\np-value ê°’ì´ ìœ ì˜ìˆ˜ì¤€(0.05) ë³´ë‹¤ í¬ë‹¤. -> ê·€ë¬´ê°€ì„¤(H_0) ì±„íƒ\n(ë§Œì•½ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•ŠëŠ”ë‹¤ë©´ ë¹„ëª¨ìˆ˜ ê²€ì •ë°©ë²•ì„ ì¨ì•¼ í•¨(ìœŒì½•ìŠ¨ì˜ ë¶€í˜¸ìˆœìœ„ ê²€ì •)\n\n\n# 4.1 (ì •ê·œì„± ë§Œì¡± o) t-ê²€ì • ì‹¤ì‹œ\nstatistic, pvalue = stats.ttest_1samp(df['mpg'], popmean=20,\n                                      alternative='two-sided') # default: two-sided\n                                      # H_1 : ì™¼ìª½ê°’ì´ ì˜¤ë¥¸ìª½ ê°’ê³¼ ê°™ì§€ ì•Šë‹¤\nprint(round(statistic, 4), round(pvalue, 4))\n# alternative (ëŒ€ë¦½ê°€ì„¤: H_1) ì˜µì…˜ : 'two-sided', 'greater', 'less'\n\n0.0851 0.9328\n\n\n\n# 4.2 (ì •ê·œì„± ë§Œì¡± x) Wilcoxon ë¶€í˜¸ìˆœìœ„ ê²€ì •\nstatistic, pvalue = stats.wilcoxon(df['mpg']-20,alternative='two-sided') \nprint(round(statistic, 4), round(pvalue, 4))\n\n249.0 0.7891\n\n\n\n# 5. ê·€ë¬´ê°€ì„¤ ê¸°ê°ì—¬ë¶€ ê²°ì •(ì±„íƒ/ê¸°ê°)\n# p-value ê°’ì´ 0.05ë³´ë‹¤ í¬ê¸° ë•Œë¬¸ì—(0.9328) ê·€ë¬´ê°€ì„¤ì„ ì±„íƒí•œë‹¤\n# ì¦‰, mpg ì—´ì˜ í‰ê· ì´ 20ê³¼ ê°™ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤\n\n# ë‹µ : ì±„íƒ\n\n\n# ì‹¤ì œë¡œ í‰ê· ì„ êµ¬í•´ë³´ë©´\ndf['mpg'].mean()\n\n20.090624999999996\n\n\n\n\n2. mtcars ë°ì´í„°ì…‹ì˜ mpg ì—´ ë°ì´í„°ì˜ í‰ê· ì´ 17ë³´ë‹¤ í¬ë‹¤ê³  í•  ìˆ˜ ìˆëŠ”ì§€ ê²€ì •í•˜ì‹œì˜¤. (ìœ ì˜ìˆ˜ì¤€ 5%)\n\n# 1. ê°€ì„¤ê²€ì •\n# H_0 : mpg ì—´ì˜ í‰ê· ì´ 17ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ë‹¤(mpg mean <= 17)\n# H_1 : mpg ì—´ì˜ í‰ê· ì´ 17ë³´ë‹¤ í¬ë‹¤(mpg mean >17)\n\n\n# 2. ìœ ì˜ìˆ˜ì¤€ í™•ì¸ : ìœ ì˜ìˆ˜ì¤€ 5%ë¡œ í™•ì¸\n\n\n# 3. ì •ê·œì„± ê²€ì •\n# H_0(ê·€ë¬´ê°€ì„¤) : ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤\n# H_1(ëŒ€ë¦½ê°€ì„¤) : ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•ŠëŠ”ë‹¤\nstatistic, pvalue = stats.shapiro(df['mpg'])\nprint(round(statistic, 4), round(pvalue, 4))\n\n0.9476 0.1229\n\n\n\n# 4.1 (ì •ê·œì„± ë§Œì¡± o) t-ê²€ì • ì‹¤ì‹œ\nstatistic, pvalue = stats.ttest_1samp(df['mpg'], popmean=17,\n                                      alternative='greater') # alternative(ëŒ€ë¦½ê°€ì„¤)\n# H_1: ì™¼ìª½ê°’(df['mpg'].mean)ì´ ì˜¤ë¥¸ìª½ê°’(popmean)ë³´ë‹¤ í¬ë‹¤\nprint(round(statistic, 4), round(pvalue, 4))\n\n2.9008 0.0034\n\n\n\n# 4.2 (ì •ê·œì„± ë§Œì¡± x) Wilcoxon ë¶€í˜¸ìˆœìœ„ ê²€ì •\nstatistic, pvalue = stats.wilcoxon(df['mpg']-17,alternative='greater') \nprint(round(statistic, 4), round(pvalue, 4))\n\n395.5 0.0066\n\n\n\n# 5. ê·€ë¬´ê°€ì„¤ ê¸°ê°ì—¬ë¶€ ê²°ì •(ì±„íƒ/ê¸°ê°)\n# p-value ê°’ì´ 0.05ë³´ë‹¤ ì‘ê¸° ë•Œë¬¸ì—(0.0034) ê·€ë¬´ê°€ì„¤ì„ ê¸°ê°í•œë‹¤(ëŒ€ë¦½ê°€ì„¤ ì±„íƒ)\n# ì¦‰, mpg ì—´ì˜ í‰ê· ì´ 17ë³´ë‹¤ í¬ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤\n\n# ë‹µ : ê¸°ê°\n\n\n\n3. mtcars ë°ì´í„°ì…‹ì˜ mpg ì—´ ë°ì´í„°ì˜ í‰ê· ì´ 17ë³´ë‹¤ ì‘ë‹¤ê³  í•  ìˆ˜ ìˆëŠ”ì§€ ê²€ì •í•˜ì‹œì˜¤. (ìœ ì˜ìˆ˜ì¤€ 5%)\n\n# 1. ê°€ì„¤ê²€ì •\n# H_0 : mpg ì—´ì˜ í‰ê· ì´ 17ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ë‹¤(mpg mean >= 17)\n# H_1 : mpg ì—´ì˜ í‰ê· ì´ 17ë³´ë‹¤ ì‘ë‹¤(mpg mean <17)\n\n\n# 2. ìœ ì˜ìˆ˜ì¤€ í™•ì¸ : ìœ ì˜ìˆ˜ì¤€ 5%ë¡œ í™•ì¸\n\n\n# 3. ì •ê·œì„± ê²€ì •\nstatistic, pvalue = stats.shapiro(df['mpg'])\nprint(round(statistic, 4), round(pvalue, 4))\n\n0.9476 0.1229\n\n\n\n# 4.1 (ì •ê·œì„± ë§Œì¡± o) t-ê²€ì • ì‹¤ì‹œ\nstatistic, pvalue = stats.ttest_1samp(df['mpg'], popmean=17,\n                                      alternative='less') # alternative(ëŒ€ë¦½ê°€ì„¤)\n# H_1: ì™¼ìª½ê°’(df['mpg'].mean)ì´ ì˜¤ë¥¸ìª½ê°’(popmean)ë³´ë‹¤ ì‘ë‹¤\nprint(round(statistic, 4), round(pvalue, 4))\n\n2.9008 0.9966\n\n\n\n# 4.2 (ì •ê·œì„± ë§Œì¡± x) Wilcoxon ë¶€í˜¸ìˆœìœ„ ê²€ì •\nstatistic, pvalue = stats.wilcoxon(df['mpg']-17,alternative='less') \nprint(round(statistic, 4), round(pvalue, 4))\n\n395.5 0.9938\n\n\n\n# 5. ê·€ë¬´ê°€ì„¤ ê¸°ê°ì—¬ë¶€ ê²°ì •(ì±„íƒ/ê¸°ê°)\n# p-value ê°’ì´ 0.05ë³´ë‹¤ í¬ê¸° ë•Œë¬¸ì—(0.9966) ê·€ë¬´ê°€ì„¤ì„ ì±„íƒí•œë‹¤\n# ì¦‰, mpg ì—´ì˜ í‰ê· ì´ 17ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤\n\n# ë‹µ : ì±„íƒ"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_2.html",
    "href": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_2.html",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 3ìœ í˜• ë¬¸ì œí’€ì´(2)",
    "section": "",
    "text": "ì‹¤ê¸° 3 ìœ í˜•(2)"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_2.html#ê²€ì •ë°©ë²•",
    "href": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_2.html#ê²€ì •ë°©ë²•",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 3ìœ í˜• ë¬¸ì œí’€ì´(2)",
    "section": "âœ… ê²€ì •ë°©ë²•",
    "text": "âœ… ê²€ì •ë°©ë²•"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_2.html#ëŒ€ì‘í‘œë³¸ìŒì²´-ë™ì¼í•œ-ê°ì²´ì˜-ì „-vs-í›„-í‰ê· ë¹„êµ",
    "href": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_2.html#ëŒ€ì‘í‘œë³¸ìŒì²´-ë™ì¼í•œ-ê°ì²´ì˜-ì „-vs-í›„-í‰ê· ë¹„êµ",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 3ìœ í˜• ë¬¸ì œí’€ì´(2)",
    "section": "1. ëŒ€ì‘í‘œë³¸(ìŒì²´) : ë™ì¼í•œ ê°ì²´ì˜ ì „ vs í›„ í‰ê· ë¹„êµ",
    "text": "1. ëŒ€ì‘í‘œë³¸(ìŒì²´) : ë™ì¼í•œ ê°ì²´ì˜ ì „ vs í›„ í‰ê· ë¹„êµ\n\n(ì •ê·œì„±o) ëŒ€ì‘í‘œë³¸(ìŒì²´) tê²€ì •(paired t-test) : ë™ì¼í•œ ê°ì²´ì˜ ì „ vs í›„ í‰ê· ë¹„êµ\n(ì •ê·œì„±x) ìœŒì½•ìŠ¨ ë¶€í˜¸ìˆœìœ„ ê²€ì •(wilcoxon)"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_2.html#ë…ë¦½í‘œë³¸-a-ì§‘ë‹¨ì˜-í‰ê· -vs-b-ì§‘ë‹¨ì˜-í‰ê· ",
    "href": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_2.html#ë…ë¦½í‘œë³¸-a-ì§‘ë‹¨ì˜-í‰ê· -vs-b-ì§‘ë‹¨ì˜-í‰ê· ",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 3ìœ í˜• ë¬¸ì œí’€ì´(2)",
    "section": "2. ë…ë¦½í‘œë³¸ : A ì§‘ë‹¨ì˜ í‰ê·  vs B ì§‘ë‹¨ì˜ í‰ê· ",
    "text": "2. ë…ë¦½í‘œë³¸ : A ì§‘ë‹¨ì˜ í‰ê·  vs B ì§‘ë‹¨ì˜ í‰ê· \n\n(ì •ê·œì„±o) ë…ë¦½í‘œë³¸ tê²€ì •(2sample t-test)\n(ì •ê·œì„±x) ìœŒì½•ìŠ¨ ìˆœìœ„í•© ê²€ì •(ranksums)"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_2.html#ê°€ì„¤ê²€ì •-ìˆœì„œì¤‘ìš”",
    "href": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_2.html#ê°€ì„¤ê²€ì •-ìˆœì„œì¤‘ìš”",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 3ìœ í˜• ë¬¸ì œí’€ì´(2)",
    "section": "âœ… ê°€ì„¤ê²€ì • ìˆœì„œ(ì¤‘ìš”!!)",
    "text": "âœ… ê°€ì„¤ê²€ì • ìˆœì„œ(ì¤‘ìš”!!)\n\n1. ëŒ€ì‘í‘œë³¸(ìŒì²´) tê²€ì •(paired t-test)\n\n\nê°€ì„¤ê²€ì •\nìœ ì˜ìˆ˜ì¤€ í™•ì¸\nì •ê·œì„± ê²€ì • (ì£¼ì˜) ì°¨ì´ê°’ì— ëŒ€í•œ ì •ê·œì„±\nê²€ì •ì‹¤ì‹œ(í†µê³„ëŸ‰, p-value í™•ì¸)\nê·€ë¬´ê°€ì„¤ ê¸°ê°ì—¬ë¶€ ê²°ì •(ì±„íƒ/ê¸°ê°)\n\n\n\n\n2. ë…ë¦½í‘œë³¸ tê²€ì •(2sample t-test)\n\n\nê°€ì„¤ê²€ì •\nìœ ì˜ìˆ˜ì¤€ í™•ì¸\nì •ê·œì„± ê²€ì • (ì£¼ì˜) ë‘ ì§‘ë‹¨ ëª¨ë‘ ì •ê·œì„±ì„ ë”°ë¥¼ ê²½ìš°\në“±ë¶„ì‚° ê²€ì •\nê²€ì •ì‹¤ì‹œ(í†µê³„ëŸ‰, p-value í™•ì¸)\nê·€ë¬´ê°€ì„¤ ê¸°ê°ì—¬ë¶€ ê²°ì •(ì±„íƒ/ê¸°ê°)"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_2.html#ì˜ˆì œë¬¸ì œ",
    "href": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_2.html#ì˜ˆì œë¬¸ì œ",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 3ìœ í˜• ë¬¸ì œí’€ì´(2)",
    "section": "âœ… ì˜ˆì œë¬¸ì œ",
    "text": "âœ… ì˜ˆì œë¬¸ì œ\n\nCase 1) ëŒ€ì‘í‘œë³¸(ìŒì²´) t ê²€ì •(paired t-test)\në¬¸ì œ 1-1 ë‹¤ìŒì€ í˜ˆì••ì•½ì„ ë¨¹ê¸° ì „, í›„ì˜ í˜ˆì•• ë°ì´í„°ì´ë‹¤.\ní˜ˆì••ì•½ì„ ë¨¹ê¸° ì „, í›„ì˜ ì°¨ì´ê°€ ìˆëŠ”ì§€ ìŒì²´ tê²€ì •ì„ ì‹¤ì‹œí•˜ì‹œì˜¤.\n(ìœ ì˜ìˆ˜ì¤€ 5%)\n\nbefore : í˜ˆì••ì•½ì„ ë¨¹ê¸°ì „ í˜ˆì••, after : í˜ˆì••ì•½ì„ ë¨¹ì€ í›„ í˜ˆì••\nH_0(ê·€ë¬´ê°€ì„¤) : after - before = 0\nH_1(ëŒ€ë¦½ê°€ì„¤) : after - before != 0\n\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom scipy.stats import shapiro\n\n\n# ë°ì´í„° ë§Œë“¤ê¸°\ndf = pd.DataFrame( {\n    'before': [120, 135, 122, 124, 135, 122, 145, 160, 155, 142, 144, 135, 167],\n    'after' : [110, 132, 123, 119, 123, 115, 140, 162, 142, 138, 135, 142, 160] })\n\n\n# 1. ê°€ì„¤ê²€ì •\n# H0 : ì•½ì„ ë¨¹ê¸°ì „ê³¼ ë¨¹ì€ í›„ì˜ í˜ˆì•• í‰ê· ì€ ê°™ë‹¤(íš¨ê³¼ê°€ ì—†ë‹¤)\n# H1 : ì•½ì„ ë¨¹ê¸°ì „ê³¼ ë¨¹ì€ í›„ì˜ í˜ˆì•• í‰ê· ì€ ê°™ì§€ ì•Šë‹¤(íš¨ê³¼ê°€ ìˆë‹¤)\n\n\n# 2. ìœ ì˜ìˆ˜ì¤€ í™•ì¸ : ìœ ì˜ìˆ˜ì¤€ 5%ë¡œ í™•ì¸\n\n\n# 3. ì •ê·œì„± ê²€ì • (ì°¨ì´ê°’ì— ëŒ€í•´ ì •ê·œì„± í™•ì¸)\nstatistic, pvalue = stats.shapiro(df['after'] - df['before'])\nprint(round(statistic,4), round(pvalue,4))\n\n0.9589 0.7363\n\n\n\np-value ê°’(0.7363)ì´ ìœ ì˜ìˆ˜ì¤€ (0.05)ë³´ë‹¤ í¬ë‹¤.\nê·€ë¬´ê°€ì„¤ (H0) ì±„íƒ(ì •ê·œì„±ê²€ì •ì˜ H0 : ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤)\n\n\n# 4.1 (ì •ê·œì„±o) ëŒ€ì‘í‘œë³¸(ìŒì²´) tê²€ì •(paired t-test)\nstatistic, pvalue = stats.ttest_rel(df['after'], df['before']\n                                    , alternative='two-sided')\nprint(round(statistic,4), round(pvalue,4))\n\n-3.1382 0.0086\n\n\n\n# 4.2 (ì •ê·œì„±x) wilcoxon ë¶€í˜¸ìˆœìœ„ ê²€ì •\nstatistic, pvalue = stats.wilcoxon(df['after']-df['before'],\n                                  alternative='two-sided')\nprint(round(statistic,4), round(pvalue,4))\n\n11.0 0.0134\n\n\n\n# 5. ê·€ë¬´ê°€ì„¤ ê¸°ê° ì—¬ë¶€ ê²°ì •(ì±„íƒ/ê¸°ê°)\n# p-value ê°’(0.0086)ì´ 0.05ë³´ë‹¤ ì‘ê¸° ë–„ë¬¸ì— ê·€ë¬´ê°€ì„¤ì„ ê¸°ê°í•œë‹¤\n# ì¦‰, ì•½ì„ ë¨¹ê¸° ì „ê³¼ ë¨¹ì€ í›„ì˜ í˜ˆì••ì€ ê°™ì§€ ì•Šë‹¤(íš¨ê³¼ê°€ ìˆë‹¤)\n\n# ë‹µ : ê¸°ê°(H1)\n\nì°¨ì´ê°€ ìˆëŠ”ì§€ ì—†ëŠ”ì§€ í™•ì¸ì€ ì–‘ì¸¡ê²€ì • // ì¦ê°€, ê°ì†Œë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì€ ë‹¨ì¸¡ê²€ì •\në¬¸ì œ 1-2\në‹¤ìŒì€ í˜ˆì••ì•½ì„ ë¨¹ê¸° ì „, í›„ì˜ í˜ˆì•• ë°ì´í„°ì´ë‹¤.\ní˜ˆì••ì•½ì„ ë¨¹ì€ í›„ í˜ˆì••ì´ ê°ì†Œí–ˆëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ìŒì²´ t ê²€ì •ì„ ì‹¤ì‹œí•˜ì‹œì˜¤\n(ìœ ì˜ìˆ˜ì¤€ 5%)\n\nbefore : í˜ˆì••ì•½ì„ ë¨¹ê¸°ì „ í˜ˆì••, after : í˜ˆì••ì•½ì„ ë¨¹ì€ í›„ì˜ í˜ˆì••\nH0(ê·€ë¬´ê°€ì„¤) : after - before >=0\nH1(ëŒ€ë¦½ê°€ì„¤) : after - before < 0\n\n\n# ë°ì´í„° ë§Œë“¤ê¸°\ndf = pd.DataFrame( {\n    'before': [120, 135, 122, 124, 135, 122, 145, 160, 155, 142, 144, 135, 167],\n    'after' : [110, 132, 123, 119, 123, 115, 140, 162, 142, 138, 135, 142, 160] })\nprint(df.head(3))\n\n   before  after\n0     120    110\n1     135    132\n2     122    123\n\n\n\n# 1. ê°€ì„¤ê²€ì •\n# H0 : ì•½ì„ ë¨¹ì€ í›„ í˜ˆì••ì´ ê°™ê±°ë‚˜ ì¦ê°€í–ˆë‹¤. (after - before >= 0)\n# H1 : ì•½ì„ ë¨¹ì€ í›„ í˜ˆì••ì´ ê°ì†Œí–ˆë‹¤.        (after - before <  0) \n\n\n# 2. ìœ ì˜ìˆ˜ì¤€ í™•ì¸ : ìœ ì˜ìˆ˜ì¤€ 5%ë¡œ í™•ì¸\n\n\nstatistic, pvalue = stats.shapiro(df['after']-df['before'])\nprint(round(statistic, 4), round(pvalue ,4))\n\n0.9589 0.7363\n\n\np-valueê°€ 0.05ë³´ë‹¤ í¬ë‹¤ ê·€ë¬´ê°€ì„¤ ì±„íƒ\n\n# 4.1 (ì •ê·œì„±o) ëŒ€ì‘í‘œë³¸(ìŒì²´) tê²€ì •(paired t-test)\nstatistic, pvalue = stats.ttest_rel(df['after'], df['before']\n                                    , alternative='less')\nprint(round(statistic,4), round(pvalue,4))\n\n-3.1382 0.0043\n\n\n\n# 4.2 (ì •ê·œì„±x) wilcoxon ë¶€í˜¸ìˆœìœ„ ê²€ì •\nstatistic, pvalue = stats.wilcoxon(df['after']-df['before'],\n                                  alternative='less')\nprint(round(statistic,4), round(pvalue,4))\n\n11.0 0.0067\n\n\n\n# 5. ê·€ë¬´ê°€ì„¤ ê¸°ê° ì—¬ë¶€ ê²°ì •(ì±„íƒ/ê¸°ê°)\n# p-value ê°’(0.0043)ì´ 0.05ë³´ë‹¤ ì‘ê¸° ë–„ë¬¸ì— ê·€ë¬´ê°€ì„¤ì„ ê¸°ê°í•œë‹¤\n# ì¦‰, ì•½ì„ ë¨¹ì€ í›„ í˜ˆì••ì´ ê°ì†Œí–ˆë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.(íš¨ê³¼ê°€ ìˆë‹¤)\n\n# ë‹µ : ê¸°ê°(H1)\n\n\n\nCase 2) ë…ë¦½í‘œë³¸ tê²€ì •(2sample t-test)\në¬¸ì œ 2-1\në‹¤ìŒì€ Aê·¸ë£¹ê³¼ Bê·¸ë£¹ ì¸ì›ì˜ í˜ˆì•• ë°ì´í„°ì´ë‹¤.\në‘ ê·¸ë£¹ì˜ í˜ˆì••í‰ê· ì´ ë‹¤ë¥´ë‹¤ê³  í•  ìˆ˜ ìˆëŠ”ì§€ ë…ë¦½í‘œë³¸ tê²€ì •ì„ ì‹¤ì‹œí•˜ì‹œì˜¤.\n(ìœ ì˜ìˆ˜ì¤€ 5%)\n\nA : Aê·¸ë£¹ ì¸ì›ì˜ í˜ˆì••, B : Bê·¸ë£¹ ì¸ì›ì˜ í˜ˆì••\nH0(ê·€ë¬´ê°€ì„¤) : A = B\nH1(ëŒ€ë¦½ê°€ì„¤) : A != B\n\n\n# ë°ì´í„° ë§Œë“¤ê¸°\ndf = pd.DataFrame( {\n    'A': [120, 135, 122, 124, 135, 122, 145, 160, 155, 142, 144, 135, 167],\n    'B' : [110, 132, 123, 119, 123, 115, 140, 162, 142, 138, 135, 142, 160] })\nprint(df.head(3))\n\n     A    B\n0  120  110\n1  135  132\n2  122  123\n\n\n\n# 1. ê°€ì„¤ê²€ì •\n# H0(ê·€ë¬´ê°€ì„¤) : Aê·¸ë£¹ê³¼ Bê·¸ë£¹ì˜ í˜ˆì•• í‰ê· ì€ ê°™ë‹¤.      (A = B)\n# H1(ëŒ€ë¦½ê°€ì„¤) : Aê·¸ë£¹ê³¼ Bê·¸ë£¹ì˜ í˜ˆì•• í‰ê· ì€ ê°™ì§€ ì•Šë‹¤. (A != B)\n\n\n# 2. ìœ ì˜ìˆ˜ì¤€ í™•ì¸ : ìœ ì˜ìˆ˜ì¤€ 5%ë¡œ í™•ì¸\n\n\n# 3. ì •ê·œì„± ê²€ì •\n# H0(ê·€ë¬´ê°€ì„¤) : ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤.\n# H1(ëŒ€ë¦½ê°€ì„¤) : ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•ŠëŠ”ë‹¤.\n\nstatisticA, pvalueA = stats.shapiro(df['A'])\nstatisticB, pvalueB = stats.shapiro(df['B'])\nprint(round(statisticA, 4), round(pvalueA, 4))\nprint(round(statisticB, 4), round(pvalueB, 4))\n\n0.9314 0.3559\n0.9498 0.5956\n\n\n\np-value ê°’(0.3559, 0.5956)ì´ ìœ ì˜ìˆ˜ì¤€(0.05)ë³´ë‹¤ í¬ë‹¤.\nê·€ë¬´ê°€ì„¤(H0) ì±„íƒ\në§Œì•½ í•˜ë‚˜ë¼ë„ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•ŠëŠ”ë‹¤ë©´ ë¹„ëª¨ìˆ˜ ê²€ì •ë°©ë²•ì„ ì¨ì•¼ í•¨\n(ìœŒì½•ìŠ¨ì˜ ìˆœìœ„í•© ê²€ì • ranksums)\n\n\n# 4. ë“±ë¶„ì‚°ì„± ê²€ì •\n# H0(ê·€ë¬´ê°€ì„¤) : ë“±ë¶„ì‚° í•œë‹¤.\n# H1(ëŒ€ë¦½ê°€ì„¤) : ë“±ë¶„ì‚° í•˜ì§€ ì•ŠëŠ”ë‹¤.\nstatistic, pvalue = stats.bartlett(df['A'], df['B'])\nprint(round(statistic, 4), round(pvalue, 4))\n\n0.0279 0.8673\n\n\n\np-value ê°’ì´ ìœ ì˜ìˆ˜ì¤€(0.05) ë³´ë‹¤ í¬ë‹¤.\nê·€ë¬´ê°€ì„¤(H0) ì±„íƒ => ë“±ë¶„ì‚°ì„±ì„ ë”°ë¥¸ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.\n\n\n# 5.1 (ì •ê·œì„±o, ë“±ë¶„ì‚°ì„± o/x) tê²€ì •\nstatistic, pvalue = stats.ttest_ind(df['A'], df['B'],\n                                   equal_var = True,\n                                   alternative='two-sided')\n# ë§Œì•½ ë“±ë¶„ì‚° í•˜ì§€ ì•Šìœ¼ë©´ Falseë¡œ ì„¤ì •\nprint(round(statistic, 4), round(pvalue, 4))\n\n0.8192 0.4207\n\n\n\n# 5.2 (ì •ê·œì„±x)ìœŒì½•ìŠ¨ì˜ ìˆœìœ„í•© ê²€ì •\nstatistic, pvalue = stats.ranksums(df['A'], df['B'],\n                                   alternative='two-sided')\nprint(round(statistic, 4), round(pvalue, 4))\n\n0.8462 0.3975\n\n\n\n# 6. ê·€ë¬´ê°€ì„¤ ê¸°ê°ì—¬ë¶€ ê²°ì •(ì±„íƒ/ê¸°ê°)\n# p-value ê°’ì´ 0.05ë³´ë‹¤ í¬ê¸° ë–„ë¬¸ì— ê·€ë¬´ê°€ì„¤ì„ ì±„íƒí•œë‹¤\n# ì¦‰, Aê·¸ë£¹ê³¼ Bê·¸ë£¹ì˜ í˜ˆì•• í‰ê· ì€ ê°™ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.\n\n# ë‹µ : ì±„íƒ(H0)\n\n\n# (ì°¸ê³ ) í‰ê· ë°ì´í„° í™•ì¸\nprint(round(df['A'].mean(), 4))\nprint(round(df['B'].mean(), 4))\n\n138.9231\n133.9231\n\n\në¬¸ì œ 2-2\në‹¤ìŒì€ Aê·¸ë£¹ê³¼ Bê·¸ë£¹ ì¸ì›ì˜ í˜ˆì•• ë°ì´í„°ì´ë‹¤.\nAê·¸ë£¹ì˜ í˜ˆì•• í‰ê· ì´ Bê·¸ë£¹ë³´ë‹¤ í¬ë‹¤ê³  í•  ìˆ˜ ìˆëŠ”ì§€ ë…ë¦½í‘œë³¸ tê²€ì •ì„ ì‹¤ì‹œí•˜ì‹œì˜¤.\n(ìœ ì˜ìˆ˜ì¤€ 5%)\n\nA : Aê·¸ë£¹ ì¸ì›ì˜ í˜ˆì••, B : Bê·¸ë£¹ ì¸ì›ì˜ í˜ˆì••\nH0(ê·€ë¬´ê°€ì„¤) : A - B <= 0 (or A <= B)\nH1(ëŒ€ë¦½ê°€ì„¤) : A - B > 0 (or A > B)\n\n\n# ë°ì´í„° ë§Œë“¤ê¸°\ndf = pd.DataFrame( {\n    'A': [120, 135, 122, 124, 135, 122, 145, 160, 155, 142, 144, 135, 167],\n    'B' : [110, 132, 123, 119, 123, 115, 140, 162, 142, 138, 135, 142, 160] })\nprint(df.head(3))\n\n     A    B\n0  120  110\n1  135  132\n2  122  123\n\n\n\n# 1. ê°€ì„¤ê²€ì •\n# H0(ê·€ë¬´ê°€ì„¤) : Aê·¸ë£¹ì˜ í˜ˆì•• í‰ê· ì´ Bê·¸ë£¹ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ë‹¤. (A <= B)\n# H1(ëŒ€ë¦½ê°€ì„¤) : Aê·¸ë£¹ì˜ í˜ˆì•• í‰ê· ì´ Bê·¸ë£¹ë³´ë‹¤ í¬ë‹¤.        (A >  B)\n\n\n# 2. ìœ ì˜ìˆ˜ì¤€ í™•ì¸ : ìœ ì˜ìˆ˜ì¤€ 5%ë¡œ í™•ì¸\n\n\n# 3. ì •ê·œì„± ê²€ì •(ì°¨ì´ê°’ì— ëŒ€í•´ ì •ê·œì„± í™•ì¸)\n# H0(ê·€ë¬´ê°€ì„¤) : ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤.\n# H1(ëŒ€ë¦½ê°€ì„¤) : ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•ŠëŠ”ë‹¤.\n\nstatisticA, pvalueA = stats.shapiro(df['A'])\nstatisticB, pvalueB = stats.shapiro(df['B'])\nprint(round(statisticA, 4), round(pvalueA, 4))\nprint(round(statisticB, 4), round(pvalueB, 4))\n\n0.9314 0.3559\n0.9498 0.5956\n\n\n\np-value ê°’(0.3559, 0.5956)ì´ ìœ ì˜ìˆ˜ì¤€(0.05)ë³´ë‹¤ í¬ë‹¤.\nê·€ë¬´ê°€ì„¤(H0) ì±„íƒ\në§Œì•½ í•˜ë‚˜ë¼ë„ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•ŠëŠ”ë‹¤ë©´ ë¹„ëª¨ìˆ˜ ê²€ì •ë°©ë²•ì„ ì¨ì•¼ í•¨\n(ìœŒì½•ìŠ¨ì˜ ìˆœìœ„í•© ê²€ì • ranksums)\n\n\n# 4. ë“±ë¶„ì‚°ì„± ê²€ì •\n# H0(ê·€ë¬´ê°€ì„¤) : ë“±ë¶„ì‚° í•œë‹¤.\n# H1(ëŒ€ë¦½ê°€ì„¤) : ë“±ë¶„ì‚° í•˜ì§€ ì•ŠëŠ”ë‹¤.\nstatistic, pvalue = stats.bartlett(df['A'], df['B'])\nprint(round(statistic, 4), round(pvalue, 4))\n\n0.0279 0.8673\n\n\n\n# 5.1 (ì •ê·œì„±o, ë“±ë¶„ì‚°ì„± o/x) tê²€ì •\nstatistic, pvalue = stats.ttest_ind(df['A'], df['B'],\n                                   equal_var = True,\n                                   alternative='greater')\n# ë§Œì•½ ë“±ë¶„ì‚° í•˜ì§€ ì•Šìœ¼ë©´ Falseë¡œ ì„¤ì •\nprint(round(statistic, 4), round(pvalue, 4))\n\n0.8192 0.2104\n\n\n\n# 5.2 (ì •ê·œì„±x)ìœŒì½•ìŠ¨ì˜ ìˆœìœ„í•© ê²€ì •\nstatistic, pvalue = stats.ranksums(df['A'], df['B'],\n                                   alternative='greater')\n# Aê·¸ë£¹ì˜ í‰ê· ê°’ì´ Bê·¸ë£¹ì˜ í‰ê· ê°’ë³´ë‹¤ í´ ìˆ˜ ìˆëŠ”ê°€ë¥¼ ëŒ€ë¦½ê°€ì„¤(H1)ìœ¼ë¡œ ì„¤ì •í–ˆê¸°ì— greater\nprint(round(statistic, 4), round(pvalue, 4))\n\n0.8462 0.1987\n\n\n\n# 6. ê·€ë¬´ê°€ì„¤ ê¸°ê°ì—¬ë¶€ ê²°ì •(ì±„íƒ/ê¸°ê°)\n# p-value ê°’ì´ 0.05ë³´ë‹¤ í¬ê¸° ë–„ë¬¸ì— ê·€ë¬´ê°€ì„¤ì„ ì±„íƒí•œë‹¤\n# ì¦‰, Aê·¸ë£¹ì˜ í˜ˆì•• í‰ê· ì´ Bê·¸ë£¹ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.\n# (Aê·¸ë£¹ì˜ í˜ˆì•• í‰ê· ì´ Bê·¸ë£¹ë³´ë‹¤ í¬ë‹¤ê³  í•  ìˆ˜ ì—†ë‹¤)\n\n# ë‹µ : ì±„íƒ(H0)"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_3.html",
    "href": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_3.html",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 3ìœ í˜• ë¬¸ì œí’€ì´(3)",
    "section": "",
    "text": "ì‹¤ê¸° 3 ìœ í˜•(3)"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_3.html#ê²€ì •ë°©ë²•",
    "href": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_3.html#ê²€ì •ë°©ë²•",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 3ìœ í˜• ë¬¸ì œí’€ì´(3)",
    "section": "âœ… ê²€ì •ë°©ë²•",
    "text": "âœ… ê²€ì •ë°©ë²•"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_3.html#ë¶„ì‚°ë¶„ì„anova-aì§‘ë‹¨-vs-bì§‘ë‹¨-vs-cì§‘ë‹¨-vs",
    "href": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_3.html#ë¶„ì‚°ë¶„ì„anova-aì§‘ë‹¨-vs-bì§‘ë‹¨-vs-cì§‘ë‹¨-vs",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 3ìœ í˜• ë¬¸ì œí’€ì´(3)",
    "section": "1. ë¶„ì‚°ë¶„ì„(ANOVA) : Aì§‘ë‹¨ vs Bì§‘ë‹¨ vs Cì§‘ë‹¨ vs â€¦",
    "text": "1. ë¶„ì‚°ë¶„ì„(ANOVA) : Aì§‘ë‹¨ vs Bì§‘ë‹¨ vs Cì§‘ë‹¨ vs â€¦\n\n(ì •ê·œì„±o) ANOVA ë¶„ì„\n(ì •ê·œì„±x) í¬ë£¨ìŠ¤ì¹¼-ì™ˆë¦¬ìŠ¤ ê²€ì •(kruskal-wallis test)"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_3.html#ê°€ì„¤ê²€ì •-ìˆœì„œì¤‘ìš”",
    "href": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_3.html#ê°€ì„¤ê²€ì •-ìˆœì„œì¤‘ìš”",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 3ìœ í˜• ë¬¸ì œí’€ì´(3)",
    "section": "âœ… ê°€ì„¤ê²€ì • ìˆœì„œ(ì¤‘ìš”!!)",
    "text": "âœ… ê°€ì„¤ê²€ì • ìˆœì„œ(ì¤‘ìš”!!)\n\n\nê°€ì„¤ê²€ì •\nìœ ì˜ìˆ˜ì¤€ í™•ì¸\nì •ê·œì„± ê²€ì • (ì£¼ì˜) ì§‘ë‹¨ ëª¨ë‘ ì •ê·œì„±ì„ ë”°ë¥¼ ê²½ìš°!\në“±ë¶„ì‚° ê²€ì •\nê²€ì •ì‹¤ì‹œ(í†µê³„ëŸ‰, p-value í™•ì¸) (ì£¼ì˜) ë“±ë¶„ì‚°ì—¬ë¶€ í™•ì¸\nê·€ë¬´ê°€ì„¤ ê¸°ê°ì—¬ë¶€ ê²°ì •(ì±„íƒ/ê¸°ê°)"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_3.html#ì˜ˆì œë¬¸ì œ",
    "href": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_3.html#ì˜ˆì œë¬¸ì œ",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 3ìœ í˜• ë¬¸ì œí’€ì´(3)",
    "section": "âœ… ì˜ˆì œë¬¸ì œ",
    "text": "âœ… ì˜ˆì œë¬¸ì œ\n\në¬¸ì œ 1-1\në‹¤ìŒì€ A, B, C ê·¸ë£¹ ì¸ì› ì„±ì  ë°ì´í„°ì´ë‹¤.\nì„¸ ê·¸ë£¹ì˜ ì„±ì  í‰ê· ì´ ê°™ë‹¤ê³  í•  ìˆ˜ ìˆëŠ”ì§€ ANOVA ë¶„ì„ì„ ì‹¤ì‹œí•˜ì‹œì˜¤.\n(ìœ ì˜ìˆ˜ì¤€ 5%)\n\nA, B, C : ê° ê·¸ë£¹ ì¸ì›ì˜ ì„±ì \nH0(ê·€ë¬´ê°€ì„¤) : A(í‰ê· ) = B(í‰ê· ) = C(í‰ê· )\nH1(ëŒ€ë¦½ê°€ì„¤) : Not H0 (ì ì–´ë„ í•˜ë‚˜ëŠ” ê°™ì§€ ì•Šë‹¤)\n\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom scipy.stats import shapiro\n\n\n# ë°ì´í„° ë§Œë“¤ê¸°\ndf = pd.DataFrame( {\n    'A': [120, 135, 122, 124, 135, 122, 145, 160, 155, 142, 144, 135, 167],\n    'B': [110, 132, 123, 119, 123, 115, 140, 162, 142, 138, 135, 142, 160],\n    'C': [130, 120, 115, 122, 133, 144, 122, 120, 110, 134, 125, 122, 122]})\nprint(df.head(3))\n\n     A    B    C\n0  120  110  130\n1  135  132  120\n2  122  123  115\n\n\n\n# 1. ê°€ì„¤ê²€ì •\n# H0 : ì„¸ ê·¸ë£¹ ì„±ì ì˜ í‰ê· ê°’ì´ ê°™ë‹¤. (A(í‰ê· ) = B(í‰ê· ) = C(í‰ê· ))\n# H1 : ì„¸ ê·¸ë£¹ì˜ ì„±ì  í‰ê· ê°’ì´ ì ì–´ë„ í•˜ë‚˜ëŠ” ê°™ì§€ ì•Šë‹¤. (not H0)\n\n\n# 2. ìœ ì˜ìˆ˜ì¤€ í™•ì¸ : ìœ ì˜ìˆ˜ì¤€ 5%ë¡œ í™•ì¸\n\n\n# 3. ì •ê·œì„± ê²€ì •\nprint(stats.shapiro(df['A']))\nprint(stats.shapiro(df['B']))\nprint(stats.shapiro(df['C']))\n\n# statistic, pvalue = stats.shapiro(df['A'])\n# print(round(statistic,4), round(pvalue,4))\n\nShapiroResult(statistic=0.9314376711845398, pvalue=0.35585272312164307)\nShapiroResult(statistic=0.9498201012611389, pvalue=0.5955665707588196)\nShapiroResult(statistic=0.9396706223487854, pvalue=0.45265132188796997)\n\n\n\nì„¸ ì§‘ë‹¨ ëª¨ë‘ p-value ê°’ì´ ìœ ì˜ìˆ˜ì¤€(0.05)ë³´ë‹¤ í¬ë‹¤\nê·€ë¬´ê°€ì„¤(H0) ì±„íƒ => ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.\në§Œì•½ í•˜ë‚˜ë¼ë„ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•ŠëŠ”ë‹¤ë©´ ë¹„ëª¨ìˆ˜ ê²€ì •ë°©ë²•ì„ ì¨ì•¼í•¨\n(í¬ë£¨ìŠ¤ì¹¼-ì™ˆë¦¬ìŠ¤ ê²€ì •)\n\n\n# 4. ë“±ë¶„ì‚°ì„± ê²€ì •\n# H0(ê·€ë¬´ê°€ì„¤) : ë“±ë¶„ì‚° í•œë‹¤\n# H1(ëŒ€ë¦½ê°€ì„¤) : ë“±ë¶„ì‚° í•˜ì§€ ì•ŠëŠ”ë‹¤\nprint(stats.bartlett(df['A'], df['B'], df['C']))\n\nBartlettResult(statistic=4.222248448848066, pvalue=0.12110174433684852)\n\n\n\np-value ê°’ì´ ìœ ì˜ìˆ˜ì¤€(0.05)ë³´ë‹¤ í¬ë‹¤\nê·€ë¬´ê°€ì„¤(H0) ì±„íƒ => ë“±ë¶„ì‚° í•œë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.\n\n\n# 5.1 (ì •ê·œì„±o, ë“±ë¶„ì‚°ì„± o) ë¶„ì‚°ë¶„ì„(F_oneway)\nimport scipy.stats as stats\nstatistic, pvalue = stats.f_oneway(df['A'], df['B'], df['C'])\n# ì£¼ì˜ : ë°ì´í„°ê°€ ê°ê° ë“¤ì–´ê°€ì•¼ í•¨(ë°‘ì— ì˜ˆì œì™€ ë¹„êµí•´ë³¼ ê²ƒ)\n\nprint(round(statistic,4), round(pvalue,4))\n\n3.6971 0.0346\n\n\n\n# 5.2 (ì •ê·œì„±o, ë“±ë¶„ì‚°ì„± x) Welch_ANOVA ë¶„ì„\n# import pingouin as pg     # pingouin íŒ¨í‚¤ì§€ ë¯¸ì§€ì›\n# pg.welch_anova(dv = \"ê·¸ë£¹ë³€ìˆ˜ëª…\", between=\"ì„±ì ë°ì´í„°\", data=ë°ì´í„°)\n# pg.welch_anova(df['A'], df['B'], df['C']) í˜•íƒœë¡œ ë¶„ì„ë¶ˆê°€\n\n\n# 5.3 (ì •ê·œì„±x, ë“±ë¶„ì‚°ì„± x) í¬ë£¨ìŠ¤ ì™ˆë¦¬ìŠ¤ ê²€ì •\nimport scipy.stats as stats\nstatistic, pvalue = stats.kruskal(df['A'], df['B'], df['C'])\n\nprint(round(statistic,4), round(pvalue,4))\n\n6.897 0.0318\n\n\n\n# 6. ê·€ë¬´ê°€ì„¤ ê¸°ê° ì—¬ë¶€ ê²°ì •(ì±„íƒ/ê¸°ê°)\n# p-value ê°’(0.0346)ì´ 0.05ë³´ë‹¤ ì‘ê¸° ë–„ë¬¸ì— ê·€ë¬´ê°€ì„¤ì„ ê¸°ê°í•œë‹¤\n# ì¦‰, A, B, C ê·¸ë£¹ì˜ ì„±ì  í‰ê· ì´ ê°™ë‹¤ê³  í•  ìˆ˜ ì—†ë‹¤.\n\n# ë‹µ : ê¸°ê°(H1)\n\n\n\në¬¸ì œ 1-2 ë°ì´í„° í˜•íƒœê°€ ë‹¤ë¥¸ ê²½ìš°\n\n# ë°ì´í„° ë§Œë“¤ê¸°\ndf2 = pd.DataFrame( {\n    'í•­ëª©': ['A','A','A','A','A','A','A','A','A','A','A','A','A',\n           'B','B','B','B','B','B','B','B','B','B','B','B','B',\n           'C','C','C','C','C','C','C','C','C','C','C','C','C',],\n    'value': [120, 135, 122, 124, 135, 122, 145, 160, 155, 142, 144, 135, 167,\n             110, 132, 123, 119, 123, 115, 140, 162, 142, 138, 135, 142, 160,\n             130, 120, 115, 122, 133, 144, 122, 120, 110, 134, 125, 122, 122]\n    })\nprint(df2.head(3))\n\n  í•­ëª©  value\n0  A    120\n1  A    135\n2  A    122\n\n\n\n# ê°ê° í•„í„°ë§í•´ì„œ ë³€ìˆ˜ëª…ì— ì €ì¥í•˜ê³  ë¶„ì„ ì§„í–‰\na = df2[ df2['í•­ëª©']=='A' ]['value']\nb = df2[ df2['í•­ëª©']=='B' ]['value']\nc = df2[ df2['í•­ëª©']=='C' ]['value']\n\n\n# ë¶„ì‚°ë¶„ì„(F_oneway)\nimport scipy.stats as stats\nstatistic, pvalue = stats.f_oneway(a, b, c)\nprint(round(statistic,4), round(pvalue,4))\n\n3.6971 0.0346"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_4.html",
    "href": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_4.html",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 3ìœ í˜• ë¬¸ì œí’€ì´(4)",
    "section": "",
    "text": "ì‹¤ê¸° 3 ìœ í˜•(4)"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_4.html#ë¶„ì„-case",
    "href": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_4.html#ë¶„ì„-case",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 3ìœ í˜• ë¬¸ì œí’€ì´(4)",
    "section": "âœ… ë¶„ì„ Case",
    "text": "âœ… ë¶„ì„ Case\n\nCase 1. ì í•©ë„ ê²€ì • - ê° ë²”ì£¼ì— ì†í•  í™•ë¥ ì´ ê°™ì€ì§€?\n\n\nCase 2. ë…ë¦½ì„± ê²€ì • - ë‘ê°œì˜ ë²”ì£¼í˜• ë³€ìˆ˜ê°€ ì„œë¡œ ë…ë¦½ì¸ì§€?"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_4.html#ê°€ì„¤ê²€ì •-ìˆœì„œì¤‘ìš”",
    "href": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_4.html#ê°€ì„¤ê²€ì •-ìˆœì„œì¤‘ìš”",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 3ìœ í˜• ë¬¸ì œí’€ì´(4)",
    "section": "âœ… ê°€ì„¤ê²€ì • ìˆœì„œ(ì¤‘ìš”!!)",
    "text": "âœ… ê°€ì„¤ê²€ì • ìˆœì„œ(ì¤‘ìš”!!)\n\n\nê°€ì„¤ê²€ì •\nìœ ì˜ìˆ˜ì¤€ í™•ì¸\nê²€ì •ì‹¤ì‹œ(í†µê³„ëŸ‰, p-value í™•ì¸)\nê·€ë¬´ê°€ì„¤ ê¸°ê°ì—¬ë¶€ ê²°ì •(ì±„íƒ/ê¸°ê°)"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_4.html#ì˜ˆì œë¬¸ì œ",
    "href": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_4.html#ì˜ˆì œë¬¸ì œ",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 3ìœ í˜• ë¬¸ì œí’€ì´(4)",
    "section": "âœ… ì˜ˆì œë¬¸ì œ",
    "text": "âœ… ì˜ˆì œë¬¸ì œ\n\nCase 1. ì í•©ë„ ê²€ì • - ê° ë²”ì£¼ì— ì†í•  í™•ë¥ ì´ ê°™ì€ì§€?\n\n\në¬¸ì œ 1-1\n\n\nëœë¤ ë°•ìŠ¤ì— ìƒí’ˆ A, B, C, Dê°€ ë“¤ì–´ìˆë‹¤.\n\n\në‹¤ìŒì€ ëœë¤ë°•ìŠ¤ì—ì„œ 100ë²ˆ ìƒí’ˆì„ êº¼ëƒˆì„ ë–„ì˜ ìƒí’ˆ ë°ì´í„°ë¼ê³  í•  ë•Œ\n\n\nìƒí’ˆì´ ë™ì¼í•œ ë¹„ìœ¨ë¡œ ë“¤ì–´ìˆë‹¤ê³  í•  ìˆ˜ ìˆëŠ”ì§€ ê²€ì •í•´ë³´ì‹œì˜¤.\n\nimport pandas as pd\nimport numpy as np\n\n\n# ë°ì´í„° ìƒì„±\nrow1 = [30,20,15,35]\ndf = pd.DataFrame([row1], columns=['A','B','C','D'])\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      30\n      20\n      15\n      35\n    \n  \n\n\n\n\n\n# 1. ê°€ì„¤ê²€ì •\n# H0 : ëœë¤ë°•ìŠ¤ì— ìƒí’ˆ A,B,C,Dê°€ ë™ì¼í•œ ë¹„ìœ¨ë¡œ ë“¤ì–´ìˆë‹¤.\n# H1 : ëœë¤ë°•ìŠ¤ì— ìƒí’ˆ A,B,C,Dê°€ ë™ì¼í•œ ë¹„ìœ¨ë¡œ ë“¤ì–´ìˆì§€ ì•Šë‹¤.\n\n\n# 2. ìœ ì˜ìˆ˜ì¤€ í™•ì¸ : ìœ ì˜ìˆ˜ì¤€ 5%ë¡œ í™•ì¸\n\n\n# 3. ê²€ì •ì‹¤ì‹œ(í†µê³„ëŸ‰, p-value)\nfrom scipy.stats import chisquare\n# chisquare(f_obs=f_obs, f_exp=f_exp)   # ê´€ì¸¡ê°’, ê¸°ëŒ€ê°’\n\n# ê´€ì¸¡ê°’ê³¼ ê¸°ëŒ€ê°’ êµ¬í•˜ê¸°\nf_obs = [30,20,15,35]\n# f_obs = df.iloc[0]\nf_exp = [25,25,25,25]\n\nstatistic, pvalue = chisquare(f_obs=f_obs, f_exp=f_exp)\nprint(statistic)\nprint(pvalue)\n\n10.0\n0.01856613546304325\n\n\n\n# 4. ê·€ë¬´ê°€ì„¤ ê¸°ê°ì—¬ë¶€ ê²°ì •(ì±„íƒ/ê¸°ê°)\n# p-value ê°’ì´ 0.05ë³´ë‹¤ ì‘ê¸° ë–„ë¬¸ì— ê·€ë¬´ê°€ì„¤ì„ ê¸°ê°í•œë‹¤.\n# ì¦‰, ëœë¤ë°•ìŠ¤ì— ìƒí’ˆ A,B,C,Dê°€ ë™ì¼í•œ ë¹„ìœ¨ë¡œ ë“¤ì–´ìˆì§€ ì•Šë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.\n\n# ë‹µ : ê¸°ê°\n\n\n\në¬¸ì œ 1-1\n\n\nëœë¤ ë°•ìŠ¤ì— ìƒí’ˆ A, B, Cê°€ ë“¤ì–´ìˆë‹¤.\n\n\në‹¤ìŒì€ ëœë¤ë°•ìŠ¤ì—ì„œ 150ë²ˆ ìƒí’ˆì„ êº¼ëƒˆì„ ë–„ì˜ ìƒí’ˆ ë°ì´í„°ë¼ê³  í•  ë•Œ\n\n\nìƒí’ˆë³„ë¡œ A 30%, B 15%, C 55% ë¹„ìœ¨ë¡œ ë“¤ì–´ìˆë‹¤ê³  í•  ìˆ˜ ìˆëŠ”ì§€ ê²€ì •í•´ë³´ì‹œì˜¤.\n\nimport pandas as pd\nimport numpy as np\n\n\n# ë°ì´í„° ìƒì„±\nrow1 = [50,25,75]\ndf = pd.DataFrame([row1], columns=['A','B','C'])\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n    \n  \n  \n    \n      0\n      50\n      25\n      75\n    \n  \n\n\n\n\n\n# 1. ê°€ì„¤ê²€ì •\n# H0 : ëœë¤ë°•ìŠ¤ì— ìƒí’ˆ A,B,Cê°€ 30%, 15%, 55%ì˜ ë¹„ìœ¨ë¡œ ë“¤ì–´ìˆë‹¤.\n# H1 : ëœë¤ë°•ìŠ¤ì— ìƒí’ˆ A,B,Cê°€ 30%, 15%, 55%ì˜ë¹„ìœ¨ë¡œ ë“¤ì–´ìˆì§€ ì•Šë‹¤.\n\n\n# 2. ìœ ì˜ìˆ˜ì¤€ í™•ì¸ : ìœ ì˜ìˆ˜ì¤€ 5%ë¡œ í™•ì¸\n\n\n# 3. ê²€ì •ì‹¤ì‹œ(í†µê³„ëŸ‰, p-value)\nfrom scipy.stats import chisquare\n# chisquare(f_obs=f_obs, f_exp=f_exp)   # ê´€ì¸¡ê°’, ê¸°ëŒ€ê°’\n\n# ê´€ì¸¡ê°’ê³¼ ê¸°ëŒ€ê°’ êµ¬í•˜ê¸°\nf_obs = [50,25,75]\n# f_obs = df.iloc[0]\n\na = 150*0.3\nb = 150*0.15\nc = 150*0.55\nf_exp = [a,b,c]\n\nstatistic, pvalue = chisquare(f_obs=f_obs, f_exp=f_exp)\nprint(statistic)\nprint(pvalue)\n\n1.5151515151515151\n0.46880153914023537\n\n\n\n# 4. ê·€ë¬´ê°€ì„¤ ê¸°ê°ì—¬ë¶€ ê²°ì •(ì±„íƒ/ê¸°ê°)\n# p-value ê°’ì´ 0.05ë³´ë‹¤ í¬ê¸° ë–„ë¬¸ì— ê·€ë¬´ê°€ì„¤ì„ ì±„íƒí•œë‹¤.\n# ì¦‰, ëœë¤ë°•ìŠ¤ì— ìƒí’ˆ A,B,Cê°€ 30%, 15%, 55%ì˜ ë¹„ìœ¨ë¡œ ë“¤ì–´ìˆë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.\n\n# ë‹µ : ì±„íƒ\n\n\n\nCase 2. ë…ë¦½ì„± ê²€ì • - ë‘ê°œì˜ ë²”ì£¼í˜• ë³€ìˆ˜ê°€ ì„œë¡œ ë…ë¦½ì¸ì§€?\n\n\në¬¸ì œ 2-1\n\n\nì—°ë ¹ëŒ€ì— ë”°ë¼ ë¨¹ëŠ” ì•„ì´ìŠ¤í¬ë¦¼ì˜ ì°¨ì´ê°€ ìˆëŠ”ì§€ ë…ë¦½ì„± ê²€ì •ì„ ì‹¤ì‹œí•˜ì‹œì˜¤.\n\nimport pandas as pd\nimport numpy as np\n\n\n# ë°ì´í„° ìƒì„±\nrow1, row2 = [200,190,250],[220,250,300]\ndf = pd.DataFrame([row1, row2], columns=['ë”¸ê¸°','ì´ˆì½”','ë°”ë‹ë¼']\n                  ,index=['10ëŒ€', '20ëŒ€'])\ndf\n\n\n\n\n\n  \n    \n      \n      ë”¸ê¸°\n      ì´ˆì½”\n      ë°”ë‹ë¼\n    \n  \n  \n    \n      10ëŒ€\n      200\n      190\n      250\n    \n    \n      20ëŒ€\n      220\n      250\n      300\n    \n  \n\n\n\n\n\n# 1. ê°€ì„¤ì„¤ì •\n# H0 : ì—°ë ¹ëŒ€ì™€ ë¨¹ëŠ” ì•„ì´ìŠ¤í¬ë¦¼ì˜ ì¢…ë¥˜ëŠ” ì„œë¡œ ê´€ë ¨ì´ ì—†ë‹¤(ë‘ ë³€ìˆ˜ëŠ” ì„œë¡œ ë…ë¦½ì´ë‹¤.)\n# H1 : ì—°ë ¹ëŒ€ì™€ ë¨¹ëŠ” ì•„ì´ìŠ¤í¬ë¦¼ì˜ ì¢…ë¥˜ëŠ” ì„œë¡œ ê´€ë ¨ì´ ìˆë‹¤(ë‘ ë³€ìˆ˜ëŠ” ì„œë¡œ ë…ë¦½ì´ ì•„ë‹ˆë‹¤.)\n\n\n# 2. ìœ ì˜ìˆ˜ì¤€ í™•ì¸ : ìœ ì˜ìˆ˜ì¤€ 5%ë¡œ í™•ì¸\n\n\n# 3.ê²€ì •ì‹¤ì‹œ(í†µê³„ëŸ‰, p-value, ê¸°ëŒ€ê°’ í™•ì¸)\nfrom scipy.stats import chi2_contingency\n\nstatistic, pvalue, dof, expected = chi2_contingency(df)\n# ê³µì‹ë¬¸ì„œìƒì— : statistic(í†µê³„ëŸ‰), pvalue, dof(ììœ ë„), expected_freq(ê¸°ëŒ€ê°’)\n\n# ì•„ë˜ì™€ ê°™ì´ ì…ë ¥í•´ë„ ë™ì¼í•œ ê²°ê³¼\n# statistic, pvalue, dof, expected = chi2_contingency([row1, row2])\n# statistic, pvalue, dof, expected = chi2_contingency([df.iloc[0], df.iloc[1]])\n\nprint(statistic)\nprint(pvalue)\nprint(dof) # ììœ ë„ = (í–‰-1) * (ì—´*1)\nprint(np.round(expected, 2)) # ë°˜ì˜¬ë¦¼í•˜ê³  ì‹¶ë‹¤ë©´ np.round()\n\n1.708360126075226\n0.4256320394874311\n2\n[[190.64 199.72 249.65]\n [229.36 240.28 300.35]]\n\n\n\n# 4. ê·€ë¬´ê°€ì„¤ ê¸°ê°ì—¬ë¶€ ê²°ì •(ì±„íƒ/ê¸°ê°)\n# p-value ê°’ì´ 0.05ë³´ë‹¤ í¬ê¸° ë–„ë¬¸ì— ê·€ë¬´ê°€ì„¤ì„ ì±„íƒí•œë‹¤.\n# ì¦‰, ì—°ë ¹ëŒ€ì™€ ë¨¹ëŠ” ì•„ì´ìŠ¤í¬ë¦¼ì˜ ì¢…ë¥˜ëŠ” ì„œë¡œ ê´€ë ¨ì´ ì—†ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.\n\n# ë‹µ : ì±„íƒ\n\n\n\n(ì¶”ê°€) ë§Œì•½ ë°ì´í„° í˜•íƒœê°€ ë‹¤ë¥¼ê²½ìš°?\n\n# (Case1) ë§Œì•½ ë°ì´í„°ê°€ ì•„ë˜ì™€ ê°™ì´ ì£¼ì–´ì§„ë‹¤ë©´?\ndf = pd.DataFrame({\n    'ì•„ì´ìŠ¤í¬ë¦¼' : ['ë”¸ê¸°', 'ì´ˆì½”', 'ë°”ë‹ë¼', 'ë”¸ê¸°', 'ì´ˆì½”', 'ë°”ë‹ë¼'],\n    'ì—°ë ¹' : ['10ëŒ€','10ëŒ€','10ëŒ€','20ëŒ€','20ëŒ€','20ëŒ€'],\n    'ì¸ì›' : [200,190,250,220,250,300]\n})\ndf\n\n\n\n\n\n  \n    \n      \n      ì•„ì´ìŠ¤í¬ë¦¼\n      ì—°ë ¹\n      ì¸ì›\n    \n  \n  \n    \n      0\n      ë”¸ê¸°\n      10ëŒ€\n      200\n    \n    \n      1\n      ì´ˆì½”\n      10ëŒ€\n      190\n    \n    \n      2\n      ë°”ë‹ë¼\n      10ëŒ€\n      250\n    \n    \n      3\n      ë”¸ê¸°\n      20ëŒ€\n      220\n    \n    \n      4\n      ì´ˆì½”\n      20ëŒ€\n      250\n    \n    \n      5\n      ë°”ë‹ë¼\n      20ëŒ€\n      300\n    \n  \n\n\n\n\n\n# pd.crosstab(index= , columns= , values= , aggfunc=sum)\ntable = pd.crosstab(index=df['ì—°ë ¹'] , columns=df['ì•„ì´ìŠ¤í¬ë¦¼'] ,\n                    values= df['ì¸ì›'], aggfunc=sum)\ntable\n# ì£¼ì˜ : index, columnsì— ìˆœì„œë¥¼ ê¼­ í™•ì¸í•˜ê¸°\n\n\n\n\n\n  \n    \n      ì•„ì´ìŠ¤í¬ë¦¼\n      ë”¸ê¸°\n      ë°”ë‹ë¼\n      ì´ˆì½”\n    \n    \n      ì—°ë ¹\n      \n      \n      \n    \n  \n  \n    \n      10ëŒ€\n      200\n      250\n      190\n    \n    \n      20ëŒ€\n      220\n      300\n      250\n    \n  \n\n\n\n\n\n# 3.ê²€ì •ì‹¤ì‹œ(í†µê³„ëŸ‰, p-value, ê¸°ëŒ€ê°’ í™•ì¸)\nfrom scipy.stats import chi2_contingency\n\nstatistic, pvalue, dof, expected = chi2_contingency(table)\n# ê³µì‹ë¬¸ì„œìƒì— : statistic(í†µê³„ëŸ‰), pvalue, dof(ììœ ë„), expected_freq(ê¸°ëŒ€ê°’)\n\nprint(statistic)\nprint(pvalue)\nprint(dof) # ììœ ë„ = (í–‰-1) * (ì—´*1)\nprint(np.round(expected, 2)) # ë°˜ì˜¬ë¦¼í•˜ê³  ì‹¶ë‹¤ë©´ np.round()\n\n1.708360126075226\n0.4256320394874311\n2\n[[190.64 249.65 199.72]\n [229.36 300.35 240.28]]\n\n\n\n# (Case2) ë§Œì•½ ë°ì´í„°ê°€ ì•„ë˜ì™€ ê°™ì´ ì£¼ì–´ì§„ë‹¤ë©´?\n# (ì´í•´ë¥¼ ìœ„í•œ ì°¸ê³ ìš© ì…ë‹ˆë‹¤. ë¹ˆë„ìˆ˜ ì¹´ìš´íŒ…)\ndf = pd.DataFrame({\n    'ì•„ì´ìŠ¤í¬ë¦¼' : ['ë”¸ê¸°', 'ì´ˆì½”', 'ë°”ë‹ë¼', 'ë”¸ê¸°', 'ì´ˆì½”', 'ë°”ë‹ë¼'],\n    'ì—°ë ¹' : ['10ëŒ€','10ëŒ€','10ëŒ€','20ëŒ€','20ëŒ€','20ëŒ€']\n})\ndf\n\n\n\n\n\n  \n    \n      \n      ì•„ì´ìŠ¤í¬ë¦¼\n      ì—°ë ¹\n    \n  \n  \n    \n      0\n      ë”¸ê¸°\n      10ëŒ€\n    \n    \n      1\n      ì´ˆì½”\n      10ëŒ€\n    \n    \n      2\n      ë°”ë‹ë¼\n      10ëŒ€\n    \n    \n      3\n      ë”¸ê¸°\n      20ëŒ€\n    \n    \n      4\n      ì´ˆì½”\n      20ëŒ€\n    \n    \n      5\n      ë°”ë‹ë¼\n      20ëŒ€\n    \n  \n\n\n\n\n\n# pd.crosstab(index, columns)\npd.crosstab(df['ì—°ë ¹'],df['ì•„ì´ìŠ¤í¬ë¦¼'])\n\n\n\n\n\n  \n    \n      ì•„ì´ìŠ¤í¬ë¦¼\n      ë”¸ê¸°\n      ë°”ë‹ë¼\n      ì´ˆì½”\n    \n    \n      ì—°ë ¹\n      \n      \n      \n    \n  \n  \n    \n      10ëŒ€\n      1\n      1\n      1\n    \n    \n      20ëŒ€\n      1\n      1\n      1\n    \n  \n\n\n\n\n\n\në¬¸ì œ2-2\n\n\níƒ€ì´íƒ€ë‹‰ì— ë°ì´í„°ì—ì„œ ì„±ë³„(sex)ê³¼ ìƒì¡´ì—¬ë¶€(survied) ë³€ìˆ˜ê°„\n\n\në…ë¦½ì„± ê²€ì •ì„ ì‹¤ì‹œí•˜ì‹œì˜¤\n\nimport seaborn as sns\ndf = sns.load_dataset('titanic')\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 15 columns):\n #   Column       Non-Null Count  Dtype   \n---  ------       --------------  -----   \n 0   survived     891 non-null    int64   \n 1   pclass       891 non-null    int64   \n 2   sex          891 non-null    object  \n 3   age          714 non-null    float64 \n 4   sibsp        891 non-null    int64   \n 5   parch        891 non-null    int64   \n 6   fare         891 non-null    float64 \n 7   embarked     889 non-null    object  \n 8   class        891 non-null    category\n 9   who          891 non-null    object  \n 10  adult_male   891 non-null    bool    \n 11  deck         203 non-null    category\n 12  embark_town  889 non-null    object  \n 13  alive        891 non-null    object  \n 14  alone        891 non-null    bool    \ndtypes: bool(2), category(2), float64(2), int64(4), object(5)\nmemory usage: 80.7+ KB\n\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      survived\n      pclass\n      sex\n      age\n      sibsp\n      parch\n      fare\n      embarked\n      class\n      who\n      adult_male\n      deck\n      embark_town\n      alive\n      alone\n    \n  \n  \n    \n      0\n      0\n      3\n      male\n      22.0\n      1\n      0\n      7.2500\n      S\n      Third\n      man\n      True\n      NaN\n      Southampton\n      no\n      False\n    \n    \n      1\n      1\n      1\n      female\n      38.0\n      1\n      0\n      71.2833\n      C\n      First\n      woman\n      False\n      C\n      Cherbourg\n      yes\n      False\n    \n    \n      2\n      1\n      3\n      female\n      26.0\n      0\n      0\n      7.9250\n      S\n      Third\n      woman\n      False\n      NaN\n      Southampton\n      yes\n      True\n    \n    \n      3\n      1\n      1\n      female\n      35.0\n      1\n      0\n      53.1000\n      S\n      First\n      woman\n      False\n      C\n      Southampton\n      yes\n      False\n    \n    \n      4\n      0\n      3\n      male\n      35.0\n      0\n      0\n      8.0500\n      S\n      Third\n      man\n      True\n      NaN\n      Southampton\n      no\n      True\n    \n  \n\n\n\n\n\n# pd.crosstab(index, columns)\ntable = pd.crosstab(df['sex'], df['survived'])\nprint(table)\n\nsurvived    0    1\nsex               \nfemale     81  233\nmale      468  109\n\n\n\n# 1. ê°€ì„¤ì„¤ì •\n# H0 : ì„±ë³„ê³¼ ìƒì¡´ ì—¬ë¶€ëŠ” ì„œë¡œ ê´€ë ¨ì´ ì—†ë‹¤(ë‘ ë³€ìˆ˜ëŠ” ì„œë¡œ ë…ë¦½ì´ë‹¤)\n# H1 : ì„±ë³„ê³¼ ìƒì¡´ ì—¬ë¶€ëŠ” ì„œë¡œ ê´€ë ¨ì´ ìˆë‹¤(ë‘ ë³€ìˆ˜ëŠ” ì„œë¡œ ë…ë¦½ì´ ì•„ë‹ˆë‹¤)\n\n\n# 2. ìœ ì˜ìˆ˜ì¤€ í™•ì¸ : ìœ ì˜ìˆ˜ì¤€ 5%ë¡œ í™•ì¸\n\n\n# 3.ê²€ì •ì‹¤ì‹œ(í†µê³„ëŸ‰, p-value, ê¸°ëŒ€ê°’ í™•ì¸)\nfrom scipy.stats import chi2_contingency\n\nstatistic, pvalue, dof, expected = chi2_contingency(table)\n# ê³µì‹ë¬¸ì„œìƒì— : statistic(í†µê³„ëŸ‰), pvalue, dof(ììœ ë„), expected_freq(ê¸°ëŒ€ê°’)\n\nprint(statistic)\nprint(pvalue)\nprint(dof) # ììœ ë„ = (í–‰-1) * (ì—´*1)\nprint(np.round(expected, 2)) # ë°˜ì˜¬ë¦¼í•˜ê³  ì‹¶ë‹¤ë©´ np.round()\n\n260.71702016732104\n1.1973570627755645e-58\n1\n[[193.47 120.53]\n [355.53 221.47]]\n\n\n\n# 4. ê·€ë¬´ê°€ì„¤ ê¸°ê°ì—¬ë¶€ ê²°ì •(ì±„íƒ/ê¸°ê°)\n# p-value ê°’(1.1973570627755645e-58)ì´ 0.05ë³´ë‹¤ ì‘ê¸° ë–„ë¬¸ì— ê·€ë¬´ê°€ì„¤ì„ ê¸°ê°í•œë‹¤.\n# ì¦‰, ì„±ë³„ê³¼ ìƒì¡´ ì—¬ë¶€ëŠ” ì„œë¡œ ê´€ë ¨ì´ ìˆë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.\n\n# ë‹µ : ê¸°ê°\n\n\n\në°ì´í„°ë¥¼ ë³€ê²½í•´ë³´ë©´ì„œ ì´í•´í•´ë´…ì‹œë‹¤.\n\n# ì„ì˜ ë°ì´í„° ìƒì„±\nsex, survived = [160,160], [250,220]\ntable = pd.DataFrame([sex, survived], columns=['0','1'], index=['female','male'])\nprint(table)\n\n          0    1\nfemale  160  160\nmale    250  220\n\n\n\n# 3.ê²€ì •ì‹¤ì‹œ(í†µê³„ëŸ‰, p-value, ê¸°ëŒ€ê°’ í™•ì¸)\nfrom scipy.stats import chi2_contingency\n\nstatistic, pvalue, dof, expected = chi2_contingency(table)\n# ê³µì‹ë¬¸ì„œìƒì— : statistic(í†µê³„ëŸ‰), pvalue, dof(ììœ ë„), expected_freq(ê¸°ëŒ€ê°’)\n\nprint(statistic)\nprint(pvalue)\nprint(dof) # ììœ ë„ = (í–‰-1) * (ì—´*1)\nprint(np.round(expected, 2)) # ë°˜ì˜¬ë¦¼í•˜ê³  ì‹¶ë‹¤ë©´ np.round()\n\n0.6541895872879862\n0.41861876333789727\n1\n[[166.08 153.92]\n [243.92 226.08]]\n\n\n\n# 4. ê·€ë¬´ê°€ì„¤ ê¸°ê°ì—¬ë¶€ ê²°ì •(ì±„íƒ/ê¸°ê°)\n# p-value ê°’ì´ 0.05ë³´ë‹¤ í¬ê¸° ë–„ë¬¸ì— ê·€ë¬´ê°€ì„¤ì„ ì±„íƒí•œë‹¤.\n# ì¦‰, ì„±ë³„ê³¼ ìƒì¡´ ì—¬ë¶€ëŠ” ì„œë¡œ ê´€ë ¨ì´ ì—†ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.\n\n# ë‹µ : ì±„íƒ"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_5.html",
    "href": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_5.html",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 3ìœ í˜• ë¬¸ì œí’€ì´(5)",
    "section": "",
    "text": "ì‹¤ê¸° 3 ìœ í˜•(5)"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_5.html#ë‹¤ì¤‘íšŒê·€ë¶„ì„",
    "href": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_5.html#ë‹¤ì¤‘íšŒê·€ë¶„ì„",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 3ìœ í˜• ë¬¸ì œí’€ì´(5)",
    "section": "âœ… ë‹¤ì¤‘íšŒê·€ë¶„ì„",
    "text": "âœ… ë‹¤ì¤‘íšŒê·€ë¶„ì„\n\nimport pandas as pd\nimport numpy as np\n\n\në‹¹ë‡¨ë³‘ í™˜ìì˜ ì§ˆë³‘ ì§„í–‰ì •ë„ ë°ì´í„°ì…‹\n\n###############  ì‹¤ê¸°í™˜ê²½ ë³µì‚¬ ì˜ì—­  ###############\nimport pandas as pd\nimport numpy as np\n# ì‹¤ê¸° ì‹œí—˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ì…‹íŒ…í•˜ê¸° (ìˆ˜ì •ê¸ˆì§€)\nfrom sklearn.datasets import load_diabetes\n# diabetes ë°ì´í„°ì…‹ ë¡œë“œ\ndiabetes = load_diabetes()\nx = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\ny = pd.DataFrame(diabetes.target)\ny.columns = ['target'] \n###############  ì‹¤ê¸°í™˜ê²½ ë³µì‚¬ ì˜ì—­  ###############\n\n\n# ë°ì´í„° ì„¤ëª…\nprint(diabetes.DESCR)\n\n.. _diabetes_dataset:\n\nDiabetes dataset\n----------------\n\nTen baseline variables, age, sex, body mass index, average blood\npressure, and six blood serum measurements were obtained for each of n =\n442 diabetes patients, as well as the response of interest, a\nquantitative measure of disease progression one year after baseline.\n\n**Data Set Characteristics:**\n\n  :Number of Instances: 442\n\n  :Number of Attributes: First 10 columns are numeric predictive values\n\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n\n  :Attribute Information:\n      - age     age in years\n      - sex\n      - bmi     body mass index\n      - bp      average blood pressure\n      - s1      tc, total serum cholesterol\n      - s2      ldl, low-density lipoproteins\n      - s3      hdl, high-density lipoproteins\n      - s4      tch, total cholesterol / HDL\n      - s5      ltg, possibly log of serum triglycerides level\n      - s6      glu, blood sugar level\n\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n\nSource URL:\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n\nFor more information see:\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n\n\n\n\n1. sklearn ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš©\n\n# sklearn ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš©\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\n\n# ë…ë¦½ë³€ìˆ˜ì™€ ì¢…ì†ë³€ìˆ˜ ì„¤ì •\nx = x[['age','sex','bmi']]\nprint(x.head())\nprint(y.head())\n\n        age       sex       bmi\n0  0.038076  0.050680  0.061696\n1 -0.001882 -0.044642 -0.051474\n2  0.085299  0.050680  0.044451\n3 -0.089063 -0.044642 -0.011595\n4  0.005383 -0.044642 -0.036385\n----------------------------------\n   target\n0   151.0\n1    75.0\n2   141.0\n3   206.0\n4   135.0\n\n\n\níšŒê·€ì‹ : y = b0 + b1x1 + b2x2 + b3x3\n(x1=age, x2=sex, x3=bmi)\n\n\n# ëª¨ë¸ë§ \nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(x, y)\n\nLinearRegression()\n\n\n\n# íšŒê·€ë¶„ì„ ê´€ë ¨ ì§€í‘œ ì¶œë ¥\n\n# 1. Rsq(ê²°ì •ê³„ìˆ˜) : model.score(x,y)\nmodel.score(x, y)\nprint(round(model.score(x,y),2))\n\n0.35\n\n\n\n# 2. íšŒê·€ê³„ìˆ˜ ì¶œë ¥ : model.coef_\nprint(np.round(model.coef_, 2))        # ì „ì²´ íšŒê·€ê³„ìˆ˜\nprint(np.round(model.coef_[0,0], 2))   # x1ì˜ íšŒê·€ê³„ìˆ˜\nprint(np.round(model.coef_[0,1], 2))   # x2ì˜ íšŒê·€ê³„ìˆ˜\nprint(np.round(model.coef_[0,2], 2))   # x3ì˜ íšŒê·€ê³„ìˆ˜\n\n[[138.9  -36.14 926.91]]\n138.9\n-36.14\n926.91\n\n\n\n# 3. íšŒê·€ê³„ìˆ˜(ì ˆí¸) : model.intercept_\nprint(np.round(model.intercept_, 2))\n\n[152.13]\n\n\n\níšŒê·€ì‹ : y = b0 + b1x1 + b2x2 + b3x3\n(x1=age, x2=sex, x3=bmi)\n\n\n\nê²°ê³¼ : 152.13 + 138.9age -36.14sex + 926.91bmi"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_5.html#statsmodels-ë¼ì´ë¸ŒëŸ¬ë¦¬-ì‚¬ìš©",
    "href": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_5.html#statsmodels-ë¼ì´ë¸ŒëŸ¬ë¦¬-ì‚¬ìš©",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 3ìœ í˜• ë¬¸ì œí’€ì´(5)",
    "section": "2. statsmodels ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©",
    "text": "2. statsmodels ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©\n\n###############  ì‹¤ê¸°í™˜ê²½ ë³µì‚¬ ì˜ì—­  ###############\nimport pandas as pd\nimport numpy as np\n# ì‹¤ê¸° ì‹œí—˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ì…‹íŒ…í•˜ê¸° (ìˆ˜ì •ê¸ˆì§€)\nfrom sklearn.datasets import load_diabetes\n# diabetes ë°ì´í„°ì…‹ ë¡œë“œ\ndiabetes = load_diabetes()\nx = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\ny = pd.DataFrame(diabetes.target)\ny.columns = ['target'] \n###############  ì‹¤ê¸°í™˜ê²½ ë³µì‚¬ ì˜ì—­  ###############\n\n\n# statsmodel.formula í™œìš©\nimport statsmodels.api as sm\n# ë…ë¦½ë³€ìˆ˜ì™€ ì¢…ì†ë³€ìˆ˜ ì„¤ì •\nx = x[['age','sex','bmi']]\ny = y[['target']]\nprint(x.head())\nprint(y.head())\n\n        age       sex       bmi\n0  0.038076  0.050680  0.061696\n1 -0.001882 -0.044642 -0.051474\n2  0.085299  0.050680  0.044451\n3 -0.089063 -0.044642 -0.011595\n4  0.005383 -0.044642 -0.036385\n   target\n0   151.0\n1    75.0\n2   141.0\n3   206.0\n4   135.0\n\n\n\n# ëª¨ë¸ë§\nimport statsmodels.api as sm\n\nx = sm.add_constant(x)        # ì£¼ì˜ : ìƒìˆ˜í•­ ì¶”ê°€í•´ì¤˜ì•¼ í•¨\nmodel = sm.OLS(y, x).fit()\n# y_pred = model.predict(x)\nsummary = model.summary()\nprint(summary)\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 target   R-squared:                       0.351\nModel:                            OLS   Adj. R-squared:                  0.346\nMethod:                 Least Squares   F-statistic:                     78.94\nDate:                Wed, 29 Nov 2023   Prob (F-statistic):           7.77e-41\nTime:                        14:29:14   Log-Likelihood:                -2451.6\nNo. Observations:                 442   AIC:                             4911.\nDf Residuals:                     438   BIC:                             4928.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        152.1335      2.964     51.321      0.000     146.307     157.960\nage          138.9039     64.254      2.162      0.031      12.618     265.189\nsex          -36.1353     63.391     -0.570      0.569    -160.724      88.453\nbmi          926.9120     63.525     14.591      0.000     802.061    1051.763\n==============================================================================\nOmnibus:                       14.687   Durbin-Watson:                   1.851\nProb(Omnibus):                  0.001   Jarque-Bera (JB):                8.290\nSkew:                           0.150   Prob(JB):                       0.0158\nKurtosis:                       2.400   Cond. No.                         23.7\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# 1. Rsq(ê²°ì •ê³„ìˆ˜)\n# r2 = 0.351\n\n# 2. íšŒê·€ê³„ìˆ˜\n# age = 138.9039\n# sex = -36.1353\n# bmi = 926.9120\n\n# 3. íšŒê·€ê³„ìˆ˜(ì ˆí¸)\n# const = 152.1335\n\n# 4. íšŒê·€ì‹ p-value\n# pvalue = 7.77e-41 # 0ì— ê°€ê¹Œìš´ ì‘ì€ ê°’"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_5.html#ê²°ê³¼-ë¹„êµí•´ë³´ê¸°-ë‘-ë¼ì´ë¸ŒëŸ¬ë¦¬-ëª¨ë‘-ê°™ì€-ê²°ê³¼ê°’ì„-ì¶œë ¥",
    "href": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_5.html#ê²°ê³¼-ë¹„êµí•´ë³´ê¸°-ë‘-ë¼ì´ë¸ŒëŸ¬ë¦¬-ëª¨ë‘-ê°™ì€-ê²°ê³¼ê°’ì„-ì¶œë ¥",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 3ìœ í˜• ë¬¸ì œí’€ì´(5)",
    "section": "(ê²°ê³¼ ë¹„êµí•´ë³´ê¸°) ë‘ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëª¨ë‘ ê°™ì€ ê²°ê³¼ê°’ì„ ì¶œë ¥",
    "text": "(ê²°ê³¼ ë¹„êµí•´ë³´ê¸°) ë‘ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëª¨ë‘ ê°™ì€ ê²°ê³¼ê°’ì„ ì¶œë ¥\n\níšŒê·€ì‹ : y = b0 + b1x1 + b2x2 + b3x3\n(x1=age, x2=sex, x3=bmi)\n\n\n1. sklearn : y = 152.13 + 138.9age -36.14sex + 926.91bmi\n\n\n2. statsmodel : y = 152.13 + 138.9age -36.14sex + 926.91bmi"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_5.html#ìƒê´€ë¶„ì„",
    "href": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_5.html#ìƒê´€ë¶„ì„",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 3ìœ í˜• ë¬¸ì œí’€ì´(5)",
    "section": "âœ… ìƒê´€ë¶„ì„",
    "text": "âœ… ìƒê´€ë¶„ì„\n\n###############  ì‹¤ê¸°í™˜ê²½ ë³µì‚¬ ì˜ì—­  ###############\nimport pandas as pd\nimport numpy as np\n# ì‹¤ê¸° ì‹œí—˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ì…‹íŒ…í•˜ê¸° (ìˆ˜ì •ê¸ˆì§€)\nfrom sklearn.datasets import load_diabetes\n# diabetes ë°ì´í„°ì…‹ ë¡œë“œ\ndiabetes = load_diabetes()\nx = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\ny = pd.DataFrame(diabetes.target)\ny.columns = ['target'] \n###############  ì‹¤ê¸°í™˜ê²½ ë³µì‚¬ ì˜ì—­  ###############\n\n\n# ìƒê´€ë¶„ì„ì„ í•  2ê°€ì§€ ë³€ìˆ˜ ì„¤ì •\nx = x['bmi']\ny = y['target']\nprint(x.head())\nprint(y.head())\n\n0    0.061696\n1   -0.051474\n2    0.044451\n3   -0.011595\n4   -0.036385\nName: bmi, dtype: float64\n0    151.0\n1     75.0\n2    141.0\n3    206.0\n4    135.0\nName: target, dtype: float64\n\n\n\n# ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\nfrom scipy.stats import pearsonr\n\n# ìƒê´€ê³„ìˆ˜ì— ëŒ€í•œ ê²€ì •ì‹¤ì‹œ\nr, pvalue = pearsonr(x, y)\n\n# ê°€ì„¤ê²€ì •\n# H0 : ë‘ ë³€ìˆ˜ê°„ ì„ í˜•ê´€ê³„ê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤ (p=0)\n# H1 : ë‘ ë³€ìˆ˜ê°„ ì„ í˜•ê´€ê³„ê°€ ì¡´ì¬í•œë‹¤ (p!=0)\n\n# 1. ìƒê´€ê³„ìˆ˜\nprint(round(r,2))\n\n# 2. p-value\nprint(round(pvalue,2))\n\n# 3. ê²€ì •í†µê³„ëŸ‰\n# í†µê³„ëŸ‰ì€ ë³„ë¡œëŒ êµ¬í•´ì•¼ í•¨ (T = r*root(n-2) / root(1-2r))\n# r = ìƒê´€ê³„ìˆ˜\n# n = ë°ì´í„°ì˜ ê°œìˆ˜\n\nn = len(x) # ë°ì´í„° ìˆ˜\nr2 = r**2  # ìƒê´€ê¼ìˆ˜ì˜ ì œê³±  \nstatistic = r * ((n-2)**0.5) / ((1-r2)**0.5)\n\nprint(round(statistic,2)) # í†µê³„ëŸ‰ê°’ì´ í¬ë©´ p-value ê°’ì´ ì‘ì•„ì§„ë‹¤\n\n# 4. ê·€ë¬´ê°€ì„¤ ê¸°ê°ì—¬ë¶€ ê²°ì •(ì±„íƒ/ê¸°ê°)\n# p-value ê°’ì´ 0.05ë³´ë‹¤ ì‘ê¸° ë–„ë¬¸ì— ê·€ë¬´ê°€ì„¤ì„ ê¸°ê°í•œë‹¤.\n# ì¦‰, ë‘ ë³€ìˆ˜ê°„ ì„ í˜•ê´€ê³„ê°€ ì¡´ì¬í•œë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.(ìƒê´€ê³„ìˆ˜ê°€ 0ì´ ì•„ë‹ˆë‹¤)\n\n# ë‹µ : ê¸°ê°\n\n0.59\n0.0\n15.19"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_6.html",
    "href": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_6.html",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 3ìœ í˜• ë¬¸ì œí’€ì´(6)",
    "section": "",
    "text": "ì‹¤ê¸° 3 ìœ í˜•(6)"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_6.html#íƒ€ì´íƒ€ë‹‰-ë°ì´í„°-ë¶ˆëŸ¬ì˜¤ê¸°ìƒì¡´ì-ì˜ˆì¸¡-ë°ì´í„°",
    "href": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_6.html#íƒ€ì´íƒ€ë‹‰-ë°ì´í„°-ë¶ˆëŸ¬ì˜¤ê¸°ìƒì¡´ì-ì˜ˆì¸¡-ë°ì´í„°",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 3ìœ í˜• ë¬¸ì œí’€ì´(6)",
    "section": "íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°(ìƒì¡´ì ì˜ˆì¸¡ ë°ì´í„°)",
    "text": "íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°(ìƒì¡´ì ì˜ˆì¸¡ ë°ì´í„°)\n\n# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\nimport pandas as pd\nimport numpy as np\n\n# Seabornì˜ ë‚´ì¥ íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\nimport seaborn as sns\ndf = sns.load_dataset('titanic')\n\n\nprint(df.head())\n\n   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n0         0       3    male  22.0      1      0   7.2500        S  Third   \n1         1       1  female  38.0      1      0  71.2833        C  First   \n2         1       3  female  26.0      0      0   7.9250        S  Third   \n3         1       1  female  35.0      1      0  53.1000        S  First   \n4         0       3    male  35.0      0      0   8.0500        S  Third   \n\n     who  adult_male deck  embark_town alive  alone  \n0    man        True  NaN  Southampton    no  False  \n1  woman       False    C    Cherbourg   yes  False  \n2  woman       False  NaN  Southampton   yes   True  \n3  woman       False    C  Southampton   yes  False  \n4    man        True  NaN  Southampton    no   True  \n\n\n\n# ë¶„ì„ ë°ì´í„° ì„¤ì •\ndf = df[['survived', 'sex', 'sibsp', 'fare']] \n# sex:ì„±ë³„, sibsp:íƒ‘ìŠ¹í•œ ë¶€ëª¨ ë° ìë…€ìˆ˜ fare:ìš”ê¸ˆ\n\nprint(df.head())\n\n   survived     sex  sibsp     fare\n0         0    male      1   7.2500\n1         1  female      1  71.2833\n2         1  female      0   7.9250\n3         1  female      1  53.1000\n4         0    male      0   8.0500\n\n\n\në¡œì§€ìŠ¤í‹± íšŒê·€ì‹ : P(1ì¼ í™•ë¥ ) = 1 / (1+exp(-f(x))\n\nf(x) = b0 + b1x1 + b2x2 + b3x3\nln(P/1-P) = b0 + b1x1 + b2x2 + b3x3\n(P = ìƒì¡´í•  í™•ë¥ , x1=sex, x2=sibsp, x3=fare)\n\n\n# ë°ì´í„° ì „ì²˜ë¦¬\n# ë³€ìˆ˜ì²˜ë¦¬\n# ë¬¸ìí˜• íƒ€ì…ì˜ ë°ì´í„°ì˜ ê²½ìš° ìˆ«ìë¡œ ë³€ê²½í•´ì¤€ë‹¤.\n# *** ì‹¤ì œ ì‹œí—˜ì—ì„œ ì§€ì‹œì‚¬í•­ì„ ë”°ë¥¼ ê²ƒ ***\n\n# ì„±ë³„ì„ map í•¨ìˆ˜ë¥¼ í™œìš©í•´ì„œ ê°ê° 1ê³¼ 0ì— í• ë‹¹í•œë‹¤. (ì—¬ì„±ì„ 1, ë‚¨ì„±ì„ 0)\n# (ì‹¤ì œ ì‹œí—˜ì˜ ì§€ì‹œ ì¡°ê±´ì— ë”°ë¥¼ ê²ƒ)\ndf['sex'] = df['sex'].map({'female':1,\n                          'male':0})\nprint(df.head())\n\n   survived  sex  sibsp     fare\n0         0    0      1   7.2500\n1         1    1      1  71.2833\n2         1    1      0   7.9250\n3         1    1      1  53.1000\n4         0    0      0   8.0500\n\n\n\nprint(df.info())\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 4 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   survived  891 non-null    int64  \n 1   sex       891 non-null    int64  \n 2   sibsp     891 non-null    int64  \n 3   fare      891 non-null    float64\ndtypes: float64(1), int64(3)\nmemory usage: 28.0 KB\nNone"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_6.html#sklearn-ë¼ì´ë¸ŒëŸ¬ë¦¬-í™œìš©",
    "href": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_6.html#sklearn-ë¼ì´ë¸ŒëŸ¬ë¦¬-í™œìš©",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 3ìœ í˜• ë¬¸ì œí’€ì´(6)",
    "section": "1. sklearn ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš©",
    "text": "1. sklearn ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš©\n\n# ë…ë¦½ë³€ìˆ˜ì™€ ì¢…ì†ë³€ìˆ˜ ì„¤ì •\nx = df.drop(['survived'], axis=1) # x = df[['sex', 'age', 'fare']]\ny = df['survived']\n\n(ì£¼ì˜) LogisticRegrresion() ê°ì²´ì•ˆì— ë°˜ë“œì‹œ penalty=Noneìœ¼ë¡œ ì…ë ¥í•´ì•¼ í•¨\n\n# ëª¨ë¸ë§\nfrom sklearn.linear_model import LogisticRegression # íšŒê·€ëŠ” LinearRegression\n\n# ë°˜ë“œì‹œ penalty = Noneìœ¼ë¡œ ì…ë ¥í•  ê²ƒí•´ì•¼ í•¨.  default='l2'\nmodel1 = LogisticRegression(penalty='none')\nmodel1.fit(x, y)\n\nLogisticRegression(penalty='none')\n\n\n\n# ë¡œì§€ìŠ¤í‹±íšŒê·€ë¶„ì„ ê´€ë ¨ ì§€í‘œ ì¶œë ¥\n\n# 1.íšŒê·€ê³„ìˆ˜ ì¶œë ¥ : model.coef_\nprint(np.round(model1.coef_, 4))        # ì „ì²´ íšŒê·€ê³„ìˆ˜\nprint(np.round(model1.coef_[0,0], 4))   # x1ì˜ íšŒê·€ê³„ìˆ˜\nprint(np.round(model1.coef_[0,1], 4))   # x2ì˜ íšŒê·€ê³„ìˆ˜\nprint(np.round(model1.coef_[0,2], 4))   # x3ì˜ íšŒê·€ê³„ìˆ˜\n\n# 2. íšŒê·€ê³„ìˆ˜(ì ˆí¸) : model.intercept_\nprint(np.round(model1.intercept_, 4))\n\n[[ 2.5668 -0.4017  0.0138]]\n2.5668\n-0.4017\n0.0138\n[-1.6964]\n\n\n\në¡œì§€ìŠ¤í‹± íšŒê·€ì‹ : P(1ì¼ í™•ë¥ ) = 1 / (1+exp(-f(x))\n\nf(x) = b0 + b1x1 + b2x2 + b3x3\nln(P/1-P) = b0 + b1x1 + b2x2 + b3x3\n(P = ìƒì¡´í•  í™•ë¥ , x1=sex, x2=sibsp, x3=fare)\n\n\n\nê²°ê³¼ : ln(P/1-P) = -1.6964 + 2.5668sex - 0.4017sibsp + 0.0138fare\n\n# 3-1. ë¡œì§€ìŠ¤í‹± íšŒê·€ëª¨í˜•ì—ì„œ sibsp ë³€ìˆ˜ê°€ í•œë‹¨ìœ„ ì¦ê°€í•  ë•Œ ìƒì¡´í•  ì˜¤ì¦ˆê°€ ëª‡ ë°° \n#      ì¦ê°€í•˜ëŠ”ì§€ ë°˜ì˜¬ë¦¼í•˜ì—¬ ì†Œìˆ˜ì  ì…‹ì§¸ ìë¦¬ê¹Œì§€ êµ¬í•˜ì‹œì˜¤.\n\n# exp(b2) ë¥¼ êµ¬í•˜ë©´ ëœë‹¤.\nresult = np.exp(model1.coef_[0,1]) #ì¸ë±ì‹± ì£¼ì˜í•˜ì„¸ìš”.\nprint(round(result,3))\n\n# í•´ì„ : sibsp ë³€ìˆ˜ê°€ í•œ ë‹¨ìœ„ ì¦ê°€í•  ë•Œ ìƒì¡´í•  ì˜¤ì¦ˆê°€ 0.669ë°° ì¦ê°€í•œë‹¤.\n#        ìƒì¡´í•  ì˜¤ì¦ˆê°€ 33% ê°ì†Œí•œë‹¤.(ìƒì¡´í•  í™•ë¥ ì´ ê°ì†Œí•œë‹¤.)\n\n0.669\n\n\n\n# 3-2. ë¡œì§€ìŠ¤í‹± íšŒê·€ëª¨í˜•ì—ì„œ ì—¬ì„±ì¼ ê²½ìš° ë‚¨ì„±ì— ë¹„í•´ ì˜¤ì¦ˆê°€ ëª‡ ë°° ì¦ê°€í•˜ëŠ”ì§€\n#      ë°˜ì˜¬ë¦¼í•˜ì—¬ ì†Œìˆ˜ì  ì…‹ì§¸ ìë¦¬ê¹Œì§€ êµ¬í•˜ì‹œì˜¤\n\n# exp(b2) ë¥¼ êµ¬í•˜ë©´ ëœë‹¤.\nresult2 = np.exp(model1.coef_[0,0]) #ì¸ë±ì‹± ì£¼ì˜í•˜ì„¸ìš”.\nprint(round(result2,3))\n\n# í•´ì„ : ì—¬ì„±ì¼ ê²½ìš° ë‚¨ì„±ì— ë¹„í•´ ìƒì¡´í•  ì˜¤ì¦ˆê°€ 13.024ë°° ì¦ê°€í•œë‹¤.\n#        ìƒì¡´í•  ì˜¤ì¦ˆê°€ 13ë°° ì¦ê°€í•œë‹¤.(ìƒì¡´í•  í™•ë¥ ì´ ì¦ê°€í•œë‹¤.)\n\n13.024"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_6.html#statsmodels-ë¼ì´ë¸ŒëŸ¬ë¦¬-ì‚¬ìš©",
    "href": "BigData_Analysis/ì‹¤ê¸°_3ìœ í˜•_6.html#statsmodels-ë¼ì´ë¸ŒëŸ¬ë¦¬-ì‚¬ìš©",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 3ìœ í˜• ë¬¸ì œí’€ì´(6)",
    "section": "2. statsmodels ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©",
    "text": "2. statsmodels ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©\n(ì£¼ì˜) ì‹¤ì œ ì˜¤ì¦ˆê°€ ëª‡ ë°° ì¦ê°€í–ˆëŠ”ì§€ ê³„ì‚°í•˜ëŠ” ë¬¸ì œê°€ ë‚˜ì˜¨ë‹¤ë©´\nsklearn ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ íšŒê·€ê³„ìˆ˜ë¥¼ ì§ì ‘êµ¬í•´ì„œ ê³„ì‚°í•  ê²ƒ(ì†Œìˆ˜ì ì´ ê²°ê³¼ê°’ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŒ)\n\n# ëª¨ë¸ë§\nimport statsmodels.api as sm\n\nx = sm.add_constant(x)       # ì£¼ì˜ : ìƒìˆ˜í•­ ì¶”ê°€í•´ì¤˜ì•¼ í•¨\nmodel2 = sm.Logit(y,x).fit() # ì£¼ì˜í•  ê²ƒ : y, x ìˆœìœ¼ë¡œ ì…ë ¥í•´ì•¼ í•¨\nsummary = model2.summary()\nprint(summary)\n\nOptimization terminated successfully.\n         Current function value: 0.483846\n         Iterations 6\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:               survived   No. Observations:                  891\nModel:                          Logit   Df Residuals:                      887\nMethod:                           MLE   Df Model:                            3\nDate:                Wed, 29 Nov 2023   Pseudo R-squ.:                  0.2734\nTime:                        19:06:26   Log-Likelihood:                -431.11\nconverged:                       True   LL-Null:                       -593.33\nCovariance Type:            nonrobust   LLR p-value:                 5.094e-70\n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -1.6964      0.129    -13.134      0.000      -1.950      -1.443\nsex            2.5668      0.179     14.321      0.000       2.216       2.918\nsibsp         -0.4017      0.095     -4.222      0.000      -0.588      -0.215\nfare           0.0138      0.003      5.367      0.000       0.009       0.019\n==============================================================================\n\n\n\n(ê²°ê³¼ ë¹„êµí•´ë³´ê¸°) ë‘ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëª¨ë‘ ê°™ì€ ê²°ê³¼ê°’ì„ ì¶œë ¥\n\níšŒê·€ì‹ : P(1ì¼ í™•ë¥ ) = 1 / (1+exp(-f(x))\nf(x) = b0 + b1x1 + b2x2 + b3x3\nln(P/1-P) = b0 + b1x1 + b2x2 + b3x3\n(P=ìƒì¡´í•  í™•ë¥ , x1=sex, x2=sibsp, x3=fare)\n\n1. sklearn : ln(P/1-P) = -1.6964 + 2.5668sex - 0.4017sibsp + 0.0138fare\n2. statsmodel : ln(P/1-P) = -1.6964 + 2.5668sex - 0.4017sibsp + 0.0138fare"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_íŒŒì´ì¬ê¸°ì´ˆ.html",
    "href": "BigData_Analysis/ì‹¤ê¸°_íŒŒì´ì¬ê¸°ì´ˆ.html",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - íŒŒì´ì¬ ê¸°ì´ˆ",
    "section": "",
    "text": "íŒŒì´ì¬ ê¸°ì´ˆ"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_íŒŒì´ì¬ê¸°ì´ˆ.html#ë°ì´í„°-íƒ€ì…object-int-float-bool-ë“±",
    "href": "BigData_Analysis/ì‹¤ê¸°_íŒŒì´ì¬ê¸°ì´ˆ.html#ë°ì´í„°-íƒ€ì…object-int-float-bool-ë“±",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - íŒŒì´ì¬ ê¸°ì´ˆ",
    "section": "1.ë°ì´í„° íƒ€ì…(object, int, float, bool ë“±)",
    "text": "1.ë°ì´í„° íƒ€ì…(object, int, float, bool ë“±)\n\n# ë°ì´í„° íƒ€ì… í™•ì¸\ndf.dtypes\n\ncar      object\nmpg     float64\ncyl       int64\ndisp    float64\nhp        int64\ndrat    float64\nwt      float64\nqsec    float64\nvs        int64\nam        int64\ngear      int64\ncarb      int64\ndtype: object\n\n\n\n# ë°ì´í„° íƒ€ì… ë³€ê²½ (1ê°œ)\ndf1 = df.copy()\ndf1 = df1.astype({'cyl':'object'})\nprint(df1.dtypes)\n\ncar      object\nmpg     float64\ncyl      object\ndisp    float64\nhp        int64\ndrat    float64\nwt      float64\nqsec    float64\nvs        int64\nam        int64\ngear      int64\ncarb      int64\ndtype: object\n\n\n\n# ë°ì´í„° íƒ€ì… ë³€ê²½ (2ê°œ ì´ìƒ)\ndf1 = df.copy()\ndf1 = df1.astype({'cyl':'int', 'gear':'object'})\nprint(df1.dtypes)\n\ncar      object\nmpg     float64\ncyl       int32\ndisp    float64\nhp        int64\ndrat    float64\nwt      float64\nqsec    float64\nvs        int64\nam        int64\ngear     object\ncarb      int64\ndtype: object\n\n\n\n#df1['cyl']\n\n\ndf1['cyl'].value_counts()\n\n8    14\n4    11\n6     7\nName: cyl, dtype: int64"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_íŒŒì´ì¬ê¸°ì´ˆ.html#ê¸°ì´ˆí†µê³„ëŸ‰í‰ê· -ì¤‘ì•™ê°’-ì‚¬ë¶„ìœ„ìˆ˜-iqr-í‘œì¤€í¸ì°¨-ë“±",
    "href": "BigData_Analysis/ì‹¤ê¸°_íŒŒì´ì¬ê¸°ì´ˆ.html#ê¸°ì´ˆí†µê³„ëŸ‰í‰ê· -ì¤‘ì•™ê°’-ì‚¬ë¶„ìœ„ìˆ˜-iqr-í‘œì¤€í¸ì°¨-ë“±",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - íŒŒì´ì¬ ê¸°ì´ˆ",
    "section": "2. ê¸°ì´ˆí†µê³„ëŸ‰(í‰ê· , ì¤‘ì•™ê°’, ì‚¬ë¶„ìœ„ìˆ˜, IQR, í‘œì¤€í¸ì°¨ ë“±)",
    "text": "2. ê¸°ì´ˆí†µê³„ëŸ‰(í‰ê· , ì¤‘ì•™ê°’, ì‚¬ë¶„ìœ„ìˆ˜, IQR, í‘œì¤€í¸ì°¨ ë“±)\n\n# Import CSV mtcars\ndf = pd.read_csv('mtcars.txt')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      Hornet Sportabout\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\ndf.shape # (í–‰, ì—´)\n\n(32, 12)\n\n\n\n# í‰ê· ê°’ êµ¬í•˜ê¸°\nmpg_mean = df['mpg'].mean()\nprint(mpg_mean)\n\n20.090624999999996\n\n\n\n# ì¤‘ì•™ê°’ êµ¬í•˜ê¸°\nmpg_median = df['mpg'].median()\nprint(mpg_median)\n\n19.2\n\n\n\n# ìµœë¹ˆê°’ êµ¬í•˜ê¸°\ncyl_mode = df['cyl'].mode()\nprint(cyl_mode)\n\n0    8\nName: cyl, dtype: int64\n\n\n\ndf['cyl'].value_counts()\n\n8    14\n4    11\n6     7\nName: cyl, dtype: int64\n\n\n\n# ë¶„ì‚°\nmpg_var = df['mpg'].var()\nprint(mpg_var)\n\n36.32410282258065\n\n\n\n# í‘œì¤€í¸ì°¨\nmpg_std = df['mpg'].std()\nprint(mpg_std)\n\n6.026948052089105\n\n\n\n# IQR (Q3 - Q1)\nQ1 = df['mpg'].quantile(.25)\nprint(Q1)\n\n15.425\n\n\n\nQ3 = df['mpg'].quantile(.75)\nprint(Q3)\n\n22.8\n\n\n\nIQR = Q3 - Q1\nprint(IQR)\n\n7.375\n\n\n\nQ2 = df['mpg'].quantile(.5)\nprint(Q2)\nprint(df['mpg'].median()) # 2ì‚¬ë¶„ìœ„ìˆ˜ì™€ ì¤‘ì•™ê°’ì€ ë™ì¼í•œ ê°’ì„ ì¶œë ¥ \n\n19.2\n19.2\n\n\n\n# ë²”ìœ„(Range) = ìµœëŒ€ê°’ - ìµœì†Œê°’\nmpg_max = df['mpg'].max()\nprint(mpg_max)\n\n33.9\n\n\n\nmpg_min = df['mpg'].min()\nprint(mpg_min)\n\n10.4\n\n\n\nmpg_range = mpg_max - mpg_min\nprint(mpg_range)\n\n23.5\n\n\n\n1) ë¶„í¬ì˜ ë¹„ëŒ€ì¹­ë„\n\n# ì™œë„\nmpg_skew = df['mpg'].skew()\nprint(mpg_skew)\n\n0.6723771376290805\n\n\n\n# ì²¨ë„\nmpg_kurt = df['mpg'].kurt()\nprint(mpg_kurt)\n\n-0.0220062914240855\n\n\n\n\n2) ê¸°íƒ€(í•©ê³„, ì ˆëŒ€ê°’, ë°ì´í„° ìˆ˜ ë“±)\n\n# í•©ê³„\nmpg_sum = df['mpg'].sum()\nprint(mpg_sum)\n\n642.9000000000001\n\n\n\n# ì ˆëŒ€ê°’\nIQR2 = Q1 - Q3\nprint(IQR2)\nprint(abs(IQR2))\n\n-7.375\n7.375\n\n\n\n# ë°ì´í„° ìˆ˜\nprint(len(df['mpg']))\n\n32\n\n\n\n\n3) ê·¸ë£¹í™”í•˜ì—¬ ê³„ì‚°í•˜ê¸° (groupby í™œìš©)\n\nimport seaborn as sns\ndf = sns.load_dataset('iris')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      sepal_length\n      sepal_width\n      petal_length\n      petal_width\n      species\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      setosa\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      setosa\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      setosa\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      setosa\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      setosa\n    \n  \n\n\n\n\n\n# species ë³„ë¡œ ê° ë³€ìˆ˜ì˜ í‰ê·  êµ¬í•´ë³´ê¸°\ndf.groupby('species').mean()\n\n\n\n\n\n  \n    \n      \n      sepal_length\n      sepal_width\n      petal_length\n      petal_width\n    \n    \n      species\n      \n      \n      \n      \n    \n  \n  \n    \n      setosa\n      5.006\n      3.428\n      1.462\n      0.246\n    \n    \n      versicolor\n      5.936\n      2.770\n      4.260\n      1.326\n    \n    \n      virginica\n      6.588\n      2.974\n      5.552\n      2.026\n    \n  \n\n\n\n\n\ndf.groupby('species').median()\n\n\n\n\n\n  \n    \n      \n      sepal_length\n      sepal_width\n      petal_length\n      petal_width\n    \n    \n      species\n      \n      \n      \n      \n    \n  \n  \n    \n      setosa\n      5.0\n      3.4\n      1.50\n      0.2\n    \n    \n      versicolor\n      5.9\n      2.8\n      4.35\n      1.3\n    \n    \n      virginica\n      6.5\n      3.0\n      5.55\n      2.0"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_íŒŒì´ì¬ê¸°ì´ˆ.html#ë°ì´í„°-ì¸ë±ì‹±-í•„í„°ë§-ì •ë ¬-ë³€ê²½-ë“±",
    "href": "BigData_Analysis/ì‹¤ê¸°_íŒŒì´ì¬ê¸°ì´ˆ.html#ë°ì´í„°-ì¸ë±ì‹±-í•„í„°ë§-ì •ë ¬-ë³€ê²½-ë“±",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - íŒŒì´ì¬ ê¸°ì´ˆ",
    "section": "3. ë°ì´í„° ì¸ë±ì‹±, í•„í„°ë§, ì •ë ¬, ë³€ê²½ ë“±",
    "text": "3. ë°ì´í„° ì¸ë±ì‹±, í•„í„°ë§, ì •ë ¬, ë³€ê²½ ë“±\n\ndf = pd.read_csv('mtcars.txt')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      Hornet Sportabout\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\n1) ë°ì´í„° ì¸ë±ì‹±\nì¸ë±ì‹± - í–‰, ì—´ì„ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ë½‘ëŠ” ê²ƒ\n\n# í–‰/ì—´ ì¸ë±ì‹± : df.loc['í–‰', 'ì—´']\ndf.loc[3, 'mpg'] # ì¸ë±ì‹±ì€ 0ë¶€í„° ì‹œì‘ì„ \n\n21.4\n\n\n\n# ì—´ë§Œ ì¸ë±ì‹±\ndf.loc[:, 'mpg'].head() # ':' ëŠ” ì „ì²´ë¥¼ ê°€ì§€ê³  ì˜¬ ë•Œ(ë¹ˆì¹¸ìœ¼ë¡œ ë‘ë©´ ì—ëŸ¬)\n\n0    21.0\n1    21.0\n2    22.8\n3    21.4\n4    18.7\nName: mpg, dtype: float64\n\n\n\ndf.loc[0:3, ['mpg', 'cyl', 'disp']] # í–‰ì—ì„œ 0~3ë²ˆ ì‚¬ì´ì˜ ì¸ë±ìŠ¤ë¥¼ ê°€ì§€ê³  ì™€ë¼\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n    \n  \n  \n    \n      0\n      21.0\n      6\n      160.0\n    \n    \n      1\n      21.0\n      6\n      160.0\n    \n    \n      2\n      22.8\n      4\n      108.0\n    \n    \n      3\n      21.4\n      6\n      258.0\n    \n  \n\n\n\n\n\ndf.loc[0:3, 'mpg':'disp']  # ì—´ì—ì„œ mpgì™€ disp ì‚¬ì´ì˜ ë³€ìˆ˜ë¥¼ ê°€ì§€ê³  ì™€ë¼\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n    \n  \n  \n    \n      0\n      21.0\n      6\n      160.0\n    \n    \n      1\n      21.0\n      6\n      160.0\n    \n    \n      2\n      22.8\n      4\n      108.0\n    \n    \n      3\n      21.4\n      6\n      258.0\n    \n  \n\n\n\n\n\n# ì•ì—ì„œ ní–‰ ì¸ë±ì‹±\ndf.head(2)\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.9\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.9\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n  \n\n\n\n\n\n# ë’¤ì—ì„œ ní–‰ ì¸ë±ì‹±\ndf.tail(3)\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      29\n      Ferrari Dino\n      19.7\n      6\n      145.0\n      175\n      3.62\n      2.77\n      15.5\n      0\n      1\n      5\n      6\n    \n    \n      30\n      Maserati Bora\n      15.0\n      8\n      301.0\n      335\n      3.54\n      3.57\n      14.6\n      0\n      1\n      5\n      8\n    \n    \n      31\n      Volvo 142E\n      21.4\n      4\n      121.0\n      109\n      4.11\n      2.78\n      18.6\n      1\n      1\n      4\n      2\n    \n  \n\n\n\n\n\n\n2) ì—´(Columns) ì¶”ê°€/ì œê±°\n\n# ì—´ ì„ íƒ\ndf_cyl = df['cyl']\ndf_cyl.head(3) # df.cyl.head(3) ê°™ì€ ê²°ê³¼ ë¹„ì¶”\n\n0    6\n1    6\n2    4\nName: cyl, dtype: int64\n\n\n\ndf_new = df[['cyl','mpg']]\ndf_new.head(3)\n\n\n\n\n\n  \n    \n      \n      cyl\n      mpg\n    \n  \n  \n    \n      0\n      6\n      21.0\n    \n    \n      1\n      6\n      21.0\n    \n    \n      2\n      4\n      22.8\n    \n  \n\n\n\n\n\n# ì—´ ì œê±°\ndf.drop(columns = ['car','mpg','cyl']).head(3)\n\n\n\n\n\n  \n    \n      \n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n  \n\n\n\n\n\n# ì—´ ì¶”ê°€\ndf2 = df.copy()\ndf2['new'] = df['mpg'] + 10 # ìƒˆë¡œìš´ ì»¬ëŸ¼ìœ¼ë¡œ ìƒê¹€\ndf2.head(3)\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n      new\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n      31.0\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n      31.0\n    \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n      32.8\n    \n  \n\n\n\n\n\n\n3) ë°ì´í„° í•„í„°ë§\n\n# 1ê°œ ì¡°ê±´ í•„í„°ë§\n# cyl = 4 ì¸ ë°ì´í„°ì˜ ìˆ˜ \ncond1 = (df['cyl'] == 4)\nlen(df[cond1])\n\n# cyl_4 = df[df['cyl']==4]\n# print(len(cyl_4))\n\n11\n\n\n\n# mpg ê°€ 22 ì´ìƒì¸ ë°ì´í„° ìˆ˜\ncond2 = (df['mpg'] >= 22)\nlen(df[cond2])\n\n9\n\n\n\n# 2ê°œ ì¡°ê±´ í•„í„°ë§ \ndf[cond1 & cond2]\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      7\n      Merc 240D\n      24.4\n      4\n      146.7\n      62\n      3.69\n      3.190\n      20.00\n      1\n      0\n      4\n      2\n    \n    \n      8\n      Merc 230\n      22.8\n      4\n      140.8\n      95\n      3.92\n      3.150\n      22.90\n      1\n      0\n      4\n      2\n    \n    \n      17\n      Fiat 128\n      32.4\n      4\n      78.7\n      66\n      4.08\n      2.200\n      19.47\n      1\n      1\n      4\n      1\n    \n    \n      18\n      Honda Civic\n      30.4\n      4\n      75.7\n      52\n      4.93\n      1.615\n      18.52\n      1\n      1\n      4\n      2\n    \n    \n      19\n      Toyota Corolla\n      33.9\n      4\n      71.1\n      65\n      4.22\n      1.835\n      19.90\n      1\n      1\n      4\n      1\n    \n    \n      25\n      Fiat X1-9\n      27.3\n      4\n      79.0\n      66\n      4.08\n      1.935\n      18.90\n      1\n      1\n      4\n      1\n    \n    \n      26\n      Porsche 914-2\n      26.0\n      4\n      120.3\n      91\n      4.43\n      2.140\n      16.70\n      0\n      1\n      5\n      2\n    \n    \n      27\n      Lotus Europa\n      30.4\n      4\n      95.1\n      113\n      3.77\n      1.513\n      16.90\n      1\n      1\n      5\n      2\n    \n  \n\n\n\n\n\n# 2ê°œ ì¡°ê±´ í•„í„°ë§ í›„ ë°ì´í„° ê°œìˆ˜ (and)\nprint(len(df[cond1 & cond2])) # print() í•¨ìˆ˜ë¥¼ ì´ìš©í•´ì„œ ê°œìˆ˜ë¥¼ ì¶œë ¥í•´ì•¼ ì¸ì •ë¨ \n\n9\n\n\n\n# 2ê°œ ì¡°ê±´ í•„í„°ë§ í›„ ë°ì´í„° ê°œìˆ˜ (or)\ndf[cond1 | cond2]\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      7\n      Merc 240D\n      24.4\n      4\n      146.7\n      62\n      3.69\n      3.190\n      20.00\n      1\n      0\n      4\n      2\n    \n    \n      8\n      Merc 230\n      22.8\n      4\n      140.8\n      95\n      3.92\n      3.150\n      22.90\n      1\n      0\n      4\n      2\n    \n    \n      17\n      Fiat 128\n      32.4\n      4\n      78.7\n      66\n      4.08\n      2.200\n      19.47\n      1\n      1\n      4\n      1\n    \n    \n      18\n      Honda Civic\n      30.4\n      4\n      75.7\n      52\n      4.93\n      1.615\n      18.52\n      1\n      1\n      4\n      2\n    \n    \n      19\n      Toyota Corolla\n      33.9\n      4\n      71.1\n      65\n      4.22\n      1.835\n      19.90\n      1\n      1\n      4\n      1\n    \n    \n      20\n      Toyota Corona\n      21.5\n      4\n      120.1\n      97\n      3.70\n      2.465\n      20.01\n      1\n      0\n      3\n      1\n    \n    \n      25\n      Fiat X1-9\n      27.3\n      4\n      79.0\n      66\n      4.08\n      1.935\n      18.90\n      1\n      1\n      4\n      1\n    \n    \n      26\n      Porsche 914-2\n      26.0\n      4\n      120.3\n      91\n      4.43\n      2.140\n      16.70\n      0\n      1\n      5\n      2\n    \n    \n      27\n      Lotus Europa\n      30.4\n      4\n      95.1\n      113\n      3.77\n      1.513\n      16.90\n      1\n      1\n      5\n      2\n    \n    \n      31\n      Volvo 142E\n      21.4\n      4\n      121.0\n      109\n      4.11\n      2.780\n      18.60\n      1\n      1\n      4\n      2\n    \n  \n\n\n\n\n\nprint(len(df[cond1 | cond2]))\n\n11\n\n\n\n# í•œë²ˆì— ì½”ë”©í•  ê²½ìš°\nprint(len(df[(df['cyl'] == 4) & (df['mpg'] >= 22)]))\nprint(len(df[(df['cyl'] == 4) | (df['mpg'] >= 22)])) # ì´ë ‡ê²Œ í•œë²ˆì— í•˜ë©´ ì‹¤ìˆ˜í•  ê°€ëŠ¥ì„± ì¡´ì¬ cond1,2ë¥¼ ë§Œë“¤ì–´ì„œ í•˜ëŠ” ê²ƒ ì¶”ì²œ\n\n9\n11\n\n\n\n\n4) ë°ì´í„° ì •ë ¬\n\n# ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (ìœ„ì—ì„œë¶€í„° ë‚´ë ¤ê°„ë‹¤)\ndf.sort_values('mpg', ascending=False).head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      19\n      Toyota Corolla\n      33.9\n      4\n      71.1\n      65\n      4.22\n      1.835\n      19.90\n      1\n      1\n      4\n      1\n    \n    \n      17\n      Fiat 128\n      32.4\n      4\n      78.7\n      66\n      4.08\n      2.200\n      19.47\n      1\n      1\n      4\n      1\n    \n    \n      27\n      Lotus Europa\n      30.4\n      4\n      95.1\n      113\n      3.77\n      1.513\n      16.90\n      1\n      1\n      5\n      2\n    \n    \n      18\n      Honda Civic\n      30.4\n      4\n      75.7\n      52\n      4.93\n      1.615\n      18.52\n      1\n      1\n      4\n      2\n    \n    \n      25\n      Fiat X1-9\n      27.3\n      4\n      79.0\n      66\n      4.08\n      1.935\n      18.90\n      1\n      1\n      4\n      1\n    \n  \n\n\n\n\n\n# ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ (ì•„ë˜ì—ì„œë¶€í„° ì˜¬ë¼ê°„ë‹¤)\ndf.sort_values('mpg', ascending=True).head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      15\n      Lincoln Continental\n      10.4\n      8\n      460.0\n      215\n      3.00\n      5.424\n      17.82\n      0\n      0\n      3\n      4\n    \n    \n      14\n      Cadillac Fleetwood\n      10.4\n      8\n      472.0\n      205\n      2.93\n      5.250\n      17.98\n      0\n      0\n      3\n      4\n    \n    \n      23\n      Camaro Z28\n      13.3\n      8\n      350.0\n      245\n      3.73\n      3.840\n      15.41\n      0\n      0\n      3\n      4\n    \n    \n      6\n      Duster 360\n      14.3\n      8\n      360.0\n      245\n      3.21\n      3.570\n      15.84\n      0\n      0\n      3\n      4\n    \n    \n      16\n      Chrysler Imperial\n      14.7\n      8\n      440.0\n      230\n      3.23\n      5.345\n      17.42\n      0\n      0\n      3\n      4\n    \n  \n\n\n\n\n\n\n5) ë°ì´í„° ë³€ê²½ (ì¡°ê±´ë¬¸)\n\nimport numpy as np \ndf = pd.read_csv(\"mtcars.txt\")\n# np.where í™œìš©\n# hp ë³€ìˆ˜ ê°’ ì¤‘ì—ì„œ 205ê°€ ë„˜ëŠ” ê°’ì€ 205ë¡œ ì²˜ë¦¬í•˜ê³ , ë‚˜ë¨¸ì§€ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€\ndf['hp'] = np.where(df['hp'] > 205, 205, df['hp'])\n\n# ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (ìœ„ì—ì„œë¶€í„° ë‚´ë ¤ê°„ë‹¤)\ndf.sort_values('hp', ascending=False).head(10)\n\n# í™œìš© : ì´ìƒì¹˜ë¥¼ Max ê°’ì´ë‚˜ Min ê°’ìœ¼ë¡œ ëŒ€ì²´í•  ê²½ìš° ì¡°ê±´ë¬¸ í™œìš©\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      16\n      Chrysler Imperial\n      14.7\n      8\n      440.0\n      205\n      3.23\n      5.345\n      17.42\n      0\n      0\n      3\n      4\n    \n    \n      30\n      Maserati Bora\n      15.0\n      8\n      301.0\n      205\n      3.54\n      3.570\n      14.60\n      0\n      1\n      5\n      8\n    \n    \n      28\n      Ford Pantera L\n      15.8\n      8\n      351.0\n      205\n      4.22\n      3.170\n      14.50\n      0\n      1\n      5\n      4\n    \n    \n      6\n      Duster 360\n      14.3\n      8\n      360.0\n      205\n      3.21\n      3.570\n      15.84\n      0\n      0\n      3\n      4\n    \n    \n      23\n      Camaro Z28\n      13.3\n      8\n      350.0\n      205\n      3.73\n      3.840\n      15.41\n      0\n      0\n      3\n      4\n    \n    \n      15\n      Lincoln Continental\n      10.4\n      8\n      460.0\n      205\n      3.00\n      5.424\n      17.82\n      0\n      0\n      3\n      4\n    \n    \n      14\n      Cadillac Fleetwood\n      10.4\n      8\n      472.0\n      205\n      2.93\n      5.250\n      17.98\n      0\n      0\n      3\n      4\n    \n    \n      13\n      Merc 450SLC\n      15.2\n      8\n      275.8\n      180\n      3.07\n      3.780\n      18.00\n      0\n      0\n      3\n      3\n    \n    \n      11\n      Merc 450SE\n      16.4\n      8\n      275.8\n      180\n      3.07\n      4.070\n      17.40\n      0\n      0\n      3\n      3\n    \n    \n      12\n      Merc 450SL\n      17.3\n      8\n      275.8\n      180\n      3.07\n      3.730\n      17.60\n      0\n      0\n      3\n      3"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_íŒŒì´ì¬ê¸°ì´ˆ.html#ê²°ì¸¡ì¹˜-ì´ìƒì¹˜-ì¤‘ë³µê°’-ì²˜ë¦¬ì œê±°-or-ëŒ€ì²´",
    "href": "BigData_Analysis/ì‹¤ê¸°_íŒŒì´ì¬ê¸°ì´ˆ.html#ê²°ì¸¡ì¹˜-ì´ìƒì¹˜-ì¤‘ë³µê°’-ì²˜ë¦¬ì œê±°-or-ëŒ€ì²´",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - íŒŒì´ì¬ ê¸°ì´ˆ",
    "section": "4. ê²°ì¸¡ì¹˜, ì´ìƒì¹˜, ì¤‘ë³µê°’ ì²˜ë¦¬(ì œê±° or ëŒ€ì²´)",
    "text": "4. ê²°ì¸¡ì¹˜, ì´ìƒì¹˜, ì¤‘ë³µê°’ ì²˜ë¦¬(ì œê±° or ëŒ€ì²´)\n\nğŸš¢ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° (íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹)\n\nì¢…ì†ë³€ìˆ˜(y) : ìƒì¡´ ì—¬ë¶€ (0 ì‚¬ë§, 1 ìƒì¡´)\n\n\në…ë¦½ë³€ìˆ˜(x) : pclass, sex, age ë“±ì˜ íƒ‘ìŠ¹ì ì •ë³´(ë³€ìˆ˜)\n\n\nimport seaborn as sns\n# ë°ì´í„°ì…‹ ëª©ë¡ : sns.get_dataset_names()\n# íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\ndf = sns.load_dataset('titanic')\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      survived\n      pclass\n      sex\n      age\n      sibsp\n      parch\n      fare\n      embarked\n      class\n      who\n      adult_male\n      deck\n      embark_town\n      alive\n      alone\n    \n  \n  \n    \n      0\n      0\n      3\n      male\n      22.0\n      1\n      0\n      7.2500\n      S\n      Third\n      man\n      True\n      NaN\n      Southampton\n      no\n      False\n    \n    \n      1\n      1\n      1\n      female\n      38.0\n      1\n      0\n      71.2833\n      C\n      First\n      woman\n      False\n      C\n      Cherbourg\n      yes\n      False\n    \n    \n      2\n      1\n      3\n      female\n      26.0\n      0\n      0\n      7.9250\n      S\n      Third\n      woman\n      False\n      NaN\n      Southampton\n      yes\n      True\n    \n    \n      3\n      1\n      1\n      female\n      35.0\n      1\n      0\n      53.1000\n      S\n      First\n      woman\n      False\n      C\n      Southampton\n      yes\n      False\n    \n    \n      4\n      0\n      3\n      male\n      35.0\n      0\n      0\n      8.0500\n      S\n      Third\n      man\n      True\n      NaN\n      Southampton\n      no\n      True\n    \n  \n\n\n\n\n\ndf.shape\n\n(891, 15)\n\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 15 columns):\n #   Column       Non-Null Count  Dtype   \n---  ------       --------------  -----   \n 0   survived     891 non-null    int64   \n 1   pclass       891 non-null    int64   \n 2   sex          891 non-null    object  \n 3   age          714 non-null    float64 \n 4   sibsp        891 non-null    int64   \n 5   parch        891 non-null    int64   \n 6   fare         891 non-null    float64 \n 7   embarked     889 non-null    object  \n 8   class        891 non-null    category\n 9   who          891 non-null    object  \n 10  adult_male   891 non-null    bool    \n 11  deck         203 non-null    category\n 12  embark_town  889 non-null    object  \n 13  alive        891 non-null    object  \n 14  alone        891 non-null    bool    \ndtypes: bool(2), category(2), float64(2), int64(4), object(5)\nmemory usage: 80.7+ KB\n\n\n\n\n1) ê²°ì¸¡ì¹˜ í™•ì¸ ë° ì²˜ë¦¬\n\n# ê²°ì¸¡ì¹˜ í™•ì¸ \ndf.isnull().sum()\n\nsurvived         0\npclass           0\nsex              0\nage            177\nsibsp            0\nparch            0\nfare             0\nembarked         2\nclass            0\nwho              0\nadult_male       0\ndeck           688\nembark_town      2\nalive            0\nalone            0\ndtype: int64\n\n\n\n# ê²°ì¸¡ì¹˜ ì œê±°\nprint(df.dropna(axis=0).shape) # í–‰ ê¸°ì¤€, defaultëŠ” axis = 0 ë”°ë¡œ ì„¤ì • ì•ˆí•´ë„ ë¨\nprint(df.dropna(axis=1).shape) # ì—´ ê¸°ì¤€\n\n(182, 15)\n(891, 11)\n\n\n\n# ê²°ì¸¡ì¹˜ ëŒ€ì²´\n# ë°ì´í„° ë³µì‚¬ \ndf2 = df.copy()\ndf2 = pd.DataFrame(df) # dfë¥¼ ë°ì´í„° í”„ë ˆì„ í˜•íƒœë¡œ ë³€í™˜\n\n\n# 1. ì¤‘ì•™ê°’/í‰ê· ê°’ ë“±ìœ¼ë¡œ ëŒ€ì²´\n\n# ë¨¼ì € ì¤‘ì•™ê°’ì„ êµ¬í•©ë‹ˆë‹¤\nmedian_age = df2['age'].median()\nprint(median_age)\n\n# í‰ê· ìœ¼ë¡œ ëŒ€ì²´í•  ê²½ìš° \n# mean_age = df2['age'].mean()\n\n28.0\n\n\n\n# êµ¬í•œ ì¤‘ì•™ê°’ìœ¼ë¡œ ê²°ì¸¡ì¹˜ë¥¼ ëŒ€ì²´í•©ë‹ˆë‹¤.\ndf2['age'] = df['age'].fillna(median_age)\n\n\n# ê²°ì¸¡ì¹˜ê°€ ì˜ ëŒ€ì²´ë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤\ndf2.isnull().sum()\n\nsurvived         0\npclass           0\nsex              0\nage              0\nsibsp            0\nparch            0\nfare             0\nembarked         2\nclass            0\nwho              0\nadult_male       0\ndeck           688\nembark_town      2\nalive            0\nalone            0\ndtype: int64\n\n\n\nprint(df['age'].mean()) # ì›ë³¸ ë°ì´í„°\nprint(df2['age'].mean()) # ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´í•œ ë°ì´í„° \n\n29.69911764705882\n29.36158249158249\n\n\n\n# ì¤‘ë³µê°’ í™•ì¸\ndf.drop_duplicates().shape\n\n(784, 15)\n\n\n\n\n2) ì´ìƒì¹˜ í™•ì¸ ë° ì²˜ë¦¬\n\n\nâœ” ìƒìê·¸ë¦¼ í™œìš© (ì´ìƒì¹˜: Q1, Q3ë¡œë¶€í„° 1.5*IQRì„ ì´ˆê³¼í•˜ëŠ” ê°’)\n\n# íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\ndf = sns.load_dataset('titanic')\n\n# (ì°¸ê³ ) ìƒìê·¸ë¦¼\nsns.boxplot(df['age'])\n\nC:\\Users\\Hyunsoo Kim\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n  warnings.warn(\n\n\n<AxesSubplot:xlabel='age'>\n\n\n\n\n\n\n# Q1, Q3, IQR êµ¬í•˜ê¸°\nQ1 = df['age'].quantile(.25)\nQ3 = df['age'].quantile(.75)\nIQR = Q3 - Q1\nprint(Q1, Q3, IQR)\n\n20.125 38.0 17.875\n\n\n\nupper = Q3 + 1.5*IQR\nlower = Q1 - 1.5*IQR\nprint(upper, lower)\n\n64.8125 -6.6875\n\n\n\n# ë¬¸ì œ : age ë³€ìˆ˜ì˜ ì´ìƒì¹˜ë¥¼ ì œì™¸í•œ ë°ì´í„° ìˆ˜ëŠ”? (ìƒìê·¸ë¦¼ ê¸°ì¤€)\ncond1 = (df['age'] <= upper)\ncond2 = (df['age'] >= lower)\nprint(len(df[cond1 & cond2]))\nprint(len(df[cond1]))\nprint(len(df))\n\n703\n703\n891\n\n\n\n# ë¬¸ì œ : age ë³€ìˆ˜ì˜ ì´ìƒì¹˜ë¥¼ ì œì™¸í•œ ë°ì´í„°ì…‹ í™•ì¸(ìƒìê·¸ë¦¼ ê¸°ì¤€)\ndf_new = df[cond1 & cond2]\ndf_new\n\n\n\n\n\n  \n    \n      \n      survived\n      pclass\n      sex\n      age\n      sibsp\n      parch\n      fare\n      embarked\n      class\n      who\n      adult_male\n      deck\n      embark_town\n      alive\n      alone\n    \n  \n  \n    \n      0\n      0\n      3\n      male\n      22.0\n      1\n      0\n      7.2500\n      S\n      Third\n      man\n      True\n      NaN\n      Southampton\n      no\n      False\n    \n    \n      1\n      1\n      1\n      female\n      38.0\n      1\n      0\n      71.2833\n      C\n      First\n      woman\n      False\n      C\n      Cherbourg\n      yes\n      False\n    \n    \n      2\n      1\n      3\n      female\n      26.0\n      0\n      0\n      7.9250\n      S\n      Third\n      woman\n      False\n      NaN\n      Southampton\n      yes\n      True\n    \n    \n      3\n      1\n      1\n      female\n      35.0\n      1\n      0\n      53.1000\n      S\n      First\n      woman\n      False\n      C\n      Southampton\n      yes\n      False\n    \n    \n      4\n      0\n      3\n      male\n      35.0\n      0\n      0\n      8.0500\n      S\n      Third\n      man\n      True\n      NaN\n      Southampton\n      no\n      True\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      885\n      0\n      3\n      female\n      39.0\n      0\n      5\n      29.1250\n      Q\n      Third\n      woman\n      False\n      NaN\n      Queenstown\n      no\n      False\n    \n    \n      886\n      0\n      2\n      male\n      27.0\n      0\n      0\n      13.0000\n      S\n      Second\n      man\n      True\n      NaN\n      Southampton\n      no\n      True\n    \n    \n      887\n      1\n      1\n      female\n      19.0\n      0\n      0\n      30.0000\n      S\n      First\n      woman\n      False\n      B\n      Southampton\n      yes\n      True\n    \n    \n      889\n      1\n      1\n      male\n      26.0\n      0\n      0\n      30.0000\n      C\n      First\n      man\n      True\n      C\n      Cherbourg\n      yes\n      True\n    \n    \n      890\n      0\n      3\n      male\n      32.0\n      0\n      0\n      7.7500\n      Q\n      Third\n      man\n      True\n      NaN\n      Queenstown\n      no\n      True\n    \n  \n\n703 rows Ã— 15 columns\n\n\n\n\n\nâœ” í‘œì¤€ì •ê·œë¶„í¬ í™œìš©(ì´ìƒì¹˜ : \\(\\pm\\) 3Z ê°’ì„ ë„˜ì–´ê°€ëŠ” ê°’)\n\n# ë°ì´í„° í‘œì¤€í™”, Z = (ê°œë³„ê°’ -  í‰ê· ) / í‘œì¤€í¸ì°¨\n\n\nmean_age = df['age'].mean()\nstd_age = df['age'].std()\nprint(mean_age)\nprint(std_age)\n\n29.69911764705882\n14.526497332334044\n\n\n\nznorm = (df['age']-mean_age) / std_age\nznorm\n\n0     -0.530005\n1      0.571430\n2     -0.254646\n3      0.364911\n4      0.364911\n         ...   \n886   -0.185807\n887   -0.736524\n888         NaN\n889   -0.254646\n890    0.158392\nName: age, Length: 891, dtype: float64\n\n\n\n# ë¬¸ì œ : ì´ìƒì¹˜ì˜ ê°œìˆ˜ëŠ” ëª‡ê°œì¸ê°€? (: Â±3Z ê¸°ì¤€)\n\n\ncond1 = (znorm > 3)\nlen(df[cond1])\n\n2\n\n\n\ncond2 = (znorm < -3)\nlen(df[cond2])\n\n0\n\n\n\nprint(len(df[cond1]) + len(df[cond2]))\n\n2\n\n\n\n\n3) ì¤‘ë³µê°’ ì œê±°\n\n# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\ndf = sns.load_dataset('titanic')\n\n\ndf.shape\n\n(891, 15)\n\n\n\ndf1 = df.copy()\ndf1 = df1.drop_duplicates()\nprint(df1.shape)\n# (ì£¼ì˜) ì˜ˆì œì—ì„œëŠ” ì¤‘ë³µê°’ì´ ìˆì–´ì„œ ì œê±°í–ˆì§€ë§Œ,\n# ì¤‘ë³µê°’ì´ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ìƒí™©ì´ë³€ ì œê±°í•  í•„ìš”ì—†ìŒ\n\n(784, 15)"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_íŒŒì´ì¬ê¸°ì´ˆ.html#ë°ì´í„°-scalingë°ì´í„°-í‘œì¤€í™”-ì •ê·œí™”",
    "href": "BigData_Analysis/ì‹¤ê¸°_íŒŒì´ì¬ê¸°ì´ˆ.html#ë°ì´í„°-scalingë°ì´í„°-í‘œì¤€í™”-ì •ê·œí™”",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - íŒŒì´ì¬ ê¸°ì´ˆ",
    "section": "âœ… 5. ë°ì´í„° scaling(ë°ì´í„° í‘œì¤€í™”, ì •ê·œí™”)",
    "text": "âœ… 5. ë°ì´í„° scaling(ë°ì´í„° í‘œì¤€í™”, ì •ê·œí™”)\n\n1) ë°ì´í„° í‘œì¤€í™”(Z-score normalization)\n\ndf = pd.read_csv(\"mtcars.txt\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      Hornet Sportabout\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\nzscaler = StandardScaler() # ë³€ìˆ˜ëª…ì€ ì‚¬ìš©í•˜ê¸° í¸í•œ ë³€ìˆ˜ëª…ìœ¼ë¡œ ì‚¬ìš©\ndf['mpg'] = zscaler.fit_transform(df[['mpg']])\ndf.head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      0.153299\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      0.153299\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      0.456737\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      Hornet 4 Drive\n      0.220730\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      Hornet Sportabout\n      -0.234427\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\n# í™•ì¸\nprint(df['mpg'].mean(), df['mpg'].std())\n\n-5.48172618408671e-16 1.016001016001524\n\n\n\n\n2) ë°ì´í„° ì •ê·œí™”(min-max normalization)\n\ndf = pd.read_csv(\"mtcars.txt\")\ndf.head\n\nfrom sklearn.preprocessing import MinMaxScaler\nmscaler = MinMaxScaler()\ndf['mpg'] = mscaler.fit_transform(df[['mpg']])\ndf.head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      0.451064\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      0.451064\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      0.527660\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      Hornet 4 Drive\n      0.468085\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      Hornet Sportabout\n      0.353191\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\n# í™•ì¸\nprint(df['mpg'].min(), df['mpg'].max())\n\n0.0 1.0"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_íŒŒì´ì¬ê¸°ì´ˆ.html#ë°ì´í„°-í•©ì¹˜ê¸°",
    "href": "BigData_Analysis/ì‹¤ê¸°_íŒŒì´ì¬ê¸°ì´ˆ.html#ë°ì´í„°-í•©ì¹˜ê¸°",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - íŒŒì´ì¬ ê¸°ì´ˆ",
    "section": "6. ë°ì´í„° í•©ì¹˜ê¸°",
    "text": "6. ë°ì´í„° í•©ì¹˜ê¸°\n\n# í–‰, ì—´ ë°©í–¥ìœ¼ë¡œ ë°ì´í„° í•©ì¹˜ê¸°\ndf = sns.load_dataset('iris')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      sepal_length\n      sepal_width\n      petal_length\n      petal_width\n      species\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      setosa\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      setosa\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      setosa\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      setosa\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      setosa\n    \n  \n\n\n\n\n\n# ë°ì´í„° 2ê°œë¡œ ë¶„ë¦¬\ndf1 = df.loc[0:30, ] # 0~30í–‰ ë°ì´í„°\ndf2 = df.loc[31:60, ] # 31~60í–‰ ë°ì´í„° \n\n\ndf1.head()\n\n\n\n\n\n  \n    \n      \n      sepal_length\n      sepal_width\n      petal_length\n      petal_width\n      species\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      setosa\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      setosa\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      setosa\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      setosa\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      setosa\n    \n  \n\n\n\n\n\ndf2.head()\n\n\n\n\n\n  \n    \n      \n      sepal_length\n      sepal_width\n      petal_length\n      petal_width\n      species\n    \n  \n  \n    \n      31\n      5.4\n      3.4\n      1.5\n      0.4\n      setosa\n    \n    \n      32\n      5.2\n      4.1\n      1.5\n      0.1\n      setosa\n    \n    \n      33\n      5.5\n      4.2\n      1.4\n      0.2\n      setosa\n    \n    \n      34\n      4.9\n      3.1\n      1.5\n      0.2\n      setosa\n    \n    \n      35\n      5.0\n      3.2\n      1.2\n      0.2\n      setosa\n    \n  \n\n\n\n\n\ndf_sum = pd.concat([df1, df2], axis=0) # í–‰ ë°©í–¥ìœ¼ë¡œ ê²°í•© (ìœ„, ì•„ë˜)\nprint(df_sum.head())\nprint(df_sum.shape)\n\n   sepal_length  sepal_width  petal_length  petal_width species\n0           5.1          3.5           1.4          0.2  setosa\n1           4.9          3.0           1.4          0.2  setosa\n2           4.7          3.2           1.3          0.2  setosa\n3           4.6          3.1           1.5          0.2  setosa\n4           5.0          3.6           1.4          0.2  setosa\n(61, 5)\n\n\n\n# ë°ì´í„° 2ê°œë¡œ ë‚˜ëˆ„ê¸°\ndf1 = df.loc[:, 'sepal_length':'petal_length'] # 1~3ì—´ ì¶”ì¶œ ë°ì´í„°\ndf2 = df.loc[:, ['petal_width','species']] # 4~5ì—´ ì¶”ì¶œ ë°ì´í„°\n\n\ndf_sum = pd.concat([df1, df2], axis=1) # ì—´ ë°©í–¥ìœ¼ë¡œ ê²°í•© (ì¢Œ, ìš°)\ndf_sum.head()\n\n\n\n\n\n  \n    \n      \n      sepal_length\n      sepal_width\n      petal_length\n      petal_width\n      species\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      setosa\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      setosa\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      setosa\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      setosa\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      setosa"
  },
  {
    "objectID": "BigData_Analysis/ì‹¤ê¸°_íŒŒì´ì¬ê¸°ì´ˆ.html#ë‚ ì§œì‹œê°„-ë°ì´í„°-index-ë‹¤ë£¨ê¸°",
    "href": "BigData_Analysis/ì‹¤ê¸°_íŒŒì´ì¬ê¸°ì´ˆ.html#ë‚ ì§œì‹œê°„-ë°ì´í„°-index-ë‹¤ë£¨ê¸°",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - íŒŒì´ì¬ ê¸°ì´ˆ",
    "section": "7. ë‚ ì§œ/ì‹œê°„ ë°ì´í„°, index ë‹¤ë£¨ê¸°",
    "text": "7. ë‚ ì§œ/ì‹œê°„ ë°ì´í„°, index ë‹¤ë£¨ê¸°\n\n1) ë‚ ì§œ ë‹¤ë£¨ê¸°\n\n# ë°ì´í„° ë§Œë“¤ê¸°\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20230105','20230105','20230223','20230223',\n            '20230312','20230422','20230511'],\n    'ë¬¼í’ˆ' : ['A','B','A','B','A','B','A'],\n    'íŒë§¤ìˆ˜' : [5,10,15,15,20,25,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [500,600,500,600,600,700,600] })\ndf\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ë¬¼í’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      20230105\n      A\n      5\n      500\n    \n    \n      1\n      20230105\n      B\n      10\n      600\n    \n    \n      2\n      20230223\n      A\n      15\n      500\n    \n    \n      3\n      20230223\n      B\n      15\n      600\n    \n    \n      4\n      20230312\n      A\n      20\n      600\n    \n    \n      5\n      20230422\n      B\n      25\n      700\n    \n    \n      6\n      20230511\n      A\n      40\n      600\n    \n  \n\n\n\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7 entries, 0 to 6\nData columns (total 4 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   ë‚ ì§œ      7 non-null      object\n 1   ë¬¼í’ˆ      7 non-null      object\n 2   íŒë§¤ìˆ˜     7 non-null      int64 \n 3   ê°œë‹¹ìˆ˜ìµ    7 non-null      int64 \ndtypes: int64(2), object(2)\nmemory usage: 352.0+ bytes\n\n\n\n# ë°ì´í„° íƒ€ì… datetimeìœ¼ë¡œ ë³€ê²½\ndf['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7 entries, 0 to 6\nData columns (total 4 columns):\n #   Column  Non-Null Count  Dtype         \n---  ------  --------------  -----         \n 0   ë‚ ì§œ      7 non-null      datetime64[ns]\n 1   ë¬¼í’ˆ      7 non-null      object        \n 2   íŒë§¤ìˆ˜     7 non-null      int64         \n 3   ê°œë‹¹ìˆ˜ìµ    7 non-null      int64         \ndtypes: datetime64[ns](1), int64(2), object(1)\nmemory usage: 352.0+ bytes\n\n\n\n# ë…„, ì›”, ì¼ ë³€ìˆ˜(ì—´) ì¶”ê°€í•˜ê¸°\ndf['year'] = df['ë‚ ì§œ'].dt.year\ndf['month'] = df['ë‚ ì§œ'].dt.month\ndf['day'] = df['ë‚ ì§œ'].dt.day\ndf\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ë¬¼í’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n      year\n      month\n      day\n    \n  \n  \n    \n      0\n      2023-01-05\n      A\n      5\n      500\n      2023\n      1\n      5\n    \n    \n      1\n      2023-01-05\n      B\n      10\n      600\n      2023\n      1\n      5\n    \n    \n      2\n      2023-02-23\n      A\n      15\n      500\n      2023\n      2\n      23\n    \n    \n      3\n      2023-02-23\n      B\n      15\n      600\n      2023\n      2\n      23\n    \n    \n      4\n      2023-03-12\n      A\n      20\n      600\n      2023\n      3\n      12\n    \n    \n      5\n      2023-04-22\n      B\n      25\n      700\n      2023\n      4\n      22\n    \n    \n      6\n      2023-05-11\n      A\n      40\n      600\n      2023\n      5\n      11\n    \n  \n\n\n\n\n\n# ë‚ ì§œ êµ¬ê°„ í•„í„°ë§\ndf[df['ë‚ ì§œ'].between('2023-01-01', '2023-01-31')] # 1ì›” 31ì¼ì€ ë¯¸í¬í•¨\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ë¬¼í’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n      year\n      month\n      day\n    \n  \n  \n    \n      0\n      2023-01-05\n      A\n      5\n      500\n      2023\n      1\n      5\n    \n    \n      1\n      2023-01-05\n      B\n      10\n      600\n      2023\n      1\n      5\n    \n  \n\n\n\n\n\n# ë‚ ì§œë¥¼ ì¸ë±ìŠ¤ë¡œ ì„¤ì •í›„ loc í•¨ìˆ˜ ì‚¬ìš©\n# ë°ì´í„° ë§Œë“¤ê¸° \ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20230105','20230105','20230223','20230223',\n            '20230312','20230422','20230511'],\n    'ë¬¼í’ˆ' : ['A','B','A','B','A','B','A'],\n    'íŒë§¤ìˆ˜' : [5,10,15,15,20,25,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [500,600,500,600,600,700,600] })\n\n# ë°ì´í„° íƒ€ì… datetimeìœ¼ë¡œ ë³€ê²½(í•„ìˆ˜)\ndf['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])\n\ndf = df.set_index('ë‚ ì§œ', drop=True) # drop=True(ë””í´íŠ¸) or False\ndf.head(3)\n\n\n\n\n\n  \n    \n      \n      ë¬¼í’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n    \n      ë‚ ì§œ\n      \n      \n      \n    \n  \n  \n    \n      2023-01-05\n      A\n      5\n      500\n    \n    \n      2023-01-05\n      B\n      10\n      600\n    \n    \n      2023-02-23\n      A\n      15\n      500\n    \n  \n\n\n\n\n\nprint(df.loc['2023-01-05':'2023-02-23']) # ë‘˜ë‹¤ ê¸°ê°„ í¬í•¨\nprint(df.loc[ (df.index>='2023-01-05') & (df.index<='2023-02-23') ])\n\n           ë¬¼í’ˆ  íŒë§¤ìˆ˜  ê°œë‹¹ìˆ˜ìµ\në‚ ì§œ                      \n2023-01-05  A    5   500\n2023-01-05  B   10   600\n2023-02-23  A   15   500\n2023-02-23  B   15   600\n           ë¬¼í’ˆ  íŒë§¤ìˆ˜  ê°œë‹¹ìˆ˜ìµ\në‚ ì§œ                      \n2023-01-05  A    5   500\n2023-01-05  B   10   600\n2023-02-23  A   15   500\n2023-02-23  B   15   600\n\n\n\n\n2) ì‹œê°„ ë‹¤ë£¨ê¸°\n\n# ì‹œê°„ ë°ì´í„° ë§Œë“¤ê¸° \ndf = pd.DataFrame({\n    'ë¬¼í’ˆ' : ['A','B','A','B','A','B','A'],\n    'íŒë§¤ìˆ˜' : [5,10,15,15,20,25,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [500,600,500,600,600,700,600] })\ntime = pd.date_range('2023-09-24 12:25:00', '2023-09-25 14:45:30', periods=7)\ndf['time'] = time\ndf = df[['time','ë¬¼í’ˆ','íŒë§¤ìˆ˜','ê°œë‹¹ìˆ˜ìµ']]\ndf\n\n\n\n\n\n  \n    \n      \n      time\n      ë¬¼í’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      1\n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2\n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      3\n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      4\n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      5\n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      6\n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# index ì´ˆê¸°í™” (ì¸ë±ìŠ¤ë¥¼ ì»¬ëŸ¼ìœ¼ë¡œ)\n# df = df.reset_index()\n# df\n\n\n# index ìƒˆë¡œ ì§€ì •\ndf = df.set_index('time')\ndf\n\n\n\n\n\n  \n    \n      \n      ë¬¼í’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n    \n      time\n      \n      \n      \n    \n  \n  \n    \n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# ì‹œê°„ ë°ì´í„° ë‹¤ë£¨ê¸°(ì£¼ì˜: ì‹œê°„ì´ indexì— ìœ„ì¹˜í•´ì•¼ í•¨)\ndf.between_time(start_time='12:25', end_time='21:00') #ì‹œê°„ ì‹œì‘, ë ëª¨ë‘ í¬í•¨\n# include_start=False, include_end=False ì˜µì…˜ì„ ì‹œì‘, ë ì‹œê°„ ì œì™¸ ê°€ëŠ¥\n\n\n\n\n\n  \n    \n      \n      ë¬¼í’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n    \n      time\n      \n      \n      \n    \n  \n  \n    \n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# ë‚ ì§œë¥¼ ì¸ë±ìŠ¤ë¡œ ì„¤ì •í›„ loc í•¨ìˆ˜ ì‚¬ìš©\nprint(df.loc['2023-09-24 12:25:00':'2023-09-24 21:11:50']) # ë‘˜ë‹¤ í¬í•¨\nprint(df.loc[ (df.index>='2023-09-24 12:25:00') & (df.index<='2023-09-24 21:11:50') ])\n\n                    ë¬¼í’ˆ  íŒë§¤ìˆ˜  ê°œë‹¹ìˆ˜ìµ\ntime                             \n2023-09-24 12:25:00  A    5   500\n2023-09-24 16:48:25  B   10   600\n2023-09-24 21:11:50  A   15   500\n                    ë¬¼í’ˆ  íŒë§¤ìˆ˜  ê°œë‹¹ìˆ˜ìµ\ntime                             \n2023-09-24 12:25:00  A    5   500\n2023-09-24 16:48:25  B   10   600\n2023-09-24 21:11:50  A   15   500"
  },
  {
    "objectID": "BigData_Analysis.html",
    "href": "BigData_Analysis.html",
    "title": "Big Data Analysis Engineer",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nsklearn\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nsklearn\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nscipy\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nscipy\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nscipy\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nscipy\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nsklearn\n\n\n\n\n\n\n\n\n\n\n\nOct 23, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nsklearn\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nsklearn\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nsklearn\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nsklearn\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html",
    "href": "Data_Mining/2022-03-04-numpy.html",
    "title": "Numpy ê¸°ë³¸",
    "section": "",
    "text": "numpy ê¸°ë³¸ ì½”ë“œ ì‹¤ìŠµ\në„êµ¬ - ë„˜íŒŒì´(NumPy)\n*ë„˜íŒŒì´(NumPy)ëŠ” íŒŒì´ì¬ì˜ ê³¼í•™ ì»´í“¨íŒ…ì„ ìœ„í•œ ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. ë„˜íŒŒì´ì˜ í•µì‹¬ì€ ê°•ë ¥í•œ N-ì°¨ì› ë°°ì—´ ê°ì²´ì…ë‹ˆë‹¤. ë˜í•œ ì„ í˜• ëŒ€ìˆ˜, í‘¸ë¦¬ì—(Fourier) ë³€í™˜, ìœ ì‚¬ ë‚œìˆ˜ ìƒì„±ê³¼ ê°™ì€ ìœ ìš©í•œ í•¨ìˆ˜ë“¤ë„ ì œê³µí•©ë‹ˆë‹¤.â€"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.zeros",
    "href": "Data_Mining/2022-03-04-numpy.html#np.zeros",
    "title": "Numpy ê¸°ë³¸",
    "section": "np.zeros",
    "text": "np.zeros\nzeros í•¨ìˆ˜ëŠ” 0ìœ¼ë¡œ ì±„ì›Œì§„ ë°°ì—´ì„ ë§Œë“­ë‹ˆë‹¤:\n\nnp.zeros(5)\n\narray([0., 0., 0., 0., 0.])\n\n\n2D ë°°ì—´(ì¦‰, í–‰ë ¬)ì„ ë§Œë“¤ë ¤ë©´ ì›í•˜ëŠ” í–‰ê³¼ ì—´ì˜ í¬ê¸°ë¥¼ íŠœí”Œë¡œ ì „ë‹¬í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒì€ \\(3 \\times 4\\) í¬ê¸°ì˜ í–‰ë ¬ì…ë‹ˆë‹¤:\n\nnp.zeros((3,4))\n\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#ìš©ì–´",
    "href": "Data_Mining/2022-03-04-numpy.html#ìš©ì–´",
    "title": "Numpy ê¸°ë³¸",
    "section": "ìš©ì–´",
    "text": "ìš©ì–´\n\në„˜íŒŒì´ì—ì„œ ê° ì°¨ì›ì„ ì¶•(axis) ì´ë¼ê³  í•©ë‹ˆë‹¤\nì¶•ì˜ ê°œìˆ˜ë¥¼ ë­í¬(rank) ë¼ê³  í•©ë‹ˆë‹¤.\n\nì˜ˆë¥¼ ë“¤ì–´, ìœ„ì˜ \\(3 \\times 4\\) í–‰ë ¬ì€ ë­í¬ 2ì¸ ë°°ì—´ì…ë‹ˆë‹¤(ì¦‰ 2ì°¨ì›ì…ë‹ˆë‹¤).\nì²« ë²ˆì§¸ ì¶•ì˜ ê¸¸ì´ëŠ” 3ì´ê³  ë‘ ë²ˆì§¸ ì¶•ì˜ ê¸¸ì´ëŠ” 4ì…ë‹ˆë‹¤.\n\në°°ì—´ì˜ ì¶• ê¸¸ì´ë¥¼ ë°°ì—´ì˜ í¬ê¸°(shape)ë¼ê³  í•©ë‹ˆë‹¤.\n\nì˜ˆë¥¼ ë“¤ì–´, ìœ„ í–‰ë ¬ì˜ í¬ê¸°ëŠ” (3, 4)ì…ë‹ˆë‹¤.\në­í¬ëŠ” í¬ê¸°ì˜ ê¸¸ì´ì™€ ê°™ìŠµë‹ˆë‹¤.\n\në°°ì—´ì˜ ì‚¬ì´ì¦ˆ(size)ëŠ” ì „ì²´ ì›ì†Œì˜ ê°œìˆ˜ì…ë‹ˆë‹¤. ì¶•ì˜ ê¸¸ì´ë¥¼ ëª¨ë‘ ê³±í•´ì„œ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤(ê°€ë ¹, \\(3 \\times 4=12\\)).\n\n\na = np.zeros((3,4))\na\n\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])\n\n\n\na.shape\n\n(3, 4)\n\n\n\na.ndim  # len(a.shape)ì™€ ê°™ìŠµë‹ˆë‹¤\n\n2\n\n\n\na.size\n\n12"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#n-ì°¨ì›-ë°°ì—´",
    "href": "Data_Mining/2022-03-04-numpy.html#n-ì°¨ì›-ë°°ì—´",
    "title": "Numpy ê¸°ë³¸",
    "section": "N-ì°¨ì› ë°°ì—´",
    "text": "N-ì°¨ì› ë°°ì—´\nì„ì˜ì˜ ë­í¬ ìˆ˜ë¥¼ ê°€ì§„ N-ì°¨ì› ë°°ì—´ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë‹¤ìŒì€ í¬ê¸°ê°€ (2,3,4)ì¸ 3D ë°°ì—´(ë­í¬=3)ì…ë‹ˆë‹¤:\n\nnp.zeros((2,2,5))\n\narray([[[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]],\n\n       [[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#ë°°ì—´-íƒ€ì…",
    "href": "Data_Mining/2022-03-04-numpy.html#ë°°ì—´-íƒ€ì…",
    "title": "Numpy ê¸°ë³¸",
    "section": "ë°°ì—´ íƒ€ì…",
    "text": "ë°°ì—´ íƒ€ì…\në„˜íŒŒì´ ë°°ì—´ì˜ íƒ€ì…ì€ ndarrayì…ë‹ˆë‹¤:\n\ntype(np.zeros((3,4)))\n\nnumpy.ndarray"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.ones",
    "href": "Data_Mining/2022-03-04-numpy.html#np.ones",
    "title": "Numpy ê¸°ë³¸",
    "section": "np.ones",
    "text": "np.ones\nndarrayë¥¼ ë§Œë“¤ ìˆ˜ ìˆëŠ” ë„˜íŒŒì´ í•¨ìˆ˜ê°€ ë§ìŠµë‹ˆë‹¤.\në‹¤ìŒì€ 1ë¡œ ì±„ì›Œì§„ \\(3 \\times 4\\) í¬ê¸°ì˜ í–‰ë ¬ì…ë‹ˆë‹¤:\n\nnp.ones((3,4))\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.full",
    "href": "Data_Mining/2022-03-04-numpy.html#np.full",
    "title": "Numpy ê¸°ë³¸",
    "section": "np.full",
    "text": "np.full\nì£¼ì–´ì§„ ê°’ìœ¼ë¡œ ì§€ì •ëœ í¬ê¸°ì˜ ë°°ì—´ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. ë‹¤ìŒì€ Ï€ë¡œ ì±„ì›Œì§„ \\(3 \\times 4\\) í¬ê¸°ì˜ í–‰ë ¬ì…ë‹ˆë‹¤.\n\nnp.full((3,4), np.pi)\n\narray([[3.14159265, 3.14159265, 3.14159265, 3.14159265],\n       [3.14159265, 3.14159265, 3.14159265, 3.14159265],\n       [3.14159265, 3.14159265, 3.14159265, 3.14159265]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.empty",
    "href": "Data_Mining/2022-03-04-numpy.html#np.empty",
    "title": "Numpy ê¸°ë³¸",
    "section": "np.empty",
    "text": "np.empty\nì´ˆê¸°í™”ë˜ì§€ ì•Šì€ \\(2 \\times 3\\) í¬ê¸°ì˜ ë°°ì—´ì„ ë§Œë“­ë‹ˆë‹¤(ë°°ì—´ì˜ ë‚´ìš©ì€ ì˜ˆì¸¡ì´ ë¶ˆê°€ëŠ¥í•˜ë©° ë©”ëª¨ë¦¬ ìƒí™©ì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤):\n\nnp.empty((2,3))\n\narray([[9.6677106e-317, 0.0000000e+000, 0.0000000e+000],\n       [0.0000000e+000, 0.0000000e+000, 0.0000000e+000]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.array",
    "href": "Data_Mining/2022-03-04-numpy.html#np.array",
    "title": "Numpy ê¸°ë³¸",
    "section": "np.array",
    "text": "np.array\narray í•¨ìˆ˜ëŠ” íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ndarrayë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤:\n\nnp.array([[1,2,3,4], [10, 20, 30, 40]])\n\narray([[ 1,  2,  3,  4],\n       [10, 20, 30, 40]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.arange",
    "href": "Data_Mining/2022-03-04-numpy.html#np.arange",
    "title": "Numpy ê¸°ë³¸",
    "section": "np.arange",
    "text": "np.arange\níŒŒì´ì¬ì˜ ê¸°ë³¸ range í•¨ìˆ˜ì™€ ë¹„ìŠ·í•œ ë„˜íŒŒì´ arange í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ndarrayë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\nnp.arange(1, 5)\n\narray([1, 2, 3, 4])\n\n\në¶€ë™ ì†Œìˆ˜ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤:\n\nnp.arange(1.0, 5.0)\n\narray([1., 2., 3., 4.])\n\n\níŒŒì´ì¬ì˜ ê¸°ë³¸ range í•¨ìˆ˜ì²˜ëŸ¼ ê±´ë„ˆ ë›°ëŠ” ì •ë„ë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\nnp.arange(1, 5, 0.5)\n\narray([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5])\n\n\në¶€ë™ ì†Œìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ì›ì†Œì˜ ê°œìˆ˜ê°€ ì¼ì •í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\nprint(np.arange(0, 5/3, 1/3)) # ë¶€ë™ ì†Œìˆ˜ ì˜¤ì°¨ ë•Œë¬¸ì—, ìµœëŒ“ê°’ì€ 4/3 ë˜ëŠ” 5/3ì´ ë©ë‹ˆë‹¤.\nprint(np.arange(0, 5/3, 0.333333333))\nprint(np.arange(0, 5/3, 0.333333334))\n\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]\n[0.         0.33333333 0.66666667 1.         1.33333334]"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.linspace",
    "href": "Data_Mining/2022-03-04-numpy.html#np.linspace",
    "title": "Numpy ê¸°ë³¸",
    "section": "np.linspace",
    "text": "np.linspace\nì´ëŸ° ì´ìœ ë¡œ ë¶€ë™ ì†Œìˆ˜ë¥¼ ì‚¬ìš©í•  ë• arange ëŒ€ì‹ ì— linspace í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. linspace í•¨ìˆ˜ëŠ” ì§€ì •ëœ ê°œìˆ˜ë§Œí¼ ë‘ ê°’ ì‚¬ì´ë¥¼ ë‚˜ëˆˆ ë°°ì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤(arangeì™€ëŠ” ë‹¤ë¥´ê²Œ ìµœëŒ“ê°’ì´ í¬í•¨ë©ë‹ˆë‹¤):\n\nprint(np.linspace(0, 5/3, 6))\n\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.randì™€-np.randn",
    "href": "Data_Mining/2022-03-04-numpy.html#np.randì™€-np.randn",
    "title": "Numpy ê¸°ë³¸",
    "section": "np.randì™€ np.randn",
    "text": "np.randì™€ np.randn\në„˜íŒŒì´ì˜ random ëª¨ë“ˆì—ëŠ” ndarrayë¥¼ ëœë¤í•œ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”í•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜ë“¤ì´ ë§ì´ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë‹¤ìŒì€ (ê· ë“± ë¶„í¬ì¸) 0ê³¼ 1ì‚¬ì´ì˜ ëœë¤í•œ ë¶€ë™ ì†Œìˆ˜ë¡œ \\(3 \\times 4\\) í–‰ë ¬ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤:\n\nnp.random.rand(3,4)\n\narray([[0.37892456, 0.17966937, 0.38206837, 0.34922123],\n       [0.80462136, 0.9845914 , 0.9416127 , 0.28305275],\n       [0.21201033, 0.54891417, 0.03781613, 0.4369229 ]])\n\n\në‹¤ìŒì€ í‰ê· ì´ 0ì´ê³  ë¶„ì‚°ì´ 1ì¸ ì¼ë³€ëŸ‰ ì •ê·œ ë¶„í¬(ê°€ìš°ì‹œì•ˆ ë¶„í¬)ì—ì„œ ìƒ˜í”Œë§í•œ ëœë¤í•œ ë¶€ë™ ì†Œìˆ˜ë¥¼ ë‹´ì€ \\(3 \\times 4\\) í–‰ë ¬ì…ë‹ˆë‹¤:\n\nnp.random.randn(3,4)\n\narray([[ 0.83811287, -0.57131751, -0.4381827 ,  1.1485899 ],\n       [ 1.45316084, -0.47259181, -1.23426057, -0.0669813 ],\n       [ 1.01003549,  1.04381736, -0.93060038,  2.39043293]])\n\n\nì´ ë¶„í¬ì˜ ëª¨ì–‘ì„ ì•Œë ¤ë©´ ë§·í”Œë¡¯ë¦½ì„ ì‚¬ìš©í•´ ê·¸ë ¤ë³´ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤(ë” ìì„¸í•œ ê²ƒì€ ë§·í”Œë¡¯ë¦½ íŠœí† ë¦¬ì–¼ì„ ì°¸ê³ í•˜ì„¸ìš”):\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\n\nplt.hist(np.random.rand(100000), density=True, bins=100, histtype=\"step\", color=\"blue\", label=\"rand\")\nplt.hist(np.random.randn(100000), density=True, bins=100, histtype=\"step\", color=\"red\", label=\"randn\")\nplt.axis([-2.5, 2.5, 0, 1.1])\nplt.legend(loc = \"upper left\")\nplt.title(\"Random distributions\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Density\")\nplt.show()"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.fromfunction",
    "href": "Data_Mining/2022-03-04-numpy.html#np.fromfunction",
    "title": "Numpy ê¸°ë³¸",
    "section": "np.fromfunction",
    "text": "np.fromfunction\ní•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ndarrayë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:\n\ndef my_function(z, y, x):\n    return x + 10 * y + 100 * z\n\nnp.fromfunction(my_function, (3, 2, 10))\n\narray([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.],\n        [ 10.,  11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.]],\n\n       [[100., 101., 102., 103., 104., 105., 106., 107., 108., 109.],\n        [110., 111., 112., 113., 114., 115., 116., 117., 118., 119.]],\n\n       [[200., 201., 202., 203., 204., 205., 206., 207., 208., 209.],\n        [210., 211., 212., 213., 214., 215., 216., 217., 218., 219.]]])\n\n\në„˜íŒŒì´ëŠ” ë¨¼ì € í¬ê¸°ê°€ (3, 2, 10)ì¸ ì„¸ ê°œì˜ ndarray(ì°¨ì›ë§ˆë‹¤ í•˜ë‚˜ì”©)ë¥¼ ë§Œë“­ë‹ˆë‹¤. ê° ë°°ì—´ì€ ì¶•ì„ ë”°ë¼ ì¢Œí‘œ ê°’ê³¼ ê°™ì€ ê°’ì„ ê°€ì§‘ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, z ì¶•ì— ìˆëŠ” ë°°ì—´ì˜ ëª¨ë“  ì›ì†ŒëŠ” z-ì¶•ì˜ ê°’ê³¼ ê°™ìŠµë‹ˆë‹¤:\n[[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n\n [[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]]\n\n [[ 2.  2.  2.  2.  2.  2.  2.  2.  2.  2.]\n  [ 2.  2.  2.  2.  2.  2.  2.  2.  2.  2.]]]\nìœ„ì˜ ì‹ x + 10 * y + 100 * zì—ì„œ x, y, zëŠ” ì‚¬ì‹¤ ndarrayì…ë‹ˆë‹¤(ë°°ì—´ì˜ ì‚°ìˆ  ì—°ì‚°ì— ëŒ€í•´ì„œëŠ” ì•„ë˜ì—ì„œ ì„¤ëª…í•©ë‹ˆë‹¤). ì¤‘ìš”í•œ ì ì€ í•¨ìˆ˜ my_functionì´ ì›ì†Œë§ˆë‹¤ í˜¸ì¶œë˜ëŠ” ê²ƒì´ ì•„ë‹ˆê³  ë”± í•œ ë²ˆ í˜¸ì¶œëœë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ê·¸ë˜ì„œ ë§¤ìš° íš¨ìœ¨ì ìœ¼ë¡œ ì´ˆê¸°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#dtype",
    "href": "Data_Mining/2022-03-04-numpy.html#dtype",
    "title": "Numpy ê¸°ë³¸",
    "section": "dtype",
    "text": "dtype\në„˜íŒŒì´ì˜ ndarrayëŠ” ëª¨ë“  ì›ì†Œê°€ ë™ì¼í•œ íƒ€ì…(ë³´í†µ ìˆ«ì)ì„ ê°€ì§€ê¸° ë•Œë¬¸ì— íš¨ìœ¨ì ì…ë‹ˆë‹¤. dtype ì†ì„±ìœ¼ë¡œ ì‰½ê²Œ ë°ì´í„° íƒ€ì…ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\nc = np.arange(1, 5)\nprint(c.dtype, c)\n\nint64 [1 2 3 4]\n\n\n\nc = np.arange(1.0, 5.0)\nprint(c.dtype, c)\n\nfloat64 [1. 2. 3. 4.]\n\n\në„˜íŒŒì´ê°€ ë°ì´í„° íƒ€ì…ì„ ê²°ì •í•˜ë„ë¡ ë‚´ë²„ë ¤ ë‘ëŠ” ëŒ€ì‹  dtype ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œ ë°°ì—´ì„ ë§Œë“¤ ë•Œ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\nd = np.arange(1, 5, dtype=np.complex64)\nprint(d.dtype, d)\n\ncomplex64 [1.+0.j 2.+0.j 3.+0.j 4.+0.j]\n\n\nê°€ëŠ¥í•œ ë°ì´í„° íƒ€ì…ì€ int8, int16, int32, int64, uint8|16|32|64, float16|32|64, complex64|128ê°€ ìˆìŠµë‹ˆë‹¤. ì „ì²´ ë¦¬ìŠ¤íŠ¸ëŠ” ì˜¨ë¼ì¸ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì„¸ìš”."
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#itemsize",
    "href": "Data_Mining/2022-03-04-numpy.html#itemsize",
    "title": "Numpy ê¸°ë³¸",
    "section": "itemsize",
    "text": "itemsize\nitemsize ì†ì„±ì€ ê° ì•„ì´í…œì˜ í¬ê¸°(ë°”ì´íŠ¸)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤:\n\ne = np.arange(1, 5, dtype=np.complex64)\ne.itemsize\n\n8"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#data-ë²„í¼",
    "href": "Data_Mining/2022-03-04-numpy.html#data-ë²„í¼",
    "title": "Numpy ê¸°ë³¸",
    "section": "data ë²„í¼",
    "text": "data ë²„í¼\në°°ì—´ì˜ ë°ì´í„°ëŠ” 1ì°¨ì› ë°”ì´íŠ¸ ë²„í¼ë¡œ ë©”ëª¨ë¦¬ì— ì €ì¥ë©ë‹ˆë‹¤. data ì†ì„±ì„ ì‚¬ìš©í•´ ì°¸ì¡°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤(ì‚¬ìš©í•  ì¼ì€ ê±°ì˜ ì—†ê² ì§€ë§Œìš”).\n\nf = np.array([[1,2],[1000, 2000]], dtype=np.int32)\nf.data\n\n<memory at 0x7f97929dd790>\n\n\níŒŒì´ì¬ 2ì—ì„œëŠ” f.dataê°€ ë²„í¼ì´ê³  íŒŒì´ì¬ 3ì—ì„œëŠ” memoryviewì…ë‹ˆë‹¤.\n\nif (hasattr(f.data, \"tobytes\")):\n    data_bytes = f.data.tobytes() # python 3\nelse:\n    data_bytes = memoryview(f.data).tobytes() # python 2\n\ndata_bytes\n\nb'\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\xe8\\x03\\x00\\x00\\xd0\\x07\\x00\\x00'\n\n\nì—¬ëŸ¬ ê°œì˜ ndarrayê°€ ë°ì´í„° ë²„í¼ë¥¼ ê³µìœ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ë‚˜ë¥¼ ìˆ˜ì •í•˜ë©´ ë‹¤ë¥¸ ê²ƒë„ ë°”ë€ë‹ˆë‹¤. ì ì‹œ í›„ì— ì˜ˆë¥¼ ì‚´í´ ë³´ê² ìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#ìì‹ ì„-ë³€ê²½",
    "href": "Data_Mining/2022-03-04-numpy.html#ìì‹ ì„-ë³€ê²½",
    "title": "Numpy ê¸°ë³¸",
    "section": "ìì‹ ì„ ë³€ê²½",
    "text": "ìì‹ ì„ ë³€ê²½\nndarrayì˜ shape ì†ì„±ì„ ì§€ì •í•˜ë©´ ê°„ë‹¨íˆ í¬ê¸°ë¥¼ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°°ì—´ì˜ ì›ì†Œ ê°œìˆ˜ëŠ” ë™ì¼í•˜ê²Œ ìœ ì§€ë©ë‹ˆë‹¤.\n\ng = np.arange(24)\nprint(g)\nprint(\"ë­í¬:\", g.ndim)\n\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\në­í¬: 1\n\n\n\ng.shape = (6, 4)\nprint(g)\nprint(\"ë­í¬:\", g.ndim)\n\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]\n [12 13 14 15]\n [16 17 18 19]\n [20 21 22 23]]\në­í¬: 2\n\n\n\ng.shape = (2, 3, 4)\nprint(g)\nprint(\"ë­í¬:\", g.ndim)\n\n[[[ 0  1  2  3]\n  [ 4  5  6  7]\n  [ 8  9 10 11]]\n\n [[12 13 14 15]\n  [16 17 18 19]\n  [20 21 22 23]]]\në­í¬: 3"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#reshape",
    "href": "Data_Mining/2022-03-04-numpy.html#reshape",
    "title": "Numpy ê¸°ë³¸",
    "section": "reshape",
    "text": "reshape\nreshape í•¨ìˆ˜ëŠ” ë™ì¼í•œ ë°ì´í„°ë¥¼ ê°€ë¦¬í‚¤ëŠ” ìƒˆë¡œìš´ ndarray ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. í•œ ë°°ì—´ì„ ìˆ˜ì •í•˜ë©´ ë‹¤ë¥¸ ê²ƒë„ í•¨ê»˜ ë°”ë€ë‹ˆë‹¤.\n\ng2 = g.reshape(4,6)\nprint(g2)\nprint(\"ë­í¬:\", g2.ndim)\n\n[[ 0  1  2  3  4  5]\n [ 6  7  8  9 10 11]\n [12 13 14 15 16 17]\n [18 19 20 21 22 23]]\në­í¬: 2\n\n\ní–‰ 1, ì—´ 2ì˜ ì›ì†Œë¥¼ 999ë¡œ ì„¤ì •í•©ë‹ˆë‹¤(ì¸ë±ì‹± ë°©ì‹ì€ ì•„ë˜ë¥¼ ì°¸ê³ í•˜ì„¸ìš”).\n\ng2[1, 2] = 999\ng2\n\narray([[  0,   1,   2,   3,   4,   5],\n       [  6,   7, 999,   9,  10,  11],\n       [ 12,  13,  14,  15,  16,  17],\n       [ 18,  19,  20,  21,  22,  23]])\n\n\nì´ì— ìƒì‘í•˜ëŠ” gì˜ ì›ì†Œë„ ìˆ˜ì •ë©ë‹ˆë‹¤.\n\ng\n\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [999,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#ravel",
    "href": "Data_Mining/2022-03-04-numpy.html#ravel",
    "title": "Numpy ê¸°ë³¸",
    "section": "ravel",
    "text": "ravel\në§ˆì§€ë§‰ìœ¼ë¡œ ravel í•¨ìˆ˜ëŠ” ë™ì¼í•œ ë°ì´í„°ë¥¼ ê°€ë¦¬í‚¤ëŠ” ìƒˆë¡œìš´ 1ì°¨ì› ndarrayë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤:\n\ng.ravel()\n\narray([  0,   1,   2,   3,   4,   5,   6,   7, 999,   9,  10,  11,  12,\n        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#ê·œì¹™-1",
    "href": "Data_Mining/2022-03-04-numpy.html#ê·œì¹™-1",
    "title": "Numpy ê¸°ë³¸",
    "section": "ê·œì¹™ 1",
    "text": "ê·œì¹™ 1\në°°ì—´ì˜ ë­í¬ê°€ ë™ì¼í•˜ì§€ ì•Šìœ¼ë©´ ë­í¬ê°€ ë§ì„ ë•Œê¹Œì§€ ë­í¬ê°€ ì‘ì€ ë°°ì—´ ì•ì— 1ì„ ì¶”ê°€í•©ë‹ˆë‹¤.\n\nh = np.arange(5).reshape(1, 1, 5)\nh\n\narray([[[0, 1, 2, 3, 4]]])\n\n\nì—¬ê¸°ì— (1,1,5) í¬ê¸°ì˜ 3D ë°°ì—´ì— (5,) í¬ê¸°ì˜ 1D ë°°ì—´ì„ ë”í•´ ë³´ì£ . ë¸Œë¡œë“œìºìŠ¤íŒ…ì˜ ê·œì¹™ 1ì´ ì ìš©ë©ë‹ˆë‹¤!\n\nh + [10, 20, 30, 40, 50]  # ë‹¤ìŒê³¼ ë™ì¼í•©ë‹ˆë‹¤: h + [[[10, 20, 30, 40, 50]]]\n\narray([[[10, 21, 32, 43, 54]]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#ê·œì¹™-2",
    "href": "Data_Mining/2022-03-04-numpy.html#ê·œì¹™-2",
    "title": "Numpy ê¸°ë³¸",
    "section": "ê·œì¹™ 2",
    "text": "ê·œì¹™ 2\níŠ¹ì • ì°¨ì›ì´ 1ì¸ ë°°ì—´ì€ ê·¸ ì°¨ì›ì—ì„œ í¬ê¸°ê°€ ê°€ì¥ í° ë°°ì—´ì˜ í¬ê¸°ì— ë§ì¶° ë™ì‘í•©ë‹ˆë‹¤. ë°°ì—´ì˜ ì›ì†Œê°€ ì°¨ì›ì„ ë”°ë¼ ë°˜ë³µë©ë‹ˆë‹¤.\n\nk = np.arange(6).reshape(2, 3)\nk\n\narray([[0, 1, 2],\n       [3, 4, 5]])\n\n\n(2,3) í¬ê¸°ì˜ 2D ndarrayì— (2,1) í¬ê¸°ì˜ 2D ë°°ì—´ì„ ë”í•´ ë³´ì£ . ë„˜íŒŒì´ëŠ” ë¸Œë¡œë“œìºìŠ¤íŒ… ê·œì¹™ 2ë¥¼ ì ìš©í•©ë‹ˆë‹¤:\n\nk + [[100], [200]]  # ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: k + [[100, 100, 100], [200, 200, 200]]\n\narray([[100, 101, 102],\n       [203, 204, 205]])\n\n\nê·œì¹™ 1ê³¼ 2ë¥¼ í•©ì¹˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ë™ì‘í•©ë‹ˆë‹¤:\n\nk + [100, 200, 300]  # ê·œì¹™ 1 ì ìš©: [[100, 200, 300]], ê·œì¹™ 2 ì ìš©: [[100, 200, 300], [100, 200, 300]]\n\narray([[100, 201, 302],\n       [103, 204, 305]])\n\n\në˜ ë§¤ìš° ê°„ë‹¨íˆ ë‹¤ìŒ ì²˜ëŸ¼ í•´ë„ ë©ë‹ˆë‹¤:\n\nk + 1000  # ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: k + [[1000, 1000, 1000], [1000, 1000, 1000]]\n\narray([[1000, 1001, 1002],\n       [1003, 1004, 1005]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#ê·œì¹™-3",
    "href": "Data_Mining/2022-03-04-numpy.html#ê·œì¹™-3",
    "title": "Numpy ê¸°ë³¸",
    "section": "ê·œì¹™ 3",
    "text": "ê·œì¹™ 3\nê·œì¹™ 1 & 2ì„ ì ìš©í–ˆì„ ë•Œ ëª¨ë“  ë°°ì—´ì˜ í¬ê¸°ê°€ ë§ì•„ì•¼ í•©ë‹ˆë‹¤.\n\ntry:\n    k + [33, 44]\nexcept ValueError as e:\n    print(e)\n\noperands could not be broadcast together with shapes (2,3) (2,) \n\n\në¸Œë¡œë“œìºìŠ¤íŒ… ê·œì¹™ì€ ì‚°ìˆ  ì—°ì‚° ë¿ë§Œ ì•„ë‹ˆë¼ ë„˜íŒŒì´ ì—°ì‚°ì—ì„œ ë§ì´ ì‚¬ìš©ë©ë‹ˆë‹¤. ì•„ë˜ì—ì„œ ë” ë³´ë„ë¡ í•˜ì£ . ë¸Œë¡œë“œìºìŠ¤íŒ…ì— ê´€í•œ ë” ìì„¸í•œ ì •ë³´ëŠ” ì˜¨ë¼ì¸ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì„¸ìš”."
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#ì—…ìºìŠ¤íŒ…",
    "href": "Data_Mining/2022-03-04-numpy.html#ì—…ìºìŠ¤íŒ…",
    "title": "Numpy ê¸°ë³¸",
    "section": "ì—…ìºìŠ¤íŒ…",
    "text": "ì—…ìºìŠ¤íŒ…\ndtypeì´ ë‹¤ë¥¸ ë°°ì—´ì„ í•©ì¹  ë•Œ ë„˜íŒŒì´ëŠ” (ì‹¤ì œ ê°’ì— ìƒê´€ì—†ì´) ëª¨ë“  ê°’ì„ ë‹¤ë£° ìˆ˜ ìˆëŠ” íƒ€ì…ìœ¼ë¡œ ì—…ìºìŠ¤íŒ…í•©ë‹ˆë‹¤.\n\nk1 = np.arange(0, 5, dtype=np.uint8)\nprint(k1.dtype, k1)\n\nuint8 [0 1 2 3 4]\n\n\n\nk2 = k1 + np.array([5, 6, 7, 8, 9], dtype=np.int8)\nprint(k2.dtype, k2)\n\nint16 [ 5  7  9 11 13]\n\n\nëª¨ë“  int8ê³¼ uint8 ê°’(-128ì—ì„œ 255ê¹Œì§€)ì„ í‘œí˜„í•˜ê¸° ìœ„í•´ int16ì´ í•„ìš”í•©ë‹ˆë‹¤. ì´ ì½”ë“œì—ì„œëŠ” uint8ì´ë©´ ì¶©ë¶„í•˜ì§€ë§Œ ì—…ìºìŠ¤íŒ…ë˜ì—ˆìŠµë‹ˆë‹¤.\n\nk3 = k1 + 1.5\nprint(k3.dtype, k3)\n\nfloat64 [1.5 2.5 3.5 4.5 5.5]"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#ndarray-ë©”ì„œë“œ",
    "href": "Data_Mining/2022-03-04-numpy.html#ndarray-ë©”ì„œë“œ",
    "title": "Numpy ê¸°ë³¸",
    "section": "ndarray ë©”ì„œë“œ",
    "text": "ndarray ë©”ì„œë“œ\nì¼ë¶€ í•¨ìˆ˜ëŠ” ndarray ë©”ì„œë“œë¡œ ì œê³µë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´:\n\na = np.array([[-2.5, 3.1, 7], [10, 11, 12]])\nprint(a)\nprint(\"í‰ê·  =\", a.mean())\n\n[[-2.5  3.1  7. ]\n [10.  11.  12. ]]\ní‰ê·  = 6.766666666666667\n\n\nì´ ëª…ë ¹ì€ í¬ê¸°ì— ìƒê´€ì—†ì´ ndarrayì— ìˆëŠ” ëª¨ë“  ì›ì†Œì˜ í‰ê· ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\në‹¤ìŒì€ ìœ ìš©í•œ ndarray ë©”ì„œë“œì…ë‹ˆë‹¤:\n\nfor func in (a.min, a.max, a.sum, a.prod, a.std, a.var):\n    print(func.__name__, \"=\", func())\n\nmin = -2.5\nmax = 12.0\nsum = 40.6\nprod = -71610.0\nstd = 5.084835843520964\nvar = 25.855555555555554\n\n\nì´ í•¨ìˆ˜ë“¤ì€ ì„ íƒì ìœ¼ë¡œ ë§¤ê°œë³€ìˆ˜ axisë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì§€ì •ëœ ì¶•ì„ ë”°ë¼ ì›ì†Œì— ì—°ì‚°ì„ ì ìš©í•˜ëŠ”ë° ì‚¬ìš©í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´:\n\nc=np.arange(24).reshape(2,3,4)\nc\n\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])\n\n\n\nc.sum(axis=0)  # ì²« ë²ˆì§¸ ì¶•ì„ ë”°ë¼ ë”í•¨, ê²°ê³¼ëŠ” 3x4 ë°°ì—´\n\narray([[12, 14, 16, 18],\n       [20, 22, 24, 26],\n       [28, 30, 32, 34]])\n\n\n\nc.sum(axis=1)  # ë‘ ë²ˆì§¸ ì¶•ì„ ë”°ë¼ ë”í•¨, ê²°ê³¼ëŠ” 2x4 ë°°ì—´\n\narray([[12, 15, 18, 21],\n       [48, 51, 54, 57]])\n\n\nì—¬ëŸ¬ ì¶•ì— ëŒ€í•´ì„œ ë”í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:\n\nc.sum(axis=(0,2))  # ì²« ë²ˆì§¸ ì¶•ê³¼ ì„¸ ë²ˆì§¸ ì¶•ì„ ë”°ë¼ ë”í•¨, ê²°ê³¼ëŠ” (3,) ë°°ì—´\n\narray([ 60,  92, 124])\n\n\n\n0+1+2+3 + 12+13+14+15, 4+5+6+7 + 16+17+18+19, 8+9+10+11 + 20+21+22+23\n\n(60, 92, 124)"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#ì¼ë°˜-í•¨ìˆ˜",
    "href": "Data_Mining/2022-03-04-numpy.html#ì¼ë°˜-í•¨ìˆ˜",
    "title": "Numpy ê¸°ë³¸",
    "section": "ì¼ë°˜ í•¨ìˆ˜",
    "text": "ì¼ë°˜ í•¨ìˆ˜\në„˜íŒŒì´ëŠ” ì¼ë°˜ í•¨ìˆ˜(universal function) ë˜ëŠ” ufuncë¼ê³  ë¶€ë¥´ëŠ” ì›ì†Œë³„ í•¨ìˆ˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´ square í•¨ìˆ˜ëŠ” ì›ë³¸ ndarrayë¥¼ ë³µì‚¬í•˜ì—¬ ê° ì›ì†Œë¥¼ ì œê³±í•œ ìƒˆë¡œìš´ ndarray ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤:\n\na = np.array([[-2.5, 3.1, 7], [10, 11, 12]])\nnp.square(a)\n\narray([[  6.25,   9.61,  49.  ],\n       [100.  , 121.  , 144.  ]])\n\n\në‹¤ìŒì€ ìœ ìš©í•œ ë‹¨í•­ ì¼ë°˜ í•¨ìˆ˜ë“¤ì…ë‹ˆë‹¤:\n\nprint(\"ì›ë³¸ ndarray\")\nprint(a)\nfor func in (np.abs, np.sqrt, np.exp, np.log, np.sign, np.ceil, np.modf, np.isnan, np.cos):\n    print(\"\\n\", func.__name__)\n    print(func(a))\n\nì›ë³¸ ndarray\n[[-2.5  3.1  7. ]\n [10.  11.  12. ]]\n\n absolute\n[[ 2.5  3.1  7. ]\n [10.  11.  12. ]]\n\n sqrt\n[[       nan 1.76068169 2.64575131]\n [3.16227766 3.31662479 3.46410162]]\n\n exp\n[[8.20849986e-02 2.21979513e+01 1.09663316e+03]\n [2.20264658e+04 5.98741417e+04 1.62754791e+05]]\n\n log\n[[       nan 1.13140211 1.94591015]\n [2.30258509 2.39789527 2.48490665]]\n\n sign\n[[-1.  1.  1.]\n [ 1.  1.  1.]]\n\n ceil\n[[-2.  4.  7.]\n [10. 11. 12.]]\n\n modf\n(array([[-0.5,  0.1,  0. ],\n       [ 0. ,  0. ,  0. ]]), array([[-2.,  3.,  7.],\n       [10., 11., 12.]]))\n\n isnan\n[[False False False]\n [False False False]]\n\n cos\n[[-0.80114362 -0.99913515  0.75390225]\n [-0.83907153  0.0044257   0.84385396]]\n\n\nRuntimeWarning: invalid value encountered in sqrt\n  print(func(a))\n<ipython-input-59-d791c8e37e6f>:5: RuntimeWarning: invalid value encountered in log\n  print(func(a))"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#ì´í•­-ì¼ë°˜-í•¨ìˆ˜",
    "href": "Data_Mining/2022-03-04-numpy.html#ì´í•­-ì¼ë°˜-í•¨ìˆ˜",
    "title": "Numpy ê¸°ë³¸",
    "section": "ì´í•­ ì¼ë°˜ í•¨ìˆ˜",
    "text": "ì´í•­ ì¼ë°˜ í•¨ìˆ˜\në‘ ê°œì˜ ndarrayì— ì›ì†Œë³„ë¡œ ì ìš©ë˜ëŠ” ì´í•­ í•¨ìˆ˜ë„ ë§ìŠµë‹ˆë‹¤. ë‘ ë°°ì—´ì´ ë™ì¼í•œ í¬ê¸°ê°€ ì•„ë‹ˆë©´ ë¸Œë¡œë“œìºìŠ¤íŒ… ê·œì¹™ì´ ì ìš©ë©ë‹ˆë‹¤:\n\na = np.array([1, -2, 3, 4])\nb = np.array([2, 8, -1, 7])\nnp.add(a, b)  # a + b ì™€ ë™ì¼\n\narray([ 3,  6,  2, 11])\n\n\n\nnp.greater(a, b)  # a > b ì™€ ë™ì¼\n\narray([False, False,  True, False])\n\n\n\nnp.maximum(a, b)\n\narray([2, 8, 3, 7])\n\n\n\nnp.copysign(a, b)\n\narray([ 1.,  2., -3.,  4.])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#ì°¨ì›-ë°°ì—´",
    "href": "Data_Mining/2022-03-04-numpy.html#ì°¨ì›-ë°°ì—´",
    "title": "Numpy ê¸°ë³¸",
    "section": "1ì°¨ì› ë°°ì—´",
    "text": "1ì°¨ì› ë°°ì—´\n1ì°¨ì› ë„˜íŒŒì´ ë°°ì—´ì€ ë³´í†µì˜ íŒŒì´ì¬ ë°°ì—´ê³¼ ë¹„ìŠ·í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\na = np.array([1, 5, 3, 19, 13, 7, 3])\na[3]\n\n19\n\n\n\na[2:5]\n\narray([ 3, 19, 13])\n\n\n\na[2:-1]\n\narray([ 3, 19, 13,  7])\n\n\n\na[:2]\n\narray([1, 5])\n\n\n\na[2::2]\n\narray([ 3, 13,  3])\n\n\n\na[::-1]\n\narray([ 3,  7, 13, 19,  3,  5,  1])\n\n\në¬¼ë¡  ì›ì†Œë¥¼ ìˆ˜ì •í•  ìˆ˜ ìˆì£ :\n\na[3]=999\na\n\narray([  1,   5,   3, 999,  13,   7,   3])\n\n\nìŠ¬ë¼ì´ì‹±ì„ ì‚¬ìš©í•´ ndarrayë¥¼ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\na[2:5] = [997, 998, 999]\na\n\narray([  1,   5, 997, 998, 999,   7,   3])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#ë³´í†µì˜-íŒŒì´ì¬-ë°°ì—´ê³¼-ì°¨ì´ì ",
    "href": "Data_Mining/2022-03-04-numpy.html#ë³´í†µì˜-íŒŒì´ì¬-ë°°ì—´ê³¼-ì°¨ì´ì ",
    "title": "Numpy ê¸°ë³¸",
    "section": "ë³´í†µì˜ íŒŒì´ì¬ ë°°ì—´ê³¼ ì°¨ì´ì ",
    "text": "ë³´í†µì˜ íŒŒì´ì¬ ë°°ì—´ê³¼ ì°¨ì´ì \në³´í†µì˜ íŒŒì´ì¬ ë°°ì—´ê³¼ ëŒ€ì¡°ì ìœ¼ë¡œ ndarray ìŠ¬ë¼ì´ì‹±ì— í•˜ë‚˜ì˜ ê°’ì„ í• ë‹¹í•˜ë©´ ìŠ¬ë¼ì´ì‹± ì „ì²´ì— ë³µì‚¬ë©ë‹ˆë‹¤. ìœ„ì—ì„œ ì–¸ê¸‰í•œ ë¸Œë¡œë“œìºìŠ¤íŒ… ë•íƒì…ë‹ˆë‹¤.\n\na[2:5] = -1\na\n\narray([ 1,  5, -1, -1, -1,  7,  3])\n\n\në˜í•œ ì´ëŸ° ì‹ìœ¼ë¡œ ndarray í¬ê¸°ë¥¼ ëŠ˜ë¦¬ê±°ë‚˜ ì¤„ì¼ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:\n\ntry:\n    a[2:5] = [1,2,3,4,5,6]  # ë„ˆë¬´ ê¸¸ì–´ìš”\nexcept ValueError as e:\n    print(e)\n\ncannot copy sequence with size 6 to array axis with dimension 3\n\n\nì›ì†Œë¥¼ ì‚­ì œí•  ìˆ˜ë„ ì—†ìŠµë‹ˆë‹¤:\n\ntry:\n    del a[2:5]\nexcept ValueError as e:\n    print(e)\n\ncannot delete array elements\n\n\nì¤‘ìš”í•œ ì ì€ ndarrayì˜ ìŠ¬ë¼ì´ì‹±ì€ ê°™ì€ ë°ì´í„° ë²„í¼ë¥¼ ë°”ë¼ë³´ëŠ” ë·°(view)ì…ë‹ˆë‹¤. ìŠ¬ë¼ì´ì‹±ëœ ê°ì²´ë¥¼ ìˆ˜ì •í•˜ë©´ ì‹¤ì œ ì›ë³¸ ndarrayê°€ ìˆ˜ì •ë©ë‹ˆë‹¤!\n\na_slice = a[2:6]\na_slice[1] = 1000\na  # ì›ë³¸ ë°°ì—´ì´ ìˆ˜ì •ë©ë‹ˆë‹¤!\n\narray([   1,    5,   -1, 1000,   -1,    7,    3])\n\n\n\na[3] = 2000\na_slice  # ë¹„ìŠ·í•˜ê²Œ ì›ë³¸ ë°°ì—´ì„ ìˆ˜ì •í•˜ë©´ ìŠ¬ë¼ì´ì‹± ê°ì²´ì—ë„ ë°˜ì˜ë©ë‹ˆë‹¤!\n\narray([  -1, 2000,   -1,    7])\n\n\në°ì´í„°ë¥¼ ë³µì‚¬í•˜ë ¤ë©´ copy ë©”ì„œë“œë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤:\n\nanother_slice = a[2:6].copy()\nanother_slice[1] = 3000\na  # ì›ë³¸ ë°°ì—´ì´ ìˆ˜ì •ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤\n\narray([   1,    5,   -1, 2000,   -1,    7,    3])\n\n\n\na[3] = 4000\nanother_slice  # ë§ˆì°¬ê°€ì§€ë¡œ ì›ë³¸ ë°°ì—´ì„ ìˆ˜ì •í•´ë„ ë³µì‚¬ëœ ë°°ì—´ì€ ë°”ë€Œì§€ ì•ŠìŠµë‹ˆë‹¤\n\narray([  -1, 3000,   -1,    7])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#ë‹¤ì°¨ì›-ë°°ì—´",
    "href": "Data_Mining/2022-03-04-numpy.html#ë‹¤ì°¨ì›-ë°°ì—´",
    "title": "Numpy ê¸°ë³¸",
    "section": "ë‹¤ì°¨ì› ë°°ì—´",
    "text": "ë‹¤ì°¨ì› ë°°ì—´\në‹¤ì°¨ì› ë°°ì—´ì€ ë¹„ìŠ·í•œ ë°©ì‹ìœ¼ë¡œ ê° ì¶•ì„ ë”°ë¼ ì¸ë±ì‹± ë˜ëŠ” ìŠ¬ë¼ì´ì‹±í•´ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤. ì½¤ë§ˆë¡œ êµ¬ë¶„í•©ë‹ˆë‹¤:\n\nb = np.arange(48).reshape(4, 12)\nb\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\n\n\n\nb[1, 2]  # í–‰ 1, ì—´ 2\n\n14\n\n\n\nb[1, :]  # í–‰ 1, ëª¨ë“  ì—´\n\narray([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\n\n\n\nb[:, 1]  # ëª¨ë“  í–‰, ì—´ 1\n\narray([ 1, 13, 25, 37])\n\n\nì£¼ì˜: ë‹¤ìŒ ë‘ í‘œí˜„ì—ëŠ” ë¯¸ë¬˜í•œ ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤:\n\nb[1, :]\n\narray([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\n\n\n\nb[1:2, :]\n\narray([[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]])\n\n\nì²« ë²ˆì§¸ í‘œí˜„ì‹ì€ (12,) í¬ê¸°ì¸ 1D ë°°ì—´ë¡œ í–‰ì´ í•˜ë‚˜ì…ë‹ˆë‹¤. ë‘ ë²ˆì§¸ëŠ” (1, 12) í¬ê¸°ì¸ 2D ë°°ì—´ë¡œ ê°™ì€ í–‰ì„ ë°˜í™˜í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#íŒ¬ì‹œ-ì¸ë±ì‹±fancy-indexing",
    "href": "Data_Mining/2022-03-04-numpy.html#íŒ¬ì‹œ-ì¸ë±ì‹±fancy-indexing",
    "title": "Numpy ê¸°ë³¸",
    "section": "íŒ¬ì‹œ ì¸ë±ì‹±(Fancy indexing)",
    "text": "íŒ¬ì‹œ ì¸ë±ì‹±(Fancy indexing)\nê´€ì‹¬ ëŒ€ìƒì˜ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ì§€ì •í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ íŒ¬ì‹œ ì¸ë±ì‹±ì´ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤.\n\nb[(0,2), 2:5]  # í–‰ 0ê³¼ 2, ì—´ 2ì—ì„œ 4(5-1)ê¹Œì§€\n\narray([[ 2,  3,  4],\n       [26, 27, 28]])\n\n\n\nb[:, (-1, 2, -1)]  # ëª¨ë“  í–‰, ì—´ -1 (ë§ˆì§€ë§‰), 2ì™€ -1 (ë‹¤ì‹œ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ)\n\narray([[11,  2, 11],\n       [23, 14, 23],\n       [35, 26, 35],\n       [47, 38, 47]])\n\n\nì—¬ëŸ¬ ê°œì˜ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ì§€ì •í•˜ë©´ ì¸ë±ìŠ¤ì— ë§ëŠ” ê°’ì´ í¬í•¨ëœ 1D ndarrayë¥¼ ë°˜í™˜ë©ë‹ˆë‹¤.\n\nb[(-1, 2, -1, 2), (5, 9, 1, 9)]  # returns a 1D array with b[-1, 5], b[2, 9], b[-1, 1] and b[2, 9] (again)\n\narray([41, 33, 37, 33])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#ê³ ì°¨ì›",
    "href": "Data_Mining/2022-03-04-numpy.html#ê³ ì°¨ì›",
    "title": "Numpy ê¸°ë³¸",
    "section": "ê³ ì°¨ì›",
    "text": "ê³ ì°¨ì›\nê³ ì°¨ì›ì—ì„œë„ ë™ì¼í•œ ë°©ì‹ì´ ì ìš©ë©ë‹ˆë‹¤. ëª‡ ê°€ì§€ ì˜ˆë¥¼ ì‚´í´ ë³´ê² ìŠµë‹ˆë‹¤:\n\nc = b.reshape(4,2,6)\nc\n\narray([[[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11]],\n\n       [[12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23]],\n\n       [[24, 25, 26, 27, 28, 29],\n        [30, 31, 32, 33, 34, 35]],\n\n       [[36, 37, 38, 39, 40, 41],\n        [42, 43, 44, 45, 46, 47]]])\n\n\n\nc[2, 1, 4]  # í–‰ë ¬ 2, í–‰ 1, ì—´ 4\n\n34\n\n\n\nc[2, :, 3]  # í–‰ë ¬ 2, ëª¨ë“  í–‰, ì—´ 3\n\narray([27, 33])\n\n\nì–´ë–¤ ì¶•ì— ëŒ€í•œ ì¸ë±ìŠ¤ë¥¼ ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ì´ ì¶•ì˜ ëª¨ë“  ì›ì†Œê°€ ë°˜í™˜ë©ë‹ˆë‹¤:\n\nc[2, 1]  # í–‰ë ¬ 2, í–‰ 1, ëª¨ë“  ì—´ì´ ë°˜í™˜ë©ë‹ˆë‹¤. c[2, 1, :]ì™€ ë™ì¼í•©ë‹ˆë‹¤.\n\narray([30, 31, 32, 33, 34, 35])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#ìƒëµ-ë¶€í˜¸-...",
    "href": "Data_Mining/2022-03-04-numpy.html#ìƒëµ-ë¶€í˜¸-...",
    "title": "Numpy ê¸°ë³¸",
    "section": "ìƒëµ ë¶€í˜¸ (...)",
    "text": "ìƒëµ ë¶€í˜¸ (...)\nìƒëµ ë¶€í˜¸(...)ë¥¼ ì“°ë©´ ëª¨ë“  ì§€ì •í•˜ì§€ ì•Šì€ ì¶•ì˜ ì›ì†Œë¥¼ í¬í•¨í•©ë‹ˆë‹¤.\n\nc[2, ...]  #  í–‰ë ¬ 2, ëª¨ë“  í–‰, ëª¨ë“  ì—´. c[2, :, :]ì™€ ë™ì¼\n\narray([[24, 25, 26, 27, 28, 29],\n       [30, 31, 32, 33, 34, 35]])\n\n\n\nc[2, 1, ...]  # í–‰ë ¬ 2, í–‰ 1, ëª¨ë“  ì—´. c[2, 1, :]ì™€ ë™ì¼\n\narray([30, 31, 32, 33, 34, 35])\n\n\n\nc[2, ..., 3]  # í–‰ë ¬ 2, ëª¨ë“  í–‰, ì—´ 3. c[2, :, 3]ì™€ ë™ì¼\n\narray([27, 33])\n\n\n\nc[..., 3]  # ëª¨ë“  í–‰ë ¬, ëª¨ë“  í–‰, ì—´ 3. c[:, :, 3]ì™€ ë™ì¼\n\narray([[ 3,  9],\n       [15, 21],\n       [27, 33],\n       [39, 45]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#ë¶ˆë¦¬ì–¸-ì¸ë±ì‹±",
    "href": "Data_Mining/2022-03-04-numpy.html#ë¶ˆë¦¬ì–¸-ì¸ë±ì‹±",
    "title": "Numpy ê¸°ë³¸",
    "section": "ë¶ˆë¦¬ì–¸ ì¸ë±ì‹±",
    "text": "ë¶ˆë¦¬ì–¸ ì¸ë±ì‹±\në¶ˆë¦¬ì–¸ ê°’ì„ ê°€ì§„ ndarrayë¥¼ ì‚¬ìš©í•´ ì¶•ì˜ ì¸ë±ìŠ¤ë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nb = np.arange(48).reshape(4, 12)\nb\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\n\n\n\nrows_on = np.array([True, False, True, False])\nb[rows_on, :]  # í–‰ 0ê³¼ 2, ëª¨ë“  ì—´. b[(0, 2), :]ì™€ ë™ì¼\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]])\n\n\n\ncols_on = np.array([False, True, False] * 4)\nb[:, cols_on]  # ëª¨ë“  í–‰, ì—´ 1, 4, 7, 10\n\narray([[ 1,  4,  7, 10],\n       [13, 16, 19, 22],\n       [25, 28, 31, 34],\n       [37, 40, 43, 46]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#np.ix_",
    "href": "Data_Mining/2022-03-04-numpy.html#np.ix_",
    "title": "Numpy ê¸°ë³¸",
    "section": "np.ix_",
    "text": "np.ix_\nì—¬ëŸ¬ ì¶•ì— ê±¸ì³ì„œëŠ” ë¶ˆë¦¬ì–¸ ì¸ë±ì‹±ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ê³  ix_ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:\n\nb[np.ix_(rows_on, cols_on)]\n\narray([[ 1,  4,  7, 10],\n       [25, 28, 31, 34]])\n\n\n\nnp.ix_(rows_on, cols_on)\n\n(array([[0],\n        [2]]),\n array([[ 1,  4,  7, 10]]))\n\n\nndarrayì™€ ê°™ì€ í¬ê¸°ì˜ ë¶ˆë¦¬ì–¸ ë°°ì—´ì„ ì‚¬ìš©í•˜ë©´ í•´ë‹¹ ìœ„ì¹˜ê°€ Trueì¸ ëª¨ë“  ì›ì†Œë¥¼ ë‹´ì€ 1D ë°°ì—´ì´ ë°˜í™˜ë©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì¡°ê±´ ì—°ì‚°ìì™€ í•¨ê»˜ ì‚¬ìš©í•©ë‹ˆë‹¤:\n\nb[b % 3 == 1]\n\narray([ 1,  4,  7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#vstack",
    "href": "Data_Mining/2022-03-04-numpy.html#vstack",
    "title": "Numpy ê¸°ë³¸",
    "section": "vstack",
    "text": "vstack\nvstack í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜ì§ìœ¼ë¡œ ìŒ“ì•„ë³´ì£ :\n\nq4 = np.vstack((q1, q2, q3))\nq4\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.]])\n\n\n\nq4.shape\n\n(10, 4)\n\n\nq1, q2, q3ê°€ ëª¨ë‘ ê°™ì€ í¬ê¸°ì´ë¯€ë¡œ ê°€ëŠ¥í•©ë‹ˆë‹¤(ìˆ˜ì§ìœ¼ë¡œ ìŒ“ê¸° ë•Œë¬¸ì— ìˆ˜ì§ ì¶•ì€ í¬ê¸°ê°€ ë‹¬ë¼ë„ ë©ë‹ˆë‹¤)."
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#hstack",
    "href": "Data_Mining/2022-03-04-numpy.html#hstack",
    "title": "Numpy ê¸°ë³¸",
    "section": "hstack",
    "text": "hstack\nhstackì„ ì‚¬ìš©í•´ ìˆ˜í‰ìœ¼ë¡œë„ ìŒ“ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\nq5 = np.hstack((q1, q3))\nq5\n\narray([[1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.]])\n\n\n\nq5.shape\n\n(3, 8)\n\n\nq1ê³¼ q3ê°€ ëª¨ë‘ 3ê°œì˜ í–‰ì„ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì— ê°€ëŠ¥í•©ë‹ˆë‹¤. q2ëŠ” 4ê°œì˜ í–‰ì„ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì— q1, q3ì™€ ìˆ˜í‰ìœ¼ë¡œ ìŒ“ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:\n\ntry:\n    q5 = np.hstack((q1, q2, q3))\nexcept ValueError as e:\n    print(e)\n\nall the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 3 and the array at index 1 has size 4"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#concatenate",
    "href": "Data_Mining/2022-03-04-numpy.html#concatenate",
    "title": "Numpy ê¸°ë³¸",
    "section": "concatenate",
    "text": "concatenate\nconcatenate í•¨ìˆ˜ëŠ” ì§€ì •í•œ ì¶•ìœ¼ë¡œë„ ë°°ì—´ì„ ìŒ“ìŠµë‹ˆë‹¤.\n\nq7 = np.concatenate((q1, q2, q3), axis=0)  # vstackê³¼ ë™ì¼\nq7\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.]])\n\n\n\nq7.shape\n\n(10, 4)\n\n\nì˜ˆìƒí–ˆê² ì§€ë§Œ hstackì€ axis=1ìœ¼ë¡œ concatenateë¥¼ í˜¸ì¶œí•˜ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#stack",
    "href": "Data_Mining/2022-03-04-numpy.html#stack",
    "title": "Numpy ê¸°ë³¸",
    "section": "stack",
    "text": "stack\nstack í•¨ìˆ˜ëŠ” ìƒˆë¡œìš´ ì¶•ì„ ë”°ë¼ ë°°ì—´ì„ ìŒ“ìŠµë‹ˆë‹¤. ëª¨ë“  ë°°ì—´ì€ ê°™ì€ í¬ê¸°ë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤.\n\nq8 = np.stack((q1, q3))\nq8\n\narray([[[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]],\n\n       [[3., 3., 3., 3.],\n        [3., 3., 3., 3.],\n        [3., 3., 3., 3.]]])\n\n\n\nq8.shape\n\n(2, 3, 4)"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#í–‰ë ¬-ì „ì¹˜",
    "href": "Data_Mining/2022-03-04-numpy.html#í–‰ë ¬-ì „ì¹˜",
    "title": "Numpy ê¸°ë³¸",
    "section": "í–‰ë ¬ ì „ì¹˜",
    "text": "í–‰ë ¬ ì „ì¹˜\nT ì†ì„±ì€ ë­í¬ê°€ 2ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ì„ ë•Œ transpose()ë¥¼ í˜¸ì¶œí•˜ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\nm1 = np.arange(10).reshape(2,5)\nm1\n\narray([[0, 1, 2, 3, 4],\n       [5, 6, 7, 8, 9]])\n\n\n\nm1.T\n\narray([[0, 5],\n       [1, 6],\n       [2, 7],\n       [3, 8],\n       [4, 9]])\n\n\nT ì†ì„±ì€ ë­í¬ê°€ 0ì´ê±°ë‚˜ 1ì¸ ë°°ì—´ì—ëŠ” ì•„ë¬´ëŸ° ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠìŠµë‹ˆë‹¤:\n\nm2 = np.arange(5)\nm2\n\narray([0, 1, 2, 3, 4])\n\n\n\nm2.T\n\narray([0, 1, 2, 3, 4])\n\n\në¨¼ì € 1D ë°°ì—´ì„ í•˜ë‚˜ì˜ í–‰ì´ ìˆëŠ” í–‰ë ¬(2D)ë¡œ ë°”ê¾¼ë‹¤ìŒ ì „ì¹˜ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\nm2r = m2.reshape(1,5)\nm2r\n\narray([[0, 1, 2, 3, 4]])\n\n\n\nm2r.T\n\narray([[0],\n       [1],\n       [2],\n       [3],\n       [4]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#í–‰ë ¬-ê³±ì…ˆ",
    "href": "Data_Mining/2022-03-04-numpy.html#í–‰ë ¬-ê³±ì…ˆ",
    "title": "Numpy ê¸°ë³¸",
    "section": "í–‰ë ¬ ê³±ì…ˆ",
    "text": "í–‰ë ¬ ê³±ì…ˆ\në‘ ê°œì˜ í–‰ë ¬ì„ ë§Œë“¤ì–´ dot ë©”ì„œë“œë¡œ í–‰ë ¬ ê³±ì…ˆì„ ì‹¤í–‰í•´ ë³´ì£ .\n\nn1 = np.arange(10).reshape(2, 5)\nn1\n\narray([[0, 1, 2, 3, 4],\n       [5, 6, 7, 8, 9]])\n\n\n\nn2 = np.arange(15).reshape(5,3)\nn2\n\narray([[ 0,  1,  2],\n       [ 3,  4,  5],\n       [ 6,  7,  8],\n       [ 9, 10, 11],\n       [12, 13, 14]])\n\n\n\nn1.dot(n2)\n\narray([[ 90, 100, 110],\n       [240, 275, 310]])\n\n\nì£¼ì˜: ì•ì„œ ì–¸ê¸‰í•œ ê²ƒì²˜ëŸ¼ n1*n2ëŠ” í–‰ë ¬ ê³±ì…ˆì´ ì•„ë‹ˆë¼ ì›ì†Œë³„ ê³±ì…ˆ(ë˜ëŠ” ì•„ë‹¤ë§ˆë¥´ ê³±ì´ë¼ ë¶€ë¦…ë‹ˆë‹¤)ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#ì—­í–‰ë ¬ê³¼-ìœ ì‚¬-ì—­í–‰ë ¬",
    "href": "Data_Mining/2022-03-04-numpy.html#ì—­í–‰ë ¬ê³¼-ìœ ì‚¬-ì—­í–‰ë ¬",
    "title": "Numpy ê¸°ë³¸",
    "section": "ì—­í–‰ë ¬ê³¼ ìœ ì‚¬ ì—­í–‰ë ¬",
    "text": "ì—­í–‰ë ¬ê³¼ ìœ ì‚¬ ì—­í–‰ë ¬\nnumpy.linalg ëª¨ë“ˆ ì•ˆì— ë§ì€ ì„ í˜• ëŒ€ìˆ˜ í•¨ìˆ˜ë“¤ì´ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ inv í•¨ìˆ˜ëŠ” ì •ë°© í–‰ë ¬ì˜ ì—­í–‰ë ¬ì„ ê³„ì‚°í•©ë‹ˆë‹¤:\n\nimport numpy.linalg as linalg\n\nm3 = np.array([[1,2,3],[5,7,11],[21,29,31]])\nm3\n\narray([[ 1,  2,  3],\n       [ 5,  7, 11],\n       [21, 29, 31]])\n\n\n\nlinalg.inv(m3)\n\narray([[-2.31818182,  0.56818182,  0.02272727],\n       [ 1.72727273, -0.72727273,  0.09090909],\n       [-0.04545455,  0.29545455, -0.06818182]])\n\n\npinv í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ìœ ì‚¬ ì—­í–‰ë ¬ì„ ê³„ì‚°í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:\n\nlinalg.pinv(m3)\n\narray([[-2.31818182,  0.56818182,  0.02272727],\n       [ 1.72727273, -0.72727273,  0.09090909],\n       [-0.04545455,  0.29545455, -0.06818182]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#ë‹¨ìœ„-í–‰ë ¬",
    "href": "Data_Mining/2022-03-04-numpy.html#ë‹¨ìœ„-í–‰ë ¬",
    "title": "Numpy ê¸°ë³¸",
    "section": "ë‹¨ìœ„ í–‰ë ¬",
    "text": "ë‹¨ìœ„ í–‰ë ¬\ní–‰ë ¬ê³¼ ê·¸ í–‰ë ¬ì˜ ì—­í–‰ë ¬ì„ ê³±í•˜ë©´ ë‹¨ìœ„ í–‰ë ¬ì´ ë©ë‹ˆë‹¤(ì‘ì€ ì†Œìˆ«ì  ì˜¤ì°¨ê°€ ìˆìŠµë‹ˆë‹¤):\n\nm3.dot(linalg.inv(m3))\n\narray([[ 1.00000000e+00, -1.66533454e-16,  0.00000000e+00],\n       [ 6.31439345e-16,  1.00000000e+00, -1.38777878e-16],\n       [ 5.21110932e-15, -2.38697950e-15,  1.00000000e+00]])\n\n\neye í•¨ìˆ˜ëŠ” NxN í¬ê¸°ì˜ ë‹¨ìœ„ í–‰ë ¬ì„ ë§Œë“­ë‹ˆë‹¤:\n\nnp.eye(3)\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#qr-ë¶„í•´",
    "href": "Data_Mining/2022-03-04-numpy.html#qr-ë¶„í•´",
    "title": "Numpy ê¸°ë³¸",
    "section": "QR ë¶„í•´",
    "text": "QR ë¶„í•´\nqr í•¨ìˆ˜ëŠ” í–‰ë ¬ì„ QR ë¶„í•´í•©ë‹ˆë‹¤:\n\nq, r = linalg.qr(m3)\nq\n\narray([[-0.04627448,  0.98786672,  0.14824986],\n       [-0.23137241,  0.13377362, -0.96362411],\n       [-0.97176411, -0.07889213,  0.22237479]])\n\n\n\nr\n\narray([[-21.61018278, -29.89331494, -32.80860727],\n       [  0.        ,   0.62427688,   1.9894538 ],\n       [  0.        ,   0.        ,  -3.26149699]])\n\n\n\nq.dot(r)  # q.rëŠ” m3ì™€ ê°™ìŠµë‹ˆë‹¤\n\narray([[ 1.,  2.,  3.],\n       [ 5.,  7., 11.],\n       [21., 29., 31.]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#í–‰ë ¬ì‹",
    "href": "Data_Mining/2022-03-04-numpy.html#í–‰ë ¬ì‹",
    "title": "Numpy ê¸°ë³¸",
    "section": "í–‰ë ¬ì‹",
    "text": "í–‰ë ¬ì‹\ndet í•¨ìˆ˜ëŠ” í–‰ë ¬ì‹ì„ ê³„ì‚°í•©ë‹ˆë‹¤:\n\nlinalg.det(m3)  # í–‰ë ¬ì‹ ê³„ì‚°\n\n43.99999999999997"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#ê³ ìœ³ê°’ê³¼-ê³ ìœ ë²¡í„°",
    "href": "Data_Mining/2022-03-04-numpy.html#ê³ ìœ³ê°’ê³¼-ê³ ìœ ë²¡í„°",
    "title": "Numpy ê¸°ë³¸",
    "section": "ê³ ìœ³ê°’ê³¼ ê³ ìœ ë²¡í„°",
    "text": "ê³ ìœ³ê°’ê³¼ ê³ ìœ ë²¡í„°\neig í•¨ìˆ˜ëŠ” ì •ë°© í–‰ë ¬ì˜ ê³ ìœ³ê°’ê³¼ ê³ ìœ ë²¡í„°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤:\n\neigenvalues, eigenvectors = linalg.eig(m3)\neigenvalues # Î»\n\narray([42.26600592, -0.35798416, -2.90802176])\n\n\n\neigenvectors # v\n\narray([[-0.08381182, -0.76283526, -0.18913107],\n       [-0.3075286 ,  0.64133975, -0.6853186 ],\n       [-0.94784057, -0.08225377,  0.70325518]])\n\n\n\nm3.dot(eigenvectors) - eigenvalues * eigenvectors  # m3.v - Î»*v = 0\n\narray([[ 8.88178420e-15,  2.22044605e-16, -3.10862447e-15],\n       [ 3.55271368e-15,  2.02615702e-15, -1.11022302e-15],\n       [ 3.55271368e-14,  3.33413852e-15, -8.43769499e-15]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#íŠ¹ì‡ê°’-ë¶„í•´",
    "href": "Data_Mining/2022-03-04-numpy.html#íŠ¹ì‡ê°’-ë¶„í•´",
    "title": "Numpy ê¸°ë³¸",
    "section": "íŠ¹ì‡ê°’ ë¶„í•´",
    "text": "íŠ¹ì‡ê°’ ë¶„í•´\nsvd í•¨ìˆ˜ëŠ” í–‰ë ¬ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ê·¸ í–‰ë ¬ì˜ íŠ¹ì‡ê°’ ë¶„í•´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤:\n\nm4 = np.array([[1,0,0,0,2], [0,0,3,0,0], [0,0,0,0,0], [0,2,0,0,0]])\nm4\n\narray([[1, 0, 0, 0, 2],\n       [0, 0, 3, 0, 0],\n       [0, 0, 0, 0, 0],\n       [0, 2, 0, 0, 0]])\n\n\n\nU, S_diag, V = linalg.svd(m4)\nU\n\narray([[ 0.,  1.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0., -1.],\n       [ 0.,  0.,  1.,  0.]])\n\n\n\nS_diag\n\narray([3.        , 2.23606798, 2.        , 0.        ])\n\n\nsvd í•¨ìˆ˜ëŠ” Î£ì˜ ëŒ€ê° ì›ì†Œ ê°’ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤. ì „ì²´ Î£ í–‰ë ¬ì€ ë‹¤ìŒê³¼ ê°™ì´ ë§Œë“­ë‹ˆë‹¤:\n\nS = np.zeros((4, 5))\nS[np.diag_indices(4)] = S_diag\nS  # Î£\n\narray([[3.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 2.23606798, 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 2.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ]])\n\n\n\nV\n\narray([[-0.        ,  0.        ,  1.        , -0.        ,  0.        ],\n       [ 0.4472136 ,  0.        ,  0.        ,  0.        ,  0.89442719],\n       [-0.        ,  1.        ,  0.        , -0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ],\n       [-0.89442719,  0.        ,  0.        ,  0.        ,  0.4472136 ]])\n\n\n\nU.dot(S).dot(V) # U.Î£.V == m4\n\narray([[1., 0., 0., 0., 2.],\n       [0., 0., 3., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 2., 0., 0., 0.]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#ëŒ€ê°ì›ì†Œì™€-ëŒ€ê°í•©",
    "href": "Data_Mining/2022-03-04-numpy.html#ëŒ€ê°ì›ì†Œì™€-ëŒ€ê°í•©",
    "title": "Numpy ê¸°ë³¸",
    "section": "ëŒ€ê°ì›ì†Œì™€ ëŒ€ê°í•©",
    "text": "ëŒ€ê°ì›ì†Œì™€ ëŒ€ê°í•©\n\nnp.diag(m3)  # m3ì˜ ëŒ€ê° ì›ì†Œì…ë‹ˆë‹¤(ì™¼ìª½ ìœ„ì—ì„œ ì˜¤ë¥¸ìª½ ì•„ë˜)\n\narray([ 1,  7, 31])\n\n\n\nnp.trace(m3)  # np.diag(m3).sum()ì™€ ê°™ìŠµë‹ˆë‹¤\n\n39"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#ì„ í˜•-ë°©ì •ì‹-í’€ê¸°",
    "href": "Data_Mining/2022-03-04-numpy.html#ì„ í˜•-ë°©ì •ì‹-í’€ê¸°",
    "title": "Numpy ê¸°ë³¸",
    "section": "ì„ í˜• ë°©ì •ì‹ í’€ê¸°",
    "text": "ì„ í˜• ë°©ì •ì‹ í’€ê¸°\nsolve í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì„ í˜• ë°©ì •ì‹ì„ í’‰ë‹ˆë‹¤:\n\n\\(2x + 6y = 6\\)\n\\(5x + 3y = -9\\)\n\n\ncoeffs  = np.array([[2, 6], [5, 3]])\ndepvars = np.array([6, -9])\nsolution = linalg.solve(coeffs, depvars)\nsolution\n\narray([-3.,  2.])\n\n\nsolutionì„ í™•ì¸í•´ ë³´ì£ :\n\ncoeffs.dot(solution), depvars  # ë„¤ ê°™ë„¤ìš”\n\n(array([ 6., -9.]), array([ 6, -9]))\n\n\nì¢‹ìŠµë‹ˆë‹¤! ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œë„ solutionì„ í™•ì¸í•´ ë³´ì£ :\n\nnp.allclose(coeffs.dot(solution), depvars)\n\nTrue"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#ë°”ì´ë„ˆë¦¬-.npy-í¬ë§·",
    "href": "Data_Mining/2022-03-04-numpy.html#ë°”ì´ë„ˆë¦¬-.npy-í¬ë§·",
    "title": "Numpy ê¸°ë³¸",
    "section": "ë°”ì´ë„ˆë¦¬ .npy í¬ë§·",
    "text": "ë°”ì´ë„ˆë¦¬ .npy í¬ë§·\nëœë¤ ë°°ì—´ì„ ë§Œë“¤ê³  ì €ì¥í•´ ë³´ì£ .\n\na = np.random.rand(2,3)\na\n\narray([[0.5435938 , 0.92886307, 0.01535158],\n       [0.4157283 , 0.9102127 , 0.55129708]])\n\n\n\nnp.save(\"my_array\", a)\n\nëì…ë‹ˆë‹¤! íŒŒì¼ ì´ë¦„ì˜ í™•ì¥ìë¥¼ ì§€ì •í•˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— ë„˜íŒŒì´ëŠ” ìë™ìœ¼ë¡œ .npyë¥¼ ë¶™ì…ë‹ˆë‹¤. íŒŒì¼ ë‚´ìš©ì„ í™•ì¸í•´ ë³´ê² ìŠµë‹ˆë‹¤:\n\nwith open(\"my_array.npy\", \"rb\") as f:\n    content = f.read()\n\ncontent\n\nb\"\\x93NUMPY\\x01\\x00v\\x00{'descr': '<f8', 'fortran_order': False, 'shape': (2, 3), }                                                          \\nY\\xc1\\xfc\\xd0\\x1ee\\xe1?\\xde{3\\t?\\xb9\\xed?\\x80V\\x08\\xef\\xa5p\\x8f?\\x96I}\\xe0J\\x9b\\xda?\\xe0U\\xfaav \\xed?\\xd8\\xe50\\xc59\\xa4\\xe1?\"\n\n\nì´ íŒŒì¼ì„ ë„˜íŒŒì´ ë°°ì—´ë¡œ ë¡œë“œí•˜ë ¤ë©´ load í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:\n\na_loaded = np.load(\"my_array.npy\")\na_loaded\n\narray([[0.5435938 , 0.92886307, 0.01535158],\n       [0.4157283 , 0.9102127 , 0.55129708]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#í…ìŠ¤íŠ¸-í¬ë§·",
    "href": "Data_Mining/2022-03-04-numpy.html#í…ìŠ¤íŠ¸-í¬ë§·",
    "title": "Numpy ê¸°ë³¸",
    "section": "í…ìŠ¤íŠ¸ í¬ë§·",
    "text": "í…ìŠ¤íŠ¸ í¬ë§·\në°°ì—´ì„ í…ìŠ¤íŠ¸ í¬ë§·ìœ¼ë¡œ ì €ì¥í•´ ë³´ì£ :\n\nnp.savetxt(\"my_array.csv\", a)\n\níŒŒì¼ ë‚´ìš©ì„ í™•ì¸í•´ ë³´ê² ìŠµë‹ˆë‹¤:\n\nwith open(\"my_array.csv\", \"rt\") as f:\n    print(f.read())\n\n5.435937959464737235e-01 9.288630656918674955e-01 1.535157809943688001e-02\n4.157283012656532994e-01 9.102126992826775620e-01 5.512970782648904944e-01\n\n\n\nì´ íŒŒì¼ì€ íƒ­ìœ¼ë¡œ êµ¬ë¶„ëœ CSV íŒŒì¼ì…ë‹ˆë‹¤. ë‹¤ë¥¸ êµ¬ë¶„ìë¥¼ ì§€ì •í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:\n\nnp.savetxt(\"my_array.csv\", a, delimiter=\",\")\n\nì´ íŒŒì¼ì„ ë¡œë“œí•˜ë ¤ë©´ loadtxt í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:\n\na_loaded = np.loadtxt(\"my_array.csv\", delimiter=\",\")\na_loaded\n\narray([[0.5435938 , 0.92886307, 0.01535158],\n       [0.4157283 , 0.9102127 , 0.55129708]])"
  },
  {
    "objectID": "Data_Mining/2022-03-04-numpy.html#ì••ì¶•ëœ-.npz-í¬ë§·",
    "href": "Data_Mining/2022-03-04-numpy.html#ì••ì¶•ëœ-.npz-í¬ë§·",
    "title": "Numpy ê¸°ë³¸",
    "section": "ì••ì¶•ëœ .npz í¬ë§·",
    "text": "ì••ì¶•ëœ .npz í¬ë§·\nì—¬ëŸ¬ ê°œì˜ ë°°ì—´ì„ ì••ì¶•ëœ í•œ íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•©ë‹ˆë‹¤:\n\nb = np.arange(24, dtype=np.uint8).reshape(2, 3, 4)\nb\n\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]], dtype=uint8)\n\n\n\nnp.savez(\"my_arrays\", my_a=a, my_b=b)\n\níŒŒì¼ ë‚´ìš©ì„ í™•ì¸í•´ ë³´ì£ . .npz íŒŒì¼ í™•ì¥ìê°€ ìë™ìœ¼ë¡œ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.\n\nwith open(\"my_arrays.npz\", \"rb\") as f:\n    content = f.read()\n\nrepr(content)[:180] + \"[...]\"\n\n'b\"PK\\\\x03\\\\x04\\\\x14\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00!\\\\x00\\\\x063\\\\xcf\\\\xb9\\\\xb0\\\\x00\\\\x00\\\\x00\\\\xb0\\\\x00\\\\x00\\\\x00\\\\x08\\\\x00\\\\x14\\\\x00my_a.npy\\\\x01\\\\x00\\\\x10\\\\x00\\\\xb0\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\xb0\\\\x00\\\\x00\\\\x[...]'\n\n\në‹¤ìŒê³¼ ê°™ì´ ì´ íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\nmy_arrays = np.load(\"my_arrays.npz\")\nmy_arrays\n\n<numpy.lib.npyio.NpzFile at 0x7f9791c73d60>\n\n\nê²Œìœ¼ë¥¸ ë¡œë”©ì„ ìˆ˜í–‰í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ì™€ ìœ ì‚¬í•œ ê°ì²´ì…ë‹ˆë‹¤:\n\nmy_arrays.keys()\n\nKeysView(<numpy.lib.npyio.NpzFile object at 0x7f9791c73d60>)\n\n\n\nmy_arrays[\"my_a\"]\n\narray([[0.5435938 , 0.92886307, 0.01535158],\n       [0.4157283 , 0.9102127 , 0.55129708]])"
  },
  {
    "objectID": "Data_Mining/2022-03-18-Python Language Basics.html",
    "href": "Data_Mining/2022-03-18-Python Language Basics.html",
    "title": "Python ê¸°ì´ˆ",
    "section": "",
    "text": "Python language bascis\n\nlanguage semantics\n\nIdentation, not braces // ë“¤ì—¬ì“°ê¸°\nfor x in array: if x < pivot : less.append(x) else: greater.append(x)\na=5; b=6; c=7\n\na=5; b=6; c=7\n\n\nc\n\n7\n\n\n\n\nEverything is an object // ì˜¤ë¸Œì íŠ¸ë¥¼ ê¸°ë³¸ í• ë‹¹í•˜ê³  ë™ì‘í•¨\n\n\nComments\nresult =[] for line in file_handle: # keep the empty line for now # if len(line) == 0: # continue result.append(line.replace(â€˜fooâ€™,â€˜barâ€™)) print(â€œReach this lineâ€) #simple statis report\n\n\nFunction and object method calls\nresult = f(x,y,z) g()\nobj.some_method(x,y,z) result = f(a,b,c,d=5,e=â€˜fooâ€™)\n\n\nVariables and argument passing\n\na=[1,2,3]\n\n\nb=a\n\n\na.append(4) #ë’¤ì— ë¶™ì—¬ì¤Œ \nb\n\n[1, 2, 3, 4]\n\n\n\na=5\ntype(a) #int - integer - ìˆ˜ì¹˜í˜• \n\nint\n\n\n\na='foo'\ntype(a)\n\nstr\n\n\n\n'5'+5\n\nTypeError: can only concatenate str (not \"int\") to str\n\n\n\na=4.5\nb=2\n# String formatting, to be visted later\nprint('a is {0}, b is {1}'.format(type(a),type(b)))\na/b\n\na is <class 'float'>, b is <class 'int'>\n\n\n2.25\n\n\n\na=5\nisinstance(a,int)\n\nTrue\n\n\n\na=5; b=4.5\nisinstance(a,(int,float))\nisinstance(b,(int,float))\n\nTrue\n\n\n\na='foo' #Tab í‚¤ë¥¼ ëˆ„ë¥´ë©´ ë‹¤ì–‘í•œ ì˜µì…˜ì´ ë‚˜ì˜¤ë©´ì„œ ëŒ€ë¬¸ìí™” ì¹´ìš´íŠ¸ë“± ì—¬ëŸ¬ì˜µì…˜ ì¡´ì¬\n\n\na.<Press Tab>\n\nSyntaxError: invalid syntax (3084391244.py, line 1)\n\n\n\na.upper()\n\n'FOO'\n\n\n\ngetattr(a,'split') #objectì— ì†í•œ ì†ì„±ì„ ê°€ì§€ê³  ì˜¨ë‹¤ \n\n<function str.split(sep=None, maxsplit=-1)>\n\n\n\n\n\nDuck typing\n\ndef isiterable(obj):\n    try:\n        iter(obj)\n        return True\n    except TypeError: # not literable\n        return False\n\n\nisiterable('a string') \n\nTrue\n\n\n\nisiterable([1,2,3])\n\nTrue\n\n\n\nisiterable(5)\n\nFalse\n\n\nif not isinstance(x,list) and isiterable(x): x=list(x)\n\n\nImports\nIn Python a module is simply a file with the .py extension containing Python code. Suppose that we had the following module:\nIf you wanted to access the variables and functions defined in some_module.py, from another file in the same directory we could do:\n\nimport some_module #ê°™ì€ ë””ë ‰í† ë¦¬ì˜ .pyíŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì„œ ì‚¬ìš©ê°€ëŠ¥ \n\nresult = some_module.f(5)\nresult\n\n7\n\n\n\npi = some_module.PI\npi\n\n3.14159\n\n\nOr equivalently:\n\nfrom some_module import f,g,PI\nresult = g(5,PI)\nresult\n\n8.14159\n\n\nBy using the as keyword you can give imports different variable names:\n\nimport some_module as sm\nfrom some_module import PI as pi, g as gf\n\nr1 = sm.f(pi)\nr2 = gf(6,pi)\n\n\nr1\n\n5.14159\n\n\n\nr2\n\n9.14159\n\n\n\n\nBinary operators and comparisons\nMost of the binary math operations and comparisons are as you might expect:\n\n5-7\n12+21.5\n5 <= 2\n\nFalse\n\n\n\na = [1,2,3]\nb=a\nc=list()\na is b\na is not c \n\nTrue\n\n\n\na == c\n\nFalse\n\n\n\na = None\na is None\n\nTrue\n\n\n\nMutable and immutable objects\nMost object in Python such as list, dict, NumPy arrays, and most user-defined types(classes), are mutable.\nThis means that the object or values that they contain can be modified\n\na_list = ['foo',2,[4,5]]\na_list[2] = (3,4)\na_list\n\n['foo', 2, (3, 4)]\n\n\nOthers, like strings and tuples, are immutable:\n\na_tuple = (3,5,(4,5))\na_tuple[1] = 'four' #tupleì€ ë³€ê²½ì´ ë¶ˆê°€ëŠ¥ í•˜ë‹¤ / listëŠ” ê°€ëŠ¥  \n\nTypeError: 'tuple' object does not support item assignment\n\n\n\n\n\nScalar Types\n\nNumeric types\nThe primary Python types for numbers are int and float. An int can store arbitrarily large numbers:\n\nival = 17239871\nival ** 6\n\n26254519291092456596965462913230729701102721\n\n\nFloating-point numbers are represented with the Python float type. Under the hood each one is a double-precision(64-bit) value. They can also be expressed with scientific notation:\n\nfval = 7.2343\nfval2 = 6.78e-5\n\n\n3/2\n\n1.5\n\n\n\ntype(3/2)\n\nfloat\n\n\n\n3//2 #ëª«\n\n1\n\n\n\ntype(3//2)\n\nint\n\n\n\n\n\nStrings\nMany people use Python for its powerful and flexible built-in string processing capabilities.\nYou can write string literals using either single quotes or double quotes:\n\na = 'one way of writing a string'\nb = 'another way'\n\nFor multiline strings with line breaks, you can use triple qutoes, either â€™â€™â€™ or â€œâ€œâ€\n\nc = \"\"\" \nThis is a longer string that\nspans multiple lines\n\"\"\"\n\n\nc\n\n' \\nThis is a longer string that\\nspans multiple lines\\n'\n\n\nIt may surprise you that this string c actually contains for lines of text;\nthe line breaks after â€œâ€œâ€ and after lines are include in the string.\nWe can count the new line characters with the count method on c:\n\nc.count('\\n')\n\n3\n\n\nPython strings are immutable; you cannot modify a string:\n\na = 'this is a string'\na[10] = 'f' #ë³€í˜•ë¶ˆê°€ \n\nTypeError: 'str' object does not support item assignment\n\n\n\nb = a.replace('string','longer string')\nb\n\n'this is a longer string'\n\n\n\na #ë³€í˜•ì€ ì•ˆë˜ë‚˜ ëŒ€ì²´ëŠ” ê°€ëŠ¥\n\n'this is a string'\n\n\nMany Python object can be converted to a string using the str function\n\na = 5.6\ns = str(a)\nprint(s)\n\n5.6\n\n\nStrings are a sequnce of Unicode characters and therefore can be treated like other sequences, such as lists and tuples(which we will explore in more detail in the next chapter):\n\ns='python'\nlist(s)\n\n['p', 'y', 't', 'h', 'o', 'n']\n\n\n\ns[:3]\n\n'pyt'\n\n\nThe syntax s[:3] is called slicing and is implemented for many kinds of Python sequences. This will be explained in moire detail later on, as it it used extensively in the book.\nThe backslash character Â is an escape character, meaning that it is used to specify special characters like newline or Unicode characters. To write a string literal with backslashes, you need to escape them:\n\nprint('12\\n34') #\\n is Enter \n\n12\n34\n\n\n\ns = '12\\\\34' #ë°±ìŠ¬ë˜ì‰¬ë¥¼ ë¬¸ìì—´ë¡œ ë°”ê¾¼ë‹¤ \nprint(s)\n\n12\\34\n\n\nAdding two strings together concatenates them and produces a new string:\n\na='this is the first half '\nb='and this is the second half'\na+b\n\n'this is the first half and this is the second half'\n\n\nString templating or formatting is another important topic.\nThe number of ways tod do so has expanded with the advent of Python 3,\nand here I will briefly describe the mechanics of one of the main interfaces.\nString objects have a format method that can be used to substitute formatted arguments into the string, producting a new string:\n\ntemplate = '{0:.2f} {1:s} are worth US${2:d}'\ntemplate\n\n'{0:.2f} {1:s} are worth US${2:d}'\n\n\n\n{0:.2f} means to format the first argument as a floating-point number with two decimal places.\n{1:s} means to format the second argument as a string.\n{2:d} means to format the third argument as an exact integer\n\n\ntemplate.format(4.560, 'Argentine Pesos',1)\n\n'4.56 Argentine Pesos are worth US$1'\n\n\n\ntemplate.format(1263.23,'won',1)\n\n'1263.23 won are worth US$1'\n\n\n\n\nBooleans\n\nThe two boolean value in python are written as True as False.\n\n\nComprasions and other conditional expressions evaluate to either True or False.\n\n\nBoolean values are combined with the and or keywords:\n\nTrue and True\n\nTrue\n\n\n\nFalse or True\n\nTrue\n\n\n\n\nType casting\nThe str, bool, int, and float types are also functions that can be used to cast values\n\ns='3.14159'\nfval=float(s)\ntype(fval)\n\nfloat\n\n\n\nint(fval)\n\n3\n\n\n\nbool(fval)\n\nTrue\n\n\n\nbool(0)\n\nFalse\n\n\n\n\n\nNone\nNone is the Python null value type. If a function does not explicitly return a value, it implicitly returns None:\n\na = None\na is None\n\nTrue\n\n\n\nb = 5\nb is not None\n\nTrue\n\n\nNone is also a common default vlaue for function arguments:\n\ndef add_and_maybe_multiply(a,b,c=None):\n    result = a+b\n\n    if c is not None:\n        result = result * c\n    \n    return result\n\n\nadd_and_maybe_multiply(5,3)\n\n8\n\n\n\nadd_and_maybe_multiply(5,3,10)\n\n80\n\n\nWhile a technical point, itâ€™s worth bearing in mind that None is not only a reserved keyword but also a unique instance of NoneType:\n\ntype(None)\n\nNoneType\n\n\n\n\nDates and times\nThe built-in Python datetime module provides datetime, date, and time types.\nThe datetime type, as you may imagine, combines the information stored in date and time and is the most commonly used:\n\nfrom datetime import datetime, date, time\ndt = datetime(2011,10,29,20,30,21) #year,month,day,hour,minute,second\ndt\n\ndatetime.datetime(2011, 10, 29, 20, 30, 21)\n\n\n\ndt.day\n\n29\n\n\n\ndt.minute\n\n30\n\n\nGiven a datetime instance, you can extract the equivalent date and time objects by calling methods on the datetime of the same name:\n\ndt.date()\n\ndatetime.date(2011, 10, 29)\n\n\n\ndt.time()\n\ndatetime.time(20, 30, 21)\n\n\nThe strfime method formats a datetime as a string:\n\ndt.strftime('%m/%d/%Y %H:%M')\n\n'10/29/2011 20:30'\n\n\n\ndt.strftime('%Y/%m/%d %H:%M')\n\n'2011/10/29 20:30'\n\n\nString can be converted (parsed) into datetime objects with the strptime function:\n\ndatetime.strptime('20091031',\"%Y%m%d\")\n\ndatetime.datetime(2009, 10, 31, 0, 0)\n\n\nWhen you are aggregating or otherwise grouping time series data, it will occasionally be useful to replace time fields of a series of datetimes-for example,replacing the minute and second fields with zero:\n\ndt.replace(minute=0,second=0)\n\ndatetime.datetime(2011, 10, 29, 20, 0)\n\n\n\ndt2 = datetime(2011,11,15,22,30) \ndelta =dt2 - dt #dt = datetime(2011,10,29,20,30,21)\ndelta\n\ndatetime.timedelta(days=17, seconds=7179)\n\n\n\ntype(delta)\n\ndatetime.timedelta\n\n\n\ndt\ndt + delta\n\ndatetime.datetime(2011, 11, 15, 22, 30)\n\n\n\n\n\nControl Flow\nPython has several built-in keywords for conditonal logic, loops, and other standard control flow concepts found in other porgramming languages.\n\nif, elif, and else\nThe if statement is one of the most well-known control flow statement types.\nIt checks a conditon that, if True, evaluates the code in the block that follows:\n\nx = -5\nif x < 0:\n    print('It is negative')\n\nIt is negative\n\n\nAn if statement can be optionally followed by one or mor elif blocks and a catch all else block if all of the conditions are False\n\nx = -5\n\nif x < 0 :\n    print('It is negative')\nelif x == 0 :\n    print('Equal to zero')\nelif 0 < x < 5 :\n    print('Postive but smaller than 5')\nelse :\n    print('Postive and larger than or equal to 5')\n\nIt is negative\n\n\nIf any of the conditions is True, no futher elif or else blocks will be reached.\nWith a compound condition using and or or, conditions are evaluated left to right and will short-circuit:\n\na = 5; b = 7\nc = 8; d = 4\nif a < b or c > d :\n    print('Made it')\n\nMade it\n\n\nIn this examplme, the comparison c > d never get evaluated because the first compar- ison was True. It is also possible to chain comparisons:\n\n4 > 3 > 2 > 1\n\nTrue\n\n\n\n3 > 5 or 2 > 1\n\nTrue\n\n\n\n3>5>2>1\n\nFalse\n\n\n\n\nfor loops\nfor loops are fir iterating over a collection (like a list or tuple) or an iterater. They standard syntax for a for loop is:\nfor value in collection: # do something with value\nYou can advance a for loop to the next iteration, skipping the remainder of the block, using the continue keyword.\nConsider this code, which sums up integers in a list and skips None values\n\nsequnce = [1,2,None,4,None,5]\ntotal = 0\n\nfor value in sequnce:\n    total += value\n\nTypeError: unsupported operand type(s) for +=: 'int' and 'NoneType'\n\n\n\nsequnce = [1,2,None,4,None,5]\ntotal = 0\n\nfor value in sequnce:\n    if value is None:\n        continue\n    total += value\n\n\ntotal\n\n12\n\n\nA for loop cna be exited altogether with the break keyword. This code sums ele- ments of the list until a 5 is reached:\n\nsequnce = [1,2,0,4,6,5,2,1]\ntotal_until_5 = 0\n\nfor value in sequnce:\n    if value == 5:\n        break\n    total_until_5 += value\n\n\ntotal_until_5 #ê°’ì´ 5ê°€ ë˜ë©´ ë©ˆì¶”ëŠ” ì¡°ê±´ìœ¼ë¡œ 1+2+0+4+6ê¹Œì§€ ê³„ì‚°í›„ 5ê°€ ì˜¤ê¸°ì— forë¬¸ì´ ì¢…ë£Œëœë‹¤ \n\n13\n\n\nThe break keyword only terminates the innermost for loop; any outer for loops will continue to run:\n\nfor i in range(4):\n    for j in range(4):\n        if j>i:\n            break\n        print((i,j))\n\n(0, 0)\n(1, 0)\n(1, 1)\n(2, 0)\n(2, 1)\n(2, 2)\n(3, 0)\n(3, 1)\n(3, 2)\n(3, 3)\n\n\nAs we will see in more detail, if the elements in the collection or iterator are sequences (tuples or list, say), they can be conveniently unpacked into variables in the for loop statement:\nfor a,b,c in iterator: # do something\n\nfor a,b,c in [[1,2,3],[4,5,6],[7,8,9]]:\n    print(a,b,c)\n\n1 2 3\n4 5 6\n7 8 9\n\n\n\nWhile loops\nA while looops specifies a conditon and a block o f code that is to be excused until the condition evaluates to False or the loops is explicitly ended with break:\n\nx = 256\ntotal = 0\nwhile x > 0:\n    if total > 500 :\n        break\n    total += x\n    x = x //2\n\n\ntotal #504\n\n504\n\n\n\nx\n\n4\n\n\n\n256+128+64+32+16+8\n\n504\n\n\n\n\npass\npass is the â€œno-opâ€(No Operation) statement in Python. It can be used in block where no action is to be taken (or as a placeholder for code not yer implemented); it is only required becauese Python uese whitespace to delimit blocks:\n\nx = -1\n\nif x < 0:\n    print('negative')\nelif x == 0:\n    # TODO: put something smart here\n    pass\nelse:\n    print('Postive!')\n\nnegative\n\n\n\n\nrange\nThe range function returns an iterator that yields a sequnce of evenly spaced intergers:\n\nrange(10)\n\nrange(0, 10)\n\n\n\nlist(range(10))\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\nBoth a start,end,and step(Which may be negative) can be given:\n\nlist(range(0,20,2)) #ë“±ì°¨ìˆ˜ì—´\n\n[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n\n\n\nlist(range(5,0,-1)) #ë¦¬ë²„ìŠ¤ ì¸ë±ìŠ¤\n\n[5, 4, 3, 2, 1]\n\n\nAs you can see, range prodices integers up to but not including the endpoint.\nA common use of range is for iterating through sequcnes by index:\n\nseq = [1,2,3,4]\nfor i in range(len(seq)):\n    val = seq[i]\n\n\nval\n\n4\n\n\nWhile you can use fuctions loke list to store all the integers generated by range in some other data structure, often the default iterator form will be what you want. This snippet sums all numbers form 0 to 99,999 that are multiples of 3 or 5:\n\nsum = 0\nfor i in range(10000) :\n    # % is the modulo operator\n    if i % 3 == 0 or i % 5 == 0:\n        sum += 1\n\n\n\n\nTernary expressions\nvalue = true - expr if conditon else false - expr\nHere, true-expr and false-expr cna be any Python expressions. It has the identical effect as the more verbose:\nif conditon: value = true-expr else: value = false-expr\n\nx = 5\n'Non-negative' if x >= 0 else 'Negative'\n\n'Non-negative'\n\n\n\nx = 5\n\na = 100 if x>= 0 else -100\na\n\n100"
  },
  {
    "objectID": "Data_Mining/2022-05-07-matplotlib and seaborn.html",
    "href": "Data_Mining/2022-05-07-matplotlib and seaborn.html",
    "title": "Visualization_with_seaborn_and_matplotlib",
    "section": "",
    "text": "seabornê³¼ matplotlibì˜ ì‹œê°í™”\nì½”ë“œ ì¶œì²˜ - ì´ì œí˜„ë‹˜ ë¸”ë¡œê·¸"
  },
  {
    "objectID": "Data_Mining/2022-05-07-matplotlib and seaborn.html#visualization-with-seaborn",
    "href": "Data_Mining/2022-05-07-matplotlib and seaborn.html#visualization-with-seaborn",
    "title": "Visualization_with_seaborn_and_matplotlib",
    "section": "Visualization with seaborn",
    "text": "Visualization with seaborn\nseabornì€ pythonì˜ ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ì¸ matplolibë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì œì‘ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\n\nimport seaborn as sns\nsns.set()\nsns.set(style=\"darkgrid\")\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nplt.rcParams['figure.figsize']=(5,5) #figureì˜ ì‚¬ì´ì¦ˆë¥¼ ì„¤ì •ì´ ê°€ëŠ¥í•˜ë‹¤ \n\n\nLoding dataset\n\n# ì‚¬ìš©í•  ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° \ndata_BM = pd.read_csv('bigmart_data.csv')\ndata_BM = data_BM.dropna(how=\"any\") #NAê°’ ì œê±°\ndata_BM[\"Visibility_Scaled\"] = data_BM[\"Item_Visibility\"] * 100 #Visibility_Scaled ì»¬ëŸ¼ì˜ ê°’ë“¤ì— 100 ê³±í•´ì¤Œ\ndata_BM.head()\n\n\n\n\n\n  \n    \n      \n      Item_Identifier\n      Item_Weight\n      Item_Fat_Content\n      Item_Visibility\n      Item_Type\n      Item_MRP\n      Outlet_Identifier\n      Outlet_Establishment_Year\n      Outlet_Size\n      Outlet_Location_Type\n      Outlet_Type\n      Item_Outlet_Sales\n      Visibility_Scaled\n    \n  \n  \n    \n      0\n      FDA15\n      9.300\n      Low Fat\n      0.016047\n      Dairy\n      249.8092\n      OUT049\n      1999\n      Medium\n      Tier 1\n      Supermarket Type1\n      3735.1380\n      1.604730\n    \n    \n      1\n      DRC01\n      5.920\n      Regular\n      0.019278\n      Soft Drinks\n      48.2692\n      OUT018\n      2009\n      Medium\n      Tier 3\n      Supermarket Type2\n      443.4228\n      1.927822\n    \n    \n      2\n      FDN15\n      17.500\n      Low Fat\n      0.016760\n      Meat\n      141.6180\n      OUT049\n      1999\n      Medium\n      Tier 1\n      Supermarket Type1\n      2097.2700\n      1.676007\n    \n    \n      4\n      NCD19\n      8.930\n      Low Fat\n      0.000000\n      Household\n      53.8614\n      OUT013\n      1987\n      High\n      Tier 3\n      Supermarket Type1\n      994.7052\n      0.000000\n    \n    \n      5\n      FDP36\n      10.395\n      Regular\n      0.000000\n      Baking Goods\n      51.4008\n      OUT018\n      2009\n      Medium\n      Tier 3\n      Supermarket Type2\n      556.6088\n      0.000000\n    \n  \n\n\n\n\n\ndata_BM.describe() #ì´ìƒì¹˜ê°€ ìˆëŠ”ì§€ í™•ì¸\n\n\n\n\n\n  \n    \n      \n      Item_Weight\n      Item_Visibility\n      Item_MRP\n      Outlet_Establishment_Year\n      Item_Outlet_Sales\n      Visibility_Scaled\n    \n  \n  \n    \n      count\n      4650.000000\n      4650.000000\n      4650.000000\n      4650.000000\n      4650.000000\n      4650.000000\n    \n    \n      mean\n      12.898675\n      0.060700\n      141.716328\n      1999.190538\n      2272.037489\n      6.070048\n    \n    \n      std\n      4.670973\n      0.044607\n      62.420534\n      7.388800\n      1497.964740\n      4.460652\n    \n    \n      min\n      4.555000\n      0.000000\n      31.490000\n      1987.000000\n      69.243200\n      0.000000\n    \n    \n      25%\n      8.770000\n      0.025968\n      94.409400\n      1997.000000\n      1125.202000\n      2.596789\n    \n    \n      50%\n      12.650000\n      0.049655\n      142.979900\n      1999.000000\n      1939.808300\n      4.965549\n    \n    \n      75%\n      17.000000\n      0.088736\n      186.614150\n      2004.000000\n      3111.616300\n      8.873565\n    \n    \n      max\n      21.350000\n      0.188323\n      266.888400\n      2009.000000\n      10256.649000\n      18.832266"
  },
  {
    "objectID": "Data_Mining/2022-05-07-matplotlib and seaborn.html#creating-basic-plots",
    "href": "Data_Mining/2022-05-07-matplotlib and seaborn.html#creating-basic-plots",
    "title": "Visualization_with_seaborn_and_matplotlib",
    "section": "1. Creating basic plots",
    "text": "1. Creating basic plots\nmatplotlibì—ì„œ ì—¬ëŸ¬ ì¤„ì´ í•„ìš”í•œ í•œ ì¤„ë¡œ seabornì—ì„œ ëª‡ ê°€ì§€ ê¸°ë³¸ í”Œë¡¯ì„ ë§Œë“œëŠ” ë°©ë²•ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n\nLine Chart\n\nì¼ë¶€ ë°ì´í„° ì„¸íŠ¸ì˜ ê²½ìš° í•œ ë³€ìˆ˜ì˜ ë³€í™”ë¥¼ ì‹œê°„ì˜ í•¨ìˆ˜ë¡œ ì´í•´í•˜ê±°ë‚˜ ì´ì™€ ìœ ì‚¬í•œ ì—°ì† ë³€ìˆ˜ë¥¼ ì´í•´í•˜ê³ ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nseabornì—ì„œ ì´ëŠ” lineplot() í•¨ìˆ˜ë¡œ ì§ì ‘ ë˜ëŠ” kind=\"line\":ì„ ì„¤ì •í•˜ì—¬ relplot()ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\nsns.lineplot(x=\"Item_Weight\", y=\"Item_MRP\",data=data_BM[:50]); #ì²˜ìŒë¶€í„° 50ë²ˆì§¸ ê¹Œì§€ì˜ ë°ì´í„°ë§Œ ì‚¬ìš©í•œë‹¤\n\n\n\n\n\n\nBar Chart\n\nSeabornì—ì„œëŠ” barplot ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨í•˜ê²Œ ë§‰ëŒ€ ì°¨íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nmatplotlibì—ì„œ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì–»ìœ¼ë ¤ë©´ ë°ì´í„° ë²”ì£¼ë¥¼ í˜„ëª…í•˜ê²Œ ê·¸ë£¹í™”í•˜ê¸° ìœ„í•´ ì¶”ê°€ ì½”ë“œë¥¼ ì‘ì„±í•´ì•¼ í–ˆìŠµë‹ˆë‹¤.\nê·¸ë¦¬ê³  ë‚˜ì„œ í”Œë¡¯ì´ ì˜¬ë°”ë¥´ê²Œ ë‚˜ì˜¤ë„ë¡ í›¨ì”¬ ë” ë§ì€ ì½”ë“œë¥¼ ì‘ì„±í•´ì•¼ í–ˆìŠµë‹ˆë‹¤.\n\n\nsns.barplot(x=\"Item_Type\", y=\"Item_MRP\", data=data_BM[:5])\n\n<AxesSubplot:xlabel='Item_Type', ylabel='Item_MRP'>\n\n\n\n\n\n\n\nHistogram\n\ndistplot()ì„ ì‚¬ìš©í•˜ì—¬ seabornì—ì„œ íˆìŠ¤í† ê·¸ë¨ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì—¬ëŸ¬ ì˜µì…˜ì´ ìˆìœ¼ë©° ë…¸íŠ¸ë¶ì—ì„œ ë” ìì„¸íˆ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n\n\nsns.distplot(data_BM['Item_MRP'])\n\n<AxesSubplot:xlabel='Item_MRP', ylabel='Density'>\n\n\n\n\n\n\n\nBox plots\n\nSeabornì—ì„œ boxplotì„ ìƒì„±í•˜ê¸° ìœ„í•´ boxplot()ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\nsns.boxplot(data_BM['Item_Outlet_Sales'], orient='vertical') \n\n<AxesSubplot:xlabel='Item_Outlet_Sales'>\n\n\n\n\n\n\n\nViolin plot\n\në°”ì´ì˜¬ë¦° í”Œë¡¯ì€ ìƒì ë° ìˆ˜ì—¼ í”Œë¡¯ê³¼ ìœ ì‚¬í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.\nì´ëŠ” í•˜ë‚˜(ë˜ëŠ” ê·¸ ì´ìƒ) ë²”ì£¼í˜• ë³€ìˆ˜ì˜ ì—¬ëŸ¬ ìˆ˜ì¤€ì— ê±¸ì¹œ ì •ëŸ‰ì  ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ë³´ì—¬ì¤Œìœ¼ë¡œì¨ í•´ë‹¹ ë¶„í¬ë¥¼ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nëª¨ë“  í”Œë¡¯ êµ¬ì„± ìš”ì†Œê°€ ì‹¤ì œ ë°ì´í„° í¬ì¸íŠ¸ì— í•´ë‹¹í•˜ëŠ” ìƒì í”Œë¡¯ê³¼ ë‹¬ë¦¬ ë°”ì´ì˜¬ë¦° í”Œë¡¯ì€ ê¸°ë³¸ ë¶„í¬ì˜ ì»¤ë„ ë°€ë„ ì¶”ì •ì„ íŠ¹ì§•ìœ¼ë¡œ í•©ë‹ˆë‹¤.\nseabornì—ì„œ violinplot()ì„ ì‚¬ìš©í•˜ì—¬ ë°”ì´ì˜¬ë¦° í”Œë¡¯ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\nsns.violinplot(data_BM['Item_Outlet_Sales'], orient='vertical', color='skyblue')\n\n<AxesSubplot:xlabel='Item_Outlet_Sales'>\n\n\n\n\n\n\n\nScatter plot\n\nê° í¬ì¸íŠ¸ëŠ” ë°ì´í„° ì„¸íŠ¸ì˜ ê´€ì°°ì„ ë‚˜íƒ€ë‚´ëŠ” í¬ì¸íŠ¸ í´ë¼ìš°ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë‘ ë³€ìˆ˜ì˜ ë¶„í¬ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\nì´ ë¬˜ì‚¬ë¥¼ í†µí•´ ëˆˆì€ ê·¸ë“¤ ì‚¬ì´ì— ì˜ë¯¸ ìˆëŠ” ê´€ê³„ê°€ ìˆëŠ”ì§€ ì—¬ë¶€ì— ëŒ€í•œ ìƒë‹¹í•œ ì–‘ì˜ ì •ë³´ë¥¼ ì¶”ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nrelplot()ì„ kind=scatter ì˜µì…˜ê³¼ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ seabornì—ì„œ ì‚°ì ë„ë¥¼ ê·¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\nsns.relplot(x=\"Item_MRP\", y=\"Item_Outlet_Sales\", data=data_BM[:200], kind=\"scatter\"); \n#sns.relplot(x=\"Item_MRP\", y=\"Item_Outlet_Sales\", data=data_BM[:200], kind=\"line\"); #kindì˜ ì„¤ì •ìœ¼ë¡œ í‘œí˜„ ì„¤ì •ì„ ë°”ê¿ˆ\n\n\n\n\n\n\nHue semantic\nì„¸ ë²ˆì§¸ ë³€ìˆ˜ì— ë”°ë¼ ì ì„ ìƒ‰ì¹ í•˜ì—¬ í”Œë¡¯ì— ë‹¤ë¥¸ ì°¨ì›ì„ ì¶”ê°€í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. Seabornì—ì„œëŠ” ì´ê²ƒì„ â€œhue semanticâ€ ì‚¬ìš©ì´ë¼ê³  í•©ë‹ˆë‹¤.\n\nsns.relplot(x=\"Item_MRP\", y=\"Item_Outlet_Sales\", hue=\"Item_Type\",data=data_BM[:200]); #hueë¡œ item_typeì— ëŒ€í•œ í”Œë¡¯ì„ ê·¸ë ¤ì¤Œ \n\n\n\n\n\nsns.lineplot(x=\"Item_Weight\", y=\"Item_MRP\",hue='Outlet_Size',data=data_BM[:150]);\n\n\n\n\n\n\nBubble plot\n\nhue semantic í™œìš©í•˜ì—¬ Item_Visibilityë³„ë¡œ ê±°í’ˆì„ ìƒ‰ì¹ í•¨ê³¼ ë™ì‹œì— ê°œë³„ ê±°í’ˆì˜ í¬ê¸°ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n\n\n# Bubble plot\nsns.relplot(x=\"Item_MRP\", y=\"Item_Outlet_Sales\", data=data_BM[:200], kind=\"scatter\", size=\"Visibility_Scaled\", hue=\"Visibility_Scaled\");\n\n\n\n\n\n\nCategory wise sub plot\n\nSeabornì—ì„œ ì¹´í…Œê³ ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í”Œë¡¯ì„ ë§Œë“¤ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\nê° Outlet_Sizeì— ëŒ€í•œ ì‚°ì ë„ë¥¼ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.\n\n\nsns.relplot(x=\"Item_Weight\", y=\"Item_Visibility\",\n            hue='Outlet_Size',style='Outlet_Size',\n            col='Outlet_Size',data=data_BM[:100]);"
  },
  {
    "objectID": "Data_Mining/2022-05-07-matplotlib and seaborn.html#advance-categorical-plots-in-seaborn",
    "href": "Data_Mining/2022-05-07-matplotlib and seaborn.html#advance-categorical-plots-in-seaborn",
    "title": "Visualization_with_seaborn_and_matplotlib",
    "section": "2. Advance categorical plots in seaborn",
    "text": "2. Advance categorical plots in seaborn\në²”ì£¼í˜• ë³€ìˆ˜ì˜ ê²½ìš° seabornì— ì„¸ ê°€ì§€ ë‹¤ë¥¸ íŒ¨ë°€ë¦¬ê°€ ìˆìŠµë‹ˆë‹¤.\ncatplot()ì—ì„œ ë°ì´í„°ì˜ ê¸°ë³¸ í‘œí˜„ì€ ì‚°ì ë„ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n\na. Categorical scatterplots\n\n\nStrip plot\n\ní•˜ë‚˜ì˜ ë³€ìˆ˜ê°€ ë²”ì£¼í˜•ì¸ ì‚°ì ë„ë¥¼ ê·¸ë¦½ë‹ˆë‹¤.\ncatplot()ì—ì„œ kind=stripì„ ì „ë‹¬í•˜ì—¬ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\nsns.catplot(x=\"Outlet_Size\", y=\"Item_Outlet_Sales\", kind='strip',data=data_BM[:250]);\n\n\n\n\n\n\nSwarm plot\n\nì´ í•¨ìˆ˜ëŠ” stripplot()ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ ì ì´ ê²¹ì¹˜ì§€ ì•Šë„ë¡ ì¡°ì •ë©ë‹ˆë‹¤(ë²”ì£¼í˜• ì¶•ì„ ë”°ë¼ë§Œ).\nì´ë ‡ê²Œ í•˜ë©´ ê°’ ë¶„í¬ë¥¼ ë” ì˜ í‘œí˜„í•  ìˆ˜ ìˆì§€ë§Œ ë§ì€ ìˆ˜ì˜ ê´€ì¸¡ì¹˜ì— ëŒ€í•´ì„œëŠ” ì˜ í™•ì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ìŠ¤íƒ€ì¼ì˜ í”Œë¡¯ì€ ë•Œë•Œë¡œ â€œbeeswarmâ€ì´ë¼ê³  ë¶ˆë¦½ë‹ˆë‹¤.\ncatplot()ì—ì„œ kind=swarmì„ ì „ë‹¬í•˜ì—¬ ì´ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\nsns.catplot(x=\"Outlet_Size\", y=\"Item_Outlet_Sales\", kind='swarm',data=data_BM[:250]);\n\n\n\n\n\n\nb. Categorical distribution plots\n\n\nBox Plots\n\nìƒì ê·¸ë¦¼ì€ ê·¹ë‹¨ê°’ê³¼ í•¨ê»˜ ë¶„í¬ì˜ 3ì‚¬ë¶„ìœ„ìˆ˜ ê°’ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\nâ€œwhiskersâ€ì€ í•˜ìœ„ ë° ìƒìœ„ ì‚¬ë¶„ìœ„ìˆ˜ì˜ 1.5 IQR ë‚´ì— ìˆëŠ” ì ìœ¼ë¡œ í™•ì¥ë˜ê³  ì´ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ê´€ì°°ì€ ë…ë¦½ì ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤.\nì¦‰, ìƒì ê·¸ë¦¼ì˜ ê° ê°’ì€ ë°ì´í„°ì˜ ì‹¤ì œ ê´€ì¸¡ê°’ì— í•´ë‹¹í•©ë‹ˆë‹¤.\n\n\nsns.catplot(x=\"Outlet_Size\", y=\"Item_Outlet_Sales\",kind=\"box\",data=data_BM);\n\n\n\n\n\n\nViolin Plots\n\nsns.catplot(x=\"Outlet_Size\", y=\"Item_Outlet_Sales\",kind=\"violin\",data=data_BM);\n\n\n\n\n\n\nPoint Plot\n\nsns.catplot(x=\"Outlet_Size\", y=\"Item_Outlet_Sales\",kind=\"point\",data=data_BM); #yì¶•ì€ ì—°ì†í˜• í”Œë xì¶•ì€ ë²”ì£¼í˜• \n\n\n\n\n\n\nBar plots\n\nsns.catplot(x=\"Outlet_Size\", y=\"Item_Outlet_Sales\",kind=\"bar\",data=data_BM);"
  },
  {
    "objectID": "Data_Mining/2022-05-07-matplotlib and seaborn.html#density-plots",
    "href": "Data_Mining/2022-05-07-matplotlib and seaborn.html#density-plots",
    "title": "Visualization_with_seaborn_and_matplotlib",
    "section": "3. Density Plots",
    "text": "3. Density Plots\níˆìŠ¤í† ê·¸ë¨ ëŒ€ì‹  Seabornì´ sn.kdeplotìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ì»¤ë„ ë°€ë„ ì¶”ì •ì„ ì‚¬ìš©í•˜ì—¬ ë¶„í¬ì˜ ë¶€ë“œëŸ¬ìš´ ì¶”ì •ì¹˜ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.:\n\n# distribution of Item Visibility\nplt.figure(figsize=(5,5))\nsns.kdeplot(data_BM['Item_Visibility'], shade=True);\n\n\n\n\n\nHistogram and Density Plot\ndistplotì„ ì‚¬ìš©í•˜ì—¬ íˆìŠ¤í† ê·¸ë¨ê³¼ KDEë¥¼ ê²°í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.:\n\nplt.figure(figsize=(10,10))\nsns.distplot(data_BM['Item_Outlet_Sales']);"
  },
  {
    "objectID": "Data_Mining/2022-05-07-matplotlib and seaborn.html#pair-plots",
    "href": "Data_Mining/2022-05-07-matplotlib and seaborn.html#pair-plots",
    "title": "Visualization_with_seaborn_and_matplotlib",
    "section": "4. Pair plots",
    "text": "4. Pair plots\n\nì¡°ì¸íŠ¸ í”Œë¡¯ì„ ë” í° ì°¨ì›ì˜ ë°ì´í„°ì„¸íŠ¸ë¡œ ì¼ë°˜í™”í•˜ë©´ ìŒ í”Œë¡¯ìœ¼ë¡œ ëë‚©ë‹ˆë‹¤. ì´ê²ƒì€ ëª¨ë“  ê°’ ìŒì„ ì„œë¡œì— ëŒ€í•´ í”Œë¡¯í•˜ë ¤ëŠ” ê²½ìš° ë‹¤ì°¨ì› ë°ì´í„° ê°„ì˜ ìƒê´€ ê´€ê³„ë¥¼ íƒìƒ‰í•˜ëŠ” ë° ë§¤ìš° ìœ ìš©í•©ë‹ˆë‹¤.\nì„¸ ê°€ì§€ ë¶“ê½ƒ ì¢…ì˜ ê½ƒìê³¼ ê½ƒë°›ì¹¨ ì¸¡ì •ê°’ì„ ë‚˜ì—´í•˜ëŠ” ì˜ ì•Œë ¤ì§„ Iris ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ê²ƒì„ ì‹œì—°í•  ê²ƒì…ë‹ˆë‹¤.\n\n\niris = sns.load_dataset(\"iris\")\nsns.pairplot(iris, hue='species', height=2.5); #ë‘˜ë‹¤ ì—°ì†í˜•ì´ë©´ scatterí”Œëìœ¼ë¡œ ë³´ì—¬ì¤Œ"
  },
  {
    "objectID": "Data_Mining/2022-05-07-matplotlib and seaborn.html#seaborn-and-matplotlib",
    "href": "Data_Mining/2022-05-07-matplotlib and seaborn.html#seaborn-and-matplotlib",
    "title": "Visualization_with_seaborn_and_matplotlib",
    "section": "Seaborn and Matplotlib",
    "text": "Seaborn and Matplotlib\n\n1.1 Load data\n\nì˜ˆì œë¡œ ì‚¬ìš©í•  í­ê·„ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\nseabornì— ë‚´ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npenguins = sns.load_dataset(\"penguins\")\npenguins.head()\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      Male\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      Female\n    \n    \n      2\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      Female\n    \n    \n      3\n      Adelie\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      Female\n    \n  \n\n\n\n\n\n\n1.2 Figure and Axes\n\nmatplotlibìœ¼ë¡œ ë„í™”ì§€figureë¥¼ ê¹”ê³  ì¶•ê³µê°„axesë¥¼ ë§Œë“­ë‹ˆë‹¤.\n1 x 2 ì¶•ê³µê°„ì„ êµ¬ì„±í•©ë‹ˆë‹¤.\n\n\nfig, axes = plt.subplots(ncols=2, figsize=(8,4))\n\nfig.tight_layout()\n\n\n\n\n\n\n1.3 plot with matplotlib\n\nmatplotlib ê¸°ëŠ¥ì„ ì´ìš©í•´ì„œ ì‚°ì ë„ë¥¼ ê·¸ë¦½ë‹ˆë‹¤.\n\nxì¶•ì€ ë¶€ë¦¬ ê¸¸ì´ bill length\nyì¶•ì€ ë¶€ë¦¬ ìœ„ ì•„ë˜ ë‘ê»˜ bill depth\nìƒ‰ìƒì€ ì¢…speciesë¡œ í•©ë‹ˆë‹¤.\nAdelie, Chinstrap, Gentooì´ ìˆìŠµë‹ˆë‹¤.\n\në‘ ì¶•ê³µê°„ ì¤‘ ì™¼ìª½ì—ë§Œ ê·¸ë¦½ë‹ˆë‹¤.\n\n\nfig, axes = plt.subplots(ncols=2,figsize=(8,4))\n\nspecies_u = penguins[\"species\"].unique()\n\nfor i, s in enumerate(species_u):\n    axes[0].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s],\n                    penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s],\n                    c=f\"C{i}\", label=s, alpha=0.3)\n    \naxes[0].legend(species_u, title=\"species\")\naxes[0].set_xlabel(\"Bill Length (mm)\")\naxes[0].set_ylabel(\"Bill Depth (mm)\")\n\n# plt.show()\nfig.tight_layout()\n\n\n\n\n\n\n1.4 Plot with seaborn\n\në‹¨ ì„¸ ì¤„ë¡œ ê±°ì˜ ë™ì¼í•œ ê·¸ë¦¼ì´ ë‚˜ì™”ìŠµë‹ˆë‹¤.\n\nscatter plotì˜ ì  í¬ê¸°ë§Œ ì‚´ì§ ì‘ìŠµë‹ˆë‹¤.\nlabelì˜ íˆ¬ëª…ë„ë§Œ ì‚´ì§ ë‹¤ë¦…ë‹ˆë‹¤.\n\nseaborn ëª…ë ¹ scatterplot()ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.\nxì¶•ê³¼ yì¶• labelë„ ë°”ê¾¸ì—ˆìŠµë‹ˆë‹¤.\n\nax=axes[1] ì¸ìì—ì„œ ë³¼ ìˆ˜ ìˆë“¯, ì¡´ì¬í•˜ëŠ” axesì— ê·¸ë¦¼ë§Œ ì–¹ì—ˆìŠµë‹ˆë‹¤.\nmatplotlib í‹€ + seaborn ê·¸ë¦¼ ì´ë¯€ë¡œ, matplotlib ëª…ë ¹ì´ ëª¨ë‘ í†µí•©ë‹ˆë‹¤.\n\n\n\nfig, axes = plt.subplots(ncols=2,figsize=(8,4))\n\nspecies_u = penguins[\"species\"].unique()\n\n# plot 0 : matplotlib\n\nfor i, s in enumerate(species_u):\n    axes[0].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s],\n                    penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s],\n                    c=f\"C{i}\", label=s, alpha=0.3)\n    \naxes[0].legend(species_u, title=\"species\")\naxes[0].set_xlabel(\"Bill Length (mm)\")\naxes[0].set_ylabel(\"Bill Depth (mm)\")\n\n\n# plot 1 : seaborn\nsns.scatterplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", data=penguins, alpha=0.3, ax=axes[1])\naxes[1].set_xlabel(\"Bill Length (mm)\")\naxes[1].set_ylabel(\"Bill Depth (mm)\")\n\nfig.tight_layout()\n\n\n\n\n\n\n1.5 matplotlib + seaborn & seaborn + matplotlib\n\nmatplotlibê³¼ seabornì´ ììœ ë¡­ê²Œ ì„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nmatplotlib ì‚°ì ë„ ìœ„ì— seaborn ì¶”ì„¸ì„ ì„ ì–¹ì„ ìˆ˜ ìˆê³ ,\nseaborn ì‚°ì ë„ ìœ„ì— matplotlib ì¤‘ì‹¬ì ì„ ì–¹ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\n\nfig, axes = plt.subplots(ncols=2, figsize=(8, 4))\n\nspecies_u = penguins[\"species\"].unique()\n\n# plot 0 : matplotlib + seaborn\nfor i, s in enumerate(species_u):\n    # matplotlib ì‚°ì ë„\n    axes[0].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s],\n                   penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s],\n                   c=f\"C{i}\", label=s, alpha=0.3\n                  )\n                  \n    # seaborn ì¶”ì„¸ì„ \n    sns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=penguins.loc[penguins[\"species\"]==s], \n                scatter=False, ax=axes[0])\n    \naxes[0].legend(species_u, title=\"species\")\naxes[0].set_xlabel(\"Bill Length (mm)\")\naxes[0].set_ylabel(\"Bill Depth (mm)\")\n\n# plot 1 : seaborn + matplotlib\n# seaborn ì‚°ì ë„\nsns.scatterplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", data=penguins, alpha=0.3, ax=axes[1])\naxes[1].set_xlabel(\"Bill Length (mm)\")\naxes[1].set_ylabel(\"Bill Depth (mm)\")\n\nfor i, s in enumerate(species_u):\n    # matplotlib ì¤‘ì‹¬ì \n    axes[1].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s].mean(),\n                   penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s].mean(),\n                   c=f\"C{i}\", alpha=1, marker=\"x\", s=100\n                  )\n\nfig.tight_layout()\n\n\n\n\n\n\n1.6 seaborn + seaborn + matplotlib\n\nseaborn scatterplot + seaborn kdeplot + matplotlib textì…ë‹ˆë‹¤\n\n\nfig, ax = plt.subplots(figsize=(6,5))\n\n# plot 0: scatter plot\nsns.scatterplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", color=\"k\", data=penguins, alpha=0.3, ax=ax, legend=False)\n\n# plot 1: kde plot\nsns.kdeplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", data=penguins, alpha=0.5, ax=ax, legend=False)\n\n# text:\nspecies_u = penguins[\"species\"].unique()\nfor i, s in enumerate(species_u):\n    ax.text(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s].mean(),\n            penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s].mean(),\n            s = s, fontdict={\"fontsize\":14, \"fontweight\":\"bold\",\"color\":\"k\"}\n            )\n\nax.set_xlabel(\"Bill Length (mm)\")\nax.set_ylabel(\"Bill Depth (mm)\")\n\nfig.tight_layout()"
  },
  {
    "objectID": "Data_Mining/2022-05-24-exercise-coordinate-reference-systems.html",
    "href": "Data_Mining/2022-05-24-exercise-coordinate-reference-systems.html",
    "title": "Coordinate Refrence Systems",
    "section": "",
    "text": "This notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link.\n\n\nimport pandas as pd\nimport geopandas as gpd\n\nfrom shapely.geometry import LineString\n\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.geospatial.ex2 import *\n\n\nExercises\n\n1) Load the data.\nRun the next code cell (without changes) to load the GPS data into a pandas DataFrame birds_df.\në‹¤ìŒ ì½”ë“œ ì…€(ë³€ê²½ ì—†ì´)ì„ ì‹¤í–‰í•˜ì—¬ GPS ë°ì´í„°ë¥¼ pandas DataFrame birds_dfì— ë¡œë“œí•©ë‹ˆë‹¤.\n\n# Load the data and print the first 5 rows\nbirds_df = pd.read_csv(\"../input/geospatial-learn-course-data/purple_martin.csv\", parse_dates=['timestamp'])\nprint(\"There are {} different birds in the dataset.\".format(birds_df[\"tag-local-identifier\"].nunique()))\nbirds_df.head()\n\nThere are 11 birds in the dataset, where each bird is identified by a unique value in the â€œtag-local-identifierâ€ column. Each bird has several measurements, collected at different times of the year.\nUse the next code cell to create a GeoDataFrame birds.\n- birds should have all of the columns from birds_df, along with a â€œgeometryâ€ column that contains Point objects with (longitude, latitude) locations.\n-birdsì—ëŠ” birds_dfì˜ ëª¨ë“  ì—´ê³¼ í•¨ê»˜ (ê²½ë„, ìœ„ë„) ìœ„ì¹˜ê°€ ìˆëŠ” Point ê°œì²´ê°€ í¬í•¨ëœ â€œgeometryâ€ ì—´ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. - Set the CRS of birds to {'init': 'epsg:4326'}.\n\n# Your code here: Create the GeoDataFrame\nbirds = gpd.GeoDataFrame(birds_df, geometry=gpd.points_from_xy(birds_df[\"location-long\"], birds_df[\"location-lat\"]))\n\n# Your code here: Set the CRS to {'init': 'epsg:4326'}\nbirds.crs = {'init' :'epsg:4326'}\n\n# Check your answer\nq_1.check()\n\n\n# Lines below will give you a hint or solution code\n#q_1.hint()\n#q_1.solution()\n\n\n\n2) Plot the data.\nNext, we load in the 'naturalearth_lowres' dataset from GeoPandas, and set americas to a GeoDataFrame containing the boundaries of all countries in the Americas (both North and South America). Run the next code cell without changes.\në‹¤ìŒìœ¼ë¡œ GeoPandasì—ì„œ naturalearth_lowres ë°ì´í„° ì„¸íŠ¸ë¥¼ ë¡œë“œí•˜ê³  americasë¥¼ ë¯¸ì£¼(ë¶ë¯¸ì™€ ë‚¨ë¯¸ ëª¨ë‘)ì˜ ëª¨ë“  êµ­ê°€ ê²½ê³„ë¥¼ í¬í•¨í•˜ëŠ” GeoDataFrameìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤. ë³€ê²½ ì—†ì´ ë‹¤ìŒ ì½”ë“œ ì…€ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\n\n# Load a GeoDataFrame with country boundaries in North/South America, print the first 5 rows\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\namericas = world.loc[world['continent'].isin(['North America', 'South America'])]\namericas.head()\n\nUse the next code cell to create a single plot that shows both: (1) the country boundaries in the americas GeoDataFrame, and (2) all of the points in the birds_gdf GeoDataFrame.\në‹¤ìŒ ì½”ë“œ ì…€ì„ ì‚¬ìš©í•˜ì—¬ (1) americas GeoDataFrameì˜ êµ­ê°€ ê²½ê³„ì™€ (2) birds_gdf GeoDataFrameì˜ ëª¨ë“  ì ì„ ëª¨ë‘ í‘œì‹œí•˜ëŠ” ë‹¨ì¼ í”Œë¡¯ì„ ë§Œë“­ë‹ˆë‹¤.\nDonâ€™t worry about any special styling here; just create a preliminary plot, as a quick sanity check that all of the data was loaded properly. In particular, you donâ€™t have to worry about color-coding the points to differentiate between birds, and you donâ€™t have to differentiate starting points from ending points. Weâ€™ll do that in the next part of the exercise.\n\n# Your code here\nax = americas.plot(figsize=(8,8), color='red', linestyle=':', edgecolor='black')\namericas.plot(markersize=1, ax=ax)\n# Uncomment to see a hint\n#q_2.hint()\n\n\n# Get credit for your work after you have created a map\nq_2.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_2.solution()\n\n\n\n3) Where does each bird start and end its journey? (Part 1)\nNow, weâ€™re ready to look more closely at each birdâ€™s path. Run the next code cell to create two GeoDataFrames: - path_gdf contains LineString objects that show the path of each bird. It uses the LineString() method to create a LineString object from a list of Point objects. - path_gdfì—ëŠ” ê° ìƒˆì˜ ê²½ë¡œë¥¼ í‘œì‹œí•˜ëŠ” LineString ê°œì²´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. LineString() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ Point ê°œì²´ ëª©ë¡ì—ì„œ LineString ê°œì²´ë¥¼ ë§Œë“­ë‹ˆë‹¤. - start_gdf contains the starting points for each bird. - start_gdfëŠ” ê° ìƒˆì˜ ì‹œì‘ ì§€ì ì„ í¬í•¨í•©ë‹ˆë‹¤.\n\n# GeoDataFrame showing path for each bird\npath_df = birds.groupby(\"tag-local-identifier\")['geometry'].apply(list).apply(lambda x: LineString(x)).reset_index()\npath_gdf = gpd.GeoDataFrame(path_df, geometry=path_df.geometry)\npath_gdf.crs = {'init' :'epsg:4326'}\n\n# GeoDataFrame showing starting point for each bird\nstart_df = birds.groupby(\"tag-local-identifier\")['geometry'].apply(list).apply(lambda x: x[0]).reset_index()\nstart_gdf = gpd.GeoDataFrame(start_df, geometry=start_df.geometry)\nstart_gdf.crs = {'init' :'epsg:4326'}\n\n# Show first five rows of GeoDataFrame\nstart_gdf.head()\n\nUse the next code cell to create a GeoDataFrame end_gdf containing the final location of each bird.\n- The format should be identical to that of start_gdf, with two columns (â€œtag-local-identifierâ€ and â€œgeometryâ€), where the â€œgeometryâ€ column contains Point objects. - í˜•ì‹ì€ ë‘ ê°œì˜ ì—´(â€œtag-local-identifierâ€ ë° â€œgeometryâ€)ì´ ìˆëŠ” start_gdf'ì˜ í˜•ì‹ê³¼ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ \"geometry\" ì—´ì€ Point ê°œì²´ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. - Set the CRS ofend_gdfto{â€˜initâ€™: â€˜epsg:4326â€™}. -end_gdfì˜ CRSë¥¼{â€˜initâ€™: â€˜epsg:4326â€™}`ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.\n\n# Your code here\nend_df = birds.groupby(\"tag-local-identifier\")['geometry'].apply(list).apply(lambda x: x[-1]).reset_index()\nend_gdf = gpd.GeoDataFrame(end_df, geometry=end_df.geometry)\nend_gdf.crs = {'init' :'epsg:4326'}\n\n# Check your answer\nq_3.check()\n\n\n# Lines below will give you a hint or solution code\n#q_3.hint()\n#q_3.solution()\n\n\n\n4) Where does each bird start and end its journey? (Part 2)\nUse the GeoDataFrames from the question above (path_gdf, start_gdf, and end_gdf) to visualize the paths of all birds on a single map. You may also want to use the americas GeoDataFrame.\nìœ„ ì§ˆë¬¸ì˜ GeoDataFrames(path_gdf, start_gdf, end_gdf)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì¼ ì§€ë„ì—ì„œ ëª¨ë“  ìƒˆì˜ ê²½ë¡œë¥¼ ì‹œê°í™”í•˜ì„¸ìš”. americas GeoDataFrameì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n\n# Your code here\nax = americas.plot(figsize=(10,10), color='none', edgecolor='gainsboro', zorder=3)\n\n# Add wild lands, campsites, and foot trails to the base map\nstart_gdf.plot(color='lightgreen', ax=ax)\npath_gdf.plot(color='maroon', markersize=2, ax=ax)\nend_gdf.plot(color='black', markersize=1, ax=ax) \n\n# Uncomment to see a hint\n#q_4.hint()\n\n\n# Get credit for your work after you have created a map\nq_4.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_4.solution()\n\n\n\n5) Where are the protected areas in South America? (Part 1)\nIt looks like all of the birds end up somewhere in South America. But are they going to protected areas?\nIn the next code cell, youâ€™ll create a GeoDataFrame protected_areas containing the locations of all of the protected areas in South America. The corresponding shapefile is located at filepath protected_filepath.\në‹¤ìŒ ì½”ë“œ ì…€ì—ì„œëŠ” ë‚¨ë¯¸ì˜ ëª¨ë“  ë³´í˜¸ ì§€ì—­ ìœ„ì¹˜ë¥¼ í¬í•¨í•˜ëŠ” GeoDataFrame protected_areasë¥¼ ìƒì„±í•©ë‹ˆë‹¤. í•´ë‹¹ shapefileì€ íŒŒì¼ ê²½ë¡œ protected_filepathì— ìˆìŠµë‹ˆë‹¤.\n\n# Path of the shapefile to load\nprotected_filepath = \"../input/geospatial-learn-course-data/SAPA_Aug2019-shapefile/SAPA_Aug2019-shapefile/SAPA_Aug2019-shapefile-polygons.shp\"\n\n# Your code here\nprotected_areas = gpd.read_file(protected_filepath)\n\n# Check your answer\nq_5.check()\n\n\n# Lines below will give you a hint or solution code\n#q_5.hint()\n#q_5.solution()\n\n\n\n6) Where are the protected areas in South America? (Part 2)\nCreate a plot that uses the protected_areas GeoDataFrame to show the locations of the protected areas in South America. (Youâ€™ll notice that some protected areas are on land, while others are in marine waters.)\nâ€˜protected_areasâ€™ GeoDataFrameì„ ì‚¬ìš©í•˜ì—¬ ë‚¨ì•„ë©”ë¦¬ì¹´ì˜ ë³´í˜¸ ì§€ì—­ ìœ„ì¹˜ë¥¼ í‘œì‹œí•˜ëŠ” í”Œë¡¯ì„ ë§Œë“­ë‹ˆë‹¤. (ì¼ë¶€ ë³´í˜¸ êµ¬ì—­ì€ ìœ¡ì§€ì— ìˆê³  ë‹¤ë¥¸ ë³´í˜¸ êµ¬ì—­ì€ ë°”ë‹¤ì— ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)\n\n# Country boundaries in South America\nsouth_america = americas.loc[americas['continent']=='South America']\n\n# Your code here: plot protected areas in South America\nax = south_america.plot(figsize=(8,8), color='whitesmoke', edgecolor='black')\nprotected_areas.plot(markersize=1, ax=ax,alpha=0.4)\n\n# Uncomment to see a hint\n#q_6.hint()\n\n\n# Get credit for your work after you have created a map\nq_6.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_6.solution()\n\n\n\n7) What percentage of South America is protected?\nYouâ€™re interested in determining what percentage of South America is protected, so that you know how much of South America is suitable for the birds.\nAs a first step, you calculate the total area of all protected lands in South America (not including marine area). To do this, you use the â€œREP_AREAâ€ and â€œREP_M_AREAâ€ columns, which contain the total area and total marine area, respectively, in square kilometers.\nRun the code cell below without changes.\n\nP_Area = sum(protected_areas['REP_AREA']-protected_areas['REP_M_AREA'])\nprint(\"South America has {} square kilometers of protected areas.\".format(P_Area))\n\nThen, to finish the calculation, youâ€™ll use the south_america GeoDataFrame.\n\nsouth_america.head()\n\nCalculate the total area of South America by following these steps: - Calculate the area of each country using the area attribute of each polygon (with EPSG 3035 as the CRS), and add up the results. The calculated area will be in units of square meters. - ê° í´ë¦¬ê³¤(CRSë¡œ EPSG 3035 ì‚¬ìš©)ì˜ â€˜areaâ€™ ì†ì„±ì„ ì‚¬ìš©í•˜ì—¬ ê° êµ­ê°€ì˜ ë©´ì ì„ ê³„ì‚°í•˜ê³  ê²°ê³¼ë¥¼ í•©ì‚°í•©ë‹ˆë‹¤. ê³„ì‚°ëœ ë©´ì ì€ í‰ë°© ë¯¸í„° ë‹¨ìœ„ì…ë‹ˆë‹¤. - Convert your answer to have units of square kilometeters. - í‰ë°© í‚¬ë¡œë¯¸í„° ë‹¨ìœ„ê°€ ë˜ë„ë¡ ë‹µì„ ë³€í™˜í•˜ì‹­ì‹œì˜¤.\n\n# Your code here: Calculate the total area of South America (in square kilometers)\ntotalArea = sum(south_america.geometry.to_crs(epsg=3035).area)/10**6\ntotalArea \n# Check your answer\nq_7.check()\n\n\n# Lines below will give you a hint or solution code\n#q_7.hint()\n#q_7.solution()\n\nRun the code cell below to calculate the percentage of South America that is protected.\n\n# What percentage of South America is protected?\npercentage_protected = P_Area/totalArea\nprint('Approximately {}% of South America is protected.'.format(round(percentage_protected*100, 2)))\n\n\n\n8) Where are the birds in South America?\nSo, are the birds in protected areas?\nê·¸ë ‡ë‹¤ë©´ ìƒˆë“¤ì€ ë³´í˜¸ êµ¬ì—­ì— ìˆìŠµë‹ˆê¹Œ?\nCreate a plot that shows for all birds, all of the locations where they were discovered in South America. Also plot the locations of all protected areas in South America.\nëª¨ë“  ìƒˆ, ë‚¨ë¯¸ì—ì„œ ë°œê²¬ëœ ëª¨ë“  ìœ„ì¹˜ë¥¼ ë³´ì—¬ì£¼ëŠ” í”Œë¡¯ì„ ë§Œë“­ë‹ˆë‹¤. ë˜í•œ ë‚¨ì•„ë©”ë¦¬ì¹´ì˜ ëª¨ë“  ë³´í˜¸ ì§€ì—­ì˜ ìœ„ì¹˜ë¥¼ â€‹â€‹í‘œì‹œí•©ë‹ˆë‹¤.\nTo exclude protected areas that are purely marine areas (with no land component), you can use the â€œMARINEâ€ column (and plot only the rows in protected_areas[protected_areas['MARINE']!='2'], instead of every row in the protected_areas GeoDataFrame).\nìˆœìˆ˜í•œ í•´ì–‘ ì§€ì—­(í† ì§€ êµ¬ì„±ìš”ì†Œ ì—†ìŒ)ì¸ ë³´í˜¸ ì§€ì—­ì„ ì œì™¸í•˜ë ¤ë©´ â€œMARINEâ€ ì—´ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤(ê·¸ë¦¬ê³  protected_areas[protected_areas['MARINE']!='2'] protected_areas GeoDataFrameì˜ ëª¨ë“  í–‰).\n\n# Your code here\nax = south_america.plot(figsize=(8,8), color='whitesmoke', edgecolor='black')\nprotected_areas[protected_areas['MARINE']!='2'].plot(ax=ax, alpha=0.4, zorder=1)\nbirds[birds.geometry.y < 0].plot(ax=ax, color='red', alpha=0.6, markersize=10, zorder=2)\n\n# Uncomment to see a hint\n#q_8.hint()\n\n\n# Get credit for your work after you have created a map\nq_8.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_8.solution()\n\n\n\n\nKeep going\nCreate stunning interactive maps with your geospatial data.\n\nHave questions or comments? Visit the course discussion forum to chat with other learners."
  },
  {
    "objectID": "Data_Mining/2022-05-24-exercise-interactive-maps.html",
    "href": "Data_Mining/2022-05-24-exercise-interactive-maps.html",
    "title": "Interactive Maps",
    "section": "",
    "text": "This notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link.\n\n\nimport pandas as pd\nimport geopandas as gpd\n\nimport folium\nfrom folium import Choropleth\nfrom folium.plugins import HeatMap\n\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.geospatial.ex3 import *\n\nWe define a function embed_map() for displaying interactive maps. It accepts two arguments: the variable containing the map, and the name of the HTML file where the map will be saved.\nThis function ensures that the maps are visible in all web browsers.\n\ndef embed_map(m, file_name):\n    from IPython.display import IFrame\n    m.save(file_name)\n    return IFrame(file_name, width='100%', height='500px')\n\n\nExercises\n\n1) Do earthquakes coincide with plate boundaries?\nRun the code cell below to create a DataFrame plate_boundaries that shows global plate boundaries. The â€œcoordinatesâ€ column is a list of (latitude, longitude) locations along the boundaries.\nì•„ë˜ ì½”ë“œ ì…€ì„ ì‹¤í–‰í•˜ì—¬ ì „ì—­ í”Œë ˆì´íŠ¸ ê²½ê³„ë¥¼ í‘œì‹œí•˜ëŠ” DataFrame plate_boundariesë¥¼ ë§Œë“­ë‹ˆë‹¤. â€œì¢Œí‘œâ€ ì—´ì€ ê²½ê³„ë¥¼ ë”°ë¼ (ìœ„ë„, ê²½ë„) ìœ„ì¹˜ì˜ ëª©ë¡ì…ë‹ˆë‹¤.\n\nplate_boundaries = gpd.read_file(\"../input/geospatial-learn-course-data/Plate_Boundaries/Plate_Boundaries/Plate_Boundaries.shp\")\nplate_boundaries['coordinates'] = plate_boundaries.apply(lambda x: [(b,a) for (a,b) in list(x.geometry.coords)], axis='columns')\nplate_boundaries.drop('geometry', axis=1, inplace=True)\n\nplate_boundaries.head()\n\nNext, run the code cell below without changes to load the historical earthquake data into a DataFrame earthquakes.\n\n# Load the data and print the first 5 rows\nearthquakes = pd.read_csv(\"../input/geospatial-learn-course-data/earthquakes1970-2014.csv\", parse_dates=[\"DateTime\"])\nearthquakes.head()\n\nThe code cell below visualizes the plate boundaries on a map. Use all of the earthquake data to add a heatmap to the same map, to determine whether earthquakes coincide with plate boundaries.\nì•„ë˜ ì½”ë“œ ì…€ì€ ì§€ë„ì—ì„œ íŒ ê²½ê³„ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤. ëª¨ë“  ì§€ì§„ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì¼í•œ ì§€ë„ì— íˆíŠ¸ë§µì„ ì¶”ê°€í•˜ì—¬ ì§€ì§„ì´ íŒ ê²½ê³„ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n\n# Create a base map with plate boundaries\nm_1 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)\nfor i in range(len(plate_boundaries)):\n    folium.PolyLine(locations=plate_boundaries.coordinates.iloc[i], weight=2, color='black').add_to(m_1)\n\n# Your code here: Add a heatmap to the map\nHeatMap(data=earthquakes[['Latitude', 'Longitude']], radius=10).add_to(m_1)\n\n# Uncomment to see a hint\n#q_1.a.hint()\n\n# Show the map\nembed_map(m_1, 'q_1.html')\n\n\n# Get credit for your work after you have created a map\nq_1.a.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_1.a.solution()\n\nSo, given the map above, do earthquakes coincide with plate boundaries?\n\n# View the solution (Run this code cell to receive credit!)\nq_1.b.solution()\n\n\n\n2) Is there a relationship between earthquake depth and proximity to a plate boundary in Japan?\nYou recently read that the depth of earthquakes tells us important information about the structure of the earth. Youâ€™re interested to see if there are any intereresting global patterns, and youâ€™d also like to understand how depth varies in Japan.\nìµœê·¼ì— ì§€ì§„ì˜ ê¹Šì´ê°€ ì¤‘ìš” ì •ë³´ë¥¼ ì•Œë ¤ì¤€ë‹¤ëŠ” ë‚´ìš©ì„ ì½ì—ˆìŠµë‹ˆë‹¤. -news_science_products) ì§€êµ¬ì˜ êµ¬ì¡°ì— ëŒ€í•´. í¥ë¯¸ë¡œìš´ ê¸€ë¡œë²Œ íŒ¨í„´ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì‹¶ê³  ì¼ë³¸ì˜ ê¹Šì´ê°€ ì–´ë–»ê²Œ ë‹¤ë¥¸ì§€ ì´í•´í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤.\n\n# Create a base map with plate boundaries\nm_2 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)\nfor i in range(len(plate_boundaries)):\n    folium.PolyLine(locations=plate_boundaries.coordinates.iloc[i], weight=2, color='black').add_to(m_2)\n    \n# Your code here: Add a map to visualize earthquake depth\ndef color_producer(val):\n    if val < 50:\n        return 'forestgreen'\n    else:\n        return 'darkred'\n\n\nfor i in range(0,len(earthquakes)):\n    folium.Circle(\n        location=[earthquakes.iloc[i]['Latitude'], earthquakes.iloc[i]['Longitude']],\n        radius=2000,\n        color=color_producer(earthquakes.iloc[i]['Depth'])).add_to(m_2)\n    \n# Uncomment to see a hint\nq_2.a.hint()\n\n# View the map\nembed_map(m_2, 'q_2.html')\n\n\n# Get credit for your work after you have created a map\nq_2.a.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_2.a.solution()\n\nCan you detect a relationship between proximity to a plate boundary and earthquake depth? Does this pattern hold globally? In Japan?\n\n# View the solution (Run this code cell to receive credit!)\nq_2.b.solution()\n\n\n\n3) Which prefectures have high population density?\nRun the next code cell (without changes) to create a GeoDataFrame prefectures that contains the geographical boundaries of Japanese prefectures.\në‹¤ìŒ ì½”ë“œ ì…€ì„ ë³€ê²½ ì—†ì´ ì‹¤í–‰í•˜ì—¬ ì¼ë³¸ í˜„ì˜ ì§€ë¦¬ì  ê²½ê³„ë¥¼ í¬í•¨í•˜ëŠ” GeoDataFrame â€™í˜„â€™ì„ ë§Œë“­ë‹ˆë‹¤.\n\n# GeoDataFrame with prefecture boundaries\nprefectures = gpd.read_file(\"../input/geospatial-learn-course-data/japan-prefecture-boundaries/japan-prefecture-boundaries/japan-prefecture-boundaries.shp\")\nprefectures.set_index('prefecture', inplace=True)\nprefectures.head()\n\nThe next code cell creates a DataFrame stats containing the population, area (in square kilometers), and population density (per square kilometer) for each Japanese prefecture. Run the code cell without changes.\në‹¤ìŒ ì½”ë“œ ì…€ì€ ê° ì¼ë³¸ í˜„ì— ëŒ€í•œ ì¸êµ¬, ë©´ì (ì œê³± í‚¬ë¡œë¯¸í„° ë‹¨ìœ„) ë° ì¸êµ¬ ë°€ë„(ì œê³± í‚¬ë¡œë¯¸í„°ë‹¹)ë¥¼ í¬í•¨í•˜ëŠ” DataFrame â€™í†µê³„â€™ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ë³€ê²½ ì—†ì´ ì½”ë“œ ì…€ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\n\n# DataFrame containing population of each prefecture\npopulation = pd.read_csv(\"../input/geospatial-learn-course-data/japan-prefecture-population.csv\")\npopulation.set_index('prefecture', inplace=True)\n\n# Calculate area (in square kilometers) of each prefecture\narea_sqkm = pd.Series(prefectures.geometry.to_crs(epsg=32654).area / 10**6, name='area_sqkm')\nstats = population.join(area_sqkm)\n\n# Add density (per square kilometer) of each prefecture\nstats['density'] = stats[\"population\"] / stats[\"area_sqkm\"]\nstats.head()\n\nUse the next code cell to create a choropleth map to visualize population density.\në‹¤ìŒ ì½”ë“œ ì…€ì„ ì‚¬ìš©í•˜ì—¬ ë“±ì¹˜ ì§€ë„ë¥¼ ë§Œë“¤ì–´ ì¸êµ¬ ë°€ë„ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.\n\n# Create a base map\nm_3 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)\n\n# Your code here: create a choropleth map to visualize population density\nChoropleth(geo_data=prefectures['geometry'].__geo_interface__, \n           data=stats['density'], \n           key_on=\"feature.id\", \n           fill_color='YlGnBu', \n           legend_name='population density'\n          ).add_to(m_3)\n\n# Uncomment to see a hint\n#q_3.a.hint()\n\n# View the map\nembed_map(m_3, 'q_3.html')\n\n\n# Get credit for your work after you have created a map\nq_3.a.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_3.a.solution()\n\nWhich three prefectures have relatively higher density than the others? Are they spread throughout the country, or all located in roughly the same geographical region? (If youâ€™re unfamiliar with Japanese geography, you might find this map useful to answer the questions.)\n\n# View the solution (Run this code cell to receive credit!)\nq_3.b.solution()\n\n\n\n4) Which high-density prefecture is prone to high-magnitude earthquakes?\nCreate a map to suggest one prefecture that might benefit from earthquake reinforcement. Your map should visualize both density and earthquake magnitude.\nì§€ì§„ ë³´ê°•ì˜ í˜œíƒì„ ë°›ì„ ìˆ˜ ìˆëŠ” í•œ í˜„ì„ ì œì•ˆí•˜ëŠ” ì§€ë„ë¥¼ ë§Œë“­ë‹ˆë‹¤. ì§€ë„ëŠ” ë°€ë„ì™€ ì§€ì§„ ê·œëª¨ë¥¼ ëª¨ë‘ ì‹œê°í™”í•´ì•¼ í•©ë‹ˆë‹¤.\n\n# Create a base map\nm_4 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)\n\n# Your code here: create a map\ndef color_producer(magnitude):\n    if magnitude > 6.5:\n        return 'red'\n    else:\n        return 'green'\n\nChoropleth(\n    geo_data=prefectures['geometry'].__geo_interface__,\n    data=stats['density'],\n    key_on=\"feature.id\",\n    fill_color='BuPu',\n    legend_name='Population density (per square kilometer)').add_to(m_4)\n\nfor i in range(0,len(earthquakes)):\n    folium.Circle(\n        location=[earthquakes.iloc[i]['Latitude'], earthquakes.iloc[i]['Longitude']],\n        popup=(\"{} ({})\").format(\n            earthquakes.iloc[i]['Magnitude'],\n            earthquakes.iloc[i]['DateTime'].year),\n        radius=earthquakes.iloc[i]['Magnitude']**5.5,\n        color=color_producer(earthquakes.iloc[i]['Magnitude'])).add_to(m_4)\n# Uncomment to see a hint\n#q_4.a.hint()\n\n# View the map\nembed_map(m_4, 'q_4.html')\n\n\n# Get credit for your work after you have created a map\nq_4.a.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_4.a.solution()\n\nWhich prefecture do you recommend for extra earthquake reinforcement?\n\n# View the solution (Run this code cell to receive credit!)\nq_4.b.solution()\n\n\n\n\nKeep going\nLearn how to convert names of places to geographic coordinates with geocoding. Youâ€™ll also explore special ways to join information from multiple GeoDataFrames.\n\nHave questions or comments? Visit the course discussion forum to chat with other learners."
  },
  {
    "objectID": "Data_Mining/2022-05-24-exercise-your-first-map.html",
    "href": "Data_Mining/2022-05-24-exercise-your-first-map.html",
    "title": "Your First Map",
    "section": "",
    "text": "This notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link.\n\n\nimport geopandas as gpd\n\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.geospatial.ex1 import *\n\n\n1) Get the data.\nUse the next cell to load the shapefile located at loans_filepath to create a GeoDataFrame world_loans.\në‹¤ìŒ ì…€ì„ ì‚¬ìš©í•˜ì—¬ loans_filepathì— ìˆëŠ” shapefileì„ ë¡œë“œí•˜ì—¬ GeoDataFrame world_loansë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n\nloans_filepath = \"../input/geospatial-learn-course-data/kiva_loans/kiva_loans/kiva_loans.shp\"\n\n# Your code here: Load the data\nworld_loans = gpd.read_file(loans_filepath)\n\n# Check your answer\nq_1.check()\n\n# Uncomment to view the first five rows of the data\n#world_loans.head()\n\n\n\n2) Plot the data.\nRun the next code cell without changes to load a GeoDataFrame world containing country boundaries.\në³€ê²½ ì—†ì´ ë‹¤ìŒ ì½”ë“œ ì…€ì„ ì‹¤í–‰í•˜ì—¬ êµ­ê°€ ê²½ê³„ê°€ í¬í•¨ëœ GeoDataFrame â€™worldâ€™ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n\n# Lines below will give you a hint or solution code\n#q_1.hint()\n#q_1.solution()\n\n\n# This dataset is provided in GeoPandas\nworld_filepath = gpd.datasets.get_path('naturalearth_lowres')\nworld = gpd.read_file(world_filepath)\nworld.head()\n\nUse the world and world_loans GeoDataFrames to visualize Kiva loan locations across the world.\nworld ë° world_loans GeoDataFramesë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ ì„¸ê³„ì˜ Kiva loan locationsë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.\n\n# Your code here\nax = world.plot(figsize=(20,20), color='none', edgecolor='gainsboro',zorder=3)\nworld_loans.plot(color='skyblue', markersize=2,ax=ax)\n# Uncomment to see a hint\n#q_2.hint()\n\n\n# Get credit for your work after you have created a map\nq_2.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_2.solution()\n\n\n\n3) Select loans based in the Philippines.\nNext, youâ€™ll focus on loans that are based in the Philippines. Use the next code cell to create a GeoDataFrame PHL_loans which contains all rows from world_loans with loans that are based in the Philippines.\në‹¤ìŒìœ¼ë¡œ í•„ë¦¬í•€ì— ê¸°ë°˜ì„ ë‘” ëŒ€ì¶œì— ì¤‘ì ì„ ë‘˜ ê²ƒì…ë‹ˆë‹¤. ë‹¤ìŒ ì½”ë“œ ì…€ì„ ì‚¬ìš©í•˜ì—¬ í•„ë¦¬í•€ì— ê¸°ë°˜ì„ ë‘” ëŒ€ì¶œì´ ìˆëŠ” world_loansì˜ ëª¨ë“  í–‰ì„ í¬í•¨í•˜ëŠ” GeoDataFrame PHL_loansë¥¼ ë§Œë“­ë‹ˆë‹¤.\n\n# Your code here\nPHL_loans = world_loans.loc[world_loans.country==\"Philippines\"].copy()\n\n\n# Check your answer\nq_3.check()\n\n\n# Lines below will give you a hint or solution code\n#q_3.hint()\n#q_3.solution()\n\n\n\n4) Understand loans in the Philippines.\nRun the next code cell without changes to load a GeoDataFrame PHL containing boundaries for all islands in the Philippines.\ní•„ë¦¬í•€ì˜ ëª¨ë“  ì„¬ì— ëŒ€í•œ ê²½ê³„ë¥¼ í¬í•¨í•˜ëŠ” GeoDataFrame PHLì„ ë¡œë“œí•˜ë ¤ë©´ ë³€ê²½ ì—†ì´ ë‹¤ìŒ ì½”ë“œ ì…€ì„ ì‹¤í–‰í•˜ì‹­ì‹œì˜¤.\n\n# Load a KML file containing island boundaries\ngpd.io.file.fiona.drvsupport.supported_drivers['KML'] = 'rw'\nPHL = gpd.read_file(\"../input/geospatial-learn-course-data/Philippines_AL258.kml\", driver='KML')\nPHL.head()\n\nUse the PHL and PHL_loans GeoDataFrames to visualize loans in the Philippines.\nâ€˜PHLâ€™ ë° â€˜PHL_loansâ€™ GeoDataFramesë¥¼ ì‚¬ìš©í•˜ì—¬ í•„ë¦¬í•€ì˜ ëŒ€ì¶œì„ ì‹œê°í™”í•©ë‹ˆë‹¤.\n\n# Your code here\nax = PHL.plot(figsize=(20,20), color='none', edgecolor='gainsboro', zorder=3)\nPHL_loans.plot(color='skyblue', markersize=2, ax=ax)\n\n# Uncomment to see a hint\n#q_4.a.hint()\n\n\n# Get credit for your work after you have created a map\nq_4.a.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_4.a.solution()\n\nCan you identify any islands where it might be useful to recruit new Field Partners? Do any islands currently look outside of Kivaâ€™s reach?\nYou might find this map useful to answer the question.\n\n# View the solution (Run this code cell to receive credit!)\nq_4.b.solution()\n\n\n\nKeep going\nContinue to learn about coordinate reference systems.\n\nHave questions or comments? Visit the course discussion forum to chat with other learners."
  },
  {
    "objectID": "Data_Mining/2022-05-25-newyorktaxi.html",
    "href": "Data_Mining/2022-05-25-newyorktaxi.html",
    "title": "NYC taxi",
    "section": "",
    "text": "NYC taxi"
  },
  {
    "objectID": "Data_Mining/2022-05-25-newyorktaxi.html#visual-analytics-with-python",
    "href": "Data_Mining/2022-05-25-newyorktaxi.html#visual-analytics-with-python",
    "title": "NYC taxi",
    "section": "Visual Analytics with Python",
    "text": "Visual Analytics with Python\nê°•ì˜ìë£Œ ì¶œì²˜ - kaggle.com\nIn this script we will explore the spatial and temporal behavior of the people of New York as can be inferred by examining their cab usage.\nThe main fields of this dataset are taxi pickup time and location, as well as dropoff location and trip duration. There is a total of around 1.4 Million trips in the dataset that took place during the first half of 2016.\nWe will study how the patterns of cab usage change throughout the year, throughout the week and throughout the day, and we will focus on difference between weekdays and weekends.\n\n%matplotlib inline\n\nfrom sklearn import decomposition\nfrom scipy import stats\nfrom sklearn import cluster\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nmatplotlib.style.use('fivethirtyeight')\nmatplotlib.rcParams['font.size'] = 12\nmatplotlib.rcParams['figure.figsize'] = (10,10)\n\n\nLoad data and preprocess measurements to sensible units\n\ndata_frame = pd.read_csv('train.csv')\n\n\ndata_frame.describe()\n\n\n\n\n\n  \n    \n      \n      vendor_id\n      passenger_count\n      pickup_longitude\n      pickup_latitude\n      dropoff_longitude\n      dropoff_latitude\n      trip_duration\n    \n  \n  \n    \n      count\n      1.458644e+06\n      1.458644e+06\n      1.458644e+06\n      1.458644e+06\n      1.458644e+06\n      1.458644e+06\n      1.458644e+06\n    \n    \n      mean\n      1.534950e+00\n      1.664530e+00\n      -7.397349e+01\n      4.075092e+01\n      -7.397342e+01\n      4.075180e+01\n      9.594923e+02\n    \n    \n      std\n      4.987772e-01\n      1.314242e+00\n      7.090186e-02\n      3.288119e-02\n      7.064327e-02\n      3.589056e-02\n      5.237432e+03\n    \n    \n      min\n      1.000000e+00\n      0.000000e+00\n      -1.219333e+02\n      3.435970e+01\n      -1.219333e+02\n      3.218114e+01\n      1.000000e+00\n    \n    \n      25%\n      1.000000e+00\n      1.000000e+00\n      -7.399187e+01\n      4.073735e+01\n      -7.399133e+01\n      4.073588e+01\n      3.970000e+02\n    \n    \n      50%\n      2.000000e+00\n      1.000000e+00\n      -7.398174e+01\n      4.075410e+01\n      -7.397975e+01\n      4.075452e+01\n      6.620000e+02\n    \n    \n      75%\n      2.000000e+00\n      2.000000e+00\n      -7.396733e+01\n      4.076836e+01\n      -7.396301e+01\n      4.076981e+01\n      1.075000e+03\n    \n    \n      max\n      2.000000e+00\n      9.000000e+00\n      -6.133553e+01\n      5.188108e+01\n      -6.133553e+01\n      4.392103e+01\n      3.526282e+06\n    \n  \n\n\n\n\n\ndata_frame.shape\n\n(1458644, 11)\n\n\n\nnp.max(data_frame['trip_duration']) #ì´ë ‡ê²Œ ë³´ë©´ì„œ ì´ìƒì¹˜ê°€ ìˆëŠ” ê²ƒì„ ëˆˆìœ¼ë¡œ ë´„ 3526282(ì „ì²˜ë¦¬ì „) --> 4764(ì „ì²˜ë¦¬í›„)\n\n3526282\n\n\n\n# remove obvious outliers / ìœ„ê²½ë„ ì¢Œí‘œê°€ íŠ€ëŠ” ê²ƒì„ ì œê±° \nallLat  = np.array(list(data_frame['pickup_latitude'])  + list(data_frame['dropoff_latitude'])) #ë‘ê°œì˜ ì»¬ëŸ¼ì„ ë¦¬ìŠ¤íŠ¸ë¡œ í•©ì³ì¤Œ \nallLong = np.array(list(data_frame['pickup_longitude']) + list(data_frame['dropoff_longitude']))\n\nlongLimits = [np.percentile(allLong, 0.3), np.percentile(allLong, 99.7)]\nlatLimits  = [np.percentile(allLat , 0.3), np.percentile(allLat , 99.7)]\ndurLimits  = [np.percentile(data_frame['trip_duration'], 0.4), np.percentile(data_frame['trip_duration'], 99.7)]\n\ndata_frame = data_frame[(data_frame['pickup_latitude']   >= latLimits[0] ) & (data_frame['pickup_latitude']   <= latLimits[1]) ] #ì´ìƒì¹˜ ì œê±°\ndata_frame = data_frame[(data_frame['dropoff_latitude']  >= latLimits[0] ) & (data_frame['dropoff_latitude']  <= latLimits[1]) ]\ndata_frame = data_frame[(data_frame['pickup_longitude']  >= longLimits[0]) & (data_frame['pickup_longitude']  <= longLimits[1])]\ndata_frame = data_frame[(data_frame['dropoff_longitude'] >= longLimits[0]) & (data_frame['dropoff_longitude'] <= longLimits[1])]\ndata_frame = data_frame[(data_frame['trip_duration']     >= durLimits[0] ) & (data_frame['trip_duration']     <= durLimits[1]) ]\ndata_frame = data_frame.reset_index(drop=True)\n\n\n# convert fields to sensible units\nmedianLat  = np.percentile(allLat,50)\nmedianLong = np.percentile(allLong,50)\n\nlatMultiplier  = 111.32\nlongMultiplier = np.cos(medianLat*(np.pi/180.0)) * 111.32\n\ndata_frame['duration [min]'] = data_frame['trip_duration']/60.0\ndata_frame['src lat [km]']   = latMultiplier  * (data_frame['pickup_latitude']   - medianLat)\ndata_frame['src long [km]']  = longMultiplier * (data_frame['pickup_longitude']  - medianLong)\ndata_frame['dst lat [km]']   = latMultiplier  * (data_frame['dropoff_latitude']  - medianLat)\ndata_frame['dst long [km]']  = longMultiplier * (data_frame['dropoff_longitude'] - medianLong)\n\nallLat  = np.array(list(data_frame['src lat [km]'])  + list(data_frame['dst lat [km]']))\nallLong = np.array(list(data_frame['src long [km]']) + list(data_frame['dst long [km]']))\n\n\n\nPlot the histograms of trip duration, latitude and longitude\n\nfig, axArray = plt.subplots(nrows=1,ncols=3,figsize=(13,4))\naxArray[0].hist(data_frame['duration [min]'],80);\naxArray[0].set_xlabel('trip duration [min]'); axArray[0].set_ylabel('counts')\naxArray[1].hist(allLat ,80); axArray[1].set_xlabel('latitude [km]')\naxArray[2].hist(allLong,80); axArray[2].set_xlabel('longitude [km]')\n#ìœ„ê²½ë„ëŠ” í°ì˜ë¯¸ëŠ” ì—†ì§€ë§Œ í”Œëìœ¼ë¡œ ì´ìƒì¹˜ê°€ ìˆëŠ”ì§€ ì‹œê°ì ìœ¼ë¡œ í™•ì¸ \n\nText(0.5, 0, 'longitude [km]')\n\n\n\n\n\n\n\nPlot the trip Duration vs.Â the Aerial Distance between pickup and dropoff\n\ndata_frame['log duration']       = np.log1p(data_frame['duration [min]'])\ndata_frame['euclidian distance'] = np.sqrt((data_frame['src lat [km]']  - data_frame['dst lat [km]'] )**2 +\n                                       (data_frame['src long [km]'] - data_frame['dst long [km]'])**2)\n\nfig, axArray = plt.subplots(nrows=1,ncols=2,figsize=(13,6))\naxArray[0].scatter(data_frame['euclidian distance'], data_frame['duration [min]'],c='b',s=5,alpha=0.01);\naxArray[0].set_xlabel('Aerial Euclidian Distance [km]'); axArray[0].set_ylabel('Duration [min]')\naxArray[0].set_xlim(data_frame['euclidian distance'].min(),data_frame['euclidian distance'].max())\naxArray[0].set_ylim(data_frame['duration [min]'].min(),data_frame['duration [min]'].max())\naxArray[0].set_title('trip Duration vs Aerial trip Distance')\n\naxArray[1].scatter(data_frame['euclidian distance'], data_frame['log duration'],c='b',s=5,alpha=0.01);\naxArray[1].set_xlabel('Aerial Euclidian Distance [km]'); axArray[1].set_ylabel('log(1+Duration) [log(min)]')\naxArray[1].set_xlim(data_frame['euclidian distance'].min(),data_frame['euclidian distance'].max())\naxArray[1].set_ylim(data_frame['log duration'].min(),data_frame['log duration'].max())\naxArray[1].set_title('log of trip Duration vs Aerial trip Distance')\n\nText(0.5, 1.0, 'log of trip Duration vs Aerial trip Distance')\n\n\n\n\n\n\n\nExercise 1\nìœ„ì˜ Scatter plotì€ Pointê°€ ë„ˆë¬´ ë§ì´ ì¡´ì¬í•˜ì—¬ ì¢‹ì€ ì‹œê°í™”ê°€ ì•„ë‹ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë³´ë‹¤ íš¨ìœ¨ì ì¸ ì‹œê°í™” ë°©ì•ˆì„ ì œì‹œí•´ ë³´ì„¸ìš”.\nmatplotlib, seaborn, plotly ë“± ë‹¤ì–‘í•œ íŒ¨í‚¤ì§€ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n\nimport seaborn as sns\n\nsns.jointplot(x=data_frame['euclidian distance'], \n              y=data_frame['duration [min]'], \n              kind=\"hex\", \n              color=\"#4CB391\",\n              xlim = (0,30),\n              ylim = (0,70))\n\n<seaborn.axisgrid.JointGrid at 0x1d6899db160>\n\n\n\n\n\n\np = sns.jointplot(x=np.log1p(data_frame['euclidian distance']),\n              y=data_frame['log duration'], \n              kind=\"hex\", \n              color=\"#4CB391\") #logë¥¼ ì·¨í•´ ì£¼ì–´ì„œ íˆíŠ¸ë§µì„ ë³€í™˜ì‹œì¼œ ë³´ì—¬ì¤Œ \n\np.ax_joint.set_xlabel('log(1+Aerial Euclidian Distance) [log(km)]')\np.ax_joint.set_ylabel('log(1+Duration) [log(min)]')\n\nText(8.060000000000002, 0.5, 'log(1+Duration) [log(min)]')\n\n\n\n\n\nWe can see that the trip distance defines the lower bound on trip duration, as one would expect.\n\n\nPlot spatial density plot of the pickup and dropoff locations\n\nimageSize = (700,700)\nlongRange = [-5,19]\nlatRange = [-13,11]\n\nallLatInds  = imageSize[0] - (imageSize[0] * (allLat  - latRange[0])  / (latRange[1]  - latRange[0]) ).astype(int)\nallLongInds =                (imageSize[1] * (allLong - longRange[0]) / (longRange[1] - longRange[0])).astype(int)\n\nlocationDensityImage = np.zeros(imageSize)\nfor latInd, longInd in zip(allLatInds,allLongInds):\n    locationDensityImage[latInd,longInd] += 1\n\nfig, ax = plt.subplots(nrows=1,ncols=1,figsize=(12,12))\nax.imshow(np.log(locationDensityImage+1),cmap='inferno')\nax.set_axis_off()\n\n\n\n\n\n\nExercise 2\nfoliumì´ë‚˜ pydeckì„ ì‚¬ìš©í•˜ì—¬ íƒì‹œì˜ ìŠ¹ì°¨ì§€ì , í•˜ì°¨ì§€ì ì„ ì‹œê°í™” í•´ë´…ì‹œë‹¤. ë””ì„œ ìŠ¹ê°ì´ ë§ì´ íƒ€ê³  ë‚´ë¦¬ëŠ”ì§€ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆê¹Œ?\n\ndata_frame.head()\n\n\n\n\n\n  \n    \n      \n      id\n      vendor_id\n      pickup_datetime\n      dropoff_datetime\n      passenger_count\n      pickup_longitude\n      pickup_latitude\n      dropoff_longitude\n      dropoff_latitude\n      store_and_fwd_flag\n      trip_duration\n      duration [min]\n      src lat [km]\n      src long [km]\n      dst lat [km]\n      dst long [km]\n      log duration\n      euclidian distance\n    \n  \n  \n    \n      0\n      id2875421\n      2\n      2016-03-14 17:24:55\n      2016-03-14 17:32:30\n      1\n      -73.982155\n      40.767937\n      -73.964630\n      40.765602\n      N\n      455\n      7.583333\n      1.516008\n      -0.110015\n      1.256121\n      1.367786\n      2.149822\n      1.500479\n    \n    \n      1\n      id2377394\n      1\n      2016-06-12 00:43:35\n      2016-06-12 00:54:38\n      1\n      -73.980415\n      40.738564\n      -73.999481\n      40.731152\n      N\n      663\n      11.050000\n      -1.753813\n      0.036672\n      -2.578912\n      -1.571088\n      2.489065\n      1.807119\n    \n    \n      2\n      id3858529\n      2\n      2016-01-19 11:35:24\n      2016-01-19 12:10:48\n      1\n      -73.979027\n      40.763939\n      -74.005333\n      40.710087\n      N\n      2124\n      35.400000\n      1.070973\n      0.153763\n      -4.923841\n      -2.064547\n      3.594569\n      6.392080\n    \n    \n      3\n      id3504673\n      2\n      2016-04-06 19:32:31\n      2016-04-06 19:39:40\n      1\n      -74.010040\n      40.719971\n      -74.012268\n      40.706718\n      N\n      429\n      7.150000\n      -3.823568\n      -2.461500\n      -5.298809\n      -2.649362\n      2.098018\n      1.487155\n    \n    \n      4\n      id2181028\n      2\n      2016-03-26 13:30:55\n      2016-03-26 13:38:10\n      1\n      -73.973053\n      40.793209\n      -73.972923\n      40.782520\n      N\n      435\n      7.250000\n      4.329328\n      0.657515\n      3.139453\n      0.668452\n      2.110213\n      1.189925\n    \n  \n\n\n\n\n\ndata_frame.columns\n\nIndex(['id', 'vendor_id', 'pickup_datetime', 'dropoff_datetime',\n       'passenger_count', 'pickup_longitude', 'pickup_latitude',\n       'dropoff_longitude', 'dropoff_latitude', 'store_and_fwd_flag',\n       'trip_duration', 'duration [min]', 'src lat [km]', 'src long [km]',\n       'dst lat [km]', 'dst long [km]', 'log duration', 'euclidian distance'],\n      dtype='object')\n\n\n\nimport pydeck as pdk\n\n\n# Define a layer to display on a map\nlayer = pdk.Layer(\n    \"HexagonLayer\",\n    data_frame[:1000],\n    get_position=[\"pickup_longitude\", \"pickup_latitude\"],\n    auto_highlight=True,\n    elevation_scale=50,\n    pickable=True,\n    elevation_range=[0, 50],\n    extruded=True,\n    coverage=1,\n)\n\n# Set the viewport location\nview_lon = np.mean(data_frame[\"pickup_longitude\"])\nview_lat = np.mean(data_frame[\"pickup_latitude\"])\n\nview_state = pdk.ViewState(\n    longitude=view_lon, latitude=view_lat, zoom=10, min_zoom=5, max_zoom=15, pitch=40.5, bearing=-27.36,\n)\n\n# Render\nr = pdk.Deck(layers=[layer], initial_view_state=view_state)\nr.show()\n#r.to_html(\"hexagon_ny_taxi.html\")\n\n\n\n\n\nlayer = pdk.Layer(\n    \"HeatmapLayer\",\n    data_frame[:1000],\n    get_position=[\"pickup_longitude\", \"pickup_latitude\"],\n    auto_highlight=True,\n    elevation_scale=50,\n    pickable=True,\n    elevation_range=[0, 50],\n    extruded=True,\n    coverage=1,\n)\n\n# Set the viewport location\nview_lon = np.mean(data_frame[\"pickup_longitude\"])\nview_lat = np.mean(data_frame[\"pickup_latitude\"])\n\nview_state = pdk.ViewState(\n    longitude=view_lon, latitude=view_lat, zoom=10, min_zoom=5, max_zoom=15, pitch=40.5, bearing=-27.36,\n)\n\n# Render\nr = pdk.Deck(layers=[layer], initial_view_state=view_state)\nr.show()\n\n\n\n\n\n#folium ì„ ì´ìš©í•´ì„œë„ ì˜ˆì œë¡œ í•´ë³´ê¸° \n\n\n\nClosing in on Manhattan\n\nimageSizeMan = (720,480)\nlatRangeMan = [-8,10]\nlongRangeMan = [-5,7]\n\nindToKeep  = np.logical_and(allLat > latRangeMan[0], allLat < latRangeMan[1])\nindToKeep  = np.logical_and(indToKeep, np.logical_and(allLong > longRangeMan[0], allLong < longRangeMan[1]))\nallLatMan  = allLat[indToKeep]\nallLongMan = allLong[indToKeep]\n\nallLatIndsMan  = (imageSizeMan[0]-1) - (imageSizeMan[0] * (allLatMan  - latRangeMan[0])\n                                                        / (latRangeMan[1] - latRangeMan[0])).astype(int)\nallLongIndsMan =                       (imageSizeMan[1] * (allLongMan - longRangeMan[0])\n                                                        / (longRangeMan[1] - longRangeMan[0])).astype(int)\n\nlocationDensityImageMan = np.zeros(imageSizeMan)\nfor latInd, longInd in zip(allLatIndsMan,allLongIndsMan):\n    locationDensityImageMan[latInd,longInd] += 1\n\nfig, ax = plt.subplots(nrows=1,ncols=1,figsize=(12,18))\nax.imshow(np.log(locationDensityImageMan+1),cmap='inferno')\nax.set_axis_off()\n\n\n\n\n\n\nCluster the Trips and Look at their distribution\n\npickupTime = pd.to_datetime(data_frame['pickup_datetime'])\n\ndata_frame['src hourOfDay'] = (pickupTime.dt.hour*60.0 + pickupTime.dt.minute)   / 60.0\ndata_frame['dst hourOfDay'] = data_frame['src hourOfDay'] + data_frame['duration [min]'] / 60.0\n\ndata_frame['dayOfWeek']     = pickupTime.dt.weekday\ndata_frame['hourOfWeek']    = data_frame['dayOfWeek']*24.0 + data_frame['src hourOfDay']\n\ndata_frame['monthOfYear']   = pickupTime.dt.month\ndata_frame['dayOfYear']     = pickupTime.dt.dayofyear\ndata_frame['weekOfYear']    = pickupTime.dt.weekofyear\ndata_frame['hourOfYear']    = data_frame['dayOfYear']*24.0 + data_frame['src hourOfDay']\n\nFutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n  data_frame['weekOfYear']    = pickupTime.dt.weekofyear\n\n\n\ntripAttributes = np.array(data_frame.loc[:,['src lat [km]','src long [km]','dst lat [km]','dst long [km]','duration [min]']])\nmeanTripAttr = tripAttributes.mean(axis=0)\nstdTripAttr  = tripAttributes.std(axis=0)\ntripAttributes = stats.zscore(tripAttributes, axis=0)\n\nnumClusters = 40\nTripKmeansModel = cluster.MiniBatchKMeans(n_clusters=numClusters, batch_size=120000, n_init=100, random_state=1)\nclusterInds = TripKmeansModel.fit_predict(tripAttributes)\n\nclusterTotalCounts, _ = np.histogram(clusterInds, bins=numClusters)\nsortedClusterInds = np.flipud(np.argsort(clusterTotalCounts))\n\nplt.figure(figsize=(12,4)); plt.title('Cluster Histogram of all trip')\nplt.bar(range(1,numClusters+1),clusterTotalCounts[sortedClusterInds])\nplt.ylabel('Frequency [counts]'); plt.xlabel('Cluster index (sorted by cluster frequency)')\nplt.xlim(0,numClusters+1)\n\n(0.0, 41.0)\n\n\n\n\n\n\n\nPlot typical Trips on the Map\n\ndef ConvertToImageCoords(latCoord, longCoord, latRange, longRange, imageSize):\n    latInds  = imageSize[0] - (imageSize[0] * (latCoord  - latRange[0])  / (latRange[1]  - latRange[0]) ).astype(int)\n    longInds =                (imageSize[1] * (longCoord - longRange[0]) / (longRange[1] - longRange[0])).astype(int)\n\n    return latInds, longInds\n\ntemplateTrips = TripKmeansModel.cluster_centers_ * np.tile(stdTripAttr,(numClusters,1)) + np.tile(meanTripAttr,(numClusters,1))\n\nsrcCoords = templateTrips[:,:2]\ndstCoords = templateTrips[:,2:4]\n\nsrcImCoords = ConvertToImageCoords(srcCoords[:,0],srcCoords[:,1], latRange, longRange, imageSize)\ndstImCoords = ConvertToImageCoords(dstCoords[:,0],dstCoords[:,1], latRange, longRange, imageSize)\n\nplt.figure(figsize=(12,12))\nplt.imshow(np.log(locationDensityImage+1),cmap='inferno'); plt.grid('off')\nplt.scatter(srcImCoords[1],srcImCoords[0],c='m',s=200,alpha=0.8)\nplt.scatter(dstImCoords[1],dstImCoords[0],c='g',s=200,alpha=0.8)\n\nfor i in range(len(srcImCoords[0])):\n    plt.arrow(srcImCoords[1][i],srcImCoords[0][i], dstImCoords[1][i]-srcImCoords[1][i], dstImCoords[0][i]-srcImCoords[0][i],\n              edgecolor='c', facecolor='c', width=0.8,alpha=0.4,head_width=10.0,head_length=10.0,length\n\nSyntaxError: unexpected EOF while parsing (<ipython-input-22-39197d099a2f>, line 22)\n\n\n\n\nCalculate the trip distribution for different hours of the weekday\n\nhoursOfDay = np.sort(data_frame['src hourOfDay'].astype(int).unique())\nclusterDistributionHourOfDay_weekday = np.zeros((len(hoursOfDay),numClusters))\nfor k, hour in enumerate(hoursOfDay):\n    slectedInds = (data_frame['src hourOfDay'].astype(int) == hour) & (data_frame['dayOfWeek'] <= 4)\n    currDistribution, _ = np.histogram(clusterInds[slectedInds], bins=numClusters)\n    clusterDistributionHourOfDay_weekday[k,:] = currDistribution[sortedClusterInds]\n\nfig, ax = plt.subplots(nrows=1,ncols=1,figsize=(12,6))\nax.set_title('Trip Distribution during Weekdays', fontsize=12)\nax.imshow(clusterDistributionHourOfDay_weekday); ax.grid('off')\nax.set_xlabel('Trip Cluster'); ax.set_ylabel('Hour of Day')\n\n\n\nCalculate the trip distribution for different hours of the weekend\n\nhoursOfDay = np.sort(data_frame['src hourOfDay'].astype(int).unique())\nclusterDistributionHourOfDay_weekend = np.zeros((len(hoursOfDay),numClusters))\nfor k, hour in enumerate(hoursOfDay):\n    slectedInds = (data_frame['src hourOfDay'].astype(int) == hour) & (data_frame['dayOfWeek'] >= 5)\n    currDistribution, _ = np.histogram(clusterInds[slectedInds], bins=numClusters)\n    clusterDistributionHourOfDay_weekend[k,:] = currDistribution[sortedClusterInds]\n\nfig, ax = plt.subplots(nrows=1,ncols=1,figsize=(12,6))\nax.set_title('Trip Distribution during Weekends', fontsize=12)\nax.imshow(clusterDistributionHourOfDay_weekend); ax.grid('off')\nax.set_xlabel('Trip Cluster'); ax.set_ylabel('Hour of Day')\n\n\n\nCalculate the trip distribution for day of week\n\ndaysOfWeek = np.sort(data_frame['dayOfWeek'].unique())\nclusterDistributionDayOfWeek = np.zeros((len(daysOfWeek),numClusters))\nfor k, day in enumerate(daysOfWeek):\n    slectedInds = data_frame['dayOfWeek'] == day\n    currDistribution, _ = np.histogram(clusterInds[slectedInds], bins=numClusters)\n    clusterDistributionDayOfWeek[k,:] = currDistribution[sortedClusterInds]\n\nplt.figure(figsize=(12,5)); plt.title('Trip Distribution throughout the Week')\nplt.imshow(clusterDistributionDayOfWeek); plt.grid('off')\nplt.xlabel('Trip Cluster'); plt.ylabel('Day of Week')\n\n\n\nCalculate the trip distribution for day of year\n\ndaysOfYear = data_frame['dayOfYear'].unique()\ndaysOfYear = np.sort(daysOfYear)\nclusterDistributionDayOfYear = np.zeros((len(daysOfYear),numClusters))\nfor k, day in enumerate(daysOfYear):\n    slectedInds = data_frame['dayOfYear'] == day\n    currDistribution, _ = np.histogram(clusterInds[slectedInds], bins=numClusters)\n    clusterDistributionDayOfYear[k,:] = currDistribution[sortedClusterInds]\n\nfig, ax = plt.subplots(nrows=1,ncols=1,figsize=(10,16))\nax.set_title('Trip Distribution throughout the Year', fontsize=12)\nax.imshow(clusterDistributionDayOfYear); ax.grid('off')\nax.set_xlabel('Trip Cluster'); ax.set_ylabel('Day of Year')\nax.annotate('Large Snowstorm', color='r', fontsize=15 ,xy=(5, 21), xytext=(20, 17),\n            arrowprops=dict(facecolor='red', shrink=0.03))\nax.annotate('Memorial Day', color='r', fontsize=15, xy=(5, 151), xytext=(20, 157),\n            arrowprops=dict(facecolor='red', shrink=0.03))\n\n\n\nComputing PCA coefficients\n\nhoursOfYear = np.sort(data_frame['hourOfYear'].astype(int).unique())\nclusterDistributionHourOfYear = np.zeros((len(range(hoursOfYear[0],hoursOfYear[-1])),numClusters))\ndayOfYearVec  = np.zeros(clusterDistributionHourOfYear.shape[0])\nweekdayVec    = np.zeros(clusterDistributionHourOfYear.shape[0])\nweekOfYearVec = np.zeros(clusterDistributionHourOfYear.shape[0])\nfor k, hour in enumerate(hoursOfYear):\n    slectedInds = data_frame['hourOfYear'].astype(int) == hour\n    currDistribution, _ = np.histogram(clusterInds[slectedInds], bins=numClusters)\n    clusterDistributionHourOfYear[k,:] = currDistribution[sortedClusterInds]\n\n    dayOfYearVec[k]  = data_frame[slectedInds]['dayOfYear'].mean()\n    weekdayVec[k]    = data_frame[slectedInds]['dayOfWeek'].mean()\n    weekOfYearVec[k] = data_frame[slectedInds]['weekOfYear'].mean()\n\nnumComponents = 3\nTripDistributionPCAModel = decomposition.PCA(n_components=numComponents,whiten=True, random_state=1)\ncompactClusterDistributionHourOfYear = TripDistributionPCAModel.fit_transform(clusterDistributionHourOfYear)\n\n\n\nCollect traces for all weeks of year\n\nlistOfFullWeeks = []\nfor uniqueVal in np.unique(weekOfYearVec):\n    if (weekOfYearVec == uniqueVal).sum() == 24*7:\n        listOfFullWeeks.append(uniqueVal)\n\nweeklyTraces = np.zeros((24*7,numComponents,len(listOfFullWeeks)))\nfor k, weekInd in enumerate(listOfFullWeeks):\n    weeklyTraces[:,:,k] = compactClusterDistributionHourOfYear[weekOfYearVec == weekInd,:]\n\nfig, axArray = plt.subplots(nrows=numComponents,ncols=1,sharex=True, figsize=(10,10))\nfig.suptitle('PCA coefficients during the Week', fontsize=25)\nfor PC_coeff in range(numComponents):\n    meanTrace = weeklyTraces[:,PC_coeff,:].mean(axis=1)\n    axArray[PC_coeff].plot(weeklyTraces[:,PC_coeff,:],'red',linewidth=1.5)\n    axArray[PC_coeff].plot(meanTrace,'k',linewidth=2.5)\n    axArray[PC_coeff].set_ylabel('PC %d coeff' %(PC_coeff+1))\n    axArray[PC_coeff].vlines([0,23,47,71,95,119,143,167], weeklyTraces[:,PC_coeff,:].min(), weeklyTraces[:,PC_coeff,:].max(), colors='black', lw=2)\n\naxArray[PC_coeff].set_xlabel('hours since start of week')\naxArray[PC_coeff].set_xlim(-0.9,24*7-0.1)\n\n\n\nExamine what different PC coefficients mean by looking at their trip template distributions\n\nfig, axArray = plt.subplots(nrows=numComponents,ncols=1,sharex=True, figsize=(12,11))\nfig.suptitle('Trip Distribution PCA Components', fontsize=25)\nfor PC_coeff in range(numComponents):\n    tripTemplateDistributionDifference = TripDistributionPCAModel.components_[PC_coeff,:] * \\\n                                         TripDistributionPCAModel.explained_variance_[PC_coeff]\n    axArray[PC_coeff].bar(range(1,numClusters+1),tripTemplateDistributionDifference)\n    axArray[PC_coeff].set_title('PCA %d component' %(PC_coeff+1))\n    axArray[PC_coeff].set_ylabel('delta frequency [counts]')\n    \naxArray[PC_coeff].set_xlabel('cluster index (sorted by cluster frequency)')\naxArray[PC_coeff].set_xlim(0,numClusters+0.5)\n\naxArray[1].hlines([-25,25], 0, numClusters+0.5, colors='r', lw=0.7)\naxArray[2].hlines([-11,11], 0, numClusters+0.5, colors='r', lw=0.7)\n\nWe can see that the first PCA component looks very similar to the overall trip distribution, suggesting that itâ€™s mainly a â€œgainâ€ component that controls just the number of total trips in that period of time."
  },
  {
    "objectID": "Data_Mining/2022-05-26-exercise-manipulating-geospatial-data.html",
    "href": "Data_Mining/2022-05-26-exercise-manipulating-geospatial-data.html",
    "title": "Manipulating Geospatial Data",
    "section": "",
    "text": "This notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link.\n\n\nimport math\nimport pandas as pd\nimport geopandas as gpd\n#from geopy.geocoders import Nominatim            # What you'd normally run\nfrom learntools.geospatial.tools import Nominatim # Just for this exercise\n\nimport folium \nfrom folium import Marker\nfrom folium.plugins import MarkerCluster\n\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.geospatial.ex4 import *\n\nYouâ€™ll use the embed_map() function from the previous exercise to visualize your maps.\n\ndef embed_map(m, file_name):\n    from IPython.display import IFrame\n    m.save(file_name)\n    return IFrame(file_name, width='100%', height='500px')\n\n\nExercises\n\n1) Geocode the missing locations.\nRun the next code cell to create a DataFrame starbucks containing Starbucks locations in the state of California.\n\n# Load and preview Starbucks locations in California\nstarbucks = pd.read_csv(\"../input/geospatial-learn-course-data/starbucks_locations.csv\")\nstarbucks.head()\n\nMost of the stores have known (latitude, longitude) locations. But, all of the locations in the city of Berkeley are missing.\n\n# How many rows in each column have missing values?\nprint(starbucks.isnull().sum())\n\n# View rows with missing locations\nrows_with_missing = starbucks[starbucks[\"City\"]==\"Berkeley\"]\nrows_with_missing\n\nUse the code cell below to fill in these values with the Nominatim geocoder.\nNote that in the tutorial, we used Nominatim() (from geopy.geocoders) to geocode values, and this is what you can use in your own projects outside of this course.\níŠœí† ë¦¬ì–¼ì—ì„œ ìš°ë¦¬ëŠ” ê°’ì„ ì§€ì˜¤ì½”ë”©í•˜ê¸° ìœ„í•´ Nominatim()(geopy.geocodersì—ì„œ)ì„ ì‚¬ìš©í–ˆìœ¼ë©° ì´ê²ƒì€ ì´ ê³¼ì • ì´ì™¸ì˜ ìì‹ ì˜ í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê²ƒì…ë‹ˆë‹¤.\nIn this exercise, you will use a slightly different function Nominatim() (from learntools.geospatial.tools). This function was imported at the top of the notebook and works identically to the function from GeoPandas.\nì´ ì—°ìŠµì—ì„œëŠ” ì•½ê°„ ë‹¤ë¥¸ í•¨ìˆ˜ Nominatim()(learntools.geospatial.toolsì—ì„œ)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ê¸°ëŠ¥ì€ ë…¸íŠ¸ë¶ ìƒë‹¨ì—ì„œ ê°€ì ¸ì˜¨ ê²ƒìœ¼ë¡œ GeoPandasì˜ ê¸°ëŠ¥ê³¼ ë™ì¼í•˜ê²Œ ì‘ë™í•©ë‹ˆë‹¤.\nSo, in other words, as long as: - you donâ€™t change the import statements at the top of the notebook, and - you call the geocoding function as geocode() in the code cell below,\nyour code will work as intended!\n\n# Create the geocoder\ngeolocator = Nominatim(user_agent=\"kaggle_learn\")\n\n# Your code here\n\ndef my_geocoder(row):\n    point = geolocator.geocode(row).point\n    return pd.Series({'Latitude': point.latitude, 'Longitude': point.longitude})\n\nberkeley_locations = rows_with_missing.apply(lambda x: my_geocoder(x['Address']), axis=1)\nstarbucks.update(berkeley_locations)\n\n# Check your answer\nq_1.check()\n\n\n# Line below will give you solution code\n#q_1.solution()\n\n\n\n2) View Berkeley locations.\nLetâ€™s take a look at the locations you just found. Visualize the (latitude, longitude) locations in Berkeley in the OpenStreetMap style.\në°©ê¸ˆ ì°¾ì€ ìœ„ì¹˜ë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. OpenStreetMap ìŠ¤íƒ€ì¼ë¡œ ë²„í´ë¦¬ì˜ (ìœ„ë„, ê²½ë„) ìœ„ì¹˜ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.\n\n# Create a base map\nm_2 = folium.Map(location=[37.88,-122.26], zoom_start=13)\n\n# Your code here: Add a marker for each Berkeley location\nfor idx, row in starbucks[starbucks[\"City\"]=='Berkeley'].iterrows():\n    Marker([row['Latitude'], row['Longitude']]).add_to(m_2)\n\n# Uncomment to see a hint\n#q_2.a.hint()\n\n# Show the map\nembed_map(m_2, 'q_2.html')\n\n\n# Get credit for your work after you have created a map\nq_2.a.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_2.a.solution()\n\nConsidering only the five locations in Berkeley, how many of the (latitude, longitude) locations seem potentially correct (are located in the correct city)?\n\n# View the solution (Run this code cell to receive credit!)\nq_2.b.solution()\n\n\n\n3) Consolidate your data.\nRun the code below to load a GeoDataFrame CA_counties containing the name, area (in square kilometers), and a unique id (in the â€œGEOIDâ€ column) for each county in the state of California. The â€œgeometryâ€ column contains a polygon with county boundaries.\n\nCA_counties = gpd.read_file(\"../input/geospatial-learn-course-data/CA_county_boundaries/CA_county_boundaries/CA_county_boundaries.shp\")\nCA_counties.head()\n\nNext, we create three DataFrames: - CA_pop contains an estimate of the population of each county. - CA_high_earners contains the number of households with an income of at least $150,000 per year. - CA_median_age contains the median age for each county.\n\nCA_pop = pd.read_csv(\"../input/geospatial-learn-course-data/CA_county_population.csv\", index_col=\"GEOID\")\nCA_high_earners = pd.read_csv(\"../input/geospatial-learn-course-data/CA_county_high_earners.csv\", index_col=\"GEOID\")\nCA_median_age = pd.read_csv(\"../input/geospatial-learn-course-data/CA_county_median_age.csv\", index_col=\"GEOID\")\n\nUse the next code cell to join the CA_counties GeoDataFrame with CA_pop, CA_high_earners, and CA_median_age.\nName the resultant GeoDataFrame CA_stats, and make sure it has 8 columns: â€œGEOIDâ€, â€œnameâ€, â€œarea_sqkmâ€, â€œgeometryâ€, â€œpopulationâ€, â€œhigh_earnersâ€, and â€œmedian_ageâ€. Also, make sure the CRS is set to {'init': 'epsg:4326'}.\nê²°ê³¼ GeoDataFrameì˜ ì´ë¦„ì„ CA_statsë¡œ ì§€ì •í•˜ê³  â€œGEOIDâ€, â€œnameâ€, â€œarea_sqkmâ€, â€œgeometryâ€, â€œpopulationâ€, â€œhigh_earnersâ€ ë° â€œmedian_ageâ€ì˜ 8ê°œ ì—´ì´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤. ë˜í•œ CRSê°€ {'init': 'epsg:4326'}ìœ¼ë¡œ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì‹­ì‹œì˜¤.\n\n# Your code here\ncols_to_add = CA_pop.join([CA_high_earners, CA_median_age]).reset_index()\nCA_stats = CA_counties.merge(cols_to_add, on=\"GEOID\")\n\n# Check your answer\nq_3.check()\n\n\n# Lines below will give you a hint or solution code\n#q_3.hint()\n#q_3.solution()\n\nNow that we have all of the data in one place, itâ€™s much easier to calculate statistics that use a combination of columns. Run the next code cell to create a â€œdensityâ€ column with the population density.\n\nCA_stats[\"density\"] = CA_stats[\"population\"] / CA_stats[\"area_sqkm\"]\n\n\n\n4) Which counties look promising?\nCollapsing all of the information into a single GeoDataFrame also makes it much easier to select counties that meet specific criteria.\nUse the next code cell to create a GeoDataFrame sel_counties that contains a subset of the rows (and all of the columns) from the CA_stats GeoDataFrame. In particular, you should select counties where:\në‹¤ìŒ ì½”ë“œ ì…€ì„ ì‚¬ìš©í•˜ì—¬ CA_stats GeoDataFrameì—ì„œ í–‰(ë° ëª¨ë“  ì—´)ì˜ í•˜ìœ„ ì§‘í•©ì„ í¬í•¨í•˜ëŠ” GeoDataFrame sel_countiesë¥¼ ë§Œë“­ë‹ˆë‹¤. íŠ¹íˆ ë‹¤ìŒê³¼ ê°™ì€ ì¹´ìš´í‹°ë¥¼ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.\n\nthere are at least 100,000 households making $150,000 per year,\nthe median age is less than 38.5, and\nthe density of inhabitants is at least 285 (per square kilometer).\n\nAdditionally, selected counties should satisfy at least one of the following criteria: - there are at least 500,000 households making $150,000 per year, - the median age is less than 35.5, or - the density of inhabitants is at least 1400 (per square kilometer).\n\n# Your code here\nsel_counties = CA_stats[((CA_stats.high_earners > 100000) &\n                         (CA_stats.median_age < 38.5) &\n                         (CA_stats.density > 285) &\n                         ((CA_stats.median_age < 35.5) |\n                         (CA_stats.density > 1400) |\n                         (CA_stats.high_earners > 500000)))]\n\n# Check your answer\nq_4.check()\n\n\n# Lines below will give you a hint or solution code\n#q_4.hint()\n#q_4.solution()\n\n\n\n5) How many stores did you identify?\nWhen looking for the next Starbucks Reserve Roastery location, youâ€™d like to consider all of the stores within the counties that you selected. So, how many stores are within the selected counties?\nTo prepare to answer this question, run the next code cell to create a GeoDataFrame starbucks_gdf with all of the starbucks locations.\n\nstarbucks_gdf = gpd.GeoDataFrame(starbucks, geometry=gpd.points_from_xy(starbucks.Longitude, starbucks.Latitude))\nstarbucks_gdf.crs = {'init': 'epsg:4326'}\n\nSo, how many stores are in the counties you selected?\nê·¸ë ‡ë‹¤ë©´ ì„ íƒí•œ ì¹´ìš´í‹°ì—ëŠ” ëª‡ ê°œì˜ ë§¤ì¥ì´ ìˆìŠµë‹ˆê¹Œ?\n\n# Fill in your answer\nlocations_of_interest = gpd.sjoin(starbucks_gdf, sel_counties)\nnum_stores = len(locations_of_interest)\n\n# Check your answer\nq_5.check()\n\n\n# Lines below will give you a hint or solution code\n#q_5.hint()\n#q_5.solution()\n\n\n\n6) Visualize the store locations.\nCreate a map that shows the locations of the stores that you identified in the previous question.\nì´ì „ ì§ˆë¬¸ì—ì„œ ì‹ë³„í•œ ìƒì ì˜ ìœ„ì¹˜ë¥¼ â€‹â€‹ë³´ì—¬ì£¼ëŠ” ì§€ë„ë¥¼ ë§Œë“œì‹­ì‹œì˜¤.\n\n# Create a base map\nm_6 = folium.Map(location=[37,-120], zoom_start=6)\n\n# Your code here: show selected store locations\nmc = MarkerCluster()\n\nlocations_of_interest = gpd.sjoin(starbucks_gdf, sel_counties)\nfor idx, row in locations_of_interest.iterrows():\n    if not math.isnan(row['Longitude']) and not math.isnan(row['Latitude']):\n        mc.add_child(folium.Marker([row['Latitude'], row['Longitude']]))\n\nm_6.add_child(mc)\n\n# Uncomment to see a hint\n#q_6.hint()\n\n# Show the map\nembed_map(m_6, 'q_6.html')\n\n\n# Get credit for your work after you have created a map\n#q_6.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_6.solution()\n\n\n\n\nKeep going\nLearn about how proximity analysis can help you to understand the relationships between points on a map.\n\nHave questions or comments? Visit the course discussion forum to chat with other learners."
  },
  {
    "objectID": "Data_Mining/2022-05-26-exercise-proximity-analysis.html",
    "href": "Data_Mining/2022-05-26-exercise-proximity-analysis.html",
    "title": "Proximity Analysis",
    "section": "",
    "text": "This notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link.\n\n\nimport math\nimport geopandas as gpd\nimport pandas as pd\nfrom shapely.geometry import MultiPolygon\n\nimport folium\nfrom folium import Choropleth, Marker\nfrom folium.plugins import HeatMap, MarkerCluster\n\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.geospatial.ex5 import *\n\nYouâ€™ll use the embed_map() function to visualize your maps.\n\ndef embed_map(m, file_name):\n    from IPython.display import IFrame\n    m.save(file_name)\n    return IFrame(file_name, width='100%', height='500px')\n\n\nExercises\n\n1) Visualize the collision data.\nRun the code cell below to load a GeoDataFrame collisions tracking major motor vehicle collisions in 2013-2018.\n\ncollisions = gpd.read_file(\"../input/geospatial-learn-course-data/NYPD_Motor_Vehicle_Collisions/NYPD_Motor_Vehicle_Collisions/NYPD_Motor_Vehicle_Collisions.shp\")\ncollisions.head()\n\nUse the â€œLATITUDEâ€ and â€œLONGITUDEâ€ columns to create an interactive map to visualize the collision data. What type of map do you think is most effective?\nâ€œLATITUDEâ€ ë° â€œLONGITUDEâ€ ì—´ì„ ì‚¬ìš©í•˜ì—¬ ì¶©ëŒ ë°ì´í„°ë¥¼ ì‹œê°í™”í•˜ëŠ” ëŒ€í™”í˜• ë§µì„ ë§Œë“­ë‹ˆë‹¤. ì–´ë–¤ ìœ í˜•ì˜ ì§€ë„ê°€ ê°€ì¥ íš¨ê³¼ì ì´ë¼ê³  ìƒê°í•˜ì‹­ë‹ˆê¹Œ?\n\nm_1 = folium.Map(location=[40.7, -74], zoom_start=11) \n\n# Your code here: Visualize the collision data\nHeatMap(data=collisions[['LATITUDE', 'LONGITUDE']], radius=9).add_to(m_1)\n\n# Uncomment to see a hint\n#q_1.hint()\n\n# Show the map\nembed_map(m_1, \"q_1.html\")\n\n\n# Get credit for your work after you have created a map\n#q_1.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_1.solution()\n\n\n\n2) Understand hospital coverage.\nRun the next code cell to load the hospital data.\n\nhospitals = gpd.read_file(\"../input/geospatial-learn-course-data/nyu_2451_34494/nyu_2451_34494/nyu_2451_34494.shp\")\nhospitals.head()\n\nUse the â€œlatitudeâ€ and â€œlongitudeâ€ columns to visualize the hospital locations.\nâ€œìœ„ë„â€ ë° â€œê²½ë„â€ ì—´ì„ ì‚¬ìš©í•˜ì—¬ ë³‘ì› ìœ„ì¹˜ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.\n\nm_2 = folium.Map(location=[40.7, -74], zoom_start=11) \n\n# Your code here: Visualize the hospital locations\nfor idx, row in hospitals.iterrows():\n    Marker([row['latitude'], row['longitude']], popup=row['name']).add_to(m_2)\n    \n# Uncomment to see a hint\n#q_2.hint()\n        \n# Show the map\nembed_map(m_2, \"q_2.html\")\n\n\n# Get credit for your work after you have created a map\nq_2.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_2.solution()\n\n\n\n3) When was the closest hospital more than 10 kilometers away?\nCreate a DataFrame outside_range containing all rows from collisions with crashes that occurred more than 10 kilometers from the closest hospital.\nê°€ì¥ ê°€ê¹Œìš´ ë³‘ì›ì—ì„œ 10km ì´ìƒ ë–¨ì–´ì§„ ê³³ì—ì„œ ë°œìƒí•œ ì¶©ëŒì´ ìˆëŠ” â€™ì¶©ëŒâ€™ì˜ ëª¨ë“  í–‰ì„ í¬í•¨í•˜ëŠ” DataFrame â€™outside_rangeâ€™ë¥¼ ë§Œë“­ë‹ˆë‹¤.\nNote that both hospitals and collisions have EPSG 2263 as the coordinate reference system, and EPSG 2263 has units of meters.\nâ€˜ë³‘ì›â€™ê³¼ â€™ì¶©ëŒâ€™ ëª¨ë‘ ì¢Œí‘œ ì°¸ì¡° ì‹œìŠ¤í…œìœ¼ë¡œ EPSG 2263ì„ ì‚¬ìš©í•˜ê³  EPSG 2263ì€ ë¯¸í„° ë‹¨ìœ„ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n\n# Your code here\ncoverage = gpd.GeoDataFrame(geometry=hospitals.geometry).buffer(10000)\nmy_union = coverage.geometry.unary_union\noutside_range = collisions.loc[~collisions[\"geometry\"].apply(lambda x: my_union.contains(x))]\n\n# Check your answer\nq_3.check()\n\n\n# Lines below will give you a hint or solution code\n#q_3.hint()\n#q_3.solution()\n\nThe next code cell calculates the percentage of collisions that occurred more than 10 kilometers away from the closest hospital.\n\npercentage = round(100*len(outside_range)/len(collisions), 2)\nprint(\"Percentage of collisions more than 10 km away from the closest hospital: {}%\".format(percentage))\n\n\n\n4) Make a recommender.\nWhen collisions occur in distant locations, it becomes even more vital that injured persons are transported to the nearest available hospital.\në©€ë¦¬ ë–¨ì–´ì§„ ê³³ì—ì„œ ì¶©ëŒì´ ë°œìƒí•˜ë©´ ë¶€ìƒìë¥¼ ê°€ì¥ ê°€ê¹Œìš´ ë³‘ì›ìœ¼ë¡œ ì´ì†¡í•˜ëŠ” ê²ƒì´ ë”ìš± ì¤‘ìš”í•´ì§‘ë‹ˆë‹¤.\nWith this in mind, you decide to create a recommender that: - takes the location of the crash (in EPSG 2263) as input, - finds the closest hospital (where distance calculations are done in EPSG 2263), and - returns the name of the closest hospital.\n\ndef best_hospital(collision_location):\n    idx_min = hospitals.geometry.distance(collision_location).idxmin()\n    my_hospital = hospitals.iloc[idx_min]\n    # Your code here\n    name = my_hospital[\"name\"]\n    return name\n\n# Test your function: this should suggest CALVARY HOSPITAL INC\nprint(best_hospital(outside_range.geometry.iloc[0]))\n\n# Check your answer\nq_4.check()\n\n\n# Lines below will give you a hint or solution code\n#q_4.hint()\n#q_4.solution()\n\n\n\n5) Which hospital is under the highest demand?\nConsidering only collisions in the outside_range DataFrame, which hospital is most recommended?\noutside_range DataFrameì—ì„œ ì¶©ëŒë§Œ ê³ ë ¤í•œë‹¤ë©´ ì–´ëŠ ë³‘ì›ì„ ê°€ì¥ ì¶”ì²œí•˜ëŠ”ê°€?\nYour answer should be a Python string that exactly matches the name of the hospital returned by the function you created in 4).\n\n# Your code here\nhighest_demand = outside_range.geometry.apply(best_hospital).value_counts().idxmax()\n\n# Check your answer\nq_5.check()\n\n\n# Lines below will give you a hint or solution code\n#q_5.hint()\n#q_5.solution()\n\n\n\n6) Where should the city construct new hospitals?\nRun the next code cell (without changes) to visualize hospital locations, in addition to collisions that occurred more than 10 kilometers away from the closest hospital.\në‹¤ìŒ ì½”ë“œ ì…€(ë³€ê²½ ì—†ì´)ì„ ì‹¤í–‰í•˜ì—¬ ê°€ì¥ ê°€ê¹Œìš´ ë³‘ì›ì—ì„œ 10km ì´ìƒ ë–¨ì–´ì§„ ê³³ì—ì„œ ë°œìƒí•œ ì¶©ëŒ ì™¸ì—ë„ ë³‘ì› ìœ„ì¹˜ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.\n\nm_6 = folium.Map(location=[40.7, -74], zoom_start=11) \n\ncoverage = gpd.GeoDataFrame(geometry=hospitals.geometry).buffer(10000)\nfolium.GeoJson(coverage.geometry.to_crs(epsg=4326)).add_to(m_6)\nHeatMap(data=outside_range[['LATITUDE', 'LONGITUDE']], radius=9).add_to(m_6)\nfolium.LatLngPopup().add_to(m_6)\n\nembed_map(m_6, 'm_6.html')\n\nClick anywhere on the map to see a pop-up with the corresponding location in latitude and longitude.\nThe city of New York reaches out to you for help with deciding locations for two brand new hospitals. They specifically want your help with identifying locations to bring the calculated percentage from step 3) to less than ten percent. Using the map (and without worrying about zoning laws or what potential buildings would have to be removed in order to build the hospitals), can you identify two locations that would help the city accomplish this goal?\nPut the proposed latitude and longitude for hospital 1 in lat_1 and long_1, respectively. (Likewise for hospital 2.)\në³‘ì› 1ì— ëŒ€í•´ ì œì•ˆëœ ìœ„ë„ì™€ ê²½ë„ë¥¼ ê°ê° lat_1ê³¼ long_1ì— ë„£ìŠµë‹ˆë‹¤. (ë³‘ì›2ë„ ë§ˆì°¬ê°€ì§€)\nThen, run the rest of the cell as-is to see the effect of the new hospitals. Your answer will be marked correct, if the two new hospitals bring the percentage to less than ten percent.\nê·¸ëŸ° ë‹¤ìŒ ë‚˜ë¨¸ì§€ ì…€ì„ ê·¸ëŒ€ë¡œ ì‹¤í–‰í•˜ì—¬ ìƒˆ ë³‘ì›ì˜ íš¨ê³¼ë¥¼ í™•ì¸í•˜ì‹­ì‹œì˜¤. ë‘ ê°œì˜ ìƒˆë¡œìš´ ë³‘ì›ì—ì„œ ë°±ë¶„ìœ¨ì„ 10% ë¯¸ë§Œìœ¼ë¡œ ë‚®ì¶”ë©´ ë‹µì´ ì •ë‹µìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤.\n\n# Your answer here: proposed location of hospital 1\nlat_1 = 40.6714\nlong_1 = -73.8492\n\n# Your answer here: proposed location of hospital 2\nlat_2 = 40.6702\nlong_2 = -73.7612\n\n# Do not modify the code below this line\ntry:\n    new_df = pd.DataFrame(\n        {'Latitude': [lat_1, lat_2],\n         'Longitude': [long_1, long_2]})\n    new_gdf = gpd.GeoDataFrame(new_df, geometry=gpd.points_from_xy(new_df.Longitude, new_df.Latitude))\n    new_gdf.crs = {'init' :'epsg:4326'}\n    new_gdf = new_gdf.to_crs(epsg=2263)\n    # get new percentage\n    new_coverage = gpd.GeoDataFrame(geometry=new_gdf.geometry).buffer(10000)\n    new_my_union = new_coverage.geometry.unary_union\n    new_outside_range = outside_range.loc[~outside_range[\"geometry\"].apply(lambda x: new_my_union.contains(x))]\n    new_percentage = round(100*len(new_outside_range)/len(collisions), 2)\n    print(\"(NEW) Percentage of collisions more than 10 km away from the closest hospital: {}%\".format(new_percentage))\n    # Did you help the city to meet its goal?\n    q_6.check()\n    # make the map\n    m = folium.Map(location=[40.7, -74], zoom_start=11) \n    folium.GeoJson(coverage.geometry.to_crs(epsg=4326)).add_to(m)\n    folium.GeoJson(new_coverage.geometry.to_crs(epsg=4326)).add_to(m)\n    for idx, row in new_gdf.iterrows():\n        Marker([row['Latitude'], row['Longitude']]).add_to(m)\n    HeatMap(data=new_outside_range[['LATITUDE', 'LONGITUDE']], radius=9).add_to(m)\n    folium.LatLngPopup().add_to(m)\n    display(embed_map(m, 'q_6.html'))\nexcept:\n    q_6.hint()\n\n\n# Uncomment to see one potential answer \n#q_6.solution()\n\n\n\n\nCongratulations!\nYou have just completed the Geospatial Analysis micro-course! Great job!\n\nHave questions or comments? Visit the course discussion forum to chat with other learners."
  },
  {
    "objectID": "Data_Mining/2022-06-07-last.html",
    "href": "Data_Mining/2022-06-07-last.html",
    "title": "COVID-19 Analysis & Visualization",
    "section": "",
    "text": "ì„œë¡  \n\në¶„ì„ ë°°ê²½ ë° ëª©ì  \n\në¶„ì„ ë°°ê²½ \në¶„ì„ ë°°ê²½ - 2022ë…„ 6ì›” í˜„ì¬ ì½”ë¡œë‚˜ì˜ ìƒí™©ì€ ë§¤ì¼ 10000ëª…ì˜ í™•ì§„ìê°€ ë‚˜ì˜¤ê³  ìˆëŠ” ìƒí™©ì´ì§€ë§Œ ì½”ë¡œë‚˜ê°€ ì²˜ìŒ ë°œë³‘í•˜ê³  ë‚˜ì„œì™€ëŠ” ì¡°ê¸ˆì€ ë‹¤ë¥¸ ë°˜ì‘ì´ë‹¤. ìµœê·¼ ì •ë¶€ì—ì„œëŠ” ì§‘ë‹¨ ë©´ì—­ì´ 90% ì´ìƒ í˜•ì„±ì´ ë˜ì–´ìˆìœ¼ë©° í™•ì§„ìì˜ ì¶”ì„¸ ë˜í•œ ê°ì†Œì„¸ë¥¼ ë³´ì´ê³  ìˆëŠ” ìƒí™©ì—ì„œ 2020ë…„ 01ì›”ë¶€í„° 2020ë…„ 06ì›”ê¹Œì§€ ìˆ˜ì§‘ëœ í•´ë‹¹ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³¼ì—° ê³¼ê±°ì™€ í˜„ì¬ì˜ ì°¨ì´ëŠ” ì–¼ë§ˆë‚˜ ìˆê³  ë‹¹ì‹œ ì •ë¶€ì™€ ë‰´ìŠ¤ì—ì„œ ì£¼ì¥í•˜ë˜ ì½”ë¡œë‚˜ì— ëŒ€í•œ ì •ë³´ëŠ” ê³¼ì—° íƒ€ë‹¹í•˜ì˜€ê³  ì˜¬ë°”ë¥¸ ì •ë³´ì˜€ëŠ”ì§€ ê¶ê¸ˆí•˜ì—¬ í•´ë‹¹ ì£¼ì œë¥¼ ì„ ì •í•˜ì—¬ ë¶„ì„ì„ ì§„í–‰í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n\në¶„ì„ ëª©ì  \në¶„ì„ ëª©ì  - ìë£Œë¥¼ ì œê³µí•œ ë°ì´ì½˜ì—ì„œëŠ” í•´ë‹¹ ìë£Œë“¤ì€ ì´ìš©í•´ì„œ ë‹¤ìŒê³¼ ê°™ì€ ì¸ê³µì§€ëŠ¥ AIë¥¼ í™œìš© ì½”ë¡œë‚˜ í™•ì‚° ë°©ì§€ì™€ ì˜ˆë°©ì„ ìœ„í•œ ì¸ì‚¬ì´íŠ¸ / ì‹œê°í™” ë°œêµ´. ì´ë¼ëŠ” ëª©ì ì„ ê°€ì§€ê³  ì§„í–‰ì„ í•˜ì˜€ìŠµë‹ˆë‹¤. í•´ì„œ ì €ëŠ” ë‹¹ì‹œ ê¸°ê°„ë™ì•ˆ ê°€ì¥ ë§ì´ í™•ì§„ëœ ì—°ë ¹ì¸µê³¼ ì£¼ëœ ê°ì—¼ ì›ì¸ê³¼ ê·¸ ì´ìœ ì— ëŒ€í•´ì„œ ì•Œì•„ë³´ê³ , ì–´ë–¤ ì—°ë ¹ì¸µì—ê²Œ ê°€ì¥ ì¹˜ëª…ì ì¸ ì§ˆë³‘ì´ì—ˆëŠì§€ì™€ ë‹¹ì‹œ ì •ë¶€ì˜ ë°©ì—­ ëŒ€ì±…ì€ íƒ€ë‹¹í•˜ì˜€ëŠ”ì§€ ì— ëŒ€í•´ì„œ ëª©ì ì„ ê°€ì§€ê³  í•´ë‹¹ ë¶„ì„ì„ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.\n\n\n\në°ì´í„° ì†Œê°œ \n\n\në°ì´í„° ì¹´í…Œê³ ë¦¬ \n\nCase Data\n\n\nCase: í•œêµ­ì˜ COVID-19 ê°ì—¼ ì‚¬ë¡€ ë°ì´í„°\n\n\nPatient Data\n\n\nPatientInfo: í•œêµ­ì˜ ì½”ë¡œë‚˜19 í™˜ì ì—­í•™ ë°ì´í„°\nPatientRoute: êµ­ë‚´ ì½”ë¡œë‚˜19 í™˜ì ê²½ë¡œ ë°ì´í„°\n\n\nTime Series Data\n\n\nTime: í•œêµ­ì˜ ì½”ë¡œë‚˜19 ìƒíƒœì˜ ì‹œê³„ì—´ ë°ì´í„°\nTimeAge: í•œêµ­ì˜ ì—°ë ¹ë³„ ì½”ë¡œë‚˜19 í˜„í™© ì‹œê³„ì—´ ë°ì´í„°\nTimeGender: í•œêµ­ì˜ ì„±ë³„ì— ë”°ë¥¸ ì½”ë¡œë‚˜19 í˜„í™©ì˜ ì‹œê³„ì—´ ë°ì´í„°\nTimeProvince: í•œêµ­ì˜ ì§€ì—­ë³„ ì½”ë¡œë‚˜19 í˜„í™© ì‹œê³„ì—´ ë°ì´í„°\n\n\nAdditional Data\n\n\nRegion: ëŒ€í•œë¯¼êµ­ ë‚´ ì§€ì—­ì˜ ìœ„ì¹˜ ë° í†µê³„ ìë£Œ\nWeather: í•œêµ­ ì§€ì—­ì˜ ë‚ ì”¨ ë°ì´í„°\nSearchTrend: êµ­ë‚´ ìµœëŒ€ í¬í„¸ì‚¬ì´íŠ¸ ë„¤ì´ë²„ì—ì„œ ê²€ìƒ‰ëœ í‚¤ì›Œë“œì˜ íŠ¸ë Œë“œ ë°ì´í„°\nSeoulFloating: ëŒ€í•œë¯¼êµ­ ì„œìš¸ ìœ ë™ì¸êµ¬ ë°ì´í„°(SKí…”ë ˆì½¤ ë¹…ë°ì´í„° í—ˆë¸Œì—ì„œ)\nPolicy: í•œêµ­ì˜ ì½”ë¡œë‚˜19ì— ëŒ€í•œ ì •ë¶€ ì •ì±… ë°ì´í„°\n\n\n\në°ì´í„° í˜•íƒœ \n\nìƒ‰ìƒì´ ì˜ë¯¸í•˜ëŠ” ê²ƒì€ ë¹„ìŠ·í•œ ì†ì„±ì„ ê°€ì§€ê³  ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.\nì—´ ì‚¬ì´ì— ì„ ì´ ì—°ê²°ë˜ì–´ ìˆë‹¤ëŠ” ê²ƒì€ ì—´ì˜ ê°’ì´ ë¶€ë¶„ì ìœ¼ë¡œ ê³µìœ ë¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\nì ì„ ì€ ì•½í•œ ê´€ë ¨ì„±ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n\nhttps://user-images.githubusercontent.com/50820635/86225695-8dca0580-bbc5-11ea-9e9b-b0ca33414d8a.PNG\n\n\në°ì´í„° ì„¸ë¶€ ì„¤ëª… \n\nimport numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('../notebook/coronavirusdataset'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n../notebook/coronavirusdataset\\Case.csv\n../notebook/coronavirusdataset\\PatientInfo.csv\n../notebook/coronavirusdataset\\PatientRoute.csv\n../notebook/coronavirusdataset\\Policy.csv\n../notebook/coronavirusdataset\\Region.csv\n../notebook/coronavirusdataset\\SearchTrend.csv\n../notebook/coronavirusdataset\\SeoulFloating.csv\n../notebook/coronavirusdataset\\Time.csv\n../notebook/coronavirusdataset\\TimeAge.csv\n../notebook/coronavirusdataset\\TimeGender.csv\n../notebook/coronavirusdataset\\TimeProvince.csv\n../notebook/coronavirusdataset\\Weather.csv\n\n\n\npath = '../notebook/coronavirusdataset/'\n\ncase = p_info = pd.read_csv(path+'Case.csv')\npatientinfo = pd.read_csv(path+'PatientInfo.csv')\npatientroute = pd.read_csv(path+'PatientRoute.csv')\ntime = pd.read_csv(path+'Time.csv')\ntimeage = pd.read_csv(path+'TimeAge.csv')\ntimegender = pd.read_csv(path+'TimeGender.csv')\ntimeprovince = pd.read_csv(path+'TimeProvince.csv')\nregion = pd.read_csv(path+'Region.csv')\nweather = pd.read_csv(path+'Weather.csv')\nsearchtrend = pd.read_csv(path+'SearchTrend.csv')\nseoulfloating = pd.read_csv(path+'SeoulFloating.csv')\npolicy = pd.read_csv(path+'Policy.csv')\n\n\nCase\n\ní•œêµ­ì˜ COVID-19 ê°ì—¼ ì‚¬ë¡€ ë°ì´í„°\n\ncase_id: the ID of the infection case\n\n\ncase_id(7) = region_code(5) + case_number(2)\nYou can check the region_code in â€˜Region.csvâ€™\nprovince: Special City / Metropolitan City / Province(-do)\ncity: City(-si) / Country (-gun) / District (-gu)\n\nThe value â€˜from other cityâ€™ means that where the group infection started is other city.\n\ngroup: TRUE: group infection / FALSE: not group\n\nIf the value is â€˜TRUEâ€™ in this column, the value of â€˜infection_casesâ€™ means the name of group.\nThe values named â€˜contact with patientâ€™, â€˜overseas inflowâ€™ and â€˜etcâ€™ are not group infection.\n\ninfection_case: the infection case (the name of group or other cases)\n\nThe value â€˜overseas inflowâ€™ means that the infection is from other country.\nThe value â€˜etcâ€™ includes individual cases, cases where relevance classification is ongoing after investigation, and cases under investigation.\n\nconfirmed: the accumulated number of the confirmed\nlatitude: the latitude of the group (WGS84)\nlongitude: the longitude of the group (WGS84)\n\n\ncase.head()\n\n\n\n\n\n  \n    \n      \n      case_id\n      province\n      city\n      group\n      infection_case\n      confirmed\n      latitude\n      longitude\n    \n  \n  \n    \n      0\n      1000001\n      Seoul\n      Yongsan-gu\n      True\n      Itaewon Clubs\n      139\n      37.538621\n      126.992652\n    \n    \n      1\n      1000002\n      Seoul\n      Gwanak-gu\n      True\n      Richway\n      119\n      37.48208\n      126.901384\n    \n    \n      2\n      1000003\n      Seoul\n      Guro-gu\n      True\n      Guro-gu Call Center\n      95\n      37.508163\n      126.884387\n    \n    \n      3\n      1000004\n      Seoul\n      Yangcheon-gu\n      True\n      Yangcheon Table Tennis Club\n      43\n      37.546061\n      126.874209\n    \n    \n      4\n      1000005\n      Seoul\n      Dobong-gu\n      True\n      Day Care Center\n      43\n      37.679422\n      127.044374\n    \n  \n\n\n\n\n\nPatientInfo\n\ní•œêµ­ì˜ ì½”ë¡œë‚˜19 í™˜ì ì—­í•™ ë°ì´í„°\n\npatient_id: the ID of the patient\n\n\npatient_id(10) = region_code(5) + patient_number(5)\nYou can check the region_code in â€˜Region.csvâ€™\nThere are two types of the patient_number\n\nlocal_num: The number given by the local government.\nglobal_num: The number given by the KCDC\n\nsex: the sex of the patient\nage: the age of the patient\n\n0s: 0 ~ 9\n10s: 10 ~ 19 â€¦\n90s: 90 ~ 99\n100s: 100 ~ 109\n\ncountry: the country of the patient\nprovince: the province of the patient\ncity: the city of the patient\ninfection_case: the case of infection\ninfected_by: the ID of who infected the patient\n\nThis column refers to the â€˜patient_idâ€™ column.\n\ncontact_number: the number of contacts with people\nsymptom_onset_date: the date of symptom onset\nconfirmed_date: the date of being confirmed\nreleased_date: the date of being released\ndeceased_date: the date of being deceased\nstate: isolated / released / deceased\n\nisolated: being isolated in the hospital\nreleased: being released from the hospital\ndeceased: being deceased\n\n\n\npatientinfo.head()\n\n\n\n\n\n  \n    \n      \n      patient_id\n      sex\n      age\n      country\n      province\n      city\n      infection_case\n      infected_by\n      contact_number\n      symptom_onset_date\n      confirmed_date\n      released_date\n      deceased_date\n      state\n    \n  \n  \n    \n      0\n      1000000001\n      male\n      50s\n      Korea\n      Seoul\n      Gangseo-gu\n      overseas inflow\n      NaN\n      75\n      2020-01-22\n      2020-01-23\n      2020-02-05\n      NaN\n      released\n    \n    \n      1\n      1000000002\n      male\n      30s\n      Korea\n      Seoul\n      Jungnang-gu\n      overseas inflow\n      NaN\n      31\n      NaN\n      2020-01-30\n      2020-03-02\n      NaN\n      released\n    \n    \n      2\n      1000000003\n      male\n      50s\n      Korea\n      Seoul\n      Jongno-gu\n      contact with patient\n      2002000001\n      17\n      NaN\n      2020-01-30\n      2020-02-19\n      NaN\n      released\n    \n    \n      3\n      1000000004\n      male\n      20s\n      Korea\n      Seoul\n      Mapo-gu\n      overseas inflow\n      NaN\n      9\n      2020-01-26\n      2020-01-30\n      2020-02-15\n      NaN\n      released\n    \n    \n      4\n      1000000005\n      female\n      20s\n      Korea\n      Seoul\n      Seongbuk-gu\n      contact with patient\n      1000000002\n      2\n      NaN\n      2020-01-31\n      2020-02-24\n      NaN\n      released\n    \n  \n\n\n\n\n\nPatientRoute\n\ní•œêµ­ì˜ ì½”ë¡œë‚˜19 í™˜ì ê²½ë¡œ ë°ì´í„°\n\npatient_id: the ID of the patient\ndate: YYYY-MM-DD\nprovince: Special City / Metropolitan City / Province(-do)\ncity: City(-si) / Country (-gun) / District (-gu)\nlatitude: the latitude of the visit (WGS84)\nlongitude: the longitude of the visit (WGS84)\n\n\npatientroute.head()\n\n\n\n\n\n  \n    \n      \n      patient_id\n      global_num\n      date\n      province\n      city\n      type\n      latitude\n      longitude\n    \n  \n  \n    \n      0\n      1000000001\n      2.0\n      2020-01-22\n      Gyeonggi-do\n      Gimpo-si\n      airport\n      37.615246\n      126.715632\n    \n    \n      1\n      1000000001\n      2.0\n      2020-01-24\n      Seoul\n      Jung-gu\n      hospital\n      37.567241\n      127.005659\n    \n    \n      2\n      1000000002\n      5.0\n      2020-01-25\n      Seoul\n      Seongbuk-gu\n      etc\n      37.592560\n      127.017048\n    \n    \n      3\n      1000000002\n      5.0\n      2020-01-26\n      Seoul\n      Seongbuk-gu\n      store\n      37.591810\n      127.016822\n    \n    \n      4\n      1000000002\n      5.0\n      2020-01-26\n      Seoul\n      Seongdong-gu\n      public_transportation\n      37.563992\n      127.029534\n    \n  \n\n\n\n\n\nTime\n\ní•œêµ­ì˜ COVID-19 ìƒíƒœì˜ ì‹œê³„ì—´ ë°ì´í„°\n\ndate: YYYY-MM-DD\ntime: Time (0 = AM 12:00 / 16 = PM 04:00)\n\nThe time for KCDC to open the information has been changed from PM 04:00 to AM 12:00 since March 2nd.\n\ntest: the accumulated number of tests\n\nA test is a diagnosis of an infection.\n\nnegative: the accumulated number of negative results\nconfirmed: the accumulated number of positive results\nreleased: the accumulated number of releases\ndeceased: the accumulated number of deceases\n\n\ntime.head()\n\n\n\n\n\n  \n    \n      \n      date\n      time\n      test\n      negative\n      confirmed\n      released\n      deceased\n    \n  \n  \n    \n      0\n      2020-01-20\n      16\n      1\n      0\n      1\n      0\n      0\n    \n    \n      1\n      2020-01-21\n      16\n      1\n      0\n      1\n      0\n      0\n    \n    \n      2\n      2020-01-22\n      16\n      4\n      3\n      1\n      0\n      0\n    \n    \n      3\n      2020-01-23\n      16\n      22\n      21\n      1\n      0\n      0\n    \n    \n      4\n      2020-01-24\n      16\n      27\n      25\n      2\n      0\n      0\n    \n  \n\n\n\n\n\nTimeAge\n\ní•œêµ­ì˜ ì—°ë ¹ë³„ ì½”ë¡œë‚˜19 í˜„í™© ì‹œê³„ì—´ ë°ì´í„°\n\ndate: YYYY-MM-DD\n\nThe status in terms of the age has been presented since March 2nd.\n\ntime: Time\nage: the age of patients\nconfirmed: the accumulated number of the confirmed\ndeceased: the accumulated number of the deceased\n\n\ntimeage.head()\n\n\n\n\n\n  \n    \n      \n      date\n      time\n      age\n      confirmed\n      deceased\n    \n  \n  \n    \n      0\n      2020-03-02\n      0\n      0s\n      32\n      0\n    \n    \n      1\n      2020-03-02\n      0\n      10s\n      169\n      0\n    \n    \n      2\n      2020-03-02\n      0\n      20s\n      1235\n      0\n    \n    \n      3\n      2020-03-02\n      0\n      30s\n      506\n      1\n    \n    \n      4\n      2020-03-02\n      0\n      40s\n      633\n      1\n    \n  \n\n\n\n\n\nTimeGender\n\ní•œêµ­ì˜ ì„±ë³„ì— ë”°ë¥¸ COVID-19 í˜„í™©ì˜ ì‹œê³„ì—´ ë°ì´í„°\n\ndate: YYYY-MM-DD\n\nThe status in terms of the gender has been presented since March 2nd.\n\ntime: Time\nsex: the gender of patients\nconfirmed: the accumulated number of the confirmed\ndeceased: the accumulated number of the deceased\n\n\ntimegender.head()\n\n\n\n\n\n  \n    \n      \n      date\n      time\n      sex\n      confirmed\n      deceased\n    \n  \n  \n    \n      0\n      2020-03-02\n      0\n      male\n      1591\n      13\n    \n    \n      1\n      2020-03-02\n      0\n      female\n      2621\n      9\n    \n    \n      2\n      2020-03-03\n      0\n      male\n      1810\n      16\n    \n    \n      3\n      2020-03-03\n      0\n      female\n      3002\n      12\n    \n    \n      4\n      2020-03-04\n      0\n      male\n      1996\n      20\n    \n  \n\n\n\n\n\nTimeProvince\n\ní•œêµ­ì˜ ì§€ì—­ë³„ ì½”ë¡œë‚˜19 í˜„í™© ì‹œê³„ì—´ ë°ì´í„°\n\ndate: YYYY-MM-DD\ntime: Time\nprovince: the province of South Korea\nconfirmed: the accumulated number of the confirmed in the province\n\nThe confirmed status in terms of the provinces has been presented since Feburary 21th.\nThe value before Feburary 21th can be different.\n\nreleased: the accumulated number of the released in the province\n\nThe confirmed status in terms of the provinces has been presented since March 5th. -The value before March 5th can be different.\n\ndeceased: the accumulated number of the deceased in the province\n\nThe confirmed status in terms of the provinces has been presented since March 5th.\nThe value before March 5th can be different.\n\n\n\ntimeprovince.head()\n\n\n\n\n\n  \n    \n      \n      date\n      time\n      province\n      confirmed\n      released\n      deceased\n    \n  \n  \n    \n      0\n      2020-01-20\n      16\n      Seoul\n      0\n      0\n      0\n    \n    \n      1\n      2020-01-20\n      16\n      Busan\n      0\n      0\n      0\n    \n    \n      2\n      2020-01-20\n      16\n      Daegu\n      0\n      0\n      0\n    \n    \n      3\n      2020-01-20\n      16\n      Incheon\n      1\n      0\n      0\n    \n    \n      4\n      2020-01-20\n      16\n      Gwangju\n      0\n      0\n      0\n    \n  \n\n\n\n\n\nRegion\n\nëŒ€í•œë¯¼êµ­ ë‚´ ì§€ì—­ì˜ ìœ„ì¹˜ ë° í†µê³„ ìë£Œ\n\ncode: the code of the region\nprovince: Special City / Metropolitan City / Province(-do)\ncity: City(-si) / Country (-gun) / District (-gu)\nlatitude: the latitude of the visit (WGS84)\nlongitude: the longitude of the visit (WGS84)\nelementary_school_count: the number of elementary schools\nkindergarten_count: the number of kindergartens\nuniversity_count: the number of universities\nacademy_ratio: the ratio of academies\nelderly_population_ratio: the ratio of the elderly population\nelderly_alone_ratio: the ratio of elderly households living alone\nnursing_home_count: the number of nursing homes\n\nSource of the statistic: KOSTAT (Statistics Korea)\n\nregion.head()\n\n\n\n\n\n  \n    \n      \n      code\n      province\n      city\n      latitude\n      longitude\n      elementary_school_count\n      kindergarten_count\n      university_count\n      academy_ratio\n      elderly_population_ratio\n      elderly_alone_ratio\n      nursing_home_count\n    \n  \n  \n    \n      0\n      10000\n      Seoul\n      Seoul\n      37.566953\n      126.977977\n      607\n      830\n      48\n      1.44\n      15.38\n      5.8\n      22739\n    \n    \n      1\n      10010\n      Seoul\n      Gangnam-gu\n      37.518421\n      127.047222\n      33\n      38\n      0\n      4.18\n      13.17\n      4.3\n      3088\n    \n    \n      2\n      10020\n      Seoul\n      Gangdong-gu\n      37.530492\n      127.123837\n      27\n      32\n      0\n      1.54\n      14.55\n      5.4\n      1023\n    \n    \n      3\n      10030\n      Seoul\n      Gangbuk-gu\n      37.639938\n      127.025508\n      14\n      21\n      0\n      0.67\n      19.49\n      8.5\n      628\n    \n    \n      4\n      10040\n      Seoul\n      Gangseo-gu\n      37.551166\n      126.849506\n      36\n      56\n      1\n      1.17\n      14.39\n      5.7\n      1080\n    \n  \n\n\n\n\n\nWeather\n\ní•œêµ­ ì§€ì—­ì˜ ë‚ ì”¨ ë°ì´í„°\n\ncode: the code of the region\nprovince: Special City / Metropolitan City / Province(-do)\ndate: YYYY-MM-DD\navg_temp: the average temperature\nmin_temp: the lowest temperature\nmax_temp: the highest temperature\nprecipitation: the daily precipitation\nmax_wind_speed: the maximum wind speed\nmost_wind_direction: the most frequent wind direction\navg_relative_humidity: the average relative humidity\n\nSource of the weather data: KMA (Korea Meteorological Administration)\n\nweather.head()\n\n\n\n\n\n  \n    \n      \n      code\n      province\n      date\n      avg_temp\n      min_temp\n      max_temp\n      precipitation\n      max_wind_speed\n      most_wind_direction\n      avg_relative_humidity\n    \n  \n  \n    \n      0\n      10000\n      Seoul\n      2016-01-01\n      1.2\n      -3.3\n      4.0\n      0.0\n      3.5\n      90.0\n      73.0\n    \n    \n      1\n      11000\n      Busan\n      2016-01-01\n      5.3\n      1.1\n      10.9\n      0.0\n      7.4\n      340.0\n      52.1\n    \n    \n      2\n      12000\n      Daegu\n      2016-01-01\n      1.7\n      -4.0\n      8.0\n      0.0\n      3.7\n      270.0\n      70.5\n    \n    \n      3\n      13000\n      Gwangju\n      2016-01-01\n      3.2\n      -1.5\n      8.1\n      0.0\n      2.7\n      230.0\n      73.1\n    \n    \n      4\n      14000\n      Incheon\n      2016-01-01\n      3.1\n      -0.4\n      5.7\n      0.0\n      5.3\n      180.0\n      83.9\n    \n  \n\n\n\n\n\nSearchTrend\n\nêµ­ë‚´ ìµœëŒ€ í¬í„¸ì¸ ë„¤ì´ë²„ì—ì„œ ê²€ìƒ‰ëœ í‚¤ì›Œë“œì˜ íŠ¸ë Œë“œ ë°ì´í„°\n\ndate: YYYY-MM-DD\ncold: the search volume of â€˜coldâ€™ in Korean language\n\nThe unit means relative value by setting the highest search volume in the period to 100.\n\nflu: the search volume of â€˜fluâ€™ in Korean language\n\nSame as above.\n\npneumonia: the search volume of â€˜pneumoniaâ€™ in Korean language -Same as above.\ncoronavirus: the search volume of â€˜coronavirusâ€™ in Korean language -Same as above.\n\nSource of the data: NAVER DataLab\n\nsearchtrend.head()\n\n\n\n\n\n  \n    \n      \n      date\n      cold\n      flu\n      pneumonia\n      coronavirus\n    \n  \n  \n    \n      0\n      2016-01-01\n      0.11663\n      0.05590\n      0.15726\n      0.00736\n    \n    \n      1\n      2016-01-02\n      0.13372\n      0.17135\n      0.20826\n      0.00890\n    \n    \n      2\n      2016-01-03\n      0.14917\n      0.22317\n      0.19326\n      0.00845\n    \n    \n      3\n      2016-01-04\n      0.17463\n      0.18626\n      0.29008\n      0.01145\n    \n    \n      4\n      2016-01-05\n      0.17226\n      0.15072\n      0.24562\n      0.01381\n    \n  \n\n\n\n\n\nSeoulFloating\n\nëŒ€í•œë¯¼êµ­ ì„œìš¸ ìœ ë™ì¸êµ¬ ë°ì´í„°(SKí…”ë ˆì½¤ ë¹…ë°ì´í„° í—ˆë¸Œì—ì„œ)\n\ndate: YYYY-MM-DD\nhour: Hour\nbirth_year: the birth year of the floating population\nsext: he sex of the floating population\nprovince: Special City / Metropolitan City / Province(-do)\ncity: City(-si) / Country (-gun) / District (-gu)\nfp_num: the number of floating population\n\nSource of the data: SKT Big Data Hub\n\nseoulfloating.head()\n\n\n\n\n\n  \n    \n      \n      date\n      hour\n      birth_year\n      sex\n      province\n      city\n      fp_num\n    \n  \n  \n    \n      0\n      2020-01-01\n      0\n      20\n      female\n      Seoul\n      Dobong-gu\n      19140\n    \n    \n      1\n      2020-01-01\n      0\n      20\n      male\n      Seoul\n      Dobong-gu\n      19950\n    \n    \n      2\n      2020-01-01\n      0\n      20\n      female\n      Seoul\n      Dongdaemun-gu\n      25450\n    \n    \n      3\n      2020-01-01\n      0\n      20\n      male\n      Seoul\n      Dongdaemun-gu\n      27050\n    \n    \n      4\n      2020-01-01\n      0\n      20\n      female\n      Seoul\n      Dongjag-gu\n      28880\n    \n  \n\n\n\n\n\nPolicy\n\ní•œêµ­ì˜ COVID-19ì— ëŒ€í•œ ì •ë¶€ ì •ì±… ë°ì´í„°\n\npolicy_id: the ID of the policy\ncountry: the country that implemented the policy\ntype: the type of the policy\ngov_policy: the policy of the government\ndetail: the detail of the policy\nstart_date: the start date of the policy\nend_date: the end date of the policy\n\n\npolicy.head()\n\n\n\n\n\n  \n    \n      \n      policy_id\n      country\n      type\n      gov_policy\n      detail\n      start_date\n      end_date\n    \n  \n  \n    \n      0\n      1\n      Korea\n      Alert\n      Infectious Disease Alert Level\n      Level 1 (Blue)\n      2020-01-03\n      2020-01-19\n    \n    \n      1\n      2\n      Korea\n      Alert\n      Infectious Disease Alert Level\n      Level 2 (Yellow)\n      2020-01-20\n      2020-01-27\n    \n    \n      2\n      3\n      Korea\n      Alert\n      Infectious Disease Alert Level\n      Level 3 (Orange)\n      2020-01-28\n      2020-02-22\n    \n    \n      3\n      4\n      Korea\n      Alert\n      Infectious Disease Alert Level\n      Level 4 (Red)\n      2020-02-23\n      NaN\n    \n    \n      4\n      5\n      Korea\n      Immigration\n      Special Immigration Procedure\n      from China\n      2020-02-04\n      NaN\n    \n  \n\n\n\n\n\n\n\në³¸ë¡  \n\nì£¼ì œ1 - ì–´ë–¤ ì—°ë ¹ì¸µì´ ê°€ì¥ ë§ì´ í™•ì§„ë˜ì—ˆëŠ”ê°€? \n\nì£¼ì œ1 - EDA \n\n# import packages - ì‚¬ìš©í•  íŒ¨í‚¤ì§€ ë¶ˆëŸ¬ì˜¤ê¸°\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom matplotlib import pyplot as plt\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\n\ntime.head() #time ë°ì´í„°ì˜ ìƒìœ„ 5ê°œë¥¼ í™•ì¸\n\n\n\n\n\n  \n    \n      \n      date\n      time\n      test\n      negative\n      confirmed\n      released\n      deceased\n    \n  \n  \n    \n      0\n      2020-01-20\n      16\n      1\n      0\n      1\n      0\n      0\n    \n    \n      1\n      2020-01-21\n      16\n      1\n      0\n      1\n      0\n      0\n    \n    \n      2\n      2020-01-22\n      16\n      4\n      3\n      1\n      0\n      0\n    \n    \n      3\n      2020-01-23\n      16\n      22\n      21\n      1\n      0\n      0\n    \n    \n      4\n      2020-01-24\n      16\n      27\n      25\n      2\n      0\n      0\n    \n  \n\n\n\n\n\n#ì‹œê°„ì˜ íë¦„ì— ë”°ë¥¸ í™•ì§„ì ì¶”ì´\nfig = go.Figure() #ë¹ˆ ë„í™”ì§€ë¥¼ ë§Œë“ ë‹¤ëŠ” ê°œë…\n\nfig.add_trace(go.Scatter(x=time['date'], y=time['confirmed'],\n                    mode='lines', \n                    name='í™•ì§„(confirmed)')) \n                    #Scatter í˜•íƒœì˜ í”Œëìœ¼ë¡œ xì¶•ì€ timeë°ì´í„°ì˜ dateì»¬ëŸ¼ì„ ì‚¬ìš©í•˜ê³  yì¶•ì€ timeë°ì´í„°ì˜ confirmed ì»¬ëŸ¼ì„ ì‚¬ìš©í•˜ê³  í‘œí˜„ ë°©ë²•ì€ lineì´ë©° ì„ ì˜ ì´ë¦„ì€ í™•ì§„ìœ¼ë¡œ ì§€ì •\n\nfig.update_layout(title='ì‹œê°„ì˜ íë¦„ì— ë”°ë¥¸ í™•ì§„ì ì¶”ì´',\n                   xaxis_title='Date',\n                   yaxis_title='Number') #ê·¸ë˜í”„ì˜ ì œëª©ê³¼ xì¶• yì¶•ì˜ ì´ë¦„ì„ ì§€ì •\n\nfig.show() #ê·¸ë˜í”„ë¥¼ ì¶œë ¥í•´ì„œ ë³´ì´ë„ë¡\n\n\n                                                \n\n\ní•´ë‹¹ ê·¸ë˜í”„ë¥¼ ë³´ë©´ ì‹œê°„ì˜ íë¦„ì— ë”°ë¥¸ ëˆ„ì  í™•ì§„ìì˜ ìˆ˜ë¥¼ ë‚˜íƒ€ë‚¸ê²ƒìœ¼ë¡œ 20ë…„ 3ì›”ë¶€í„° ê°€íŒŒë¥¸ ê²½ì‚¬ë¥¼ ë³´ì´ë©´ì„œ ìš°ìƒí–¥í•´ì„œ ì¦ê°€í•˜ëŠ” ì¶”ì„¸ë¥¼ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=time['date'], y=time['confirmed'],\n                    mode='lines', \n                    name='í™•ì§„(confirmed)'))\n\nfig.add_trace(go.Scatter(x=time['date'], y=time['released'],\n                    mode='lines', \n                    name='í•´ì œ(released)'))\n                    \nfig.add_trace(go.Scatter(x=time['date'], y=time['deceased'],\n                    mode='lines', \n                    name='ì‚¬ë§(deceased)'))\n\nfig.update_layout(title='ì‹œê°„ì˜ íë¦„ì— ë”°ë¥¸ ì½”ë¡œë‚˜ì˜ ì¶”ì´',\n                   xaxis_title='Date',\n                   yaxis_title='Number')\n\nfig.show()\n\n\n                                                \n\n\në‹¤ìŒ ê·¸ë˜í”„ëŠ” ì‹œê°„ì˜ íë¦„ì— ë”°ë¥¸ ì½”ë¡œë‚˜ì˜ ì¶”ì´ë¡œ í™•ì§„ìì™€ ê²©ë¦¬í•´ì œìì˜ ì¶”ì´ì™€ ì‚¬ë§ìì˜ ì¶”ì´ì— ëŒ€í•´ì„œ ë³´ì—¬ì£¼ê³  ìˆìœ¼ë©° í™•ì§„ìì™€ ê²©ë¦¬í•´ì œìì˜ ì¶”ì´ëŠ” ìœ ì‚¬í•˜ê²Œ ìš°ìƒí–¥í•˜ëŠ” ëª¨ìŠµì„ ë³´ì—¬ì£¼ê³  ìˆìœ¼ë©° ì‚¬ë§ìëŠ” í™•ì§„ìì™€ ê²©ë¦¬ìì˜ ìˆ˜ì— ë¹„í•´ì„œëŠ” ì ì–´ì„œ ëˆˆì— ë³´ì´ëŠ” ë³€í™”ëŠ” ì—†ìŠµë‹ˆë‹¤.\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=time['date'], y=time['confirmed'],\n                    mode='lines', \n                    name='í™•ì§„(confirmed)'))\n\nfig.add_trace(go.Scatter(x=time['date'],y=time['negative'],\n             mode='lines', name='ìŒì„±(Negative)'))\n\nfig.add_trace(go.Scatter(x=time['date'],y=time['test'],\n             mode='markers', name='ê²€ì‚¬(Test)'))\n\nfig.update_layout(title='ì‹œê°„ì˜ íë¦„ì— ë”°ë¥¸ ì½”ë¡œë‚˜ì˜ ê²€ì‚¬ ì¶”ì´',\n                   xaxis_title='Date',\n                   yaxis_title='Number')\n\nfig.show()\n\n\n                                                \n\n\ní•´ë‹¹ ê·¸ë˜í”„ëŠ” ì‹œê°„ì˜ íë¦„ì— ë”°ë¥¸ ì½”ë¡œë‚˜ì˜ ê²€ì‚¬ ì¶”ì´ë¡œ ê²€ì‚¬ìˆ˜ì™€ ìŒì„±ì˜ ìˆ˜ê°€ ê±°ì˜ ë¶™ì–´ì„œ ìš°ìƒí–¥í•˜ëŠ” ëª¨ìŠµì´ê³  í™•ì§„ìëŠ” ì´ì— ë¹„í•´ ë³€ë™ì´ ì—†ì–´ ë³´ì´ëŠ” ëª¨ìŠµì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê²€ì‚¬ë¥¼ ë§ì´ í–ˆì§€ë§Œ ì´ì— ë¹„í•´ì„œ í™•ì§„ì´ ëœ ì •ë„ëŠ” ìƒë‹¹íˆ ì ìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nfig = go.Figure()\nfig.add_trace(go.Bar(x=time['date'],y=time['confirmed'].diff(), \n                     name='confirmed', marker_color='rgba(152, 0, 0, .8)'))\n\nfig.update_layout(title='ì¼ë‹¨ìœ„ í™•ì§„ì ìˆ˜',\n                   xaxis_title='Date',\n                   yaxis_title='Number')\n\nfig.show()\n\n\n                                                \n\n\në‹¤ìŒì€ ì¼ë‹¨ìœ„ í™•ì§„ìì˜ ìˆ˜ë¥¼ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤. 20ë…„ 3ì›” ì¸ê·¼ì—ì„œ 800ì—¬ëª… ê¹Œì§€ ì¼ì¼ í™•ì§„ë˜ëŠ” ëª¨ìŠµì„ ë³´ì´ê³  ì ì°¨ ê°ì†Œí•˜ëŠ” ëª¨ìŠµì„ ë³´ì´ëŠ” í˜•íƒœì…ë‹ˆë‹¤\n\n\nì£¼ì œ1 - ì—°ë ¹ëŒ€ë³„ í™•ì§„ ë¹„ìœ¨ \n\ndisplay(timeage.head()) #timeage ë°ì´í„°ì…‹ì˜ ê¸°ë³¸ì ì¸ í˜•íƒœë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•´ ìƒìœ„ 5ê°œì˜ í–‰ë§Œ ì¶”ì¶œ\ndisplay(timeage.age.unique()) #timeage ë°ì´í„°ì…‹ì—ì„œ age ì»¬ëŸ¼ì—ì„œ ì–´ë–¤ ì—°ë ¹ì¸µì´ ìˆëŠ”ì§€ unique í•¨ìˆ˜ë¥¼ í†µí•´ì„œ ì¶”ì¶œ\n\n\n\n\n\n  \n    \n      \n      date\n      time\n      age\n      confirmed\n      deceased\n    \n  \n  \n    \n      0\n      2020-03-02\n      0\n      0s\n      32\n      0\n    \n    \n      1\n      2020-03-02\n      0\n      10s\n      169\n      0\n    \n    \n      2\n      2020-03-02\n      0\n      20s\n      1235\n      0\n    \n    \n      3\n      2020-03-02\n      0\n      30s\n      506\n      1\n    \n    \n      4\n      2020-03-02\n      0\n      40s\n      633\n      1\n    \n  \n\n\n\n\narray(['0s', '10s', '20s', '30s', '40s', '50s', '60s', '70s', '80s'],\n      dtype=object)\n\n\n\nage_list = timeage.age.unique()\nage_list #ì•ì—ì„œ ì„¤ëª…í•œ ì—°ë ¹ëŒ€ë¥¼ ë”°ë¡œ ì¶”ì¶œí•˜ì—¬ age_listë¼ëŠ” ê³³ì— í• ë‹¹ì„ ì‹œí‚´ \n\narray(['0s', '10s', '20s', '30s', '40s', '50s', '60s', '70s', '80s'],\n      dtype=object)\n\n\n\nfig, ax = plt.subplots(figsize = (13,7)) #ë„í™”ì§€(Figure : fig)ë¥¼ ê¹”ê³  ê·¸ë˜í”„ë¥¼ ê·¸ë¦´ êµ¬ì—­(Axes : ax)ì„ ì •ì˜í•©ë‹ˆë‹¤. figsizeë¥¼ í†µí•´ì„œ ë„í™”ì§€ì˜ í¬ê¸°ë¥¼ ì§€ì •í•´ì¤€ë‹¤\n#ì´ëŠ” objection oriented APIìœ¼ë¡œ ê·¸ë˜í”„ì˜ ê° ë¶€ë¶„ì„ ê°ì²´ë¡œ ì§€ì •í•˜ê³  ê·¸ë¦¬ëŠ” ìœ í˜•ì´ë‹¤\nsns.barplot(age_list,timeage.confirmed[-9:])\nax.set_xlabel('age',size=13) #ì—°ë ¹\nax.set_ylabel('number of case',size=13) #ì¼€ì´ìŠ¤ì˜ íšŸìˆ˜\nplt.title('Confirmed Cases by Age')\nplt.show()\n\nc:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning:\n\nPass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n\n\n\n\n\n\nì—°ë ¹ëŒ€ë³„ë¡œ ë¶„ì„ì„ í•´ë³¸ê²°ê³¼ 20ëŒ€ê°€ ì••ë„ì ìœ¼ë¡œ ë§ì€ ìˆ˜ë¥¼ ì°¨ì§€í•˜ê³  ìˆëŠ” í˜•íƒœì˜ í”Œëì„ ë³¼ ìˆ˜ê°€ ìˆë‹¤\n2020ë…„ ì—°ë ¹ëŒ€ë³„ ì¸êµ¬\n\nhttps://kosis.kr/visual/populationKorea/experienceYard/populationPyramid.do?mb=N&menuId=M_3_2\n\ní•´ë‹¹ ìë£Œë¥¼ ì´ìš©í•´ì„œ ì¸êµ¬ìˆ˜ì™€ í™•ì§„ ë¹„ìœ¨ì„ í™•ì¸í•˜ì—¬ ê³¼ì—° 20ëŒ€ê°€ ì¸êµ¬ìˆ˜ê°€ ë§ì•„ì„œ ì´ë ‡ê²Œ ë§ì´ í™•ì§„ì´ ë˜ì—ˆëŠ”ê°€ì— ëŒ€í•´ì„œ ì•Œì•„ë³¸ë‹¤\n\nage_order = pd.DataFrame() #ë¹ˆ ë°ì´í„° í”„ë ˆì„ì„ ìƒì„±\nage_order['age']  = age_list #ì•ì„œ ìƒì„±í•œ age_listë¥¼ ìƒˆë¡œ ë§Œë“œëŠ” ë°ì´í„° í”„ë ˆì„ì— ageë¼ëŠ” ì´ë¦„ì˜ ì»¬ëŸ¼ìœ¼ë¡œ í• ë‹¹\nage_order['population'] = [4054901, 4769187, 7037893, 7174782, 8257903, 8575336, 6476602, 3598811, 1657942] #populationì´ë¼ëŠ” ì»¬ëŸ¼ì— í†µê³„ì²­ í™ˆí˜ì´ì§€ì—ì„œ í™•ì¸í•œ ê°’ì„ ì…ë ¥\nage_order['proportion'] = round(age_order['population']/sum(age_order['population'])*100,2) \n#ì¸êµ¬ ë¹„ìœ¨ì„ êµ¬í•˜ê¸° ìœ„í•´ ëª¨ë“  ì¸êµ¬ìˆ˜ë¥¼ ë”í•˜ê³  ê° ì—°ë ¹ë³„ë¡œ ë‚˜ëˆ„ê³  ì†Œìˆ˜ì ìœ¼ë¡œ ë‚˜ì˜¤ëŠ”ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ 100ì„ ê³±í•˜ê³  ì†Œìˆ˜ì  2ë²ˆì§¸ ìë¦¬ê¹Œì§€ í‘œí˜„ì´ ë˜ë„ë¡ ì„¤ì •\nage_order = age_order.sort_values('age') #ageë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¬ì •ë ¬\nage_order.set_index(np.arange(1,10),inplace=True) #ì¸ë±ìŠ¤ì˜ ì„¤ì •ì„ 1~10ìˆœìœ¼ë¡œ ë“¤ì–´ê°€ë„ë¡ ì„¤ì •í•˜ê³  ë³¸ë˜ ìˆë˜ê²ƒì€ ëŒ€ì²´í•´ì„œ ì‚¬ìš©í•˜ë„ë¡\nage_order\n\n\n\n\n\n  \n    \n      \n      age\n      population\n      proportion\n    \n  \n  \n    \n      1\n      0s\n      4054901\n      7.86\n    \n    \n      2\n      10s\n      4769187\n      9.24\n    \n    \n      3\n      20s\n      7037893\n      13.64\n    \n    \n      4\n      30s\n      7174782\n      13.90\n    \n    \n      5\n      40s\n      8257903\n      16.00\n    \n    \n      6\n      50s\n      8575336\n      16.62\n    \n    \n      7\n      60s\n      6476602\n      12.55\n    \n    \n      8\n      70s\n      3598811\n      6.97\n    \n    \n      9\n      80s\n      1657942\n      3.21\n    \n  \n\n\n\n\n\nfig, ax = plt.subplots(figsize=(13, 7)) \nplt.title('Korea Age Proportion', fontsize=17)\nsns.barplot(age_list, age_order.proportion[-9:])\nax.set_xlabel('Age', size=13)\nax.set_ylabel('Rate (%)', size=13)\nplt.show() \n#í•œêµ­ì˜ 2020ë…„ ì—°ë ¹ë³„ ì¸êµ¬ì˜ ìˆ˜ë¥¼ ë‚˜íƒ€ë‚¸ í‘œì´ë‹¤ \n\nc:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning:\n\nPass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n\n\n\n\n\n\nì˜ˆìƒê³¼ëŠ” ë‹¤ë¥´ê²Œ 20ëŒ€ê°€ ê°€ì¥ ë§ì€ ì¸êµ¬ìˆ˜ë¥¼ ê°€ì§€ê³  ìˆëŠ” ì—°ë ¹ëŒ€ê°€ ì•„ë‹Œ 40,50ëŒ€ê°€ ê°€ì¥ ì¸êµ¬ìˆ˜ê°€ ë§ì€ ì—°ë ¹ëŒ€ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ë¡œ 20ëŒ€ ì¸êµ¬ìˆ˜ê°€ ë‹¤ë¥¸ ì—°ë ¹ì¸µì— ë¹„í•´ ë§ê¸° ë•Œë¬¸ì— í™•ì§„ì´ ë§ì´ ëœê²ƒì€ ì•„ë‹ˆë‹¤.\n\nconfirmed_by_population = age_order.sort_values('age') #'age'ë¼ëŠ” ì»¬ëŸ¼ìœ¼ë¡œ ì •ë ¬\nconfirmed_by_population['confirmed'] = list(timeage[-9:].confirmed) #confirmedë¼ëŠ” ì»¬ëŸ¼ì„ ë§Œë“¤ê³  timeageì˜ í•´ë‹¹ ë¦¬ìŠ¤íŠ¸ë¥¼ í• ë‹¹ ì‹œí‚´\n\n\nconfirmed_by_population['confirmed_ratio'] = confirmed_by_population['confirmed']/confirmed_by_population['population'] *100 #ì¸êµ¬ë¹„ìœ¨ì— ë”°ë¥¸ í™•ì§„ ë¹„ìœ¨ ì»¬ëŸ¼ ì¶”ê°€\ndisplay(confirmed_by_population)\n\n\n\n\n\n  \n    \n      \n      age\n      population\n      proportion\n      confirmed\n      confirmed_ratio\n    \n  \n  \n    \n      1\n      0s\n      4054901\n      7.86\n      193\n      0.004760\n    \n    \n      2\n      10s\n      4769187\n      9.24\n      708\n      0.014845\n    \n    \n      3\n      20s\n      7037893\n      13.64\n      3362\n      0.047770\n    \n    \n      4\n      30s\n      7174782\n      13.90\n      1496\n      0.020851\n    \n    \n      5\n      40s\n      8257903\n      16.00\n      1681\n      0.020356\n    \n    \n      6\n      50s\n      8575336\n      16.62\n      2286\n      0.026658\n    \n    \n      7\n      60s\n      6476602\n      12.55\n      1668\n      0.025754\n    \n    \n      8\n      70s\n      3598811\n      6.97\n      850\n      0.023619\n    \n    \n      9\n      80s\n      1657942\n      3.21\n      556\n      0.033536\n    \n  \n\n\n\n\n\n## 3. Plot confirmed rate by age\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Population-adjusted Confirmed Rate by Age', fontsize=17)\nsns.barplot(age_list, confirmed_by_population.confirmed_ratio[-9:])\nax.set_xlabel('Age', size=13)\nax.set_ylabel('Confirmed rate (%)', size=13)\nplt.show() #ì¸êµ¬ ë¹„ìœ¨ì— ë”°ë¥¸ í™•ì§„ í™•ë¥ \n\nc:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning:\n\nPass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n\n\n\n\n\n\nì¸êµ¬ ë¹„ìœ¨ì— ë”°ë¥¸ í™•ì§„ìì˜ ìˆ˜ë¥¼ ë³´ì•„ë„ 20ëŒ€ê°€ ì¸êµ¬ìˆ˜ê°€ ë§ì€ ì—°ë ¹ëŒ€ì¸ 40,50ëŒ€ ë³´ë‹¤ë„ í™•ì—°í•˜ê²Œ ë§ì€ ê²ƒì„ ì•Œ ìˆ˜ ìˆìœ¼ë©° ì˜¤íˆë ¤ 80ëŒ€ ì´ìƒì˜ ì—°ë ¹ëŒ€ê°€ ì°¨ì§€í•˜ëŠ” ë¹„ìœ¨ì´ ì¦ê°€í•˜ì˜€ë‹¤.\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(23, 7)) #1í–‰ 2ì—´ì˜ ë„í™”ì§€ë¥¼ ìƒì„±\n\n## 1. Confirmed Cases by Age\nax[0].set_title('Confirmed Cases by Age', fontsize=15)\nax[0].bar(age_list, confirmed_by_population.confirmed)\n\n## 2. Population-adjusted Confirmed Rate\nax[1].set_title('Population-adjusted Confirmed Rate', fontsize=15)\nax[1].bar(age_list, confirmed_by_population.confirmed_ratio)\n\nplt.show() \n\n\n\n\në‹¤ìŒ ë‘ê°œì˜ í”Œëì„ ë³´ë©´ ì•ì„  ê·¸ë˜í”„ì—ì„œëŠ” 20ëŒ€ì˜ í™•ì§„ í™•ë¥ ì´ ë‹¤ë¥¸ ì—°ë ¹ëŒ€ì— ë¹„í•´ì„œ ì•ë„ì ìœ¼ë¡œ ë†’ì•˜ì§€ë§Œ ì¸êµ¬ì˜ ë¹„ìœ¨ì— ë”°ë¥¸ í™•ì§„ì˜ ë¹„ìœ¨ì„ ë‚˜íƒ€ë‚´ëŠ” ë‘ë²ˆì§¸ í”Œëì„ ë³´ë©´ ì•„ì§ë„ 20ëŒ€ê°€ ë‹¤ë¥¸ ì—°ë ¹ëŒ€ì— ë¹„í•´ì„œ ë†’ê¸°ëŠ” í•˜ì§€ë§Œ ì²«ë²ˆì§¸ ê·¸ë˜í”„ì— ë¹„í•´ì„œëŠ” ì¡°ê¸ˆì€ ë‚®ì•„ì§„ ëª¨ìŠµê³¼ 60~80ëŒ€ê¹Œì§€ ì—°ë ¹ì¸µì˜ ë¹„ì¤‘ì´ ì¡°ê¸ˆì€ ì¦ê°€ í–ˆë‹¤ëŠ” ì‚¬ì‹¤ì„ ë‘ê°œì˜ í”Œëì„ ë¹„êµí•˜ë©´ì„œ ì•Œ ìˆ˜ìˆìŠµë‹ˆë‹¤ ë”°ë¼ì„œ ì €ëŠ” ë‹¤ë¥¸ ì—°ë ¹ì— ë¹„í•´ ì••ë„ì ìœ¼ë¡œ ë§ì€ í™•ì§„ ë¹„ìœ¨ì„ ê°€ì§€ê³  ìˆëŠ” 20ëŒ€ì— ëŒ€í•´ì„œ ì§‘ì¤‘ì ìœ¼ë¡œ ë¶„ì„ì„ í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n\n\nì£¼ì œ1 - ì—°ë ¹ëŒ€ë³„ í™•ì§„ ê²½ë¡œ \n\npatientinfo.head()\n\n\n\n\n\n  \n    \n      \n      patient_id\n      sex\n      age\n      country\n      province\n      city\n      infection_case\n      infected_by\n      contact_number\n      symptom_onset_date\n      confirmed_date\n      released_date\n      deceased_date\n      state\n    \n  \n  \n    \n      0\n      1000000001\n      male\n      50s\n      Korea\n      Seoul\n      Gangseo-gu\n      overseas inflow\n      NaN\n      75\n      2020-01-22\n      2020-01-23\n      2020-02-05\n      NaN\n      released\n    \n    \n      1\n      1000000002\n      male\n      30s\n      Korea\n      Seoul\n      Jungnang-gu\n      overseas inflow\n      NaN\n      31\n      NaN\n      2020-01-30\n      2020-03-02\n      NaN\n      released\n    \n    \n      2\n      1000000003\n      male\n      50s\n      Korea\n      Seoul\n      Jongno-gu\n      contact with patient\n      2002000001\n      17\n      NaN\n      2020-01-30\n      2020-02-19\n      NaN\n      released\n    \n    \n      3\n      1000000004\n      male\n      20s\n      Korea\n      Seoul\n      Mapo-gu\n      overseas inflow\n      NaN\n      9\n      2020-01-26\n      2020-01-30\n      2020-02-15\n      NaN\n      released\n    \n    \n      4\n      1000000005\n      female\n      20s\n      Korea\n      Seoul\n      Seongbuk-gu\n      contact with patient\n      1000000002\n      2\n      NaN\n      2020-01-31\n      2020-02-24\n      NaN\n      released\n    \n  \n\n\n\n\n\npatientinfo['infection_case'] = patientinfo['infection_case'].astype(str).apply(lambda x:x.split()[0])\n#PatientInfo['infection_case']\ninfectionCase = patientinfo.pivot_table(index='infection_case',columns='age',\nvalues='patient_id',aggfunc=\"count\")\n#infectionCase\n#ì „ì²´ ê°ì—¼ ì¼€ì´ìŠ¤\npatientTotal = infectionCase.fillna(0).sum(axis=1)\npatientTotal = patientTotal.sort_values(ascending = False)[:5]\n# 20ëŒ€ ê°ì—¼ ì¼€ì´ìŠ¤\npatient20s = infectionCase['20s'].dropna()\npatient20sTop = patient20s.sort_values(ascending=False)[:5]\n\n\ndisplay(patientTotal)\ndisplay(patient20sTop)\n\ninfection_case\ncontact     1112.0\nnan          827.0\noverseas     653.0\netc          638.0\nGuro-gu      112.0\ndtype: float64\n\n\ninfection_case\noverseas       269.0\nnan            221.0\ncontact        172.0\netc            127.0\nShincheonji     41.0\nName: 20s, dtype: float64\n\n\n\nfig = go.Figure()\nfig.add_trace(go.Pie(labels=patientTotal.index, values=patientTotal.values,\n                     textinfo='label+percent'))\n\nfig.update_layout(title='Confirmed infection case Total AGe')\n\nfig.show()\n\n\n                                                \n\n\nì „ì²´ ì—°ë ¹ì¸¡ì˜ ê°ì—¼ì›ì¸ì— ëŒ€í•´ì„œ ë³¸ë‹¤ë©´ ì ‘ì´‰ì— ì˜í•œ í™•ì§„ì´ 33% í•´ì™¸ ì…êµ­ì´ 19% ê·¸ì™¸ nanê³¼ etcê°€ ê°ê° 24,19%ì˜ ë¹„ìœ¨ì„ ì°¨ì§€í•˜ê³  ìˆë‹¤\n\nfig = go.Figure()\nfig.add_trace(go.Pie(labels=patient20sTop.index, values=patient20sTop.values,\n                     textinfo='label+percent'))\n\nfig.update_layout(title='Confirmed infection case 20s AGe')\n\nfig.show()\n\n\n                                                \n\n\n20ëŒ€ ì—°ë ¹ì˜ ê·¸ë£¹ì€ ì „ì²´ì—°ë ¹ì— ë¹„í•´ í•´ì™¸ì…êµ­ê³¼ nanì´ ê°ê° 32 26%ë¥¼ ì°¨ì§€ í•˜ê³  ìˆë‹¤. ê·¸ëŸ¬ë‚˜ í•´ë‹¹ ë°ì´í„°ì—ëŠ” ì›ì¸ì„ ì•Œìˆ˜ ì—†ëŠ” nanë°ì´ê°€ ì „ì²´ì˜ 1/4ê°€ëŸ‰ì„ ì°¨ì§€ í•˜ê¸° ë•Œë¬¸ì— ì •í™•í•œ ë¶„ì„ì„ í•˜ê¸° ì–´ë µë‹¤\nê·¸ë ‡ë‹¤ë©´ í™•ì§„ëœ 20ëŒ€ê°€ ë§ì´ ëŒì•„ë‹¤ë‹Œ ì¥ì†Œì— ëŒ€í•´ì„œ patientinfo ë°ì´í„°ë¥¼ ì´ìš©í•´ì„œ ì°¾ì•„ë³´ê² ìŠµë‹ˆë‹¤\n\npatientroute = pd.read_csv(path+'PatientRoute.csv')\n\n\npatientroute.head()\n\n\n\n\n\n  \n    \n      \n      patient_id\n      global_num\n      date\n      province\n      city\n      type\n      latitude\n      longitude\n    \n  \n  \n    \n      0\n      1000000001\n      2.0\n      2020-01-22\n      Gyeonggi-do\n      Gimpo-si\n      airport\n      37.615246\n      126.715632\n    \n    \n      1\n      1000000001\n      2.0\n      2020-01-24\n      Seoul\n      Jung-gu\n      hospital\n      37.567241\n      127.005659\n    \n    \n      2\n      1000000002\n      5.0\n      2020-01-25\n      Seoul\n      Seongbuk-gu\n      etc\n      37.592560\n      127.017048\n    \n    \n      3\n      1000000002\n      5.0\n      2020-01-26\n      Seoul\n      Seongbuk-gu\n      store\n      37.591810\n      127.016822\n    \n    \n      4\n      1000000002\n      5.0\n      2020-01-26\n      Seoul\n      Seongdong-gu\n      public_transportation\n      37.563992\n      127.029534\n    \n  \n\n\n\n\n\npatientroute[['patient_id','date','type']] #í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ\n\n\n\n\n\n  \n    \n      \n      patient_id\n      date\n      type\n    \n  \n  \n    \n      0\n      1000000001\n      2020-01-22\n      airport\n    \n    \n      1\n      1000000001\n      2020-01-24\n      hospital\n    \n    \n      2\n      1000000002\n      2020-01-25\n      etc\n    \n    \n      3\n      1000000002\n      2020-01-26\n      store\n    \n    \n      4\n      1000000002\n      2020-01-26\n      public_transportation\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      6709\n      6100000090\n      2020-03-24\n      airport\n    \n    \n      6710\n      6100000090\n      2020-03-24\n      airport\n    \n    \n      6711\n      6100000090\n      2020-03-25\n      store\n    \n    \n      6712\n      6100000090\n      2020-03-25\n      hospital\n    \n    \n      6713\n      6100000090\n      2020-03-25\n      store\n    \n  \n\n6714 rows Ã— 3 columns\n\n\n\n\nplaces = patientroute.type.unique()\nsimproute = patientroute[['patient_id','date','type']]\nagedf = patientinfo[['patient_id','age']]\nsimproutewage = pd.merge(simproute,agedf,how='left')\nfiplot = simproutewage.set_index('type')\nfiplot_count = fiplot.groupby('type').count().patient_id.sort_values()\n\n\nfig = fiplot_count.iplot(asFigure = True, kind='bar')\nfig.show()\n\n\n                                                \n\n\ní•´ë‹¹ ê·¸ë˜í”„ëŠ” ì „ì²´ ì—°ë ¹ì˜ í™•ì§„ ì›ì¸ì— ëŒ€í•œ ê²ƒì„ ì¹´ìš´íŠ¸ ì‹œí‚¨ ê²°ê³¼ë¡œ etcì™€ hospitalì´ ê°€ì¥ ë§ì€ ê²ƒì„ ë³´ì´ë‚˜ ë³‘ì›ì€ í™•ì§„ìê°€ ì´ìƒì¦ì„¸ë¥¼ ëŠë¼ê³  ì°¾ì•„ê°€ëŠ” ë‹¹ì—°í•œ ê²½ë¡œì´ë¯€ë¡œ ì œì™¸ë¥¼ í•˜ê³  etc ë˜í•œ ì–´ëŠ ê³³ì— ë‹¤ë…€ì™”ëŠ”ì§€ ì •í™•í•˜ê²Œ ì•Œ ìˆ˜ ì—†ì–´ì„œ ì œì™¸ë¥¼ í•˜ê³  ë‹¤ì‹œ ì§„í–‰ì„ í•´ë³´ê² ìŠµë‹ˆë‹¤.\n\ntwtfi = fiplot[fiplot.age == '20s']\nuntwtfi = fiplot[fiplot.age != '20s']\ntwt = twtfi.groupby('type').count().patient_id\nuntwt = untwtfi.groupby('type').count().patient_id\ntwt = twt[~twt.index.isin( ['etc','hospital'])]\nuntwt = untwt[~untwt.index.isin( ['etc','hospital'])]\nfig = go.Figure()\nfig.add_trace(go.Bar(x = twt.index, \n                     y = twt,\n                     name = '20s',\n                     marker_color='indianred'))\nfig.add_trace(go.Bar(x = untwt.index, \n                     y = untwt,\n                     name = 'except 20s',\n                     marker_color='lightsalmon'))\nfig.update_layout(barmode='group', xaxis_tickangle=-45)\nfig.show()\n\n\n                                                \n\n\në‹¤ìŒ ê·¸ë˜í”„ëŠ” 20ëŒ€ì™€ ê·¸ì™¸ì˜ ì—°ë ¹ì¸µì´ í™•ì§„ëœ ì›ì¸ì— ëŒ€í•œê²ƒìœ¼ë¡œ ì•ì„œ ë§í•œê²ƒ ì²˜ëŸ¼ etcì™€ hospitalì€ ê°€ì¥ ë§ì€ ë¹„ìœ¨ì„ ì°¨ì§€ í•˜ì§€ë§Œ ë¶„ì„ì„ í•˜ëŠ”ë° í¬ê²Œ ë„ì›€ì´ ë˜ì§€ ì•ŠëŠ”ë‹¤ê³  íŒë‹¨ì„ í•˜ì—¬ ì œì™¸ë¥¼ í•˜ê³  ì§„í–‰ì„ í•œ ê²°ê³¼ ì´ë‹¤.\nê³µí†µì ìœ¼ë¡œ ë§ì´ ë°©ë¬¸í•˜ëŠ” storeê³¼ churchëŠ” ë¹„ìŠ·í•œ ì–‘ìƒì„ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, 20ëŒ€ëŠ” restaurant, pcë°©, cafe, bar ë“±ì—ì„œ í›¨ì”¬ ë§ì€ ë°©ë¬¸ë¹„ìœ¨ì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.\n\n\n\nì£¼ì œ2 - ì½”ë¡œë‚˜ëŠ” ëˆ„êµ¬ì—ê²Œ ê°€ì¥ ì¹˜ëª…ì ì¸ê°€ \n\nì£¼ì œ2 - ì—°ë ¹ë³„ë¡œ í™•ì§„ìì˜ ì¹˜ëª…ë¥  \n\ntime.head()\n\n\n\n\n\n  \n    \n      \n      date\n      time\n      test\n      negative\n      confirmed\n      released\n      deceased\n    \n  \n  \n    \n      0\n      2020-01-20\n      16\n      1\n      0\n      1\n      0\n      0\n    \n    \n      1\n      2020-01-21\n      16\n      1\n      0\n      1\n      0\n      0\n    \n    \n      2\n      2020-01-22\n      16\n      4\n      3\n      1\n      0\n      0\n    \n    \n      3\n      2020-01-23\n      16\n      22\n      21\n      1\n      0\n      0\n    \n    \n      4\n      2020-01-24\n      16\n      27\n      25\n      2\n      0\n      0\n    \n  \n\n\n\n\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=time['date'], y=time['deceased'],\n                    mode='lines', # Line plotë§Œ ê·¸ë¦¬ê¸°\n                    name='ì‚¬ë§(deceased)'))\n\nfig.update_layout(title='ì‹œê°„ì˜ íë¦„ì— ë”°ë¥¸ í™•ì§„ì ì‚¬ë§ ì¶”ì´',\n                   xaxis_title='Date',\n                   yaxis_title='Number')\n\nfig.show()\n\n\n                                                \n\n\ní•´ë‹¹ ê·¸ë˜í”„ëŠ” ì‹œê°„ì˜ íë¦„ì— ë”°ë¥¸ í™•ì§„ìì˜ ì‚¬ë§ ì¶”ì´ë¡œì„œ 2020ë…„ 3ì›” ë¶€í„° ê³„ì†í•´ì„œ ìš°ìƒí–¥í•˜ëŠ” ëª¨ìŠµì„ ë³¼ ìˆ˜ ìˆë‹¤\n\nfig, ax = plt.subplots(figsize = (13,7)) #ë„í™”ì§€(Figure : fig)ë¥¼ ê¹”ê³  ê·¸ë˜í”„ë¥¼ ê·¸ë¦´ êµ¬ì—­(Axes : ax)ì„ ì •ì˜í•©ë‹ˆë‹¤. figsizeë¥¼ í†µí•´ì„œ ë„í™”ì§€ì˜ í¬ê¸°ë¥¼ ì§€ì •í•´ì¤€ë‹¤\n#ì´ëŠ” objection oriented APIìœ¼ë¡œ ê·¸ë˜í”„ì˜ ê° ë¶€ë¶„ì„ ê°ì²´ë¡œ ì§€ì •í•˜ê³  ê·¸ë¦¬ëŠ” ìœ í˜•ì´ë‹¤\nsns.barplot(age_list,timeage.deceased[-9:])\nax.set_xlabel('age',size=13) #ì—°ë ¹\nax.set_ylabel('number of case',size=13) #ì¼€ì´ìŠ¤ì˜ íšŸìˆ˜\nplt.title('Deceased Cases by Age')\nplt.show()\n\nc:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning:\n\nPass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n\n\n\n\n\n\në‚˜ì´ê°€ ë§ì•„ì§ˆìˆ˜ë¡ ì‚¬ë§ìì˜ ë¹„ìœ¨ì´ ë†’ë‹¤ ê³¼ì—° ë‚˜ì´ê°€ ë§ì•„ì§ˆìˆ˜ë¡ ì¸êµ¬ìˆ˜ê°€ ë§ì•„ì ¸ì„œ ì´ëŸ¬í•œ í˜„ìƒì´ ë‚˜ì˜¤ëŠ” ì§€ ì¸êµ¬ ë¹„ìœ¨ì— ë”°ë¥¸ ì‚¬ë§ìì— ëŒ€í•´ì„œ ë‹¤ì‹œ í•œë²ˆ ì‚´í´ë³´ì\n\nconfirmed_by_population = age_order.sort_values('age')\nconfirmed_by_population['deceased'] = list(timeage[-9:].deceased)\n\n# 2. Get confirmed ratio regarding population\nconfirmed_by_population['deceased_ratio'] = confirmed_by_population['deceased']/confirmed_by_population['population'] *100\ndisplay(confirmed_by_population)\n\n\n\n\n\n  \n    \n      \n      age\n      population\n      proportion\n      deceased\n      deceased_ratio\n    \n  \n  \n    \n      1\n      0s\n      4054901\n      7.86\n      0\n      0.000000\n    \n    \n      2\n      10s\n      4769187\n      9.24\n      0\n      0.000000\n    \n    \n      3\n      20s\n      7037893\n      13.64\n      0\n      0.000000\n    \n    \n      4\n      30s\n      7174782\n      13.90\n      2\n      0.000028\n    \n    \n      5\n      40s\n      8257903\n      16.00\n      3\n      0.000036\n    \n    \n      6\n      50s\n      8575336\n      16.62\n      15\n      0.000175\n    \n    \n      7\n      60s\n      6476602\n      12.55\n      41\n      0.000633\n    \n    \n      8\n      70s\n      3598811\n      6.97\n      82\n      0.002279\n    \n    \n      9\n      80s\n      1657942\n      3.21\n      139\n      0.008384\n    \n  \n\n\n\n\n\n## 3. Plot confirmed rate by age\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Population-adjusted Deceased Rate by Age', fontsize=17)\nsns.barplot(age_list, confirmed_by_population.deceased_ratio[-9:])\nax.set_xlabel('Age', size=13)\nax.set_ylabel('Deceased rate (%)', size=13)\nplt.show() #ì¸êµ¬ ë¹„ìœ¨ì— ë”°ë¥¸ í™•ì§„ í™•ë¥ \n\nc:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning:\n\nPass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n\n\n\n\n\n\nì¸êµ¬ ë¹„ìœ¨ì— ë”°ë¥¸ ì‚¬ë§ìì˜ ë¹„ìœ¨ì„ ì‚´í´ë³¸ ê²°ê³¼ì´ë‹¤ 0~20ëŒ€ ê¹Œì§€ëŠ” ì‚¬ë§ìëŠ” ì—†ìœ¼ë©° 30ëŒ€ë¶€í„° í™•ì§„ìœ¼ë¡œ ì¸í•œ ì‚¬ë§ìê°€ ì¡´ì¬í•œë‹¤. ê·¸ëŸ¬ë‚˜ 80ëŒ€ ì´ìƒì˜ ì—°ë ¹ì¸µì˜ ê²½ìš°ëŠ” ê°€ì¥ ë§ì€ ì¸êµ¬ìˆ˜ë¥¼ ê°€ì§„ ì—°ë ¹ì¸µë„ ì•„ë‹ˆì§€ë§Œ ì‚¬ë§ìì˜ ë¹„ì¤‘ì´ ê°€ì¥ ë†’ì€ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ì´ë¡œ ì½”ë¡œë‚˜ ë°”ì´ëŸ¬ìŠ¤ëŠ” ê³ ì—°ë ¹ì¸µ ì¼ìˆ˜ë¡ ê°€ì¥ ì¹˜ëª…ì ì¸ ì§ˆë³‘ì„ì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤.\nê·¸ë ‡ë‹¤ë©´ ê³ ì—°ë ¹ì¸µì˜ í™•ì§„ ì›ì¸ì— ëŒ€í•´ì„œ ì•Œì•„ë³´ì\n\n\nì£¼ì œ2 - ì—°ë ¹ëŒ€ì˜ í™•ì§„ ì›ì¸ \n\naged_pat = patientinfo[(patientinfo['age'] == '60s')|(patientinfo['age'] == '70s')|\n                (patientinfo['age'] == '80s')][['province','age','infection_case']]\n                \naged_inf = pd.DataFrame(aged_pat['infection_case'].value_counts())\n#ê³ ì—°ë ¹ì¸¡ì˜ í™•ì§„ ì›ì¸\n\n\npatientinfo['infection_case'] = patientinfo['infection_case'].astype(str).apply(lambda x:x.split()[0])\n#PatientInfo['infection_case']\ninfectionCase = patientinfo.pivot_table(index='infection_case',columns='age',\nvalues='patient_id',aggfunc=\"count\")\n#infectionCase\n#ì „ì²´ ê°ì—¼ ì¼€ì´ìŠ¤\npatientTotal = infectionCase.fillna(0).sum(axis=1)\npatientTotal = patientTotal.sort_values(ascending = False)[:5]\n# 60ëŒ€ ê°ì—¼ ì¼€ì´ìŠ¤\npatient60s = infectionCase['60s'].dropna()\npatient60sTop = patient60s.sort_values(ascending=False)[:5]\n# 70ëŒ€ ê°ì—¼ ì¼€ì´ìŠ¤\npatient70s = infectionCase['70s'].dropna()\npatient70sTop = patient70s.sort_values(ascending=False)[:5]\n# 80ëŒ€ ê°ì—¼ ì¼€ì´ìŠ¤\npatient80s = infectionCase['80s'].dropna()\npatient80sTop = patient80s.sort_values(ascending=False)[:5]\n\n\nfig = go.Figure()\nfig.add_trace(go.Pie(labels=patient60sTop.index, values=patient60sTop.values,\n                     textinfo='label+percent'))\n\nfig.update_layout(title='Confirmed infection case 60s AGe')\n\nfig.show()\n\n\n                                                \n\n\n\nfig = go.Figure()\nfig.add_trace(go.Pie(labels=patient70sTop.index, values=patient70sTop.values,\n                     textinfo='label+percent'))\n\nfig.update_layout(title='Confirmed infection case 70s AGe')\n\nfig.show()\n\n\n                                                \n\n\n\nfig = go.Figure()\nfig.add_trace(go.Pie(labels=patient80sTop.index, values=patient80sTop.values,\n                     textinfo='label+percent'))\n\nfig.update_layout(title='Confirmed infection case 80s AGe')\n\nfig.show()\n\n\n                                                \n\n\n60,70,80ëŒ€ì˜ í™•ì§„ ì›ì¸ì„ ë³´ë‹ˆ nanê³¼ etcê°€ ë§ê¸°ëŠ” í•˜ì§€ë§Œ ë‹¤ë¥¸ ì—°ë ¹ëŒ€ì— ë¹„í•´ì„œ contactê°€ ë§ë‹¤ëŠ” ì‚¬ì‹¤ì„ ì•Œìˆ˜ ìˆë‹¤. ê·¸ë˜ë„ nanê³¼ etcê°€ ë§ì•„ í™•ì§„ì˜ ì£¼ìš” ì›ì¸ì„ ì ‘ì´‰ì— ì˜í•´ì„œ ë¼ê³  ë‹¨ì •í•  ìˆ˜ëŠ” ì—†ë‹¤\nê·¸ë ‡ë‹¤ë©´ ë‚˜ì´ê°€ ë§ì€ ì‚¬ëŒë“¤ì€ ì™„ì¹˜ ê¸°ê°„ì´ ê¸¸ì–´ì„œ ì¹˜ëª…ë¥ ì´ ë†’ì€ ê²ƒì´ì§€ ì´ì— ëŒ€í•œ ê´€ê³„ì— ëŒ€í•´ ì•Œì•„ë³´ì•˜ìŠµë‹ˆë‹¤\n\n\nì£¼ì œ2 - ì—°ë ¹ëŒ€ë³„ íšŒë³µê¸°ê°„ \n\npatientinfo\n\n\n\n\n\n  \n    \n      \n      patient_id\n      sex\n      age\n      country\n      province\n      city\n      infection_case\n      infected_by\n      contact_number\n      symptom_onset_date\n      confirmed_date\n      released_date\n      deceased_date\n      state\n    \n  \n  \n    \n      0\n      1000000001\n      male\n      50s\n      Korea\n      Seoul\n      Gangseo-gu\n      overseas\n      NaN\n      75\n      2020-01-22\n      2020-01-23\n      2020-02-05\n      NaN\n      released\n    \n    \n      1\n      1000000002\n      male\n      30s\n      Korea\n      Seoul\n      Jungnang-gu\n      overseas\n      NaN\n      31\n      NaN\n      2020-01-30\n      2020-03-02\n      NaN\n      released\n    \n    \n      2\n      1000000003\n      male\n      50s\n      Korea\n      Seoul\n      Jongno-gu\n      contact\n      2002000001\n      17\n      NaN\n      2020-01-30\n      2020-02-19\n      NaN\n      released\n    \n    \n      3\n      1000000004\n      male\n      20s\n      Korea\n      Seoul\n      Mapo-gu\n      overseas\n      NaN\n      9\n      2020-01-26\n      2020-01-30\n      2020-02-15\n      NaN\n      released\n    \n    \n      4\n      1000000005\n      female\n      20s\n      Korea\n      Seoul\n      Seongbuk-gu\n      contact\n      1000000002\n      2\n      NaN\n      2020-01-31\n      2020-02-24\n      NaN\n      released\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      5160\n      7000000015\n      female\n      30s\n      Korea\n      Jeju-do\n      Jeju-do\n      overseas\n      NaN\n      25\n      NaN\n      2020-05-30\n      2020-06-13\n      NaN\n      released\n    \n    \n      5161\n      7000000016\n      NaN\n      NaN\n      Korea\n      Jeju-do\n      Jeju-do\n      overseas\n      NaN\n      NaN\n      NaN\n      2020-06-16\n      2020-06-24\n      NaN\n      released\n    \n    \n      5162\n      7000000017\n      NaN\n      NaN\n      Bangladesh\n      Jeju-do\n      Jeju-do\n      overseas\n      NaN\n      72\n      NaN\n      2020-06-18\n      NaN\n      NaN\n      isolated\n    \n    \n      5163\n      7000000018\n      NaN\n      NaN\n      Bangladesh\n      Jeju-do\n      Jeju-do\n      overseas\n      NaN\n      NaN\n      NaN\n      2020-06-18\n      NaN\n      NaN\n      isolated\n    \n    \n      5164\n      7000000019\n      NaN\n      NaN\n      Bangladesh\n      Jeju-do\n      Jeju-do\n      overseas\n      NaN\n      NaN\n      NaN\n      2020-06-18\n      NaN\n      NaN\n      isolated\n    \n  \n\n5165 rows Ã— 14 columns\n\n\n\n\nfrom datetime import datetime\n\n\npat_rel = patientinfo[['age','confirmed_date','released_date']]\n#pat_rel['diff'] = pat_rel.released_date - pat_rel.confirmed_date\nstr(pat_rel.confirmed_date)\npat_rel.released_date = pd.to_datetime(pat_rel['released_date'], format='%Y %m %d')\npat_rel.confirmed_date = pd.to_datetime(pat_rel['confirmed_date'], format='%Y %m %d')\npat_rel['diff'] = pat_rel.released_date - pat_rel.confirmed_date\npat_rel = pat_rel[:5161] #ê²©ë¦¬ ë‚ ì§œ ì—†ëŠ” ê²ƒ ì‚­ì œ\ndisplay(pat_rel)\n\n\n\n\n\n  \n    \n      \n      age\n      confirmed_date\n      released_date\n      diff\n    \n  \n  \n    \n      0\n      50s\n      2020-01-23\n      2020-02-05\n      13 days\n    \n    \n      1\n      30s\n      2020-01-30\n      2020-03-02\n      32 days\n    \n    \n      2\n      50s\n      2020-01-30\n      2020-02-19\n      20 days\n    \n    \n      3\n      20s\n      2020-01-30\n      2020-02-15\n      16 days\n    \n    \n      4\n      20s\n      2020-01-31\n      2020-02-24\n      24 days\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      5156\n      30s\n      2020-04-03\n      2020-05-19\n      46 days\n    \n    \n      5157\n      20s\n      2020-04-03\n      2020-05-05\n      32 days\n    \n    \n      5158\n      10s\n      2020-04-14\n      2020-04-26\n      12 days\n    \n    \n      5159\n      30s\n      2020-05-09\n      2020-06-12\n      34 days\n    \n    \n      5160\n      30s\n      2020-05-30\n      2020-06-13\n      14 days\n    \n  \n\n5161 rows Ã— 4 columns\n\n\n\n\ndisplay(pat_rel['diff'].mean()) \ndisplay(pat_rel['diff'].min()) \ndisplay(pat_rel['diff'].max()) \n\nTimedelta('24 days 17:48:39.041614123')\n\n\nTimedelta('0 days 00:00:00')\n\n\nTimedelta('114 days 00:00:00')\n\n\ní‰ê·  ì™„ì¹˜ì¼ì€ 24ì¼\nìµœëŒ€ ì™„ì¹˜ì¼ì€ 114ì¼ ì…ë‹ˆë‹¤.\n\npat_rel['over_avg'] = np.where(pat_rel['diff']>'24 days 17:48:39.041614123',1,0)\nover_av_released = pat_rel[pat_rel['over_avg']==1]\nunder_av_released = pat_rel[pat_rel['over_avg']==0]\n\nover_av=pd.DataFrame(over_av_released['age'].value_counts().sort_index()).reset_index()\nunder_av=pd.DataFrame(under_av_released['age'].value_counts().sort_index()).reset_index()\n\n#ì—°ë ¹ëŒ€ì¸µë³„ë¡œ ê°ì—¼ììˆ˜ê°€ í™•ì—°íˆ ë‹¤ë¥´ê¸°ë•Œë¬¸ì— ê° ì—°ë ¹ì¸µë³„ì˜ ë¹„ìœ¨ë¡œ ê³„ì‚°\nunder_av['per']=under_av['age']/(under_av['age']+over_av['age']) \nover_av['per']=over_av['age']/(under_av['age']+over_av['age'])\n\n#ì»¬ëŸ¼ ì¬ì •ë¦¬\nunder_av.columns=['age', 'count', 'under_per']\nover_av.columns=['age', 'count', 'over_per']\n\n\nover_av = pd.DataFrame({'age':['0s','10s','20s','30s','40s','50s','60s','70s','80s','90s'],\n                             'count':[7,20,156,90,86,119,90,46,39,8],\n                             'over_per':[0.106061,0.006289,0.026212,0.264856,0.172414,0.135647,0.232877,0.326087,0.2598887,0.487500]})\n\n\nfig = go.Figure()\n\nfig.add_trace(go.Bar(x=under_av.under_per, y=under_av.age, name='ë¹ ë¥¸ ì™„ì¹˜ ê¸°ê°„',\n                     orientation='h'))\n\nfig.add_trace(go.Bar(x=over_av.over_per, y=over_av.age, name='ì˜¤ëœ ì™„ì¹˜ ê¸°ê°„',\n                     text=over_av.over_per, texttemplate='%{x:.1%}', textposition='inside',\n                    textfont=dict(color='white'),\n                    orientation='h'))\nfig.update_layout(barmode='stack',\n                  paper_bgcolor='rgb(248, 248, 255)',\n                  plot_bgcolor='rgb(248, 248, 255)',\n                 )\nfig.update_layout(title='ì—°ë ¹ëŒ€ì— ë”°ë¥¸ íšŒë³µ ê¸°ê°„')\n\nfig.show()\n\n\n                                                \n\n\nê·¸ë˜í”„ë¥¼ ë³¸ë‹¤ë©´ 0~20ëŒ€ì˜ ì—°ë ¹ì¸ ì€ 10% ë¯¸ë§Œìœ¼ë¡œ í‰ê· ë³´ë‹¤ ë¹ ë¥¸ ì™„ì¹˜ ê¸°ê°„ì„ ê°€ì§€ê³  ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê³ ì—°ë ¹ì¸µì¸ 70,80,90ëŒ€ì˜ ê²½ìš° 32,26,48%ë¡œ ë‹¤ë¥¸ ì—°ë ¹ì¸µì— ë¹„í•´ì„œ ë†’ê¸°ëŠ” í•˜ì§€ë§Œ ê³¼ë°˜ì„ ë„˜ì§€ ì•Šê¸°ì— ê³ ì—°ë ¹ì¸µì´ë¼ê³  ëª¨ë‘ê°€ ì¥ê¸°ê°„ì˜ íšŒë³µ ê¸°ê°„ì„ ê°€ì§„ë‹¤ê³  íŒë‹¨í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.\n\n\n\nì£¼ì œ3 - ì •ë¶€ì˜ ì •ì±…ì€ íƒ€ë‹¹í–ˆëŠ”ê°€? \n\nì£¼ì œ3 - ê°ì—¼ë³‘ ê²½ë³´ ë‹¨ê³„ ê³µí‘œ ì‹œì  \n\npolicy.head()\n\n\n\n\n\n  \n    \n      \n      policy_id\n      country\n      type\n      gov_policy\n      detail\n      start_date\n      end_date\n    \n  \n  \n    \n      0\n      1\n      Korea\n      Alert\n      Infectious Disease Alert Level\n      Level 1 (Blue)\n      2020-01-03\n      2020-01-19\n    \n    \n      1\n      2\n      Korea\n      Alert\n      Infectious Disease Alert Level\n      Level 2 (Yellow)\n      2020-01-20\n      2020-01-27\n    \n    \n      2\n      3\n      Korea\n      Alert\n      Infectious Disease Alert Level\n      Level 3 (Orange)\n      2020-01-28\n      2020-02-22\n    \n    \n      3\n      4\n      Korea\n      Alert\n      Infectious Disease Alert Level\n      Level 4 (Red)\n      2020-02-23\n      NaN\n    \n    \n      4\n      5\n      Korea\n      Immigration\n      Special Immigration Procedure\n      from China\n      2020-02-04\n      NaN\n    \n  \n\n\n\n\n\npolicy.isna().sum() #ì—¬ê¸°ì„œ end_dateì˜ NAê°’ì´ ë„ˆë¬´ ë§ì€ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ê¸°ì ì„ start_dateë¡œ ì§€ì •í•˜ê³  ì‚¬ìš©ì„ í•˜ê² ë‹¤.\n\npolicy_id      0\ncountry        0\ntype           0\ngov_policy     0\ndetail         2\nstart_date     0\nend_date      37\ndtype: int64\n\n\n\npolicy_alerts = policy[policy.type == 'Alert']\ndisplay(policy_alerts)\n\n\n\n\n\n  \n    \n      \n      policy_id\n      country\n      type\n      gov_policy\n      detail\n      start_date\n      end_date\n    \n  \n  \n    \n      0\n      1\n      Korea\n      Alert\n      Infectious Disease Alert Level\n      Level 1 (Blue)\n      2020-01-03\n      2020-01-19\n    \n    \n      1\n      2\n      Korea\n      Alert\n      Infectious Disease Alert Level\n      Level 2 (Yellow)\n      2020-01-20\n      2020-01-27\n    \n    \n      2\n      3\n      Korea\n      Alert\n      Infectious Disease Alert Level\n      Level 3 (Orange)\n      2020-01-28\n      2020-02-22\n    \n    \n      3\n      4\n      Korea\n      Alert\n      Infectious Disease Alert Level\n      Level 4 (Red)\n      2020-02-23\n      NaN\n    \n  \n\n\n\n\n\nfig, ax = plt.subplots(figsize=(13,7))\nplt.title(\"Infection Disease Alert Level\",size=17)\nplt.plot(time.date.unique(), time.confirmed.diff(),\n         color = 'gray', lw = 3)\nax.set_xticks(ax.get_xticks()[::int(len(time.date.unique())/8)])\n\nfor day, color in zip(policy_alerts.start_date.values[1:], ['yellow','orange','red']):\n    ax.axvline(day, ls=':', color=color, lw=4)\nax.legend(['confirmed','alert level 2','alert level 3','alert level 4'])\nplt.xlabel('date')\nplt.ylabel('Number of Confirmed')\nplt.show()\n\n\n\n\në‹¤ìŒì€ ê°ì—¼ë³‘ì˜ ê²½ë³´ ë‹¨ê³„ ë³„ë¡œ í•´ë‹¹ ì‹œì ê³¼ í™•ì§„ìì˜ ì¼ì¼ ì¶”ì´ë¥¼ ë‚˜íƒ€ë‚¸ ê²ƒìœ¼ë¡œ 2,3ë‹¨ê³„ëŠ” ë°œìƒì„ í•˜ê³  êµ­ë‚´ì— ë“¤ì–´ì˜¨ ì‹œì ì— ê³µí‘œê°€ ë˜ì—ˆê³  ê°€ì¥ ê°•ë ¥í•œ ë‹¨ê³„ì¸ 4ë‹¨ê³„ëŠ” ì¼ì¼ í™•ì§„ìê°€ ì •ì ì— ì´ë¥´ê¸° ì „ì— ê³µí‘œê°€ ëœ ì‚¬ì‹¤ì„ ì•Œìˆ˜ ìˆìŠµë‹ˆë‹¤.\nê·¸ë ‡ë‹¤ë©´ ì •ë¶€ì˜ ê±°ë¦¬ë‘ê¸°ëŠ” ì–´ë– í•˜ì˜€ëŠ”ì§€ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.\n\n\nì£¼ì œ3 - ì •ë¶€ì˜ ê±°ë¦¬ë‘ê¸° ê³µí‘œ ì‹œì  \n\npolicy_social = policy[policy.type == 'Social'][:-1]\ndisplay(policy_social)\n\n\n\n\n\n  \n    \n      \n      policy_id\n      country\n      type\n      gov_policy\n      detail\n      start_date\n      end_date\n    \n  \n  \n    \n      28\n      29\n      Korea\n      Social\n      Social Distancing Campaign\n      Strong\n      2020-02-29\n      2020-03-21\n    \n    \n      29\n      30\n      Korea\n      Social\n      Social Distancing Campaign\n      Strong\n      2020-03-22\n      2020-04-19\n    \n    \n      30\n      31\n      Korea\n      Social\n      Social Distancing Campaign\n      Weak\n      2020-04-20\n      2020-05-05\n    \n    \n      31\n      32\n      Korea\n      Social\n      Social Distancing Campaign\n      Weak(1st)\n      2020-05-06\n      NaN\n    \n  \n\n\n\n\n\nfig, ax = plt.subplots(figsize=(13,7))\nplt.title(\"Social Distancing Campaign\",size=17)\nplt.plot(time.date.unique(), time.confirmed.diff(),\n         color = 'gray', lw = 3)\nax.set_xticks(ax.get_xticks()[::int(len(time.date.unique())/8)])\n\nfor day, color in zip(policy_social.start_date.values[:], ['red','red','orange']):\n    ax.axvline(day, ls=':', color=color, lw=4)\nax.legend(['confirmed','strong','strong','weak'])\nplt.xlabel('date')\nplt.ylabel('Number of Confirmed')\nplt.show()\n\n\n\n\n3ì›” 23ì¼ì— ê°ì—¼ë³‘ ê²½ê³ ê°€ 4ë‹¨ê³„ ë°œí‘œ ë˜ê³  ë‚˜ì„œ 3ì›” 29ì¼ë¶€í„° ì‚¬íšŒì  ê±°ë¦¬ë‘ê¸°ë¥¼ ì§„í–‰ í•˜ì˜€ìŠµë‹ˆë‹¤. ë‘ë²ˆì˜ ê°•í•œ ê±°ë¦¬ë‘ê¸°ë¥¼ ìœ ì§€í•˜ê³  4ì›” 20ì¼ë¶€í„° ê±°ë¦¬ë‘ê¸°ì˜ ë‹¨ê³„ë¥¼ ë‚®ì·„ìŠµë‹ˆë‹¤. ê±°ë¦¬ë‘ê¸°ë¥¼ í™•ì§„ìì˜ ì •ì ì— ë‹¤ë‹¤ë¥´ëŠ” ì‹œì ì—ì„œ ë°œí‘œí•˜ê³  ê·¸ ì´í›„ ê°ì†Œí•˜ëŠ” ì¼ì¼ í™•ì§„ìì˜ ìˆ˜ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤ ë”°ë¼ì„œ ë¶„ì„ì„ í•˜ê³  ìˆëŠ” í•´ë‹¹ ê¸°ê°„ ë™ì•ˆì€ ì •ë¶€ì˜ ë°©ì—­ ì •ì±…ì€ ì„±ê³µì„ í–ˆë‹¤ê³  í•  ìˆ˜ ìˆë‹¤\n\n\n\n\nê²°ë¡  \n\n20ëŒ€ê°€ ë¶„ì„ ê¸°ê°„ë™ì•ˆ ê°€ì¥ ë§ì´ í™•ì§„ì´ ëœ ì—°ë ¹ì¸µì´ì—ˆìœ¼ë©°, 20ëŒ€ì˜ ì¸êµ¬ìˆ˜ê°€ ë‹¤ë¥¸ ì—°ë ¹ì¸µì— ë¹„í•´ì„œ ë§ì•„ì„œê°€ ì•„ë‹Œ ë‹¤ë¥¸ ì—°ë ¹ì¸µì— ë¹„í•´ì„œ í™œë™ ë°˜ê²½ì´ ë„“ê³  í™œë°œí•˜ë©° ë¶ˆí•„ìš”í•œ ë°©ë¬¸ ì§€ì—­ê³¼ ì¥ì†Œì— ìì£¼ ë°©ë¬¸ì„ í•˜ì˜€ê¸° ë•Œë¬¸ì— 20ëŒ€ê°€ ê°€ì¥ ë§ì´ í™•ì§„ëœ ì—°ë ¹ì¸µì´ë¼ëŠ” ê²°ë¡ ì„ ë‚´ë ¸ìŠµë‹ˆë‹¤.\nì½”ë¡œë‚˜ ë°”ì´ëŸ¬ìŠ¤ëŠ” ê³ ì—°ë ¹ì¸µì´ ë ìˆ˜ë¡ ë”ìš± ì¹˜ëª…ì ì´ë¼ëŠ” ì‚¬ì‹¤ì„ ì–»ì—ˆìŠµë‹ˆë‹¤. ì¹˜ëª…ë¥ ì´ ì¸êµ¬ ëŒ€ë¹„ ë¹„ìœ¨ë¡œ ì‚´í´ë³´ì•„ë„ 80ëŒ€ ì´ìƒì´ ê°€ì¥ ì••ë„ì ìœ¼ë¡œ ë§ì•˜ê³  60,70ëŒ€ ë˜í•œ ì ì§€ ì•Šì€ ì¹˜ëª…ë¥ ì„ ê°€ì§€ê³  ìˆìŒì„ ì•Œê²Œ ë˜ì—ˆìœ¼ë©°, ì™„ì¹˜ê¹Œì§€ ê±¸ë¦¬ëŠ” ê¸°ê°„ì€ ê³ ì—°ë ¹ì¸µì¼ìˆ˜ë¡ ì €ì—°ë ¹ì¸µì— ë¹„í•´ì„œ í‰ê·  ì´ìƒìœ¼ë¡œ ì˜¤ë˜ ê±¸ë¦¬ê¸°ëŠ” í•˜ì˜€ìœ¼ë‚˜ ê³¼ë°˜ì´ìƒì´ê±°ë‚˜ ë‹¤ë¥¸ ì—°ë ¹ì¸µì— ë¹„í•´ì„œ ì¡°ê¸ˆì€ ë§ì¹˜ë§Œ í¬ê²Œ ìƒê´€ì„±ì´ ìˆì–´ë³´ì§€ëŠ” ì•Šì•˜ìŠµë‹ˆë‹¤.\nì •ë¶€ì˜ ê±°ë¦¬ë‘ê¸°ëŠ” ë‹¹ì‹œ ì‹ ì²œì§€ë¡œ ì¸í•´ì„œ ì¼ì¼ í™•ì§„ìê°€ 800ëª… ê°€ëŸ‰ ë‚˜ì˜¤ë˜ ì‹œì ì— ê³µí‘œë˜ê³  ê·¸ ì´í›„ë¡œ ë¶„ì„ê¸°ê°„ ë™ì•ˆì—ëŠ” ê°ì†Œì„¸ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ë¡œ ì •ë¶€ì˜ ê±°ë¦¬ë‘ê¸°ëŠ” ì ì–´ë„ ë‹¹ì‹œì—ëŠ” íƒ€ë‹¹í–ˆë‹¤ë¼ëŠ” ê²°ë¡ ì„ ë‚´ë¦¬ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.\n\nì½”ë¡œë‚˜ ì¢…ì‹ ë° ì˜ˆë°©ì„ ìœ„í•´ì„œëŠ” í•´ì™¸ ìœ ì…ì— ì˜í•œ í™•ì§„ìë¥¼ ì°¨ë‹¨í•´ì•¼í•©ë‹ˆë‹¤. í˜„ì¬ ì…êµ­ìì— ëŒ€í•œ ê²€ì‚¬ ë° 2ì£¼ ìê°€ê²©ë¦¬ ë“± ë§ì€ ë…¸ë ¥ì´ ì§„í–‰ë˜ê³  ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, ê·¸ëŸ¼ì—ë„ ìœ ì˜ì‚¬í•­ì„ ì˜ ë”°ë¥´ì§€ ì•ŠëŠ” ì¼ë¶€ ì¸ì›ì— ì˜í•´ì„œ ì‹ ì²œì§€ì™€ ê°™ì€ í° ì§‘ë‹¨ ê°ì—¼ì´ ë°œìƒë  ìˆ˜ ìˆë‹¤ëŠ” ì‚¬ì‹¤ì„ ìŠì§€ ë§ì•„ì•¼í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, ì½”ë¡œë‚˜ì— ëŒ€í•œ ê²½ê°ì‹¬ê³¼ ì¸ì‹ì„ ì˜ ì‹¬ì–´ì£¼ì–´ì•¼í•˜ë©°, íŠ¹íˆë‚˜ ê°€ì¥ ì•ˆì¼í•˜ê²Œ ìƒê°í•˜ëŠ” 20ëŒ€ì˜ ì¸ì‹ ë³€í™”ë¥¼ ì´ëŒì–´ì•¼ í•  ê²ƒì…ë‹ˆë‹¤. ë˜í•œ, 20ëŒ€ì˜ í–‰ë™ íŒ¨í„´ ë° ë°©ë¬¸ ê²½ë¡œë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°ì—¼ ìœ„í—˜ì´ ìˆëŠ” ì—…ì¢…ì€ íŠ¹íˆë‚˜ ë”ìš± ì‹ ê²½ì¨ì„œ ì‚¬íšŒì  ê±°ë¦¬ë‘ê¸°, ë§ˆìŠ¤í¬ ì°©ìš©, ì†ì„¸ì •ì œì™€ ì†ì”»ê¸° ë“±ì„ ë”ìš± ê¶Œì¥í•˜ë„ë¡ í•´ì•¼í•©ë‹ˆë‹¤.\në¶„ì„ì„ í•˜ë©´ì„œ ëŠë‚€ í•œê³„ì ì€ ìë£Œì— etcë‚˜ NaNìœ¼ë¡œ í‘œì‹œëœ ìë£Œì˜ í˜•íƒœê°€ ë§ì•„ì„œ ì •í™•í•œ ì›ì¸ë“¤ì„ ì°¾ê¸°ì— ì–´ë ¤ì›€ì´ ìˆì—ˆìŠµë‹ˆë‹¤.\nìë£Œ ì¶œì²˜ ë° ì°¸ê³  ì¶œì²˜ - https://dacon.io/competitions/official/235590/overview/description (ë°ì´ì½˜ - ì½”ë¡œë‚˜ ë°ì´í„° ì‹œê°í™” AI ê²½ì§„ëŒ€íšŒ) - https://www.kaggle.com/datasets/kimjihoo/coronavirusdataset (kaggle - Data Science for COVID-19 in South Korea) - https://chancoding.tistory.com/119 (plotly line plot) - https://plotly.com/python/pie-charts/(about pie plot)"
  },
  {
    "objectID": "Data_Mining.html",
    "href": "Data_Mining.html",
    "title": "Data Mining",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\n\n\n\n\n\n\n\n\n\nJun 7, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nFolium\n\n\nGeopandas\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nFolium\n\n\nGeopandas\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nGeopandas\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nGeopandas\n\n\nFolium\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nGeopandas\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\nScipy\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPython\n\n\nSeaborn\n\n\nMatplotlib\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nMar 18, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPython\n\n\nNumpy\n\n\n\n\n\n\n\n\n\n\n\nMar 4, 2022\n\n\nHyunsoo Kim\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Data_Visualization/ì‹¤ê¸°_1ìœ í˜•.html",
    "href": "Data_Visualization/ì‹¤ê¸°_1ìœ í˜•.html",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 1ìœ í˜• ë¬¸ì œí’€ì´",
    "section": "",
    "text": "ì‹¤ê¸° 1 ìœ í˜•"
  },
  {
    "objectID": "Data_Visualization/ì‹¤ê¸°_1ìœ í˜•.html#ë°ì´í„°-ë‹¤ë£¨ê¸°-ìœ í˜•",
    "href": "Data_Visualization/ì‹¤ê¸°_1ìœ í˜•.html#ë°ì´í„°-ë‹¤ë£¨ê¸°-ìœ í˜•",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 1ìœ í˜• ë¬¸ì œí’€ì´",
    "section": "âœ… ë°ì´í„° ë‹¤ë£¨ê¸° ìœ í˜•",
    "text": "âœ… ë°ì´í„° ë‹¤ë£¨ê¸° ìœ í˜•\n\n\në°ì´í„° íƒ€ì…(object, int, float, bool ë“±)\nê¸°ì´ˆí†µê³„ëŸ‰(í‰ê· , ì¤‘ì•™ê°’, ì‚¬ë¶„ìœ„ìˆ˜, IQR, í‘œì¤€í¸ì°¨ ë“±)\në°ì´í„° ì¸ë±ì‹±, í•„í„°ë§, ì •ë ¬, ë³€ê²½ ë“±\nì¤‘ë³µê°’, ê²°ì¸¡ì¹˜, ì´ìƒì¹˜ ì²˜ë¦¬ (ì œê±° or ëŒ€ì²´)\në°ì´í„° Scaling (ë°ì´í„° í‘œì¤€í™”(z), ë°ì´í„° ì •ê·œí™”(min-max))\në°ì´í„° í•©ì¹˜ê¸°\në‚ ì§œ/ì‹œê°„ ë°ì´í„°, index ë‹¤ë£¨ê¸°"
  },
  {
    "objectID": "Data_Visualization/ì‹¤ê¸°_1ìœ í˜•.html#í•µì‹¬ë¬¸ì œ-27ê°œ-110",
    "href": "Data_Visualization/ì‹¤ê¸°_1ìœ í˜•.html#í•µì‹¬ë¬¸ì œ-27ê°œ-110",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 1ìœ í˜• ë¬¸ì œí’€ì´",
    "section": "âœ… í•µì‹¬ë¬¸ì œ 27ê°œ (1~10)",
    "text": "âœ… í•µì‹¬ë¬¸ì œ 27ê°œ (1~10)\n\n# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\nimport pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"mtcars.txt\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      Hornet Sportabout\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\n# ë¬¸ì œ 1\n# mpg ë³€ìˆ˜ì˜ ì œ 1ì‚¬ë¶„ìœ„ìˆ˜ë¥¼ êµ¬í•˜ê³  ì •ìˆ˜ê°’ìœ¼ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (í’€ì´)\nQ1 = df['mpg'].quantile(.25)\nprint(round(Q1))\n\n15\n\n\n\n# ë¬¸ì œ 2\n# mpg ê°’ì´ 19ì´ìƒ 21ì´í•˜ì¸ ë°ì´í„°ì˜ ìˆ˜ë¥¼ êµ¬í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (í’€ì´ 1)\ncond1 = (df[(df['mpg']>=19) & (df['mpg']<=21)])\nprint(len(cond1))\n\n# (í’€ì´ 2)\n# cond1 = (df['mpg']>=19)\n# cond2 = (df['mpg']<=21)\n# print(len(df[cond1&cond2]))\n\n5\n\n\n\n# ë¬¸ì œ 3\n# hp ë³€ìˆ˜ì˜ IQR ê°’ì„ êµ¬í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (í’€ì´)\nQ1 = df['hp'].quantile(.25)\nQ3 = df['hp'].quantile(.75)\nIQR = Q3 - Q1\nprint(IQR)\n\n83.5\n\n\n\n# ë¬¸ì œ 4 \n# wt ë³€ìˆ˜ì˜ ìƒìœ„ 10ê°œ ê°’ì˜ ì´í•©ì„ êµ¬í•˜ì—¬ ì†Œìˆ˜ì ì„ ë²„ë¦¬ê³  ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (í’€ì´)\ntop10 = df.sort_values('wt', ascending=False)\nsum_top10 = sum(top10['wt'].head(10))\nprint(int(sum_top10))\n\n# top_10 = df.sort_values('wt',ascending=False).head(10)\n# print(int(sum(top_10['wt']))) #ì£¼ì˜: ì†Œìˆ˜ì  ë°˜ì˜¬ë¦¼ì´ ì•„ë‹ˆë¼ ë²„ë¦¬ëŠ” ë¬¸ì œ\n\n42\n\n\n\n# ë¬¸ì œ 5\n# ì „ì²´ ìë™ì°¨ì—ì„œ cylê°€ 6ì¸ ë¹„ìœ¨ì´ ì–¼ë§ˆì¸ì§€ ì†Œìˆ˜ì  ì²«ì§¸ì§œë¦¬ê¹Œì§€ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\ncond1 = len(df[df['cyl']==6])\ncond2 = len(df['cyl'])\nprint(round(cond1/cond2, 1))\n\n0.2\n\n\n\n# ë¬¸ì œ 6\n# ì²«ë²ˆì§¸ í–‰ë¶€í„° ìˆœì„œëŒ€ë¡œ 10ê°œ ë½‘ì€ í›„ mpg ì—´ì˜ í‰ê· ê°’ì„ ë°˜ì˜¬ë¦¼í•˜ì—¬ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (í’€ì´)\ncond1 = df.head(10) # df[:10], df.loc[0:9]\nmean_mpg = cond1['mpg'].mean()\nprint(round(mean_mpg))\n\n20\n\n\n\n# ë¬¸ì œ 7\n# ì²«ë²ˆì§¸ í–‰ë¶€í„° ìˆœì„œëŒ€ë¡œ 50% ê¹Œì§€ ë°ì´í„°ë¥¼ ë½‘ì•„ wt ë³€ìˆ˜ì˜ ì¤‘ì•™ê°’ì„ êµ¬í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (í’€ì´)\n# 50% ë°ì´í„°ì— í•´ë‹¹í•˜ëŠ” í–‰ì˜ ìˆ˜ (ì •ìˆ˜ë¡œ êµ¬í•˜ê¸°)\np50 = int(len(df)*0.5)\ndf50 = df[:p50]\nprint(df50['wt'].median())\n\n# len(df)/2\n# cond1 = df[:17]\n# cond2 = cond1['wt'].median()\n# print(cond2)\n\n3.44\n\n\n\n# ë¬¸ì œ 8\n# ê²°ì¸¡ê°’ì´ ìˆëŠ” ë°ì´í„°ì˜ ìˆ˜ë¥¼ ì¶œë ¥í•˜ì‹œì˜¤\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ì œí’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3.0\n      300\n    \n    \n      1\n      20220105\n      B\n      NaN\n      400\n    \n    \n      2\n      None\n      None\n      5.0\n      500\n    \n    \n      3\n      20230127\n      B\n      10.0\n      600\n    \n    \n      4\n      20220203\n      A\n      10.0\n      400\n    \n  \n\n\n\n\n\n# (í’€ì´)\nm_value = df.isnull().sum()\nprint(m_value.sum())\n\n5\n\n\n\n# ë¬¸ì œ 9 \n# 'íŒë§¤ìˆ˜' ì»¬ëŸ¼ì˜ ê²°ì¸¡ê°’ì„ íŒë§¤ìˆ˜ì˜ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´í•˜ê³  íŒë§¤ìˆ˜ì˜ \n#  í‰ê· ê°’ì„ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\n\n\n# (í’€ì´)\ndf[df['íŒë§¤ìˆ˜'].isnull()]\nmedian = df['íŒë§¤ìˆ˜'].median()\ndf['íŒë§¤ìˆ˜'] = df['íŒë§¤ìˆ˜'].fillna(median)\nmean = df['íŒë§¤ìˆ˜'].mean()\nprint(int(mean))\n\n15\n\n\n\n# ë¬¸ì œ 10\n# íŒë§¤ìˆ˜ ì»¬ëŸ¼ì— ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ì„ ì œê±°í•˜ê³ \n# ì²«ë²ˆì§¸ í–‰ë¶€í„° ìˆœì„œëŒ€ë¡œ 50%ê¹Œì§€ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬\n# íŒë§¤ìˆ˜ ë³€ìˆ˜ì˜ Q1(ì œ1ì‚¬ë¶„ìœ„ìˆ˜) ê°’ì„ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\n\n\n# (í’€ì´)\n# ê²°ì¸¡ì¹˜ ì‚­ì œ(í–‰ ê¸°ì¤€)\ndf = df['íŒë§¤ìˆ˜'].dropna()\n# ì²«ë²ˆì§¸ í–‰ë¶€í„° ìˆœì„œëŒ€ë¡œ 50%ê¹Œì§€ì˜ ë°ì´í„° ì¶”ì¶œ\nper50 = int(len(df)*0.5)\ndf = df[:per50]\n# ê°’êµ¬í•˜ê¸°\nprint(round(df.quantile(.25)))\n\n# df[df['íŒë§¤ìˆ˜'].isnull()]\n# df['íŒë§¤ìˆ˜'] = df['íŒë§¤ìˆ˜'].dropna()\n# int(len(df['íŒë§¤ìˆ˜'])/2)\n# cond1 = df['íŒë§¤ìˆ˜'].head(6)\n# Q1 = cond1.quantile(.25)\n# print(int(Q1))\n\n5"
  },
  {
    "objectID": "Data_Visualization/ì‹¤ê¸°_1ìœ í˜•.html#í•µì‹¬ë¬¸ì œ-27ê°œ-1120",
    "href": "Data_Visualization/ì‹¤ê¸°_1ìœ í˜•.html#í•µì‹¬ë¬¸ì œ-27ê°œ-1120",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 1ìœ í˜• ë¬¸ì œí’€ì´",
    "section": "âœ… í•µì‹¬ë¬¸ì œ 27ê°œ (11~20)",
    "text": "âœ… í•µì‹¬ë¬¸ì œ 27ê°œ (11~20)\n\ndf = pd.read_csv('mtcars.txt')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      Hornet Sportabout\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\n# ë¬¸ì œ 11\n# cylê°€ 4ì¸ ìë™ì°¨ì™€ 6ì¸ ìë™ì°¨ ê·¸ë£¹ì˜ mpg í‰ê· ê°’ì´ ì°¨ì´ë¥¼ ì ˆëŒ€ê°’ìœ¼ë¡œ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\ncyl_4 = df[df['cyl']==4]\ncyl_4_mean = cyl_4['mpg'].mean()\n\ncyl_6 = df[df['cyl']==6]\ncyl_6_mean = cyl_6['mpg'].mean()\n\nprint(round(abs(cyl_4_mean - cyl_6_mean)))\n\n7\n\n\n\n# ë¬¸ì œ 12\n# hp ë³€ìˆ˜ì— ëŒ€í•´ ë°ì´í„°í‘œì¤€í™”(Z-score)ë¥¼ ì§„í–‰í•˜ê³  ì´ìƒì¹˜ì˜ ìˆ˜ë¥¼ êµ¬í•˜ì‹œì˜¤\n# (ë‹¨, ì´ìƒì¹˜ëŠ” Zê°’ì´ 1.5ë¥¼ ì´ˆê³¼í•˜ê±°ë‚˜ -1.5ë¯¸ë§Œì¸ ê°’ì´ë‹¤)\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\n# Z = (X-í‰ê· ) / í‘œì¤€í¸ì°¨\nstd = df['hp'].std()\nmean_hp = df['hp'].mean()\ndf['zscore'] = (df['hp']-mean_hp)/std\n\ncond1 = (df['zscore']>1.5)\ncond2 = (df['zscore']<-1.5)\nprint(len(df[cond1] + df[cond2]))\n\n2\n\n\n\n# ë¬¸ì œ 13\n# mpg ì»¬ëŸ¼ì„ ìµœì†ŒìµœëŒ€ Scalingì„ ì§„í–‰í•œ í›„ 0.7ë³´ë‹¤ í° ê°’ì„ ê°€ì§€ëŠ” ë ˆì½”ë“œ ìˆ˜ë¥¼ êµ¬í•˜ë¼\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\nfrom sklearn.preprocessing import MinMaxScaler\nmscaler = MinMaxScaler()\ndf['mpg'] = mscaler.fit_transform(df[['mpg']])\nprint(len(df[df['mpg']>0.7]))\n\n# ê³µì‹ : (x-min) / (max-min)\n# min_mpg = df['mpg'].min()\n# max_mpg = df['mpg'].max()\n# df['mpg'] = (df['mpg'] - min_mpg)/(max_mpg - min_mpg)\n# cond1 = (df['mpg']>0.7)\n# print(len(df[cond1]))\n\n5\n\n\n\n# ë¬¸ì œ 14\n# wt ì»¬ëŸ¼ì— ëŒ€í•´ ìƒìê·¸ë¦¼ ê¸°ì¤€ìœ¼ë¡œ ì´ìƒì¹˜ì˜ ê°œìˆ˜ë¥¼ êµ¬í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\n# ì´ìƒì¹˜ : Q1, Q3 ë¡œë¶€í„° 1.5*IQRì„ ë„˜ì–´ê°€ëŠ” ê°’\nQ1 = df['wt'].quantile(.25)\nQ3 = df['wt'].quantile(.75)\nIQR = Q3-Q1\n\nupper = Q3 + (1.5*IQR)\nlower = Q1 - (1.5*IQR)\n\ncond1 = (df['wt'] > upper)\ncond2 = (df['wt'] < lower)\n\nprint(len(df[cond1] + df[cond2]))\n\n3\n\n\n\n# ë¬¸ì œ 15\n# íŒë§¤ìˆ˜ ì»¬ëŸ¼ì˜ ê²°ì¸¡ì¹˜ë¥¼ ìµœì†Œê°’ìœ¼ë¡œ ëŒ€ì²´í•˜ê³ \n# ê²°ì¸¡ì¹˜ê°€ ìˆì„ ë•Œì™€ ìµœì†Œê°’ìœ¼ë¡œ ëŒ€ì²´í–ˆì„ ë•Œ\n# í‰ê· ê°’ì˜ ì°¨ì´ë¥¼ ì ˆëŒ€ê°’ìœ¼ë¡œ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ì œí’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3.0\n      300\n    \n    \n      1\n      20220105\n      B\n      NaN\n      400\n    \n    \n      2\n      None\n      None\n      5.0\n      500\n    \n    \n      3\n      20230127\n      B\n      10.0\n      600\n    \n    \n      4\n      20220203\n      A\n      10.0\n      400\n    \n    \n      5\n      20220205\n      None\n      10.0\n      500\n    \n    \n      6\n      20230210\n      A\n      15.0\n      500\n    \n    \n      7\n      20230223\n      B\n      15.0\n      600\n    \n    \n      8\n      20230312\n      A\n      20.0\n      600\n    \n    \n      9\n      20230422\n      B\n      NaN\n      700\n    \n    \n      10\n      20220505\n      A\n      30.0\n      600\n    \n    \n      11\n      20230511\n      A\n      40.0\n      600\n    \n  \n\n\n\n\n\n# (í’€ì´)\n# ë°ì´í„° ë³µì‚¬\ndf2 = df.copy()\n# ìµœì†Œê°’ìœ¼ë¡œ ê²°ì¸¡ì¹˜ ëŒ€ì²´\nmin = df['íŒë§¤ìˆ˜'].min()\ndf2['íŒë§¤ìˆ˜'] = df2['íŒë§¤ìˆ˜'].fillna(min)\n\n# ê²°ì¸¡ì¹˜ê°€ ìˆì„ë•Œ, ëŒ€ì²´í–ˆì„ë•Œ í‰ê· \nm_yes = df['íŒë§¤ìˆ˜'].mean()\nm_no = df2['íŒë§¤ìˆ˜'].mean()\n\nprint(round(abs(m_yes - m_no)))\n\n2\n\n\n\n# ë¬¸ì œ 16\n# vsë³€ìˆ˜ê°€ 0ì´ ì•„ë‹Œ ì°¨ëŸ‰ ì¤‘ì— mpg ê°’ì´ ê°€ì¥ í° ì°¨ëŸ‰ì˜ hp ê°’ì„ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\ncond1 = df[df['vs']!=0]\n# mpg ê°’ì´ ê°€ì¥ í° ì°¨ëŸ‰(ë‚´ë¦¼ì°¨ìˆœ)\ncond1 = cond1.sort_values('mpg',ascending=False)\n# cond1\nprint(cond1['hp'].iloc[0])\n\n65\n\n\n\n# ë¬¸ì œ 17\n# gear ë³€ìˆ˜ê°’ì´ 3, 4ì¸ ë‘ ê·¸ë£¹ì˜ hp í‘œì¤€í¸ì°¨ê°’ì˜ ì°¨ì´ë¥¼ ì ˆëŒ€ê°’ìœ¼ë¡œ \n# ì†Œìˆ˜ì  ì²«ì§¸ìë¦¬ê¹Œì§€ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\n# ë‘ê°œ ê·¸ë£¹ìœ¼ë¡œ í•„í„°ë§\ncond1 = df[df['gear']==3]\ncond2 = df[df['gear']==4]\n# std êµ¬í•˜ê¸°\nstd3 = cond1['hp'].std()\nstd4 = cond2['hp'].std()\n\nprint(round(abs(std3 - std4),1))\n\n21.8\n\n\n\n# ë¬¸ì œ 18\n# gear ë³€ìˆ˜ì˜ ê°‘ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ mpg í‰ê· ê°’ì„ ì‚°ì¶œí•˜ê³ \n# í‰ê· ê°’ì´ ë†’ì€ ê·¸ë£¹ì˜ mpg ì œ3ì‚¬ë¶„ìœ„ìˆ˜ ê°’ì„ êµ¬í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\ndf['gear'].unique()\ngear3 = df[df['gear']==3]\ngear4 = df[df['gear']==4]\ngear5 = df[df['gear']==5]\n\nm_3 = gear3['mpg'].mean()\nm_4 = gear4['mpg'].mean()\nm_5 = gear5['mpg'].mean()\n\n(m_3, m_4, m_5) # m_4ì˜ í‰ê· ê°’ì´ ê°€ì¥ í¼\nQ3 = gear4['mpg'].quantile(.75)\nprint(Q3)\n\n28.075\n\n\n\n# (í’€ì´_v2)\n# gear, mpg ë³€ìˆ˜ë§Œ í•„í„°ë§\ndf = df.loc[:, ['gear', 'mpg']]\n\n# gear ë³€ìˆ˜ë¡œ ê·¸ë£¹í™”í•˜ì—¬ mpg í‰ê· ê°’ ë³´ê¸°\n# print(df.groupby('gear').mean()) # gear 4ê·¸ë£¹ì´ ì œì¼ ë†’ìŒ\n\n# gear=4ì¸ ê·¸ë£¹ì˜ mpg Q3 êµ¬í•˜ê¸°\ngear4 = df[df['gear']==4]\nprint(gear4['mpg'].quantile(.75))\n\n28.075\n\n\n\n# ë¬¸ì œ 19\n# hp í•­ëª©ì˜ ìƒìœ„ 7ë²ˆì§¸ ê°’ìœ¼ë¡œ ìƒìœ„ 7ê°œ ê°’ì„ ë³€í™˜í•œ í›„,\n# hpê°€ 150 ì´ìƒì¸ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ hpì˜ í‰ê· ê°’ì„ ë°˜ì˜¬ë¦¼í•˜ì—¬ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\n# hp ì—´ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬\ndf = df.sort_values('hp', ascending=False)\n\n# ì¸ë±ìŠ¤ ì´ˆê¸°í™” - ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬ì‹œ ìµœì´ˆì˜ ì¸ë±ìŠ¤ë¡œ ìˆê¸°ì—\ndf = df.reset_index(drop=True) # drop=True ëŠ” ê¸°ì¡´ index ì‚­ì œ\n# print(df)\n\n# hp ìƒìœ„ 7ë²ˆì§¸ ê°’ í™•ì¸\ntop7 = df['hp'].loc[6] # top7 = 205\n\n# np.where í™œìš©\nimport numpy as np\ndf['hp'] = np.where(df['hp'] >= 205, top7, df['hp'])\n# np.where(ì¡°ê±´, ì¡°ê±´ì— í•´ë‹¹í•  ë•Œ ê°’, ê·¸ë ‡ì§€ ì•Šì„ ë•Œ ê°’)\n\n# hp 150ì´ìƒì¸ ë°ì´í„°\ncond1 = (df['hp']>=150)\ndf = df[cond1]\nprint(round(df['hp'].mean()))\n\n187\n\n\n\n# ë¬¸ì œ 20\n# car ë³€ìˆ˜ì— Merc ë¬´êµ¬ê°€ í¬í•¨ëœ ìë™ì°¨ì˜ mpg í‰ê· ê°’ì„ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\ndf2 = df[df['car'].str.contains('Merc')] # ë¬¸ìì—´ ì¶”ì¶œ ì•Œì•„ë‘ê¸°\nprint(round(df2['mpg'].mean()))\n\n# ì‹œí—˜í™˜ê²½ì—ì„œ ë‹µêµ¬í•˜ëŠ” ë°©ë²•(reset_index() ì‚¬ìš© í›„)\n# ì‹œí—˜ì—ì„œëŠ” carê°€ indexë¡œ ë˜ì–´ ìˆìŒ\n# df.reset_index(inplace=True)\n# df2 = df[df['index'].str.contains(\"Merc\")] \n# print(round(df2['mpg'].mean()))\n\n19"
  },
  {
    "objectID": "Data_Visualization/ì‹¤ê¸°_1ìœ í˜•.html#í•µì‹¬ë¬¸ì œ-27ê°œ-2127",
    "href": "Data_Visualization/ì‹¤ê¸°_1ìœ í˜•.html#í•µì‹¬ë¬¸ì œ-27ê°œ-2127",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 1ìœ í˜• ë¬¸ì œí’€ì´",
    "section": "âœ… í•µì‹¬ë¬¸ì œ 27ê°œ (21~27)",
    "text": "âœ… í•µì‹¬ë¬¸ì œ 27ê°œ (21~27)\n\n# ë¬¸ì œ 21\n# 22ë…„ 1ë¶„ê¸° Aì œí’ˆì˜ ë§¤ì¶œì•¡ì„ êµ¬í•˜ì‹œì˜¤\n# (ë§¤ì¶œì•¡ = íŒë§¤ìˆ˜*ê°œë‹¹ìˆ˜ìµ)\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ì œí’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# # (í’€ì´)\n# cond1 = df[df['ë‚ ì§œ']<='20220431']\n# cond2 = cond1[cond1['ì œí’ˆ']=='A']\n# cond2['ë§¤ì¶œì•¡'] = (cond2['íŒë§¤ìˆ˜']*cond2['ê°œë‹¹ìˆ˜ìµ'])\n# cond2\n\n# print(cond2['ë§¤ì¶œì•¡'].sum())\n\n\n# (í’€ì´_v2)\n# ë°ì´í„° íƒ€ì… datetimeìœ¼ë¡œ ë³€ê²½\ndf['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])\n\n# ë…„, ì›”, ì¼ ë³€ìˆ˜(ì—´) ì¶”ê°€í•˜ê¸°\ndf['year'] = df['ë‚ ì§œ'].dt.year\ndf['month'] = df['ë‚ ì§œ'].dt.month\ndf['day'] = df['ë‚ ì§œ'].dt.day\n\n# ë§¤ì¶œì•¡ ë³€ìˆ˜ ì¶”ê°€í•˜ê¸°\ndf['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\n# 22ë…„ìœ¼ë¡œ í•„í„°ë§\ndf = df[df['year']==2022]\ndf.head()\n\n# 1,2,3ì›” ë§¤ì¶œì•¡ ê³„ì‚°\nm1 = df[df['month']==1]['ë§¤ì¶œì•¡'].sum()\nm2 = df[df['month']==2]['ë§¤ì¶œì•¡'].sum()\nm3 = df[df['month']==3]['ë§¤ì¶œì•¡'].sum()\nprint(m1+m2+m3)\n\n11900\n\n\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['year'] = df['ë‚ ì§œ'].dt.year\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['month'] = df['ë‚ ì§œ'].dt.month\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['day'] = df['ë‚ ì§œ'].dt.day\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\n\n\n# ë¬¸ì œ 22\n# 22ë…„ê³¼ 23ë…„ì˜ ì´ ë§¤ì¶œì•¡ ì°¨ì´ë¥¼ ì ˆëŒ€ê°’ìœ¼ë¡œ êµ¬í•˜ì‹œì˜¤\n# (ë§¤ì¶œì•¡ = íŒë§¤ìˆ˜*ê°œë‹¹ìˆ˜ìµ)\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ì œí’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# (í’€ì´)\n# ë°ì´í„° íƒ€ì… datetimeìœ¼ë¡œ ë³€ê²½\ndf['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])\n\n# ë…„ ë³€ìˆ˜(ì—´) ì¶”ê°€í•˜ê¸°\ndf['year'] = df['ë‚ ì§œ'].dt.year\n\n# ë§¤ì¶œì•¡ ë³€ìˆ˜ ì¶”ê°€í•˜ê¸°\ndf['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\ncond22 = df[df['year']==2022]\ncond23 = df[df['year']==2023]\n\nprint(abs((cond22['ë§¤ì¶œì•¡'].sum()) - (cond23['ë§¤ì¶œì•¡'].sum())))\n\n48600\n\n\n\n# ë¬¸ì œ 23\n# 23ë…„ ì´ ë§¤ì¶œì•¡ì´ í° ì œí’ˆì˜ 23ë…„ íŒë§¤ìˆ˜ë¥¼ êµ¬í•˜ì‹œì˜¤\n# (ë§¤ì¶œì•¡ = íŒë§¤ìˆ˜*ê°œë‹¹ìˆ˜ìµ)\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ì œí’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# ë°ì´í„° íƒ€ì… datetimeìœ¼ë¡œ ë³€ê²½\ndf['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])\n\n# ë…„ ë³€ìˆ˜(ì—´) ì¶”ê°€í•˜ê¸°\ndf['year'] = df['ë‚ ì§œ'].dt.year\n\n# ë§¤ì¶œì•¡ ë³€ìˆ˜ ì¶”ê°€í•˜ê¸°\ndf['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\ndf = df[df['year']==2023]\n# df.head()\n\n# 23ë…„ A ë§¤ì¶œì•¡ê³¼ B ë§¤ì¶œì•¡ ë³„ë„ë¡œ êµ¬í•˜ê¸°\n# A ë§¤ì¶œì•¡\ndf_a = df[df['ì œí’ˆ']=='A']\na_sales = df_a['ë§¤ì¶œì•¡'].sum()\n# print(a_sales) # 46000\n\n# B ë§¤ì¶œì•¡\ndf_b = df[df['ì œí’ˆ']=='B']\nb_sales = df_b['ë§¤ì¶œì•¡'].sum()\n# print(b_sales) # 32500\n\n# A ì œí’ˆì˜ 23ë…„ íŒë§¤ìˆ˜\na_sum = df_a['íŒë§¤ìˆ˜'].sum()\nprint(a_sum)\n\n80\n\n\n\n# ë¬¸ì œ 24\n# ë§¤ì¶œì•¡ì´ 4ì²œì› ì´ˆê³¼, 1ë§Œì› ë¯¸ë§Œì¸ ë°ì´í„° ìˆ˜ë¥¼ ì¶œë ¥í•˜ì‹œì˜¤\n# (ë§¤ì¶œì•¡ = íŒë§¤ìˆ˜*ê°œë‹¹ìˆ˜ìµ)\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ì œí’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# (í’€ì´)\ndf['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\ncond1 = df['ë§¤ì¶œì•¡']>4000\ncond2 = df['ë§¤ì¶œì•¡']<10000\n\ndf = df[cond1&cond2]\nprint(len(df))\n\n4\n\n\n\n# ë¬¸ì œ 25\n# 23ë…„ 9ì›” 24ì¼ 16:00~22:00 ì‚¬ì´ì— ì „ì²´ ì œí’ˆì˜ íŒë§¤ìˆ˜ë¥¼ êµ¬í•˜ì‹œì˜¤\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë¬¼í’ˆ' : ['A','B','A','B','A','B','A'],\n    'íŒë§¤ìˆ˜' : [5,10,15,15,20,25,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [500,600,500,600,600,700,600]})\ntime = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf['time'] = time\ndf = df[['time', 'ë¬¼í’ˆ', 'íŒë§¤ìˆ˜', 'ê°œë‹¹ìˆ˜ìµ']]\ndf\n\n\n\n\n\n  \n    \n      \n      time\n      ë¬¼í’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      1\n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2\n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      3\n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      4\n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      5\n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      6\n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# (í’€ì´)\n# ì»¬ëŸ¼ê³¼ ì¸ë±ìŠ¤ì— ë‘˜ë‹¤ time ë³€ìˆ˜ë¥¼ ë†“ê³  í’€ì´\n# ë°ì´í„° íƒ€ì… datetimeìœ¼ë¡œ ë³€ê²½ (í•­ìƒ df.info()ë¡œ ë°ì´í„° íƒ€ì… í™•ì¸í•  ê²ƒ)\ndf['time'] = pd.to_datetime(df['time'])\n\n# index ìƒˆë¡œ ì§€ì •\ndf = df.set_index('time', drop=False) # defaultëŠ” True\n\n# 9ì›” 24ì¼ í•„í„°ë§ // df['ë³€ìˆ˜'].between( , )\ndf_after = df[df['time'].between('2023-09-24', '2023-09-25')] # 25ì¼ì€ ë¯¸í¬í•¨\n\n# ì‹œê°„ í•„í„°ë§ 16:00 ~ 22:00 (ì£¼ì˜: ì‹œê°„ì´ indexì— ìœ„ì¹˜í•´ì•¼ í•¨)\ndf = df_after.between_time(start_time='16:00', end_time='22:00') # í¬í•¨ ê¸°ì¤€\n\n# ì „ì²´ íŒë§¤ìˆ˜ êµ¬í•˜ê¸°\nprint(df['íŒë§¤ìˆ˜'].sum())\n\n25\n\n\n\n# (í’€ì´2) // ìœ„ì— df ìƒˆë¡œ ë¶ˆëŸ¬ì™€ì„œ ì‹¤í–‰í•´ë³´ê¸°\n# ë°ì´í„° íƒ€ì… datetimeìœ¼ë¡œ ë³€ê²½\ndf['time'] = pd.to_datetime(df['time'])\n\n# index ìƒˆë¡œ ì§€ì •\ndf = df.set_index('time')\n\n# loc í•¨ìˆ˜ë¡œ í•„í„°ë§\ndf = df.loc[(df.index >= '2023-09-24 16:00:00') & (df.index <= '2023-09-24 22:00:00')]\n\n# ì „ì²´ íŒë§¤ìˆ˜ êµ¬í•˜ê¸°\nprint(df['íŒë§¤ìˆ˜'].sum())\n\n25\n\n\n\n# ë¬¸ì œ 26\n# 9ì›” 25ì¼ 00:10~12:00ê¹Œì§€ì˜ Bë¬¼í’ˆì˜ ë§¤ì¶œì•¡ ì´í•©ì„ êµ¬í•˜ì‹œì˜¤\n# (ë§¤ì¶œì•¡ = íŒë§¤ìˆ˜*ê°œë‹¹ìˆ˜ìµ)\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë¬¼í’ˆ' : ['A','B','A','B','A','B','A'],\n    'íŒë§¤ìˆ˜' : [5,10,15,15,20,25,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [500,600,500,600,600,700,600]})\ndf['time'] = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf = df[['time', 'ë¬¼í’ˆ', 'íŒë§¤ìˆ˜', 'ê°œë‹¹ìˆ˜ìµ']]\ndf = df.set_index('time',drop=True)\ndf\n\n\n\n\n\n  \n    \n      \n      ë¬¼í’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n    \n      time\n      \n      \n      \n    \n  \n  \n    \n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# (í’€ì´)\n# ë§¤ì¶œì•¡ ë³€ìˆ˜ ì¶”ê°€\ndf['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\n# loc í•¨ìˆ˜ë¡œ í•„í„°ë§\ndf = df.loc[(df.index >= '2023-09-25 00:10:00') & (df.index <= '2023-09-25 12:00:00')]\n\n# B ë¬¼í’ˆì˜ ë§¤ì¶œì•¡ ì´í•©\ncond1 = df[df['ë¬¼í’ˆ']=='B']\nprint(cond1['ë§¤ì¶œì•¡'].sum())\n\n26500\n\n\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\1645811142.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\n\n\n# ë¬¸ì œ 27\n# 9ì›” 24ì¼ 12:00~24:00ê¹Œì§€ì˜ Aë¬¼í’ˆì˜ ë§¤ì¶œì•¡ ì´í•©ì„ êµ¬í•˜ì‹œì˜¤\n# (ë§¤ì¶œì•¡ = íŒë§¤ìˆ˜*ê°œë‹¹ìˆ˜ìµ)\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë¬¼í’ˆ' : ['A','B','A','B','A','B','A'],\n    'íŒë§¤ìˆ˜' : [5,10,15,15,20,25,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [500,600,500,600,600,700,600]})\ndf['time'] = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf = df[['time', 'ë¬¼í’ˆ', 'íŒë§¤ìˆ˜', 'ê°œë‹¹ìˆ˜ìµ']]\ndf = df.set_index('time',drop=True)\ndf\n\n\n\n\n\n  \n    \n      \n      ë¬¼í’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n    \n      time\n      \n      \n      \n    \n  \n  \n    \n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# (í’€ì´)\n# ë§¤ì¶œì•¡ ë³€ìˆ˜ ì¶”ê°€\ndf['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\n# loc í•¨ìˆ˜ë¡œ í•„í„°ë§\ndf = df.loc[(df.index >= '2023-09-24 12:00:00') & (df.index < '2023-09-25 00:00:00')]\n\n# A ë¬¼í’ˆì˜ ë§¤ì¶œì•¡ ì´í•©\ncond1 = df[df['ë¬¼í’ˆ']=='A']\nprint(cond1['ë§¤ì¶œì•¡'].sum())\n\n10000"
  },
  {
    "objectID": "Data_Visualization.html",
    "href": "Data_Visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Profile",
    "section": "",
    "text": "í•œë‚¨ëŒ€í•™êµ ë¹„ì¦ˆë‹ˆìŠ¤í†µê³„í•™ê³¼ | 2018.03 ~ 2021.02\ní•œë‚¨ëŒ€í•™êµ ë¹…ë°ì´í„°ì‘ìš©í•™ê³¼ | 2021.03 ~ 2024.02 \n\n\n\n\n\nê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€ ì¥ê´€ìƒ | 2022.09\ní•œêµ­ìˆ˜ìì›ê³µì‚¬ ì‚¬ì¥ìƒ | 2022.10 \n\n\n\n\n\në°ì´í„° ì²­ë…„ ìº í¼ìŠ¤ | 2022.08 ~ 2022.09\në¹…ë¦¬ë” AI ì‚°í•™í˜‘ë ¥ | 2022.08 ~ 2022.10\në©‹ìŸì´ì‚¬ìì²˜ëŸ¼ 11ê¸° | 2023.03 ~ 2023.12\níƒœë¸”ë¡œ ì‹ ë³‘í›ˆë ¨ì†Œ 21ê¸° | 2023.10 ~ 2023.11\n\n\n\n\n\në°ì´í„° ë¶„ì„ ì¤€ì „ë¬¸ê°€ (ADsP) | 2023.06\nSQL ê°œë°œì (SQLD) | 2023.10\në¹…ë°ì´í„°ë¶„ì„ê¸°ì‚¬_í•„ê¸° (BAE) | 2023.10"
  },
  {
    "objectID": "OpenData_Analysis/ì‹¤ê¸°_1ìœ í˜•.html",
    "href": "OpenData_Analysis/ì‹¤ê¸°_1ìœ í˜•.html",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 1ìœ í˜• ë¬¸ì œí’€ì´",
    "section": "",
    "text": "ì‹¤ê¸° 1 ìœ í˜•"
  },
  {
    "objectID": "OpenData_Analysis/ì‹¤ê¸°_1ìœ í˜•.html#ë°ì´í„°-ë‹¤ë£¨ê¸°-ìœ í˜•",
    "href": "OpenData_Analysis/ì‹¤ê¸°_1ìœ í˜•.html#ë°ì´í„°-ë‹¤ë£¨ê¸°-ìœ í˜•",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 1ìœ í˜• ë¬¸ì œí’€ì´",
    "section": "âœ… ë°ì´í„° ë‹¤ë£¨ê¸° ìœ í˜•",
    "text": "âœ… ë°ì´í„° ë‹¤ë£¨ê¸° ìœ í˜•\n\n\në°ì´í„° íƒ€ì…(object, int, float, bool ë“±)\nê¸°ì´ˆí†µê³„ëŸ‰(í‰ê· , ì¤‘ì•™ê°’, ì‚¬ë¶„ìœ„ìˆ˜, IQR, í‘œì¤€í¸ì°¨ ë“±)\në°ì´í„° ì¸ë±ì‹±, í•„í„°ë§, ì •ë ¬, ë³€ê²½ ë“±\nì¤‘ë³µê°’, ê²°ì¸¡ì¹˜, ì´ìƒì¹˜ ì²˜ë¦¬ (ì œê±° or ëŒ€ì²´)\në°ì´í„° Scaling (ë°ì´í„° í‘œì¤€í™”(z), ë°ì´í„° ì •ê·œí™”(min-max))\në°ì´í„° í•©ì¹˜ê¸°\në‚ ì§œ/ì‹œê°„ ë°ì´í„°, index ë‹¤ë£¨ê¸°"
  },
  {
    "objectID": "OpenData_Analysis/ì‹¤ê¸°_1ìœ í˜•.html#í•µì‹¬ë¬¸ì œ-27ê°œ-110",
    "href": "OpenData_Analysis/ì‹¤ê¸°_1ìœ í˜•.html#í•µì‹¬ë¬¸ì œ-27ê°œ-110",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 1ìœ í˜• ë¬¸ì œí’€ì´",
    "section": "âœ… í•µì‹¬ë¬¸ì œ 27ê°œ (1~10)",
    "text": "âœ… í•µì‹¬ë¬¸ì œ 27ê°œ (1~10)\n\n# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\nimport pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"mtcars.txt\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      Hornet Sportabout\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\n# ë¬¸ì œ 1\n# mpg ë³€ìˆ˜ì˜ ì œ 1ì‚¬ë¶„ìœ„ìˆ˜ë¥¼ êµ¬í•˜ê³  ì •ìˆ˜ê°’ìœ¼ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (í’€ì´)\nQ1 = df['mpg'].quantile(.25)\nprint(round(Q1))\n\n15\n\n\n\n# ë¬¸ì œ 2\n# mpg ê°’ì´ 19ì´ìƒ 21ì´í•˜ì¸ ë°ì´í„°ì˜ ìˆ˜ë¥¼ êµ¬í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (í’€ì´ 1)\ncond1 = (df[(df['mpg']>=19) & (df['mpg']<=21)])\nprint(len(cond1))\n\n# (í’€ì´ 2)\n# cond1 = (df['mpg']>=19)\n# cond2 = (df['mpg']<=21)\n# print(len(df[cond1&cond2]))\n\n5\n\n\n\n# ë¬¸ì œ 3\n# hp ë³€ìˆ˜ì˜ IQR ê°’ì„ êµ¬í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (í’€ì´)\nQ1 = df['hp'].quantile(.25)\nQ3 = df['hp'].quantile(.75)\nIQR = Q3 - Q1\nprint(IQR)\n\n83.5\n\n\n\n# ë¬¸ì œ 4 \n# wt ë³€ìˆ˜ì˜ ìƒìœ„ 10ê°œ ê°’ì˜ ì´í•©ì„ êµ¬í•˜ì—¬ ì†Œìˆ˜ì ì„ ë²„ë¦¬ê³  ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (í’€ì´)\ntop10 = df.sort_values('wt', ascending=False)\nsum_top10 = sum(top10['wt'].head(10))\nprint(int(sum_top10))\n\n# top_10 = df.sort_values('wt',ascending=False).head(10)\n# print(int(sum(top_10['wt']))) #ì£¼ì˜: ì†Œìˆ˜ì  ë°˜ì˜¬ë¦¼ì´ ì•„ë‹ˆë¼ ë²„ë¦¬ëŠ” ë¬¸ì œ\n\n42\n\n\n\n# ë¬¸ì œ 5\n# ì „ì²´ ìë™ì°¨ì—ì„œ cylê°€ 6ì¸ ë¹„ìœ¨ì´ ì–¼ë§ˆì¸ì§€ ì†Œìˆ˜ì  ì²«ì§¸ì§œë¦¬ê¹Œì§€ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\ncond1 = len(df[df['cyl']==6])\ncond2 = len(df['cyl'])\nprint(round(cond1/cond2, 1))\n\n0.2\n\n\n\n# ë¬¸ì œ 6\n# ì²«ë²ˆì§¸ í–‰ë¶€í„° ìˆœì„œëŒ€ë¡œ 10ê°œ ë½‘ì€ í›„ mpg ì—´ì˜ í‰ê· ê°’ì„ ë°˜ì˜¬ë¦¼í•˜ì—¬ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (í’€ì´)\ncond1 = df.head(10) # df[:10], df.loc[0:9]\nmean_mpg = cond1['mpg'].mean()\nprint(round(mean_mpg))\n\n20\n\n\n\n# ë¬¸ì œ 7\n# ì²«ë²ˆì§¸ í–‰ë¶€í„° ìˆœì„œëŒ€ë¡œ 50% ê¹Œì§€ ë°ì´í„°ë¥¼ ë½‘ì•„ wt ë³€ìˆ˜ì˜ ì¤‘ì•™ê°’ì„ êµ¬í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (í’€ì´)\n# 50% ë°ì´í„°ì— í•´ë‹¹í•˜ëŠ” í–‰ì˜ ìˆ˜ (ì •ìˆ˜ë¡œ êµ¬í•˜ê¸°)\np50 = int(len(df)*0.5)\ndf50 = df[:p50]\nprint(df50['wt'].median())\n\n# len(df)/2\n# cond1 = df[:17]\n# cond2 = cond1['wt'].median()\n# print(cond2)\n\n3.44\n\n\n\n# ë¬¸ì œ 8\n# ê²°ì¸¡ê°’ì´ ìˆëŠ” ë°ì´í„°ì˜ ìˆ˜ë¥¼ ì¶œë ¥í•˜ì‹œì˜¤\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ì œí’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3.0\n      300\n    \n    \n      1\n      20220105\n      B\n      NaN\n      400\n    \n    \n      2\n      None\n      None\n      5.0\n      500\n    \n    \n      3\n      20230127\n      B\n      10.0\n      600\n    \n    \n      4\n      20220203\n      A\n      10.0\n      400\n    \n  \n\n\n\n\n\n# (í’€ì´)\nm_value = df.isnull().sum()\nprint(m_value.sum())\n\n5\n\n\n\n# ë¬¸ì œ 9 \n# 'íŒë§¤ìˆ˜' ì»¬ëŸ¼ì˜ ê²°ì¸¡ê°’ì„ íŒë§¤ìˆ˜ì˜ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´í•˜ê³  íŒë§¤ìˆ˜ì˜ \n#  í‰ê· ê°’ì„ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\n\n\n# (í’€ì´)\ndf[df['íŒë§¤ìˆ˜'].isnull()]\nmedian = df['íŒë§¤ìˆ˜'].median()\ndf['íŒë§¤ìˆ˜'] = df['íŒë§¤ìˆ˜'].fillna(median)\nmean = df['íŒë§¤ìˆ˜'].mean()\nprint(int(mean))\n\n15\n\n\n\n# ë¬¸ì œ 10\n# íŒë§¤ìˆ˜ ì»¬ëŸ¼ì— ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ì„ ì œê±°í•˜ê³ \n# ì²«ë²ˆì§¸ í–‰ë¶€í„° ìˆœì„œëŒ€ë¡œ 50%ê¹Œì§€ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬\n# íŒë§¤ìˆ˜ ë³€ìˆ˜ì˜ Q1(ì œ1ì‚¬ë¶„ìœ„ìˆ˜) ê°’ì„ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\n\n\n# (í’€ì´)\n# ê²°ì¸¡ì¹˜ ì‚­ì œ(í–‰ ê¸°ì¤€)\ndf = df['íŒë§¤ìˆ˜'].dropna()\n# ì²«ë²ˆì§¸ í–‰ë¶€í„° ìˆœì„œëŒ€ë¡œ 50%ê¹Œì§€ì˜ ë°ì´í„° ì¶”ì¶œ\nper50 = int(len(df)*0.5)\ndf = df[:per50]\n# ê°’êµ¬í•˜ê¸°\nprint(round(df.quantile(.25)))\n\n# df[df['íŒë§¤ìˆ˜'].isnull()]\n# df['íŒë§¤ìˆ˜'] = df['íŒë§¤ìˆ˜'].dropna()\n# int(len(df['íŒë§¤ìˆ˜'])/2)\n# cond1 = df['íŒë§¤ìˆ˜'].head(6)\n# Q1 = cond1.quantile(.25)\n# print(int(Q1))\n\n5"
  },
  {
    "objectID": "OpenData_Analysis/ì‹¤ê¸°_1ìœ í˜•.html#í•µì‹¬ë¬¸ì œ-27ê°œ-1120",
    "href": "OpenData_Analysis/ì‹¤ê¸°_1ìœ í˜•.html#í•µì‹¬ë¬¸ì œ-27ê°œ-1120",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 1ìœ í˜• ë¬¸ì œí’€ì´",
    "section": "âœ… í•µì‹¬ë¬¸ì œ 27ê°œ (11~20)",
    "text": "âœ… í•µì‹¬ë¬¸ì œ 27ê°œ (11~20)\n\ndf = pd.read_csv('mtcars.txt')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      Hornet Sportabout\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\n# ë¬¸ì œ 11\n# cylê°€ 4ì¸ ìë™ì°¨ì™€ 6ì¸ ìë™ì°¨ ê·¸ë£¹ì˜ mpg í‰ê· ê°’ì´ ì°¨ì´ë¥¼ ì ˆëŒ€ê°’ìœ¼ë¡œ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\ncyl_4 = df[df['cyl']==4]\ncyl_4_mean = cyl_4['mpg'].mean()\n\ncyl_6 = df[df['cyl']==6]\ncyl_6_mean = cyl_6['mpg'].mean()\n\nprint(round(abs(cyl_4_mean - cyl_6_mean)))\n\n7\n\n\n\n# ë¬¸ì œ 12\n# hp ë³€ìˆ˜ì— ëŒ€í•´ ë°ì´í„°í‘œì¤€í™”(Z-score)ë¥¼ ì§„í–‰í•˜ê³  ì´ìƒì¹˜ì˜ ìˆ˜ë¥¼ êµ¬í•˜ì‹œì˜¤\n# (ë‹¨, ì´ìƒì¹˜ëŠ” Zê°’ì´ 1.5ë¥¼ ì´ˆê³¼í•˜ê±°ë‚˜ -1.5ë¯¸ë§Œì¸ ê°’ì´ë‹¤)\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\n# Z = (X-í‰ê· ) / í‘œì¤€í¸ì°¨\nstd = df['hp'].std()\nmean_hp = df['hp'].mean()\ndf['zscore'] = (df['hp']-mean_hp)/std\n\ncond1 = (df['zscore']>1.5)\ncond2 = (df['zscore']<-1.5)\nprint(len(df[cond1] + df[cond2]))\n\n2\n\n\n\n# ë¬¸ì œ 13\n# mpg ì»¬ëŸ¼ì„ ìµœì†ŒìµœëŒ€ Scalingì„ ì§„í–‰í•œ í›„ 0.7ë³´ë‹¤ í° ê°’ì„ ê°€ì§€ëŠ” ë ˆì½”ë“œ ìˆ˜ë¥¼ êµ¬í•˜ë¼\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\nfrom sklearn.preprocessing import MinMaxScaler\nmscaler = MinMaxScaler()\ndf['mpg'] = mscaler.fit_transform(df[['mpg']])\nprint(len(df[df['mpg']>0.7]))\n\n# ê³µì‹ : (x-min) / (max-min)\n# min_mpg = df['mpg'].min()\n# max_mpg = df['mpg'].max()\n# df['mpg'] = (df['mpg'] - min_mpg)/(max_mpg - min_mpg)\n# cond1 = (df['mpg']>0.7)\n# print(len(df[cond1]))\n\n5\n\n\n\n# ë¬¸ì œ 14\n# wt ì»¬ëŸ¼ì— ëŒ€í•´ ìƒìê·¸ë¦¼ ê¸°ì¤€ìœ¼ë¡œ ì´ìƒì¹˜ì˜ ê°œìˆ˜ë¥¼ êµ¬í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\n# ì´ìƒì¹˜ : Q1, Q3 ë¡œë¶€í„° 1.5*IQRì„ ë„˜ì–´ê°€ëŠ” ê°’\nQ1 = df['wt'].quantile(.25)\nQ3 = df['wt'].quantile(.75)\nIQR = Q3-Q1\n\nupper = Q3 + (1.5*IQR)\nlower = Q1 - (1.5*IQR)\n\ncond1 = (df['wt'] > upper)\ncond2 = (df['wt'] < lower)\n\nprint(len(df[cond1] + df[cond2]))\n\n3\n\n\n\n# ë¬¸ì œ 15\n# íŒë§¤ìˆ˜ ì»¬ëŸ¼ì˜ ê²°ì¸¡ì¹˜ë¥¼ ìµœì†Œê°’ìœ¼ë¡œ ëŒ€ì²´í•˜ê³ \n# ê²°ì¸¡ì¹˜ê°€ ìˆì„ ë•Œì™€ ìµœì†Œê°’ìœ¼ë¡œ ëŒ€ì²´í–ˆì„ ë•Œ\n# í‰ê· ê°’ì˜ ì°¨ì´ë¥¼ ì ˆëŒ€ê°’ìœ¼ë¡œ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ì œí’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3.0\n      300\n    \n    \n      1\n      20220105\n      B\n      NaN\n      400\n    \n    \n      2\n      None\n      None\n      5.0\n      500\n    \n    \n      3\n      20230127\n      B\n      10.0\n      600\n    \n    \n      4\n      20220203\n      A\n      10.0\n      400\n    \n    \n      5\n      20220205\n      None\n      10.0\n      500\n    \n    \n      6\n      20230210\n      A\n      15.0\n      500\n    \n    \n      7\n      20230223\n      B\n      15.0\n      600\n    \n    \n      8\n      20230312\n      A\n      20.0\n      600\n    \n    \n      9\n      20230422\n      B\n      NaN\n      700\n    \n    \n      10\n      20220505\n      A\n      30.0\n      600\n    \n    \n      11\n      20230511\n      A\n      40.0\n      600\n    \n  \n\n\n\n\n\n# (í’€ì´)\n# ë°ì´í„° ë³µì‚¬\ndf2 = df.copy()\n# ìµœì†Œê°’ìœ¼ë¡œ ê²°ì¸¡ì¹˜ ëŒ€ì²´\nmin = df['íŒë§¤ìˆ˜'].min()\ndf2['íŒë§¤ìˆ˜'] = df2['íŒë§¤ìˆ˜'].fillna(min)\n\n# ê²°ì¸¡ì¹˜ê°€ ìˆì„ë•Œ, ëŒ€ì²´í–ˆì„ë•Œ í‰ê· \nm_yes = df['íŒë§¤ìˆ˜'].mean()\nm_no = df2['íŒë§¤ìˆ˜'].mean()\n\nprint(round(abs(m_yes - m_no)))\n\n2\n\n\n\n# ë¬¸ì œ 16\n# vsë³€ìˆ˜ê°€ 0ì´ ì•„ë‹Œ ì°¨ëŸ‰ ì¤‘ì— mpg ê°’ì´ ê°€ì¥ í° ì°¨ëŸ‰ì˜ hp ê°’ì„ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\ncond1 = df[df['vs']!=0]\n# mpg ê°’ì´ ê°€ì¥ í° ì°¨ëŸ‰(ë‚´ë¦¼ì°¨ìˆœ)\ncond1 = cond1.sort_values('mpg',ascending=False)\n# cond1\nprint(cond1['hp'].iloc[0])\n\n65\n\n\n\n# ë¬¸ì œ 17\n# gear ë³€ìˆ˜ê°’ì´ 3, 4ì¸ ë‘ ê·¸ë£¹ì˜ hp í‘œì¤€í¸ì°¨ê°’ì˜ ì°¨ì´ë¥¼ ì ˆëŒ€ê°’ìœ¼ë¡œ \n# ì†Œìˆ˜ì  ì²«ì§¸ìë¦¬ê¹Œì§€ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\n# ë‘ê°œ ê·¸ë£¹ìœ¼ë¡œ í•„í„°ë§\ncond1 = df[df['gear']==3]\ncond2 = df[df['gear']==4]\n# std êµ¬í•˜ê¸°\nstd3 = cond1['hp'].std()\nstd4 = cond2['hp'].std()\n\nprint(round(abs(std3 - std4),1))\n\n21.8\n\n\n\n# ë¬¸ì œ 18\n# gear ë³€ìˆ˜ì˜ ê°‘ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ mpg í‰ê· ê°’ì„ ì‚°ì¶œí•˜ê³ \n# í‰ê· ê°’ì´ ë†’ì€ ê·¸ë£¹ì˜ mpg ì œ3ì‚¬ë¶„ìœ„ìˆ˜ ê°’ì„ êµ¬í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\ndf['gear'].unique()\ngear3 = df[df['gear']==3]\ngear4 = df[df['gear']==4]\ngear5 = df[df['gear']==5]\n\nm_3 = gear3['mpg'].mean()\nm_4 = gear4['mpg'].mean()\nm_5 = gear5['mpg'].mean()\n\n(m_3, m_4, m_5) # m_4ì˜ í‰ê· ê°’ì´ ê°€ì¥ í¼\nQ3 = gear4['mpg'].quantile(.75)\nprint(Q3)\n\n28.075\n\n\n\n# (í’€ì´_v2)\n# gear, mpg ë³€ìˆ˜ë§Œ í•„í„°ë§\ndf = df.loc[:, ['gear', 'mpg']]\n\n# gear ë³€ìˆ˜ë¡œ ê·¸ë£¹í™”í•˜ì—¬ mpg í‰ê· ê°’ ë³´ê¸°\n# print(df.groupby('gear').mean()) # gear 4ê·¸ë£¹ì´ ì œì¼ ë†’ìŒ\n\n# gear=4ì¸ ê·¸ë£¹ì˜ mpg Q3 êµ¬í•˜ê¸°\ngear4 = df[df['gear']==4]\nprint(gear4['mpg'].quantile(.75))\n\n28.075\n\n\n\n# ë¬¸ì œ 19\n# hp í•­ëª©ì˜ ìƒìœ„ 7ë²ˆì§¸ ê°’ìœ¼ë¡œ ìƒìœ„ 7ê°œ ê°’ì„ ë³€í™˜í•œ í›„,\n# hpê°€ 150 ì´ìƒì¸ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ hpì˜ í‰ê· ê°’ì„ ë°˜ì˜¬ë¦¼í•˜ì—¬ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\n# hp ì—´ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬\ndf = df.sort_values('hp', ascending=False)\n\n# ì¸ë±ìŠ¤ ì´ˆê¸°í™” - ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬ì‹œ ìµœì´ˆì˜ ì¸ë±ìŠ¤ë¡œ ìˆê¸°ì—\ndf = df.reset_index(drop=True) # drop=True ëŠ” ê¸°ì¡´ index ì‚­ì œ\n# print(df)\n\n# hp ìƒìœ„ 7ë²ˆì§¸ ê°’ í™•ì¸\ntop7 = df['hp'].loc[6] # top7 = 205\n\n# np.where í™œìš©\nimport numpy as np\ndf['hp'] = np.where(df['hp'] >= 205, top7, df['hp'])\n# np.where(ì¡°ê±´, ì¡°ê±´ì— í•´ë‹¹í•  ë•Œ ê°’, ê·¸ë ‡ì§€ ì•Šì„ ë•Œ ê°’)\n\n# hp 150ì´ìƒì¸ ë°ì´í„°\ncond1 = (df['hp']>=150)\ndf = df[cond1]\nprint(round(df['hp'].mean()))\n\n187\n\n\n\n# ë¬¸ì œ 20\n# car ë³€ìˆ˜ì— Merc ë¬´êµ¬ê°€ í¬í•¨ëœ ìë™ì°¨ì˜ mpg í‰ê· ê°’ì„ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\ndf2 = df[df['car'].str.contains('Merc')] # ë¬¸ìì—´ ì¶”ì¶œ ì•Œì•„ë‘ê¸°\nprint(round(df2['mpg'].mean()))\n\n# ì‹œí—˜í™˜ê²½ì—ì„œ ë‹µêµ¬í•˜ëŠ” ë°©ë²•(reset_index() ì‚¬ìš© í›„)\n# ì‹œí—˜ì—ì„œëŠ” carê°€ indexë¡œ ë˜ì–´ ìˆìŒ\n# df.reset_index(inplace=True)\n# df2 = df[df['index'].str.contains(\"Merc\")] \n# print(round(df2['mpg'].mean()))\n\n19"
  },
  {
    "objectID": "OpenData_Analysis/ì‹¤ê¸°_1ìœ í˜•.html#í•µì‹¬ë¬¸ì œ-27ê°œ-2127",
    "href": "OpenData_Analysis/ì‹¤ê¸°_1ìœ í˜•.html#í•µì‹¬ë¬¸ì œ-27ê°œ-2127",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 1ìœ í˜• ë¬¸ì œí’€ì´",
    "section": "âœ… í•µì‹¬ë¬¸ì œ 27ê°œ (21~27)",
    "text": "âœ… í•µì‹¬ë¬¸ì œ 27ê°œ (21~27)\n\n# ë¬¸ì œ 21\n# 22ë…„ 1ë¶„ê¸° Aì œí’ˆì˜ ë§¤ì¶œì•¡ì„ êµ¬í•˜ì‹œì˜¤\n# (ë§¤ì¶œì•¡ = íŒë§¤ìˆ˜*ê°œë‹¹ìˆ˜ìµ)\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ì œí’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# # (í’€ì´)\n# cond1 = df[df['ë‚ ì§œ']<='20220431']\n# cond2 = cond1[cond1['ì œí’ˆ']=='A']\n# cond2['ë§¤ì¶œì•¡'] = (cond2['íŒë§¤ìˆ˜']*cond2['ê°œë‹¹ìˆ˜ìµ'])\n# cond2\n\n# print(cond2['ë§¤ì¶œì•¡'].sum())\n\n\n# (í’€ì´_v2)\n# ë°ì´í„° íƒ€ì… datetimeìœ¼ë¡œ ë³€ê²½\ndf['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])\n\n# ë…„, ì›”, ì¼ ë³€ìˆ˜(ì—´) ì¶”ê°€í•˜ê¸°\ndf['year'] = df['ë‚ ì§œ'].dt.year\ndf['month'] = df['ë‚ ì§œ'].dt.month\ndf['day'] = df['ë‚ ì§œ'].dt.day\n\n# ë§¤ì¶œì•¡ ë³€ìˆ˜ ì¶”ê°€í•˜ê¸°\ndf['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\n# 22ë…„ìœ¼ë¡œ í•„í„°ë§\ndf = df[df['year']==2022]\ndf.head()\n\n# 1,2,3ì›” ë§¤ì¶œì•¡ ê³„ì‚°\nm1 = df[df['month']==1]['ë§¤ì¶œì•¡'].sum()\nm2 = df[df['month']==2]['ë§¤ì¶œì•¡'].sum()\nm3 = df[df['month']==3]['ë§¤ì¶œì•¡'].sum()\nprint(m1+m2+m3)\n\n11900\n\n\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['year'] = df['ë‚ ì§œ'].dt.year\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['month'] = df['ë‚ ì§œ'].dt.month\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['day'] = df['ë‚ ì§œ'].dt.day\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\n\n\n# ë¬¸ì œ 22\n# 22ë…„ê³¼ 23ë…„ì˜ ì´ ë§¤ì¶œì•¡ ì°¨ì´ë¥¼ ì ˆëŒ€ê°’ìœ¼ë¡œ êµ¬í•˜ì‹œì˜¤\n# (ë§¤ì¶œì•¡ = íŒë§¤ìˆ˜*ê°œë‹¹ìˆ˜ìµ)\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ì œí’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# (í’€ì´)\n# ë°ì´í„° íƒ€ì… datetimeìœ¼ë¡œ ë³€ê²½\ndf['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])\n\n# ë…„ ë³€ìˆ˜(ì—´) ì¶”ê°€í•˜ê¸°\ndf['year'] = df['ë‚ ì§œ'].dt.year\n\n# ë§¤ì¶œì•¡ ë³€ìˆ˜ ì¶”ê°€í•˜ê¸°\ndf['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\ncond22 = df[df['year']==2022]\ncond23 = df[df['year']==2023]\n\nprint(abs((cond22['ë§¤ì¶œì•¡'].sum()) - (cond23['ë§¤ì¶œì•¡'].sum())))\n\n48600\n\n\n\n# ë¬¸ì œ 23\n# 23ë…„ ì´ ë§¤ì¶œì•¡ì´ í° ì œí’ˆì˜ 23ë…„ íŒë§¤ìˆ˜ë¥¼ êµ¬í•˜ì‹œì˜¤\n# (ë§¤ì¶œì•¡ = íŒë§¤ìˆ˜*ê°œë‹¹ìˆ˜ìµ)\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ì œí’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# ë°ì´í„° íƒ€ì… datetimeìœ¼ë¡œ ë³€ê²½\ndf['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])\n\n# ë…„ ë³€ìˆ˜(ì—´) ì¶”ê°€í•˜ê¸°\ndf['year'] = df['ë‚ ì§œ'].dt.year\n\n# ë§¤ì¶œì•¡ ë³€ìˆ˜ ì¶”ê°€í•˜ê¸°\ndf['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\ndf = df[df['year']==2023]\n# df.head()\n\n# 23ë…„ A ë§¤ì¶œì•¡ê³¼ B ë§¤ì¶œì•¡ ë³„ë„ë¡œ êµ¬í•˜ê¸°\n# A ë§¤ì¶œì•¡\ndf_a = df[df['ì œí’ˆ']=='A']\na_sales = df_a['ë§¤ì¶œì•¡'].sum()\n# print(a_sales) # 46000\n\n# B ë§¤ì¶œì•¡\ndf_b = df[df['ì œí’ˆ']=='B']\nb_sales = df_b['ë§¤ì¶œì•¡'].sum()\n# print(b_sales) # 32500\n\n# A ì œí’ˆì˜ 23ë…„ íŒë§¤ìˆ˜\na_sum = df_a['íŒë§¤ìˆ˜'].sum()\nprint(a_sum)\n\n80\n\n\n\n# ë¬¸ì œ 24\n# ë§¤ì¶œì•¡ì´ 4ì²œì› ì´ˆê³¼, 1ë§Œì› ë¯¸ë§Œì¸ ë°ì´í„° ìˆ˜ë¥¼ ì¶œë ¥í•˜ì‹œì˜¤\n# (ë§¤ì¶œì•¡ = íŒë§¤ìˆ˜*ê°œë‹¹ìˆ˜ìµ)\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ì œí’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# (í’€ì´)\ndf['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\ncond1 = df['ë§¤ì¶œì•¡']>4000\ncond2 = df['ë§¤ì¶œì•¡']<10000\n\ndf = df[cond1&cond2]\nprint(len(df))\n\n4\n\n\n\n# ë¬¸ì œ 25\n# 23ë…„ 9ì›” 24ì¼ 16:00~22:00 ì‚¬ì´ì— ì „ì²´ ì œí’ˆì˜ íŒë§¤ìˆ˜ë¥¼ êµ¬í•˜ì‹œì˜¤\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë¬¼í’ˆ' : ['A','B','A','B','A','B','A'],\n    'íŒë§¤ìˆ˜' : [5,10,15,15,20,25,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [500,600,500,600,600,700,600]})\ntime = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf['time'] = time\ndf = df[['time', 'ë¬¼í’ˆ', 'íŒë§¤ìˆ˜', 'ê°œë‹¹ìˆ˜ìµ']]\ndf\n\n\n\n\n\n  \n    \n      \n      time\n      ë¬¼í’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      1\n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2\n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      3\n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      4\n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      5\n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      6\n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# (í’€ì´)\n# ì»¬ëŸ¼ê³¼ ì¸ë±ìŠ¤ì— ë‘˜ë‹¤ time ë³€ìˆ˜ë¥¼ ë†“ê³  í’€ì´\n# ë°ì´í„° íƒ€ì… datetimeìœ¼ë¡œ ë³€ê²½ (í•­ìƒ df.info()ë¡œ ë°ì´í„° íƒ€ì… í™•ì¸í•  ê²ƒ)\ndf['time'] = pd.to_datetime(df['time'])\n\n# index ìƒˆë¡œ ì§€ì •\ndf = df.set_index('time', drop=False) # defaultëŠ” True\n\n# 9ì›” 24ì¼ í•„í„°ë§ // df['ë³€ìˆ˜'].between( , )\ndf_after = df[df['time'].between('2023-09-24', '2023-09-25')] # 25ì¼ì€ ë¯¸í¬í•¨\n\n# ì‹œê°„ í•„í„°ë§ 16:00 ~ 22:00 (ì£¼ì˜: ì‹œê°„ì´ indexì— ìœ„ì¹˜í•´ì•¼ í•¨)\ndf = df_after.between_time(start_time='16:00', end_time='22:00') # í¬í•¨ ê¸°ì¤€\n\n# ì „ì²´ íŒë§¤ìˆ˜ êµ¬í•˜ê¸°\nprint(df['íŒë§¤ìˆ˜'].sum())\n\n25\n\n\n\n# (í’€ì´2) // ìœ„ì— df ìƒˆë¡œ ë¶ˆëŸ¬ì™€ì„œ ì‹¤í–‰í•´ë³´ê¸°\n# ë°ì´í„° íƒ€ì… datetimeìœ¼ë¡œ ë³€ê²½\ndf['time'] = pd.to_datetime(df['time'])\n\n# index ìƒˆë¡œ ì§€ì •\ndf = df.set_index('time')\n\n# loc í•¨ìˆ˜ë¡œ í•„í„°ë§\ndf = df.loc[(df.index >= '2023-09-24 16:00:00') & (df.index <= '2023-09-24 22:00:00')]\n\n# ì „ì²´ íŒë§¤ìˆ˜ êµ¬í•˜ê¸°\nprint(df['íŒë§¤ìˆ˜'].sum())\n\n25\n\n\n\n# ë¬¸ì œ 26\n# 9ì›” 25ì¼ 00:10~12:00ê¹Œì§€ì˜ Bë¬¼í’ˆì˜ ë§¤ì¶œì•¡ ì´í•©ì„ êµ¬í•˜ì‹œì˜¤\n# (ë§¤ì¶œì•¡ = íŒë§¤ìˆ˜*ê°œë‹¹ìˆ˜ìµ)\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë¬¼í’ˆ' : ['A','B','A','B','A','B','A'],\n    'íŒë§¤ìˆ˜' : [5,10,15,15,20,25,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [500,600,500,600,600,700,600]})\ndf['time'] = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf = df[['time', 'ë¬¼í’ˆ', 'íŒë§¤ìˆ˜', 'ê°œë‹¹ìˆ˜ìµ']]\ndf = df.set_index('time',drop=True)\ndf\n\n\n\n\n\n  \n    \n      \n      ë¬¼í’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n    \n      time\n      \n      \n      \n    \n  \n  \n    \n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# (í’€ì´)\n# ë§¤ì¶œì•¡ ë³€ìˆ˜ ì¶”ê°€\ndf['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\n# loc í•¨ìˆ˜ë¡œ í•„í„°ë§\ndf = df.loc[(df.index >= '2023-09-25 00:10:00') & (df.index <= '2023-09-25 12:00:00')]\n\n# B ë¬¼í’ˆì˜ ë§¤ì¶œì•¡ ì´í•©\ncond1 = df[df['ë¬¼í’ˆ']=='B']\nprint(cond1['ë§¤ì¶œì•¡'].sum())\n\n26500\n\n\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\1645811142.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\n\n\n# ë¬¸ì œ 27\n# 9ì›” 24ì¼ 12:00~24:00ê¹Œì§€ì˜ Aë¬¼í’ˆì˜ ë§¤ì¶œì•¡ ì´í•©ì„ êµ¬í•˜ì‹œì˜¤\n# (ë§¤ì¶œì•¡ = íŒë§¤ìˆ˜*ê°œë‹¹ìˆ˜ìµ)\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë¬¼í’ˆ' : ['A','B','A','B','A','B','A'],\n    'íŒë§¤ìˆ˜' : [5,10,15,15,20,25,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [500,600,500,600,600,700,600]})\ndf['time'] = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf = df[['time', 'ë¬¼í’ˆ', 'íŒë§¤ìˆ˜', 'ê°œë‹¹ìˆ˜ìµ']]\ndf = df.set_index('time',drop=True)\ndf\n\n\n\n\n\n  \n    \n      \n      ë¬¼í’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n    \n      time\n      \n      \n      \n    \n  \n  \n    \n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# (í’€ì´)\n# ë§¤ì¶œì•¡ ë³€ìˆ˜ ì¶”ê°€\ndf['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\n# loc í•¨ìˆ˜ë¡œ í•„í„°ë§\ndf = df.loc[(df.index >= '2023-09-24 12:00:00') & (df.index < '2023-09-25 00:00:00')]\n\n# A ë¬¼í’ˆì˜ ë§¤ì¶œì•¡ ì´í•©\ncond1 = df[df['ë¬¼í’ˆ']=='A']\nprint(cond1['ë§¤ì¶œì•¡'].sum())\n\n10000"
  },
  {
    "objectID": "OpenData_Analysis.html",
    "href": "OpenData_Analysis.html",
    "title": "OpenData Analysis",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nNumpy\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2023\n\n\nHyunsoo Kim\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Regression_Analysis/ì‹¤ê¸°_1ìœ í˜•.html",
    "href": "Regression_Analysis/ì‹¤ê¸°_1ìœ í˜•.html",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 1ìœ í˜• ë¬¸ì œí’€ì´",
    "section": "",
    "text": "ì‹¤ê¸° 1 ìœ í˜•"
  },
  {
    "objectID": "Regression_Analysis/ì‹¤ê¸°_1ìœ í˜•.html#ë°ì´í„°-ë‹¤ë£¨ê¸°-ìœ í˜•",
    "href": "Regression_Analysis/ì‹¤ê¸°_1ìœ í˜•.html#ë°ì´í„°-ë‹¤ë£¨ê¸°-ìœ í˜•",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 1ìœ í˜• ë¬¸ì œí’€ì´",
    "section": "âœ… ë°ì´í„° ë‹¤ë£¨ê¸° ìœ í˜•",
    "text": "âœ… ë°ì´í„° ë‹¤ë£¨ê¸° ìœ í˜•\n\n\në°ì´í„° íƒ€ì…(object, int, float, bool ë“±)\nê¸°ì´ˆí†µê³„ëŸ‰(í‰ê· , ì¤‘ì•™ê°’, ì‚¬ë¶„ìœ„ìˆ˜, IQR, í‘œì¤€í¸ì°¨ ë“±)\në°ì´í„° ì¸ë±ì‹±, í•„í„°ë§, ì •ë ¬, ë³€ê²½ ë“±\nì¤‘ë³µê°’, ê²°ì¸¡ì¹˜, ì´ìƒì¹˜ ì²˜ë¦¬ (ì œê±° or ëŒ€ì²´)\në°ì´í„° Scaling (ë°ì´í„° í‘œì¤€í™”(z), ë°ì´í„° ì •ê·œí™”(min-max))\në°ì´í„° í•©ì¹˜ê¸°\në‚ ì§œ/ì‹œê°„ ë°ì´í„°, index ë‹¤ë£¨ê¸°"
  },
  {
    "objectID": "Regression_Analysis/ì‹¤ê¸°_1ìœ í˜•.html#í•µì‹¬ë¬¸ì œ-27ê°œ-110",
    "href": "Regression_Analysis/ì‹¤ê¸°_1ìœ í˜•.html#í•µì‹¬ë¬¸ì œ-27ê°œ-110",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 1ìœ í˜• ë¬¸ì œí’€ì´",
    "section": "âœ… í•µì‹¬ë¬¸ì œ 27ê°œ (1~10)",
    "text": "âœ… í•µì‹¬ë¬¸ì œ 27ê°œ (1~10)\n\n# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\nimport pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"mtcars.txt\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      Hornet Sportabout\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\n# ë¬¸ì œ 1\n# mpg ë³€ìˆ˜ì˜ ì œ 1ì‚¬ë¶„ìœ„ìˆ˜ë¥¼ êµ¬í•˜ê³  ì •ìˆ˜ê°’ìœ¼ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (í’€ì´)\nQ1 = df['mpg'].quantile(.25)\nprint(round(Q1))\n\n15\n\n\n\n# ë¬¸ì œ 2\n# mpg ê°’ì´ 19ì´ìƒ 21ì´í•˜ì¸ ë°ì´í„°ì˜ ìˆ˜ë¥¼ êµ¬í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (í’€ì´ 1)\ncond1 = (df[(df['mpg']>=19) & (df['mpg']<=21)])\nprint(len(cond1))\n\n# (í’€ì´ 2)\n# cond1 = (df['mpg']>=19)\n# cond2 = (df['mpg']<=21)\n# print(len(df[cond1&cond2]))\n\n5\n\n\n\n# ë¬¸ì œ 3\n# hp ë³€ìˆ˜ì˜ IQR ê°’ì„ êµ¬í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (í’€ì´)\nQ1 = df['hp'].quantile(.25)\nQ3 = df['hp'].quantile(.75)\nIQR = Q3 - Q1\nprint(IQR)\n\n83.5\n\n\n\n# ë¬¸ì œ 4 \n# wt ë³€ìˆ˜ì˜ ìƒìœ„ 10ê°œ ê°’ì˜ ì´í•©ì„ êµ¬í•˜ì—¬ ì†Œìˆ˜ì ì„ ë²„ë¦¬ê³  ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (í’€ì´)\ntop10 = df.sort_values('wt', ascending=False)\nsum_top10 = sum(top10['wt'].head(10))\nprint(int(sum_top10))\n\n# top_10 = df.sort_values('wt',ascending=False).head(10)\n# print(int(sum(top_10['wt']))) #ì£¼ì˜: ì†Œìˆ˜ì  ë°˜ì˜¬ë¦¼ì´ ì•„ë‹ˆë¼ ë²„ë¦¬ëŠ” ë¬¸ì œ\n\n42\n\n\n\n# ë¬¸ì œ 5\n# ì „ì²´ ìë™ì°¨ì—ì„œ cylê°€ 6ì¸ ë¹„ìœ¨ì´ ì–¼ë§ˆì¸ì§€ ì†Œìˆ˜ì  ì²«ì§¸ì§œë¦¬ê¹Œì§€ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\ncond1 = len(df[df['cyl']==6])\ncond2 = len(df['cyl'])\nprint(round(cond1/cond2, 1))\n\n0.2\n\n\n\n# ë¬¸ì œ 6\n# ì²«ë²ˆì§¸ í–‰ë¶€í„° ìˆœì„œëŒ€ë¡œ 10ê°œ ë½‘ì€ í›„ mpg ì—´ì˜ í‰ê· ê°’ì„ ë°˜ì˜¬ë¦¼í•˜ì—¬ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (í’€ì´)\ncond1 = df.head(10) # df[:10], df.loc[0:9]\nmean_mpg = cond1['mpg'].mean()\nprint(round(mean_mpg))\n\n20\n\n\n\n# ë¬¸ì œ 7\n# ì²«ë²ˆì§¸ í–‰ë¶€í„° ìˆœì„œëŒ€ë¡œ 50% ê¹Œì§€ ë°ì´í„°ë¥¼ ë½‘ì•„ wt ë³€ìˆ˜ì˜ ì¤‘ì•™ê°’ì„ êµ¬í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (í’€ì´)\n# 50% ë°ì´í„°ì— í•´ë‹¹í•˜ëŠ” í–‰ì˜ ìˆ˜ (ì •ìˆ˜ë¡œ êµ¬í•˜ê¸°)\np50 = int(len(df)*0.5)\ndf50 = df[:p50]\nprint(df50['wt'].median())\n\n# len(df)/2\n# cond1 = df[:17]\n# cond2 = cond1['wt'].median()\n# print(cond2)\n\n3.44\n\n\n\n# ë¬¸ì œ 8\n# ê²°ì¸¡ê°’ì´ ìˆëŠ” ë°ì´í„°ì˜ ìˆ˜ë¥¼ ì¶œë ¥í•˜ì‹œì˜¤\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ì œí’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3.0\n      300\n    \n    \n      1\n      20220105\n      B\n      NaN\n      400\n    \n    \n      2\n      None\n      None\n      5.0\n      500\n    \n    \n      3\n      20230127\n      B\n      10.0\n      600\n    \n    \n      4\n      20220203\n      A\n      10.0\n      400\n    \n  \n\n\n\n\n\n# (í’€ì´)\nm_value = df.isnull().sum()\nprint(m_value.sum())\n\n5\n\n\n\n# ë¬¸ì œ 9 \n# 'íŒë§¤ìˆ˜' ì»¬ëŸ¼ì˜ ê²°ì¸¡ê°’ì„ íŒë§¤ìˆ˜ì˜ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´í•˜ê³  íŒë§¤ìˆ˜ì˜ \n#  í‰ê· ê°’ì„ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\n\n\n# (í’€ì´)\ndf[df['íŒë§¤ìˆ˜'].isnull()]\nmedian = df['íŒë§¤ìˆ˜'].median()\ndf['íŒë§¤ìˆ˜'] = df['íŒë§¤ìˆ˜'].fillna(median)\nmean = df['íŒë§¤ìˆ˜'].mean()\nprint(int(mean))\n\n15\n\n\n\n# ë¬¸ì œ 10\n# íŒë§¤ìˆ˜ ì»¬ëŸ¼ì— ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ì„ ì œê±°í•˜ê³ \n# ì²«ë²ˆì§¸ í–‰ë¶€í„° ìˆœì„œëŒ€ë¡œ 50%ê¹Œì§€ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬\n# íŒë§¤ìˆ˜ ë³€ìˆ˜ì˜ Q1(ì œ1ì‚¬ë¶„ìœ„ìˆ˜) ê°’ì„ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\n\n\n# (í’€ì´)\n# ê²°ì¸¡ì¹˜ ì‚­ì œ(í–‰ ê¸°ì¤€)\ndf = df['íŒë§¤ìˆ˜'].dropna()\n# ì²«ë²ˆì§¸ í–‰ë¶€í„° ìˆœì„œëŒ€ë¡œ 50%ê¹Œì§€ì˜ ë°ì´í„° ì¶”ì¶œ\nper50 = int(len(df)*0.5)\ndf = df[:per50]\n# ê°’êµ¬í•˜ê¸°\nprint(round(df.quantile(.25)))\n\n# df[df['íŒë§¤ìˆ˜'].isnull()]\n# df['íŒë§¤ìˆ˜'] = df['íŒë§¤ìˆ˜'].dropna()\n# int(len(df['íŒë§¤ìˆ˜'])/2)\n# cond1 = df['íŒë§¤ìˆ˜'].head(6)\n# Q1 = cond1.quantile(.25)\n# print(int(Q1))\n\n5"
  },
  {
    "objectID": "Regression_Analysis/ì‹¤ê¸°_1ìœ í˜•.html#í•µì‹¬ë¬¸ì œ-27ê°œ-1120",
    "href": "Regression_Analysis/ì‹¤ê¸°_1ìœ í˜•.html#í•µì‹¬ë¬¸ì œ-27ê°œ-1120",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 1ìœ í˜• ë¬¸ì œí’€ì´",
    "section": "âœ… í•µì‹¬ë¬¸ì œ 27ê°œ (11~20)",
    "text": "âœ… í•µì‹¬ë¬¸ì œ 27ê°œ (11~20)\n\ndf = pd.read_csv('mtcars.txt')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      Hornet Sportabout\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\n# ë¬¸ì œ 11\n# cylê°€ 4ì¸ ìë™ì°¨ì™€ 6ì¸ ìë™ì°¨ ê·¸ë£¹ì˜ mpg í‰ê· ê°’ì´ ì°¨ì´ë¥¼ ì ˆëŒ€ê°’ìœ¼ë¡œ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\ncyl_4 = df[df['cyl']==4]\ncyl_4_mean = cyl_4['mpg'].mean()\n\ncyl_6 = df[df['cyl']==6]\ncyl_6_mean = cyl_6['mpg'].mean()\n\nprint(round(abs(cyl_4_mean - cyl_6_mean)))\n\n7\n\n\n\n# ë¬¸ì œ 12\n# hp ë³€ìˆ˜ì— ëŒ€í•´ ë°ì´í„°í‘œì¤€í™”(Z-score)ë¥¼ ì§„í–‰í•˜ê³  ì´ìƒì¹˜ì˜ ìˆ˜ë¥¼ êµ¬í•˜ì‹œì˜¤\n# (ë‹¨, ì´ìƒì¹˜ëŠ” Zê°’ì´ 1.5ë¥¼ ì´ˆê³¼í•˜ê±°ë‚˜ -1.5ë¯¸ë§Œì¸ ê°’ì´ë‹¤)\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\n# Z = (X-í‰ê· ) / í‘œì¤€í¸ì°¨\nstd = df['hp'].std()\nmean_hp = df['hp'].mean()\ndf['zscore'] = (df['hp']-mean_hp)/std\n\ncond1 = (df['zscore']>1.5)\ncond2 = (df['zscore']<-1.5)\nprint(len(df[cond1] + df[cond2]))\n\n2\n\n\n\n# ë¬¸ì œ 13\n# mpg ì»¬ëŸ¼ì„ ìµœì†ŒìµœëŒ€ Scalingì„ ì§„í–‰í•œ í›„ 0.7ë³´ë‹¤ í° ê°’ì„ ê°€ì§€ëŠ” ë ˆì½”ë“œ ìˆ˜ë¥¼ êµ¬í•˜ë¼\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\nfrom sklearn.preprocessing import MinMaxScaler\nmscaler = MinMaxScaler()\ndf['mpg'] = mscaler.fit_transform(df[['mpg']])\nprint(len(df[df['mpg']>0.7]))\n\n# ê³µì‹ : (x-min) / (max-min)\n# min_mpg = df['mpg'].min()\n# max_mpg = df['mpg'].max()\n# df['mpg'] = (df['mpg'] - min_mpg)/(max_mpg - min_mpg)\n# cond1 = (df['mpg']>0.7)\n# print(len(df[cond1]))\n\n5\n\n\n\n# ë¬¸ì œ 14\n# wt ì»¬ëŸ¼ì— ëŒ€í•´ ìƒìê·¸ë¦¼ ê¸°ì¤€ìœ¼ë¡œ ì´ìƒì¹˜ì˜ ê°œìˆ˜ë¥¼ êµ¬í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\n# ì´ìƒì¹˜ : Q1, Q3 ë¡œë¶€í„° 1.5*IQRì„ ë„˜ì–´ê°€ëŠ” ê°’\nQ1 = df['wt'].quantile(.25)\nQ3 = df['wt'].quantile(.75)\nIQR = Q3-Q1\n\nupper = Q3 + (1.5*IQR)\nlower = Q1 - (1.5*IQR)\n\ncond1 = (df['wt'] > upper)\ncond2 = (df['wt'] < lower)\n\nprint(len(df[cond1] + df[cond2]))\n\n3\n\n\n\n# ë¬¸ì œ 15\n# íŒë§¤ìˆ˜ ì»¬ëŸ¼ì˜ ê²°ì¸¡ì¹˜ë¥¼ ìµœì†Œê°’ìœ¼ë¡œ ëŒ€ì²´í•˜ê³ \n# ê²°ì¸¡ì¹˜ê°€ ìˆì„ ë•Œì™€ ìµœì†Œê°’ìœ¼ë¡œ ëŒ€ì²´í–ˆì„ ë•Œ\n# í‰ê· ê°’ì˜ ì°¨ì´ë¥¼ ì ˆëŒ€ê°’ìœ¼ë¡œ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ì œí’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3.0\n      300\n    \n    \n      1\n      20220105\n      B\n      NaN\n      400\n    \n    \n      2\n      None\n      None\n      5.0\n      500\n    \n    \n      3\n      20230127\n      B\n      10.0\n      600\n    \n    \n      4\n      20220203\n      A\n      10.0\n      400\n    \n    \n      5\n      20220205\n      None\n      10.0\n      500\n    \n    \n      6\n      20230210\n      A\n      15.0\n      500\n    \n    \n      7\n      20230223\n      B\n      15.0\n      600\n    \n    \n      8\n      20230312\n      A\n      20.0\n      600\n    \n    \n      9\n      20230422\n      B\n      NaN\n      700\n    \n    \n      10\n      20220505\n      A\n      30.0\n      600\n    \n    \n      11\n      20230511\n      A\n      40.0\n      600\n    \n  \n\n\n\n\n\n# (í’€ì´)\n# ë°ì´í„° ë³µì‚¬\ndf2 = df.copy()\n# ìµœì†Œê°’ìœ¼ë¡œ ê²°ì¸¡ì¹˜ ëŒ€ì²´\nmin = df['íŒë§¤ìˆ˜'].min()\ndf2['íŒë§¤ìˆ˜'] = df2['íŒë§¤ìˆ˜'].fillna(min)\n\n# ê²°ì¸¡ì¹˜ê°€ ìˆì„ë•Œ, ëŒ€ì²´í–ˆì„ë•Œ í‰ê· \nm_yes = df['íŒë§¤ìˆ˜'].mean()\nm_no = df2['íŒë§¤ìˆ˜'].mean()\n\nprint(round(abs(m_yes - m_no)))\n\n2\n\n\n\n# ë¬¸ì œ 16\n# vsë³€ìˆ˜ê°€ 0ì´ ì•„ë‹Œ ì°¨ëŸ‰ ì¤‘ì— mpg ê°’ì´ ê°€ì¥ í° ì°¨ëŸ‰ì˜ hp ê°’ì„ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\ncond1 = df[df['vs']!=0]\n# mpg ê°’ì´ ê°€ì¥ í° ì°¨ëŸ‰(ë‚´ë¦¼ì°¨ìˆœ)\ncond1 = cond1.sort_values('mpg',ascending=False)\n# cond1\nprint(cond1['hp'].iloc[0])\n\n65\n\n\n\n# ë¬¸ì œ 17\n# gear ë³€ìˆ˜ê°’ì´ 3, 4ì¸ ë‘ ê·¸ë£¹ì˜ hp í‘œì¤€í¸ì°¨ê°’ì˜ ì°¨ì´ë¥¼ ì ˆëŒ€ê°’ìœ¼ë¡œ \n# ì†Œìˆ˜ì  ì²«ì§¸ìë¦¬ê¹Œì§€ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\n# ë‘ê°œ ê·¸ë£¹ìœ¼ë¡œ í•„í„°ë§\ncond1 = df[df['gear']==3]\ncond2 = df[df['gear']==4]\n# std êµ¬í•˜ê¸°\nstd3 = cond1['hp'].std()\nstd4 = cond2['hp'].std()\n\nprint(round(abs(std3 - std4),1))\n\n21.8\n\n\n\n# ë¬¸ì œ 18\n# gear ë³€ìˆ˜ì˜ ê°‘ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ mpg í‰ê· ê°’ì„ ì‚°ì¶œí•˜ê³ \n# í‰ê· ê°’ì´ ë†’ì€ ê·¸ë£¹ì˜ mpg ì œ3ì‚¬ë¶„ìœ„ìˆ˜ ê°’ì„ êµ¬í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\ndf['gear'].unique()\ngear3 = df[df['gear']==3]\ngear4 = df[df['gear']==4]\ngear5 = df[df['gear']==5]\n\nm_3 = gear3['mpg'].mean()\nm_4 = gear4['mpg'].mean()\nm_5 = gear5['mpg'].mean()\n\n(m_3, m_4, m_5) # m_4ì˜ í‰ê· ê°’ì´ ê°€ì¥ í¼\nQ3 = gear4['mpg'].quantile(.75)\nprint(Q3)\n\n28.075\n\n\n\n# (í’€ì´_v2)\n# gear, mpg ë³€ìˆ˜ë§Œ í•„í„°ë§\ndf = df.loc[:, ['gear', 'mpg']]\n\n# gear ë³€ìˆ˜ë¡œ ê·¸ë£¹í™”í•˜ì—¬ mpg í‰ê· ê°’ ë³´ê¸°\n# print(df.groupby('gear').mean()) # gear 4ê·¸ë£¹ì´ ì œì¼ ë†’ìŒ\n\n# gear=4ì¸ ê·¸ë£¹ì˜ mpg Q3 êµ¬í•˜ê¸°\ngear4 = df[df['gear']==4]\nprint(gear4['mpg'].quantile(.75))\n\n28.075\n\n\n\n# ë¬¸ì œ 19\n# hp í•­ëª©ì˜ ìƒìœ„ 7ë²ˆì§¸ ê°’ìœ¼ë¡œ ìƒìœ„ 7ê°œ ê°’ì„ ë³€í™˜í•œ í›„,\n# hpê°€ 150 ì´ìƒì¸ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ hpì˜ í‰ê· ê°’ì„ ë°˜ì˜¬ë¦¼í•˜ì—¬ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\n# hp ì—´ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬\ndf = df.sort_values('hp', ascending=False)\n\n# ì¸ë±ìŠ¤ ì´ˆê¸°í™” - ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬ì‹œ ìµœì´ˆì˜ ì¸ë±ìŠ¤ë¡œ ìˆê¸°ì—\ndf = df.reset_index(drop=True) # drop=True ëŠ” ê¸°ì¡´ index ì‚­ì œ\n# print(df)\n\n# hp ìƒìœ„ 7ë²ˆì§¸ ê°’ í™•ì¸\ntop7 = df['hp'].loc[6] # top7 = 205\n\n# np.where í™œìš©\nimport numpy as np\ndf['hp'] = np.where(df['hp'] >= 205, top7, df['hp'])\n# np.where(ì¡°ê±´, ì¡°ê±´ì— í•´ë‹¹í•  ë•Œ ê°’, ê·¸ë ‡ì§€ ì•Šì„ ë•Œ ê°’)\n\n# hp 150ì´ìƒì¸ ë°ì´í„°\ncond1 = (df['hp']>=150)\ndf = df[cond1]\nprint(round(df['hp'].mean()))\n\n187\n\n\n\n# ë¬¸ì œ 20\n# car ë³€ìˆ˜ì— Merc ë¬´êµ¬ê°€ í¬í•¨ëœ ìë™ì°¨ì˜ mpg í‰ê· ê°’ì„ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\ndf2 = df[df['car'].str.contains('Merc')] # ë¬¸ìì—´ ì¶”ì¶œ ì•Œì•„ë‘ê¸°\nprint(round(df2['mpg'].mean()))\n\n# ì‹œí—˜í™˜ê²½ì—ì„œ ë‹µêµ¬í•˜ëŠ” ë°©ë²•(reset_index() ì‚¬ìš© í›„)\n# ì‹œí—˜ì—ì„œëŠ” carê°€ indexë¡œ ë˜ì–´ ìˆìŒ\n# df.reset_index(inplace=True)\n# df2 = df[df['index'].str.contains(\"Merc\")] \n# print(round(df2['mpg'].mean()))\n\n19"
  },
  {
    "objectID": "Regression_Analysis/ì‹¤ê¸°_1ìœ í˜•.html#í•µì‹¬ë¬¸ì œ-27ê°œ-2127",
    "href": "Regression_Analysis/ì‹¤ê¸°_1ìœ í˜•.html#í•µì‹¬ë¬¸ì œ-27ê°œ-2127",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 1ìœ í˜• ë¬¸ì œí’€ì´",
    "section": "âœ… í•µì‹¬ë¬¸ì œ 27ê°œ (21~27)",
    "text": "âœ… í•µì‹¬ë¬¸ì œ 27ê°œ (21~27)\n\n# ë¬¸ì œ 21\n# 22ë…„ 1ë¶„ê¸° Aì œí’ˆì˜ ë§¤ì¶œì•¡ì„ êµ¬í•˜ì‹œì˜¤\n# (ë§¤ì¶œì•¡ = íŒë§¤ìˆ˜*ê°œë‹¹ìˆ˜ìµ)\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ì œí’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# # (í’€ì´)\n# cond1 = df[df['ë‚ ì§œ']<='20220431']\n# cond2 = cond1[cond1['ì œí’ˆ']=='A']\n# cond2['ë§¤ì¶œì•¡'] = (cond2['íŒë§¤ìˆ˜']*cond2['ê°œë‹¹ìˆ˜ìµ'])\n# cond2\n\n# print(cond2['ë§¤ì¶œì•¡'].sum())\n\n\n# (í’€ì´_v2)\n# ë°ì´í„° íƒ€ì… datetimeìœ¼ë¡œ ë³€ê²½\ndf['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])\n\n# ë…„, ì›”, ì¼ ë³€ìˆ˜(ì—´) ì¶”ê°€í•˜ê¸°\ndf['year'] = df['ë‚ ì§œ'].dt.year\ndf['month'] = df['ë‚ ì§œ'].dt.month\ndf['day'] = df['ë‚ ì§œ'].dt.day\n\n# ë§¤ì¶œì•¡ ë³€ìˆ˜ ì¶”ê°€í•˜ê¸°\ndf['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\n# 22ë…„ìœ¼ë¡œ í•„í„°ë§\ndf = df[df['year']==2022]\ndf.head()\n\n# 1,2,3ì›” ë§¤ì¶œì•¡ ê³„ì‚°\nm1 = df[df['month']==1]['ë§¤ì¶œì•¡'].sum()\nm2 = df[df['month']==2]['ë§¤ì¶œì•¡'].sum()\nm3 = df[df['month']==3]['ë§¤ì¶œì•¡'].sum()\nprint(m1+m2+m3)\n\n11900\n\n\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['year'] = df['ë‚ ì§œ'].dt.year\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['month'] = df['ë‚ ì§œ'].dt.month\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['day'] = df['ë‚ ì§œ'].dt.day\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\n\n\n# ë¬¸ì œ 22\n# 22ë…„ê³¼ 23ë…„ì˜ ì´ ë§¤ì¶œì•¡ ì°¨ì´ë¥¼ ì ˆëŒ€ê°’ìœ¼ë¡œ êµ¬í•˜ì‹œì˜¤\n# (ë§¤ì¶œì•¡ = íŒë§¤ìˆ˜*ê°œë‹¹ìˆ˜ìµ)\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ì œí’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# (í’€ì´)\n# ë°ì´í„° íƒ€ì… datetimeìœ¼ë¡œ ë³€ê²½\ndf['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])\n\n# ë…„ ë³€ìˆ˜(ì—´) ì¶”ê°€í•˜ê¸°\ndf['year'] = df['ë‚ ì§œ'].dt.year\n\n# ë§¤ì¶œì•¡ ë³€ìˆ˜ ì¶”ê°€í•˜ê¸°\ndf['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\ncond22 = df[df['year']==2022]\ncond23 = df[df['year']==2023]\n\nprint(abs((cond22['ë§¤ì¶œì•¡'].sum()) - (cond23['ë§¤ì¶œì•¡'].sum())))\n\n48600\n\n\n\n# ë¬¸ì œ 23\n# 23ë…„ ì´ ë§¤ì¶œì•¡ì´ í° ì œí’ˆì˜ 23ë…„ íŒë§¤ìˆ˜ë¥¼ êµ¬í•˜ì‹œì˜¤\n# (ë§¤ì¶œì•¡ = íŒë§¤ìˆ˜*ê°œë‹¹ìˆ˜ìµ)\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ì œí’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# ë°ì´í„° íƒ€ì… datetimeìœ¼ë¡œ ë³€ê²½\ndf['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])\n\n# ë…„ ë³€ìˆ˜(ì—´) ì¶”ê°€í•˜ê¸°\ndf['year'] = df['ë‚ ì§œ'].dt.year\n\n# ë§¤ì¶œì•¡ ë³€ìˆ˜ ì¶”ê°€í•˜ê¸°\ndf['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\ndf = df[df['year']==2023]\n# df.head()\n\n# 23ë…„ A ë§¤ì¶œì•¡ê³¼ B ë§¤ì¶œì•¡ ë³„ë„ë¡œ êµ¬í•˜ê¸°\n# A ë§¤ì¶œì•¡\ndf_a = df[df['ì œí’ˆ']=='A']\na_sales = df_a['ë§¤ì¶œì•¡'].sum()\n# print(a_sales) # 46000\n\n# B ë§¤ì¶œì•¡\ndf_b = df[df['ì œí’ˆ']=='B']\nb_sales = df_b['ë§¤ì¶œì•¡'].sum()\n# print(b_sales) # 32500\n\n# A ì œí’ˆì˜ 23ë…„ íŒë§¤ìˆ˜\na_sum = df_a['íŒë§¤ìˆ˜'].sum()\nprint(a_sum)\n\n80\n\n\n\n# ë¬¸ì œ 24\n# ë§¤ì¶œì•¡ì´ 4ì²œì› ì´ˆê³¼, 1ë§Œì› ë¯¸ë§Œì¸ ë°ì´í„° ìˆ˜ë¥¼ ì¶œë ¥í•˜ì‹œì˜¤\n# (ë§¤ì¶œì•¡ = íŒë§¤ìˆ˜*ê°œë‹¹ìˆ˜ìµ)\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ì œí’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# (í’€ì´)\ndf['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\ncond1 = df['ë§¤ì¶œì•¡']>4000\ncond2 = df['ë§¤ì¶œì•¡']<10000\n\ndf = df[cond1&cond2]\nprint(len(df))\n\n4\n\n\n\n# ë¬¸ì œ 25\n# 23ë…„ 9ì›” 24ì¼ 16:00~22:00 ì‚¬ì´ì— ì „ì²´ ì œí’ˆì˜ íŒë§¤ìˆ˜ë¥¼ êµ¬í•˜ì‹œì˜¤\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë¬¼í’ˆ' : ['A','B','A','B','A','B','A'],\n    'íŒë§¤ìˆ˜' : [5,10,15,15,20,25,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [500,600,500,600,600,700,600]})\ntime = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf['time'] = time\ndf = df[['time', 'ë¬¼í’ˆ', 'íŒë§¤ìˆ˜', 'ê°œë‹¹ìˆ˜ìµ']]\ndf\n\n\n\n\n\n  \n    \n      \n      time\n      ë¬¼í’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      1\n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2\n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      3\n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      4\n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      5\n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      6\n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# (í’€ì´)\n# ì»¬ëŸ¼ê³¼ ì¸ë±ìŠ¤ì— ë‘˜ë‹¤ time ë³€ìˆ˜ë¥¼ ë†“ê³  í’€ì´\n# ë°ì´í„° íƒ€ì… datetimeìœ¼ë¡œ ë³€ê²½ (í•­ìƒ df.info()ë¡œ ë°ì´í„° íƒ€ì… í™•ì¸í•  ê²ƒ)\ndf['time'] = pd.to_datetime(df['time'])\n\n# index ìƒˆë¡œ ì§€ì •\ndf = df.set_index('time', drop=False) # defaultëŠ” True\n\n# 9ì›” 24ì¼ í•„í„°ë§ // df['ë³€ìˆ˜'].between( , )\ndf_after = df[df['time'].between('2023-09-24', '2023-09-25')] # 25ì¼ì€ ë¯¸í¬í•¨\n\n# ì‹œê°„ í•„í„°ë§ 16:00 ~ 22:00 (ì£¼ì˜: ì‹œê°„ì´ indexì— ìœ„ì¹˜í•´ì•¼ í•¨)\ndf = df_after.between_time(start_time='16:00', end_time='22:00') # í¬í•¨ ê¸°ì¤€\n\n# ì „ì²´ íŒë§¤ìˆ˜ êµ¬í•˜ê¸°\nprint(df['íŒë§¤ìˆ˜'].sum())\n\n25\n\n\n\n# (í’€ì´2) // ìœ„ì— df ìƒˆë¡œ ë¶ˆëŸ¬ì™€ì„œ ì‹¤í–‰í•´ë³´ê¸°\n# ë°ì´í„° íƒ€ì… datetimeìœ¼ë¡œ ë³€ê²½\ndf['time'] = pd.to_datetime(df['time'])\n\n# index ìƒˆë¡œ ì§€ì •\ndf = df.set_index('time')\n\n# loc í•¨ìˆ˜ë¡œ í•„í„°ë§\ndf = df.loc[(df.index >= '2023-09-24 16:00:00') & (df.index <= '2023-09-24 22:00:00')]\n\n# ì „ì²´ íŒë§¤ìˆ˜ êµ¬í•˜ê¸°\nprint(df['íŒë§¤ìˆ˜'].sum())\n\n25\n\n\n\n# ë¬¸ì œ 26\n# 9ì›” 25ì¼ 00:10~12:00ê¹Œì§€ì˜ Bë¬¼í’ˆì˜ ë§¤ì¶œì•¡ ì´í•©ì„ êµ¬í•˜ì‹œì˜¤\n# (ë§¤ì¶œì•¡ = íŒë§¤ìˆ˜*ê°œë‹¹ìˆ˜ìµ)\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë¬¼í’ˆ' : ['A','B','A','B','A','B','A'],\n    'íŒë§¤ìˆ˜' : [5,10,15,15,20,25,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [500,600,500,600,600,700,600]})\ndf['time'] = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf = df[['time', 'ë¬¼í’ˆ', 'íŒë§¤ìˆ˜', 'ê°œë‹¹ìˆ˜ìµ']]\ndf = df.set_index('time',drop=True)\ndf\n\n\n\n\n\n  \n    \n      \n      ë¬¼í’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n    \n      time\n      \n      \n      \n    \n  \n  \n    \n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# (í’€ì´)\n# ë§¤ì¶œì•¡ ë³€ìˆ˜ ì¶”ê°€\ndf['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\n# loc í•¨ìˆ˜ë¡œ í•„í„°ë§\ndf = df.loc[(df.index >= '2023-09-25 00:10:00') & (df.index <= '2023-09-25 12:00:00')]\n\n# B ë¬¼í’ˆì˜ ë§¤ì¶œì•¡ ì´í•©\ncond1 = df[df['ë¬¼í’ˆ']=='B']\nprint(cond1['ë§¤ì¶œì•¡'].sum())\n\n26500\n\n\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\1645811142.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\n\n\n# ë¬¸ì œ 27\n# 9ì›” 24ì¼ 12:00~24:00ê¹Œì§€ì˜ Aë¬¼í’ˆì˜ ë§¤ì¶œì•¡ ì´í•©ì„ êµ¬í•˜ì‹œì˜¤\n# (ë§¤ì¶œì•¡ = íŒë§¤ìˆ˜*ê°œë‹¹ìˆ˜ìµ)\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë¬¼í’ˆ' : ['A','B','A','B','A','B','A'],\n    'íŒë§¤ìˆ˜' : [5,10,15,15,20,25,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [500,600,500,600,600,700,600]})\ndf['time'] = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf = df[['time', 'ë¬¼í’ˆ', 'íŒë§¤ìˆ˜', 'ê°œë‹¹ìˆ˜ìµ']]\ndf = df.set_index('time',drop=True)\ndf\n\n\n\n\n\n  \n    \n      \n      ë¬¼í’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n    \n      time\n      \n      \n      \n    \n  \n  \n    \n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# (í’€ì´)\n# ë§¤ì¶œì•¡ ë³€ìˆ˜ ì¶”ê°€\ndf['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\n# loc í•¨ìˆ˜ë¡œ í•„í„°ë§\ndf = df.loc[(df.index >= '2023-09-24 12:00:00') & (df.index < '2023-09-25 00:00:00')]\n\n# A ë¬¼í’ˆì˜ ë§¤ì¶œì•¡ ì´í•©\ncond1 = df[df['ë¬¼í’ˆ']=='A']\nprint(cond1['ë§¤ì¶œì•¡'].sum())\n\n10000"
  },
  {
    "objectID": "Regression_Analysis.html",
    "href": "Regression_Analysis.html",
    "title": "Regression Analysis",
    "section": "",
    "text": "ì˜ˆì œë¥¼ í†µí•œ íšŒê·€ë¶„ì„\n\n\ngetwd() #\"C:/Users/Hyunsoo Kim/Documents/lecture/regression_analysis\"\n\n[1] \"C:/Users/Hyunsoo Kim/Desktop/senior_grade/blog/my-quarto-website\"\n\n\n\n\n\n\n\ndata_2.5<-read.table(\"All_Data/p031.txt\",header=TRUE,sep=\"\\t\")\n\ndim(data_2.5) #14 2\n\n[1] 14  2\n\nhead(data_2.5)\n\n  Minutes Units\n1      23     1\n2      29     2\n3      49     3\n4      64     4\n5      74     4\n6      87     5\n\nX<-data_2.5$Units\n\nY<-data_2.5$Minutes\n\n\n\n\n\ndf<-data.frame(\n\n  #1:length(X),\n\n  Y,\n\n  X,\n\n  Y-mean(Y),\n\n  X-mean(X),\n\n  (Y-mean(Y))^2,\n\n  (X-mean(X))^2,\n\n  (Y-mean(Y))*(X-mean(X))\n\n)\n\ndf\n\n     Y  X Y...mean.Y. X...mean.X. X.Y...mean.Y...2 X.X...mean.X...2\n1   23  1 -74.2142857          -5     5.507760e+03               25\n2   29  2 -68.2142857          -4     4.653189e+03               16\n3   49  3 -48.2142857          -3     2.324617e+03                9\n4   64  4 -33.2142857          -2     1.103189e+03                4\n5   74  4 -23.2142857          -2     5.389031e+02                4\n6   87  5 -10.2142857          -1     1.043316e+02                1\n7   96  6  -1.2142857           0     1.474490e+00                0\n8   97  6  -0.2142857           0     4.591837e-02                0\n9  109  7  11.7857143           1     1.389031e+02                1\n10 119  8  21.7857143           2     4.746173e+02                4\n11 149  9  51.7857143           3     2.681760e+03                9\n12 145  9  47.7857143           3     2.283474e+03                9\n13 154 10  56.7857143           4     3.224617e+03               16\n14 166 10  68.7857143           4     4.731474e+03               16\n   X.Y...mean.Y......X...mean.X..\n1                       371.07143\n2                       272.85714\n3                       144.64286\n4                        66.42857\n5                        46.42857\n6                        10.21429\n7                         0.00000\n8                         0.00000\n9                        11.78571\n10                       43.57143\n11                      155.35714\n12                      143.35714\n13                      227.14286\n14                      275.14286\n\n\n\n\n\n\nCOV_XY<-sum((Y-mean(Y))*(X-mean(X))) / (length(X)-1) #136\n\n### cov() í•¨ìˆ˜\n\ncov(X,Y) #136\n\n[1] 136\n\n### ìƒê´€ê³„ìˆ˜(correalationship)\n\n### cor() í•¨ìˆ˜\n\ncor(X,Y) #0.9936987 \n\n[1] 0.9936987\n\n\n\n\n\n\n\ndata_2.5<-read.table(\"All_Data/p031.txt\",header=TRUE,sep=\"\\t\")\n\nx<-data_2.5$Units\n\ny<-data_2.5$Minutes\n\n\n\n\ncor_xy<- COV_XY / (sd(x)*sd(y))\n\ncor_xy\n\n[1] 0.9936987\n\n### cor() í•¨ìˆ˜\n\ncor(x,y)\n\n[1] 0.9936987\n\ncor(y,x)\n\n[1] 0.9936987\n\ndata_2.5\n\n   Minutes Units\n1       23     1\n2       29     2\n3       49     3\n4       64     4\n5       74     4\n6       87     5\n7       96     6\n8       97     6\n9      109     7\n10     119     8\n11     149     9\n12     145     9\n13     154    10\n14     166    10\n\ncor(data_2.5)\n\n          Minutes     Units\nMinutes 1.0000000 0.9936987\nUnits   0.9936987 1.0000000\n\n\n\n\n\n\nclass(X)\n\n[1] \"numeric\"\n\nclass(Y) #both numeric\n\n[1] \"numeric\"\n\nplot(X,Y, pch=19,xlab=\"Units\",ylab=\"Minutes\") \n\n\n\n\n\n\n\n\ndata_2.3<-read.table(\"All_Data/p029a.txt\",header=TRUE,sep=\"\\t\")\n\ndata_2.3\n\n    Y  X\n1   1 -7\n2  14 -6\n3  25 -5\n4  34 -4\n5  41 -3\n6  46 -2\n7  49 -1\n8  50  0\n9  49  1\n10 46  2\n11 41  3\n12 34  4\n13 25  5\n14 14  6\n15  1  7\n\nX<-data_2.3$X\n\nY<-data_2.3$Y\n\n\n\n\n\nplot(X,Y)\n\n\n\ncor(X,Y) # 0 ì™„ë²½í•˜ê²Œ 2ì°¨í•¨ìˆ˜ì˜ í˜•íƒœë„ 0ì´ ë‚˜ì˜´(ì§ì„ ì˜ í˜•íƒœê°€ ì•„ë‹Œê²ƒë§Œ)\n\n[1] 0\n\n\n\n\n\n\ndata_2.4<-read.table(\"All_Data/p029b.txt\",header=TRUE,sep=\"\\t\")\n\n\n\n\n\nplot(data_2.4$X1,data_2.4$Y1, pch=19); abline(3,0.5) #ê¸°ìš¸ê¸° 3 ì ˆí¸0.5ì¸ ì„ ì„ ì¶”ê°€í•´ë¼ \n\n\n\nplot(data_2.4$X2,data_2.4$Y2, pch=19); abline(3,0.5)\n\n\n\nplot(data_2.4$X3,data_2.4$Y3, pch=19); abline(3,0.5)\n\n\n\nplot(data_2.4$X4,data_2.4$Y4, pch=19); abline(3,0.5)\n\n\n\nm<-matrix(1:4,ncol=2,byrow=TRUE) #2í–‰ì˜ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„± \n\nm\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nlayout(m)\n\nplot(Y1~X1, data=data_2.4,pch=19); abline(3,0.5)\n\n#y~x  -> y=ax+b ì´ëŸ¬í•œ í˜•íƒœë¥¼ ê°€ì§€ëŠ” ëª¨í˜•ì‹ì´ë¼ëŠ” ì˜ë¯¸ \n\nplot(Y2~X2, data=data_2.4,pch=19); abline(3,0.5)\n\nplot(Y3~X3, data=data_2.4,pch=19); abline(3,0.5)\n\nplot(Y4~X4, data=data_2.4,pch=19); abline(3,0.5)\n\n\n\nlayout(1) #ë³€í™˜ì„ ë‹¤ì‹œ í•˜ì§€ ì•Šìœ¼ë©´ ì„¤ì •í•œ ë§¤íŠ¸ë¦­ìŠ¤ì˜ ë¹„ìœ¨ë¡œ ê·¸ë˜í”„ê°€ ê·¸ë ¤ì§ í•´ì œ í•„ìš” \n\n# cor()\n\ncor(data_2.4$X1,data_2.4$Y1) #0.8164205\n\n[1] 0.8164205\n\ncor(data_2.4$X2,data_2.4$Y2) #0.8162365\n\n[1] 0.8162365\n\ncor(data_2.4$X3,data_2.4$Y3) #0.8162867\n\n[1] 0.8162867\n\ncor(data_2.4$X4,data_2.4$Y4) #0.8165214\n\n[1] 0.8165214\n\ncor(data_2.4) #ì´ë ‡ê²Œ í•œë²ˆì— í•  ìˆ˜ ìˆìœ¼ë‚˜ ê°€ë…ì„± ë–¨ì–´ì§ \n\n           Y1         X1         Y2         X2         Y3         X3         Y4\nY1  1.0000000  0.8164205  0.7500054  0.8164205  0.4687167  0.8164205 -0.4891162\nX1  0.8164205  1.0000000  0.8162365  1.0000000  0.8162867  1.0000000 -0.3140467\nY2  0.7500054  0.8162365  1.0000000  0.8162365  0.5879193  0.8162365 -0.4780949\nX2  0.8164205  1.0000000  0.8162365  1.0000000  0.8162867  1.0000000 -0.3140467\nY3  0.4687167  0.8162867  0.5879193  0.8162867  1.0000000  0.8162867 -0.1554718\nX3  0.8164205  1.0000000  0.8162365  1.0000000  0.8162867  1.0000000 -0.3140467\nY4 -0.4891162 -0.3140467 -0.4780949 -0.3140467 -0.1554718 -0.3140467  1.0000000\nX4 -0.5290927 -0.5000000 -0.7184365 -0.5000000 -0.3446610 -0.5000000  0.8165214\n           X4\nY1 -0.5290927\nX1 -0.5000000\nY2 -0.7184365\nX2 -0.5000000\nY3 -0.3446610\nX3 -0.5000000\nY4  0.8165214\nX4  1.0000000\n\n\n\n\n\n\n\n\n\ndata_2.5<-read.table(\"All_Data/p031.txt\",header=TRUE,sep=\"\\t\")\n\nx<-data_2.5$Units\n\ny<-data_2.5$Minutes\n\n\n\n\n\nsum((y-mean(y))*(x-mean(x))) #1768\n\n[1] 1768\n\nsum((x-mean(x))^2) #114\n\n[1] 114\n\nbeta1_hat<-sum((y-mean(y))*(x-mean(x))) / sum((x-mean(x))^2)\n\nbeta1_hat #15.50877\n\n[1] 15.50877\n\nbeta0_hat <- mean(y) - (beta1_hat*mean(x))\n\nbeta0_hat #4.161654\n\n[1] 4.161654\n\n### ìµœì†Œì œê³±íšŒê·€ ë°©ì •ì‹\n\n# Minutes = 4.161654 + 15.50877 * Units\n\nplot(beta0_hat+beta1_hat*x,pch=19);\n\n\n\n# 4ê°œì˜ ê³ ì¥ ë‚œ ë¶€í’ˆì„ ìˆ˜ë¦¬í•˜ëŠ”ë° ê±¸ë¦¬ëŠ” ì—ì¸¡ì‹œê°„\n\n4.161654 + 15.50877 * 4 #66.19673\n\n[1] 66.19673\n\nunits<-4\n\nbeta0_hat + beta1_hat*units\n\n[1] 66.19674\n\n### ì í•©ê°’(Fitted value)\n\ny_hat<-beta0_hat + beta1_hat*(x)\n\n### ìµœì†Œ ì œê³± ì”ì°¨(residual)\n\ne<-y-y_hat\n\ne #í•©ì´ 0ì´ë¼ëŠ” íŠ¹ì§•ì´ ì¡´ì¬\n\n [1]  3.3295739 -6.1791980 -1.6879699 -2.1967419  7.8032581  5.2944862\n [7] -1.2142857 -0.2142857 -3.7230576 -9.2318296  5.2593985  1.2593985\n[13] -5.2493734  6.7506266\n\nsum(e) #1.278977e-13 0ì— ê·¼ì‚¬í•œ ì¶”ì§€ê°€ ë‚˜ì˜´\n\n[1] 1.278977e-13\n\n\n\n\n\n\ndf_2.7<-data.frame(\n\n  x=x,\n\n  y=y,\n\n  y_hat,\n\n  e\n\n)\n\ndf_2.7\n\n    x   y     y_hat          e\n1   1  23  19.67043  3.3295739\n2   2  29  35.17920 -6.1791980\n3   3  49  50.68797 -1.6879699\n4   4  64  66.19674 -2.1967419\n5   4  74  66.19674  7.8032581\n6   5  87  81.70551  5.2944862\n7   6  96  97.21429 -1.2142857\n8   6  97  97.21429 -0.2142857\n9   7 109 112.72306 -3.7230576\n10  8 119 128.23183 -9.2318296\n11  9 149 143.74060  5.2593985\n12  9 145 143.74060  1.2593985\n13 10 154 159.24937 -5.2493734\n14 10 166 159.24937  6.7506266\n\n### lm() í•¨ìˆ˜ (linear model)\n\n# Minutes = beta0 + beta1 * Units + epsilon\n\n# ëª¨í˜•ì‹ : y~x\n\nlm(y~x)\n\n\nCall:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)            x  \n      4.162       15.509  \n\nres_lm<-lm(Minutes~Units,data=data_2.5)\n\nres_lm\n\n\nCall:\nlm(formula = Minutes ~ Units, data = data_2.5)\n\nCoefficients:\n(Intercept)        Units  \n      4.162       15.509  \n\n# ë¦¬ìŠ¤íŠ¸ì˜ ì´ë¦„ \n\nnames(res_lm)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n# íšŒê·€ê³„ìˆ˜\n\nres_lm$coefficients\n\n(Intercept)       Units \n   4.161654   15.508772 \n\ncoef(res_lm)\n\n(Intercept)       Units \n   4.161654   15.508772 \n\n# ì í•©ê°’\n\nres_lm$fitted.values\n\n        1         2         3         4         5         6         7         8 \n 19.67043  35.17920  50.68797  66.19674  66.19674  81.70551  97.21429  97.21429 \n        9        10        11        12        13        14 \n112.72306 128.23183 143.74060 143.74060 159.24937 159.24937 \n\nfitted(res_lm)\n\n        1         2         3         4         5         6         7         8 \n 19.67043  35.17920  50.68797  66.19674  66.19674  81.70551  97.21429  97.21429 \n        9        10        11        12        13        14 \n112.72306 128.23183 143.74060 143.74060 159.24937 159.24937 \n\n# ìµœì†Œì œê³±ì”ì°¨\n\nres_lm$residuals\n\n         1          2          3          4          5          6          7 \n 3.3295739 -6.1791980 -1.6879699 -2.1967419  7.8032581  5.2944862 -1.2142857 \n         8          9         10         11         12         13         14 \n-0.2142857 -3.7230576 -9.2318296  5.2593985  1.2593985 -5.2493734  6.7506266 \n\nresid(res_lm)\n\n         1          2          3          4          5          6          7 \n 3.3295739 -6.1791980 -1.6879699 -2.1967419  7.8032581  5.2944862 -1.2142857 \n         8          9         10         11         12         13         14 \n-0.2142857 -3.7230576 -9.2318296  5.2593985  1.2593985 -5.2493734  6.7506266 \n\nresiduals(res_lm)\n\n         1          2          3          4          5          6          7 \n 3.3295739 -6.1791980 -1.6879699 -2.1967419  7.8032581  5.2944862 -1.2142857 \n         8          9         10         11         12         13         14 \n-0.2142857 -3.7230576 -9.2318296  5.2593985  1.2593985 -5.2493734  6.7506266 \n\n\n\n\n\n\nplot(beta0_hat+beta1_hat*x,pch=19);\n\n#abline(beta0_hat,beta1_hat)\n\nabline(res_lm)\n\n\n\n\n\n\n\n\n\ndata_2.5<-read.table(\"All_Data/p031.txt\",header=TRUE,sep=\"\\t\")\n\nres_lm <- lm(Minutes ~ Units, data = data_2.5)\n\nres_lm\n\n\nCall:\nlm(formula = Minutes ~ Units, data = data_2.5)\n\nCoefficients:\n(Intercept)        Units  \n      4.162       15.509  \n\nres_lm_summ<-summary(res_lm)\n\nres_lm_summ #Pr(>|t|) - p-value\n\n\nCall:\nlm(formula = Minutes ~ Units, data = data_2.5)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.2318 -3.3415 -0.7143  4.7769  7.8033 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    4.162      3.355    1.24    0.239    \nUnits         15.509      0.505   30.71 8.92e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.392 on 12 degrees of freedom\nMultiple R-squared:  0.9874,    Adjusted R-squared:  0.9864 \nF-statistic: 943.2 on 1 and 12 DF,  p-value: 8.916e-13\n\n# unitì€ ì‹œê°„ì— ì˜í–¥ì„ì¤€ë‹¤ ì•½15.5ë¶„ ë§Œí¼ì”© \n\n# coefficientì—ì„œ p-valueì— ëŒ€í•´ì„œ ì•Œ ìˆ˜ ìˆìŒ \n\n# beta_0ëŠ” 0ì´ë¼ê³  ë³´ë©´ë˜ëŠëƒ? p-valueê°€ 0.05ë³´ë‹¤ í¬ê¸°ì— \n\n\n\n\n\n\n\nconfint(res_lm) # beta_0,1ì˜ 95% ì‹ ë¢°êµ¬ê°„ì„ ë½‘ì•„ì¤Œ \n\n                2.5 %   97.5 %\n(Intercept) -3.148482 11.47179\nUnits       14.408512 16.60903\n\n?confint #level = 1-alpha\n\nstarting httpd help server ... done\n\nconfint(res_lm, level=0.90) # 90%ì˜ ì‹ ë¢°êµ¬ê°„\n\n                 5 %     95 %\n(Intercept) -1.81810 10.14141\nUnits       14.60875 16.40879\n\n\n\n\n\n\n# 4ê°œì˜ ê³ ì¥ë‚œ ë¶€í’ˆì„ ìˆ˜ë¦¬í•˜ëŠ” ë°ì— ê±¸ë¦¬ëŠ” ì‹œê°„ ì˜ˆì¸¡\n\nx<-4\n\n4.161654 + 15.508772 *4\n\n[1] 66.19674\n\nres_lm$coefficients[1]+(res_lm$coefficients[2]*x)\n\n(Intercept) \n   66.19674 \n\n### predict()\n\ndf<-data.frame(Units=4) \n\npredict(res_lm,newdata=df) # res_lmì„ ë§Œë“¤ë•Œ ì‚¬ìš©í•œ ë°ì´í„°í˜•ì‹ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ì–´ì•¼í•¨\n\n       1 \n66.19674 \n\nres_lm_pred<-predict(res_lm,newdata=df,se.fit=TRUE)\n\n### ì˜ˆì¸¡ê°’\n\nres_lm_pred$fit\n\n       1 \n66.19674 \n\n### í‘œì¤€ì˜¤ì°¨\n\nres_lm_pred$se.fit # í‰ê· ë°˜ì‘ì— ëŒ€í•œ í‘œì¤€ì˜¤ì°¨ \n\n[1] 1.759688\n\n### ì˜ˆì¸¡í•œê³„\n\ndf<-data.frame(Units=4) #ì˜ˆì œì„œëŠ” 4ëŒ€ê¸°ì¤€\n\ndf<-data.frame(Units=1:10) #ë‹¤ë¥¸ê²ƒë„ ë³´ê³  ì‹¶ì€ê²½ìš° \n\nres_lm_pred_int_p<-predict(res_lm,newdata=df,interval=\"prediction\")\n\n### ì‹ ë¢°í•œê³„\n\ndf<-data.frame(Units=1:10) #ë‹¤ë¥¸ê²ƒë„ ë³´ê³  ì‹¶ì€ê²½ìš° \n\nres_lm_pred_int_c<-predict(res_lm,newdata=df,interval=\"confidence\") #ë‘˜ì˜ ì°¨ì´ë¥¼ ë³´ë©´ ì˜ˆì¸¡í•œê³„ì˜ ë²”ìœ„ê°€ ë”í¼ \n\n### ì˜ˆì¸¡í•œê³„ & ì‹ ë¢°í•œê³„\n\n# ì‹ ë¢°í•œê³„ëŠ” í‰ê· ì—ì„œ ë©€ì–´ì§€ë§Œ ì˜¤ì°¨ì˜ë²”ìœ„ê°€ ì»¤ì§€ê³  í‰ê· ì— ë‹¤ê°€ê°ˆìˆ˜ë¡ ì˜¤ì°¨ê°€ ì¤„ì–´ë“¬\n\nplot(Minutes~Units,data=data_2.5,pch=19)\n\nabline(res_lm,col=\"red\",lwd=2)\n\nlines(1:10,res_lm_pred_int_p[,\"lwr\"],col=\"darkgreen\")\n\nlines(1:10,res_lm_pred_int_p[,\"upr\"],col=\"darkgreen\")\n\nlines(1:10,res_lm_pred_int_c[,\"lwr\"],col=\"blue\")\n\nlines(1:10,res_lm_pred_int_c[,\"upr\"],col=\"blue\")\n\n\n\n\n\n\n\n\nres_lm_summ<-summary(res_lm)\n\nres_lm_summ #Multiple R-squared:0.9874 -> ë°˜ì‘ë³€ìˆ˜ì˜ ì „ì²´ë³€ì´ì¤‘ 98.94%ê°€ ì˜ˆì¸¡ë³€ìˆ˜ì— ì˜í•´ ì„¤ëª…ëœë‹¤\n\n\nCall:\nlm(formula = Minutes ~ Units, data = data_2.5)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.2318 -3.3415 -0.7143  4.7769  7.8033 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    4.162      3.355    1.24    0.239    \nUnits         15.509      0.505   30.71 8.92e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.392 on 12 degrees of freedom\nMultiple R-squared:  0.9874,    Adjusted R-squared:  0.9864 \nF-statistic: 943.2 on 1 and 12 DF,  p-value: 8.916e-13\n\n# ë§Œì•½ R-squaredê°€ 1ì´ë©´ ì™„ë²½í•œ ì„ í˜•ì˜ ê´€ê³„ 100%ë¼ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.\n\n# R-squaredëŠ” ë³€ìˆ˜ê°€ ë“¤ì–´ê°ˆìˆ˜ë¡ ì»¤ì§€ê¸°ì— adjust R-squaredë¥¼ ì‚¬ìš© ì¶”í›„ ì„¤ëª… \n\n\n\n\n\n# Minutes = beta1 + Units + epsilon\n\nres_lm_no<-lm(Minutes~Units-1,data=data_2.5)\n\nres_lm_no\n\n\nCall:\nlm(formula = Minutes ~ Units - 1, data = data_2.5)\n\nCoefficients:\nUnits  \n16.07  \n\nsummary(res_lm_no)\n\n\nCall:\nlm(formula = Minutes ~ Units - 1, data = data_2.5)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5955 -2.4733  0.4417  5.0243  9.7023 \n\nCoefficients:\n      Estimate Std. Error t value Pr(>|t|)    \nUnits  16.0744     0.2213   72.62   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.502 on 13 degrees of freedom\nMultiple R-squared:  0.9975,    Adjusted R-squared:  0.9974 \nF-statistic:  5274 on 1 and 13 DF,  p-value: < 2.2e-16\n\ncoef(summary(res_lm_no)) #rsquared=0.9975\n\n      Estimate Std. Error  t value     Pr(>|t|)\nUnits 16.07443  0.2213341 72.62519 2.380325e-18\n\n\n\n\n\n\ny<-rnorm(30)\n\nt.test(y,mu=0)\n\n\n    One Sample t-test\n\ndata:  y\nt = 0.37112, df = 29, p-value = 0.7132\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.2786540  0.4022005\nsample estimates:\n mean of x \n0.06177325 \n\nsummary(lm(y~1))\n\n\nCall:\nlm(formula = y ~ 1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.02445 -0.52714 -0.08273  0.63884  1.65052 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  0.06177    0.16645   0.371    0.713\n\nResidual standard error: 0.9117 on 29 degrees of freedom\n\n\n\n\n\n\n\n\n\ndata_3.3<-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\ndim(data_3.3)\n\n[1] 30  7\n\nclass(data_3.3)\n\n[1] \"data.frame\"\n\nsapply(data_3.3,class) #all numeric\n\n        Y        X1        X2        X3        X4        X5        X6 \n\"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \n\nsummary(data_3.3) #ëª¨ë“ ë³€ìˆ˜ê°€ numericì´ë©´ ë¶„ìœ„ìˆ˜ë„ ë³´ì—¬ì¤€ë‹¤ \n\n       Y               X1             X2              X3              X4       \n Min.   :40.00   Min.   :37.0   Min.   :30.00   Min.   :34.00   Min.   :43.00  \n 1st Qu.:58.75   1st Qu.:58.5   1st Qu.:45.00   1st Qu.:47.00   1st Qu.:58.25  \n Median :65.50   Median :65.0   Median :51.50   Median :56.50   Median :63.50  \n Mean   :64.63   Mean   :66.6   Mean   :53.13   Mean   :56.37   Mean   :64.63  \n 3rd Qu.:71.75   3rd Qu.:77.0   3rd Qu.:62.50   3rd Qu.:66.75   3rd Qu.:71.00  \n Max.   :85.00   Max.   :90.0   Max.   :83.00   Max.   :75.00   Max.   :88.00  \n       X5              X6       \n Min.   :49.00   Min.   :25.00  \n 1st Qu.:69.25   1st Qu.:35.00  \n Median :77.50   Median :41.00  \n Mean   :74.77   Mean   :42.93  \n 3rd Qu.:80.00   3rd Qu.:47.75  \n Max.   :92.00   Max.   :72.00  \n\n### ì‚°ì ë„ í–‰ë ¬\n\nplot(data_3.3)\n\n\n\n\n\n\n\n\nlm(Y~X1+X2+X3+X4+X5+X6,data=data_3.3)\n\n\nCall:\nlm(formula = Y ~ X1 + X2 + X3 + X4 + X5 + X6, data = data_3.3)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n   10.78708      0.61319     -0.07305      0.32033      0.08173      0.03838  \n         X6  \n   -0.21706  \n\nlm(Y~.,data=data_3.3) # X1+X2+X3+X4+X5+X6ì“°ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ .ì„ ì¨ì„œ ëª¨ë“  ë³€ìˆ˜ë¥¼ ë‹¤ì¨ì¤Œ \n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n   10.78708      0.61319     -0.07305      0.32033      0.08173      0.03838  \n         X6  \n   -0.21706  \n\n\n\n\n\n\ndata_3.3<-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\nlm(Y~X1+X2,data=data_3.3)\n\n\nCall:\nlm(formula = Y ~ X1 + X2, data = data_3.3)\n\nCoefficients:\n(Intercept)           X1           X2  \n   15.32762      0.78034     -0.05016  \n\n# (Intercept)         X1           X2  \n\n#  15.32762      0.78034     -0.05016\n\n# 1) Yì—ì„œ X1 íš¨ê³¼ ì œê±°\n\nm1<-lm(Y~X1,data=data_3.3) # y prime\n\nm1$residuals # x1ì´ ì„¤ëª…í•˜ì§€ ëª»í•œê°’ / x1ì˜ íš¨ê³¼ë¥¼ ì œê±°í•œ ê°’\n\n           1            2            3            4            5            6 \n -9.86142016   0.32865220   3.80099328  -0.91673799   7.76411473 -12.87985944 \n           7            8            9           10           11           12 \n -6.93517726   0.02794419  -4.25432454   6.59248165   9.62936020   7.34709147 \n          13           14           15           16           17           18 \n  7.83787183  -9.00893436   4.51872455  -1.29120309  -4.51815400   5.34709147 \n          19           20           21           22           23           24 \n -2.19900672  -8.14368889   5.43928784   3.59248165 -11.18056744  -2.29688270 \n          25           26           27           28           29           30 \n  7.87475038  -6.48127545   7.02794419  -9.38907907   6.48184600   5.74567546 \n\n# 2) X2ì—ì„œ X1 íš¨ê³¼ ì œê±°\n\nm2<-lm(X2~X1,data=data_3.3) # x2 prime \n\nm2$residuals \n\n          1           2           3           4           5           6 \n-15.1300345  -0.7994502  13.1223579  -6.2864182  -2.9818979   1.8178376 \n          7           8           9          10          11          12 \n-11.3385461  -7.4428019  10.9659742  -5.2603543   6.8439016  -2.7473223 \n         13          14          15          16          17          18 \n  6.2266138  21.4529422  -4.4688659 -15.1382816   1.4268783  15.2526777 \n         19          20          21          22          23          24 \n -8.8776421  19.2787417  -6.4866827   1.7396457  -0.8255141   4.0524132 \n         25          26          27          28          29          30 \n -4.6691304   7.5311341   0.5571981  -4.2082264   8.4268783 -22.0340258 \n\n# 3) X1ì˜ íš¨ê³¼ê°€ ì œê±°ëœ Yì™€ X2ì˜ ì í•© - ì›ì ì„ ì§€ë‚˜ëŠ” íšŒê·€ì„ \n\nlm(m1$residuals~m2$residuals-1) # ì›ì ì„ ì§€ë‚˜ë©´ -1ë¥¼ í•˜ê³  ì§„í–‰ // -3.25e-17\n\n\nCall:\nlm(formula = m1$residuals ~ m2$residuals - 1)\n\nCoefficients:\nm2$residuals  \n    -0.05016  \n\n# ë‹¤ë¥¸ íš¨ê³¼ ì—†ì´(ë‹¤ë¥¸ê°’ì´ ê³ ì •) Yì— ì˜í–¥ì„ ì£¼ëŠ” ìˆœìˆ˜í•œ X2ì˜ ê°’\n\n# m2$residuals  : -0.05016  ==  X2 : -0.05016  \n\n### ë‹¨ìœ„ê¸¸ì´ ì²™ë„í™” - ì˜ì‚¬ìš©í•˜ì§€ì•ŠìŒ\n\nfn_scaling_len<-function(x){\n\n  x0<-x-mean(x)\n\n  x0/sqrt(sum(x0^2))\n\n}\n\ndata_3.3_len<-sapply(data_3.3, fn_scaling_len)\n\ndata_3.3_len<-data.frame(data_3.3_len)\n\nsummary(data_3.3_len)\n\n       Y                  X1                 X2                 X3           \n Min.   :-0.37579   Min.   :-0.41282   Min.   :-0.35109   Min.   :-0.353871  \n 1st Qu.:-0.08975   1st Qu.:-0.11297   1st Qu.:-0.12344   1st Qu.:-0.148193  \n Median : 0.01322   Median :-0.02231   Median :-0.02479   Median : 0.002109  \n Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.000000  \n 3rd Qu.: 0.10857   3rd Qu.: 0.14504   3rd Qu.: 0.14216   3rd Qu.: 0.164278  \n Max.   : 0.31070   Max.   : 0.32635   Max.   : 0.45328   Max.   : 0.294804  \n       X4                 X5                 X6          \n Min.   :-0.38637   Min.   :-0.48356   Min.   :-0.32367  \n 1st Qu.:-0.11401   1st Qu.:-0.10353   1st Qu.:-0.14318  \n Median :-0.02024   Median : 0.05130   Median :-0.03489  \n Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.00000  \n 3rd Qu.: 0.11371   3rd Qu.: 0.09821   3rd Qu.: 0.08693  \n Max.   : 0.41733   Max.   : 0.32341   Max.   : 0.52461  \n\nlm(Y~.,data=data_3.3_len)\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3_len)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n -1.259e-16    6.707e-01   -7.343e-02    3.089e-01    6.981e-02    3.120e-02  \n         X6  \n -1.835e-01  \n\n### í‘œì¤€í™”\n\n# scale()\n\ndata_3.3_std<-scale(data_3.3)\n\n#summary(data_3.3_std)\n\n#sapply(data_3.3_std, sd, na.rm=T)\n\n#class(data_3.3_std) #\"matrix\"\n\ndata_3.3_std<-data.frame(data_3.3_std)\n\n#class(data_3.3_std) #\"data.frame\"\n\n#sapply(data_3.3_std, sd, na.rm=T)\n\nlm(Y~.,data=data_3.3_std) # betaê²Œìˆ˜ êµ¬í•˜ê¸° \n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3_std)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n -7.717e-16    6.707e-01   -7.343e-02    3.089e-01    6.981e-02    3.120e-02  \n         X6  \n -1.835e-01  \n\n\n\n\n\n\nres_lm<-lm(Y~.,data=data_3.3)\n\nres_lm\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n   10.78708      0.61319     -0.07305      0.32033      0.08173      0.03838  \n         X6  \n   -0.21706  \n\nsummary(res_lm)\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.9418  -4.3555   0.3158   5.5425  11.5990 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 10.78708   11.58926   0.931 0.361634    \nX1           0.61319    0.16098   3.809 0.000903 ***\nX2          -0.07305    0.13572  -0.538 0.595594    \nX3           0.32033    0.16852   1.901 0.069925 .  \nX4           0.08173    0.22148   0.369 0.715480    \nX5           0.03838    0.14700   0.261 0.796334    \nX6          -0.21706    0.17821  -1.218 0.235577    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.068 on 23 degrees of freedom\nMultiple R-squared:  0.7326,    Adjusted R-squared:  0.6628 \nF-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05\n\nm1<-summary(lm(Y~X1+X2+X3+X4+X5+X6,data=data_3.3)) #Adjusted R-squared:  0.6628 \n\nm2<-summary(lm(Y~X1+X2+X3+X4+X5,data=data_3.3)) #Adjusted R-squared:  0.6561 \n\n# X6ê°€ ë“¤ì–´ê°€ëŠ” ê²ƒì´ ë” ì¢‹ì€ ëª¨ë¸ \n\nm1$adj.r.squared\n\n[1] 0.662846\n\nm2$adj.r.squared #summaryì—ì„œ ë³´ë‹¤ ë” ì •í™•í•˜ê²Œ ìˆ˜ì¹˜ê°€ ë‚˜ì˜´ \n\n[1] 0.6560539\n\n\n\n\n\n\nres_lm_summ<-summary(res_lm)\n\nres_lm_summ #p-valueì˜ ì¡´ì¬ëŠ” ë¬´ì–¸ê°€ë¥¼ ê²€ì •í–ˆë‹¤ë¼ëŠ” ë°˜ì¦\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.9418  -4.3555   0.3158   5.5425  11.5990 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 10.78708   11.58926   0.931 0.361634    \nX1           0.61319    0.16098   3.809 0.000903 ***\nX2          -0.07305    0.13572  -0.538 0.595594    \nX3           0.32033    0.16852   1.901 0.069925 .  \nX4           0.08173    0.22148   0.369 0.715480    \nX5           0.03838    0.14700   0.261 0.796334    \nX6          -0.21706    0.17821  -1.218 0.235577    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.068 on 23 degrees of freedom\nMultiple R-squared:  0.7326,    Adjusted R-squared:  0.6628 \nF-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05\n\n# p-value<0.05 H_1 ê·€ë¬´ê°€ì„¤ ì±„íƒ \n\n# p-value>0.05 H_0 ì˜ê°€ì„¤ ì±„íƒ // X1ì„ ì œì™¸í•˜ê³ ëŠ” ì˜ê°€ì„¤ ìœ ì˜í•œ ì˜ë¯¸ê°€ ì—†ìŒ(Yì—ì˜í–¥ì£¼ëŠ”)\n\n# ëª¨ë‘ë‹¤ 0ì´ë¼ëŠ” ê°€ì„¤ì„ ê°€ì§€ê³  ë¶„ëª¨ ë¶„ìì˜ ì˜¤ì°¨ê°€ ì¹´ì´ì œê³±ì„ ë”°ë¥´ê³  ê±°ê¸°ì„œ ë‚˜ì˜¨ í†µê³„ëŸ‰\n\n# F-ë¶„í¬ ììœ ë„ëŠ” ë¶„ì ë¶„ëª¨ ë‘ê°œë¥¼ ê°€ì§ //ëª¨ì•„ì„œ ê³„ì‚°ì„ í•˜ê¸°ì— ê°ê° ê³„ì‚°í•˜ëŠ”ê²ƒê³¼ ê²°ê³¼ë‹¤ë¦„ \n\n# ì˜ê°€ì„¤-ëª¨ë“  íšŒê·€ê³„ìˆ˜ê°€ 0ì´ë‹¤.\n\n# ëŒ€ë¦½ê°€ì„¤-ì ì–´ë„ í•˜ë‚˜ëŠ” 0ì´ ì•„ë‹ˆë‹¤. p-value: 1.24e-05 <0.05 ëŒ€ë¦½ê°€ì„¤ ì±„íƒ \n\n# p-valueê°€ 0.05ë³´ë‹¤ ì‘ìœ¼ë©´ ëŒ€ë¦½ê°€ì„¤ ì±„íƒ!!!!!! ê¸°ì–µí•´ \n\n# íšŒê·€ê³„ìˆ˜ì— ëŒ€í•œ ì‹ ë¢°êµ¬ê°„ - 95% ì‹ ë¢°í•œê³„\n\nconfint(res_lm) #-13.18712881 ~ 34.7612816\n\n                   2.5 %     97.5 %\n(Intercept) -13.18712881 34.7612816\nX1            0.28016866  0.9462066\nX2           -0.35381806  0.2077178\nX3           -0.02827872  0.6689430\nX4           -0.37642935  0.5398936\nX5           -0.26570179  0.3424647\nX6           -0.58571106  0.1515977\n\n#X1  0.28016866  0.9462066  ì‚¬ì´ì— 0ì´ ë“¤ì–´ê°€ìˆìœ¼ë©´ ì˜í–¥ì„ ì¤€ë‹¤ë¼ëŠê±¸ ì˜ë¯¸\n\n#X2 -0.35381806  0.2077178  p-valueì—†ì´ë„ ì•Œ ìˆ˜ ìˆìŒ \n\n#X5ê°€ ê°€ì¥ ì˜í–¥ì´ ì ìŒ p-valueê°€ ê°€ì¥ í¬ê¸°ì—(ì˜í–¥ íš¨ê³¼ì˜ í¬ê¸°ë¥¼ ë¹„êµí• ë•Œ)\n\n#p-valueê°€ ì‘ì„ ìˆ˜ë¡ ì˜í–¥ì„ ë§ì´ ì¤€ë‹¤ betaê°’ì„ ë³´ëŠ” ê²ƒì´ ì•„ë‹Œ p-valueë¥¼ ë³´ëŠ” ê²ƒ ì¤‘ìš”\n\n#ê°€ì¥ ì˜ë¯¸ìˆëŠ” ë³€ìˆ˜?->p-valueê°€ì¥ ì‘ì€ê±° // ëŒ€ë¦½ê°€ì„¤ì±„íƒ Yì— ì˜í–¥ì„ ê°€ì¥\n\n\n\n\n\n\n\n\n# H_0: beta_1:beta_6=0\n\nmodel_reduce<-lm(Y~1,data=data_3.3)\n\nmodel_full<-lm(Y~.,data=data_3.3)\n\nanova(model_reduce,model_full)\n\nAnalysis of Variance Table\n\nModel 1: Y ~ 1\nModel 2: Y ~ X1 + X2 + X3 + X4 + X5 + X6\n  Res.Df  RSS Df Sum of Sq      F   Pr(>F)    \n1     29 4297                                 \n2     23 1149  6      3148 10.502 1.24e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#ëŒ€ë¦½ê°€ì„¤ = ì™„ì „ëª¨í˜•ì´ ì ì ˆí•˜ë‹¤ / 1.24e-05 *** < 0.05 \n\n#ì˜ë¯¸ ìˆëŠ” ì˜ˆì¸¡ ë³€ìˆ˜ê°€ í•œê°œ ì´ìƒ ì¡´ì¬í•œë‹¤ \n\nsummary(model_full) #summaryì—ì„œ beta_1~beta_6ê¹Œì§€ ëª¨ë‘ê°€ 0ì´ë¼ëŠ” ê°€ì„¤ë¡œ ì§„í–‰ì„ ì´ë¯¸í•¨\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.9418  -4.3555   0.3158   5.5425  11.5990 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 10.78708   11.58926   0.931 0.361634    \nX1           0.61319    0.16098   3.809 0.000903 ***\nX2          -0.07305    0.13572  -0.538 0.595594    \nX3           0.32033    0.16852   1.901 0.069925 .  \nX4           0.08173    0.22148   0.369 0.715480    \nX5           0.03838    0.14700   0.261 0.796334    \nX6          -0.21706    0.17821  -1.218 0.235577    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.068 on 23 degrees of freedom\nMultiple R-squared:  0.7326,    Adjusted R-squared:  0.6628 \nF-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05\n\n#F-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05\n\n#ì˜ˆìƒ// ê°€ì¥ì˜ë¯¸ìˆëŠ”ë³€ìˆ˜? -> X1 ì´ìœ ?-> p-value 0.000903 ë¡œ ê°€ì¥ ì‘ê¸°ì— ì˜í–¥ë§ì´ ì¤„ê²ƒìœ¼ë¡œ ì˜ˆì¸¡ \n\n\n\n\n\nmodel_reduce<-lm(Y~X1+X3,data=data_3.3)\n\nmodel_full<-lm(Y~.,data=data_3.3)\n\nanova(model_reduce,model_full) #0.7158 > 0.05\n\nAnalysis of Variance Table\n\nModel 1: Y ~ X1 + X3\nModel 2: Y ~ X1 + X2 + X3 + X4 + X5 + X6\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1     27 1254.7                           \n2     23 1149.0  4    105.65 0.5287 0.7158\n\n#ì˜ê°€ì„¤ì€ H_0: b_2=b_4=b_5=b_6=0 ì´ë¼ëŠ” ì‚¬ì‹¤ì„ ì•Œ ìˆ˜ ìˆë‹¤ \n\n#b_1&b_3ëŠ” ë°˜ì‘ë³€ìˆ˜ì— ìœ ì˜í•œ ë°˜ì‘ì„ ì¤€ë‹¤ë¼ëŠ” ê²ƒë„ ì—°ê³„í•˜ì—¬ ì•Œ ìˆ˜ ìˆë‹¤ \n\n\n\n\n\n#í•´ë‹¹ ì¡°ê±´ì´ ì£¼ì–´ì§€ê³  ë§Œì¡±í•  ë•Œ beta_1=beta_3ì€ ë§ëŠ”ê°€?\n\nmodel_reduce<-lm(Y~I(X1-X3),data=data_3.3) #Ië¥¼ ì”Œìš°ë©´ ìƒˆë¡œìš´ ë³€ìˆ˜ë¥¼ ë§Œë“ ê²ƒê³¼ ë™ì¼\n\n# X1-X3ë¥¼ í•œ ê·¸ìì²´ë¥¼ ë¶„ì„í•˜ë¼ëŠ” ì˜ë¯¸//ë³¸ë˜ëŠ” X1-X3 í•´ì„œ ìƒˆë¡œìš´ ë³€ìˆ˜ë¥¼ ë§Œë“¤ì–´ì„œ í•´ì•¼í•¨ \n\nmodel_full<-lm(Y~X1+X3,data=data_3.3)\n\nanova(model_reduce,model_full) \n\nAnalysis of Variance Table\n\nModel 1: Y ~ I(X1 - X3)\nModel 2: Y ~ X1 + X3\n  Res.Df    RSS Df Sum of Sq     F    Pr(>F)    \n1     28 3846.7                                 \n2     27 1254.6  1      2592 55.78 4.925e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#install.packages(\"car\")\n\nlibrary(car)\n\nWarning: package 'car' was built under R version 4.3.2\n\n\nLoading required package: carData\n\n\nWarning: package 'carData' was built under R version 4.3.2\n\nmodel_full<-lm(Y~X1+X3,data=data_3.3)\n\ncar::linearHypothesis(model_full,c(\"X1=X3\"))\n\nLinear hypothesis test\n\nHypothesis:\nX1 - X3 = 0\n\nModel 1: restricted model\nModel 2: Y ~ X1 + X3\n\n  Res.Df    RSS Df Sum of Sq      F  Pr(>F)  \n1     28 1424.6                              \n2     27 1254.7  1    169.95 3.6572 0.06649 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n# H_0: beta_1+beta_3=1 | beta_2=beta_3:beta_6=0\n\nmodel_full<-lm(Y~X1+X3,data=data_3.3)\n\ncar::linearHypothesis(model_full,c(\"X1 + X3 = 1\"))\n\nLinear hypothesis test\n\nHypothesis:\nX1  + X3 = 1\n\nModel 1: restricted model\nModel 2: Y ~ X1 + X3\n\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1     28 1329.5                           \n2     27 1254.7  1    74.898 1.6118 0.2151\n\n# x1ì˜ íš¨ê³¼ê°€ ì¦ê°€í•˜ë©´ x3ì˜ íš¨ê³¼ëŠ” ê°ì†Œí•œë‹¤ ìƒëŒ€ì ì¸ ê´€ê³„ (ë°˜ëŒ€ë¡œë„ ê°€ëŠ¥)\n\n\n\n\n\nmodel_full<-lm(Y~.,data_3.3)\n\n# ì˜ˆì¸¡ê°’ - ì í•©ê°’\n\nmodel_full$fitted.values\n\n       1        2        3        4        5        6        7        8 \n51.11030 61.35277 69.93944 61.22684 74.45380 53.94185 67.14841 70.09701 \n       9       10       11       12       13       14       15       16 \n79.53099 59.19846 57.92572 55.40103 59.58168 70.21401 76.54933 84.54785 \n      17       18       19       20       21       22       23       24 \n76.15013 61.39736 68.01656 55.62014 42.60324 63.81902 63.66400 44.62475 \n      25       26       27       28       29       30 \n57.31710 67.84347 75.14036 56.04535 77.66053 76.87850 \n\n# ì˜ˆì¸¡í•œê³„(Prediction Limits)\n\npredict(model_full,newdata = data_3.3,interval = \"prediction\")\n\n        fit      lwr       upr\n1  51.11030 34.16999  68.05060\n2  61.35277 46.34536  76.36018\n3  69.93944 53.94267  85.93622\n4  61.22684 45.44586  77.00783\n5  74.45380 59.17630  89.73129\n6  53.94185 37.21813  70.66557\n7  67.14841 51.64493  82.65189\n8  70.09701 54.54384  85.65017\n9  79.53099 62.71383  96.34814\n10 59.19846 44.03506  74.36185\n11 57.92572 42.00674  73.84470\n12 55.40103 39.79333  71.00873\n13 59.58168 43.39853  75.76483\n14 70.21401 52.06636  88.36167\n15 76.54933 60.79444  92.30422\n16 84.54785 66.41374 102.68197\n17 76.15013 59.99991  92.30036\n18 61.39736 43.23384  79.56088\n19 68.01656 52.44673  83.58639\n20 55.62014 39.63744  71.60284\n21 42.60324 26.35046  58.85603\n22 63.81902 48.40145  79.23659\n23 63.66400 48.56222  78.76578\n24 44.62475 27.25435  61.99514\n25 57.31710 41.29380  73.34041\n26 67.84347 49.98605  85.70089\n27 75.14036 59.31975  90.96097\n28 56.04535 40.18723  71.90348\n29 77.66053 61.97564  93.34541\n30 76.87850 60.27441  93.48258\n\n# ì‹ ë¢°í•œê³„(Confidence limits)\n\npredict(model_full,newdata = data_3.3,interval = \"confidence\")\n\n        fit      lwr      upr\n1  51.11030 42.55502 59.66557\n2  61.35277 57.97029 64.73524\n3  69.93944 63.44979 76.42909\n4  61.22684 55.28897 67.16471\n5  74.45380 70.02428 78.88332\n6  53.94185 45.82386 62.05984\n7  67.14841 61.99316 72.30367\n8  70.09701 64.79421 75.39980\n9  79.53099 71.22222 87.83975\n10 59.19846 55.18008 63.21683\n11 57.92572 51.63028 64.22116\n12 55.40103 49.94035 60.86171\n13 59.58168 52.64531 66.51805\n14 70.21401 59.46431 80.96372\n15 76.54933 70.68118 82.41748\n16 84.54785 73.82102 95.27468\n17 76.15013 69.29093 83.00933\n18 61.39736 50.62090 72.17383\n19 68.01656 62.66507 73.36805\n20 55.62014 49.16527 62.07502\n21 42.60324 35.50593 49.70055\n22 63.81902 58.92819 68.70985\n23 63.66400 59.88479 67.44321\n24 44.62475 35.24662 54.00288\n25 57.31710 50.76233 63.87187\n26 67.84347 57.59134 78.09561\n27 75.14036 69.09798 81.18275\n28 56.04535 49.90540 62.18531\n29 77.66053 71.98300 83.33806\n30 76.87850 69.00992 84.74707\n\n\n\n\n\n\n\ndata_3.3<-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\nY<-data_3.3$Y\n\nX<-data_3.3[,-1]\n\nX<-cbind(1,X)\n\nX<-as.matrix(X)\n\n#beta_hat<-solve(t(X) %*% X) %*% t(X) %*% Y # %*%í–‰ë ¬ ê³„ì‚° \n\nP<-solve(t(X) %*% X) %*% t(X)\n\nbeta_hat<- P %*% Y\n\nlm(Y~.,data_3.3)\n\n\nCall:\nlm(formula = Y ~ ., data = data_3.3)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5  \n   10.78708      0.61319     -0.07305      0.32033      0.08173      0.03838  \n         X6  \n   -0.21706  \n\n\n\n\n\n\n\n\n# í‘œì¤€í™” ì”ì°¨\n\ndata_3.3<-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\nres_lm<-lm(Y~.,data=data_3.3)\n\nclass(res_lm)\n\n[1] \"lm\"\n\nmode(res_lm)\n\n[1] \"list\"\n\nnames(res_lm)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\nres_lm$fitted.values\n\n       1        2        3        4        5        6        7        8 \n51.11030 61.35277 69.93944 61.22684 74.45380 53.94185 67.14841 70.09701 \n       9       10       11       12       13       14       15       16 \n79.53099 59.19846 57.92572 55.40103 59.58168 70.21401 76.54933 84.54785 \n      17       18       19       20       21       22       23       24 \n76.15013 61.39736 68.01656 55.62014 42.60324 63.81902 63.66400 44.62475 \n      25       26       27       28       29       30 \n57.31710 67.84347 75.14036 56.04535 77.66053 76.87850 \n\nstr(res_lm)\n\nList of 12\n $ coefficients : Named num [1:7] 10.7871 0.6132 -0.0731 0.3203 0.0817 ...\n  ..- attr(*, \"names\")= chr [1:7] \"(Intercept)\" \"X1\" \"X2\" \"X3\" ...\n $ residuals    : Named num [1:30] -8.11 1.647 1.061 -0.227 6.546 ...\n  ..- attr(*, \"names\")= chr [1:30] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:30] -354.011 54.107 2.742 11.715 -0.971 ...\n  ..- attr(*, \"names\")= chr [1:30] \"(Intercept)\" \"X1\" \"X2\" \"X3\" ...\n $ rank         : int 7\n $ fitted.values: Named num [1:30] 51.1 61.4 69.9 61.2 74.5 ...\n  ..- attr(*, \"names\")= chr [1:30] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:7] 0 1 2 3 4 5 6\n $ qr           :List of 5\n  ..$ qr   : num [1:30, 1:7] -5.477 0.183 0.183 0.183 0.183 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:30] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ : chr [1:7] \"(Intercept)\" \"X1\" \"X2\" \"X3\" ...\n  .. ..- attr(*, \"assign\")= int [1:7] 0 1 2 3 4 5 6\n  ..$ qraux: num [1:7] 1.18 1 1.29 1.1 1.07 ...\n  ..$ pivot: int [1:7] 1 2 3 4 5 6 7\n  ..$ tol  : num 1e-07\n  ..$ rank : int 7\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 23\n $ xlevels      : Named list()\n $ call         : language lm(formula = Y ~ ., data = data_3.3)\n $ terms        :Classes 'terms', 'formula'  language Y ~ X1 + X2 + X3 + X4 + X5 + X6\n  .. ..- attr(*, \"variables\")= language list(Y, X1, X2, X3, X4, X5, X6)\n  .. ..- attr(*, \"factors\")= int [1:7, 1:6] 0 1 0 0 0 0 0 0 0 1 ...\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:7] \"Y\" \"X1\" \"X2\" \"X3\" ...\n  .. .. .. ..$ : chr [1:6] \"X1\" \"X2\" \"X3\" \"X4\" ...\n  .. ..- attr(*, \"term.labels\")= chr [1:6] \"X1\" \"X2\" \"X3\" \"X4\" ...\n  .. ..- attr(*, \"order\")= int [1:6] 1 1 1 1 1 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n  .. ..- attr(*, \"predvars\")= language list(Y, X1, X2, X3, X4, X5, X6)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:7] \"numeric\" \"numeric\" \"numeric\" \"numeric\" ...\n  .. .. ..- attr(*, \"names\")= chr [1:7] \"Y\" \"X1\" \"X2\" \"X3\" ...\n $ model        :'data.frame':  30 obs. of  7 variables:\n  ..$ Y : num [1:30] 43 63 71 61 81 43 58 71 72 67 ...\n  ..$ X1: num [1:30] 51 64 70 63 78 55 67 75 82 61 ...\n  ..$ X2: num [1:30] 30 51 68 45 56 49 42 50 72 45 ...\n  ..$ X3: num [1:30] 39 54 69 47 66 44 56 55 67 47 ...\n  ..$ X4: num [1:30] 61 63 76 54 71 54 66 70 71 62 ...\n  ..$ X5: num [1:30] 92 73 86 84 83 49 68 66 83 80 ...\n  ..$ X6: num [1:30] 45 47 48 35 47 34 35 41 31 41 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language Y ~ X1 + X2 + X3 + X4 + X5 + X6\n  .. .. ..- attr(*, \"variables\")= language list(Y, X1, X2, X3, X4, X5, X6)\n  .. .. ..- attr(*, \"factors\")= int [1:7, 1:6] 0 1 0 0 0 0 0 0 0 1 ...\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:7] \"Y\" \"X1\" \"X2\" \"X3\" ...\n  .. .. .. .. ..$ : chr [1:6] \"X1\" \"X2\" \"X3\" \"X4\" ...\n  .. .. ..- attr(*, \"term.labels\")= chr [1:6] \"X1\" \"X2\" \"X3\" \"X4\" ...\n  .. .. ..- attr(*, \"order\")= int [1:6] 1 1 1 1 1 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n  .. .. ..- attr(*, \"predvars\")= language list(Y, X1, X2, X3, X4, X5, X6)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:7] \"numeric\" \"numeric\" \"numeric\" \"numeric\" ...\n  .. .. .. ..- attr(*, \"names\")= chr [1:7] \"Y\" \"X1\" \"X2\" \"X3\" ...\n - attr(*, \"class\")= chr \"lm\"\n\n# ì”ì°¨ \n\nres_lm$residuals\n\n          1           2           3           4           5           6 \n -8.1102953   1.6472337   1.0605589  -0.2268416   6.5462010 -10.9418499 \n          7           8           9          10          11          12 \n -9.1484140   0.9029929  -7.5309862   7.8015424   6.0742817  11.5989723 \n         13          14          15          16          17          18 \n  9.4183197  -2.2140147   0.4506705  -3.5478519  -2.1501319   3.6026355 \n         19          20          21          22          23          24 \n -3.0165587  -5.6201442   7.3967582   0.1809831 -10.6639999  -4.6247464 \n         25          26          27          28          29          30 \n  5.6828983  -1.8434727   2.8596385  -8.0453540   7.3394730   5.1215016 \n\nresid(res_lm) #ì‹¤ì œê°’ì—ì„œ ì˜ˆì¸¡ëœ ê°’ì„ ëº¸ê°’\n\n          1           2           3           4           5           6 \n -8.1102953   1.6472337   1.0605589  -0.2268416   6.5462010 -10.9418499 \n          7           8           9          10          11          12 \n -9.1484140   0.9029929  -7.5309862   7.8015424   6.0742817  11.5989723 \n         13          14          15          16          17          18 \n  9.4183197  -2.2140147   0.4506705  -3.5478519  -2.1501319   3.6026355 \n         19          20          21          22          23          24 \n -3.0165587  -5.6201442   7.3967582   0.1809831 -10.6639999  -4.6247464 \n         25          26          27          28          29          30 \n  5.6828983  -1.8434727   2.8596385  -8.0453540   7.3394730   5.1215016 \n\n### ë‚´ì  í‘œì¤€í™”ì”ì°¨\n\nrstandard(res_lm)\n\n          1           2           3           4           5           6 \n-1.41498026  0.23955370  0.16744867 -0.03512080  0.97184596 -1.86133876 \n          7           8           9          10          11          12 \n-1.38317210  0.13709194 -1.29490454  1.14799070  0.95218982  1.76906521 \n         13          14          15          16          17          18 \n 1.51371017 -0.46212316  0.06961486 -0.73868563 -0.34446368  0.75418016 \n         19          20          21          22          23          24 \n-0.45861365 -0.88618779  1.19699287  0.02717120 -1.56184734 -0.85286680 \n         25          26          27          28          29          30 \n 0.89948517 -0.36581416  0.44430497 -1.25422677  1.12683185  0.85971512 \n\n### ì™¸ì  í‘œì¤€í™”ì”ì°¨\n\nMASS::studres(res_lm)\n\n          1           2           3           4           5           6 \n-1.44835328  0.23458097  0.16386794 -0.03434974  0.97062209 -1.97526518 \n          7           8           9          10          11          12 \n-1.41280382  0.13413337 -1.31529351  1.15637546  0.95017640  1.86145176 \n         13          14          15          16          17          18 \n 1.56019127 -0.45407837  0.06809185 -0.73117411 -0.33776450  0.74689589 \n         19          20          21          22          23          24 \n-0.45059801 -0.88189556  1.20894332  0.02657438 -1.61559196 -0.84763116 \n         25          26          27          28          29          30 \n 0.89560731 -0.35881868  0.43641573 -1.27088888  1.13380428  0.85466249 \n\nredsid_df<-data.frame(\n\n  Y=data_3.3$Y,\n\n  Y_hat=res_lm$fitted.values,\n\n  resid=resid(res_lm),\n\n  rstandard=rstandard(res_lm),\n\n  studres=MASS::studres(res_lm)\n\n)\n\nredsid_df\n\n    Y    Y_hat       resid   rstandard     studres\n1  43 51.11030  -8.1102953 -1.41498026 -1.44835328\n2  63 61.35277   1.6472337  0.23955370  0.23458097\n3  71 69.93944   1.0605589  0.16744867  0.16386794\n4  61 61.22684  -0.2268416 -0.03512080 -0.03434974\n5  81 74.45380   6.5462010  0.97184596  0.97062209\n6  43 53.94185 -10.9418499 -1.86133876 -1.97526518\n7  58 67.14841  -9.1484140 -1.38317210 -1.41280382\n8  71 70.09701   0.9029929  0.13709194  0.13413337\n9  72 79.53099  -7.5309862 -1.29490454 -1.31529351\n10 67 59.19846   7.8015424  1.14799070  1.15637546\n11 64 57.92572   6.0742817  0.95218982  0.95017640\n12 67 55.40103  11.5989723  1.76906521  1.86145176\n13 69 59.58168   9.4183197  1.51371017  1.56019127\n14 68 70.21401  -2.2140147 -0.46212316 -0.45407837\n15 77 76.54933   0.4506705  0.06961486  0.06809185\n16 81 84.54785  -3.5478519 -0.73868563 -0.73117411\n17 74 76.15013  -2.1501319 -0.34446368 -0.33776450\n18 65 61.39736   3.6026355  0.75418016  0.74689589\n19 65 68.01656  -3.0165587 -0.45861365 -0.45059801\n20 50 55.62014  -5.6201442 -0.88618779 -0.88189556\n21 50 42.60324   7.3967582  1.19699287  1.20894332\n22 64 63.81902   0.1809831  0.02717120  0.02657438\n23 53 63.66400 -10.6639999 -1.56184734 -1.61559196\n24 40 44.62475  -4.6247464 -0.85286680 -0.84763116\n25 63 57.31710   5.6828983  0.89948517  0.89560731\n26 66 67.84347  -1.8434727 -0.36581416 -0.35881868\n27 78 75.14036   2.8596385  0.44430497  0.43641573\n28 48 56.04535  -8.0453540 -1.25422677 -1.27088888\n29 85 77.66053   7.3394730  1.12683185  1.13380428\n30 82 76.87850   5.1215016  0.85971512  0.85466249\n\n\n\n\n\n\n\n\n\na<-rnorm(100,70,10) #ì—°ì†í˜• ë°ì´í„°\n\n# íˆìŠ¤í† ê·¸ë¨ \n\nhist(a)\n\n\n\nhist(a,breaks=5) #ë²”ìœ„ë¥¼ ì¡°ì ˆ ë§‰ëŒ€ì˜ 5ë²ˆ ìë¦„ \n\n\n\n# ì¤„ê¸° ì ê·¸ë¦¼ \n\nstem(a)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  4 | 7\n  5 | 003\n  5 | 567779999\n  6 | 111112222234444444\n  6 | 5555566677888888999\n  7 | 011112222233334\n  7 | 5556666666677788888999\n  8 | 01222334\n  8 | 679\n  9 | 00\n\nstem(round(a)) #ì¤„ê¸°ìì„ ê·¸ë¦´ë•ŒëŠ” ë°˜ì˜¬ë¦¼ì„ í•˜ê³  í•­ìƒ ì§„í–‰ \n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  4 | 7\n  5 | 003\n  5 | 567779999\n  6 | 111112222234444444\n  6 | 5555566677888888999\n  7 | 011112222233334\n  7 | 5556666666677788888999\n  8 | 01222334\n  8 | 679\n  9 | 00\n\nstem(round(a),scale=2) #scaleì„ 2ë°°ë¡œ ëŠ˜ë ¤ë¼ 5ê¸°ì¤€ìœ¼ë¡œ ë°˜ìœ¼ë¡œ ì˜ë¼ì„œ \n\n\n  The decimal point is at the |\n\n  46 | 0\n  48 | \n  50 | 00\n  52 | 0\n  54 | 0\n  56 | 0000\n  58 | 0000\n  60 | 00000\n  62 | 000000\n  64 | 000000000000\n  66 | 00000\n  68 | 000000000\n  70 | 00000\n  72 | 000000000\n  74 | 0000\n  76 | 00000000000\n  78 | 00000000\n  80 | 00\n  82 | 00000\n  84 | 0\n  86 | 00\n  88 | 0\n  90 | 00\n\n# ëª¨ë“ ë°ì´í„°ë¥¼ ë³¼ ìˆ˜ìˆëŠ” ì¥ì  ë°ì´í„°ê°€ ë§ìœ¼ë©´ êµ¬ë¦¼ \n\n# ì í”Œë¡¯\n\nidx<-rep(1,length(a)) #aì˜ ê°¯ìˆ˜ì— ë§ì¶°ì„œ 1ë¥¼ ë°˜ë³µ \n\nplot(idx,a)\n\n\n\nplot(jitter(idx),a,xlim=c(0.5,1.5))\n\n\n\n# ìƒìê·¸ë¦¼\n\nboxplot(a) #ì‚¬ë¶„ìœ„ìˆ˜ì— ëŒ€í•´ì„œ ì•Œ ìˆ˜ ìˆìŒ \n\n# ìƒìê·¸ë¦¼ + ì í”Œë¡¯\n\nboxplot(a)\n\npoints(jitter(idx),a)\n\n\n\n\n\n\n\n\ndata_4.1<-read.table(\"All_Data/p103.txt\",header=T,sep=\"\\t\")\n\ndata_4.1\n\n       Y   X1   X2\n1  12.37 2.23 9.66\n2  12.66 2.57 8.94\n3  12.00 3.87 4.40\n4  11.93 3.10 6.64\n5  11.06 3.39 4.91\n6  13.03 2.83 8.52\n7  13.13 3.02 8.04\n8  11.44 2.14 9.05\n9  12.86 3.04 7.71\n10 10.84 3.26 5.11\n11 11.20 3.39 5.05\n12 11.56 2.35 8.51\n13 10.83 2.76 6.59\n14 12.63 3.90 4.90\n15 12.46 3.16 6.96\n\nclass(data_4.1) #data.frame\n\n[1] \"data.frame\"\n\n# ì‚°ì ë„ í–‰ë ¬\n\nplot(data_4.1)\n\ncor(data_4.1) #ìƒê´€ê³„ìˆ˜\n\n             Y           X1         X2\nY  1.000000000  0.002497966  0.4340688\nX1 0.002497966  1.000000000 -0.8997765\nX2 0.434068758 -0.899776481  1.0000000\n\npairs(data_4.1)\n\n\n\n# Correlation panel\n\npanel.cor<-function(x,y){\n\n  par(usr=c(0,1,0,1))\n\n  r<-round(cor(x,y),digits = 3)\n\n  text(0.5,0.5,r,cex=1.5)\n\n}\n\npairs(data_4.1,lower.panel = panel.cor)\n\n\n\n# íšŒì „ë„í‘œ, ë™ì  ê·¸ë˜í”„(3ì°¨ì›)\n\n#install.packages(\"rgl\")\n\nlibrary(rgl)\n\nWarning: package 'rgl' was built under R version 4.3.2\n\nplot3d(x=data_4.1$X1,y=data_4.1$X2,z=data_4.1$Y) \n\n\n\n\n\ndata_3.3<-read.table(\"All_Data/p060.txt\",header=T,sep=\"\\t\")\n\nres_lm<-lm(Y~X1+X3,data=data_3.3)\n\nlayout(matrix(1:4,nrow=2,byrow=T))\n\nplot(res_lm) #íšŒê·€ì§„ë‹¨ í”Œëì´ ë‚˜ì˜´ \n\n\n\nlayout(1)\n\n# 1. í‘œì¤€í™”ì”ì°¨ì˜ ì •ê·œí™•ë¥ ë¶„í¬\n\nplot(res_lm,2)\n\n\n\n# 2. í‘œì¤€í™”ì”ì°¨ ëŒ€ ê° ì˜ˆì¸¡ë³€ìˆ˜ë“¤ì˜ ì‚°ì ë„\n\nplot(res_lm,3) #ì´ëŸ°ê²½ìš°ì—ëŠ” ë°ì´í„°ë¥¼ ì¶”ê°€í•œë‹¤ ì¢Œì¸¡í•˜ë‹¨ì´ ì—†ì–´ì„œ \n\n\n\n# 3. í‘œì¤€í™”ì”ì°¨ ëŒ€ ì í•©ê°’ì˜ í”Œë¡¯\n\n# 4. í‘œì¤€í™”ì”ì°¨ì˜ ì¸ë±ìŠ¤ í”Œë¡¯"
  },
  {
    "objectID": "R_Basic.html",
    "href": "R_Basic.html",
    "title": "R Basic",
    "section": "",
    "text": "ëª¨ë‘ë¥¼ ìœ„í•œ R ë°ì´í„° ë¶„ì„ ì…ë¬¸\n\n\n\n\n2 + 3  # 2 ë”í•˜ê¸° 3\n## [1] 5\n(3 + 6) * 8\n## [1] 72\n2 ^ 3  # 2ì˜ ì„¸ì œê³±\n## [1] 8\n8 %% 3\n## [1] 2\n\n\n7 + 4\n## [1] 11\n\n\nlog(10) + 5 # ë¡œê·¸í•¨ìˆ˜\n## [1] 7.302585\nsqrt(25) # ì œê³±ê·¼\n## [1] 5\nmax(5, 3, 2) # ê°€ì¥ í° ê°’\n## [1] 5\n\n\na <- 10\nb <- 20\nc <- a+b\nprint(c)\n## [1] 30\n\n\na <- 125\na\n## [1] 125\nprint(a)\n## [1] 125\n\n\na <- 10 # aì— ìˆ«ì ì €ì¥\nb <- 20\na + b # a+bì˜ ê²°ê³¼ ì¶œë ¥\n## [1] 30\na <- \"A\" # aì— ë¬¸ì ì €ì¥\na + b # a+bì˜ ê²°ê³¼ ì¶œë ¥. ì—ëŸ¬ ë°œìƒ\n## Error in a + b: non-numeric argument to binary operator\n\n\nx <- c(1, 2, 3) # ìˆ«ìí˜• ë²¡í„°\ny <- c(\"a\", \"b\", \"c\") # ë¬¸ìí˜• ë²¡í„°\nz <- c(TRUE, TRUE, FALSE, TRUE) # ë…¼ë¦¬í˜• ë²¡í„°\nx ; y ;z\n## [1] 1 2 3\n## [1] \"a\" \"b\" \"c\"\n## [1]  TRUE  TRUE FALSE  TRUE\n\n\nw <- c(1, 2, 3, \"a\", \"b\", \"c\")\nw\n## [1] \"1\" \"2\" \"3\" \"a\" \"b\" \"c\"\n\n\nv1 <- 50:90\nv1\n##  [1] 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74\n## [26] 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90\nv2 <- c(1, 2, 5, 50:90)\nv2\n##  [1]  1  2  5 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n## [26] 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90\n\n\nv3 <- seq(1, 101, 3)\nv3\n##  [1]   1   4   7  10  13  16  19  22  25  28  31  34  37  40  43  46  49  52  55\n## [20]  58  61  64  67  70  73  76  79  82  85  88  91  94  97 100\nv4 <- seq(0.1, 1.0, 0.1)\nv4\n##  [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n\n\nv5 <- rep(1, times = 5) # 1ì„ 5ë²ˆ ë°˜ë³µ\nv5\n## [1] 1 1 1 1 1\nv6 <- rep(1:5, times = 3) # 1ì—ì„œ 5ê¹Œì§€ 3ë²ˆ ë°˜ë³µ\nv6\n##  [1] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5\nv7 <- rep(c(1, 5, 9), times = 3) # 1, 5, 9ë¥¼ 3ë²ˆ ë°˜ë³µ\nv7\n## [1] 1 5 9 1 5 9 1 5 9\nv8 <- rep(1:5, each = 3) # 1ì—ì„œ 5ë¥¼ ê°ê° 3ë²ˆ ë°˜ë³µ\nv8\n##  [1] 1 1 1 2 2 2 3 3 3 4 4 4 5 5 5\n\nrep(1:3, each = 3, times = 3)\n##  [1] 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3\nrep(1:3, times = 3, each = 3)\n##  [1] 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3\n\n\nscore <- c(90, 85, 70) # ì„±ì \nscore\n## [1] 90 85 70\nnames(score) # scoreì— ì €ì¥ëœ ê°’ë“¤ì˜ ì´ë¦„ì„ ë³´ì´ì‹œì˜¤\n## NULL\nnames(score) <- c(\"John\", \"Tom\", \"Jane\") # ê°’ë“¤ì— ì´ë¦„ì„ ë¶€ì—¬\nnames(score) # scoreì— ì €ì¥ëœ ê°’ë“¤ì˜ ì´ë¦„ì„ ë³´ì´ì‹œì˜¤\n## [1] \"John\" \"Tom\"  \"Jane\"\nscore # ì´ë¦„ê³¼ í•¨ê»˜ ê°’ì´ ì¶œë ¥\n## John  Tom Jane \n##   90   85   70\n\n\nd <- c(1, 4, 3, 7, 8)\nd[1]\n## [1] 1\nd[2]\n## [1] 4\nd[3]\n## [1] 3\nd[4]\n## [1] 7\nd[5]\n## [1] 8\nd[6]\n## [1] NA\nd[c(2, 4)]\n## [1] 4 7\n\n\nd <- c(1, 4, 3, 7, 8)\nd[c(1, 3, 5)] # 1, 3, 5ë²ˆì§¸ ê°’ ì¶œë ¥\n## [1] 1 3 8\nd[1:3] # ì²˜ìŒ ì„¸ ê°œì˜ ê°’ ì¶œë ¥\n## [1] 1 4 3\nd[seq(1, 5, 2)] # í™€ìˆ˜ ë²ˆì§¸ ê°’ ì¶œë ¥\n## [1] 1 3 8\nd[-2] # 2ë²ˆì§¸ ê°’ ì œì™¸í•˜ê³  ì¶œë ¥\n## [1] 1 3 7 8\nd[-c(3:5)] # 3~5ë²ˆì§¸ ê°’ì€ ì œì™¸í•˜ê³  ì¶œë ¥\n## [1] 1 4\n\n\nGNP <- c(2000, 2450, 960)\nGNP\n## [1] 2000 2450  960\nnames(GNP) <- c(\"Korea\", \"Japan\", \"Nepal\")\nGNP\n## Korea Japan Nepal \n##  2000  2450   960\nGNP[1]\n## Korea \n##  2000\nGNP[\"Korea\"]\n## Korea \n##  2000\nGNP_NEW <- GNP[c(\"Korea\", \"Nepal\")]\nGNP_NEW\n## Korea Nepal \n##  2000   960\n\n\nv1 <- c(1, 5, 7, 8, 9)\nv1\n## [1] 1 5 7 8 9\nv1[2] <- 3 # v1ì˜ 2ë²ˆì§¸ ê°’ì„ 3ìœ¼ë¡œ ë³€ê²½\nv1\n## [1] 1 3 7 8 9\nv1[c(1, 5)] <- c(10, 20) # v1ì˜ 1, 5ë²ˆì§¸ ê°’ì„ ê°ê° 10, 20ìœ¼ë¡œ ë³€ê²½\nv1\n## [1] 10  3  7  8 20\n\n\nd <- c(1, 4, 3, 7, 8)\n2 * d\n## [1]  2  8  6 14 16\nd - 5\n## [1] -4 -1 -2  2  3\n3 * d + 4\n## [1]  7 16 13 25 28\n\n\nx <- c(1, 2, 3)\ny <- c(4, 5, 6)\nx + y # ëŒ€ì‘í•˜ëŠ” ì›ì†Œë¼ë¦¬ ë”í•˜ì—¬ ì¶œë ¥\n## [1] 5 7 9\nx * y # ëŒ€ì‘í•˜ëŠ” ì›ì†Œë¼ë¦¬ ê³±í•˜ì—¬ ì¶œë ¥\n## [1]  4 10 18\nz <- x + y # x, yë¥¼ ë”í•˜ì—¬ zì— ì €ì¥\nz\n## [1] 5 7 9\n\n\nd <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\nsum(d) # dì— í¬í•¨ëœ ê°’ë“¤ì˜ í•©\n## [1] 55\nsum(2 * d) # dì— í¬í•¨ëœ ê°’ë“¤ì— 2ë¥¼ ê³±í•œ í›„ í•©í•œ ê°’\n## [1] 110\nlength(d) # dì— í¬í•¨ëœ ê°’ë“¤ì˜ ê°œìˆ˜\n## [1] 10\nmean(d[1:5]) # 1~5ë²ˆì§¸ ê°’ë“¤ì˜ í‰ê· \n## [1] 3\nmax(d) # dì— í¬í•¨ëœ ê°’ë“¤ì˜ ìµœëŒ“ê°’\n## [1] 10\nmin(d) # dì— í¬í•¨ëœ ê°’ë“¤ì˜ ìµœì†Ÿê°’\n## [1] 1\nsort(d) # ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬\n##  [1]  1  2  3  4  5  6  7  8  9 10\nsort(d, decreasing = FALSE) # ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬\n##  [1]  1  2  3  4  5  6  7  8  9 10\nsort(d, decreasing = TRUE) # ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬\n##  [1] 10  9  8  7  6  5  4  3  2  1\nsort(d, TRUE) # ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬\n##  [1] 10  9  8  7  6  5  4  3  2  1\n\nv1 <- median(d)\nv1\n## [1] 5.5\nv2 <- sum(d) / length(d)\nv2\n## [1] 5.5\nmean(d)\n## [1] 5.5\n\n\nd <- c(1, 2, 3, 4, 5, 6, 7, 8, 9)\nd >= 5\n## [1] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\nd[d > 5] # 5ë³´ë‹¤ í° ê°’\n## [1] 6 7 8 9\nsum(d > 5) # 5ë³´ë‹¤ í° ê°’ì˜ ê°œìˆ˜ë¥¼ ì¶œë ¥\n## [1] 4\nsum(d[d > 5]) # 5ë³´ë‹¤ í° ê°’ì˜ í•©ê³„ë¥¼ ì¶œë ¥\n## [1] 30\nd == 5\n## [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n\ncondi <- d > 5 & d < 8 # ì¡°ê±´ì„ ë³€ìˆ˜ì— ì €ì¥\nd[condi] # ì¡°ê±´ì— ë§ëŠ” ê°’ë“¤ì„ ì„ íƒ\n## [1] 6 7\nd[d > 5 & d < 8]\n## [1] 6 7\n\n\nds <- c(90, 85, 70, 84)\nmy.info <- list(name = 'Tom', age = 60, status = TRUE, score = ds)\nmy.info # ë¦¬ìŠ¤íŠ¸ì— ì €ì¥ëœ ë‚´ìš©ì„ ëª¨ë‘ ì¶œë ¥\n## $name\n## [1] \"Tom\"\n## \n## $age\n## [1] 60\n## \n## $status\n## [1] TRUE\n## \n## $score\n## [1] 90 85 70 84\nmy.info[1] # ì´ë¦„ì´ë‘ ë‚´ìš© ë‹¤ ì¶œë ¥\n## $name\n## [1] \"Tom\"\nmy.info[[1]] # ë¦¬ìŠ¤íŠ¸ì˜ ì²« ë²ˆì§¸ ê°’ì„ ì¶œë ¥\n## [1] \"Tom\"\nmy.info$name # ë¦¬ìŠ¤íŠ¸ì—ì„œ ê°’ì˜ ì´ë¦„ì´ nameì¸ ê°’ì„ ì¶œë ¥\n## [1] \"Tom\"\nmy.info[[4]] # ë¦¬ìŠ¤íŠ¸ì˜ ë„¤ ë²ˆì§¸ ê°’ì„ ì¶œë ¥\n## [1] 90 85 70 84\n\n\nbt <- c('A', 'B', 'B', 'O', 'AB', 'A') # ë¬¸ìí˜• ë²¡í„° bt ì •ì˜\nbt.new <- factor(bt) # íŒ©í„° bt.new ì •ì˜\nbt # ë²¡í„° btì˜ ë‚´ìš© ì¶œë ¥\n## [1] \"A\"  \"B\"  \"B\"  \"O\"  \"AB\" \"A\"\nbt.new # íŒ©í„° bt.newì˜ ë‚´ìš© ì¶œë ¥\n## [1] A  B  B  O  AB A \n## Levels: A AB B O\nbt[5] # ë²¡í„° btì˜ 5ë²ˆì§¸ ê°’ ì¶œë ¥\n## [1] \"AB\"\nbt.new[5] # íŒ©í„° bt.newì˜ 5ë²ˆì§¸ ê°’ ì¶œë ¥\n## [1] AB\n## Levels: A AB B O\nlevels(bt.new) # íŒ©í„°ì— ì €ì¥ëœ ê°’ì˜ ì¢…ë¥˜ë¥¼ ì¶œë ¥\n## [1] \"A\"  \"AB\" \"B\"  \"O\"\nas.integer(bt.new) # íŒ©í„°ì˜ ë¬¸ìê°’ì„ ìˆ«ìë¡œ ë°”ê¾¸ì–´ ì¶œë ¥\n## [1] 1 3 3 4 2 1\nbt.new[7] <- 'B' # íŒ©í„° bt.newì˜ 7ë²ˆì§¸ì— 'B' ì €ì¥\nbt.new[8] <- 'C' # íŒ©í„° bt.newì˜ 8ë²ˆì§¸ì— 'C' ì €ì¥\n## Warning in `[<-.factor`(`*tmp*`, 8, value = \"C\"): invalid factor level, NA\n## generated\nbt.new # íŒ©í„° bt.newì˜ ë‚´ìš© ì¶œë ¥\n## [1] A    B    B    O    AB   A    B    <NA>\n## Levels: A AB B O\n\n\n\n\n\nz <- matrix(1:20, nrow = 4, ncol = 5)\nz # ë§¤íŠ¸ë¦­ìŠ¤ zì˜ ë‚´ìš©ì„ ì¶œë ¥\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\n\n\nz2 <- matrix(1:20, nrow = 4, ncol = 5, byrow = T)\nz2 # ë§¤íŠ¸ë¦­ìŠ¤ z2ì˜ ë‚´ìš©ì„ ì¶œë ¥\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    2    3    4    5\n## [2,]    6    7    8    9   10\n## [3,]   11   12   13   14   15\n## [4,]   16   17   18   19   20\n\nz <- matrix(1:16, nrow = 4, ncol = 5)\n## Warning in matrix(1:16, nrow = 4, ncol = 5): data length [16] is not a\n## sub-multiple or multiple of the number of columns [5]\nz\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13    1\n## [2,]    2    6   10   14    2\n## [3,]    3    7   11   15    3\n## [4,]    4    8   12   16    4\n\n\nx <- 1:4 # ë²¡í„° x ìƒì„±\ny <- 5:8 # ë²¡í„° y ìƒì„±\nz <- matrix(1:20, nrow = 4, ncol = 5) # ë§¤íŠ¸ë¦­ìŠ¤ z ìƒì„±\n\nm1 <- cbind(x, y) # xì™€ yë¥¼ ì—´ ë°©í–¥ìœ¼ë¡œ ê²°í•©í•˜ì—¬ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±\nm1 # ë§¤íŠ¸ë¦­ìŠ¤ m1ì˜ ë‚´ìš©ì„ ì¶œë ¥\n##      x y\n## [1,] 1 5\n## [2,] 2 6\n## [3,] 3 7\n## [4,] 4 8\nm2 <- rbind(x, y) # xì™€ yë¥¼ í–‰ ë°©í–¥ìœ¼ë¡œ ê²°í•©í•˜ì—¬ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±\nm2 # ë§¤íŠ¸ë¦­ìŠ¤ m2ì˜ ë‚´ìš©ì„ ì¶œë ¥\n##   [,1] [,2] [,3] [,4]\n## x    1    2    3    4\n## y    5    6    7    8\nm3 <- rbind(m2, x) # m2ì™€ ë²¡í„° xë¥¼ í–‰ ë°©í–¥ìœ¼ë¡œ ê²°í•©\nm3 # ë§¤íŠ¸ë¦­ìŠ¤ m3ì˜ ë‚´ìš©ì„ ì¶œë ¥\n##   [,1] [,2] [,3] [,4]\n## x    1    2    3    4\n## y    5    6    7    8\n## x    1    2    3    4\nm4 <- cbind(z, x) # ë§¤íŠ¸ë¦­ìŠ¤ zì™€ ë²¡í„° xë¥¼ ì—´ ë°©í–¥ìœ¼ë¡œ ê²°í•©\nm4 # ë§¤íŠ¸ë¦­ìŠ¤ m4ì˜ ë‚´ìš©ì„ ì¶œë ¥\n##                   x\n## [1,] 1 5  9 13 17 1\n## [2,] 2 6 10 14 18 2\n## [3,] 3 7 11 15 19 3\n## [4,] 4 8 12 16 20 4\n\nx <- 1:5\nm5 <- cbind(z, x)\n## Warning in cbind(z, x): number of rows of result is not a multiple of vector\n## length (arg 2)\nm5\n##                   x\n## [1,] 1 5  9 13 17 1\n## [2,] 2 6 10 14 18 2\n## [3,] 3 7 11 15 19 3\n## [4,] 4 8 12 16 20 4\n\n\nz <- matrix(1:20, nrow = 4, ncol = 5) # ë§¤íŠ¸ë¦­ìŠ¤ z ìƒì„±\nz # ë§¤íŠ¸ë¦­ìŠ¤ zì˜ ë‚´ìš© ì¶œë ¥\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\n\nz[2, 3] # 2í–‰ 3ì—´ì— ìˆëŠ” ê°’\n## [1] 10\nz[1, 4] # 1í–‰ 4ì—´ì— ìˆëŠ” ê°’\n## [1] 13\nz[2, ] # 2í–‰ì— ìˆëŠ” ëª¨ë“  ê°’\n## [1]  2  6 10 14 18\nz[, 4] # 4ì—´ì— ìˆëŠ” ëª¨ë“  ê°’\n## [1] 13 14 15 16\nz[, ]\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\nz\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\n\n\nz <- matrix(1:20, nrow = 4, ncol = 5) # ë§¤íŠ¸ë¦­ìŠ¤ z ìƒì„±\nz # ë§¤íŠ¸ë¦­ìŠ¤ zì˜ ë‚´ìš© ì¶œë ¥\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\n\nz[2, 1:3] # 2í–‰ì˜ ê°’ ì¤‘ 1~3ì—´ì— ìˆëŠ” ê°’\n## [1]  2  6 10\nz[1, c(1, 2, 4)] # 1í–‰ì˜ ê°’ ì¤‘ 1, 2, 4ì—´ì— ìˆëŠ” ê°’\n## [1]  1  5 13\nz[1:2, ] # 1, 2í–‰ì— ìˆëŠ” ëª¨ë“  ê°’\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\nz[, c(1, 4)] # 1, 4ì—´ì— ìˆëŠ” ëª¨ë“  ê°’\n##      [,1] [,2]\n## [1,]    1   13\n## [2,]    2   14\n## [3,]    3   15\n## [4,]    4   16\n\n\nscore <- matrix(c(90, 85, 69, 78,\n                  85, 96, 49, 95,\n                  90, 80, 70, 60),\n                nrow = 4,\n                ncol = 3)\nscore\n##      [,1] [,2] [,3]\n## [1,]   90   85   90\n## [2,]   85   96   80\n## [3,]   69   49   70\n## [4,]   78   95   60\nrownames(score) <- c('John', 'Tom', 'Mark', 'Jane')\ncolnames(score) <- c('English', 'Math', 'Science')\nscore\n##      English Math Science\n## John      90   85      90\n## Tom       85   96      80\n## Mark      69   49      70\n## Jane      78   95      60\n\n\nscore['John', 'Math'] # Johnì˜ ìˆ˜í•™ ì„±ì \n## [1] 85\nscore['Tom', c('Math', 'Science')] # Tomì˜ ìˆ˜í•™, ê³¼í•™ ì„±ì \n##    Math Science \n##      96      80\nscore['Mark', ] # Markì˜ ëª¨ë“  ê³¼ëª© ì„±ì \n## English    Math Science \n##      69      49      70\nscore[, 'English'] # ëª¨ë“  í•™ìƒì˜ ì˜ì–´ ì„±ì \n## John  Tom Mark Jane \n##   90   85   69   78\nrownames(score) # scoreì˜ í–‰ì˜ ì´ë¦„\n## [1] \"John\" \"Tom\"  \"Mark\" \"Jane\"\ncolnames(score) # scoreì˜ ì—´ì˜ ì´ë¦„\n## [1] \"English\" \"Math\"    \"Science\"\ncolnames(score)[2] # scoreì˜ ì—´ì˜ ì´ë¦„ ì¤‘ ë‘ ë²ˆì§¸ ê°’\n## [1] \"Math\"\n\n\ncity <- c(\"Seoul\", \"Tokyo\", \"Washington\") # ë¬¸ìë¡œ ì´ë£¨ì–´ì§„ ë²¡í„°\nrank <- c(1, 3, 2) # ìˆ«ìë¡œ ì´ë£¨ì–´ì§„ ë²¡í„°\ncity.info <- data.frame(city, rank) # ë°ì´í„°í”„ë ˆì„ ìƒì„±\ncity.info # city.infoì˜ ë‚´ìš© ì¶œë ¥\n##         city rank\n## 1      Seoul    1\n## 2      Tokyo    3\n## 3 Washington    2\n\n\n# iris\niris[, c(1:2)] # 1, 2ì—´ì˜ ëª¨ë“  ë°ì´í„°\n##     Sepal.Length Sepal.Width\n## 1            5.1         3.5\n## 2            4.9         3.0\n## 3            4.7         3.2\n## 4            4.6         3.1\n## 5            5.0         3.6\n## 6            5.4         3.9\n## 7            4.6         3.4\n## 8            5.0         3.4\n## 9            4.4         2.9\n## 10           4.9         3.1\n## 11           5.4         3.7\n## 12           4.8         3.4\n## 13           4.8         3.0\n## 14           4.3         3.0\n## 15           5.8         4.0\n## 16           5.7         4.4\n## 17           5.4         3.9\n## 18           5.1         3.5\n## 19           5.7         3.8\n## 20           5.1         3.8\n## 21           5.4         3.4\n## 22           5.1         3.7\n## 23           4.6         3.6\n## 24           5.1         3.3\n## 25           4.8         3.4\n## 26           5.0         3.0\n## 27           5.0         3.4\n## 28           5.2         3.5\n## 29           5.2         3.4\n## 30           4.7         3.2\n## 31           4.8         3.1\n## 32           5.4         3.4\n## 33           5.2         4.1\n## 34           5.5         4.2\n## 35           4.9         3.1\n## 36           5.0         3.2\n## 37           5.5         3.5\n## 38           4.9         3.6\n## 39           4.4         3.0\n## 40           5.1         3.4\n## 41           5.0         3.5\n## 42           4.5         2.3\n## 43           4.4         3.2\n## 44           5.0         3.5\n## 45           5.1         3.8\n## 46           4.8         3.0\n## 47           5.1         3.8\n## 48           4.6         3.2\n## 49           5.3         3.7\n## 50           5.0         3.3\n## 51           7.0         3.2\n## 52           6.4         3.2\n## 53           6.9         3.1\n## 54           5.5         2.3\n## 55           6.5         2.8\n## 56           5.7         2.8\n## 57           6.3         3.3\n## 58           4.9         2.4\n## 59           6.6         2.9\n## 60           5.2         2.7\n## 61           5.0         2.0\n## 62           5.9         3.0\n## 63           6.0         2.2\n## 64           6.1         2.9\n## 65           5.6         2.9\n## 66           6.7         3.1\n## 67           5.6         3.0\n## 68           5.8         2.7\n## 69           6.2         2.2\n## 70           5.6         2.5\n## 71           5.9         3.2\n## 72           6.1         2.8\n## 73           6.3         2.5\n## 74           6.1         2.8\n## 75           6.4         2.9\n## 76           6.6         3.0\n## 77           6.8         2.8\n## 78           6.7         3.0\n## 79           6.0         2.9\n## 80           5.7         2.6\n## 81           5.5         2.4\n## 82           5.5         2.4\n## 83           5.8         2.7\n## 84           6.0         2.7\n## 85           5.4         3.0\n## 86           6.0         3.4\n## 87           6.7         3.1\n## 88           6.3         2.3\n## 89           5.6         3.0\n## 90           5.5         2.5\n## 91           5.5         2.6\n## 92           6.1         3.0\n## 93           5.8         2.6\n## 94           5.0         2.3\n## 95           5.6         2.7\n## 96           5.7         3.0\n## 97           5.7         2.9\n## 98           6.2         2.9\n## 99           5.1         2.5\n## 100          5.7         2.8\n## 101          6.3         3.3\n## 102          5.8         2.7\n## 103          7.1         3.0\n## 104          6.3         2.9\n## 105          6.5         3.0\n## 106          7.6         3.0\n## 107          4.9         2.5\n## 108          7.3         2.9\n## 109          6.7         2.5\n## 110          7.2         3.6\n## 111          6.5         3.2\n## 112          6.4         2.7\n## 113          6.8         3.0\n## 114          5.7         2.5\n## 115          5.8         2.8\n## 116          6.4         3.2\n## 117          6.5         3.0\n## 118          7.7         3.8\n## 119          7.7         2.6\n## 120          6.0         2.2\n## 121          6.9         3.2\n## 122          5.6         2.8\n## 123          7.7         2.8\n## 124          6.3         2.7\n## 125          6.7         3.3\n## 126          7.2         3.2\n## 127          6.2         2.8\n## 128          6.1         3.0\n## 129          6.4         2.8\n## 130          7.2         3.0\n## 131          7.4         2.8\n## 132          7.9         3.8\n## 133          6.4         2.8\n## 134          6.3         2.8\n## 135          6.1         2.6\n## 136          7.7         3.0\n## 137          6.3         3.4\n## 138          6.4         3.1\n## 139          6.0         3.0\n## 140          6.9         3.1\n## 141          6.7         3.1\n## 142          6.9         3.1\n## 143          5.8         2.7\n## 144          6.8         3.2\n## 145          6.7         3.3\n## 146          6.7         3.0\n## 147          6.3         2.5\n## 148          6.5         3.0\n## 149          6.2         3.4\n## 150          5.9         3.0\niris[, c(1, 3, 5)] # 1, 3, 5ì—´ì˜ ëª¨ë“  ë°ì´í„°\n##     Sepal.Length Petal.Length    Species\n## 1            5.1          1.4     setosa\n## 2            4.9          1.4     setosa\n## 3            4.7          1.3     setosa\n## 4            4.6          1.5     setosa\n## 5            5.0          1.4     setosa\n## 6            5.4          1.7     setosa\n## 7            4.6          1.4     setosa\n## 8            5.0          1.5     setosa\n## 9            4.4          1.4     setosa\n## 10           4.9          1.5     setosa\n## 11           5.4          1.5     setosa\n## 12           4.8          1.6     setosa\n## 13           4.8          1.4     setosa\n## 14           4.3          1.1     setosa\n## 15           5.8          1.2     setosa\n## 16           5.7          1.5     setosa\n## 17           5.4          1.3     setosa\n## 18           5.1          1.4     setosa\n## 19           5.7          1.7     setosa\n## 20           5.1          1.5     setosa\n## 21           5.4          1.7     setosa\n## 22           5.1          1.5     setosa\n## 23           4.6          1.0     setosa\n## 24           5.1          1.7     setosa\n## 25           4.8          1.9     setosa\n## 26           5.0          1.6     setosa\n## 27           5.0          1.6     setosa\n## 28           5.2          1.5     setosa\n## 29           5.2          1.4     setosa\n## 30           4.7          1.6     setosa\n## 31           4.8          1.6     setosa\n## 32           5.4          1.5     setosa\n## 33           5.2          1.5     setosa\n## 34           5.5          1.4     setosa\n## 35           4.9          1.5     setosa\n## 36           5.0          1.2     setosa\n## 37           5.5          1.3     setosa\n## 38           4.9          1.4     setosa\n## 39           4.4          1.3     setosa\n## 40           5.1          1.5     setosa\n## 41           5.0          1.3     setosa\n## 42           4.5          1.3     setosa\n## 43           4.4          1.3     setosa\n## 44           5.0          1.6     setosa\n## 45           5.1          1.9     setosa\n## 46           4.8          1.4     setosa\n## 47           5.1          1.6     setosa\n## 48           4.6          1.4     setosa\n## 49           5.3          1.5     setosa\n## 50           5.0          1.4     setosa\n## 51           7.0          4.7 versicolor\n## 52           6.4          4.5 versicolor\n## 53           6.9          4.9 versicolor\n## 54           5.5          4.0 versicolor\n## 55           6.5          4.6 versicolor\n## 56           5.7          4.5 versicolor\n## 57           6.3          4.7 versicolor\n## 58           4.9          3.3 versicolor\n## 59           6.6          4.6 versicolor\n## 60           5.2          3.9 versicolor\n## 61           5.0          3.5 versicolor\n## 62           5.9          4.2 versicolor\n## 63           6.0          4.0 versicolor\n## 64           6.1          4.7 versicolor\n## 65           5.6          3.6 versicolor\n## 66           6.7          4.4 versicolor\n## 67           5.6          4.5 versicolor\n## 68           5.8          4.1 versicolor\n## 69           6.2          4.5 versicolor\n## 70           5.6          3.9 versicolor\n## 71           5.9          4.8 versicolor\n## 72           6.1          4.0 versicolor\n## 73           6.3          4.9 versicolor\n## 74           6.1          4.7 versicolor\n## 75           6.4          4.3 versicolor\n## 76           6.6          4.4 versicolor\n## 77           6.8          4.8 versicolor\n## 78           6.7          5.0 versicolor\n## 79           6.0          4.5 versicolor\n## 80           5.7          3.5 versicolor\n## 81           5.5          3.8 versicolor\n## 82           5.5          3.7 versicolor\n## 83           5.8          3.9 versicolor\n## 84           6.0          5.1 versicolor\n## 85           5.4          4.5 versicolor\n## 86           6.0          4.5 versicolor\n## 87           6.7          4.7 versicolor\n## 88           6.3          4.4 versicolor\n## 89           5.6          4.1 versicolor\n## 90           5.5          4.0 versicolor\n## 91           5.5          4.4 versicolor\n## 92           6.1          4.6 versicolor\n## 93           5.8          4.0 versicolor\n## 94           5.0          3.3 versicolor\n## 95           5.6          4.2 versicolor\n## 96           5.7          4.2 versicolor\n## 97           5.7          4.2 versicolor\n## 98           6.2          4.3 versicolor\n## 99           5.1          3.0 versicolor\n## 100          5.7          4.1 versicolor\n## 101          6.3          6.0  virginica\n## 102          5.8          5.1  virginica\n## 103          7.1          5.9  virginica\n## 104          6.3          5.6  virginica\n## 105          6.5          5.8  virginica\n## 106          7.6          6.6  virginica\n## 107          4.9          4.5  virginica\n## 108          7.3          6.3  virginica\n## 109          6.7          5.8  virginica\n## 110          7.2          6.1  virginica\n## 111          6.5          5.1  virginica\n## 112          6.4          5.3  virginica\n## 113          6.8          5.5  virginica\n## 114          5.7          5.0  virginica\n## 115          5.8          5.1  virginica\n## 116          6.4          5.3  virginica\n## 117          6.5          5.5  virginica\n## 118          7.7          6.7  virginica\n## 119          7.7          6.9  virginica\n## 120          6.0          5.0  virginica\n## 121          6.9          5.7  virginica\n## 122          5.6          4.9  virginica\n## 123          7.7          6.7  virginica\n## 124          6.3          4.9  virginica\n## 125          6.7          5.7  virginica\n## 126          7.2          6.0  virginica\n## 127          6.2          4.8  virginica\n## 128          6.1          4.9  virginica\n## 129          6.4          5.6  virginica\n## 130          7.2          5.8  virginica\n## 131          7.4          6.1  virginica\n## 132          7.9          6.4  virginica\n## 133          6.4          5.6  virginica\n## 134          6.3          5.1  virginica\n## 135          6.1          5.6  virginica\n## 136          7.7          6.1  virginica\n## 137          6.3          5.6  virginica\n## 138          6.4          5.5  virginica\n## 139          6.0          4.8  virginica\n## 140          6.9          5.4  virginica\n## 141          6.7          5.6  virginica\n## 142          6.9          5.1  virginica\n## 143          5.8          5.1  virginica\n## 144          6.8          5.9  virginica\n## 145          6.7          5.7  virginica\n## 146          6.7          5.2  virginica\n## 147          6.3          5.0  virginica\n## 148          6.5          5.2  virginica\n## 149          6.2          5.4  virginica\n## 150          5.9          5.1  virginica\niris[, c(\"Sepal.Length\", \"Species\")] # 1, 5ì—´ì˜ ëª¨ë“  ë°ì´í„°\n##     Sepal.Length    Species\n## 1            5.1     setosa\n## 2            4.9     setosa\n## 3            4.7     setosa\n## 4            4.6     setosa\n## 5            5.0     setosa\n## 6            5.4     setosa\n## 7            4.6     setosa\n## 8            5.0     setosa\n## 9            4.4     setosa\n## 10           4.9     setosa\n## 11           5.4     setosa\n## 12           4.8     setosa\n## 13           4.8     setosa\n## 14           4.3     setosa\n## 15           5.8     setosa\n## 16           5.7     setosa\n## 17           5.4     setosa\n## 18           5.1     setosa\n## 19           5.7     setosa\n## 20           5.1     setosa\n## 21           5.4     setosa\n## 22           5.1     setosa\n## 23           4.6     setosa\n## 24           5.1     setosa\n## 25           4.8     setosa\n## 26           5.0     setosa\n## 27           5.0     setosa\n## 28           5.2     setosa\n## 29           5.2     setosa\n## 30           4.7     setosa\n## 31           4.8     setosa\n## 32           5.4     setosa\n## 33           5.2     setosa\n## 34           5.5     setosa\n## 35           4.9     setosa\n## 36           5.0     setosa\n## 37           5.5     setosa\n## 38           4.9     setosa\n## 39           4.4     setosa\n## 40           5.1     setosa\n## 41           5.0     setosa\n## 42           4.5     setosa\n## 43           4.4     setosa\n## 44           5.0     setosa\n## 45           5.1     setosa\n## 46           4.8     setosa\n## 47           5.1     setosa\n## 48           4.6     setosa\n## 49           5.3     setosa\n## 50           5.0     setosa\n## 51           7.0 versicolor\n## 52           6.4 versicolor\n## 53           6.9 versicolor\n## 54           5.5 versicolor\n## 55           6.5 versicolor\n## 56           5.7 versicolor\n## 57           6.3 versicolor\n## 58           4.9 versicolor\n## 59           6.6 versicolor\n## 60           5.2 versicolor\n## 61           5.0 versicolor\n## 62           5.9 versicolor\n## 63           6.0 versicolor\n## 64           6.1 versicolor\n## 65           5.6 versicolor\n## 66           6.7 versicolor\n## 67           5.6 versicolor\n## 68           5.8 versicolor\n## 69           6.2 versicolor\n## 70           5.6 versicolor\n## 71           5.9 versicolor\n## 72           6.1 versicolor\n## 73           6.3 versicolor\n## 74           6.1 versicolor\n## 75           6.4 versicolor\n## 76           6.6 versicolor\n## 77           6.8 versicolor\n## 78           6.7 versicolor\n## 79           6.0 versicolor\n## 80           5.7 versicolor\n## 81           5.5 versicolor\n## 82           5.5 versicolor\n## 83           5.8 versicolor\n## 84           6.0 versicolor\n## 85           5.4 versicolor\n## 86           6.0 versicolor\n## 87           6.7 versicolor\n## 88           6.3 versicolor\n## 89           5.6 versicolor\n## 90           5.5 versicolor\n## 91           5.5 versicolor\n## 92           6.1 versicolor\n## 93           5.8 versicolor\n## 94           5.0 versicolor\n## 95           5.6 versicolor\n## 96           5.7 versicolor\n## 97           5.7 versicolor\n## 98           6.2 versicolor\n## 99           5.1 versicolor\n## 100          5.7 versicolor\n## 101          6.3  virginica\n## 102          5.8  virginica\n## 103          7.1  virginica\n## 104          6.3  virginica\n## 105          6.5  virginica\n## 106          7.6  virginica\n## 107          4.9  virginica\n## 108          7.3  virginica\n## 109          6.7  virginica\n## 110          7.2  virginica\n## 111          6.5  virginica\n## 112          6.4  virginica\n## 113          6.8  virginica\n## 114          5.7  virginica\n## 115          5.8  virginica\n## 116          6.4  virginica\n## 117          6.5  virginica\n## 118          7.7  virginica\n## 119          7.7  virginica\n## 120          6.0  virginica\n## 121          6.9  virginica\n## 122          5.6  virginica\n## 123          7.7  virginica\n## 124          6.3  virginica\n## 125          6.7  virginica\n## 126          7.2  virginica\n## 127          6.2  virginica\n## 128          6.1  virginica\n## 129          6.4  virginica\n## 130          7.2  virginica\n## 131          7.4  virginica\n## 132          7.9  virginica\n## 133          6.4  virginica\n## 134          6.3  virginica\n## 135          6.1  virginica\n## 136          7.7  virginica\n## 137          6.3  virginica\n## 138          6.4  virginica\n## 139          6.0  virginica\n## 140          6.9  virginica\n## 141          6.7  virginica\n## 142          6.9  virginica\n## 143          5.8  virginica\n## 144          6.8  virginica\n## 145          6.7  virginica\n## 146          6.7  virginica\n## 147          6.3  virginica\n## 148          6.5  virginica\n## 149          6.2  virginica\n## 150          5.9  virginica\niris[1:5, ] # 1~5í–‰ì˜ ëª¨ë“  ë°ì´í„°\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\niris[1:5, c(1, 3)] # 1~5í–‰ì˜ ë°ì´í„° ì¤‘ 1, 3ì—´ì˜ ë°ì´í„°\n##   Sepal.Length Petal.Length\n## 1          5.1          1.4\n## 2          4.9          1.4\n## 3          4.7          1.3\n## 4          4.6          1.5\n## 5          5.0          1.4\n\n\ndim(iris) # í–‰ê³¼ ì—´ì˜ ê°œìˆ˜ ì¶œë ¥\n## [1] 150   5\nnrow(iris) # í–‰ì˜ ê°œìˆ˜ ì¶œë ¥\n## [1] 150\nncol(iris) # ì—´ì˜ ê°œìˆ˜ ì¶œë ¥\n## [1] 5\ncolnames(iris) # ì—´ ì´ë¦„ ì¶œë ¥, names()ì™€ ê²°ê³¼ ë™ì¼\n## [1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"\nhead(iris) # ë°ì´í„°ì…‹ì˜ ì•ë¶€ë¶„ ì¼ë¶€ ì¶œë ¥\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\ntail(iris) # ë°ì´í„°ì…‹ì˜ ë’·ë¶€ë¶„ ì¼ë¶€ ì¶œë ¥\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n## 145          6.7         3.3          5.7         2.5 virginica\n## 146          6.7         3.0          5.2         2.3 virginica\n## 147          6.3         2.5          5.0         1.9 virginica\n## 148          6.5         3.0          5.2         2.0 virginica\n## 149          6.2         3.4          5.4         2.3 virginica\n## 150          5.9         3.0          5.1         1.8 virginica\n\nhead(iris, 10)\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1           5.1         3.5          1.4         0.2  setosa\n## 2           4.9         3.0          1.4         0.2  setosa\n## 3           4.7         3.2          1.3         0.2  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\n## 5           5.0         3.6          1.4         0.2  setosa\n## 6           5.4         3.9          1.7         0.4  setosa\n## 7           4.6         3.4          1.4         0.3  setosa\n## 8           5.0         3.4          1.5         0.2  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\ntail(iris, 20)\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n## 131          7.4         2.8          6.1         1.9 virginica\n## 132          7.9         3.8          6.4         2.0 virginica\n## 133          6.4         2.8          5.6         2.2 virginica\n## 134          6.3         2.8          5.1         1.5 virginica\n## 135          6.1         2.6          5.6         1.4 virginica\n## 136          7.7         3.0          6.1         2.3 virginica\n## 137          6.3         3.4          5.6         2.4 virginica\n## 138          6.4         3.1          5.5         1.8 virginica\n## 139          6.0         3.0          4.8         1.8 virginica\n## 140          6.9         3.1          5.4         2.1 virginica\n## 141          6.7         3.1          5.6         2.4 virginica\n## 142          6.9         3.1          5.1         2.3 virginica\n## 143          5.8         2.7          5.1         1.9 virginica\n## 144          6.8         3.2          5.9         2.3 virginica\n## 145          6.7         3.3          5.7         2.5 virginica\n## 146          6.7         3.0          5.2         2.3 virginica\n## 147          6.3         2.5          5.0         1.9 virginica\n## 148          6.5         3.0          5.2         2.0 virginica\n## 149          6.2         3.4          5.4         2.3 virginica\n## 150          5.9         3.0          5.1         1.8 virginica\n\n\nstr(iris) # ë°ì´í„°ì…‹ ìš”ì•½ ì •ë³´ ë³´ê¸°\n## 'data.frame':    150 obs. of  5 variables:\n##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n##  $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\niris[, 5] # í’ˆì¢… ë°ì´í„° ë³´ê¸°\n##   [1] setosa     setosa     setosa     setosa     setosa     setosa    \n##   [7] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [13] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [19] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [25] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [31] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [37] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [43] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [49] setosa     setosa     versicolor versicolor versicolor versicolor\n##  [55] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [61] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [67] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [73] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [79] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [85] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [91] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [97] versicolor versicolor versicolor versicolor virginica  virginica \n## [103] virginica  virginica  virginica  virginica  virginica  virginica \n## [109] virginica  virginica  virginica  virginica  virginica  virginica \n## [115] virginica  virginica  virginica  virginica  virginica  virginica \n## [121] virginica  virginica  virginica  virginica  virginica  virginica \n## [127] virginica  virginica  virginica  virginica  virginica  virginica \n## [133] virginica  virginica  virginica  virginica  virginica  virginica \n## [139] virginica  virginica  virginica  virginica  virginica  virginica \n## [145] virginica  virginica  virginica  virginica  virginica  virginica \n## Levels: setosa versicolor virginica\nunique(iris[, 5]) # í’ˆì¢…ì˜ ì¢…ë¥˜ ë³´ê¸°(ì¤‘ë³µ ì œê±°)\n## [1] setosa     versicolor virginica \n## Levels: setosa versicolor virginica\ntable(iris[, \"Species\"]) # í’ˆì¢…ì˜ ì¢…ë¥˜ë³„ í–‰ì˜ ê°œìˆ˜ ì„¸ê¸°\n## \n##     setosa versicolor  virginica \n##         50         50         50\n\n\niris[, -5]\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width\n## 1            5.1         3.5          1.4         0.2\n## 2            4.9         3.0          1.4         0.2\n## 3            4.7         3.2          1.3         0.2\n## 4            4.6         3.1          1.5         0.2\n## 5            5.0         3.6          1.4         0.2\n## 6            5.4         3.9          1.7         0.4\n## 7            4.6         3.4          1.4         0.3\n## 8            5.0         3.4          1.5         0.2\n## 9            4.4         2.9          1.4         0.2\n## 10           4.9         3.1          1.5         0.1\n## 11           5.4         3.7          1.5         0.2\n## 12           4.8         3.4          1.6         0.2\n## 13           4.8         3.0          1.4         0.1\n## 14           4.3         3.0          1.1         0.1\n## 15           5.8         4.0          1.2         0.2\n## 16           5.7         4.4          1.5         0.4\n## 17           5.4         3.9          1.3         0.4\n## 18           5.1         3.5          1.4         0.3\n## 19           5.7         3.8          1.7         0.3\n## 20           5.1         3.8          1.5         0.3\n## 21           5.4         3.4          1.7         0.2\n## 22           5.1         3.7          1.5         0.4\n## 23           4.6         3.6          1.0         0.2\n## 24           5.1         3.3          1.7         0.5\n## 25           4.8         3.4          1.9         0.2\n## 26           5.0         3.0          1.6         0.2\n## 27           5.0         3.4          1.6         0.4\n## 28           5.2         3.5          1.5         0.2\n## 29           5.2         3.4          1.4         0.2\n## 30           4.7         3.2          1.6         0.2\n## 31           4.8         3.1          1.6         0.2\n## 32           5.4         3.4          1.5         0.4\n## 33           5.2         4.1          1.5         0.1\n## 34           5.5         4.2          1.4         0.2\n## 35           4.9         3.1          1.5         0.2\n## 36           5.0         3.2          1.2         0.2\n## 37           5.5         3.5          1.3         0.2\n## 38           4.9         3.6          1.4         0.1\n## 39           4.4         3.0          1.3         0.2\n## 40           5.1         3.4          1.5         0.2\n## 41           5.0         3.5          1.3         0.3\n## 42           4.5         2.3          1.3         0.3\n## 43           4.4         3.2          1.3         0.2\n## 44           5.0         3.5          1.6         0.6\n## 45           5.1         3.8          1.9         0.4\n## 46           4.8         3.0          1.4         0.3\n## 47           5.1         3.8          1.6         0.2\n## 48           4.6         3.2          1.4         0.2\n## 49           5.3         3.7          1.5         0.2\n## 50           5.0         3.3          1.4         0.2\n## 51           7.0         3.2          4.7         1.4\n## 52           6.4         3.2          4.5         1.5\n## 53           6.9         3.1          4.9         1.5\n## 54           5.5         2.3          4.0         1.3\n## 55           6.5         2.8          4.6         1.5\n## 56           5.7         2.8          4.5         1.3\n## 57           6.3         3.3          4.7         1.6\n## 58           4.9         2.4          3.3         1.0\n## 59           6.6         2.9          4.6         1.3\n## 60           5.2         2.7          3.9         1.4\n## 61           5.0         2.0          3.5         1.0\n## 62           5.9         3.0          4.2         1.5\n## 63           6.0         2.2          4.0         1.0\n## 64           6.1         2.9          4.7         1.4\n## 65           5.6         2.9          3.6         1.3\n## 66           6.7         3.1          4.4         1.4\n## 67           5.6         3.0          4.5         1.5\n## 68           5.8         2.7          4.1         1.0\n## 69           6.2         2.2          4.5         1.5\n## 70           5.6         2.5          3.9         1.1\n## 71           5.9         3.2          4.8         1.8\n## 72           6.1         2.8          4.0         1.3\n## 73           6.3         2.5          4.9         1.5\n## 74           6.1         2.8          4.7         1.2\n## 75           6.4         2.9          4.3         1.3\n## 76           6.6         3.0          4.4         1.4\n## 77           6.8         2.8          4.8         1.4\n## 78           6.7         3.0          5.0         1.7\n## 79           6.0         2.9          4.5         1.5\n## 80           5.7         2.6          3.5         1.0\n## 81           5.5         2.4          3.8         1.1\n## 82           5.5         2.4          3.7         1.0\n## 83           5.8         2.7          3.9         1.2\n## 84           6.0         2.7          5.1         1.6\n## 85           5.4         3.0          4.5         1.5\n## 86           6.0         3.4          4.5         1.6\n## 87           6.7         3.1          4.7         1.5\n## 88           6.3         2.3          4.4         1.3\n## 89           5.6         3.0          4.1         1.3\n## 90           5.5         2.5          4.0         1.3\n## 91           5.5         2.6          4.4         1.2\n## 92           6.1         3.0          4.6         1.4\n## 93           5.8         2.6          4.0         1.2\n## 94           5.0         2.3          3.3         1.0\n## 95           5.6         2.7          4.2         1.3\n## 96           5.7         3.0          4.2         1.2\n## 97           5.7         2.9          4.2         1.3\n## 98           6.2         2.9          4.3         1.3\n## 99           5.1         2.5          3.0         1.1\n## 100          5.7         2.8          4.1         1.3\n## 101          6.3         3.3          6.0         2.5\n## 102          5.8         2.7          5.1         1.9\n## 103          7.1         3.0          5.9         2.1\n## 104          6.3         2.9          5.6         1.8\n## 105          6.5         3.0          5.8         2.2\n## 106          7.6         3.0          6.6         2.1\n## 107          4.9         2.5          4.5         1.7\n## 108          7.3         2.9          6.3         1.8\n## 109          6.7         2.5          5.8         1.8\n## 110          7.2         3.6          6.1         2.5\n## 111          6.5         3.2          5.1         2.0\n## 112          6.4         2.7          5.3         1.9\n## 113          6.8         3.0          5.5         2.1\n## 114          5.7         2.5          5.0         2.0\n## 115          5.8         2.8          5.1         2.4\n## 116          6.4         3.2          5.3         2.3\n## 117          6.5         3.0          5.5         1.8\n## 118          7.7         3.8          6.7         2.2\n## 119          7.7         2.6          6.9         2.3\n## 120          6.0         2.2          5.0         1.5\n## 121          6.9         3.2          5.7         2.3\n## 122          5.6         2.8          4.9         2.0\n## 123          7.7         2.8          6.7         2.0\n## 124          6.3         2.7          4.9         1.8\n## 125          6.7         3.3          5.7         2.1\n## 126          7.2         3.2          6.0         1.8\n## 127          6.2         2.8          4.8         1.8\n## 128          6.1         3.0          4.9         1.8\n## 129          6.4         2.8          5.6         2.1\n## 130          7.2         3.0          5.8         1.6\n## 131          7.4         2.8          6.1         1.9\n## 132          7.9         3.8          6.4         2.0\n## 133          6.4         2.8          5.6         2.2\n## 134          6.3         2.8          5.1         1.5\n## 135          6.1         2.6          5.6         1.4\n## 136          7.7         3.0          6.1         2.3\n## 137          6.3         3.4          5.6         2.4\n## 138          6.4         3.1          5.5         1.8\n## 139          6.0         3.0          4.8         1.8\n## 140          6.9         3.1          5.4         2.1\n## 141          6.7         3.1          5.6         2.4\n## 142          6.9         3.1          5.1         2.3\n## 143          5.8         2.7          5.1         1.9\n## 144          6.8         3.2          5.9         2.3\n## 145          6.7         3.3          5.7         2.5\n## 146          6.7         3.0          5.2         2.3\n## 147          6.3         2.5          5.0         1.9\n## 148          6.5         3.0          5.2         2.0\n## 149          6.2         3.4          5.4         2.3\n## 150          5.9         3.0          5.1         1.8\ncolSums(iris[, -5]) # ì—´ë³„ í•©ê³„\n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n##        876.5        458.6        563.7        179.9\ncolMeans(iris[, -5]) # ì—´ë³„ í‰ê· \n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n##     5.843333     3.057333     3.758000     1.199333\nrowSums(iris[, -5]) # í–‰ë³„ í•©ê³„\n##   [1] 10.2  9.5  9.4  9.4 10.2 11.4  9.7 10.1  8.9  9.6 10.8 10.0  9.3  8.5 11.2\n##  [16] 12.0 11.0 10.3 11.5 10.7 10.7 10.7  9.4 10.6 10.3  9.8 10.4 10.4 10.2  9.7\n##  [31]  9.7 10.7 10.9 11.3  9.7  9.6 10.5 10.0  8.9 10.2 10.1  8.4  9.1 10.7 11.2\n##  [46]  9.5 10.7  9.4 10.7  9.9 16.3 15.6 16.4 13.1 15.4 14.3 15.9 11.6 15.4 13.2\n##  [61] 11.5 14.6 13.2 15.1 13.4 15.6 14.6 13.6 14.4 13.1 15.7 14.2 15.2 14.8 14.9\n##  [76] 15.4 15.8 16.4 14.9 12.8 12.8 12.6 13.6 15.4 14.4 15.5 16.0 14.3 14.0 13.3\n##  [91] 13.7 15.1 13.6 11.6 13.8 14.1 14.1 14.7 11.7 13.9 18.1 15.5 18.1 16.6 17.5\n## [106] 19.3 13.6 18.3 16.8 19.4 16.8 16.3 17.4 15.2 16.1 17.2 16.8 20.4 19.5 14.7\n## [121] 18.1 15.3 19.2 15.7 17.8 18.2 15.6 15.8 16.9 17.6 18.2 20.1 17.0 15.7 15.7\n## [136] 19.1 17.7 16.8 15.6 17.5 17.8 17.4 15.5 18.2 18.2 17.2 15.7 16.7 17.3 15.8\nrowMeans(iris[, -5]) # í–‰ë³„ í‰ê· \n##   [1] 2.550 2.375 2.350 2.350 2.550 2.850 2.425 2.525 2.225 2.400 2.700 2.500\n##  [13] 2.325 2.125 2.800 3.000 2.750 2.575 2.875 2.675 2.675 2.675 2.350 2.650\n##  [25] 2.575 2.450 2.600 2.600 2.550 2.425 2.425 2.675 2.725 2.825 2.425 2.400\n##  [37] 2.625 2.500 2.225 2.550 2.525 2.100 2.275 2.675 2.800 2.375 2.675 2.350\n##  [49] 2.675 2.475 4.075 3.900 4.100 3.275 3.850 3.575 3.975 2.900 3.850 3.300\n##  [61] 2.875 3.650 3.300 3.775 3.350 3.900 3.650 3.400 3.600 3.275 3.925 3.550\n##  [73] 3.800 3.700 3.725 3.850 3.950 4.100 3.725 3.200 3.200 3.150 3.400 3.850\n##  [85] 3.600 3.875 4.000 3.575 3.500 3.325 3.425 3.775 3.400 2.900 3.450 3.525\n##  [97] 3.525 3.675 2.925 3.475 4.525 3.875 4.525 4.150 4.375 4.825 3.400 4.575\n## [109] 4.200 4.850 4.200 4.075 4.350 3.800 4.025 4.300 4.200 5.100 4.875 3.675\n## [121] 4.525 3.825 4.800 3.925 4.450 4.550 3.900 3.950 4.225 4.400 4.550 5.025\n## [133] 4.250 3.925 3.925 4.775 4.425 4.200 3.900 4.375 4.450 4.350 3.875 4.550\n## [145] 4.550 4.300 3.925 4.175 4.325 3.950\n\n\nz <- matrix(1:20, nrow = 4, ncol = 5)\nz\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\nt(z) # í–‰ê³¼ì—´ ë°©í–¥ ì „í™˜\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    2    3    4\n## [2,]    5    6    7    8\n## [3,]    9   10   11   12\n## [4,]   13   14   15   16\n## [5,]   17   18   19   20\n\n\nIR.1 <- subset(iris, Species == \"setosa\")\nIR.1\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1           5.1         3.5          1.4         0.2  setosa\n## 2           4.9         3.0          1.4         0.2  setosa\n## 3           4.7         3.2          1.3         0.2  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\n## 5           5.0         3.6          1.4         0.2  setosa\n## 6           5.4         3.9          1.7         0.4  setosa\n## 7           4.6         3.4          1.4         0.3  setosa\n## 8           5.0         3.4          1.5         0.2  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\n## 11          5.4         3.7          1.5         0.2  setosa\n## 12          4.8         3.4          1.6         0.2  setosa\n## 13          4.8         3.0          1.4         0.1  setosa\n## 14          4.3         3.0          1.1         0.1  setosa\n## 15          5.8         4.0          1.2         0.2  setosa\n## 16          5.7         4.4          1.5         0.4  setosa\n## 17          5.4         3.9          1.3         0.4  setosa\n## 18          5.1         3.5          1.4         0.3  setosa\n## 19          5.7         3.8          1.7         0.3  setosa\n## 20          5.1         3.8          1.5         0.3  setosa\n## 21          5.4         3.4          1.7         0.2  setosa\n## 22          5.1         3.7          1.5         0.4  setosa\n## 23          4.6         3.6          1.0         0.2  setosa\n## 24          5.1         3.3          1.7         0.5  setosa\n## 25          4.8         3.4          1.9         0.2  setosa\n## 26          5.0         3.0          1.6         0.2  setosa\n## 27          5.0         3.4          1.6         0.4  setosa\n## 28          5.2         3.5          1.5         0.2  setosa\n## 29          5.2         3.4          1.4         0.2  setosa\n## 30          4.7         3.2          1.6         0.2  setosa\n## 31          4.8         3.1          1.6         0.2  setosa\n## 32          5.4         3.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\n## 35          4.9         3.1          1.5         0.2  setosa\n## 36          5.0         3.2          1.2         0.2  setosa\n## 37          5.5         3.5          1.3         0.2  setosa\n## 38          4.9         3.6          1.4         0.1  setosa\n## 39          4.4         3.0          1.3         0.2  setosa\n## 40          5.1         3.4          1.5         0.2  setosa\n## 41          5.0         3.5          1.3         0.3  setosa\n## 42          4.5         2.3          1.3         0.3  setosa\n## 43          4.4         3.2          1.3         0.2  setosa\n## 44          5.0         3.5          1.6         0.6  setosa\n## 45          5.1         3.8          1.9         0.4  setosa\n## 46          4.8         3.0          1.4         0.3  setosa\n## 47          5.1         3.8          1.6         0.2  setosa\n## 48          4.6         3.2          1.4         0.2  setosa\n## 49          5.3         3.7          1.5         0.2  setosa\n## 50          5.0         3.3          1.4         0.2  setosa\n\nIR.2 <- subset(iris, Sepal.Length > 5.0 & Sepal.Width > 4.0)\nIR.2\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 16          5.7         4.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\nIR.2[, c(2, 4)] # 2, 4ì—´ì˜ ê°’ë§Œ ì¶”ì¶œ\n##    Sepal.Width Petal.Width\n## 16         4.4         0.4\n## 33         4.1         0.1\n## 34         4.2         0.2\n\nIR.3 <- subset(iris, Sepal.Length > 5.0 | Sepal.Width > 4.0)\nIR.3\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 1            5.1         3.5          1.4         0.2     setosa\n## 6            5.4         3.9          1.7         0.4     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 15           5.8         4.0          1.2         0.2     setosa\n## 16           5.7         4.4          1.5         0.4     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 34           5.5         4.2          1.4         0.2     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 49           5.3         3.7          1.5         0.2     setosa\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 114          5.7         2.5          5.0         2.0  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 120          6.0         2.2          5.0         1.5  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 122          5.6         2.8          4.9         2.0  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 127          6.2         2.8          4.8         1.8  virginica\n## 128          6.1         3.0          4.9         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\n\n\na <- matrix(1:20, 4, 5)\nb <- matrix(21:40, 4, 5)\na ; b\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    5    9   13   17\n## [2,]    2    6   10   14   18\n## [3,]    3    7   11   15   19\n## [4,]    4    8   12   16   20\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   21   25   29   33   37\n## [2,]   22   26   30   34   38\n## [3,]   23   27   31   35   39\n## [4,]   24   28   32   36   40\n\n2 * a # ë§¤íŠ¸ë¦­ìŠ¤ aì— ì €ì¥ëœ ê°’ë“¤ì— 2ë¥¼ ê³±í•˜ê¸°\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    2   10   18   26   34\n## [2,]    4   12   20   28   36\n## [3,]    6   14   22   30   38\n## [4,]    8   16   24   32   40\nb - 5\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   16   20   24   28   32\n## [2,]   17   21   25   29   33\n## [3,]   18   22   26   30   34\n## [4,]   19   23   27   31   35\n2 * a + 3 * b\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   65   85  105  125  145\n## [2,]   70   90  110  130  150\n## [3,]   75   95  115  135  155\n## [4,]   80  100  120  140  160\n\na + b\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   22   30   38   46   54\n## [2,]   24   32   40   48   56\n## [3,]   26   34   42   50   58\n## [4,]   28   36   44   52   60\nb - a\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   20   20   20   20   20\n## [2,]   20   20   20   20   20\n## [3,]   20   20   20   20   20\n## [4,]   20   20   20   20   20\nb / a\n##           [,1]     [,2]     [,3]     [,4]     [,5]\n## [1,] 21.000000 5.000000 3.222222 2.538462 2.176471\n## [2,] 11.000000 4.333333 3.000000 2.428571 2.111111\n## [3,]  7.666667 3.857143 2.818182 2.333333 2.052632\n## [4,]  6.000000 3.500000 2.666667 2.250000 2.000000\na * b\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   21  125  261  429  629\n## [2,]   44  156  300  476  684\n## [3,]   69  189  341  525  741\n## [4,]   96  224  384  576  800\n\na <- a * 3\na\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    3   15   27   39   51\n## [2,]    6   18   30   42   54\n## [3,]    9   21   33   45   57\n## [4,]   12   24   36   48   60\nb <- b - 5\nb\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]   16   20   24   28   32\n## [2,]   17   21   25   29   33\n## [3,]   18   22   26   30   34\n## [4,]   19   23   27   31   35\n\n\nclass(iris) # iris ë°ì´í„°ì…‹ì˜ ìë£Œêµ¬ì¡° í™•ì¸\n## [1] \"data.frame\"\nclass(state.x77) # state.x77 ë°ì´í„°ì…‹ì˜ ìë£Œêµ¬ì¡° í™•ì¸\n## [1] \"matrix\" \"array\"\nis.matrix(iris) # ë°ì´í„°ì…‹ì´ ë§¤íŠ¸ë¦­ìŠ¤ì¸ì§€ë¥¼ í™•ì¸í•˜ëŠ” í•¨ìˆ˜\n## [1] FALSE\nis.data.frame(iris) # ë°ì´í„°ì…‹ì´ ë°ì´í„°í”„ë ˆì„ì¸ì§€ë¥¼ í™•ì¸í•˜ëŠ” í•¨ìˆ˜\n## [1] TRUE\nis.matrix(state.x77)\n## [1] TRUE\nis.data.frame(state.x77)\n## [1] FALSE\n\n\n# ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜\nst <- data.frame(state.x77)\nhead(st)\n##            Population Income Illiteracy Life.Exp Murder HS.Grad Frost   Area\n## Alabama          3615   3624        2.1    69.05   15.1    41.3    20  50708\n## Alaska            365   6315        1.5    69.31   11.3    66.7   152 566432\n## Arizona          2212   4530        1.8    70.55    7.8    58.1    15 113417\n## Arkansas         2110   3378        1.9    70.66   10.1    39.9    65  51945\n## California      21198   5114        1.1    71.71   10.3    62.6    20 156361\n## Colorado         2541   4884        0.7    72.06    6.8    63.9   166 103766\nclass(st)\n## [1] \"data.frame\"\n\n\niris[, \"Species\"] # ê²°ê³¼=ë²¡í„°. ë§¤íŠ¸ë¦­ìŠ¤ì™€ ë°ì´í„°í”„ë ˆì„ ëª¨ë‘ ê°€ëŠ¥\n##   [1] setosa     setosa     setosa     setosa     setosa     setosa    \n##   [7] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [13] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [19] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [25] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [31] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [37] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [43] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [49] setosa     setosa     versicolor versicolor versicolor versicolor\n##  [55] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [61] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [67] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [73] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [79] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [85] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [91] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [97] versicolor versicolor versicolor versicolor virginica  virginica \n## [103] virginica  virginica  virginica  virginica  virginica  virginica \n## [109] virginica  virginica  virginica  virginica  virginica  virginica \n## [115] virginica  virginica  virginica  virginica  virginica  virginica \n## [121] virginica  virginica  virginica  virginica  virginica  virginica \n## [127] virginica  virginica  virginica  virginica  virginica  virginica \n## [133] virginica  virginica  virginica  virginica  virginica  virginica \n## [139] virginica  virginica  virginica  virginica  virginica  virginica \n## [145] virginica  virginica  virginica  virginica  virginica  virginica \n## Levels: setosa versicolor virginica\niris[, 5] # ê²°ê³¼=ë²¡í„°. ë§¤íŠ¸ë¦­ìŠ¤ì™€ ë°ì´í„°í”„ë ˆì„ ëª¨ë‘ ê°€ëŠ¥\n##   [1] setosa     setosa     setosa     setosa     setosa     setosa    \n##   [7] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [13] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [19] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [25] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [31] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [37] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [43] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [49] setosa     setosa     versicolor versicolor versicolor versicolor\n##  [55] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [61] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [67] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [73] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [79] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [85] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [91] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [97] versicolor versicolor versicolor versicolor virginica  virginica \n## [103] virginica  virginica  virginica  virginica  virginica  virginica \n## [109] virginica  virginica  virginica  virginica  virginica  virginica \n## [115] virginica  virginica  virginica  virginica  virginica  virginica \n## [121] virginica  virginica  virginica  virginica  virginica  virginica \n## [127] virginica  virginica  virginica  virginica  virginica  virginica \n## [133] virginica  virginica  virginica  virginica  virginica  virginica \n## [139] virginica  virginica  virginica  virginica  virginica  virginica \n## [145] virginica  virginica  virginica  virginica  virginica  virginica \n## Levels: setosa versicolor virginica\niris[\"Species\"] # ê²°ê³¼=ë°ì´í„°í”„ë ˆì„. ë°ì´í„°í”„ë ˆì„ë§Œ ê°€ëŠ¥\n##        Species\n## 1       setosa\n## 2       setosa\n## 3       setosa\n## 4       setosa\n## 5       setosa\n## 6       setosa\n## 7       setosa\n## 8       setosa\n## 9       setosa\n## 10      setosa\n## 11      setosa\n## 12      setosa\n## 13      setosa\n## 14      setosa\n## 15      setosa\n## 16      setosa\n## 17      setosa\n## 18      setosa\n## 19      setosa\n## 20      setosa\n## 21      setosa\n## 22      setosa\n## 23      setosa\n## 24      setosa\n## 25      setosa\n## 26      setosa\n## 27      setosa\n## 28      setosa\n## 29      setosa\n## 30      setosa\n## 31      setosa\n## 32      setosa\n## 33      setosa\n## 34      setosa\n## 35      setosa\n## 36      setosa\n## 37      setosa\n## 38      setosa\n## 39      setosa\n## 40      setosa\n## 41      setosa\n## 42      setosa\n## 43      setosa\n## 44      setosa\n## 45      setosa\n## 46      setosa\n## 47      setosa\n## 48      setosa\n## 49      setosa\n## 50      setosa\n## 51  versicolor\n## 52  versicolor\n## 53  versicolor\n## 54  versicolor\n## 55  versicolor\n## 56  versicolor\n## 57  versicolor\n## 58  versicolor\n## 59  versicolor\n## 60  versicolor\n## 61  versicolor\n## 62  versicolor\n## 63  versicolor\n## 64  versicolor\n## 65  versicolor\n## 66  versicolor\n## 67  versicolor\n## 68  versicolor\n## 69  versicolor\n## 70  versicolor\n## 71  versicolor\n## 72  versicolor\n## 73  versicolor\n## 74  versicolor\n## 75  versicolor\n## 76  versicolor\n## 77  versicolor\n## 78  versicolor\n## 79  versicolor\n## 80  versicolor\n## 81  versicolor\n## 82  versicolor\n## 83  versicolor\n## 84  versicolor\n## 85  versicolor\n## 86  versicolor\n## 87  versicolor\n## 88  versicolor\n## 89  versicolor\n## 90  versicolor\n## 91  versicolor\n## 92  versicolor\n## 93  versicolor\n## 94  versicolor\n## 95  versicolor\n## 96  versicolor\n## 97  versicolor\n## 98  versicolor\n## 99  versicolor\n## 100 versicolor\n## 101  virginica\n## 102  virginica\n## 103  virginica\n## 104  virginica\n## 105  virginica\n## 106  virginica\n## 107  virginica\n## 108  virginica\n## 109  virginica\n## 110  virginica\n## 111  virginica\n## 112  virginica\n## 113  virginica\n## 114  virginica\n## 115  virginica\n## 116  virginica\n## 117  virginica\n## 118  virginica\n## 119  virginica\n## 120  virginica\n## 121  virginica\n## 122  virginica\n## 123  virginica\n## 124  virginica\n## 125  virginica\n## 126  virginica\n## 127  virginica\n## 128  virginica\n## 129  virginica\n## 130  virginica\n## 131  virginica\n## 132  virginica\n## 133  virginica\n## 134  virginica\n## 135  virginica\n## 136  virginica\n## 137  virginica\n## 138  virginica\n## 139  virginica\n## 140  virginica\n## 141  virginica\n## 142  virginica\n## 143  virginica\n## 144  virginica\n## 145  virginica\n## 146  virginica\n## 147  virginica\n## 148  virginica\n## 149  virginica\n## 150  virginica\niris[5] # ê²°ê³¼=ë°ì´í„°í”„ë ˆì„. ë°ì´í„°í”„ë ˆì„ë§Œ ê°€ëŠ¥\n##        Species\n## 1       setosa\n## 2       setosa\n## 3       setosa\n## 4       setosa\n## 5       setosa\n## 6       setosa\n## 7       setosa\n## 8       setosa\n## 9       setosa\n## 10      setosa\n## 11      setosa\n## 12      setosa\n## 13      setosa\n## 14      setosa\n## 15      setosa\n## 16      setosa\n## 17      setosa\n## 18      setosa\n## 19      setosa\n## 20      setosa\n## 21      setosa\n## 22      setosa\n## 23      setosa\n## 24      setosa\n## 25      setosa\n## 26      setosa\n## 27      setosa\n## 28      setosa\n## 29      setosa\n## 30      setosa\n## 31      setosa\n## 32      setosa\n## 33      setosa\n## 34      setosa\n## 35      setosa\n## 36      setosa\n## 37      setosa\n## 38      setosa\n## 39      setosa\n## 40      setosa\n## 41      setosa\n## 42      setosa\n## 43      setosa\n## 44      setosa\n## 45      setosa\n## 46      setosa\n## 47      setosa\n## 48      setosa\n## 49      setosa\n## 50      setosa\n## 51  versicolor\n## 52  versicolor\n## 53  versicolor\n## 54  versicolor\n## 55  versicolor\n## 56  versicolor\n## 57  versicolor\n## 58  versicolor\n## 59  versicolor\n## 60  versicolor\n## 61  versicolor\n## 62  versicolor\n## 63  versicolor\n## 64  versicolor\n## 65  versicolor\n## 66  versicolor\n## 67  versicolor\n## 68  versicolor\n## 69  versicolor\n## 70  versicolor\n## 71  versicolor\n## 72  versicolor\n## 73  versicolor\n## 74  versicolor\n## 75  versicolor\n## 76  versicolor\n## 77  versicolor\n## 78  versicolor\n## 79  versicolor\n## 80  versicolor\n## 81  versicolor\n## 82  versicolor\n## 83  versicolor\n## 84  versicolor\n## 85  versicolor\n## 86  versicolor\n## 87  versicolor\n## 88  versicolor\n## 89  versicolor\n## 90  versicolor\n## 91  versicolor\n## 92  versicolor\n## 93  versicolor\n## 94  versicolor\n## 95  versicolor\n## 96  versicolor\n## 97  versicolor\n## 98  versicolor\n## 99  versicolor\n## 100 versicolor\n## 101  virginica\n## 102  virginica\n## 103  virginica\n## 104  virginica\n## 105  virginica\n## 106  virginica\n## 107  virginica\n## 108  virginica\n## 109  virginica\n## 110  virginica\n## 111  virginica\n## 112  virginica\n## 113  virginica\n## 114  virginica\n## 115  virginica\n## 116  virginica\n## 117  virginica\n## 118  virginica\n## 119  virginica\n## 120  virginica\n## 121  virginica\n## 122  virginica\n## 123  virginica\n## 124  virginica\n## 125  virginica\n## 126  virginica\n## 127  virginica\n## 128  virginica\n## 129  virginica\n## 130  virginica\n## 131  virginica\n## 132  virginica\n## 133  virginica\n## 134  virginica\n## 135  virginica\n## 136  virginica\n## 137  virginica\n## 138  virginica\n## 139  virginica\n## 140  virginica\n## 141  virginica\n## 142  virginica\n## 143  virginica\n## 144  virginica\n## 145  virginica\n## 146  virginica\n## 147  virginica\n## 148  virginica\n## 149  virginica\n## 150  virginica\niris$Species # ê²°ê³¼=ë²¡í„°. ë°ì´í„°í”„ë ˆì„ë§Œ ê°€ëŠ¥\n##   [1] setosa     setosa     setosa     setosa     setosa     setosa    \n##   [7] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [13] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [19] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [25] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [31] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [37] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [43] setosa     setosa     setosa     setosa     setosa     setosa    \n##  [49] setosa     setosa     versicolor versicolor versicolor versicolor\n##  [55] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [61] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [67] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [73] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [79] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [85] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [91] versicolor versicolor versicolor versicolor versicolor versicolor\n##  [97] versicolor versicolor versicolor versicolor virginica  virginica \n## [103] virginica  virginica  virginica  virginica  virginica  virginica \n## [109] virginica  virginica  virginica  virginica  virginica  virginica \n## [115] virginica  virginica  virginica  virginica  virginica  virginica \n## [121] virginica  virginica  virginica  virginica  virginica  virginica \n## [127] virginica  virginica  virginica  virginica  virginica  virginica \n## [133] virginica  virginica  virginica  virginica  virginica  virginica \n## [139] virginica  virginica  virginica  virginica  virginica  virginica \n## [145] virginica  virginica  virginica  virginica  virginica  virginica \n## Levels: setosa versicolor virginica\n\n\ngetwd()\n## [1] \"C:/Users/Hyunsoo Kim/Desktop/senior_grade/blog/my-quarto-website\"\n# setwd(\"G:/ë‚´ ë“œë¼ì´ë¸Œ/202202/R_Basic/data\") # ì‘ì—… í´ë” ì§€ì •\nair <- read.csv(\"./R_Basic/data/airquality.csv\", header = T) # .csv íŒŒì¼ ì½ê¸°\nhead(air)\n##                                    version.https...git.lfs.github.com.spec.v1\n## 1 oid sha256:6fdc84af524856a54abe063336bfea6511e9fb5dfcd2ec6e1dfa9e1e4d8c7357\n## 2                                                                   size 3044\n\n\nmy.iris <- subset(iris, Species = 'Setosa') # Setosa í’ˆì¢… ë°ì´í„°ë§Œ ì¶”ì¶œ\n## Warning: In subset.data.frame(iris, Species = \"Setosa\") :\n##  extra argument 'Species' will be disregarded\nwrite.csv(my.iris, \"./R_Basic/data/my_iris_1.csv\") # .csv íŒŒì¼ì— ì €ì¥í•˜ê¸°\n\n\n\n\n\njob.type <- 'A'\nif (job.type == 'B') {\n    bonus <- 200 # ì§ë¬´ ìœ í˜•ì´ Bì¼ ë•Œ ì‹¤í–‰\n} else {\n    bonus <- 100 # ì§ë¬´ ìœ í˜•ì´ Bê°€ ì•„ë‹Œ ë‚˜ë¨¸ì§€ ê²½ìš° ì‹¤í–‰\n}\nprint(bonus)\n## [1] 100\n\n\njob.type <- 'B'\nbonus <- 100\nif (job.type == 'A') {\n    bonus <- 200 # ì§ë¬´ ìœ í˜•ì´ Aì¼ ë•Œ ì‹¤í–‰\n}\nprint(bonus)\n## [1] 100\n\n\nscore <- 85\n\nif (score > 90) {\n    grade <- 'A'\n} else if (score > 80) {\n    grade <- 'B'\n} else if (score > 70) {\n    grade <- 'C'\n} else if (score > 60) {\n    grade <- 'D'\n} else {\n    grade <- 'F'\n}\n\nprint(grade)\n## [1] \"B\"\n\n\na <- 10\nb <- 20\nif (a > 5 & b > 5) {    # and ì‚¬ìš©\n    print(a + b)\n}\n## [1] 30\n\nif (a > 5 | b > 30) {   # or ì‚¬ìš©\n    print(a * b)\n}\n## [1] 200\n\nif (a > 5 & b > 30) {\n    print(a * b)\n}\n\nif (a > 20 | b > 30) {\n    print(a * b)\n}\n\nif (a > 20 & b > 15) {\n    print(a * b)\n}\n\nr_basic <- 70\npython_basic <- 82\n\nif (r_basic > 80 & python_basic > 80) {\n    grade <- \"Excellent\"\n} else {\n    grade <- \"Good\"\n}\ngrade\n## [1] \"Good\"\n\n\na <- 10\nb <- 20\n\nif (a > b) {\n    c <- a\n} else {\n    c <- b\n}\nprint(c)\n## [1] 20\n\na <- 10\nb <- 20\n\nc <- ifelse(a > b, a, b)\nprint(c)\n## [1] 20\n\n\nfor(i in 1:5) {\n    print('*')\n}\n## [1] \"*\"\n## [1] \"*\"\n## [1] \"*\"\n## [1] \"*\"\n## [1] \"*\"\n\nfor (i in 1:5) {\n    print(i)\n}\n## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\n\nfor (i in 1:5) {\n    a <- i * 2\n    print(a)\n}\n## [1] 2\n## [1] 4\n## [1] 6\n## [1] 8\n## [1] 10\n\n# for (i in 1:10000) {\n#     a <- i * 2\n#     print(a)\n# }\n\n# for (i in 1:10000) {\n#     a <- i * 2 / 1521 + 10000\n#     print(a)\n# }\n\n\nfor (i in 6:10) {\n    print(i)\n}\n## [1] 6\n## [1] 7\n## [1] 8\n## [1] 9\n## [1] 10\n\n\nfor(i in 1:9) {\n    cat('2 *', i, '=', 2 * i, '\\n')\n}\n## 2 * 1 = 2 \n## 2 * 2 = 4 \n## 2 * 3 = 6 \n## 2 * 4 = 8 \n## 2 * 5 = 10 \n## 2 * 6 = 12 \n## 2 * 7 = 14 \n## 2 * 8 = 16 \n## 2 * 9 = 18\n\nfor (i in 1:9) {\n    cat('2 *', i, '=', 2 * i)\n}\n## 2 * 1 = 22 * 2 = 42 * 3 = 62 * 4 = 82 * 5 = 102 * 6 = 122 * 7 = 142 * 8 = 162 * 9 = 18\n\nfor (i in 1:9) {\n    j <- i:10\n    print(j)\n}\n##  [1]  1  2  3  4  5  6  7  8  9 10\n## [1]  2  3  4  5  6  7  8  9 10\n## [1]  3  4  5  6  7  8  9 10\n## [1]  4  5  6  7  8  9 10\n## [1]  5  6  7  8  9 10\n## [1]  6  7  8  9 10\n## [1]  7  8  9 10\n## [1]  8  9 10\n## [1]  9 10\n\n\nfor(i in 1:20) {\n    if (i %% 2 == 0) {  # ì§ìˆ˜ì¸ì§€ í™•ì¸\n        print(i)\n    }\n}\n## [1] 2\n## [1] 4\n## [1] 6\n## [1] 8\n## [1] 10\n## [1] 12\n## [1] 14\n## [1] 16\n## [1] 18\n## [1] 20\n\n\nsum <- 0\nfor (i in 1:100) {\n    sum <- sum + i  # sumì— i ê°’ì„ ëˆ„ì \n}\nprint(sum)\n## [1] 5050\n\nsum <- 0\nfor (i in 1:100) {\n    sum <- sum + i\n    print(c(sum, i))\n}\n## [1] 1 1\n## [1] 3 2\n## [1] 6 3\n## [1] 10  4\n## [1] 15  5\n## [1] 21  6\n## [1] 28  7\n## [1] 36  8\n## [1] 45  9\n## [1] 55 10\n## [1] 66 11\n## [1] 78 12\n## [1] 91 13\n## [1] 105  14\n## [1] 120  15\n## [1] 136  16\n## [1] 153  17\n## [1] 171  18\n## [1] 190  19\n## [1] 210  20\n## [1] 231  21\n## [1] 253  22\n## [1] 276  23\n## [1] 300  24\n## [1] 325  25\n## [1] 351  26\n## [1] 378  27\n## [1] 406  28\n## [1] 435  29\n## [1] 465  30\n## [1] 496  31\n## [1] 528  32\n## [1] 561  33\n## [1] 595  34\n## [1] 630  35\n## [1] 666  36\n## [1] 703  37\n## [1] 741  38\n## [1] 780  39\n## [1] 820  40\n## [1] 861  41\n## [1] 903  42\n## [1] 946  43\n## [1] 990  44\n## [1] 1035   45\n## [1] 1081   46\n## [1] 1128   47\n## [1] 1176   48\n## [1] 1225   49\n## [1] 1275   50\n## [1] 1326   51\n## [1] 1378   52\n## [1] 1431   53\n## [1] 1485   54\n## [1] 1540   55\n## [1] 1596   56\n## [1] 1653   57\n## [1] 1711   58\n## [1] 1770   59\n## [1] 1830   60\n## [1] 1891   61\n## [1] 1953   62\n## [1] 2016   63\n## [1] 2080   64\n## [1] 2145   65\n## [1] 2211   66\n## [1] 2278   67\n## [1] 2346   68\n## [1] 2415   69\n## [1] 2485   70\n## [1] 2556   71\n## [1] 2628   72\n## [1] 2701   73\n## [1] 2775   74\n## [1] 2850   75\n## [1] 2926   76\n## [1] 3003   77\n## [1] 3081   78\n## [1] 3160   79\n## [1] 3240   80\n## [1] 3321   81\n## [1] 3403   82\n## [1] 3486   83\n## [1] 3570   84\n## [1] 3655   85\n## [1] 3741   86\n## [1] 3828   87\n## [1] 3916   88\n## [1] 4005   89\n## [1] 4095   90\n## [1] 4186   91\n## [1] 4278   92\n## [1] 4371   93\n## [1] 4465   94\n## [1] 4560   95\n## [1] 4656   96\n## [1] 4753   97\n## [1] 4851   98\n## [1] 4950   99\n## [1] 5050  100\nprint(sum)\n## [1] 5050\n\nsum <- 0\nfor (i in 1:100) {\n    print(c(sum, i))\n    sum <- sum + i\n}\n## [1] 0 1\n## [1] 1 2\n## [1] 3 3\n## [1] 6 4\n## [1] 10  5\n## [1] 15  6\n## [1] 21  7\n## [1] 28  8\n## [1] 36  9\n## [1] 45 10\n## [1] 55 11\n## [1] 66 12\n## [1] 78 13\n## [1] 91 14\n## [1] 105  15\n## [1] 120  16\n## [1] 136  17\n## [1] 153  18\n## [1] 171  19\n## [1] 190  20\n## [1] 210  21\n## [1] 231  22\n## [1] 253  23\n## [1] 276  24\n## [1] 300  25\n## [1] 325  26\n## [1] 351  27\n## [1] 378  28\n## [1] 406  29\n## [1] 435  30\n## [1] 465  31\n## [1] 496  32\n## [1] 528  33\n## [1] 561  34\n## [1] 595  35\n## [1] 630  36\n## [1] 666  37\n## [1] 703  38\n## [1] 741  39\n## [1] 780  40\n## [1] 820  41\n## [1] 861  42\n## [1] 903  43\n## [1] 946  44\n## [1] 990  45\n## [1] 1035   46\n## [1] 1081   47\n## [1] 1128   48\n## [1] 1176   49\n## [1] 1225   50\n## [1] 1275   51\n## [1] 1326   52\n## [1] 1378   53\n## [1] 1431   54\n## [1] 1485   55\n## [1] 1540   56\n## [1] 1596   57\n## [1] 1653   58\n## [1] 1711   59\n## [1] 1770   60\n## [1] 1830   61\n## [1] 1891   62\n## [1] 1953   63\n## [1] 2016   64\n## [1] 2080   65\n## [1] 2145   66\n## [1] 2211   67\n## [1] 2278   68\n## [1] 2346   69\n## [1] 2415   70\n## [1] 2485   71\n## [1] 2556   72\n## [1] 2628   73\n## [1] 2701   74\n## [1] 2775   75\n## [1] 2850   76\n## [1] 2926   77\n## [1] 3003   78\n## [1] 3081   79\n## [1] 3160   80\n## [1] 3240   81\n## [1] 3321   82\n## [1] 3403   83\n## [1] 3486   84\n## [1] 3570   85\n## [1] 3655   86\n## [1] 3741   87\n## [1] 3828   88\n## [1] 3916   89\n## [1] 4005   90\n## [1] 4095   91\n## [1] 4186   92\n## [1] 4278   93\n## [1] 4371   94\n## [1] 4465   95\n## [1] 4560   96\n## [1] 4656   97\n## [1] 4753   98\n## [1] 4851   99\n## [1] 4950  100\nprint(sum)\n## [1] 5050\n\n\nnorow <- nrow(iris)                             # irisì˜ í–‰ì˜ ìˆ˜\nmylabel <- c()                                  # ë¹„ì–´ ìˆëŠ” ë²¡í„° ì„ ì–¸\nfor (i in 1:norow) {\n    if (iris$Petal.Length[i] <= 1.6) {          # ê½ƒìì˜ ê¸¸ì´ì— ë”°ë¼ ë ˆì´ë¸” ê²°ì •\n        mylabel[i] <- 'L'\n    } else if (iris$Petal.Length[i] >= 5.1) {\n        mylabel[i] <- 'H'\n    } else {\n        mylabel[i] <- 'M'\n    }\n    print(c(iris$Petal.Length[i], mylabel))\n}\n## [1] \"1.4\" \"L\"  \n## [1] \"1.4\" \"L\"   \"L\"  \n## [1] \"1.3\" \"L\"   \"L\"   \"L\"  \n## [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"  \n## [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [1] \"1.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"  \n## [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"  \n## [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"  \n##  [1] \"1.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"  \n##  [1] \"1.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"  \n##  [1] \"1.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"  \n##  [1] \"1\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\"\n##  [1] \"1.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"  \n##  [1] \"1.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [1] \"1.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"  \n##  [1] \"1.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"  \n##  [1] \"1.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"  \n##  [1] \"1.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"  \n##  [1] \"4.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"  \n##  [1] \"4.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\"\n##  [1] \"4.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"  \n##  [1] \"3.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"  \n##  [1] \"4.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"  \n##  [1] \"4\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [58] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [1] \"4.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [58] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [1] \"4.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"  \n##  [1] \"4.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"  \n##  [1] \"4.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"5\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [58] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [77] \"M\" \"M\" \"M\"\n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"3.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"  \n##  [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"  \n##  [1] \"4.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [58] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [77] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [1] \"4.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [20] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n## [39] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [58] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n## [77] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [1] \"3.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [1] \"4.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [97] \"M\"  \n##  [1] \"4.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [97] \"M\"   \"M\"  \n##  [1] \"4.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n## [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n## [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n## [97] \"M\"   \"M\"   \"M\"  \n##   [1] \"3\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##   [1] \"4.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##   [1] \"6\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\"\n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"  \n##   [1] \"5.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"6.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"4.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n##   [1] \"6.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"  \n##   [1] \"5.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"  \n##   [1] \"6.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"  \n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n## [109] \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"  \n##   [1] \"5.3\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"  \n##   [1] \"5.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"6.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"6.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n## [109] \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n##   [1] \"5.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"  \n##   [1] \"4.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"  \n##   [1] \"6.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"  \n##   [1] \"4.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"  \n##   [1] \"5.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"  \n##   [1] \"6\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n## [109] \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"M\" \"H\" \"M\" \"H\"\n## [127] \"H\"\n##   [1] \"4.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"  \n##   [1] \"4.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"  \n##   [1] \"5.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"  \n##   [1] \"6.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"6.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"  \n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"6.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.5\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"4.8\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n##   [1] \"5.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"  \n##   [1] \"5.6\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"  \n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n##   [1] \"5.9\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"  \n##   [1] \"5.7\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"   \"H\"  \n##   [1] \"5.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"   \"H\"   \"H\"  \n##   [1] \"5\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"L\" \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\"\n## [109] \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"M\" \"H\" \"M\" \"H\"\n## [127] \"H\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\"\n## [145] \"H\" \"H\" \"H\" \"M\"\n##   [1] \"5.2\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"   \"H\"   \"H\"   \"M\"   \"H\"  \n##   [1] \"5.4\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"  \n##   [1] \"5.1\" \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [13] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [25] \"M\"   \"M\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"  \n##  [37] \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"L\"   \"M\"   \"L\"   \"L\"  \n##  [49] \"L\"   \"L\"   \"L\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [61] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [73] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [85] \"H\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"M\"  \n##  [97] \"M\"   \"M\"   \"M\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"  \n## [109] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [121] \"M\"   \"H\"   \"M\"   \"H\"   \"M\"   \"H\"   \"H\"   \"M\"   \"M\"   \"H\"   \"H\"   \"H\"  \n## [133] \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"   \"H\"  \n## [145] \"H\"   \"H\"   \"H\"   \"M\"   \"H\"   \"H\"   \"H\"\nprint(mylabel)                                  # ë ˆì´ë¸” ì¶œë ¥\n##   [1] \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [19] \"M\" \"L\" \"M\" \"L\" \"L\" \"M\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\"\n##  [37] \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"L\" \"L\" \"L\" \"L\" \"L\" \"M\" \"M\" \"M\" \"M\"\n##  [55] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [73] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n##  [91] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\"\n## [109] \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"M\" \"H\" \"M\" \"H\" \"H\"\n## [127] \"M\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"M\" \"H\" \"H\" \"H\" \"H\" \"H\"\n## [145] \"H\" \"H\" \"M\" \"H\" \"H\" \"H\"\nnewds <- data.frame(iris$Petal.Length, mylabel) # ê½ƒìì˜ ê¸¸ì´ì™€ ë ˆì´ë¸” ê²°í•©\nhead(newds)                                     # ìƒˆë¡œìš´ ë°ì´í„°ì…‹ ë‚´ìš© ì¶œë ¥\n##   iris.Petal.Length mylabel\n## 1               1.4       L\n## 2               1.4       L\n## 3               1.3       L\n## 4               1.5       L\n## 5               1.4       L\n## 6               1.7       M\n\n\nsum <- 0\ni <- 1\nwhile (i <= 100) {\n    sum <- sum + i      # sumì— i ê°’ì„ ëˆ„ì \n    i <- i + 1          # i ê°’ì„ 1 ì¦ê°€ì‹œí‚´\n    print(c(sum, i))\n}\n## [1] 1 2\n## [1] 3 3\n## [1] 6 4\n## [1] 10  5\n## [1] 15  6\n## [1] 21  7\n## [1] 28  8\n## [1] 36  9\n## [1] 45 10\n## [1] 55 11\n## [1] 66 12\n## [1] 78 13\n## [1] 91 14\n## [1] 105  15\n## [1] 120  16\n## [1] 136  17\n## [1] 153  18\n## [1] 171  19\n## [1] 190  20\n## [1] 210  21\n## [1] 231  22\n## [1] 253  23\n## [1] 276  24\n## [1] 300  25\n## [1] 325  26\n## [1] 351  27\n## [1] 378  28\n## [1] 406  29\n## [1] 435  30\n## [1] 465  31\n## [1] 496  32\n## [1] 528  33\n## [1] 561  34\n## [1] 595  35\n## [1] 630  36\n## [1] 666  37\n## [1] 703  38\n## [1] 741  39\n## [1] 780  40\n## [1] 820  41\n## [1] 861  42\n## [1] 903  43\n## [1] 946  44\n## [1] 990  45\n## [1] 1035   46\n## [1] 1081   47\n## [1] 1128   48\n## [1] 1176   49\n## [1] 1225   50\n## [1] 1275   51\n## [1] 1326   52\n## [1] 1378   53\n## [1] 1431   54\n## [1] 1485   55\n## [1] 1540   56\n## [1] 1596   57\n## [1] 1653   58\n## [1] 1711   59\n## [1] 1770   60\n## [1] 1830   61\n## [1] 1891   62\n## [1] 1953   63\n## [1] 2016   64\n## [1] 2080   65\n## [1] 2145   66\n## [1] 2211   67\n## [1] 2278   68\n## [1] 2346   69\n## [1] 2415   70\n## [1] 2485   71\n## [1] 2556   72\n## [1] 2628   73\n## [1] 2701   74\n## [1] 2775   75\n## [1] 2850   76\n## [1] 2926   77\n## [1] 3003   78\n## [1] 3081   79\n## [1] 3160   80\n## [1] 3240   81\n## [1] 3321   82\n## [1] 3403   83\n## [1] 3486   84\n## [1] 3570   85\n## [1] 3655   86\n## [1] 3741   87\n## [1] 3828   88\n## [1] 3916   89\n## [1] 4005   90\n## [1] 4095   91\n## [1] 4186   92\n## [1] 4278   93\n## [1] 4371   94\n## [1] 4465   95\n## [1] 4560   96\n## [1] 4656   97\n## [1] 4753   98\n## [1] 4851   99\n## [1] 4950  100\n## [1] 5050  101\nprint(sum)\n## [1] 5050\n\n#---------------------------------------#\n# ì˜¤ë¥˜ ì—†ì´ ê³„ì† ì‹¤í–‰ë¨\n# sum <- 0\n# i <- 1\n# while(i >= 1) {\n#   sum <- sum + i # sumì— i ê°’ì„ ëˆ„ì \n#   i <- i + 1 # i ê°’ì„ 1 ì¦ê°€ì‹œí‚´\n#   print(c(sum,i))\n# }\n# print(sum)\n#---------------------------------------#\n\n\nsum <- 0\nfor (i in 1:10) {\n    sum <- sum + i\n    print(c(sum, i))\n    if (i >= 5)\n        break\n}\n## [1] 1 1\n## [1] 3 2\n## [1] 6 3\n## [1] 10  4\n## [1] 15  5\nsum\n## [1] 15\n\n\nsum <- 0\nfor (i in 1:10) {\n    if (i %% 2 == 0)\n        next # %% = ë‚˜ë¨¸ì§€\n    sum <- sum + i\n    print(c(sum, i))\n}\n## [1] 1 1\n## [1] 4 3\n## [1] 9 5\n## [1] 16  7\n## [1] 25  9\nsum\n## [1] 25\n\n\napply(iris[, 1:4], 1, mean) # row ë°©í–¥ìœ¼ë¡œ í•¨ìˆ˜ ì ìš©\n##   [1] 2.550 2.375 2.350 2.350 2.550 2.850 2.425 2.525 2.225 2.400 2.700 2.500\n##  [13] 2.325 2.125 2.800 3.000 2.750 2.575 2.875 2.675 2.675 2.675 2.350 2.650\n##  [25] 2.575 2.450 2.600 2.600 2.550 2.425 2.425 2.675 2.725 2.825 2.425 2.400\n##  [37] 2.625 2.500 2.225 2.550 2.525 2.100 2.275 2.675 2.800 2.375 2.675 2.350\n##  [49] 2.675 2.475 4.075 3.900 4.100 3.275 3.850 3.575 3.975 2.900 3.850 3.300\n##  [61] 2.875 3.650 3.300 3.775 3.350 3.900 3.650 3.400 3.600 3.275 3.925 3.550\n##  [73] 3.800 3.700 3.725 3.850 3.950 4.100 3.725 3.200 3.200 3.150 3.400 3.850\n##  [85] 3.600 3.875 4.000 3.575 3.500 3.325 3.425 3.775 3.400 2.900 3.450 3.525\n##  [97] 3.525 3.675 2.925 3.475 4.525 3.875 4.525 4.150 4.375 4.825 3.400 4.575\n## [109] 4.200 4.850 4.200 4.075 4.350 3.800 4.025 4.300 4.200 5.100 4.875 3.675\n## [121] 4.525 3.825 4.800 3.925 4.450 4.550 3.900 3.950 4.225 4.400 4.550 5.025\n## [133] 4.250 3.925 3.925 4.775 4.425 4.200 3.900 4.375 4.450 4.350 3.875 4.550\n## [145] 4.550 4.300 3.925 4.175 4.325 3.950\napply(iris[, 1:4], 2, mean) # col ë°©í–¥ìœ¼ë¡œ í•¨ìˆ˜ ì ìš©\n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n##     5.843333     3.057333     3.758000     1.199333\n\nresult <- c()\nfor (i in 1:4) {\n    iris_col <- iris[, i]\n    iris_col_mean_temp <- mean(iris_col)\n    result <- c(result, iris_col_mean_temp)\n}\nresult\n## [1] 5.843333 3.057333 3.758000 1.199333\n\n\nmymax <- function(x, y) {\n    num.max <- x\n    if (y > x) {\n        num.max <- y\n    }\n    return(num.max)\n}\n\n\nmymax(10, 15)\n## [1] 15\na <- mymax(20, 15)\nb <- mymax(31, 45)\nprint(a + b)\n## [1] 65\n\n\nmydiv <- function(x, y = 2) {\n    result <- x / y\n    return(result)\n}\n\nmydiv(x = 10, y = 3) # ë§¤ê°œë³€ìˆ˜ ì´ë¦„ê³¼ ë§¤ê°œë³€ìˆ˜ê°’ì„ ìŒìœ¼ë¡œ ì…ë ¥\n## [1] 3.333333\nmydiv(10, 3) # ë§¤ê°œë³€ìˆ˜ê°’ë§Œ ì…ë ¥\n## [1] 3.333333\nmydiv(10) # xì— ëŒ€í•œ ê°’ë§Œ ì…ë ¥(y ê°’ì´ ìƒëµë¨)\n## [1] 5\n\n\nmyfunc <- function(x, y) {\n    val.sum <- x + y\n    val.mul <- x * y\n    return(list(sum = val.sum, mul = val.mul))\n}\n\nresult <- myfunc(5, 8)\nresult\n## $sum\n## [1] 13\n## \n## $mul\n## [1] 40\ns <- result$sum # 5, 8ì˜ í•©\nm <- result$mul # 5, 8ì˜ ê³±\ncat('5+8=', s, '\\n')\n## 5+8= 13\ncat('5*8=', m, '\\n')\n## 5*8= 40\n\n\ngetwd()\n## [1] \"C:/Users/Hyunsoo Kim/Desktop/senior_grade/blog/my-quarto-website\"\n# source(\"myfunc.R\") # myfunc.R ì•ˆì— ìˆëŠ” í•¨ìˆ˜ ì‹¤í–‰\n\na <- mydiv(20, 4) # í•¨ìˆ˜ í˜¸ì¶œ\nb <- mydiv(30, 4) # í•¨ìˆ˜ í˜¸ì¶œ\na + b\n## [1] 12.5\nmydiv(mydiv(20, 2), 5) # í•¨ìˆ˜ í˜¸ì¶œ\n## [1] 2\n\n\nscore <- c(76, 84, 69, 50, 95, 60, 82, 71, 88, 84)\nwhich(score == 69) # ì„±ì ì´ 69ì¸ í•™ìƒì€ ëª‡ ë²ˆì§¸ì— ìˆë‚˜\n## [1] 3\nwhich(score >= 85) # ì„±ì ì´ 85 ì´ìƒì¸ í•™ìƒì€ ëª‡ ë²ˆì§¸ì— ìˆë‚˜\n## [1] 5 9\n\nmax(score) # ìµœê³  ì ìˆ˜ëŠ” ëª‡ ì ì¸ê°€\n## [1] 95\nwhich.max(score) # ìµœê³  ì ìˆ˜ëŠ” ëª‡ ë²ˆì§¸ì— ìˆë‚˜\n## [1] 5\nscore[which.max(score)] # ìµœê³  ì ìˆ˜ëŠ” ëª‡ ì ì¸ê°€\n## [1] 95\n\nmin(score) # ìµœì € ì ìˆ˜ëŠ” ëª‡ ì ì¸ê°€\n## [1] 50\nwhich.min(score) # ìµœì € ì ìˆ˜ëŠ” ëª‡ ë²ˆì§¸ì— ìˆë‚˜\n## [1] 4\nscore[which.min(score)] # ìµœì € ì ìˆ˜ëŠ” ëª‡ ì ì¸ê°€\n## [1] 50\n\n\nscore <- c(76, 84, 69, 50, 95, 60, 82, 71, 88, 84)\nidx <- which(score <= 60) # ì„±ì ì´ 60 ì´í•˜ì¸ ê°’ë“¤ì˜ ì¸ë±ìŠ¤\nidx\n## [1] 4 6\nscore[idx]\n## [1] 50 60\nscore[idx] <- 61 # ì„±ì ì´ 60 ì´í•˜ì¸ ê°’ë“¤ì€ 61ì ìœ¼ë¡œ ì„±ì  ìƒí–¥ ì¡°ì •\nscore # ìƒí–¥ ì¡°ì •ëœ ì„±ì  í™•ì¸\n##  [1] 76 84 69 61 95 61 82 71 88 84\n\nidx <- which(score >= 80) # ì„±ì ì´ 80 ì´ìƒì¸ ê°’ë“¤ì˜ ì¸ë±ìŠ¤\nidx\n## [1]  2  5  7  9 10\nscore[idx]\n## [1] 84 95 82 88 84\nscore.high <- score[idx] # ì„±ì ì´ 80 ì´ìƒì¸ ê°’ë“¤ë§Œ ì¶”ì¶œí•˜ì—¬ ì €ì¥\nscore.high # score.highì˜ ë‚´ìš© í™•ì¸\n## [1] 84 95 82 88 84\n\n\niris\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 1            5.1         3.5          1.4         0.2     setosa\n## 2            4.9         3.0          1.4         0.2     setosa\n## 3            4.7         3.2          1.3         0.2     setosa\n## 4            4.6         3.1          1.5         0.2     setosa\n## 5            5.0         3.6          1.4         0.2     setosa\n## 6            5.4         3.9          1.7         0.4     setosa\n## 7            4.6         3.4          1.4         0.3     setosa\n## 8            5.0         3.4          1.5         0.2     setosa\n## 9            4.4         2.9          1.4         0.2     setosa\n## 10           4.9         3.1          1.5         0.1     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 12           4.8         3.4          1.6         0.2     setosa\n## 13           4.8         3.0          1.4         0.1     setosa\n## 14           4.3         3.0          1.1         0.1     setosa\n## 15           5.8         4.0          1.2         0.2     setosa\n## 16           5.7         4.4          1.5         0.4     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 23           4.6         3.6          1.0         0.2     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 25           4.8         3.4          1.9         0.2     setosa\n## 26           5.0         3.0          1.6         0.2     setosa\n## 27           5.0         3.4          1.6         0.4     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 30           4.7         3.2          1.6         0.2     setosa\n## 31           4.8         3.1          1.6         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 34           5.5         4.2          1.4         0.2     setosa\n## 35           4.9         3.1          1.5         0.2     setosa\n## 36           5.0         3.2          1.2         0.2     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 38           4.9         3.6          1.4         0.1     setosa\n## 39           4.4         3.0          1.3         0.2     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 41           5.0         3.5          1.3         0.3     setosa\n## 42           4.5         2.3          1.3         0.3     setosa\n## 43           4.4         3.2          1.3         0.2     setosa\n## 44           5.0         3.5          1.6         0.6     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\n## 46           4.8         3.0          1.4         0.3     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 48           4.6         3.2          1.4         0.2     setosa\n## 49           5.3         3.7          1.5         0.2     setosa\n## 50           5.0         3.3          1.4         0.2     setosa\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 107          4.9         2.5          4.5         1.7  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 114          5.7         2.5          5.0         2.0  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 120          6.0         2.2          5.0         1.5  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 122          5.6         2.8          4.9         2.0  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 127          6.2         2.8          4.8         1.8  virginica\n## 128          6.1         3.0          4.9         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\niris$Petal.Length\n##   [1] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 1.5 1.6 1.4 1.1 1.2 1.5 1.3 1.4\n##  [19] 1.7 1.5 1.7 1.5 1.0 1.7 1.9 1.6 1.6 1.5 1.4 1.6 1.6 1.5 1.5 1.4 1.5 1.2\n##  [37] 1.3 1.4 1.3 1.5 1.3 1.3 1.3 1.6 1.9 1.4 1.6 1.4 1.5 1.4 4.7 4.5 4.9 4.0\n##  [55] 4.6 4.5 4.7 3.3 4.6 3.9 3.5 4.2 4.0 4.7 3.6 4.4 4.5 4.1 4.5 3.9 4.8 4.0\n##  [73] 4.9 4.7 4.3 4.4 4.8 5.0 4.5 3.5 3.8 3.7 3.9 5.1 4.5 4.5 4.7 4.4 4.1 4.0\n##  [91] 4.4 4.6 4.0 3.3 4.2 4.2 4.2 4.3 3.0 4.1 6.0 5.1 5.9 5.6 5.8 6.6 4.5 6.3\n## [109] 5.8 6.1 5.1 5.3 5.5 5.0 5.1 5.3 5.5 6.7 6.9 5.0 5.7 4.9 6.7 4.9 5.7 6.0\n## [127] 4.8 4.9 5.6 5.8 6.1 6.4 5.6 5.1 5.6 6.1 5.6 5.5 4.8 5.4 5.6 5.1 5.1 5.9\n## [145] 5.7 5.2 5.0 5.2 5.4 5.1\niris$Petal.Length > 5.0\n##   [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n##  [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [97] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n## [109]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n## [121]  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n## [133]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n## [145]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\nwhich(iris$Petal.Length > 5.0)\n##  [1]  84 101 102 103 104 105 106 108 109 110 111 112 113 115 116 117 118 119 121\n## [20] 123 125 126 129 130 131 132 133 134 135 136 137 138 140 141 142 143 144 145\n## [39] 146 148 149 150\n\niris$Petal.Length[iris$Petal.Length > 5.0]\n##  [1] 5.1 6.0 5.1 5.9 5.6 5.8 6.6 6.3 5.8 6.1 5.1 5.3 5.5 5.1 5.3 5.5 6.7 6.9 5.7\n## [20] 6.7 5.7 6.0 5.6 5.8 6.1 6.4 5.6 5.1 5.6 6.1 5.6 5.5 5.4 5.6 5.1 5.1 5.9 5.7\n## [39] 5.2 5.2 5.4 5.1\n\nidx <- which(iris$Petal.Length > 5.0) # ê½ƒìì˜ ê¸¸ì´ê°€ 5.0 ì´ìƒì¸ ê°’ë“¤ì˜ ì¸ë±ìŠ¤\nidx\n##  [1]  84 101 102 103 104 105 106 108 109 110 111 112 113 115 116 117 118 119 121\n## [20] 123 125 126 129 130 131 132 133 134 135 136 137 138 140 141 142 143 144 145\n## [39] 146 148 149 150\niris.big <- iris[idx, ] # ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ê°’ë§Œ ì¶”ì¶œí•˜ì—¬ ì €ì¥\niris.big\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\n\n\n# 1~4ì—´ì˜ ê°’ ì¤‘ 5ë³´ë‹¤ í° ê°’ì˜ í–‰ê³¼ ì—´ì˜ ìœ„ì¹˜\nwhich(iris[, 1:4] > 5.0)\n##   [1]   1   6  11  15  16  17  18  19  20  21  22  24  28  29  32  33  34  37\n##  [19]  40  45  47  49  51  52  53  54  55  56  57  59  60  62  63  64  65  66\n##  [37]  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84\n##  [55]  85  86  87  88  89  90  91  92  93  95  96  97  98  99 100 101 102 103\n##  [73] 104 105 106 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122\n##  [91] 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140\n## [109] 141 142 143 144 145 146 147 148 149 150 384 401 402 403 404 405 406 408\n## [127] 409 410 411 412 413 415 416 417 418 419 421 423 425 426 429 430 431 432\n## [145] 433 434 435 436 437 438 440 441 442 443 444 445 446 448 449 450\nwhich(iris[, 1:4] > 5.0, arr.ind = TRUE) # arr.ind = TRUE : ì¡°ê±´ì— ë§ëŠ” ì¸ë±ìŠ¤ê¹Œì§€ ë°˜í™˜\n##        row col\n##   [1,]   1   1\n##   [2,]   6   1\n##   [3,]  11   1\n##   [4,]  15   1\n##   [5,]  16   1\n##   [6,]  17   1\n##   [7,]  18   1\n##   [8,]  19   1\n##   [9,]  20   1\n##  [10,]  21   1\n##  [11,]  22   1\n##  [12,]  24   1\n##  [13,]  28   1\n##  [14,]  29   1\n##  [15,]  32   1\n##  [16,]  33   1\n##  [17,]  34   1\n##  [18,]  37   1\n##  [19,]  40   1\n##  [20,]  45   1\n##  [21,]  47   1\n##  [22,]  49   1\n##  [23,]  51   1\n##  [24,]  52   1\n##  [25,]  53   1\n##  [26,]  54   1\n##  [27,]  55   1\n##  [28,]  56   1\n##  [29,]  57   1\n##  [30,]  59   1\n##  [31,]  60   1\n##  [32,]  62   1\n##  [33,]  63   1\n##  [34,]  64   1\n##  [35,]  65   1\n##  [36,]  66   1\n##  [37,]  67   1\n##  [38,]  68   1\n##  [39,]  69   1\n##  [40,]  70   1\n##  [41,]  71   1\n##  [42,]  72   1\n##  [43,]  73   1\n##  [44,]  74   1\n##  [45,]  75   1\n##  [46,]  76   1\n##  [47,]  77   1\n##  [48,]  78   1\n##  [49,]  79   1\n##  [50,]  80   1\n##  [51,]  81   1\n##  [52,]  82   1\n##  [53,]  83   1\n##  [54,]  84   1\n##  [55,]  85   1\n##  [56,]  86   1\n##  [57,]  87   1\n##  [58,]  88   1\n##  [59,]  89   1\n##  [60,]  90   1\n##  [61,]  91   1\n##  [62,]  92   1\n##  [63,]  93   1\n##  [64,]  95   1\n##  [65,]  96   1\n##  [66,]  97   1\n##  [67,]  98   1\n##  [68,]  99   1\n##  [69,] 100   1\n##  [70,] 101   1\n##  [71,] 102   1\n##  [72,] 103   1\n##  [73,] 104   1\n##  [74,] 105   1\n##  [75,] 106   1\n##  [76,] 108   1\n##  [77,] 109   1\n##  [78,] 110   1\n##  [79,] 111   1\n##  [80,] 112   1\n##  [81,] 113   1\n##  [82,] 114   1\n##  [83,] 115   1\n##  [84,] 116   1\n##  [85,] 117   1\n##  [86,] 118   1\n##  [87,] 119   1\n##  [88,] 120   1\n##  [89,] 121   1\n##  [90,] 122   1\n##  [91,] 123   1\n##  [92,] 124   1\n##  [93,] 125   1\n##  [94,] 126   1\n##  [95,] 127   1\n##  [96,] 128   1\n##  [97,] 129   1\n##  [98,] 130   1\n##  [99,] 131   1\n## [100,] 132   1\n## [101,] 133   1\n## [102,] 134   1\n## [103,] 135   1\n## [104,] 136   1\n## [105,] 137   1\n## [106,] 138   1\n## [107,] 139   1\n## [108,] 140   1\n## [109,] 141   1\n## [110,] 142   1\n## [111,] 143   1\n## [112,] 144   1\n## [113,] 145   1\n## [114,] 146   1\n## [115,] 147   1\n## [116,] 148   1\n## [117,] 149   1\n## [118,] 150   1\n## [119,]  84   3\n## [120,] 101   3\n## [121,] 102   3\n## [122,] 103   3\n## [123,] 104   3\n## [124,] 105   3\n## [125,] 106   3\n## [126,] 108   3\n## [127,] 109   3\n## [128,] 110   3\n## [129,] 111   3\n## [130,] 112   3\n## [131,] 113   3\n## [132,] 115   3\n## [133,] 116   3\n## [134,] 117   3\n## [135,] 118   3\n## [136,] 119   3\n## [137,] 121   3\n## [138,] 123   3\n## [139,] 125   3\n## [140,] 126   3\n## [141,] 129   3\n## [142,] 130   3\n## [143,] 131   3\n## [144,] 132   3\n## [145,] 133   3\n## [146,] 134   3\n## [147,] 135   3\n## [148,] 136   3\n## [149,] 137   3\n## [150,] 138   3\n## [151,] 140   3\n## [152,] 141   3\n## [153,] 142   3\n## [154,] 143   3\n## [155,] 144   3\n## [156,] 145   3\n## [157,] 146   3\n## [158,] 148   3\n## [159,] 149   3\n## [160,] 150   3\n\nidx <- which(iris[, 1:4] > 5.0, arr.ind = TRUE)\niris[idx[, 1], ]\n##       Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 1              5.1         3.5          1.4         0.2     setosa\n## 6              5.4         3.9          1.7         0.4     setosa\n## 11             5.4         3.7          1.5         0.2     setosa\n## 15             5.8         4.0          1.2         0.2     setosa\n## 16             5.7         4.4          1.5         0.4     setosa\n## 17             5.4         3.9          1.3         0.4     setosa\n## 18             5.1         3.5          1.4         0.3     setosa\n## 19             5.7         3.8          1.7         0.3     setosa\n## 20             5.1         3.8          1.5         0.3     setosa\n## 21             5.4         3.4          1.7         0.2     setosa\n## 22             5.1         3.7          1.5         0.4     setosa\n## 24             5.1         3.3          1.7         0.5     setosa\n## 28             5.2         3.5          1.5         0.2     setosa\n## 29             5.2         3.4          1.4         0.2     setosa\n## 32             5.4         3.4          1.5         0.4     setosa\n## 33             5.2         4.1          1.5         0.1     setosa\n## 34             5.5         4.2          1.4         0.2     setosa\n## 37             5.5         3.5          1.3         0.2     setosa\n## 40             5.1         3.4          1.5         0.2     setosa\n## 45             5.1         3.8          1.9         0.4     setosa\n## 47             5.1         3.8          1.6         0.2     setosa\n## 49             5.3         3.7          1.5         0.2     setosa\n## 51             7.0         3.2          4.7         1.4 versicolor\n## 52             6.4         3.2          4.5         1.5 versicolor\n## 53             6.9         3.1          4.9         1.5 versicolor\n## 54             5.5         2.3          4.0         1.3 versicolor\n## 55             6.5         2.8          4.6         1.5 versicolor\n## 56             5.7         2.8          4.5         1.3 versicolor\n## 57             6.3         3.3          4.7         1.6 versicolor\n## 59             6.6         2.9          4.6         1.3 versicolor\n## 60             5.2         2.7          3.9         1.4 versicolor\n## 62             5.9         3.0          4.2         1.5 versicolor\n## 63             6.0         2.2          4.0         1.0 versicolor\n## 64             6.1         2.9          4.7         1.4 versicolor\n## 65             5.6         2.9          3.6         1.3 versicolor\n## 66             6.7         3.1          4.4         1.4 versicolor\n## 67             5.6         3.0          4.5         1.5 versicolor\n## 68             5.8         2.7          4.1         1.0 versicolor\n## 69             6.2         2.2          4.5         1.5 versicolor\n## 70             5.6         2.5          3.9         1.1 versicolor\n## 71             5.9         3.2          4.8         1.8 versicolor\n## 72             6.1         2.8          4.0         1.3 versicolor\n## 73             6.3         2.5          4.9         1.5 versicolor\n## 74             6.1         2.8          4.7         1.2 versicolor\n## 75             6.4         2.9          4.3         1.3 versicolor\n## 76             6.6         3.0          4.4         1.4 versicolor\n## 77             6.8         2.8          4.8         1.4 versicolor\n## 78             6.7         3.0          5.0         1.7 versicolor\n## 79             6.0         2.9          4.5         1.5 versicolor\n## 80             5.7         2.6          3.5         1.0 versicolor\n## 81             5.5         2.4          3.8         1.1 versicolor\n## 82             5.5         2.4          3.7         1.0 versicolor\n## 83             5.8         2.7          3.9         1.2 versicolor\n## 84             6.0         2.7          5.1         1.6 versicolor\n## 85             5.4         3.0          4.5         1.5 versicolor\n## 86             6.0         3.4          4.5         1.6 versicolor\n## 87             6.7         3.1          4.7         1.5 versicolor\n## 88             6.3         2.3          4.4         1.3 versicolor\n## 89             5.6         3.0          4.1         1.3 versicolor\n## 90             5.5         2.5          4.0         1.3 versicolor\n## 91             5.5         2.6          4.4         1.2 versicolor\n## 92             6.1         3.0          4.6         1.4 versicolor\n## 93             5.8         2.6          4.0         1.2 versicolor\n## 95             5.6         2.7          4.2         1.3 versicolor\n## 96             5.7         3.0          4.2         1.2 versicolor\n## 97             5.7         2.9          4.2         1.3 versicolor\n## 98             6.2         2.9          4.3         1.3 versicolor\n## 99             5.1         2.5          3.0         1.1 versicolor\n## 100            5.7         2.8          4.1         1.3 versicolor\n## 101            6.3         3.3          6.0         2.5  virginica\n## 102            5.8         2.7          5.1         1.9  virginica\n## 103            7.1         3.0          5.9         2.1  virginica\n## 104            6.3         2.9          5.6         1.8  virginica\n## 105            6.5         3.0          5.8         2.2  virginica\n## 106            7.6         3.0          6.6         2.1  virginica\n## 108            7.3         2.9          6.3         1.8  virginica\n## 109            6.7         2.5          5.8         1.8  virginica\n## 110            7.2         3.6          6.1         2.5  virginica\n## 111            6.5         3.2          5.1         2.0  virginica\n## 112            6.4         2.7          5.3         1.9  virginica\n## 113            6.8         3.0          5.5         2.1  virginica\n## 114            5.7         2.5          5.0         2.0  virginica\n## 115            5.8         2.8          5.1         2.4  virginica\n## 116            6.4         3.2          5.3         2.3  virginica\n## 117            6.5         3.0          5.5         1.8  virginica\n## 118            7.7         3.8          6.7         2.2  virginica\n## 119            7.7         2.6          6.9         2.3  virginica\n## 120            6.0         2.2          5.0         1.5  virginica\n## 121            6.9         3.2          5.7         2.3  virginica\n## 122            5.6         2.8          4.9         2.0  virginica\n## 123            7.7         2.8          6.7         2.0  virginica\n## 124            6.3         2.7          4.9         1.8  virginica\n## 125            6.7         3.3          5.7         2.1  virginica\n## 126            7.2         3.2          6.0         1.8  virginica\n## 127            6.2         2.8          4.8         1.8  virginica\n## 128            6.1         3.0          4.9         1.8  virginica\n## 129            6.4         2.8          5.6         2.1  virginica\n## 130            7.2         3.0          5.8         1.6  virginica\n## 131            7.4         2.8          6.1         1.9  virginica\n## 132            7.9         3.8          6.4         2.0  virginica\n## 133            6.4         2.8          5.6         2.2  virginica\n## 134            6.3         2.8          5.1         1.5  virginica\n## 135            6.1         2.6          5.6         1.4  virginica\n## 136            7.7         3.0          6.1         2.3  virginica\n## 137            6.3         3.4          5.6         2.4  virginica\n## 138            6.4         3.1          5.5         1.8  virginica\n## 139            6.0         3.0          4.8         1.8  virginica\n## 140            6.9         3.1          5.4         2.1  virginica\n## 141            6.7         3.1          5.6         2.4  virginica\n## 142            6.9         3.1          5.1         2.3  virginica\n## 143            5.8         2.7          5.1         1.9  virginica\n## 144            6.8         3.2          5.9         2.3  virginica\n## 145            6.7         3.3          5.7         2.5  virginica\n## 146            6.7         3.0          5.2         2.3  virginica\n## 147            6.3         2.5          5.0         1.9  virginica\n## 148            6.5         3.0          5.2         2.0  virginica\n## 149            6.2         3.4          5.4         2.3  virginica\n## 150            5.9         3.0          5.1         1.8  virginica\n## 84.1           6.0         2.7          5.1         1.6 versicolor\n## 101.1          6.3         3.3          6.0         2.5  virginica\n## 102.1          5.8         2.7          5.1         1.9  virginica\n## 103.1          7.1         3.0          5.9         2.1  virginica\n## 104.1          6.3         2.9          5.6         1.8  virginica\n## 105.1          6.5         3.0          5.8         2.2  virginica\n## 106.1          7.6         3.0          6.6         2.1  virginica\n## 108.1          7.3         2.9          6.3         1.8  virginica\n## 109.1          6.7         2.5          5.8         1.8  virginica\n## 110.1          7.2         3.6          6.1         2.5  virginica\n## 111.1          6.5         3.2          5.1         2.0  virginica\n## 112.1          6.4         2.7          5.3         1.9  virginica\n## 113.1          6.8         3.0          5.5         2.1  virginica\n## 115.1          5.8         2.8          5.1         2.4  virginica\n## 116.1          6.4         3.2          5.3         2.3  virginica\n## 117.1          6.5         3.0          5.5         1.8  virginica\n## 118.1          7.7         3.8          6.7         2.2  virginica\n## 119.1          7.7         2.6          6.9         2.3  virginica\n## 121.1          6.9         3.2          5.7         2.3  virginica\n## 123.1          7.7         2.8          6.7         2.0  virginica\n## 125.1          6.7         3.3          5.7         2.1  virginica\n## 126.1          7.2         3.2          6.0         1.8  virginica\n## 129.1          6.4         2.8          5.6         2.1  virginica\n## 130.1          7.2         3.0          5.8         1.6  virginica\n## 131.1          7.4         2.8          6.1         1.9  virginica\n## 132.1          7.9         3.8          6.4         2.0  virginica\n## 133.1          6.4         2.8          5.6         2.2  virginica\n## 134.1          6.3         2.8          5.1         1.5  virginica\n## 135.1          6.1         2.6          5.6         1.4  virginica\n## 136.1          7.7         3.0          6.1         2.3  virginica\n## 137.1          6.3         3.4          5.6         2.4  virginica\n## 138.1          6.4         3.1          5.5         1.8  virginica\n## 140.1          6.9         3.1          5.4         2.1  virginica\n## 141.1          6.7         3.1          5.6         2.4  virginica\n## 142.1          6.9         3.1          5.1         2.3  virginica\n## 143.1          5.8         2.7          5.1         1.9  virginica\n## 144.1          6.8         3.2          5.9         2.3  virginica\n## 145.1          6.7         3.3          5.7         2.5  virginica\n## 146.1          6.7         3.0          5.2         2.3  virginica\n## 148.1          6.5         3.0          5.2         2.0  virginica\n## 149.1          6.2         3.4          5.4         2.3  virginica\n## 150.1          5.9         3.0          5.1         1.8  virginica\n\niris[, 1:4][idx]\n##   [1] 5.1 5.4 5.4 5.8 5.7 5.4 5.1 5.7 5.1 5.4 5.1 5.1 5.2 5.2 5.4 5.2 5.5 5.5\n##  [19] 5.1 5.1 5.1 5.3 7.0 6.4 6.9 5.5 6.5 5.7 6.3 6.6 5.2 5.9 6.0 6.1 5.6 6.7\n##  [37] 5.6 5.8 6.2 5.6 5.9 6.1 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7 5.5 5.5 5.8 6.0\n##  [55] 5.4 6.0 6.7 6.3 5.6 5.5 5.5 6.1 5.8 5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 7.1\n##  [73] 6.3 6.5 7.6 7.3 6.7 7.2 6.5 6.4 6.8 5.7 5.8 6.4 6.5 7.7 7.7 6.0 6.9 5.6\n##  [91] 7.7 6.3 6.7 7.2 6.2 6.1 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 6.3 6.4 6.0 6.9\n## [109] 6.7 6.9 5.8 6.8 6.7 6.7 6.3 6.5 6.2 5.9 5.1 6.0 5.1 5.9 5.6 5.8 6.6 6.3\n## [127] 5.8 6.1 5.1 5.3 5.5 5.1 5.3 5.5 6.7 6.9 5.7 6.7 5.7 6.0 5.6 5.8 6.1 6.4\n## [145] 5.6 5.1 5.6 6.1 5.6 5.5 5.4 5.6 5.1 5.1 5.9 5.7 5.2 5.2 5.4 5.1\n\n\n\n\n\nfavorite <- c('WINTER', 'SUMMER', 'SPRING', 'SUMMER', 'SUMMER',\n              'FALL', 'FALL', 'SUMMER', 'SPRING', 'SPRING')\nfavorite # favoriteì˜ ë‚´ìš© ì¶œë ¥\n##  [1] \"WINTER\" \"SUMMER\" \"SPRING\" \"SUMMER\" \"SUMMER\" \"FALL\"   \"FALL\"   \"SUMMER\"\n##  [9] \"SPRING\" \"SPRING\"\ntable(favorite) # ë„ìˆ˜ë¶„í¬í‘œ ê³„ì‚°\n## favorite\n##   FALL SPRING SUMMER WINTER \n##      2      3      4      1\nlength(favorite)\n## [1] 10\ntable(favorite) / length(favorite) # ë¹„ìœ¨ ì¶œë ¥\n## favorite\n##   FALL SPRING SUMMER WINTER \n##    0.2    0.3    0.4    0.1\n\n\nds <- table(favorite)\nds\n## favorite\n##   FALL SPRING SUMMER WINTER \n##      2      3      4      1\nbarplot(ds, main = 'favorite season')\n\n\n\n\n\nds <- table(favorite)\nds\n## favorite\n##   FALL SPRING SUMMER WINTER \n##      2      3      4      1\npie(ds, main = 'favorite season')\n\n\n\n\n\nfavorite.color <- c(2, 3, 2, 1, 1, 2, 2, 1, 3, 2, 1, 3, 2, 1, 2)\nds <- table(favorite.color)\nds\n## favorite.color\n## 1 2 3 \n## 5 7 3\nbarplot(ds, main = 'favorite color')\n\n\n\ncolors <- c('green', 'red', 'blue')\nnames(ds) <- colors # ìë£Œê°’ 1, 2, 3ì„ green, red, blueë¡œ ë³€ê²½\nds\n## green   red  blue \n##     5     7     3\nbarplot(ds, main = 'favorite color', col = colors) # ìƒ‰ ì§€ì • ë§‰ëŒ€ê·¸ë˜í”„\n\n\n\nbarplot(ds, main = 'favorite color', col = c('green', 'red', 'blue'))\npie(ds, main = 'favorite color', col = colors) # ìƒ‰ ì§€ì • ì›ê·¸ë˜í”„\n\n\n\n\n\nweight <- c(60, 62, 64, 65, 68, 69)\nweight.heavy <- c(weight, 120)\nweight\n## [1] 60 62 64 65 68 69\nweight.heavy\n## [1]  60  62  64  65  68  69 120\n\nmean(weight) # í‰ê· \n## [1] 64.66667\nmean(weight.heavy) # í‰ê· \n## [1] 72.57143\n\nmedian(weight) # ì¤‘ì•™ê°’\n## [1] 64.5\nmedian(weight.heavy) # ì¤‘ì•™ê°’\n## [1] 65\n\nmean(weight, trim = 0.2) # ì ˆì‚¬í‰ê· (ìƒí•˜ìœ„ 20% ì œì™¸)\n## [1] 64.75\nmean(weight.heavy, trim = 0.2) # ì ˆì‚¬í‰ê· (ìƒí•˜ìœ„ 20% ì œì™¸)\n## [1] 65.6\n\n\nmydata <- c(60, 62, 64, 65, 68, 69, 120)\nquantile(mydata)\n##    0%   25%   50%   75%  100% \n##  60.0  63.0  65.0  68.5 120.0\nquantile(mydata, (0:10) / 10) # 10% ë‹¨ìœ„ë¡œ êµ¬ê°„ì„ ë‚˜ëˆ„ì–´ ê³„ì‚°\n##    0%   10%   20%   30%   40%   50%   60%   70%   80%   90%  100% \n##  60.0  61.2  62.4  63.6  64.4  65.0  66.8  68.2  68.8  89.4 120.0\nsummary(mydata) # ìµœì†Œê°’, ì¤‘ì•™ê°’, í‰ê· ê°’, 3ë¶„ìœ„ ê°’, ìµœëŒ€ê°’\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   60.00   63.00   65.00   72.57   68.50  120.00\n\nmydata <- 0:1000\nquantile(mydata)\n##   0%  25%  50%  75% 100% \n##    0  250  500  750 1000\nquantile(mydata, (0:10) / 10)\n##   0%  10%  20%  30%  40%  50%  60%  70%  80%  90% 100% \n##    0  100  200  300  400  500  600  700  800  900 1000\nsummary(mydata)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##       0     250     500     500     750    1000\n?quantile\n## starting httpd help server ... done\n\n\nmydata <- c(60, 62, 64, 65, 68, 69, 120)\nvar(mydata) # ë¶„ì‚°\n## [1] 447.2857\nsd(mydata) # í‘œì¤€í¸ì°¨\n## [1] 21.14913\nrange(mydata) # ê°’ì˜ ë²”ìœ„\n## [1]  60 120\ndiff(range(mydata)) # ìµœëŒ€ê°’, ìµœì†Œê°’ì˜ ì°¨ì´\n## [1] 60\n\n\ndist <- cars[, 2] # ìë™ì°¨ ì œë™ê±°ë¦¬\nhist(dist,                            # ìë£Œ(data)\n     main = \"Histogram for ì œë™ê±°ë¦¬\", # ì œëª©\n     xlab = \"ì œë™ê±°ë¦¬\",               # xì¶• ë ˆì´ë¸”\n     ylab = \"ë¹ˆë„ìˆ˜\",                 # yì¶• ë ˆì´ë¸”\n     border = \"blue\",                 # ë§‰ëŒ€ í…Œë‘ë¦¬ìƒ‰\n     col = rainbow(10),               # ë§‰ëŒ€ ìƒ‰\n     las = 2,                         # xì¶• ê¸€ì”¨ ë°©í–¥(0~3)\n     breaks = seq(0, 120, 10))        # ë§‰ëŒ€ ê°œìˆ˜ ì¡°ì ˆ\n\n\n\n\n\ndist <- cars[,2] # ìë™ì°¨ ì œë™ê±°ë¦¬(ë‹¨ìœ„: í”¼íŠ¸(ft))\nboxplot(dist, main = \"ìë™ì°¨ ì œë™ê±°ë¦¬\") # â˜…â˜…â˜…â˜…â˜…\n\n\n\n\n\nboxplot.stats(dist)\n## $stats\n## [1]  2 26 36 56 93\n## \n## $n\n## [1] 50\n## \n## $conf\n## [1] 29.29663 42.70337\n## \n## $out\n## [1] 120\nboxplot.stats(dist)$stats\n## [1]  2 26 36 56 93\nboxplot.stats(dist)$stats[4]\n## [1] 56\n\n\nboxplot(Petal.Length ~ Species, data = iris, main = \"í’ˆì¢…ë³„ ê½ƒìì˜ ê¸¸ì´\")\n\n\n\n\npar(mfrow = c(1, 3)) # 1*3 ê°€ìƒí™”ë©´ ë¶„í• \n\nbarplot(\n    table(mtcars$carb),\n    main = \"Barplot of Carburetors\",\n    xlab = \"#of carburetors\",\n    ylab = \"frequency\",\n    col = \"blue\"\n)\n\nbarplot(\n    table(mtcars$cyl),\n    main = \"Barplot of Cylender\",\n    xlab = \"#of cylender\",\n    ylab = \"frequency\",\n    col = \"red\"\n)\n\nbarplot(\n    table(mtcars$gear),\n    main = \"Barplot of Grar\",\n    xlab = \"#of gears\",\n    ylab = \"frequency\",\n    col = \"green\"\n)\n\n\n\n\npar(mfrow = c(1, 1)) # ê°€ìƒí™”ë©´ ë¶„í•  í•´ì œ\n\n\n\n\n\nwt <- mtcars$wt                 # ì¤‘ëŸ‰ ìë£Œ\nmpg <- mtcars$mpg               # ì—°ë¹„ ìë£Œ\nplot(wt, mpg,                   # 2ê°œ ë³€ìˆ˜(xì¶•, yì¶•)\n     main = \"ì¤‘ëŸ‰-ì—°ë¹„ ê·¸ë˜í”„\", # ì œëª©\n     xlab = \"ì¤‘ëŸ‰\",             # xì¶• ë ˆì´ë¸”\n     ylab = \"ì—°ë¹„(MPG)\",        # yì¶• ë ˆì´ë¸”\n     col = \"red\",               # pointì˜ color\n     pch = 11)                  # pointì˜ ì¢…ë¥˜\n\n\n\n\n\nvars <- c(\"mpg\", \"disp\", \"drat\", \"wt\") # ëŒ€ìƒ ë³€ìˆ˜(ì—°ë¹„, ë°°ê¸°ëŸ‰, í›„ë°©ì°¨ì¸¡ ë¹„ìœ¨, ì¤‘ëŸ‰)\ntarget <- mtcars[, vars]\nhead(target)\n##                    mpg disp drat    wt\n## Mazda RX4         21.0  160 3.90 2.620\n## Mazda RX4 Wag     21.0  160 3.90 2.875\n## Datsun 710        22.8  108 3.85 2.320\n## Hornet 4 Drive    21.4  258 3.08 3.215\n## Hornet Sportabout 18.7  360 3.15 3.440\n## Valiant           18.1  225 2.76 3.460\npairs(target, main = \"Multi Plots\")    # ëŒ€ìƒ ë°ì´í„°\n\n\n\n\n\niris.2 <- iris[, 3:4]              # ë°ì´í„° ì¤€ë¹„\npoint <- as.numeric(iris$Species)  # ì ì˜ ëª¨ì–‘\npoint                              # point ë‚´ìš© ì¶œë ¥\n##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n##  [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n##  [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3\n## [112] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n## [149] 3 3\ncolor <- c(\"red\", \"green\", \"blue\") # ì ì˜ ì»¬ëŸ¬\nplot(iris.2,\n     main = \"Iris plot\",\n     pch = c(point),\n     col = color[point])\n\n\n\n\n\nbeers = c(5, 2, 9, 8, 3, 7, 3, 5, 3, 5) # ìë£Œ ì…ë ¥\nbal <- c(0.1, 0.03, 0.19, 0.12, 0.04, 0.0095, 0.07, 0.06, 0.02, 0.05)\ntbl <- data.frame(beers, bal)           # ë°ì´í„°í”„ë ˆì„ ìƒì„±\ntbl\n##    beers    bal\n## 1      5 0.1000\n## 2      2 0.0300\n## 3      9 0.1900\n## 4      8 0.1200\n## 5      3 0.0400\n## 6      7 0.0095\n## 7      3 0.0700\n## 8      5 0.0600\n## 9      3 0.0200\n## 10     5 0.0500\nplot(bal ~ beers, data = tbl)           # ì‚°ì ë„ plot(beers, bal)\nres <- lm(bal ~ beers, data = tbl)      # íšŒê·€ì‹ ë„ì¶œ\nabline(res)                             # íšŒê·€ì„  ê·¸ë¦¬ê¸°\n\n\n\ncor(beers, bal)                         # ìƒê´€ê³„ìˆ˜ ê³„ì‚°\n## [1] 0.6797025\n\n\ncor(iris[, 1:4]) # 4ê°œ ë³€ìˆ˜ ê°„ ìƒê´€ì„± ë¶„ì„\n##              Sepal.Length Sepal.Width Petal.Length Petal.Width\n## Sepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411\n## Sepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259\n## Petal.Length    0.8717538  -0.4284401    1.0000000   0.9628654\n## Petal.Width     0.8179411  -0.3661259    0.9628654   1.0000000\n\n\nmonth = 1:12 # ìë£Œ ì…ë ¥\nlate = c(5, 8, 7, 9, 4, 6, 12, 13, 8, 6, 6, 4) # ìë£Œ ì…ë ¥\nplot(month,                # x data\n     late,                 # y data\n     main = \"ì§€ê°ìƒ í†µê³„\", # ì œëª©\n     type = \"l\",           # ê·¸ë˜í”„ì˜ ì¢…ë¥˜ ì„ íƒ(ì•ŒíŒŒë²³)\n     lty = 1,              # ì„ ì˜ ì¢…ë¥˜(line type) ì„ íƒ\n     lwd = 1,              # ì„ ì˜ êµµê¸° ì„ íƒ\n     xlab = \"Month\",       # xì¶• ë ˆì´ë¸”\n     ylab = \"Late cnt\")    # yì¶• ë ˆì´ë¸”\n\n\n\n\n\nmonth = 1:12\nlate1 = c(5, 8, 7, 9, 4, 6, 12, 13, 8, 6, 6, 4)\nlate2 = c(4, 6, 5, 8, 7, 8, 10, 11, 6, 5, 7, 3)\nplot(month,                  # x data\n     late1,                  # y data\n     main = \"Late Students\",\n     type = \"b\",             # ê·¸ë˜í”„ì˜ ì¢…ë¥˜ ì„ íƒ(ì•ŒíŒŒë²³)\n     lty = 1,                # ì„ ì˜ ì¢…ë¥˜(line type) ì„ íƒ\n     col = \"red\",            # ì„ ì˜ ìƒ‰ ì„ íƒ\n     xlab = \"Month\",         # xì¶• ë ˆì´ë¸”\n     ylab = \"Late cnt\",      # yì¶• ë ˆì´ë¸”\n     ylim = c(1, 15))        # yì¶• ê°’ì˜ (í•˜í•œ, ìƒí•œ)\n\nlines(month,                 # x data\n      late2,                 # y data\n      type = \"b\",            # ì„ ì˜ ì¢…ë¥˜(line type) ì„ íƒ\n      col = \"blue\")          # ì„ ì˜ ìƒ‰ ì„ íƒ\n\n\n\n\n\n## (1) ë¶„ì„ ëŒ€ìƒ ë°ì´í„°ì…‹ ì¤€ë¹„\n# install.packages(\"mlbench\")\nlibrary(mlbench)\ndata(\"BostonHousing\")\nmyds <- BostonHousing[, c(\"crim\", \"rm\", \"dis\", \"tax\", \"medv\")]\n\n## (2) grp ë³€ìˆ˜ ì¶”ê°€ â˜…â˜…â˜…â˜…â˜…\ngrp <- c()\nfor (i in 1:nrow(myds)) {\n    # myds$medv ê°’ì— ë”°ë¼ ê·¸ë£¹ ë¶„ë¥˜\n    if (myds$medv[i] >= 25.0) {\n        grp[i] <- \"H\"\n    } else if (myds$medv[i] <= 17.0) {\n        grp[i] <- \"L\"\n    } else {\n        grp[i] <- \"M\"\n    }\n}\ngrp <- factor(grp) # ë¬¸ì ë²¡í„°ë¥¼ íŒ©í„° íƒ€ì…ìœ¼ë¡œ ë³€ê²½\ngrp <- factor(grp, levels = c(\"H\", \"M\", \"L\")) # ë ˆë²¨ì˜ ìˆœì„œë¥¼ H, L, M -> H, M, L\n\nmyds <- data.frame(myds, grp) # mydsì— grp ì—´ ì¶”ê°€\n\n## (3) ë°ì´í„°ì…‹ì˜ í˜•íƒœì™€ ê¸°ë³¸ì ì¸ ë‚´ìš© íŒŒì•…\nstr(myds)\n## 'data.frame':    506 obs. of  6 variables:\n##  $ crim: num  0.00632 0.02731 0.02729 0.03237 0.06905 ...\n##  $ rm  : num  6.58 6.42 7.18 7 7.15 ...\n##  $ dis : num  4.09 4.97 4.97 6.06 6.06 ...\n##  $ tax : num  296 242 242 222 222 222 311 311 311 311 ...\n##  $ medv: num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...\n##  $ grp : Factor w/ 3 levels \"H\",\"M\",\"L\": 2 2 1 1 1 1 2 1 3 2 ...\nhead(myds)\n##      crim    rm    dis tax medv grp\n## 1 0.00632 6.575 4.0900 296 24.0   M\n## 2 0.02731 6.421 4.9671 242 21.6   M\n## 3 0.02729 7.185 4.9671 242 34.7   H\n## 4 0.03237 6.998 6.0622 222 33.4   H\n## 5 0.06905 7.147 6.0622 222 36.2   H\n## 6 0.02985 6.430 6.0622 222 28.7   H\ntable(myds$grp) # ì£¼íƒ ê°€ê²© ê·¸ë£¹ë³„ ë¶„í¬\n## \n##   H   M   L \n## 132 247 127\n\n## (4) íˆìŠ¤í† ê·¸ë¨ì— ì˜í•œ ê´€ì¸¡ê°’ì˜ ë¶„í¬ í™•ì¸\npar(mfrow = c(2, 3)) # 2*3 ê°€ìƒí™”ë©´ ë¶„í• \nfor (i in 1:5) {\n    hist(myds[, i], main = colnames(myds)[i], col = \"yellow\")\n}\npar(mfrow = c(1, 1)) # 2*3 ê°€ìƒí™”ë©´ ë¶„í•  í•´ì œ\n\n\n\n\n## (5) ìƒìê·¸ë¦¼ì— ì˜í•œ ê´€ì¸¡ê°’ì˜ ë¶„í¬ í™•ì¸\npar(mfrow = c(2, 3)) # 2*3 ê°€ìƒí™”ë©´ ë¶„í• \nfor (i in 1:5) {\n    boxplot(myds[, i], main = colnames(myds)[i])\n}\npar(mfrow = c(1, 1)) # 2*3 ê°€ìƒí™”ë©´ ë¶„í•  í•´ì œ\n\n\n\n\n## (6) ê·¸ë£¹ë³„ ê´€ì¸¡ê°’ ë¶„í¬ì˜ í™•ì¸\nboxplot(myds$crim ~ myds$grp, main = \"1ì¸ë‹¹ ë²”ì£„ìœ¨\")\n\n\n\nboxplot(myds$rm ~ myds$grp, main = \"ë°©ì˜ ê°œìˆ˜\")\n\n\n\nboxplot(myds$dis ~ myds$grp, main = \"ì§ì—… ì„¼í„°ê¹Œì§€ì˜ ê±°ë¦¬\")\n\n\n\nboxplot(myds$tax ~ myds$grp, main = \"ì¬ì‚°ì„¸ìœ¨\")\n\n\n\n\n## (7) ë‹¤ì¤‘ ì‚°ì ë„ë¥¼ í†µí•œ ë³€ìˆ˜ ê°„ ìƒê´€ ê´€ê³„ì˜ í™•ì¸\npairs(myds[, -6]) # 6ë²ˆì§¸ ì—´ ì œê±°(grp)\npairs(myds[, 1:5])\n\n\n\n\n## (8) ê·¸ë£¹ ì •ë³´ë¥¼ í¬í•¨í•œ ë³€ìˆ˜ ê°„ ìƒê´€ ê´€ê³„ì˜ í™•ì¸\npoint <- as.integer(myds$grp) # ì ì˜ ëª¨ì–‘ ì§€ì •\ncolor <- c(\"red\", \"green\", \"blue\") # ì ì˜ ìƒ‰ ì§€ì •\npairs(myds[, -6], pch = point, col = color[point])\n\n\n\n\n## (9) ë³€ìˆ˜ ê°„ ìƒê´€ê³„ìˆ˜ì˜ í™•ì¸\ncor(myds[, -6])\n##            crim         rm        dis        tax       medv\n## crim  1.0000000 -0.2192467 -0.3796701  0.5827643 -0.3883046\n## rm   -0.2192467  1.0000000  0.2052462 -0.2920478  0.6953599\n## dis  -0.3796701  0.2052462  1.0000000 -0.5344316  0.2499287\n## tax   0.5827643 -0.2920478 -0.5344316  1.0000000 -0.4685359\n## medv -0.3883046  0.6953599  0.2499287 -0.4685359  1.0000000\ncor(myds[1:5])\n##            crim         rm        dis        tax       medv\n## crim  1.0000000 -0.2192467 -0.3796701  0.5827643 -0.3883046\n## rm   -0.2192467  1.0000000  0.2052462 -0.2920478  0.6953599\n## dis  -0.3796701  0.2052462  1.0000000 -0.5344316  0.2499287\n## tax   0.5827643 -0.2920478 -0.5344316  1.0000000 -0.4685359\n## medv -0.3883046  0.6953599  0.2499287 -0.4685359  1.0000000\n\n\n\n\n\nz <- c(1, 2, 3, NA, 5, NA, 8)   # ê²°ì¸¡ê°’ì´ í¬í•¨ëœ ë²¡í„° z\nsum(z)                          # ì •ìƒ ê³„ì‚°ì´ ì•ˆ ë¨\n## [1] NA\nis.na(z)                        # NA ì—¬ë¶€ í™•ì¸\n## [1] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE\nsum(is.na(z))                   # NAì˜ ê°œìˆ˜ í™•ì¸\n## [1] 2\nsum(z, na.rm = TRUE)            # NAë¥¼ ì œì™¸í•˜ê³  í•©ê³„ë¥¼ ê³„ì‚°\n## [1] 19\n\n\nz1 <- c(1, 2, 3, NA, 5, NA, 8)          # ê²°ì¸¡ê°’ì´ í¬í•¨ëœ ë²¡í„° z1\nz2 <- c(5, 8, 1, NA, 3, NA, 7)          # ê²°ì¸¡ê°’ì´ í¬í•¨ëœ ë²¡í„° z2\nz1[is.na(z1)] <- 0                      # NAë¥¼ 0ìœ¼ë¡œ ì¹˜í™˜\nz1\n## [1] 1 2 3 0 5 0 8\n\nz1[is.na(z1)] <- mean(z1, na.rm = TRUE) # NAë¥¼ z1ì˜ í‰ê· ê°’ìœ¼ë¡œ ì¹˜í™˜\nz1\n## [1] 1 2 3 0 5 0 8\n\nz3 <- as.vector(na.omit(z2))            # NAë¥¼ ì œê±°í•˜ê³  ìƒˆë¡œìš´ ë²¡í„° ìƒì„±\nz3\n## [1] 5 8 1 3 7\n\n\n# NAë¥¼ í¬í•¨í•˜ëŠ” test ë°ì´í„° ìƒì„±\nx <- iris\nx[1, 2] <- NA\nx[1, 3] <- NA\nx[2, 3] <- NA\nx[3, 4] <- NA\nhead(x)\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1          NA           NA         0.2  setosa\n## 2          4.9         3.0           NA         0.2  setosa\n## 3          4.7         3.2          1.3          NA  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\n\n\n# forë¬¸ì„ ì´ìš©í•œ ë°©ë²• â˜…â˜…â˜…â˜…â˜…\nfor (i in 1:ncol(x)) {\n    this.na <- is.na(x[, i])\n    cat(colnames(x)[i], \"\\t\", sum(this.na), \"\\n\")\n}\n## Sepal.Length      0 \n## Sepal.Width   1 \n## Petal.Length      2 \n## Petal.Width   1 \n## Species   0\n\n# applyë¥¼ ì´ìš©í•œ ë°©ë²•\ncol_na <- function(y) {\n    return(sum(is.na(y)))\n}\n\nna_count <- apply(x, 2, FUN = col_na)\nna_count\n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n##            0            1            2            1            0\n\n\nrowSums(is.na(x))           # í–‰ë³„ NAì˜ ê°œìˆ˜\n##   [1] 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n##  [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n##  [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n## [112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n## [149] 0 0\nsum(rowSums(is.na(x)) > 0)  # NAê°€ í¬í•¨ëœ í–‰ì˜ ê°œìˆ˜\n## [1] 3\n\nsum(is.na(x))               # ë°ì´í„°ì…‹ ì „ì²´ì—ì„œ NA ê°œìˆ˜\n## [1] 4\n\n\nhead(x)\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1          NA           NA         0.2  setosa\n## 2          4.9         3.0           NA         0.2  setosa\n## 3          4.7         3.2          1.3          NA  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\nx[!complete.cases(x), ]     # NAê°€ í¬í•¨ëœ í–‰ë“¤ ì¶œë ¥\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1          NA           NA         0.2  setosa\n## 2          4.9         3.0           NA         0.2  setosa\n## 3          4.7         3.2          1.3          NA  setosa\ny <- x[complete.cases(x), ] # NAê°€ í¬í•¨ëœ í–‰ë“¤ ì œê±°\nhead(y)                     # ìƒˆë¡œìš´ ë°ì´í„°ì…‹ yì˜ ë‚´ìš© í™•ì¸\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\n## 7          4.6         3.4          1.4         0.3  setosa\n## 8          5.0         3.4          1.5         0.2  setosa\n## 9          4.4         2.9          1.4         0.2  setosa\n\n\nst <- data.frame(state.x77)\nboxplot(st$Income)\n\n\n\nboxplot.stats(st$Income)\n## $stats\n## [1] 3098 3983 4519 4815 5348\n## \n## $n\n## [1] 50\n## \n## $conf\n## [1] 4333.093 4704.907\n## \n## $out\n## [1] 6315\n# stats (ê° ë³€ìˆ˜ì˜ ìµœì†Œê°’, 1ì‚¬ë¶„ìœ„ìˆ˜, 2ì‚¬ë¶„ìœ„ìˆ˜, 3ì‚¬ë¶„ìœ„ìˆ˜, ìµœëŒ€ê°’ì´ ì €ì¥ë˜ì–´ ìˆëŠ” í–‰ë ¬)\n# n (ê° ê·¸ë£¹ë§ˆë‹¤ì˜ ê´€ì¸¡ê°’ ìˆ˜ë¥¼ ì €ì¥í•œ ë²¡í„°)\n# conf (ì¤‘ì•™ê°’ì˜ 95% ì‹ ë¢°êµ¬ê°„, median+-1.58*IQR/(n)^0.5)\n# out (ì´ìƒì¹˜)\nboxplot.stats(st$Income)$out\n## [1] 6315\n\n\nout.val <- boxplot.stats(st$Income)$out     # íŠ¹ì´ê°’ ì¶”ì¶œ\n\nst$Income %in% out.val\n##  [1] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [49] FALSE FALSE\nst$Income == out.val\n##  [1] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [49] FALSE FALSE\n\nst$Income[st$Income %in% out.val] <- NA     # íŠ¹ì´ê°’ì„ NAë¡œ ëŒ€ì²´\nst$Income[st$Income == out.val] <- NA\n\nhead(st)\n##            Population Income Illiteracy Life.Exp Murder HS.Grad Frost   Area\n## Alabama          3615   3624        2.1    69.05   15.1    41.3    20  50708\n## Alaska            365     NA        1.5    69.31   11.3    66.7   152 566432\n## Arizona          2212   4530        1.8    70.55    7.8    58.1    15 113417\n## Arkansas         2110   3378        1.9    70.66   10.1    39.9    65  51945\n## California      21198   5114        1.1    71.71   10.3    62.6    20 156361\n## Colorado         2541   4884        0.7    72.06    6.8    63.9   166 103766\nnewdata <- st[complete.cases(st), ]         # NAê°€ í¬í•¨ëœ í–‰ ì œê±° â˜…â˜…â˜…â˜…â˜…\nhead(newdata)\n##             Population Income Illiteracy Life.Exp Murder HS.Grad Frost   Area\n## Alabama           3615   3624        2.1    69.05   15.1    41.3    20  50708\n## Arizona           2212   4530        1.8    70.55    7.8    58.1    15 113417\n## Arkansas          2110   3378        1.9    70.66   10.1    39.9    65  51945\n## California       21198   5114        1.1    71.71   10.3    62.6    20 156361\n## Colorado          2541   4884        0.7    72.06    6.8    63.9   166 103766\n## Connecticut       3100   5348        1.1    72.48    3.1    56.0   139   4862\n\n\nv1 <- c(1, 7, 6, 8, 4, 2, 3)\norder(v1)\n## [1] 1 6 7 5 3 2 4\n\nv1 <- sort(v1) # ì˜¤ë¦„ì°¨ìˆœ\nv1\n## [1] 1 2 3 4 6 7 8\nv1[order(v1)]\n## [1] 1 2 3 4 6 7 8\n\nv2 <- sort(v1, decreasing = T) # ë‚´ë¦¼ì°¨ìˆœ\nv2\n## [1] 8 7 6 4 3 2 1\n\n\nhead(iris)\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\norder(iris$Sepal.Length)\n##   [1]  14   9  39  43  42   4   7  23  48   3  30  12  13  25  31  46   2  10\n##  [19]  35  38  58 107   5   8  26  27  36  41  44  50  61  94   1  18  20  22\n##  [37]  24  40  45  47  99  28  29  33  60  49   6  11  17  21  32  85  34  37\n##  [55]  54  81  82  90  91  65  67  70  89  95 122  16  19  56  80  96  97 100\n##  [73] 114  15  68  83  93 102 115 143  62  71 150  63  79  84  86 120 139  64\n##  [91]  72  74  92 128 135  69  98 127 149  57  73  88 101 104 124 134 137 147\n## [109]  52  75 112 116 129 133 138  55 105 111 117 148  59  76  66  78  87 109\n## [127] 125 141 145 146  77 113 144  53 121 140 142  51 103 110 126 130 108 131\n## [145] 106 118 119 123 136 132\niris[order(iris$Sepal.Length), ]                    # ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ì •ë ¬\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 14           4.3         3.0          1.1         0.1     setosa\n## 9            4.4         2.9          1.4         0.2     setosa\n## 39           4.4         3.0          1.3         0.2     setosa\n## 43           4.4         3.2          1.3         0.2     setosa\n## 42           4.5         2.3          1.3         0.3     setosa\n## 4            4.6         3.1          1.5         0.2     setosa\n## 7            4.6         3.4          1.4         0.3     setosa\n## 23           4.6         3.6          1.0         0.2     setosa\n## 48           4.6         3.2          1.4         0.2     setosa\n## 3            4.7         3.2          1.3         0.2     setosa\n## 30           4.7         3.2          1.6         0.2     setosa\n## 12           4.8         3.4          1.6         0.2     setosa\n## 13           4.8         3.0          1.4         0.1     setosa\n## 25           4.8         3.4          1.9         0.2     setosa\n## 31           4.8         3.1          1.6         0.2     setosa\n## 46           4.8         3.0          1.4         0.3     setosa\n## 2            4.9         3.0          1.4         0.2     setosa\n## 10           4.9         3.1          1.5         0.1     setosa\n## 35           4.9         3.1          1.5         0.2     setosa\n## 38           4.9         3.6          1.4         0.1     setosa\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 107          4.9         2.5          4.5         1.7  virginica\n## 5            5.0         3.6          1.4         0.2     setosa\n## 8            5.0         3.4          1.5         0.2     setosa\n## 26           5.0         3.0          1.6         0.2     setosa\n## 27           5.0         3.4          1.6         0.4     setosa\n## 36           5.0         3.2          1.2         0.2     setosa\n## 41           5.0         3.5          1.3         0.3     setosa\n## 44           5.0         3.5          1.6         0.6     setosa\n## 50           5.0         3.3          1.4         0.2     setosa\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 1            5.1         3.5          1.4         0.2     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 28           5.2         3.5          1.5         0.2     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 49           5.3         3.7          1.5         0.2     setosa\n## 6            5.4         3.9          1.7         0.4     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 34           5.5         4.2          1.4         0.2     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 122          5.6         2.8          4.9         2.0  virginica\n## 16           5.7         4.4          1.5         0.4     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 114          5.7         2.5          5.0         2.0  virginica\n## 15           5.8         4.0          1.2         0.2     setosa\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 102          5.8         2.7          5.1         1.9  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 150          5.9         3.0          5.1         1.8  virginica\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 120          6.0         2.2          5.0         1.5  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 128          6.1         3.0          4.9         1.8  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 127          6.2         2.8          4.8         1.8  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 112          6.4         2.7          5.3         1.9  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 105          6.5         3.0          5.8         2.2  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 109          6.7         2.5          5.8         1.8  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 113          6.8         3.0          5.5         2.1  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 121          6.9         3.2          5.7         2.3  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 103          7.1         3.0          5.9         2.1  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\niris[order(iris$Sepal.Length, decreasing = T), ]    # ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 132          7.9         3.8          6.4         2.0  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 121          6.9         3.2          5.7         2.3  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 113          6.8         3.0          5.5         2.1  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 109          6.7         2.5          5.8         1.8  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 105          6.5         3.0          5.8         2.2  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 112          6.4         2.7          5.3         1.9  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 127          6.2         2.8          4.8         1.8  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 128          6.1         3.0          4.9         1.8  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 120          6.0         2.2          5.0         1.5  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 150          5.9         3.0          5.1         1.8  virginica\n## 15           5.8         4.0          1.2         0.2     setosa\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 102          5.8         2.7          5.1         1.9  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 16           5.7         4.4          1.5         0.4     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 114          5.7         2.5          5.0         2.0  virginica\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 122          5.6         2.8          4.9         2.0  virginica\n## 34           5.5         4.2          1.4         0.2     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 6            5.4         3.9          1.7         0.4     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 49           5.3         3.7          1.5         0.2     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 1            5.1         3.5          1.4         0.2     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 5            5.0         3.6          1.4         0.2     setosa\n## 8            5.0         3.4          1.5         0.2     setosa\n## 26           5.0         3.0          1.6         0.2     setosa\n## 27           5.0         3.4          1.6         0.4     setosa\n## 36           5.0         3.2          1.2         0.2     setosa\n## 41           5.0         3.5          1.3         0.3     setosa\n## 44           5.0         3.5          1.6         0.6     setosa\n## 50           5.0         3.3          1.4         0.2     setosa\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 2            4.9         3.0          1.4         0.2     setosa\n## 10           4.9         3.1          1.5         0.1     setosa\n## 35           4.9         3.1          1.5         0.2     setosa\n## 38           4.9         3.6          1.4         0.1     setosa\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 107          4.9         2.5          4.5         1.7  virginica\n## 12           4.8         3.4          1.6         0.2     setosa\n## 13           4.8         3.0          1.4         0.1     setosa\n## 25           4.8         3.4          1.9         0.2     setosa\n## 31           4.8         3.1          1.6         0.2     setosa\n## 46           4.8         3.0          1.4         0.3     setosa\n## 3            4.7         3.2          1.3         0.2     setosa\n## 30           4.7         3.2          1.6         0.2     setosa\n## 4            4.6         3.1          1.5         0.2     setosa\n## 7            4.6         3.4          1.4         0.3     setosa\n## 23           4.6         3.6          1.0         0.2     setosa\n## 48           4.6         3.2          1.4         0.2     setosa\n## 42           4.5         2.3          1.3         0.3     setosa\n## 9            4.4         2.9          1.4         0.2     setosa\n## 39           4.4         3.0          1.3         0.2     setosa\n## 43           4.4         3.2          1.3         0.2     setosa\n## 14           4.3         3.0          1.1         0.1     setosa\n\niris.new <- iris[order(iris$Sepal.Length), ]        # ì •ë ¬ëœ ë°ì´í„°ë¥¼ ì €ì¥\nhead(iris.new)\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 14          4.3         3.0          1.1         0.1  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 39          4.4         3.0          1.3         0.2  setosa\n## 43          4.4         3.2          1.3         0.2  setosa\n## 42          4.5         2.3          1.3         0.3  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\niris[order(iris$Species,-iris$Petal.Length, decreasing = T), ] # ì •ë ¬ ê¸°ì¤€ì´ 2ê°œ\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 107          4.9         2.5          4.5         1.7  virginica\n## 127          6.2         2.8          4.8         1.8  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 122          5.6         2.8          4.9         2.0  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 128          6.1         3.0          4.9         1.8  virginica\n## 114          5.7         2.5          5.0         2.0  virginica\n## 120          6.0         2.2          5.0         1.5  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 101          6.3         3.3          6.0         2.5  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 23           4.6         3.6          1.0         0.2     setosa\n## 14           4.3         3.0          1.1         0.1     setosa\n## 15           5.8         4.0          1.2         0.2     setosa\n## 36           5.0         3.2          1.2         0.2     setosa\n## 3            4.7         3.2          1.3         0.2     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 39           4.4         3.0          1.3         0.2     setosa\n## 41           5.0         3.5          1.3         0.3     setosa\n## 42           4.5         2.3          1.3         0.3     setosa\n## 43           4.4         3.2          1.3         0.2     setosa\n## 1            5.1         3.5          1.4         0.2     setosa\n## 2            4.9         3.0          1.4         0.2     setosa\n## 5            5.0         3.6          1.4         0.2     setosa\n## 7            4.6         3.4          1.4         0.3     setosa\n## 9            4.4         2.9          1.4         0.2     setosa\n## 13           4.8         3.0          1.4         0.1     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 34           5.5         4.2          1.4         0.2     setosa\n## 38           4.9         3.6          1.4         0.1     setosa\n## 46           4.8         3.0          1.4         0.3     setosa\n## 48           4.6         3.2          1.4         0.2     setosa\n## 50           5.0         3.3          1.4         0.2     setosa\n## 4            4.6         3.1          1.5         0.2     setosa\n## 8            5.0         3.4          1.5         0.2     setosa\n## 10           4.9         3.1          1.5         0.1     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 16           5.7         4.4          1.5         0.4     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 35           4.9         3.1          1.5         0.2     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 49           5.3         3.7          1.5         0.2     setosa\n## 12           4.8         3.4          1.6         0.2     setosa\n## 26           5.0         3.0          1.6         0.2     setosa\n## 27           5.0         3.4          1.6         0.4     setosa\n## 30           4.7         3.2          1.6         0.2     setosa\n## 31           4.8         3.1          1.6         0.2     setosa\n## 44           5.0         3.5          1.6         0.6     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 6            5.4         3.9          1.7         0.4     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 25           4.8         3.4          1.9         0.2     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\niris[order(iris$Species, decreasing = T, iris$Petal.Length), ]\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 119          7.7         2.6          6.9         2.3  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 101          6.3         3.3          6.0         2.5  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\n## 114          5.7         2.5          5.0         2.0  virginica\n## 120          6.0         2.2          5.0         1.5  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 122          5.6         2.8          4.9         2.0  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 128          6.1         3.0          4.9         1.8  virginica\n## 127          6.2         2.8          4.8         1.8  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 107          4.9         2.5          4.5         1.7  virginica\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 25           4.8         3.4          1.9         0.2     setosa\n## 45           5.1         3.8          1.9         0.4     setosa\n## 6            5.4         3.9          1.7         0.4     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 24           5.1         3.3          1.7         0.5     setosa\n## 12           4.8         3.4          1.6         0.2     setosa\n## 26           5.0         3.0          1.6         0.2     setosa\n## 27           5.0         3.4          1.6         0.4     setosa\n## 30           4.7         3.2          1.6         0.2     setosa\n## 31           4.8         3.1          1.6         0.2     setosa\n## 44           5.0         3.5          1.6         0.6     setosa\n## 47           5.1         3.8          1.6         0.2     setosa\n## 4            4.6         3.1          1.5         0.2     setosa\n## 8            5.0         3.4          1.5         0.2     setosa\n## 10           4.9         3.1          1.5         0.1     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 16           5.7         4.4          1.5         0.4     setosa\n## 20           5.1         3.8          1.5         0.3     setosa\n## 22           5.1         3.7          1.5         0.4     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 35           4.9         3.1          1.5         0.2     setosa\n## 40           5.1         3.4          1.5         0.2     setosa\n## 49           5.3         3.7          1.5         0.2     setosa\n## 1            5.1         3.5          1.4         0.2     setosa\n## 2            4.9         3.0          1.4         0.2     setosa\n## 5            5.0         3.6          1.4         0.2     setosa\n## 7            4.6         3.4          1.4         0.3     setosa\n## 9            4.4         2.9          1.4         0.2     setosa\n## 13           4.8         3.0          1.4         0.1     setosa\n## 18           5.1         3.5          1.4         0.3     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 34           5.5         4.2          1.4         0.2     setosa\n## 38           4.9         3.6          1.4         0.1     setosa\n## 46           4.8         3.0          1.4         0.3     setosa\n## 48           4.6         3.2          1.4         0.2     setosa\n## 50           5.0         3.3          1.4         0.2     setosa\n## 3            4.7         3.2          1.3         0.2     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 39           4.4         3.0          1.3         0.2     setosa\n## 41           5.0         3.5          1.3         0.3     setosa\n## 42           4.5         2.3          1.3         0.3     setosa\n## 43           4.4         3.2          1.3         0.2     setosa\n## 15           5.8         4.0          1.2         0.2     setosa\n## 36           5.0         3.2          1.2         0.2     setosa\n## 14           4.3         3.0          1.1         0.1     setosa\n## 23           4.6         3.6          1.0         0.2     setosa\n\n\nsp <- split(iris, iris$Species) # í’ˆì¢…ë³„ë¡œ ë°ì´í„° ë¶„ë¦¬\nsp                              # ë¶„ë¦¬ ê²°ê³¼ í™•ì¸\n## $setosa\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1           5.1         3.5          1.4         0.2  setosa\n## 2           4.9         3.0          1.4         0.2  setosa\n## 3           4.7         3.2          1.3         0.2  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\n## 5           5.0         3.6          1.4         0.2  setosa\n## 6           5.4         3.9          1.7         0.4  setosa\n## 7           4.6         3.4          1.4         0.3  setosa\n## 8           5.0         3.4          1.5         0.2  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\n## 11          5.4         3.7          1.5         0.2  setosa\n## 12          4.8         3.4          1.6         0.2  setosa\n## 13          4.8         3.0          1.4         0.1  setosa\n## 14          4.3         3.0          1.1         0.1  setosa\n## 15          5.8         4.0          1.2         0.2  setosa\n## 16          5.7         4.4          1.5         0.4  setosa\n## 17          5.4         3.9          1.3         0.4  setosa\n## 18          5.1         3.5          1.4         0.3  setosa\n## 19          5.7         3.8          1.7         0.3  setosa\n## 20          5.1         3.8          1.5         0.3  setosa\n## 21          5.4         3.4          1.7         0.2  setosa\n## 22          5.1         3.7          1.5         0.4  setosa\n## 23          4.6         3.6          1.0         0.2  setosa\n## 24          5.1         3.3          1.7         0.5  setosa\n## 25          4.8         3.4          1.9         0.2  setosa\n## 26          5.0         3.0          1.6         0.2  setosa\n## 27          5.0         3.4          1.6         0.4  setosa\n## 28          5.2         3.5          1.5         0.2  setosa\n## 29          5.2         3.4          1.4         0.2  setosa\n## 30          4.7         3.2          1.6         0.2  setosa\n## 31          4.8         3.1          1.6         0.2  setosa\n## 32          5.4         3.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\n## 35          4.9         3.1          1.5         0.2  setosa\n## 36          5.0         3.2          1.2         0.2  setosa\n## 37          5.5         3.5          1.3         0.2  setosa\n## 38          4.9         3.6          1.4         0.1  setosa\n## 39          4.4         3.0          1.3         0.2  setosa\n## 40          5.1         3.4          1.5         0.2  setosa\n## 41          5.0         3.5          1.3         0.3  setosa\n## 42          4.5         2.3          1.3         0.3  setosa\n## 43          4.4         3.2          1.3         0.2  setosa\n## 44          5.0         3.5          1.6         0.6  setosa\n## 45          5.1         3.8          1.9         0.4  setosa\n## 46          4.8         3.0          1.4         0.3  setosa\n## 47          5.1         3.8          1.6         0.2  setosa\n## 48          4.6         3.2          1.4         0.2  setosa\n## 49          5.3         3.7          1.5         0.2  setosa\n## 50          5.0         3.3          1.4         0.2  setosa\n## \n## $versicolor\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 58           4.9         2.4          3.3         1.0 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 61           5.0         2.0          3.5         1.0 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 94           5.0         2.3          3.3         1.0 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 99           5.1         2.5          3.0         1.1 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## \n## $virginica\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n## 101          6.3         3.3          6.0         2.5 virginica\n## 102          5.8         2.7          5.1         1.9 virginica\n## 103          7.1         3.0          5.9         2.1 virginica\n## 104          6.3         2.9          5.6         1.8 virginica\n## 105          6.5         3.0          5.8         2.2 virginica\n## 106          7.6         3.0          6.6         2.1 virginica\n## 107          4.9         2.5          4.5         1.7 virginica\n## 108          7.3         2.9          6.3         1.8 virginica\n## 109          6.7         2.5          5.8         1.8 virginica\n## 110          7.2         3.6          6.1         2.5 virginica\n## 111          6.5         3.2          5.1         2.0 virginica\n## 112          6.4         2.7          5.3         1.9 virginica\n## 113          6.8         3.0          5.5         2.1 virginica\n## 114          5.7         2.5          5.0         2.0 virginica\n## 115          5.8         2.8          5.1         2.4 virginica\n## 116          6.4         3.2          5.3         2.3 virginica\n## 117          6.5         3.0          5.5         1.8 virginica\n## 118          7.7         3.8          6.7         2.2 virginica\n## 119          7.7         2.6          6.9         2.3 virginica\n## 120          6.0         2.2          5.0         1.5 virginica\n## 121          6.9         3.2          5.7         2.3 virginica\n## 122          5.6         2.8          4.9         2.0 virginica\n## 123          7.7         2.8          6.7         2.0 virginica\n## 124          6.3         2.7          4.9         1.8 virginica\n## 125          6.7         3.3          5.7         2.1 virginica\n## 126          7.2         3.2          6.0         1.8 virginica\n## 127          6.2         2.8          4.8         1.8 virginica\n## 128          6.1         3.0          4.9         1.8 virginica\n## 129          6.4         2.8          5.6         2.1 virginica\n## 130          7.2         3.0          5.8         1.6 virginica\n## 131          7.4         2.8          6.1         1.9 virginica\n## 132          7.9         3.8          6.4         2.0 virginica\n## 133          6.4         2.8          5.6         2.2 virginica\n## 134          6.3         2.8          5.1         1.5 virginica\n## 135          6.1         2.6          5.6         1.4 virginica\n## 136          7.7         3.0          6.1         2.3 virginica\n## 137          6.3         3.4          5.6         2.4 virginica\n## 138          6.4         3.1          5.5         1.8 virginica\n## 139          6.0         3.0          4.8         1.8 virginica\n## 140          6.9         3.1          5.4         2.1 virginica\n## 141          6.7         3.1          5.6         2.4 virginica\n## 142          6.9         3.1          5.1         2.3 virginica\n## 143          5.8         2.7          5.1         1.9 virginica\n## 144          6.8         3.2          5.9         2.3 virginica\n## 145          6.7         3.3          5.7         2.5 virginica\n## 146          6.7         3.0          5.2         2.3 virginica\n## 147          6.3         2.5          5.0         1.9 virginica\n## 148          6.5         3.0          5.2         2.0 virginica\n## 149          6.2         3.4          5.4         2.3 virginica\n## 150          5.9         3.0          5.1         1.8 virginica\nsummary(sp)                     # ë¶„ë¦¬ ê²°ê³¼ ìš”ì•½\n##            Length Class      Mode\n## setosa     5      data.frame list\n## versicolor 5      data.frame list\n## virginica  5      data.frame list\nsp$setosa                       # setosa í’ˆì¢…ì˜ ë°ì´í„° í™•ì¸\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1           5.1         3.5          1.4         0.2  setosa\n## 2           4.9         3.0          1.4         0.2  setosa\n## 3           4.7         3.2          1.3         0.2  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\n## 5           5.0         3.6          1.4         0.2  setosa\n## 6           5.4         3.9          1.7         0.4  setosa\n## 7           4.6         3.4          1.4         0.3  setosa\n## 8           5.0         3.4          1.5         0.2  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\n## 11          5.4         3.7          1.5         0.2  setosa\n## 12          4.8         3.4          1.6         0.2  setosa\n## 13          4.8         3.0          1.4         0.1  setosa\n## 14          4.3         3.0          1.1         0.1  setosa\n## 15          5.8         4.0          1.2         0.2  setosa\n## 16          5.7         4.4          1.5         0.4  setosa\n## 17          5.4         3.9          1.3         0.4  setosa\n## 18          5.1         3.5          1.4         0.3  setosa\n## 19          5.7         3.8          1.7         0.3  setosa\n## 20          5.1         3.8          1.5         0.3  setosa\n## 21          5.4         3.4          1.7         0.2  setosa\n## 22          5.1         3.7          1.5         0.4  setosa\n## 23          4.6         3.6          1.0         0.2  setosa\n## 24          5.1         3.3          1.7         0.5  setosa\n## 25          4.8         3.4          1.9         0.2  setosa\n## 26          5.0         3.0          1.6         0.2  setosa\n## 27          5.0         3.4          1.6         0.4  setosa\n## 28          5.2         3.5          1.5         0.2  setosa\n## 29          5.2         3.4          1.4         0.2  setosa\n## 30          4.7         3.2          1.6         0.2  setosa\n## 31          4.8         3.1          1.6         0.2  setosa\n## 32          5.4         3.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\n## 35          4.9         3.1          1.5         0.2  setosa\n## 36          5.0         3.2          1.2         0.2  setosa\n## 37          5.5         3.5          1.3         0.2  setosa\n## 38          4.9         3.6          1.4         0.1  setosa\n## 39          4.4         3.0          1.3         0.2  setosa\n## 40          5.1         3.4          1.5         0.2  setosa\n## 41          5.0         3.5          1.3         0.3  setosa\n## 42          4.5         2.3          1.3         0.3  setosa\n## 43          4.4         3.2          1.3         0.2  setosa\n## 44          5.0         3.5          1.6         0.6  setosa\n## 45          5.1         3.8          1.9         0.4  setosa\n## 46          4.8         3.0          1.4         0.3  setosa\n## 47          5.1         3.8          1.6         0.2  setosa\n## 48          4.6         3.2          1.4         0.2  setosa\n## 49          5.3         3.7          1.5         0.2  setosa\n## 50          5.0         3.3          1.4         0.2  setosa\nsetosa <- sp$setosa\n\n\nsubset(iris, Species == \"setosa\")\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1           5.1         3.5          1.4         0.2  setosa\n## 2           4.9         3.0          1.4         0.2  setosa\n## 3           4.7         3.2          1.3         0.2  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\n## 5           5.0         3.6          1.4         0.2  setosa\n## 6           5.4         3.9          1.7         0.4  setosa\n## 7           4.6         3.4          1.4         0.3  setosa\n## 8           5.0         3.4          1.5         0.2  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\n## 11          5.4         3.7          1.5         0.2  setosa\n## 12          4.8         3.4          1.6         0.2  setosa\n## 13          4.8         3.0          1.4         0.1  setosa\n## 14          4.3         3.0          1.1         0.1  setosa\n## 15          5.8         4.0          1.2         0.2  setosa\n## 16          5.7         4.4          1.5         0.4  setosa\n## 17          5.4         3.9          1.3         0.4  setosa\n## 18          5.1         3.5          1.4         0.3  setosa\n## 19          5.7         3.8          1.7         0.3  setosa\n## 20          5.1         3.8          1.5         0.3  setosa\n## 21          5.4         3.4          1.7         0.2  setosa\n## 22          5.1         3.7          1.5         0.4  setosa\n## 23          4.6         3.6          1.0         0.2  setosa\n## 24          5.1         3.3          1.7         0.5  setosa\n## 25          4.8         3.4          1.9         0.2  setosa\n## 26          5.0         3.0          1.6         0.2  setosa\n## 27          5.0         3.4          1.6         0.4  setosa\n## 28          5.2         3.5          1.5         0.2  setosa\n## 29          5.2         3.4          1.4         0.2  setosa\n## 30          4.7         3.2          1.6         0.2  setosa\n## 31          4.8         3.1          1.6         0.2  setosa\n## 32          5.4         3.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\n## 35          4.9         3.1          1.5         0.2  setosa\n## 36          5.0         3.2          1.2         0.2  setosa\n## 37          5.5         3.5          1.3         0.2  setosa\n## 38          4.9         3.6          1.4         0.1  setosa\n## 39          4.4         3.0          1.3         0.2  setosa\n## 40          5.1         3.4          1.5         0.2  setosa\n## 41          5.0         3.5          1.3         0.3  setosa\n## 42          4.5         2.3          1.3         0.3  setosa\n## 43          4.4         3.2          1.3         0.2  setosa\n## 44          5.0         3.5          1.6         0.6  setosa\n## 45          5.1         3.8          1.9         0.4  setosa\n## 46          4.8         3.0          1.4         0.3  setosa\n## 47          5.1         3.8          1.6         0.2  setosa\n## 48          4.6         3.2          1.4         0.2  setosa\n## 49          5.3         3.7          1.5         0.2  setosa\n## 50          5.0         3.3          1.4         0.2  setosa\nsubset(iris, Sepal.Length > 7.5)\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n## 106          7.6         3.0          6.6         2.1 virginica\n## 118          7.7         3.8          6.7         2.2 virginica\n## 119          7.7         2.6          6.9         2.3 virginica\n## 123          7.7         2.8          6.7         2.0 virginica\n## 132          7.9         3.8          6.4         2.0 virginica\n## 136          7.7         3.0          6.1         2.3 virginica\nsubset(iris, Sepal.Length > 5.1 &\n           Sepal.Width > 3.9)\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 15          5.8         4.0          1.2         0.2  setosa\n## 16          5.7         4.4          1.5         0.4  setosa\n## 33          5.2         4.1          1.5         0.1  setosa\n## 34          5.5         4.2          1.4         0.2  setosa\nsubset(iris, Sepal.Length > 5.1 |\n           Sepal.Width > 3.9)\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 6            5.4         3.9          1.7         0.4     setosa\n## 11           5.4         3.7          1.5         0.2     setosa\n## 15           5.8         4.0          1.2         0.2     setosa\n## 16           5.7         4.4          1.5         0.4     setosa\n## 17           5.4         3.9          1.3         0.4     setosa\n## 19           5.7         3.8          1.7         0.3     setosa\n## 21           5.4         3.4          1.7         0.2     setosa\n## 28           5.2         3.5          1.5         0.2     setosa\n## 29           5.2         3.4          1.4         0.2     setosa\n## 32           5.4         3.4          1.5         0.4     setosa\n## 33           5.2         4.1          1.5         0.1     setosa\n## 34           5.5         4.2          1.4         0.2     setosa\n## 37           5.5         3.5          1.3         0.2     setosa\n## 49           5.3         3.7          1.5         0.2     setosa\n## 51           7.0         3.2          4.7         1.4 versicolor\n## 52           6.4         3.2          4.5         1.5 versicolor\n## 53           6.9         3.1          4.9         1.5 versicolor\n## 54           5.5         2.3          4.0         1.3 versicolor\n## 55           6.5         2.8          4.6         1.5 versicolor\n## 56           5.7         2.8          4.5         1.3 versicolor\n## 57           6.3         3.3          4.7         1.6 versicolor\n## 59           6.6         2.9          4.6         1.3 versicolor\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 62           5.9         3.0          4.2         1.5 versicolor\n## 63           6.0         2.2          4.0         1.0 versicolor\n## 64           6.1         2.9          4.7         1.4 versicolor\n## 65           5.6         2.9          3.6         1.3 versicolor\n## 66           6.7         3.1          4.4         1.4 versicolor\n## 67           5.6         3.0          4.5         1.5 versicolor\n## 68           5.8         2.7          4.1         1.0 versicolor\n## 69           6.2         2.2          4.5         1.5 versicolor\n## 70           5.6         2.5          3.9         1.1 versicolor\n## 71           5.9         3.2          4.8         1.8 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 73           6.3         2.5          4.9         1.5 versicolor\n## 74           6.1         2.8          4.7         1.2 versicolor\n## 75           6.4         2.9          4.3         1.3 versicolor\n## 76           6.6         3.0          4.4         1.4 versicolor\n## 77           6.8         2.8          4.8         1.4 versicolor\n## 78           6.7         3.0          5.0         1.7 versicolor\n## 79           6.0         2.9          4.5         1.5 versicolor\n## 80           5.7         2.6          3.5         1.0 versicolor\n## 81           5.5         2.4          3.8         1.1 versicolor\n## 82           5.5         2.4          3.7         1.0 versicolor\n## 83           5.8         2.7          3.9         1.2 versicolor\n## 84           6.0         2.7          5.1         1.6 versicolor\n## 85           5.4         3.0          4.5         1.5 versicolor\n## 86           6.0         3.4          4.5         1.6 versicolor\n## 87           6.7         3.1          4.7         1.5 versicolor\n## 88           6.3         2.3          4.4         1.3 versicolor\n## 89           5.6         3.0          4.1         1.3 versicolor\n## 90           5.5         2.5          4.0         1.3 versicolor\n## 91           5.5         2.6          4.4         1.2 versicolor\n## 92           6.1         3.0          4.6         1.4 versicolor\n## 93           5.8         2.6          4.0         1.2 versicolor\n## 95           5.6         2.7          4.2         1.3 versicolor\n## 96           5.7         3.0          4.2         1.2 versicolor\n## 97           5.7         2.9          4.2         1.3 versicolor\n## 98           6.2         2.9          4.3         1.3 versicolor\n## 100          5.7         2.8          4.1         1.3 versicolor\n## 101          6.3         3.3          6.0         2.5  virginica\n## 102          5.8         2.7          5.1         1.9  virginica\n## 103          7.1         3.0          5.9         2.1  virginica\n## 104          6.3         2.9          5.6         1.8  virginica\n## 105          6.5         3.0          5.8         2.2  virginica\n## 106          7.6         3.0          6.6         2.1  virginica\n## 108          7.3         2.9          6.3         1.8  virginica\n## 109          6.7         2.5          5.8         1.8  virginica\n## 110          7.2         3.6          6.1         2.5  virginica\n## 111          6.5         3.2          5.1         2.0  virginica\n## 112          6.4         2.7          5.3         1.9  virginica\n## 113          6.8         3.0          5.5         2.1  virginica\n## 114          5.7         2.5          5.0         2.0  virginica\n## 115          5.8         2.8          5.1         2.4  virginica\n## 116          6.4         3.2          5.3         2.3  virginica\n## 117          6.5         3.0          5.5         1.8  virginica\n## 118          7.7         3.8          6.7         2.2  virginica\n## 119          7.7         2.6          6.9         2.3  virginica\n## 120          6.0         2.2          5.0         1.5  virginica\n## 121          6.9         3.2          5.7         2.3  virginica\n## 122          5.6         2.8          4.9         2.0  virginica\n## 123          7.7         2.8          6.7         2.0  virginica\n## 124          6.3         2.7          4.9         1.8  virginica\n## 125          6.7         3.3          5.7         2.1  virginica\n## 126          7.2         3.2          6.0         1.8  virginica\n## 127          6.2         2.8          4.8         1.8  virginica\n## 128          6.1         3.0          4.9         1.8  virginica\n## 129          6.4         2.8          5.6         2.1  virginica\n## 130          7.2         3.0          5.8         1.6  virginica\n## 131          7.4         2.8          6.1         1.9  virginica\n## 132          7.9         3.8          6.4         2.0  virginica\n## 133          6.4         2.8          5.6         2.2  virginica\n## 134          6.3         2.8          5.1         1.5  virginica\n## 135          6.1         2.6          5.6         1.4  virginica\n## 136          7.7         3.0          6.1         2.3  virginica\n## 137          6.3         3.4          5.6         2.4  virginica\n## 138          6.4         3.1          5.5         1.8  virginica\n## 139          6.0         3.0          4.8         1.8  virginica\n## 140          6.9         3.1          5.4         2.1  virginica\n## 141          6.7         3.1          5.6         2.4  virginica\n## 142          6.9         3.1          5.1         2.3  virginica\n## 143          5.8         2.7          5.1         1.9  virginica\n## 144          6.8         3.2          5.9         2.3  virginica\n## 145          6.7         3.3          5.7         2.5  virginica\n## 146          6.7         3.0          5.2         2.3  virginica\n## 147          6.3         2.5          5.0         1.9  virginica\n## 148          6.5         3.0          5.2         2.0  virginica\n## 149          6.2         3.4          5.4         2.3  virginica\n## 150          5.9         3.0          5.1         1.8  virginica\nsubset(iris, Sepal.Length > 7.6,\n       select = c(Petal.Length, Petal.Width))\n##     Petal.Length Petal.Width\n## 118          6.7         2.2\n## 119          6.9         2.3\n## 123          6.7         2.0\n## 132          6.4         2.0\n## 136          6.1         2.3\n\n\nx <- 1:10\nsample(x, size = 5, replace = FALSE) # ë¹„ë³µì›ì¶”ì¶œ\n## [1] 6 4 8 7 2\nsample(x, size = 5, replace = TRUE)\n## [1] 9 5 6 3 7\n\nx <- 1:45\nsample(x, size = 6, replace = FALSE)\n## [1] 31 21 37  7  9 13\n\n\nidx <- sample(1:nrow(iris), size = 50,\n              replace = FALSE)\niris.50 <- iris[idx, ]  # 50ê°œì˜ í–‰ ì¶”ì¶œ\ndim(iris.50)            # í–‰ê³¼ ì—´ì˜ ê°œìˆ˜ í™•ì¸\n## [1] 50  5\nhead(iris.50)\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 107          4.9         2.5          4.5         1.7  virginica\n## 4            4.6         3.1          1.5         0.2     setosa\n## 60           5.2         2.7          3.9         1.4 versicolor\n## 72           6.1         2.8          4.0         1.3 versicolor\n## 142          6.9         3.1          5.1         2.3  virginica\n## 87           6.7         3.1          4.7         1.5 versicolor\n\n\nsample(1:20, size = 5)\n## [1] 14 18 20  2  4\nsample(1:20, size = 5)\n## [1] 19  8  1 16 11\nsample(1:20, size = 5)\n## [1] 11  7 20 12 18\n\n# ê°™ì€ ê°’ì´ ì¶”ì¶œë˜ë„ë¡ ê³ ì •ì‹œí‚¤ê³  ì‹¶ë‹¤ë©´\n# set.seed() í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ seedê°’ì„ ì§€ì •í•´ì£¼ë©´ ëœë‹¤.\nset.seed(100)\nsample(1:20, size = 5)\n## [1] 10  6 16 14 12\nset.seed(100)\nsample(1:20, size = 5)\n## [1] 10  6 16 14 12\nset.seed(100)\nsample(1:20, size = 5)\n## [1] 10  6 16 14 12\n\n\ncombn(1:5, 3) # 1~5ì—ì„œ 3ê°œë¥¼ ë½‘ëŠ” ì¡°í•©\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n## [1,]    1    1    1    1    1    1    2    2    2     3\n## [2,]    2    2    2    3    3    4    3    3    4     4\n## [3,]    3    4    5    4    5    5    4    5    5     5\n\nx = c(\"red\", \"green\", \"blue\", \"black\", \"white\")\ncom <- combn(x, 2) # xì˜ ì›ì†Œë¥¼ 2ê°œì”© ë½‘ëŠ” ì¡°í•©\ncom\n##      [,1]    [,2]   [,3]    [,4]    [,5]    [,6]    [,7]    [,8]    [,9]   \n## [1,] \"red\"   \"red\"  \"red\"   \"red\"   \"green\" \"green\" \"green\" \"blue\"  \"blue\" \n## [2,] \"green\" \"blue\" \"black\" \"white\" \"blue\"  \"black\" \"white\" \"black\" \"white\"\n##      [,10]  \n## [1,] \"black\"\n## [2,] \"white\"\n\nfor (i in 1:ncol(com)) {\n    # ì¡°í•©ì„ ì¶œë ¥\n    cat(com[, i], \"\\n\")\n}\n## red green \n## red blue \n## red black \n## red white \n## green blue \n## green black \n## green white \n## blue black \n## blue white \n## black white\n\n\n# aggregate(data, by = 'ê¸°ì¤€ì´ ë˜ëŠ” ì»¬ëŸ¼', FUN)\nagg <- aggregate(iris[, -5], by = list(iris$Species), FUN = mean)\nagg\n##      Group.1 Sepal.Length Sepal.Width Petal.Length Petal.Width\n## 1     setosa        5.006       3.428        1.462       0.246\n## 2 versicolor        5.936       2.770        4.260       1.326\n## 3  virginica        6.588       2.974        5.552       2.026\n\n\n# aggregateëŠ” ë°ì´í„°ì˜ íŠ¹ì • ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ í†µê³„ëŸ‰ì„ êµ¬í•´ì£¼ëŠ” í•¨ìˆ˜\nagg <- aggregate(iris[, -5], by = list(í‘œì¤€í¸ì°¨ = iris$Species), FUN = sd)\nagg\n##     í‘œì¤€í¸ì°¨ Sepal.Length Sepal.Width Petal.Length Petal.Width\n## 1     setosa    0.3524897   0.3790644    0.1736640   0.1053856\n## 2 versicolor    0.5161711   0.3137983    0.4699110   0.1977527\n## 3  virginica    0.6358796   0.3224966    0.5518947   0.2746501\n\n\nhead(mtcars)\n##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\nagg <- aggregate(mtcars, by = list(cyl = mtcars$cyl, vs = mtcars$vs), FUN = max)\nagg\n##   cyl vs  mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## 1   4  0 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n## 2   6  0 21.0   6 160.0 175 3.90 2.875 17.02  0  1    5    6\n## 3   8  0 19.2   8 472.0 335 4.22 5.424 18.00  0  1    5    8\n## 4   4  1 33.9   4 146.7 113 4.93 3.190 22.90  1  1    5    2\n## 5   6  1 21.4   6 258.0 123 3.92 3.460 20.22  1  0    4    4\n\nagg <- aggregate(mtcars, by = list(cyl = mtcars$cyl, vs = mtcars$vs), FUN = mean)\nagg\n##   cyl vs      mpg cyl   disp       hp     drat       wt     qsec vs        am\n## 1   4  0 26.00000   4 120.30  91.0000 4.430000 2.140000 16.70000  0 1.0000000\n## 2   6  0 20.56667   6 155.00 131.6667 3.806667 2.755000 16.32667  0 1.0000000\n## 3   8  0 15.10000   8 353.10 209.2143 3.229286 3.999214 16.77214  0 0.1428571\n## 4   4  1 26.73000   4 103.62  81.8000 4.035000 2.300300 19.38100  1 0.7000000\n## 5   6  1 19.12500   6 204.55 115.2500 3.420000 3.388750 19.21500  1 0.0000000\n##       gear     carb\n## 1 5.000000 2.000000\n## 2 4.333333 4.666667\n## 3 3.285714 3.500000\n## 4 4.000000 1.500000\n## 5 3.500000 2.500000\n\n\nx <- data.frame(name = c(\"a\", \"b\", \"c\"), math = c(90, 80, 40))\ny <- data.frame(name = c(\"a\", \"b\", \"d\"), korean = c(75, 60, 90))\nx ; y\n##   name math\n## 1    a   90\n## 2    b   80\n## 3    c   40\n##   name korean\n## 1    a     75\n## 2    b     60\n## 3    d     90\n\n\nz <- merge(x, y, by = c(\"name\"))\nz\n##   name math korean\n## 1    a   90     75\n## 2    b   80     60\n\n\nmerge(x, y, all.x = T)  # ì²« ë²ˆì§¸ ë°ì´í„°ì…‹ì˜ í–‰ë“¤ì€ ëª¨ë‘ í‘œì‹œë˜ë„ë¡\n##   name math korean\n## 1    a   90     75\n## 2    b   80     60\n## 3    c   40     NA\nmerge(x, y, all.y = T)  # ë‘ ë²ˆì§¸ ë°ì´í„°ì…‹ì˜ í–‰ë“¤ì€ ëª¨ë‘ í‘œì‹œë˜ë„ë¡\n##   name math korean\n## 1    a   90     75\n## 2    b   80     60\n## 3    d   NA     90\nmerge(x, y, all = T)    # ë‘ ë°ì´í„°ì…‹ì˜ ëª¨ë“  í–‰ë“¤ì´ í‘œì‹œë˜ë„ë¡\n##   name math korean\n## 1    a   90     75\n## 2    b   80     60\n## 3    c   40     NA\n## 4    d   NA     90\n\n\nx <- data.frame(name = c(\"a\", \"b\", \"c\"), math = c(90, 80, 40))\ny <- data.frame(sname = c(\"a\", \"b\", \"d\"), korean = c(75, 60, 90))\nx # ë³‘í•© ê¸°ì¤€ ì—´ì˜ ì´ë¦„ì´ name\n##   name math\n## 1    a   90\n## 2    b   80\n## 3    c   40\ny # ë³‘í•© ê¸°ì¤€ ì—´ì˜ ì´ë¦„ì´ sname\n##   sname korean\n## 1     a     75\n## 2     b     60\n## 3     d     90\nmerge(x, y, by.x = c(\"name\"), by.y = c(\"sname\"))\n##   name math korean\n## 1    a   90     75\n## 2    b   80     60"
  },
  {
    "objectID": "Spatial_Information_Analysis/ì‹¤ê¸°_1ìœ í˜•.html",
    "href": "Spatial_Information_Analysis/ì‹¤ê¸°_1ìœ í˜•.html",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 1ìœ í˜• ë¬¸ì œí’€ì´",
    "section": "",
    "text": "ì‹¤ê¸° 1 ìœ í˜•"
  },
  {
    "objectID": "Spatial_Information_Analysis/ì‹¤ê¸°_1ìœ í˜•.html#ë°ì´í„°-ë‹¤ë£¨ê¸°-ìœ í˜•",
    "href": "Spatial_Information_Analysis/ì‹¤ê¸°_1ìœ í˜•.html#ë°ì´í„°-ë‹¤ë£¨ê¸°-ìœ í˜•",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 1ìœ í˜• ë¬¸ì œí’€ì´",
    "section": "âœ… ë°ì´í„° ë‹¤ë£¨ê¸° ìœ í˜•",
    "text": "âœ… ë°ì´í„° ë‹¤ë£¨ê¸° ìœ í˜•\n\n\në°ì´í„° íƒ€ì…(object, int, float, bool ë“±)\nê¸°ì´ˆí†µê³„ëŸ‰(í‰ê· , ì¤‘ì•™ê°’, ì‚¬ë¶„ìœ„ìˆ˜, IQR, í‘œì¤€í¸ì°¨ ë“±)\në°ì´í„° ì¸ë±ì‹±, í•„í„°ë§, ì •ë ¬, ë³€ê²½ ë“±\nì¤‘ë³µê°’, ê²°ì¸¡ì¹˜, ì´ìƒì¹˜ ì²˜ë¦¬ (ì œê±° or ëŒ€ì²´)\në°ì´í„° Scaling (ë°ì´í„° í‘œì¤€í™”(z), ë°ì´í„° ì •ê·œí™”(min-max))\në°ì´í„° í•©ì¹˜ê¸°\në‚ ì§œ/ì‹œê°„ ë°ì´í„°, index ë‹¤ë£¨ê¸°"
  },
  {
    "objectID": "Spatial_Information_Analysis/ì‹¤ê¸°_1ìœ í˜•.html#í•µì‹¬ë¬¸ì œ-27ê°œ-110",
    "href": "Spatial_Information_Analysis/ì‹¤ê¸°_1ìœ í˜•.html#í•µì‹¬ë¬¸ì œ-27ê°œ-110",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 1ìœ í˜• ë¬¸ì œí’€ì´",
    "section": "âœ… í•µì‹¬ë¬¸ì œ 27ê°œ (1~10)",
    "text": "âœ… í•µì‹¬ë¬¸ì œ 27ê°œ (1~10)\n\n# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\nimport pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"mtcars.txt\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      Hornet Sportabout\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\n# ë¬¸ì œ 1\n# mpg ë³€ìˆ˜ì˜ ì œ 1ì‚¬ë¶„ìœ„ìˆ˜ë¥¼ êµ¬í•˜ê³  ì •ìˆ˜ê°’ìœ¼ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (í’€ì´)\nQ1 = df['mpg'].quantile(.25)\nprint(round(Q1))\n\n15\n\n\n\n# ë¬¸ì œ 2\n# mpg ê°’ì´ 19ì´ìƒ 21ì´í•˜ì¸ ë°ì´í„°ì˜ ìˆ˜ë¥¼ êµ¬í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (í’€ì´ 1)\ncond1 = (df[(df['mpg']>=19) & (df['mpg']<=21)])\nprint(len(cond1))\n\n# (í’€ì´ 2)\n# cond1 = (df['mpg']>=19)\n# cond2 = (df['mpg']<=21)\n# print(len(df[cond1&cond2]))\n\n5\n\n\n\n# ë¬¸ì œ 3\n# hp ë³€ìˆ˜ì˜ IQR ê°’ì„ êµ¬í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (í’€ì´)\nQ1 = df['hp'].quantile(.25)\nQ3 = df['hp'].quantile(.75)\nIQR = Q3 - Q1\nprint(IQR)\n\n83.5\n\n\n\n# ë¬¸ì œ 4 \n# wt ë³€ìˆ˜ì˜ ìƒìœ„ 10ê°œ ê°’ì˜ ì´í•©ì„ êµ¬í•˜ì—¬ ì†Œìˆ˜ì ì„ ë²„ë¦¬ê³  ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (í’€ì´)\ntop10 = df.sort_values('wt', ascending=False)\nsum_top10 = sum(top10['wt'].head(10))\nprint(int(sum_top10))\n\n# top_10 = df.sort_values('wt',ascending=False).head(10)\n# print(int(sum(top_10['wt']))) #ì£¼ì˜: ì†Œìˆ˜ì  ë°˜ì˜¬ë¦¼ì´ ì•„ë‹ˆë¼ ë²„ë¦¬ëŠ” ë¬¸ì œ\n\n42\n\n\n\n# ë¬¸ì œ 5\n# ì „ì²´ ìë™ì°¨ì—ì„œ cylê°€ 6ì¸ ë¹„ìœ¨ì´ ì–¼ë§ˆì¸ì§€ ì†Œìˆ˜ì  ì²«ì§¸ì§œë¦¬ê¹Œì§€ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\ncond1 = len(df[df['cyl']==6])\ncond2 = len(df['cyl'])\nprint(round(cond1/cond2, 1))\n\n0.2\n\n\n\n# ë¬¸ì œ 6\n# ì²«ë²ˆì§¸ í–‰ë¶€í„° ìˆœì„œëŒ€ë¡œ 10ê°œ ë½‘ì€ í›„ mpg ì—´ì˜ í‰ê· ê°’ì„ ë°˜ì˜¬ë¦¼í•˜ì—¬ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (í’€ì´)\ncond1 = df.head(10) # df[:10], df.loc[0:9]\nmean_mpg = cond1['mpg'].mean()\nprint(round(mean_mpg))\n\n20\n\n\n\n# ë¬¸ì œ 7\n# ì²«ë²ˆì§¸ í–‰ë¶€í„° ìˆœì„œëŒ€ë¡œ 50% ê¹Œì§€ ë°ì´í„°ë¥¼ ë½‘ì•„ wt ë³€ìˆ˜ì˜ ì¤‘ì•™ê°’ì„ êµ¬í•˜ì‹œì˜¤\ndf = pd.read_csv(\"mtcars.txt\")\n\n\n# (í’€ì´)\n# 50% ë°ì´í„°ì— í•´ë‹¹í•˜ëŠ” í–‰ì˜ ìˆ˜ (ì •ìˆ˜ë¡œ êµ¬í•˜ê¸°)\np50 = int(len(df)*0.5)\ndf50 = df[:p50]\nprint(df50['wt'].median())\n\n# len(df)/2\n# cond1 = df[:17]\n# cond2 = cond1['wt'].median()\n# print(cond2)\n\n3.44\n\n\n\n# ë¬¸ì œ 8\n# ê²°ì¸¡ê°’ì´ ìˆëŠ” ë°ì´í„°ì˜ ìˆ˜ë¥¼ ì¶œë ¥í•˜ì‹œì˜¤\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ì œí’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3.0\n      300\n    \n    \n      1\n      20220105\n      B\n      NaN\n      400\n    \n    \n      2\n      None\n      None\n      5.0\n      500\n    \n    \n      3\n      20230127\n      B\n      10.0\n      600\n    \n    \n      4\n      20220203\n      A\n      10.0\n      400\n    \n  \n\n\n\n\n\n# (í’€ì´)\nm_value = df.isnull().sum()\nprint(m_value.sum())\n\n5\n\n\n\n# ë¬¸ì œ 9 \n# 'íŒë§¤ìˆ˜' ì»¬ëŸ¼ì˜ ê²°ì¸¡ê°’ì„ íŒë§¤ìˆ˜ì˜ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´í•˜ê³  íŒë§¤ìˆ˜ì˜ \n#  í‰ê· ê°’ì„ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\n\n\n# (í’€ì´)\ndf[df['íŒë§¤ìˆ˜'].isnull()]\nmedian = df['íŒë§¤ìˆ˜'].median()\ndf['íŒë§¤ìˆ˜'] = df['íŒë§¤ìˆ˜'].fillna(median)\nmean = df['íŒë§¤ìˆ˜'].mean()\nprint(int(mean))\n\n15\n\n\n\n# ë¬¸ì œ 10\n# íŒë§¤ìˆ˜ ì»¬ëŸ¼ì— ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ì„ ì œê±°í•˜ê³ \n# ì²«ë²ˆì§¸ í–‰ë¶€í„° ìˆœì„œëŒ€ë¡œ 50%ê¹Œì§€ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬\n# íŒë§¤ìˆ˜ ë³€ìˆ˜ì˜ Q1(ì œ1ì‚¬ë¶„ìœ„ìˆ˜) ê°’ì„ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\n\n\n# (í’€ì´)\n# ê²°ì¸¡ì¹˜ ì‚­ì œ(í–‰ ê¸°ì¤€)\ndf = df['íŒë§¤ìˆ˜'].dropna()\n# ì²«ë²ˆì§¸ í–‰ë¶€í„° ìˆœì„œëŒ€ë¡œ 50%ê¹Œì§€ì˜ ë°ì´í„° ì¶”ì¶œ\nper50 = int(len(df)*0.5)\ndf = df[:per50]\n# ê°’êµ¬í•˜ê¸°\nprint(round(df.quantile(.25)))\n\n# df[df['íŒë§¤ìˆ˜'].isnull()]\n# df['íŒë§¤ìˆ˜'] = df['íŒë§¤ìˆ˜'].dropna()\n# int(len(df['íŒë§¤ìˆ˜'])/2)\n# cond1 = df['íŒë§¤ìˆ˜'].head(6)\n# Q1 = cond1.quantile(.25)\n# print(int(Q1))\n\n5"
  },
  {
    "objectID": "Spatial_Information_Analysis/ì‹¤ê¸°_1ìœ í˜•.html#í•µì‹¬ë¬¸ì œ-27ê°œ-1120",
    "href": "Spatial_Information_Analysis/ì‹¤ê¸°_1ìœ í˜•.html#í•µì‹¬ë¬¸ì œ-27ê°œ-1120",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 1ìœ í˜• ë¬¸ì œí’€ì´",
    "section": "âœ… í•µì‹¬ë¬¸ì œ 27ê°œ (11~20)",
    "text": "âœ… í•µì‹¬ë¬¸ì œ 27ê°œ (11~20)\n\ndf = pd.read_csv('mtcars.txt')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      Hornet Sportabout\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\n# ë¬¸ì œ 11\n# cylê°€ 4ì¸ ìë™ì°¨ì™€ 6ì¸ ìë™ì°¨ ê·¸ë£¹ì˜ mpg í‰ê· ê°’ì´ ì°¨ì´ë¥¼ ì ˆëŒ€ê°’ìœ¼ë¡œ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\ncyl_4 = df[df['cyl']==4]\ncyl_4_mean = cyl_4['mpg'].mean()\n\ncyl_6 = df[df['cyl']==6]\ncyl_6_mean = cyl_6['mpg'].mean()\n\nprint(round(abs(cyl_4_mean - cyl_6_mean)))\n\n7\n\n\n\n# ë¬¸ì œ 12\n# hp ë³€ìˆ˜ì— ëŒ€í•´ ë°ì´í„°í‘œì¤€í™”(Z-score)ë¥¼ ì§„í–‰í•˜ê³  ì´ìƒì¹˜ì˜ ìˆ˜ë¥¼ êµ¬í•˜ì‹œì˜¤\n# (ë‹¨, ì´ìƒì¹˜ëŠ” Zê°’ì´ 1.5ë¥¼ ì´ˆê³¼í•˜ê±°ë‚˜ -1.5ë¯¸ë§Œì¸ ê°’ì´ë‹¤)\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\n# Z = (X-í‰ê· ) / í‘œì¤€í¸ì°¨\nstd = df['hp'].std()\nmean_hp = df['hp'].mean()\ndf['zscore'] = (df['hp']-mean_hp)/std\n\ncond1 = (df['zscore']>1.5)\ncond2 = (df['zscore']<-1.5)\nprint(len(df[cond1] + df[cond2]))\n\n2\n\n\n\n# ë¬¸ì œ 13\n# mpg ì»¬ëŸ¼ì„ ìµœì†ŒìµœëŒ€ Scalingì„ ì§„í–‰í•œ í›„ 0.7ë³´ë‹¤ í° ê°’ì„ ê°€ì§€ëŠ” ë ˆì½”ë“œ ìˆ˜ë¥¼ êµ¬í•˜ë¼\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\nfrom sklearn.preprocessing import MinMaxScaler\nmscaler = MinMaxScaler()\ndf['mpg'] = mscaler.fit_transform(df[['mpg']])\nprint(len(df[df['mpg']>0.7]))\n\n# ê³µì‹ : (x-min) / (max-min)\n# min_mpg = df['mpg'].min()\n# max_mpg = df['mpg'].max()\n# df['mpg'] = (df['mpg'] - min_mpg)/(max_mpg - min_mpg)\n# cond1 = (df['mpg']>0.7)\n# print(len(df[cond1]))\n\n5\n\n\n\n# ë¬¸ì œ 14\n# wt ì»¬ëŸ¼ì— ëŒ€í•´ ìƒìê·¸ë¦¼ ê¸°ì¤€ìœ¼ë¡œ ì´ìƒì¹˜ì˜ ê°œìˆ˜ë¥¼ êµ¬í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\n# ì´ìƒì¹˜ : Q1, Q3 ë¡œë¶€í„° 1.5*IQRì„ ë„˜ì–´ê°€ëŠ” ê°’\nQ1 = df['wt'].quantile(.25)\nQ3 = df['wt'].quantile(.75)\nIQR = Q3-Q1\n\nupper = Q3 + (1.5*IQR)\nlower = Q1 - (1.5*IQR)\n\ncond1 = (df['wt'] > upper)\ncond2 = (df['wt'] < lower)\n\nprint(len(df[cond1] + df[cond2]))\n\n3\n\n\n\n# ë¬¸ì œ 15\n# íŒë§¤ìˆ˜ ì»¬ëŸ¼ì˜ ê²°ì¸¡ì¹˜ë¥¼ ìµœì†Œê°’ìœ¼ë¡œ ëŒ€ì²´í•˜ê³ \n# ê²°ì¸¡ì¹˜ê°€ ìˆì„ ë•Œì™€ ìµœì†Œê°’ìœ¼ë¡œ ëŒ€ì²´í–ˆì„ ë•Œ\n# í‰ê· ê°’ì˜ ì°¨ì´ë¥¼ ì ˆëŒ€ê°’ìœ¼ë¡œ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105',None,'20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B',None,'B','A',None,'A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,None,5,10,10,10,15,15,20,None,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ì œí’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3.0\n      300\n    \n    \n      1\n      20220105\n      B\n      NaN\n      400\n    \n    \n      2\n      None\n      None\n      5.0\n      500\n    \n    \n      3\n      20230127\n      B\n      10.0\n      600\n    \n    \n      4\n      20220203\n      A\n      10.0\n      400\n    \n    \n      5\n      20220205\n      None\n      10.0\n      500\n    \n    \n      6\n      20230210\n      A\n      15.0\n      500\n    \n    \n      7\n      20230223\n      B\n      15.0\n      600\n    \n    \n      8\n      20230312\n      A\n      20.0\n      600\n    \n    \n      9\n      20230422\n      B\n      NaN\n      700\n    \n    \n      10\n      20220505\n      A\n      30.0\n      600\n    \n    \n      11\n      20230511\n      A\n      40.0\n      600\n    \n  \n\n\n\n\n\n# (í’€ì´)\n# ë°ì´í„° ë³µì‚¬\ndf2 = df.copy()\n# ìµœì†Œê°’ìœ¼ë¡œ ê²°ì¸¡ì¹˜ ëŒ€ì²´\nmin = df['íŒë§¤ìˆ˜'].min()\ndf2['íŒë§¤ìˆ˜'] = df2['íŒë§¤ìˆ˜'].fillna(min)\n\n# ê²°ì¸¡ì¹˜ê°€ ìˆì„ë•Œ, ëŒ€ì²´í–ˆì„ë•Œ í‰ê· \nm_yes = df['íŒë§¤ìˆ˜'].mean()\nm_no = df2['íŒë§¤ìˆ˜'].mean()\n\nprint(round(abs(m_yes - m_no)))\n\n2\n\n\n\n# ë¬¸ì œ 16\n# vsë³€ìˆ˜ê°€ 0ì´ ì•„ë‹Œ ì°¨ëŸ‰ ì¤‘ì— mpg ê°’ì´ ê°€ì¥ í° ì°¨ëŸ‰ì˜ hp ê°’ì„ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\ncond1 = df[df['vs']!=0]\n# mpg ê°’ì´ ê°€ì¥ í° ì°¨ëŸ‰(ë‚´ë¦¼ì°¨ìˆœ)\ncond1 = cond1.sort_values('mpg',ascending=False)\n# cond1\nprint(cond1['hp'].iloc[0])\n\n65\n\n\n\n# ë¬¸ì œ 17\n# gear ë³€ìˆ˜ê°’ì´ 3, 4ì¸ ë‘ ê·¸ë£¹ì˜ hp í‘œì¤€í¸ì°¨ê°’ì˜ ì°¨ì´ë¥¼ ì ˆëŒ€ê°’ìœ¼ë¡œ \n# ì†Œìˆ˜ì  ì²«ì§¸ìë¦¬ê¹Œì§€ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\n# ë‘ê°œ ê·¸ë£¹ìœ¼ë¡œ í•„í„°ë§\ncond1 = df[df['gear']==3]\ncond2 = df[df['gear']==4]\n# std êµ¬í•˜ê¸°\nstd3 = cond1['hp'].std()\nstd4 = cond2['hp'].std()\n\nprint(round(abs(std3 - std4),1))\n\n21.8\n\n\n\n# ë¬¸ì œ 18\n# gear ë³€ìˆ˜ì˜ ê°‘ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ mpg í‰ê· ê°’ì„ ì‚°ì¶œí•˜ê³ \n# í‰ê· ê°’ì´ ë†’ì€ ê·¸ë£¹ì˜ mpg ì œ3ì‚¬ë¶„ìœ„ìˆ˜ ê°’ì„ êµ¬í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\ndf['gear'].unique()\ngear3 = df[df['gear']==3]\ngear4 = df[df['gear']==4]\ngear5 = df[df['gear']==5]\n\nm_3 = gear3['mpg'].mean()\nm_4 = gear4['mpg'].mean()\nm_5 = gear5['mpg'].mean()\n\n(m_3, m_4, m_5) # m_4ì˜ í‰ê· ê°’ì´ ê°€ì¥ í¼\nQ3 = gear4['mpg'].quantile(.75)\nprint(Q3)\n\n28.075\n\n\n\n# (í’€ì´_v2)\n# gear, mpg ë³€ìˆ˜ë§Œ í•„í„°ë§\ndf = df.loc[:, ['gear', 'mpg']]\n\n# gear ë³€ìˆ˜ë¡œ ê·¸ë£¹í™”í•˜ì—¬ mpg í‰ê· ê°’ ë³´ê¸°\n# print(df.groupby('gear').mean()) # gear 4ê·¸ë£¹ì´ ì œì¼ ë†’ìŒ\n\n# gear=4ì¸ ê·¸ë£¹ì˜ mpg Q3 êµ¬í•˜ê¸°\ngear4 = df[df['gear']==4]\nprint(gear4['mpg'].quantile(.75))\n\n28.075\n\n\n\n# ë¬¸ì œ 19\n# hp í•­ëª©ì˜ ìƒìœ„ 7ë²ˆì§¸ ê°’ìœ¼ë¡œ ìƒìœ„ 7ê°œ ê°’ì„ ë³€í™˜í•œ í›„,\n# hpê°€ 150 ì´ìƒì¸ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ hpì˜ í‰ê· ê°’ì„ ë°˜ì˜¬ë¦¼í•˜ì—¬ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\n# hp ì—´ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬\ndf = df.sort_values('hp', ascending=False)\n\n# ì¸ë±ìŠ¤ ì´ˆê¸°í™” - ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬ì‹œ ìµœì´ˆì˜ ì¸ë±ìŠ¤ë¡œ ìˆê¸°ì—\ndf = df.reset_index(drop=True) # drop=True ëŠ” ê¸°ì¡´ index ì‚­ì œ\n# print(df)\n\n# hp ìƒìœ„ 7ë²ˆì§¸ ê°’ í™•ì¸\ntop7 = df['hp'].loc[6] # top7 = 205\n\n# np.where í™œìš©\nimport numpy as np\ndf['hp'] = np.where(df['hp'] >= 205, top7, df['hp'])\n# np.where(ì¡°ê±´, ì¡°ê±´ì— í•´ë‹¹í•  ë•Œ ê°’, ê·¸ë ‡ì§€ ì•Šì„ ë•Œ ê°’)\n\n# hp 150ì´ìƒì¸ ë°ì´í„°\ncond1 = (df['hp']>=150)\ndf = df[cond1]\nprint(round(df['hp'].mean()))\n\n187\n\n\n\n# ë¬¸ì œ 20\n# car ë³€ìˆ˜ì— Merc ë¬´êµ¬ê°€ í¬í•¨ëœ ìë™ì°¨ì˜ mpg í‰ê· ê°’ì„ ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ì‹œì˜¤\ndf = pd.read_csv('mtcars.txt')\n\n\n# (í’€ì´)\ndf2 = df[df['car'].str.contains('Merc')] # ë¬¸ìì—´ ì¶”ì¶œ ì•Œì•„ë‘ê¸°\nprint(round(df2['mpg'].mean()))\n\n# ì‹œí—˜í™˜ê²½ì—ì„œ ë‹µêµ¬í•˜ëŠ” ë°©ë²•(reset_index() ì‚¬ìš© í›„)\n# ì‹œí—˜ì—ì„œëŠ” carê°€ indexë¡œ ë˜ì–´ ìˆìŒ\n# df.reset_index(inplace=True)\n# df2 = df[df['index'].str.contains(\"Merc\")] \n# print(round(df2['mpg'].mean()))\n\n19"
  },
  {
    "objectID": "Spatial_Information_Analysis/ì‹¤ê¸°_1ìœ í˜•.html#í•µì‹¬ë¬¸ì œ-27ê°œ-2127",
    "href": "Spatial_Information_Analysis/ì‹¤ê¸°_1ìœ í˜•.html#í•µì‹¬ë¬¸ì œ-27ê°œ-2127",
    "title": "ë¹…ë¶„ê¸° ì‹¤ê¸° - 1ìœ í˜• ë¬¸ì œí’€ì´",
    "section": "âœ… í•µì‹¬ë¬¸ì œ 27ê°œ (21~27)",
    "text": "âœ… í•µì‹¬ë¬¸ì œ 27ê°œ (21~27)\n\n# ë¬¸ì œ 21\n# 22ë…„ 1ë¶„ê¸° Aì œí’ˆì˜ ë§¤ì¶œì•¡ì„ êµ¬í•˜ì‹œì˜¤\n# (ë§¤ì¶œì•¡ = íŒë§¤ìˆ˜*ê°œë‹¹ìˆ˜ìµ)\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ì œí’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# # (í’€ì´)\n# cond1 = df[df['ë‚ ì§œ']<='20220431']\n# cond2 = cond1[cond1['ì œí’ˆ']=='A']\n# cond2['ë§¤ì¶œì•¡'] = (cond2['íŒë§¤ìˆ˜']*cond2['ê°œë‹¹ìˆ˜ìµ'])\n# cond2\n\n# print(cond2['ë§¤ì¶œì•¡'].sum())\n\n\n# (í’€ì´_v2)\n# ë°ì´í„° íƒ€ì… datetimeìœ¼ë¡œ ë³€ê²½\ndf['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])\n\n# ë…„, ì›”, ì¼ ë³€ìˆ˜(ì—´) ì¶”ê°€í•˜ê¸°\ndf['year'] = df['ë‚ ì§œ'].dt.year\ndf['month'] = df['ë‚ ì§œ'].dt.month\ndf['day'] = df['ë‚ ì§œ'].dt.day\n\n# ë§¤ì¶œì•¡ ë³€ìˆ˜ ì¶”ê°€í•˜ê¸°\ndf['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\n# 22ë…„ìœ¼ë¡œ í•„í„°ë§\ndf = df[df['year']==2022]\ndf.head()\n\n# 1,2,3ì›” ë§¤ì¶œì•¡ ê³„ì‚°\nm1 = df[df['month']==1]['ë§¤ì¶œì•¡'].sum()\nm2 = df[df['month']==2]['ë§¤ì¶œì•¡'].sum()\nm3 = df[df['month']==3]['ë§¤ì¶œì•¡'].sum()\nprint(m1+m2+m3)\n\n11900\n\n\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['year'] = df['ë‚ ì§œ'].dt.year\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['month'] = df['ë‚ ì§œ'].dt.month\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['day'] = df['ë‚ ì§œ'].dt.day\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\876841120.py:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\n\n\n# ë¬¸ì œ 22\n# 22ë…„ê³¼ 23ë…„ì˜ ì´ ë§¤ì¶œì•¡ ì°¨ì´ë¥¼ ì ˆëŒ€ê°’ìœ¼ë¡œ êµ¬í•˜ì‹œì˜¤\n# (ë§¤ì¶œì•¡ = íŒë§¤ìˆ˜*ê°œë‹¹ìˆ˜ìµ)\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ì œí’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# (í’€ì´)\n# ë°ì´í„° íƒ€ì… datetimeìœ¼ë¡œ ë³€ê²½\ndf['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])\n\n# ë…„ ë³€ìˆ˜(ì—´) ì¶”ê°€í•˜ê¸°\ndf['year'] = df['ë‚ ì§œ'].dt.year\n\n# ë§¤ì¶œì•¡ ë³€ìˆ˜ ì¶”ê°€í•˜ê¸°\ndf['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\ncond22 = df[df['year']==2022]\ncond23 = df[df['year']==2023]\n\nprint(abs((cond22['ë§¤ì¶œì•¡'].sum()) - (cond23['ë§¤ì¶œì•¡'].sum())))\n\n48600\n\n\n\n# ë¬¸ì œ 23\n# 23ë…„ ì´ ë§¤ì¶œì•¡ì´ í° ì œí’ˆì˜ 23ë…„ íŒë§¤ìˆ˜ë¥¼ êµ¬í•˜ì‹œì˜¤\n# (ë§¤ì¶œì•¡ = íŒë§¤ìˆ˜*ê°œë‹¹ìˆ˜ìµ)\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ì œí’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# ë°ì´í„° íƒ€ì… datetimeìœ¼ë¡œ ë³€ê²½\ndf['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])\n\n# ë…„ ë³€ìˆ˜(ì—´) ì¶”ê°€í•˜ê¸°\ndf['year'] = df['ë‚ ì§œ'].dt.year\n\n# ë§¤ì¶œì•¡ ë³€ìˆ˜ ì¶”ê°€í•˜ê¸°\ndf['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\ndf = df[df['year']==2023]\n# df.head()\n\n# 23ë…„ A ë§¤ì¶œì•¡ê³¼ B ë§¤ì¶œì•¡ ë³„ë„ë¡œ êµ¬í•˜ê¸°\n# A ë§¤ì¶œì•¡\ndf_a = df[df['ì œí’ˆ']=='A']\na_sales = df_a['ë§¤ì¶œì•¡'].sum()\n# print(a_sales) # 46000\n\n# B ë§¤ì¶œì•¡\ndf_b = df[df['ì œí’ˆ']=='B']\nb_sales = df_b['ë§¤ì¶œì•¡'].sum()\n# print(b_sales) # 32500\n\n# A ì œí’ˆì˜ 23ë…„ íŒë§¤ìˆ˜\na_sum = df_a['íŒë§¤ìˆ˜'].sum()\nprint(a_sum)\n\n80\n\n\n\n# ë¬¸ì œ 24\n# ë§¤ì¶œì•¡ì´ 4ì²œì› ì´ˆê³¼, 1ë§Œì› ë¯¸ë§Œì¸ ë°ì´í„° ìˆ˜ë¥¼ ì¶œë ¥í•˜ì‹œì˜¤\n# (ë§¤ì¶œì•¡ = íŒë§¤ìˆ˜*ê°œë‹¹ìˆ˜ìµ)\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë‚ ì§œ' : ['20220103','20220105','20230105','20230127','20220203','20220205',\n            '20230210','20230223','20230312','20230422','20220505','20230511'],\n    'ì œí’ˆ' : ['A','B','A','B','A','B','A','B','A','B','A','A'],\n    'íŒë§¤ìˆ˜' : [3,5,5,10,10,10,15,15,20,25,30,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [300,400,500,600,400,500,500,600,600,700,600,600]\n})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      ë‚ ì§œ\n      ì œí’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      20220103\n      A\n      3\n      300\n    \n    \n      1\n      20220105\n      B\n      5\n      400\n    \n    \n      2\n      20230105\n      A\n      5\n      500\n    \n    \n      3\n      20230127\n      B\n      10\n      600\n    \n    \n      4\n      20220203\n      A\n      10\n      400\n    \n  \n\n\n\n\n\n# (í’€ì´)\ndf['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\ncond1 = df['ë§¤ì¶œì•¡']>4000\ncond2 = df['ë§¤ì¶œì•¡']<10000\n\ndf = df[cond1&cond2]\nprint(len(df))\n\n4\n\n\n\n# ë¬¸ì œ 25\n# 23ë…„ 9ì›” 24ì¼ 16:00~22:00 ì‚¬ì´ì— ì „ì²´ ì œí’ˆì˜ íŒë§¤ìˆ˜ë¥¼ êµ¬í•˜ì‹œì˜¤\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë¬¼í’ˆ' : ['A','B','A','B','A','B','A'],\n    'íŒë§¤ìˆ˜' : [5,10,15,15,20,25,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [500,600,500,600,600,700,600]})\ntime = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf['time'] = time\ndf = df[['time', 'ë¬¼í’ˆ', 'íŒë§¤ìˆ˜', 'ê°œë‹¹ìˆ˜ìµ']]\ndf\n\n\n\n\n\n  \n    \n      \n      time\n      ë¬¼í’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n  \n  \n    \n      0\n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      1\n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2\n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      3\n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      4\n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      5\n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      6\n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# (í’€ì´)\n# ì»¬ëŸ¼ê³¼ ì¸ë±ìŠ¤ì— ë‘˜ë‹¤ time ë³€ìˆ˜ë¥¼ ë†“ê³  í’€ì´\n# ë°ì´í„° íƒ€ì… datetimeìœ¼ë¡œ ë³€ê²½ (í•­ìƒ df.info()ë¡œ ë°ì´í„° íƒ€ì… í™•ì¸í•  ê²ƒ)\ndf['time'] = pd.to_datetime(df['time'])\n\n# index ìƒˆë¡œ ì§€ì •\ndf = df.set_index('time', drop=False) # defaultëŠ” True\n\n# 9ì›” 24ì¼ í•„í„°ë§ // df['ë³€ìˆ˜'].between( , )\ndf_after = df[df['time'].between('2023-09-24', '2023-09-25')] # 25ì¼ì€ ë¯¸í¬í•¨\n\n# ì‹œê°„ í•„í„°ë§ 16:00 ~ 22:00 (ì£¼ì˜: ì‹œê°„ì´ indexì— ìœ„ì¹˜í•´ì•¼ í•¨)\ndf = df_after.between_time(start_time='16:00', end_time='22:00') # í¬í•¨ ê¸°ì¤€\n\n# ì „ì²´ íŒë§¤ìˆ˜ êµ¬í•˜ê¸°\nprint(df['íŒë§¤ìˆ˜'].sum())\n\n25\n\n\n\n# (í’€ì´2) // ìœ„ì— df ìƒˆë¡œ ë¶ˆëŸ¬ì™€ì„œ ì‹¤í–‰í•´ë³´ê¸°\n# ë°ì´í„° íƒ€ì… datetimeìœ¼ë¡œ ë³€ê²½\ndf['time'] = pd.to_datetime(df['time'])\n\n# index ìƒˆë¡œ ì§€ì •\ndf = df.set_index('time')\n\n# loc í•¨ìˆ˜ë¡œ í•„í„°ë§\ndf = df.loc[(df.index >= '2023-09-24 16:00:00') & (df.index <= '2023-09-24 22:00:00')]\n\n# ì „ì²´ íŒë§¤ìˆ˜ êµ¬í•˜ê¸°\nprint(df['íŒë§¤ìˆ˜'].sum())\n\n25\n\n\n\n# ë¬¸ì œ 26\n# 9ì›” 25ì¼ 00:10~12:00ê¹Œì§€ì˜ Bë¬¼í’ˆì˜ ë§¤ì¶œì•¡ ì´í•©ì„ êµ¬í•˜ì‹œì˜¤\n# (ë§¤ì¶œì•¡ = íŒë§¤ìˆ˜*ê°œë‹¹ìˆ˜ìµ)\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë¬¼í’ˆ' : ['A','B','A','B','A','B','A'],\n    'íŒë§¤ìˆ˜' : [5,10,15,15,20,25,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [500,600,500,600,600,700,600]})\ndf['time'] = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf = df[['time', 'ë¬¼í’ˆ', 'íŒë§¤ìˆ˜', 'ê°œë‹¹ìˆ˜ìµ']]\ndf = df.set_index('time',drop=True)\ndf\n\n\n\n\n\n  \n    \n      \n      ë¬¼í’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n    \n      time\n      \n      \n      \n    \n  \n  \n    \n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# (í’€ì´)\n# ë§¤ì¶œì•¡ ë³€ìˆ˜ ì¶”ê°€\ndf['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\n# loc í•¨ìˆ˜ë¡œ í•„í„°ë§\ndf = df.loc[(df.index >= '2023-09-25 00:10:00') & (df.index <= '2023-09-25 12:00:00')]\n\n# B ë¬¼í’ˆì˜ ë§¤ì¶œì•¡ ì´í•©\ncond1 = df[df['ë¬¼í’ˆ']=='B']\nprint(cond1['ë§¤ì¶œì•¡'].sum())\n\n26500\n\n\nC:\\Users\\Hyunsoo Kim\\AppData\\Local\\Temp\\ipykernel_2420\\1645811142.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\n\n\n# ë¬¸ì œ 27\n# 9ì›” 24ì¼ 12:00~24:00ê¹Œì§€ì˜ Aë¬¼í’ˆì˜ ë§¤ì¶œì•¡ ì´í•©ì„ êµ¬í•˜ì‹œì˜¤\n# (ë§¤ì¶œì•¡ = íŒë§¤ìˆ˜*ê°œë‹¹ìˆ˜ìµ)\n\n# ë°ì´í„° ìƒì„±(ìˆ˜ì •ê¸ˆì§€)\ndf = pd.DataFrame({\n    'ë¬¼í’ˆ' : ['A','B','A','B','A','B','A'],\n    'íŒë§¤ìˆ˜' : [5,10,15,15,20,25,40],\n    'ê°œë‹¹ìˆ˜ìµ' : [500,600,500,600,600,700,600]})\ndf['time'] = pd.date_range('2023-09-24 12:25:00','2023-09-25 14:45:30', periods=7)\ndf = df[['time', 'ë¬¼í’ˆ', 'íŒë§¤ìˆ˜', 'ê°œë‹¹ìˆ˜ìµ']]\ndf = df.set_index('time',drop=True)\ndf\n\n\n\n\n\n  \n    \n      \n      ë¬¼í’ˆ\n      íŒë§¤ìˆ˜\n      ê°œë‹¹ìˆ˜ìµ\n    \n    \n      time\n      \n      \n      \n    \n  \n  \n    \n      2023-09-24 12:25:00\n      A\n      5\n      500\n    \n    \n      2023-09-24 16:48:25\n      B\n      10\n      600\n    \n    \n      2023-09-24 21:11:50\n      A\n      15\n      500\n    \n    \n      2023-09-25 01:35:15\n      B\n      15\n      600\n    \n    \n      2023-09-25 05:58:40\n      A\n      20\n      600\n    \n    \n      2023-09-25 10:22:05\n      B\n      25\n      700\n    \n    \n      2023-09-25 14:45:30\n      A\n      40\n      600\n    \n  \n\n\n\n\n\n# (í’€ì´)\n# ë§¤ì¶œì•¡ ë³€ìˆ˜ ì¶”ê°€\ndf['ë§¤ì¶œì•¡'] = df['íŒë§¤ìˆ˜'] * df['ê°œë‹¹ìˆ˜ìµ']\n\n# loc í•¨ìˆ˜ë¡œ í•„í„°ë§\ndf = df.loc[(df.index >= '2023-09-24 12:00:00') & (df.index < '2023-09-25 00:00:00')]\n\n# A ë¬¼í’ˆì˜ ë§¤ì¶œì•¡ ì´í•©\ncond1 = df[df['ë¬¼í’ˆ']=='A']\nprint(cond1['ë§¤ì¶œì•¡'].sum())\n\n10000"
  }
]